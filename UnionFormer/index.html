<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization | zhaozw后院</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="发表于CVPR2024,集成三个视图的UnionFormer框架,一个调节不同尺度上空间一致性的篡改特征提取网络BSFI-Net。">
<meta property="og:type" content="article">
<meta property="og:title" content="UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization">
<meta property="og:url" content="https://zhaozw-szu.github.io/UnionFormer/index.html">
<meta property="og:site_name" content="zhaozw后院">
<meta property="og:description" content="发表于CVPR2024,集成三个视图的UnionFormer框架,一个调节不同尺度上空间一致性的篡改特征提取网络BSFI-Net。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/postimages/UnionFormer/image-20240618124653610.png">
<meta property="article:published_time" content="2024-06-16T06:04:07.000Z">
<meta property="article:modified_time" content="2025-01-07T07:50:57.167Z">
<meta property="article:author" content="Zhaozw">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/postimages/UnionFormer/image-20240618124653610.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/UnionFormer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-07 15:50:57'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-database"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/postimages/UnionFormer/image-20240618124653610.png')"><nav id="nav"><span id="blog-info"><a href="/" title="zhaozw后院"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">zhaozw后院</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-database"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-16T06:04:07.000Z" title="发表于 2024-06-16 14:04:07">2024-06-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-07T07:50:57.167Z" title="更新于 2025-01-07 15:50:57">2025-01-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B/">图像篡改检测</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><center>
UnionFormer: Unified-Learning Transformer with Multi-View Representation
for Image Manipulation Detection and Localization <a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><img
src="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a>
</center>
<center>
<span class="math inline">\(\text{Shuaibo Li}^{1,2}\quad\text{Wei
Ma}^{1\dagger}\quad\text{Jianwei Guo}^2\quad\text{Shibiao
Xu}^3\quad\text{Benchong Li}^1\quad\text{Xiaopeng Zhang}^2\)</span>
</center>
<center>
北京理工大学1、MAIS(中国科学院自动化研究所)2、北京邮电大学3
</center>
<details close>
<br/>
<summary>
论文（arxiv）
</summary>
<br/>

	<div class="row">
    <embed src="/postpdfs/UnionFormer/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf">
	</div>


<br/>
</details>
<h1 id="摘要">摘要</h1>
<p>​  我们提出了一个新的框架，通过统一学习集成了三个视图上的篡改线索，用于图像操作检测和定位。特别地，我们构建了一个BSFI-Net，从RGB和噪声视图中提取篡改特征，在调节不同尺度上的空间一致性的同时，增强了对边界伪影的响应性。此外，为了探索对象之间的不一致性作为一种新的线索视角，我们将对象一致性建模与篡改检测和定位结合成一个三任务统一的学习过程，使它们能够相互促进和改进。</p>
<p>​  因此，我们在多尺度监督下获得了一个统一的操作鉴别表示，从三个角度整合信息。这种集成便于高效的并行检测和定位篡改。我们在不同的数据集上进行了大量的实验，结果表明，该方法在篡改检测和定位方面优于最先进的方法。</p>
<h1 id="引言">1. 引言</h1>
<p>​  数字图像篡改可分为三大类[19]：拼接，即将区域从一幅图像复制到另一幅图像；复制-移动，包括复制或移动同一图像中的元素；移除，删除图像部分和创建视觉一致的内容以掩盖改变的过程。这些操作在被篡改区域和周围环境之间留下痕迹，造成真实区域和伪造区域之间的不一致。与传统的强调高级语义信息的传统检测或分割任务不同，图像篡改检测优先考虑局部语义无关的线索，以区分真实性，而不是语义内容。因此，篡改检测的关键挑战是学习结合不同层次信息并捕获真实和篡改区域之间多尺度不一致的通用特征。以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层[23,27,40,71]的特征，不能充分表示篡改痕迹。受[9,12,67]的启发，我们设计了一个专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,
Boundary Sensitive Feature Interaction
Network），并将其作为特征编码器集成到我们的框架中。BSFI-Net是一个并行的cnn-Transformer结构，它可以加强边缘响应，同时有效地在局部特征和全局表示之间进行交互，以探索不同尺度上图像内部的一致性。<br/>​  另一方面，许多在RGB视图中难以察觉的篡改伪影在噪声视图中变得明显明显。使用固定的[18]或可学习的高通滤波器[6,35,66]将RGB图像转换为噪声图，可以抑制内容，并突出显示低级的伪造线索。因此，开发一种同时建模RGB和噪声维度的多视图策略对于检测细微的篡改痕迹至关重要。我们的框架采用了一个双流架构来独立地构建RGB和噪声视图的表示，随后合并它们以提高鉴别能力和泛化性。此外，我们还结合了对比监督，以改善这两种观点之间的协作。<br/>​  此外，为了创建空间相干和语义一致的图像，篡改操作总是改变整个对象来隐藏证据，即执行对象级操作。目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息。相反，我们认为图像操作检测应该不仅仅是识别分布外的像素或补丁，以捕获由操作导致的对象一致性和分布的异常。由于扩散模型[4,5,20,30,44,65,69]生成的超真实的篡改图像，利用对象视图信息变得特别重要。基于扩散的模型[4,30,44]反复更新了整个图像的初始噪声，增强了空间连续性，留下了更少的RGB和噪声痕迹。此外，与真实的图像源不同，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。最近的扩散模型[20,29,55,64]试图通过采用以对象为中心的方法来解决这个问题，强调了使用对象视图线索进行篡改检测的必要性和可行性。然而，创建和集成这样的新视图与其他视图，以篡改伪影表示是一个重大的挑战，需要新的架构和学习策略。<br/>​  考虑到上述要点，我们引入了UnionFormer，一个用于图像操作检测和定位的多视图表示的统一学习transformer框架，如图1所示。</p>
<p><img src="../postimages/UnionFormer/image-20240617100605323.png"
alt="image-20240617100605323" /><br/>图1.UnionFormer的组成概述。我们通过整合来自三个视图表示的篡改线索来实现同时的篡改检测和定位，每个视图由不同的颜色背景表示。我们通过BSFI-Net获得了RGB和噪声视图下的表示，并在统一学习中构建了基于两者的对象视图表示。同时，将三个视图的信息交互融合成统一的操作判别表示（UMDR,
unified manipulation discriminative representation）进行检测和定位</p>
<p>​  首先，我们使用BSFI-Net作为特征编码器，获得在RGB和噪声视图下的通用化特征，并将其进行组合。然后，我们利用融合的特征进行一个单一化的学习过程，其中包括三个子任务：对象一致性建模、伪造检测和伪造定位。在统一学习中，我们的模型建立了对象视图表示，并将三个视图信息集成到一个统一的操作鉴别表示（UMDR,
unified manipulation discriminative
representation）中，同时完成伪造检测和定位。综上所述，我们的主要贡献如下：</p>
<ul>
<li>我们提出了一种新的图像取证transformer框架，UnionFormer。通过多尺度监督的统一学习，整合三个视角的信息，同时执行图像操作检测和定位。</li>
<li>我们引入了BSFI-Net，一种用于高级人工表示学习的混合网络结构，它增强了边界响应，同时揭示了不同层次的局部不一致性。</li>
<li>通过对UMDR的统一学习，我们构建了一种创新的对象视图表示方法，能够从三个视图中捕获对象之间的不一致性和聚合信息，用于伪造检测。</li>
<li>我们通过各种基准进行了全面的实验，证明了我们的方法在检测和定位任务中都获得了最先进的结果。</li>
</ul>
<h1 id="方法">2. 方法</h1>
<p>​  在本节中，我们首先提供对工会成员的概述和对每个组件的详细介绍。我们的目标是充分利用来自三个视图的丰富工件来同时进行篡改检测和定位。我们通过在多尺度监督下的统一学习过程来实现这一目标。<br/>​  如图1所示，首先使用受约束的CNN
[7]将输入的RGB图像X转换为噪声视图表示N = C
(X)，可以显示低级的篡改。<br/>​  然后，将X和N分别输入边界敏感特征交互网络（BSFI-Net）进行特征编码。高频边缘特征(H)与X或N一起作为BSFI-Net的输入，以提高边缘响应性。这使得我们能够在RGB和噪声视图下获得可推广的和可鉴别的特征，构造两个特征金字塔
$ f_r = _1(X,H), f_n = _2(N,H) $
。<br/>​  随后，我们使用区域建议网络（RPN）[51]从特征fr中获得一组感兴趣的区域（RoIs），用pi表示。从fr和fn中提取RoI信息，然后扁平得到建议的嵌入表示，记为ri，ni。将每个方案的RGB特征ri和噪声特征ni连接起来，生成融合的方案特征di，并将其输入到I变压器编码器层。<br/>​  在统一学习阶段，我们处理了三个子任务：建模对象的一致性、真实性的二进制分类和篡改区域定位。在转换器编码器之后，将伪造-判别查询嵌入DI输入到统一操作判别表示部分，对三个子任务生成三个预测。如图1所示，我们对三个子任务采用了具有统一形式的多尺度监督，包括Lcls、Locm和Lloc。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618124653610.png"
alt="image-20240618124653610" />
<figcaption aria-hidden="true">image-20240618124653610</figcaption>
</figure>
<h2 id="特征交互编码">2.1 特征交互编码</h2>
<h3 id="rgb和噪声视图表示">2.1.1 RGB和噪声视图表示。</h3>
<p>​  在特征编码阶段，我们利用一个双流结构来利用来自RGB和噪声视图的线索。RGB流被设计为捕获视觉上明显的篡改伪影，而噪声流旨在探索被篡改区域和真实区域之间的分布不一致性。我们利用[7]中提出的可学习约束卷积层将RGB图像转换为噪声视图。如第2节所述，被篡改区域及其周围环境的边缘表现出更明显的篡改线索。因此，我们增强了两个流中的高频边缘信息，将网络的响应集中在被篡改的区域。具体来说，我们利用离散余弦变换（DCT）将图像数据X转换为频域，然后应用高通滤波器得到高频分量。然后，我们将高频分量转换回空间域，以促进特征交互和保持局部一致性。因此，我们得到的边缘增强信息H如下：
<span
class="math display">\[H=\mathcal{T}_d^{-1}\left(\mathcal{F}_h\left(\mathcal{T}_d(X),\beta\right)\right)\]</span>
​  其中Td表示DCT，Fh表示高通滤波器，β为阈值。我们将X和N分别输入到BSFI-Net中，以及H来进行特征编码，如图2所示。</p>
<h3 id="边界敏感特征交互网络">2.1.2 边界敏感特征交互网络。</h3>
<p>​  除了增强边界响应外，集成局部特征和全局表示对图像伪造检测也至关重要。这就要求进行全面分析在不同尺度上的图像内部的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-Transformer并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p>
<p><img src="../postimages/UnionFormer/image-20240617110632461.png"
alt="image-20240617110632461" /><br/>图2.BSFI-Net的概述。FCU表示特征耦合单元，BOB表示边界向块。</p>
<p>​  如图2所示，CNN分支作为主分支，以一个RGB或噪声图像作为输入，对局部信息进行编码。变压器分支以输入作为边缘增强信息H，引导CNN分支聚焦于被篡改的区域，并将图像补丁之间的长距离不一致传输给它。我们使用[48]提出的特征耦合单元（FCU）来消除来自CNN分支的特征映射和来自transformer分支的补丁嵌入之间的错位。此外，我们还设计了一个面向边界的块（BOB），以方便将高级补丁一致性和边界信息从变压器分支传输到CNN分支，从而指导CNN分支。<br/>​  CNN分支由5个卷积块组成，类似于ResNet构造[24]。与[16,48]一样，transformer分支由5个重复的transformer块组成，由一个多头自注意模块和一个MLP块组成。采用与ViT
[16]相同的令牌化操作。在FCU中，在添加补丁嵌入和CNN特征之前，使用1×1的卷积和重新采样来对齐通道和空间维度。在BOB中，CNN分支的特征映射被输入1×1卷积层、批归一化层、s型层，并通过双线性插值上采样到高分辨率。然后，将来自CNN分支的特征与长距离判别权值进行元素级乘法。我们将BSFI-Net作为特征编码器进行预训练，生成RGB和噪声视图表示，特征金字塔网络[38]基于中间特征映射{C2、C3、C4、C5}生成两个特征金字塔fr，fn。培训细节详见第4.1节。</p>
<h2 id="特征对比性协作">2.2 特征对比性协作</h2>
<p>​  在特征协作阶段，受[51,56]的启发，我们首先使用一个基于RGB特征金字塔fr的区域建议网络（RPN）来生成一组感兴趣的区域（RoIs）。然后，我们利用RoIAlign
[25]从两个流的特征金字塔fr和fn中提取RoIs的信息。除了特征连接之外，我们还采用对比监督来促进两个视图之间的协作。我们将来自不同流的被篡改的建议视为积极建议，被篡改的建议和真实建议被指定为负对。在InfoNCE损失[47,67]之后，对比度损失被定义为：
<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{con}}=-
\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{1})}-
\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{2})}\end{aligned}\]</span>
​  式中，s0表示正对之间的相似性，s1表示RGB篡改嵌入与噪声真实嵌入之间的相似性，s2表示RGB真实嵌入与噪声篡改嵌入之间的相似性。对比损失Lcon引入统一学习监督，将在第3.3节进行讨论。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618124629395.png"
alt="image-20240618124629395" />
<figcaption aria-hidden="true">image-20240618124629395</figcaption>
</figure>
<h2 id="具有多尺度监督下的统一学习">2.3 具有多尺度监督下的统一学习</h2>
<p>​  <strong>Transformer编码器。</strong>我们的统一学习模块是一个仅限编码器的transformer架构，它处理融合的提议嵌入二，以及它们的特定位置编码作为输入。在转换器编码器的每一层中，自我注意机制通过不同的建议嵌入来聚合信息，并捕获它们的长距离依赖关系，这意味着对象的一致性。详细地说，我们使用了一个变压器解码器，具有六层，宽度为512，和8个注意头。变压器内的前馈网络（FFN）的隐藏大小为2048。在转换器编码器之后，我们生成判别查询嵌入DI，并输入统一操作判别表示（UMDR）部分，以生成三个子任务的预测，即。对象一致性建模、图像操作检测和定位。</p>
<p>​  <strong>统一伪造判别表示</strong>。在转换器编码器之后，DI中的每个篡改判别查询都表示对应建议的三个视图中的篡改线索。图3显示了三个子任务的学习过程。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240617214850871.png"
alt="image-20240617214850871" />
<figcaption aria-hidden="true">image-20240617214850871</figcaption>
</figure>
<p>图3。多尺度监督下的UMDR学习。图像内部在不同尺度上的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-变压器并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p>
<p>​  UMDR是在真实性分类、对象一致性建模和操作定位分支的监督下学习的。与DETR
[9]和SOLQ
[12]一样，分类分支是一个完全连接的（FC）层，用来预测真实性可信度Pˆc。目标一致性建模分支是一个多层感知（MLP），隐藏大小为256，用于预测目标空间信息Pˆo。操作定位分支也是一个隐藏大小为1024的多层感知来预测定位掩码向量Pˆm。对前两个分支机构的监管类似于DETR[9]。在第三个分支中，我们利用对地面真实掩码进行编码得到的掩模向量作为监督信息。在推理过程中，将压缩后的编码过程应用于Pˆm来重构定位掩码。在压缩编码中，我们利用主成分分析（PCA）将二维空间二值掩模转换为一维掩模向量。</p>
<p>​  <strong>损失函数。</strong>UnionFormer监督的总体损失职能可表示为：
<span
class="math display">\[\mathcal{L}_{union}=\lambda_{cls}\cdot\mathcal{L}_{cls}+\mathcal{L}_{ocm}+\lambda_{loc}\cdot\mathcal{L}_{loc}+\beta\cdot\mathcal{L}_{con},\]</span>
​  其中Lcls表示分类的focal损失[39]。Lloc表示定位掩码向量监督的L1损失。Lcon是在第3.2节中引入的对比性学习损失。λcls、λloc、β是相应的调制系数。Locm是对象一致性建模的损失，其定义为：
<span
class="math display">\[\mathcal{L}_{\mathrm{ocm}}=\lambda_{L_1}\cdot\mathcal{L}_{L_1}+\lambda_{gious}\cdot\mathcal{L}_{gious}\]</span>
​  其中LL1和Lleam为L1损失和广义IoU损失[52]，与DETR相同。λL1和λgious是对应的系数。在[12]之后，Lloc不包括在二部匹配过程中。</p>
<h1 id="实验">3. 实验</h1>
<h2 id="实验设置">3.1 实验设置</h2>
<p>​  <strong>训练</strong>。我们使用了一个大规模的训练数据集，包括各种类型的篡改和真实的图像。它分为五个部分：
1) CASIA v2 [14]，2)Fantastic Reality[32]，3)Tampered COCO,，来自COCO
2017数据集[37]，4)Tampered RAISE，基于RAISE数据集[11]构建，5)从COCO
2017和RAISE数据集中选择的原始图像。我们在合成数据中随机添加高斯噪声或应用JPEG压缩来模拟现实场景中的视觉质量和篡改轨迹。在训练过程中，我们依次分三个阶段对BSFI-Net、RPN和UnionFormer进行训练。</p>
<p>​  <strong>测试。</strong>为了全面评估和比较我们的模型与各种最先进的方法，我们使用了6个公开可用的测试数据集和另一个由混合扩散模型[4]创建的超真实篡改图像数据集。具体来说，我们使用了CASIA
v1 [14]、Columbia[26]、Coverage[61]、NIST16 [22]、IMD20 [46]和CocoGlide
[23]。然后，我们构建了BDNIE，包括512张由先进的混合扩散模型生成的超真实的假图像，用于文本驱动的自然图像编辑。训练和测试数据的细节载于补充资料。</p>
<p>​  <strong>评价指标。</strong>我们评估了该方法在图像篡改检测和定位任务中的性能。对于定位图像操作的任务，我们报告了像素级的曲线下面积（AUC）和F1分数，同时使用最佳的和固定的0.5阈值。对于[23]之后的检测任务，我们采用图像级AUC和平衡精度，同时考虑假报警和遗漏检测，在这种情况下，阈值设置为0.5。为了保证比较的公平性和准确性，我们从文献[23,59]中取出了其他方法的一些结果值。</p>
<p>​  <strong>实施细节。</strong>BSFI-Net采用AdamW优化器[41]进行了100个周期的交叉熵损失训练，批处理大小为512，权重衰减为0.05。初始学习速率被设置为0.001，并在余弦时间表中衰减。</p>
<p>​  在与Lunion一起训练完整的UnionFormer时，受[56,63]的启发，我们采用36周期（3×）计划来训练UnionFormer进行2.7×105次迭代，批大小为16。在这个阶段还使用了一个AdamW优化器。学习速率在开始时被设置为10−4，并在1.8×105和2.4×105迭代时乘以0.1。</p>
<h2 id="与最先进的技术相比较">3.2 与最先进的技术相比较</h2>
<p>​  <strong>Baseline。</strong>为了确保公平和准确的比较，我们只选择了最先进的方法，其中作者提供了预训练的模型，发布的源代码，或在通用标准[27,40,59]下进行评估。为了减少偏差，我们只考虑了在不与测试数据集重叠的数据集上训练的方法或版本。详细地说，我们包括了7种最先进的方法：MantraNet[62]，SPAN[27]，PSCC-Net[40]，MVSS-Net[13]，CAT-Netv2[34]，ObjectFormer[59]，和TruFor
[23]。</p>
<p>​  <strong>定位结果。</strong>表2和表1分别显示了基于像素级AUC和F1评分指标的图像篡改定位结果。排名最高的方法用粗体表示，一条水平线表示排名第二的方法，在表4和表3中也采用了相同的注释。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240617115640307.png"
alt="image-20240617115640307" />
<figcaption aria-hidden="true">image-20240617115640307</figcaption>
</figure>
<p>​  我们的方法在所有数据集上展示了像素级AUC评估的最佳性能。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240617115601335.png"
alt="image-20240617115601335" />
<figcaption aria-hidden="true">image-20240617115601335</figcaption>
</figure>
<p>​  对于f1评估，我们的方法在所有数据集上排名最好或第二。平均而言，无论是否使用最优或固定的阈值，我们都获得了显著的优势。事实上，在包含基于扩散的局部操作的相对新颖的CocoGlide数据集上，我们在两个阈值上分别比排名第二的TruOfor高出2.2%和1.3%。这是由于联合前体构建的对象视图伪影表达式，它可以揭示由扩散模型生成的区域和真实区域之间的不一致性。这些比较表明，我们的方法具有较强的泛化和捕获篡改伪的能力。</p>
<p>​  <strong>检测结果。</strong>表4为篡改检测的比较结果。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240617115836976.png"
alt="image-20240617115836976" />
<figcaption aria-hidden="true">image-20240617115836976</figcaption>
</figure>
<p>​  在[23]之后，我们使用定位映射的最大值作为未明确为检测任务设计的方法的检测统计量。UnionFormer在除Columbia外的所有数据集上都取得了最佳的性能，并在平均结果上显示了显著的优势，无论是通过AUC还是平衡精度测量。正如[13,23]中提到的，精度对阈值选择很敏感，如果没有良好校准的数据集，很难确定。然而，我们的方法和次要的TruFor在这个要求很高的场景中取得了值得称赞的结果。我们在平均AUC和精度上分别保持了2.5%和2%的领先优势。这一优势主要归因于我们的框架的统一学习过程。统一学习通常会促进对定位和检测任务的相互增强。通过统一的操作鉴别表示，掌握了两个子任务，进一步提高了模型的性能。</p>
<p>​  <strong>鲁棒性评估。</strong>我们通过对NIST
16数据集图像应用图像失真，验证了UnionFormer的鲁棒性。在[40,59]之后，我们包括了四种类型的畸变：
1)将图像的大小改变到不同的尺度；2)应用核大小为k的高斯模糊；3)添加以标准偏差σ为特征的高斯噪声；4)对图像进行JPEG压缩，使用质量因子q。我们比较了像素级AUC与其他方法的性能。表3显示，我们的方法对各种失真操作表现出鲁棒性，优于其他方法。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240617120221607.png"
alt="image-20240617120221607" />
<figcaption aria-hidden="true">image-20240617120221607</figcaption>
</figure>
<h1 id="可视化结果">4. 可视化结果</h1>
<h2 id="定性比较">4.1 定性比较</h2>
<p>​  <img src="../postimages/UnionFormer/image-20240618103731575.png"
alt="image-20240618103731575" /></p>
<p>​  图4显示了跨不同数据集的定位结果。我们的方法可以准确地定位被篡改的区域，预测更详细和清晰的边界。这是由于我们的多视图特征捕获和BSFI-Net，其中频率信息增强了边缘响应，而分支之间的交互作用增强了特征的泛化和识别。由于对对象视图线索的建模和统一的学习框架，我们的方法在具有挑战性的BDNIE数据集上取得了令人满意的结果，而其他方法都失败了。</p>
<h2 id="不同视图表示法的可视化">4.2 不同视图表示法的可视化</h2>
<p>​  在图5中，我们可视化了BSFI-Net中变压器分支的噪声特征和边缘引导特征。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618103938959.png"
alt="image-20240618103938959" />
<figcaption aria-hidden="true">image-20240618103938959</figcaption>
</figure>
<p>如列1到4所示，一些图像在RGB视图中可能看起来很自然，但它们被篡改/真实的部分很容易在频域或噪声视图中被容易区分出来。第5列和第6列显示了由一个CNN分支和BSFI-Net的双分支生成的RGB特性。与只使用CNN分支相比，BSFI-Net更准确地激活了被篡改的区域，这得益于变压器分支提供的边缘引导和长距离线索。</p>
<p>​  此外，我们还定量地分析了对象视图，如图6所示。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618104100130.png"
alt="image-20240618104100130" />
<figcaption aria-hidden="true">image-20240618104100130</figcaption>
</figure>
<p>​  在统一学习阶段，我们从transformer编码器中推导出亲和矩阵Ai。基于Ai，我们随机选择提案嵌入的一个子集，计算它们与其他建议的平均亲和力，记为ei。然后将ei归一化到范围[0,1]，并作为一个颜色系数来可视化建议，较浅的颜色表示较低的亲和力。结果表明，使用伪造物体的提案与其他区域的平均亲和力较低，这表明UMDR能够捕捉真实物体和虚假物体之间的不一致性。</p>
<h1 id="消融研究">5. 消融研究</h1>
<p>​  我们进行了消融研究，以评估我们的方法中关键成分的影响。定量结果见表5。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618104307449.png"
alt="image-20240618104307449" />
<figcaption aria-hidden="true">image-20240618104307449</figcaption>
</figure>
<p>​  我们可以观察到，通过在第一个基线模型上添加噪声流，CASIA
v1的AUC得分增加8.7%，NIST 16增加8.3%，同时进一步增加对象视图表示，CASIA
v1继续增加10.7%，NIST
16继续增加7.4%。这证明了噪声和对象视图表示的有效性。此外，当缺乏对比监督，或BSFI-Net被ResNet-
50
[24]取代时，模型的性能会显著下降。这突出了两个流之间的交互的有效性和BSFI-Net在描述伪造制品方面的特殊能力。</p>
<p>​  BSFI-Net中的BOB和FCU模块改善了其两个分支之间的交互作用，并有效地消除了它们之间的特征失调。当单独去除BOB或FCU时，整体模型在NIST
16数据集上的定位AUC得分分别下降了4.8%和6.3%。</p>
<p>​  我们进一步进行了实验，研究了UMDR中几个关键因素的影响。λloc，Locm，掩码向量维度nv，以及压缩编码的类型。</p>
<figure>
<img src="../postimages/UnionFormer/image-20240618104459423.png"
alt="image-20240618104459423" />
<figcaption aria-hidden="true">image-20240618104459423</figcaption>
</figure>
<p>​  我们比较了三种压缩编码方法：稀疏编码[15]、离散余弦变换（DCT）[2]和主成分分析（PCA）[1]。如表6所示，当设置对比损失时，以PCA为编码类型，并将λloc和Locm分别设置为1和256时，该模型在NIST
16数据集上表现最好。</p>
<h1 id="结论">6. 结论</h1>
<p>​  在本文中，<br/>​  我们介绍了UnionFormer，一个联合学习transformer框架，它利用来自三个不同视图的线索来进行图像操作检测和定位。UnionFormer使用BSFI-Net作为特  征编码器，在RGB和噪声视图下提取具有高度区分性的特征。然后，通过三个任务的统一学习过程，UnionFormer建模了对象之间的不连续性，即对象视图表示，并学习统一的判别表示。从三种观点整合信息的统一表示具有较强的通用性和区分性。它可以准确地识别各种图像操作，无论是传统的手动编辑还是基于扩散模型的自然语言驱动的篡改。此外，统一的学习框架使子任务的相互增强，实现了高精度的检测和定位。在不同的数据集上进行的综合实验证明了该方法的有效性。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/UnionFormer/">https://zhaozw-szu.github.io/UnionFormer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">zhaozw后院</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post_share"><div class="social-share" data-image="/postimages/UnionFormer/image-20240618124653610.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Image_Super-Resolution/" title="Deep Learning for Image Super-Resolution"><img class="cover" src="/img/coverImage/cover3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Deep Learning for Image Super-Resolution</div></div></a></div><div class="next-post pull-right"><a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/" title="机器学习报告1"><img class="cover" src="/img/coverImage/cover4.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习报告1</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/" title="A New Benchmark and Model for Challenging Image Manipulation Detection"><img class="cover" src="/postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-28</div><div class="title">A New Benchmark and Model for Challenging Image Manipulation Detection</div></div></a></div><div><a href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/" title="Attentive and Contrastive Image Manipulation Localization With Boundary Guidance"><img class="cover" src="/postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-10</div><div class="title">Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</div></div></a></div><div><a href="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/" title="CatmullRom Splines-Based Regression for Image Forgery Localization"><img class="cover" src="/postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-03</div><div class="title">CatmullRom Splines-Based Regression for Image Forgery Localization</div></div></a></div><div><a href="/DH-GAN/" title="DH-GAN:Image manipulation localization via a dual homology-aware generative adversarial network"><img class="cover" src="/postimages/DH-GAN/image-20240824153543322.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-24</div><div class="title">DH-GAN:Image manipulation localization via a dual homology-aware generative adversarial network</div></div></a></div><div><a href="/HRnet/" title="HRnet"><img class="cover" src="/img/coverImage/cover4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-09</div><div class="title">HRnet</div></div></a></div><div><a href="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/" title="Learning Discriminative Noise Guidance for Image Forgery Detection and Localization"><img class="cover" src="/postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-01</div><div class="title">Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">可以用表情包和匿名评论了</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1. 引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">2. 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E4%BA%A4%E4%BA%92%E7%BC%96%E7%A0%81"><span class="toc-text">2.1 特征交互编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rgb%E5%92%8C%E5%99%AA%E5%A3%B0%E8%A7%86%E5%9B%BE%E8%A1%A8%E7%A4%BA"><span class="toc-text">2.1.1 RGB和噪声视图表示。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%B9%E7%95%8C%E6%95%8F%E6%84%9F%E7%89%B9%E5%BE%81%E4%BA%A4%E4%BA%92%E7%BD%91%E7%BB%9C"><span class="toc-text">2.1.2 边界敏感特征交互网络。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%AF%B9%E6%AF%94%E6%80%A7%E5%8D%8F%E4%BD%9C"><span class="toc-text">2.2 特征对比性协作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E6%9C%89%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%91%E7%9D%A3%E4%B8%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E5%AD%A6%E4%B9%A0"><span class="toc-text">2.3 具有多尺度监督下的统一学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">3. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-text">3.1 实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%8A%80%E6%9C%AF%E7%9B%B8%E6%AF%94%E8%BE%83"><span class="toc-text">3.2 与最先进的技术相比较</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C"><span class="toc-text">4. 可视化结果</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E6%AF%94%E8%BE%83"><span class="toc-text">4.1 定性比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E8%A7%86%E5%9B%BE%E8%A1%A8%E7%A4%BA%E6%B3%95%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">4.2 不同视图表示法的可视化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6"><span class="toc-text">5. 消融研究</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">6. 结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/" title="A Survey on Deep Clustering:From the Prior Perspective"><img src="/postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107223309527.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Survey on Deep Clustering:From the Prior Perspective"/></a><div class="content"><a class="title" href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/" title="A Survey on Deep Clustering:From the Prior Perspective">A Survey on Deep Clustering:From the Prior Perspective</a><time datetime="2025-01-07T14:20:13.000Z" title="发表于 2025-01-07 22:20:13">2025-01-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Fuzzy-clustering-method-for-image-segmentation/" title="Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation"><img src="/postimages/DH-GAN/image-20240824153543322.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation"/></a><div class="content"><a class="title" href="/Fuzzy-clustering-method-for-image-segmentation/" title="Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation">Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation</a><time datetime="2024-12-29T14:13:57.000Z" title="发表于 2024-12-29 22:13:57">2024-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/" title="Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection"><img src="/postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226110937373.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection"/></a><div class="content"><a class="title" href="/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/" title="Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection">Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection</a><time datetime="2024-12-26T02:50:46.000Z" title="发表于 2024-12-26 10:50:46">2024-12-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/" title="Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning"><img src="/postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165700880.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning"/></a><div class="content"><a class="title" href="/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/" title="Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning">Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning</a><time datetime="2024-12-23T08:43:50.000Z" title="发表于 2024-12-23 16:43:50">2024-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Fuzzy-Machine-Learning/" title="Fuzzy_Machine_Learning"><img src="/img/coverImage/cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Fuzzy_Machine_Learning"/></a><div class="content"><a class="title" href="/Fuzzy-Machine-Learning/" title="Fuzzy_Machine_Learning">Fuzzy_Machine_Learning</a><time datetime="2024-12-23T03:20:44.000Z" title="发表于 2024-12-23 11:20:44">2024-12-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'all'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
  //- console.log('MathJax loaded')
} else {
  // 重置 TeX 状态并重新渲染
  MathJax.startup.promise.then(() => {
    MathJax.texReset();  // 重置 TeX 编号等状态
    MathJax.typesetPromise();
  });

  //- MathJax.startup.document.state(0)
  //- MathJax.texReset()
  //- MathJax.typesetPromise()
  //- console.log('MathJax reset')
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>