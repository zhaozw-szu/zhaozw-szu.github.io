<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Towards Generalizable Detector for Generated Image | 喵</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="发表于NeurIPS 2025，完全基于自然图像训练的模型在生成图像检测，提出无需生成图片训练的方法DEnD，天才般的想法，与博主之前的某个想法不谋而合，无需训练便能达到sota，厉害！">
<meta property="og:type" content="article">
<meta property="og:title" content="Towards Generalizable Detector for Generated Image">
<meta property="og:url" content="https://zhaozw-szu.github.io/Towards-Generalizable-Detector-for-Generated-Image/index.html">
<meta property="og:site_name" content="喵">
<meta property="og:description" content="发表于NeurIPS 2025，完全基于自然图像训练的模型在生成图像检测，提出无需生成图片训练的方法DEnD，天才般的想法，与博主之前的某个想法不谋而合，无需训练便能达到sota，厉害！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110170346642.png">
<meta property="article:published_time" content="2025-11-10T08:25:46.000Z">
<meta property="article:modified_time" content="2025-11-10T12:25:14.505Z">
<meta property="article:author" content="Zhaozw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110170346642.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/Towards-Generalizable-Detector-for-Generated-Image/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Towards Generalizable Detector for Generated Image',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-10 20:25:14'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">191</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 等级</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110170346642.png')"><nav id="nav"><span id="blog-info"><a href="/" title="喵"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">喵</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 等级</span></a></li><li><a class="site-page child" href="/paper/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Towards Generalizable Detector for Generated Image</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">创建于</span><time class="post-meta-date-created" datetime="2025-11-10T08:25:46.000Z" title="创建于 2025-11-10 16:25:46">2025-11-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-10T12:25:14.505Z" title="更新于 2025-11-10 20:25:14">2025-11-10</time><span class="post-meta-separator">|</span><i class="fas fa-star fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><span class="post-rank">A类会议,NeurIPS,2025</span></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%97%A0%E7%9B%91%E7%9D%A3/">无监督</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%97%A0%E7%9B%91%E7%9D%A3/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a></span><span class="post-meta-separator">|</span><a target="_blank" rel="noopener" href="https://github.com/dav-joy-thon/DEnD-Detection"><img src="https://img.shields.io/github/stars/dav-joy-thon/DEnD-Detection?style=flat" alt="GitHub Stars: dav-joy-thon/DEnD-Detection" loading="lazy"></a></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Towards Generalizable Detector for Generated Image"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div id="article-description">发表于NeurIPS 2025，完全基于自然图像训练的模型在生成图像检测，提出无需生成图片训练的方法DEnD，天才般的想法，与博主之前的某个想法不谋而合，无需训练便能达到sota，厉害！<div class="disclaimer">博主观点不代表文章作者观点</div></div><article class="post-content" id="article-container"><p>Towards Generalizable Detector for Generated Image</p>
<p><strong>Qianshu Cai</strong>1 ，<strong>Chao Wu</strong>2<em>,</em>3
，<strong>Yonggang Zhang</strong>4 ，<strong>Jun Yu</strong>5
，<strong>Xinmei Tian</strong>1
<em>∗</em><br/>1中国科学技术大学教育部脑科学智能感知与认知重点实验室，<br/>2浙江大学<br/>3河北通信学院人工智能学院<br/>4香港科技大学<br/>5哈尔滨工业大学深圳校区智能科学与工程学院</p>
<h1 id="摘要">摘要</h1>
<p>​  有效检测生成图像对于防范其滥用风险至关重要。尽管已取得显著进展，但一个根本性挑战依然存在：如何确保检测器的泛化能力。为此，我们提出一种受人类认知机制启发的全新视角来理解和改进生成图像检测技术——人类通过识别特定模式来判断图像是否自然，因为这些模式超出了自然图像特征的范畴。这本质上与分布外（OOD）检测相关，该检测方法通过识别语义模式（即标签）超出分布内（ID）样本语义模式空间的样本来实现。通过将生成图像的模式视为
OOD
样本，我们证明了仅在自然图像上训练的模型，在轻度假设条件下即可保证泛化能力。这一方法将生成图像检测的泛化挑战转化为自然图像模式的拟合问题。基于此洞见，我们通过ID能量的视角提出了一种可泛化的检测方法。理论结果揭示了该方法的泛化风险，而多项基准测试的实验结果充分证明了我们方法的有效性。代码可在
https://github.com/dav-joy-thon/DEnD-Detection 上找到。</p>
<h1 id="引言">1.引言</h1>
<p>​  近年来，生成式人工智能领域取得了重大突破。特别是基于扩散模型的生成技术[22,44,10,56]在图像合成领域展现出革命性进展。包括Stable
Diffusion[56]、DALL-E
3[50]、Midjourney[41]和FLUX[29]在内的先进生成模型，让用户只需简单输入文本提示即可生成高度逼真的图像。更值得一提的是，视频生成领域的明星产品Sora[47]不仅能制作高清逼真视频，还能模拟真实物理效果。然而，这种技术的迅猛发展也暗藏风险与挑战。恶意分子滥用生成图像进行欺诈行为，已引发人们对媒体信息真实性的质疑。因此，开发具备强大泛化能力的有效生成图像检测系统，以应对这些新兴威胁，显得尤为重要。<br/>​  在生成图像检测领域，现有方法大多依赖训练二元分类器[69,68,64]。例如，DIRE[69]采用扩散重建误差作为指标训练二元分类器。而aeroblade[55]则提出了一种无需训练的方案，通过利用自动编码器的重建误差来检测由潜在扩散模型（LDM）[56]生成的图像。不过该方法存在局限性：只能识别
LDM
生成的图像，且需要获取用于图像生成的自动编码器。然而，这些方法面临着一个根本性挑战：如何确保所构建检测器的泛化能力。在实际应用中，我们经常遇到底层架构未知的生成模型，这使得泛化能力的挑战尤为突出。</p>
<p>​  为应对泛化挑战，我们重新审视人类识别生成图像的过程。仅接触过自然图像的人类，能够通过独特特征辨别生成图像。这可能源于人类感知到生成图像的模式存在于自然图像模式所覆盖空间之外。从这个角度看，这种“空间外操作”同样适用于检测分布外（OOD）数据。具体而言，
OOD
检测器需要识别具有语义模式（即标签）的样本，这些样本位于分布内（ID）样本语义模式空间之外。人类识别生成图像的过程与分布外（OOD）检测原理[73]高度契合——人类仅接触过自然图像（ID）却能识别生成图像（OOD），而模型仅接触过ID样本却能检测
OOD
样本。这引发了一个基础却鲜少被探讨的问题：仅接触过自然图像的模型能否用于检测生成图像？</p>
<p>​  本研究提出一个创新视角：通过 OOD
检测的视角审视生成图像检测。在此框架下，自然图像的特征模式被视为ID数据，而生成图像的特征模式则属于
OOD 数据。基于 OOD
检测的可学习性理论[12]，我们研究了生成图像检测的泛化能力，证明在温和假设条件下，基于自然图像训练的模型能确保生成图像检测的泛化能力。然而，生成图像检测依赖于特定特征模式的非重叠空间，而
OOD 检测则聚焦于ID与 OOD 数据语义标签的非重叠空间。具体而言， OOD
检测可利用基于ID数据标签空间训练的分类器，但生成图像检测无法使用针对语义标签训练的分类器。</p>
<p>​  为应对这一挑战，我们从基于密度和能量的 OOD
检测方法中汲取灵感。这些方法揭示了ID数据的能量（密度）低于（高于）OOD
数据的特性，这是因为模型在训练过程中会优先最小化（最大化）ID数据的能量（密度）。因此，我们沿用前人研究思路，重新定义生成图像检测中ID数据的能量指标。受[61]开创性研究启发，我们发现DINOv2[48]等自监督模型具备潜在能力（附录A中对此见解进行了详细说明）——能够识别生成图像与自然图像之间的模式差异。理论研究表明，自监督模型[34]的核心学习目标本质上是最小化ID数据（即自然图像）的差异能量评分。基于这一洞见，我们提出了一种名为差异能量检测（DEnD，differential
energy-based
detection）的创新框架，通过预训练自监督模型来识别生成图像，该框架展现出强大的泛化能力。</p>
<p>​  大量实验表明，我们的方法相较于基于训练的方法[69,68,64]具有更强的泛化能力，并且优于当前最先进的（SOTA）无训练方法。我们的主要贡献可归纳如下：</p>
<ul>
<li>我们提出一种创新视角，通过将自然图像模式视为ID数据、生成图像模式视为
OOD
数据，来理解和改进生成图像检测。在此框架下，我们证明在自然图像上训练的模型，在温和假设条件下，能够确保生成图像检测的泛化能力。</li>
<li>本研究以能量基 OOD
检测为灵感，提出一种名为差异能量检测（DEnD）的创新框架，用于鉴别生成图像，并从理论上保证其泛化能力。</li>
<li>综合实验结果表明，我们的DEnD框架不仅超越了无需 SOTA
训练的方法，还优于大多数基于训练的检测器。此外，当面对Sora等无法访问的生成模型时，我们的方法展现出显著的泛化能力。</li>
</ul>
<h1 id="相关工作">2.相关工作</h1>
<p>​  高级图像生成模型<br/>​  近年来，生成模型因其能生成高质量合成图像而备受关注。生成对抗网络（GANs）[16,2,26,24]为图像生成奠定了基础。生成对抗网络（GANs）问世后，基于扩散的生成技术[22,44,10]在图像合成领域取得革命性突破。近期的先进生成模型如Stable
Diffusion[56]、DALL-E
3[50]、Midjourney[41]和FLUX[29]，均能根据文本描述生成细节丰富的图像，这标志着生成能力实现了重大飞跃。</p>
<p>​  生成图像检测<br/>​  早期生成图像检测的研究主要聚焦于颜色线索[39]和饱和度线索[40]。然而，随着ProGAN[15]和扩散模型[22]的出现，这些特征在检测任务中已不再可靠。与此同时，基于频率的检测方法[28,13,58,32]也大量涌现。主流的训练型方法主要致力于训练二元分类器网络。例如，CNNspot[68]通过特定数据增强技术，成功将基于ProGAN训练的二元分类器推广到其他架构。DIRE则利用扩散模型的重构误差来训练分类器。不过，训练型方法往往受限于泛化能力不足和高昂的训练成本。为应对这些挑战，无训练方法应运而生。aeroblade通过在潜在扩散模型中使用自动编码器计算重构误差来检测生成图像，但其效果仅限于LDMs。ZED[8]采用在自然图像上预训练的无损编码器，利用编码成本差异来检测生成图像。rigid[20]利用视觉基础模型表示空间中真实图像与AI生成图像对微小噪声扰动的鲁棒性差异。
FSD
[43]从图像中提取法医显微结构，并使用高斯混合模型对真实图像的分布进行建模。基于这些前期研究成果，我们从
OOD
检测的视角重新审视生成图像检测任务，并构建了DEnD框架。该框架在理论保障下展现出卓越的泛化能力。</p>
<p>​  基于能量的 OOD
检测<br/>​  分布外检测（OOD）是当前研究领域的关键方向，其核心在于识别与训练数据分布存在显著差异的样本。传统方法依赖于从softmax输出中提取置信度评分[21]，但神经网络对远离训练数据的输入可能产生过高的置信度[42]。相比之下，基于能量的
OOD 检测[33]通过将输入映射为标量值——该值在分布内数据中较低、在 OOD
数据中较高——从而实现更优性能。理论层面，[12]不仅确立了 OOD
检测可学习性的必要条件，还提出了若干表征特定应用场景下 OOD
检测可学习性的充分条件。这一理论基础构成了我们方法的理论根基。</p>
<h1 id="准备工作">3.准备工作</h1>
<p>​  本节受人类认知过程启发，将生成图像检测建模为 OOD
检测任务（参见第3.1节），并阐明本文的核心目标（参见第3.2节）。</p>
<h2 id="定义">3.1.定义</h2>
<p>​  本节将详细阐述如何将生成图像检测转化为 OOD
检测任务。给定一个包含自然图像和生成图像的特征空间<span
class="math inline">\({\mathcal X}\subset{\mathbb
R}^d\)</span>，以及两个模式空间<span class="math inline">\({\mathcal
T}_n:=\{1\}\)</span>（表示自然图像的特征模式）和<span
class="math inline">\({\mathcal
T}_g:=\{2\}\)</span>（表示生成图像的特征模式）。我们将自然图像视为ID数据，生成图像视为
OOD 数据。因此，我们得到定义在<span class="math inline">\({\mathcal X}
\times {\mathcal T}_n\)</span>上的一个ID联合分布<span
class="math inline">\(D_{X_nT_n}\)</span>，其中<span
class="math inline">\(X_n\in{\mathcal X}\)</span>和<span
class="math inline">\(T_n\in{\mathcal
T}_n\)</span>是随机变量。我们还定义了 OOD 联合分布<span
class="math inline">\(D_{X_gT_g}\)</span>，其中<span
class="math inline">\(X_g\in{\mathcal X}\)</span>和<span
class="math inline">\(T_g\in{\mathcal
T}_g\)</span>是随机变量。实证观察显示，自然图像与合成图像以任意且未知的比例混合：
<span class="math display">\[D_{X
T}:=(1-\pi^{\mathrm{out}})D_{X_{n}T_{n}}+\pi^{\mathrm{out}}D_{X_{g}T_{g}},\]</span>
​  其中常数 <span
class="math inline">\(\pi^{\mathrm{out}}\in[0,1)\)</span>表示未知的类别先验概率。我们只能观测到边缘分布：
<span
class="math display">\[D_{X}:=(1-\pi^{\mathrm{out}})D_{X_{n}}+\pi^{\mathrm{out}}D_{X_{g}}.\]</span>
​  我们将函数空间的一个子集定义为假设空间<span
class="math inline">\({\mathcal H}\subset\{h:{\mathcal
X}\rightarrow\{1,2\}\}\)</span>。1代表自然图像，2代表生成图像。h称为假设函数。我们研究假设空间<span
class="math inline">\({\mathcal
H}\)</span>的存在性，使得任何属于基于密度的空间 <span
class="math inline">\({\mathcal D}_{HT}^{\mu,b}\)</span>的联合分布 <span
class="math inline">\(D_{XT}\)</span>（参见附录C.2）都能满足泛化性（参见附录C.1）。</p>
<h2 id="目标">3.2.目标</h2>
<p>​  我们的设计目标可表述如下：通过使用在数据集S上训练的模型f来设计检测器g，使得对于从混合边缘分布<span
class="math inline">\(D_X\)</span>中抽取的任意测试数据x，检测器能够区分输入数据是自然生成的还是合成生成的。我们定义差异能量评分
<span
class="math inline">\(\lambda\)</span>（参见第4.3节）。检测器将评分较低的数据归类为自然图像，评分较高的数据归类为合成图像。训练数据<span
class="math inline">\(S:=\lbrace\mathbf{x}^{1},\ldots,\mathbf{x}^{n}\rbrace\)</span>是从自然图像的联合分布<span
class="math inline">\(D_{X_n}\)</span>中独立同分布抽取的。</p>
<h1 id="方法">4.方法</h1>
<p>​  在本节中，我们首先证明第3.1节所建模的检测器在温和假设条件下具有泛化能力（详见第4.1节）。基于理论框架，我们随后探讨了
OOD 检测的先进方法——基于能量的 OOD
检测（参见第4.2节）。然而实验表明，直接应用基于能量的 OOD
检测会导致性能欠佳。受此现象启发，并借鉴自监督学习的训练目标，我们提出了一种无需训练的泛化生成图像检测框架DEnD（详见第4.3节）。我们进一步为所提方法的泛化能力提供了理论保障（详见第4.4节），既验证了其实际有效性，又确立了理论可靠性。</p>
<h2 id="生成检测器的泛化能力">4.1生成检测器的泛化能力</h2>
<p>​  尽管我们已经完成了模型构建，但无法确定该检测器在何种情况下具备泛化能力。我们考虑了学习理论中的一个重要假设——可实现性假设（详见附录C.3）。该假设表明：假设空间<span
class="math inline">\({\mathcal
H}\)</span>中至少存在一个模型能够完美拟合训练数据，即不存在分类错误。基于此假设，我们从
OOD 检测的可学习性[12]中推导出一个重要引理：</p>
<p><strong>引理 4.1</strong> 在基于密度的空间<span
class="math inline">\({\mathcal D}_{HT}^{\mu,b}\)</span>中，若 <span
class="math inline">\(\mu({\mathcal X}) &lt;
+\infin\)</span>且满足可实现性假设，且当<span
class="math inline">\({\mathcal
H}\)</span>具有有限Natarajan维度[59]时，其在<span
class="math inline">\({\mathcal D}_{HT}^{\mu,b}\)</span>中的 OOD
检测器是可学习的。</p>
<p>​  在我们的理论框架中，检测器的泛化能力与 OOD
检测的可学习性具有等价性。因此，我们推导出若干检测器泛化能力的判定条件：</p>
<ul>
<li><span class="math inline">\(\mu({\mathcal X}) &lt;
+\infin\)</span>，即特征空间具有有限测度。</li>
<li>我们选择了一个合适的假设空间<span class="math inline">\({\mathcal
H}\)</span>，该空间满足可实现性假设。</li>
<li>我们选择的假设空间<span class="math inline">\({\mathcal
H}\)</span>具有有限的Natarajan维度，这表明模型的复杂性得到控制，且能够很好地泛化到未见过的数据上。</li>
</ul>
<h2 id="基于能量的检测器">4.2 基于能量的检测器</h2>
<p>​  由于我们已通过理论验证，将 OOD
检测任务构建的检测器在温和假设下具有泛化能力，因此我们考虑能否直接应用OOD检测方法来有效检测生成图像。我们采用了一种先进的
OOD 检测方法：基于能量的 OOD 检测[33] （附录E中提供了关于其他 OOD
检测方法的讨论）。</p>
<p>​  我们考虑一个判别性神经分类器<span
class="math inline">\(q(\mathbf{x}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{K}\)</span>，该分类器将输入<span
class="math inline">\(\mathbf{x}\in\mathbb{R}^{D}\)</span>映射为logit。基于能量的
OOD 检测定义了<span
class="math inline">\(\mathbf{x}\in\mathbb{R}^{D}\)</span>上的自由能函数<span
class="math inline">\(E({\bf x};q)\)</span>为： <span
class="math display">\[E({\bf
x};q)=-\tau\cdot\log\sum_{i}^{K}e^{q_i({\bf x})/\tau}.\]</span> ​  <span
class="math inline">\(q_i({\bf x})\)</span>表示<span
class="math inline">\(q({\bf x})\)</span>的第i个索引。温度系数<span
class="math inline">\(\tau\)</span> 被视作超参数。由于第i个标签<span
class="math inline">\(q_i({\bf x})\)</span>对应的logit可表示为<span
class="math inline">\(q_{i}({\bf x})=(f({\bf x}),{\bf
w}_{i})\)</span>，因此我们可以将自由能函数重新表述为： <span
class="math display">\[E({\bf x})=-\tau\cdot\log\sum_{i}^{K}e^{(f({\bf
x}),{\bf w}_{i})/\tau},\]</span> ​  其中<span
class="math inline">\(f(\mathbf{x}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}\)</span>表示从输入x中提取的特征，<span
class="math inline">\(\mathbf{w}_{i}\in\mathbb{R}^{d}\)</span>表示对应第i个标签的权重。我们用（a，b）表示向量a和b的内积。文献[33]从理论上证明，采用负对数似然（NLL）损失函数训练的模型会降低分布内数据点的能量值。在实际检测过程中，能量值较高的输入自然被视为
OOD 输入，反之亦然。</p>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110170346642.png"
alt="image-20251110170346642" />
<figcaption aria-hidden="true">image-20251110170346642</figcaption>
</figure>
<blockquote>
<p>图1：(a)
我们提出基于分数的鉴别能检测（DEnD）框架用于生成图像检测，自然图像的鉴别能分数较低，反之则较高。(b)
直接采用基于能量的 OOD 检测方法进行检测效果欠佳。(c)
受自监督学习训练目标启发，我们提出通过鉴别能分数检测生成图像的方法，该方法展现出强大的泛化能力（详见附录B）。</p>
</blockquote>
<p>​  遗憾的是，我们的实验表明自然图像与生成图像的能量分布完全无法区分（见图1b），导致检测器难以进行有效区分。这种局限性源于基于能量的OOD检测通常依赖于采用NLL损失函数训练的语义标签分类器，而自然图像与生成图像的差异主要体现在高层次模式的差异，而非简单的语义标签不匹配。为解决这一难题，我们提出用能捕捉非标签全局模式的模型替代语义标签分类器，并重新定义与模型训练目标相匹配的能量函数。</p>
<h2 id="基于差异能量的检测器">4.3 基于差异能量的检测器</h2>
<p>​  如文献[61]所示，与在标签空间中运行的监督模型相比，自监督模型对全局特征表现出更高的敏感性。这一特性赋予了自监督模型潜在能力，使其能够识别生成图像与自然图像之间的模式差异。<br/>​  在自监督学习[18]中，常用的方法如下：给定一个包含N个样本的批次，其特征提取器<span
class="math inline">\(f(*)\)</span>对锚点x的正样本表示为<span
class="math inline">\({\bf
x}^+=m(x)\)</span>，其中m(x)表示高斯模糊等随机变换。其余N−1个样本则视为负样本。给定训练样本x，损失函数可表示为：
<span
class="math display">\[\displaystyle-\log\displaystyle\frac{e^{(f_{\theta}({\bf
x}),f_{\theta}({\bf
x}^{+}))/\tau)}}{\displaystyle\sum_{i=0}^{N}e^{(f_{\theta}({\bf
x}),f_{\theta}({\bf x}_{i}))/\tau}},\]</span> ​  其中符号<span
class="math inline">\({\bf x}_0={\bf
x}^+\)</span>表示随机变换后的正样本。在自监督学习框架下，每个负样本<span
class="math inline">\({\bf
x}_i\)</span>在判别模型中对应一个独立类别。因此，负样本的特征向量<span
class="math inline">\(f({\bf x}_i)\)</span>与其所属类别的权重<span
class="math inline">\(\mathbf{w}_{i}\)</span>相对应。通过整合公式4和公式5，我们重新定义能量函数（下文所述能量函数均遵循本节所述的具体公式）如下：
<span class="math display">\[E({\bf x};f)=\sum_{i=0}^{N}e^{(f({\bf
x}),f({\bf x}_{i}))/\tau}.\]</span>
​  该总和对应一个正样本和N个负样本。在基于ID数据训练的自监督模型<span
class="math inline">\(f(*)\)</span>中，其训练目标可表示为： <span
class="math display">\[\operatorname*{min}_{\theta}{\mathbb E}_{
\mathbf{x}\sim P_{\mathrm{ID}},m\sim{\mathcal M}}E({\bf
x};f_{\theta}).\]</span> ​  在我们的框架中，随机变换函数<span
class="math inline">\(m(*)\)</span>被视为一个随机变量，该变量取自定义的概率分布<span
class="math inline">\({\mathcal
M}\)</span>。该分布综合体现了对数据进行各类变换时的概率可能性。<br/>​  我们从随机变换分布M中抽取k个样本点，每个样本点对应一个变换函数<span
class="math inline">\(m_i(*)\)</span>。训练目标可表示为： <span
class="math display">\[\operatorname*{min}_{\theta}\mathbb{R}_{\mathbf{x}\sim
P_{\mathrm{ID}}}\left[\frac{1}{k}\sum_{i}^{k}E_{m_{i}}(\mathbf{x};f_{\theta})\right].\]</span>
​  符号<span class="math inline">\(E_{m_{i}}\)</span>表示由随机变换<span
class="math inline">\(m_i(*)\)</span>产生的能量。<br/>​  因此，对于ID分布中x的能量，对于任意
ϵ &gt;0，我们可以推导出（完整推导过程见附录D）： <span
class="math display">\[|E(\mathbf{x};f)-E(m(\mathbf{x});f)|\lt
\epsilon.\]</span>
​  如图2所示，我们在ImageNet数据集[9]上进行实验，分别采用随机变换分布<span
class="math inline">\({\mathcal
M}\)</span>在1、3、5、10和15次采样。为平衡准确性和计算成本，最终选择单次采样方案。</p>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110195749110.png"
alt="image-20251110195749110" />
<figcaption aria-hidden="true">image-20251110195749110</figcaption>
</figure>
<p>​  因此，我们可以推导出：对于任意的<span
class="math inline">\(\epsilon\gt
0\)</span>，任何ID数据x（即自然图像），以下结论成立： <span
class="math display">\[|E(\mathbf{x};f)-E(m(\mathbf{x});f)|\lt
\epsilon\]</span> ​  基于该方法，我们通过以下方式计算出差异能量评分：
<span class="math display">\[\lambda({\bf x};f,m)=|E({\bf x};f)-E(m({\bf
x});f)|\,.\]</span>
​  根据公式10的推导，自监督模型的训练目标可表述为最小化ID数据（自然图像）的微分能量分数。因此，对于从ID分布中抽取的x（对应自然图像），如图1c所示，
<span class="math inline">\(\lambda({\bf
x};f,m)\)</span>的值相对较小。鉴于微分能量分数的判别能力，我们将其应用于生成图像检测：
<span class="math display">\[g({\bf x};\gamma,m,f)=\begin{cases}1(n a t
u r a l)&amp;\mathrm{if}\ \lambda\leq\gamma,\\2(g e n e r a t e
d)&amp;\mathrm{if}\ \lambda\gt\gamma,\\\end{cases}\]</span> ​  其中<span
class="math inline">\(\gamma\)</span>为阈值（有关阈值的更多详细说明，请参见附录F），<span
class="math inline">\(f\)</span>表示预训练的自监督模型。实际应用中，我们采用强大的自监督视觉变换器（ViT）模型DINOv2（具体自监督模型选择详见附录I.2），该模型基于海量自然图像数据集完成预训练。系统通过差异能量评分进行分类：评分较高的图像被判定为生成图像，评分较低的则判定为自然图像。</p>
<h2 id="dend的泛化性">4.4 DEnD的泛化性</h2>
<p>​  本节在第4.1节的基础上，从理论层面构建了基于差分能量的检测（DEnD，Differential
Energy-based Detection）框架，验证了该框架在生成图像检测中的泛化性。</p>
<p>​  在第4.1节中，我们指出为确保检测器的泛化能力，假设空间必须满足可实现性假设。因此，我们首先验证所提出的DEnD是否符合该假设。我们的方法设计了<span
class="math inline">\({\mathcal
H^*}\)</span>，其包含一个基于评分的分类器（见图1a）： <span
class="math display">\[h_{\gamma}({\bf
x})=\begin{cases}1&amp;\mathrm{if}\ \lambda({\bf
x};m,f)\leq\gamma,\\2&amp;\mathrm{if}\ \lambda({\bf
x};m,f)\gt\gamma,\\\end{cases}\]</span>
​  DEnD的设计利用了这样一个特性：在自然图像中，f函数会导致<span
class="math inline">\(\lambda({\bf
x};m,f)\)</span>值相对较低，而在生成图像中则会显著升高，这种差异源于自监督模型的训练目标。该特性为以下定理奠定了理论基础：</p>
<p><strong>引理4.2</strong> 若存在满足以下条件的阈值 <span
class="math inline">\(\lambda^\prime\in{\mathbb R}\)</span>： <span
class="math display">\[\operatorname*{sup}_{\bf x\in s u p p
D_{X_{n}}}\lambda({\bf
x};f,m)\lt\gamma^{\prime}\lt\operatorname*{inf}_{\bf x\in s u p p
D_{X_{g}}}\lambda({\bf x};f,m),\]</span> ​  假设空间<span
class="math inline">\({\mathcal
H^*}\)</span>满足可实现性假设，其中supp表示支撑集。</p>
<p>​  证明过程详见附录C.4。该定理表明，由于我们方法的判别能力，自然图像与生成图像之间的差异能量分数具有可分离性，且可实现性假设成立。这为我们的方法提供了关键的普适性保证。基于DEnD对可实现性假设的遵循，我们进一步为所提出的DEnD建立泛化性：</p>
<p><strong>引理4.3</strong> 若假设空间<span
class="math inline">\({\mathcal
H^*}\)</span>具有有限的Natarajan维度，则DEnD框架在 <span
class="math inline">\({\mathcal D}_{HT}^{\mu,b}\)</span>下对<span
class="math inline">\({\mathcal H^*}\)</span>具有泛化性。</p>
<p>​  证明详见附录C.5。该定理充分展现了DEnD在设计中巧妙运用微分能量评分的能力，确保了理论框架下的泛化性能。从实际应用角度看，正如第5节所述，我们的方法展现出卓越的泛化能力，与理论分析结果高度吻合。这两个方面都充分证明了我们方法的普适性。</p>
<h1 id="实验">5.实验</h1>
<p>​  本节通过一系列实验，评估生成图像检测器在涉及未知生成模型的实际场景中的表现。实验结果表明，我们的方法具有显著优势（消融研究详见附录I）。</p>
<h2 id="设置">5.1.设置</h2>
<p>​  <strong>数据集</strong><br/>​  我们评估了生成图像检测器在两个常用数据集上的性能：ImageNet
[9] 和 LSUN -bedroom
[74]。针对ImageNet数据集，生成图像采用了ADM[10]、ADM-G、 LDM
[56]、DiT-XL2[51]、BigGAN[2]、GigaGAN[24]、StyleGAN[26]、RQ-Transformer[30]和MaskGIT[5]等生成模型。针对
LSUN-BEDROOM数据集，我们采用ADM、 DDPM [22]、iDDPM[44]、Diffusion
Projected GAN[70]、Projected GAN[70]、StyleGAN[26]和Unleashing
Transformer[1]等生成模型生成图像。为验证本方法在未知生成模型场景下的优越性，我们在GenImage[79]和AIGCDetectBenchmark[77]两大通用基准上进行检测器评估。GenImage包含Stable
Diffusion V1.4[56]、Stable Diffusion V1.5[56]、glide[45]、 VQDM
[17]、Wukong[72]、BigGAN、ADM和Midjourney[41]；AIGCDetectBenchmark[77]则涵盖ProGAN[25]、StyleGAN、BigGAN、StarGAN[7]、GauGAN[49]、StyleGAN2[27]、
WFIR [71]、ADM、glide、Midjourney、Stable Diffusion V1.4、Stable
Diffusion V1.5、 VQDM
、Wukong和DALL-E2[54]。为验证本方法对不可用生成模型的泛化能力，我们还在Sora[47]上进行了检测器评估。具体数据集来源详见附录G。</p>
<p>​  <strong>评估指标</strong><br/>​  我们沿袭先驱研究者的足迹，采用平均精度（AP，Average
Precision）和接收者操作特征曲线下面积（AUROC，Area Under the Receiver
Operating Characteristic
Curve）作为核心评估指标。在部分实验中，为确保与现有基准的可比性，我们还额外纳入准确率（ACC，accuracy）作为补充评估指标。</p>
<p>​  <strong>基线网络</strong><br/>​  我们采用基于训练和无训练两种方法作为基准。对于基于训练的方法，我们选取DIRE[69]、CNNspot[68]、UnivFD[46]、
DRCT
[6]和NPR[64]作为基准模型。部分基准模型直接采用其论文中报告的结果，包括Frank[14]、Durall[11]、Patchfor[4]、F3Net[52]、SelfBland[60]、GANDetection[38]、LGrad[65]、ResNet-50[19]、DeiTS[66]、Swin-T[35]、Spec[75]、FreDect[13]、Fusing[23]、
LNP
[32]、GenDet[78]、LaRE2[37]和GramNet[36]。对于免训练方法，我们采用aeroblade[55]和rigid[20]作为基准模型。</p>
<p>​  <strong>实验细节</strong><br/>​  在实验中，我们采用了强大的预训练自监督模型DINOv2[48]。我们选用其ViT-L/14模型，该模型以速度与性能的完美平衡著称。通过设置批量大小N=128和温度系数
τ
=0.6，我们获得了最佳性能表现（详见附录I.1）。关于m(x)的选取，我们采用均值为0、方差为0.04的高斯噪声（详见附录H）。</p>
<h2 id="主要结果">5.2.主要结果</h2>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110193747397.png"
alt="image-20251110193747397" />
<figcaption aria-hidden="true">image-20251110193747397</figcaption>
</figure>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110193801229.png"
alt="image-20251110193801229" />
<figcaption aria-hidden="true">image-20251110193801229</figcaption>
</figure>
<p>​  <strong>与现有方法的比较</strong><br/>​  如表1和表2所示，相较于在
LSUN
-Bedroom和ImageNet数据集上基于训练的方法，我们的方法具有更强的泛化能力，在对抗大多数生成模型时表现更优，整体水平显著提升，充分展现了我们这种无需训练的通用方法的优越性。与无需训练的Aeroblade和Rigid方法相比，我们的方法在多数生成模型上展现出显著优势。虽然Rigid采用基于经验观察的噪声方法，但我们的方法源自自监督模型的训练目标。这种基础性视角使我们设计出更有效的差异能量评分机制，从而获得更优性能。</p>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110194008537.png"
alt="image-20251110194008537" />
<figcaption aria-hidden="true">image-20251110194008537</figcaption>
</figure>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110194040205.png"
alt="image-20251110194040205" />
<figcaption aria-hidden="true">image-20251110194040205</figcaption>
</figure>
<p>​  如表3和表5所示，面对更先进复杂的生成模型时，基于训练的方法通常在处理训练过程中未见过的生成模型时表现欠佳。相比之下，我们的方法具有出色的泛化能力，性能显著优于现有基于训练的方法。然而，由于预训练模型表征能力的局限，当处理某些高保真图像（尤其是来自Stable
Diffusion的图像）时，我们的方法会出现性能下降。我们认为，采用表征能力更强的模型可以缓解这一局限。</p>
<p>​  <strong>关于泛化能力的讨论</strong><br/>​  我们的方法在泛化性能方面展现出显著提升。从训练角度而言，传统基于训练的方法常面临过拟合问题。如表5所示，基于ProGAN训练的模型仅在其他GAN生成样本上表现良好。相比之下，我们的免训练方法天生规避过拟合风险。从理论层面看，由于不同生成架构的多样性，生成图像的模式可能千差万别。这种复杂性给基于训练的方法带来重大挑战。而我们的方法将所有生成图像的模式视为
OOD
数据，从而在各类生成模型间保持强大的泛化能力。此外，我们的方法还提供了泛化性的理论保障，这是现有方法所不具备的独特优势。</p>
<p>​  <strong>在Sora上的评估</strong><br/>​  Sora等视频生成模型的架构往往未知，这使得检测这类新型未知模型更具挑战性。</p>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110194308574.png"
alt="image-20251110194308574" />
<figcaption aria-hidden="true">image-20251110194308574</figcaption>
</figure>
<p>​  如表4所示，我们在Sora上的实验表明，我们的方法具有强大的泛化能力，即使在未知架构的生成模型上测试，仍能取得具有竞争力的性能——这是现有方法所不具备的关键优势。</p>
<h2 id="鲁棒性评估">5.3.鲁棒性评估</h2>
<p>​  在实际应用中，检测器经常需要处理质量下降的图像。例如，有损压缩可能产生伪影，而通信信道传输过程中通常会产生噪声。基于先前研究[55]，我们评估了检测器在JPEG压缩、高斯噪声和高斯模糊等常见场景下的鲁棒性。这些实验均在ImageNet数据集上完成。</p>
<figure>
<img
src="../postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110194448144.png"
alt="image-20251110194448144" />
<figcaption aria-hidden="true">image-20251110194448144</figcaption>
</figure>
<blockquote>
<p>图3：检测器在处理退化图像时的性能表现。(a)：JPEG格式图像，质量参数q。(b)：高斯噪声，标准差
<span class="math inline">\(\sigma\)</span> 。(c)：高斯模糊，标准差
<span class="math inline">\(\sigma\)</span> 。</p>
</blockquote>
<p>​  如图3所示，深度神经网络（Deep Neural
Network，DEnD）在各类图像退化场景中均展现出卓越性能，充分体现了其强大的鲁棒性。相比之下，其他基于训练的方法往往表现欠佳。这种优势源于我们方法的内在泛化能力——该能力建立在坚实的理论基础之上，使其能够始终如一地将退化的ID数据（自然图像）分类为ID数据（自然图像）。</p>
<h1 id="限制">6.限制</h1>
<p>​  1) 本研究将生成图像检测问题转化为 OOD 检测任务，并基于能量型 OOD
检测方法提出创新框架。虽然当前方案以能量型 OOD
检测为核心，但我们明确承认其他先进 OOD
检测策略的可行性。后续研究将重点探索其他 OOD 检测策略的适用性。2)
实验结果表明，我们的方法凭借理论保障展现出卓越性能。然而受限于训练集规模，预训练模型在面对真实数据分布偏移时（详见附录J），往往难以充分发挥框架潜力。未来我们将通过微调模型来提升泛化能力。</p>
<h1 id="结论">7.结论</h1>
<p>​  本文从人类认知辨别生成图像的能力中获得启发，提出了一种全新的生成图像检测理解与改进视角：将其建模为
OOD
检测任务。基于此，我们阐明了完全基于自然图像训练的模型在生成图像检测中的可行性。为实现这一洞见，我们引入了基于微分能量的检测（DEnD）框架——一种无需训练且具有普适性的生成图像检测方法。大量实验表明，我们的方法在常见基准测试中表现优异。此外，该方法展现出卓越的泛化能力，能有效处理架构未知的生成模型（如Sora）。更广泛地说，我们的研究不仅在理论上有所贡献，还提供了一种兼具高效性和泛化能力的生成图像检测方法，为应对日益严峻的图像伪造危机提供了解决方案。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/Towards-Generalizable-Detector-for-Generated-Image/">https://zhaozw-szu.github.io/Towards-Generalizable-Detector-for-Generated-Image/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">喵</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/postimages/Towards-Generalizable-Detector-for-Generated-Image/image-20251110170346642.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/MOOD/" title="Rethinking Out-of-distribution (OOD) Detection:Masked Image Modeling is All You Need"><img class="cover" src="/postimages/MOOD/framework.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Rethinking Out-of-distribution (OOD) Detection:Masked Image Modeling is All You Need</div></div></a></div><div class="next-post pull-right"><a href="/ForgeLens/" title="ForgeLens:Data-Efficient Forgery Focus for Generalizable Forgery Image Detection"><img class="cover" src="/postimages/ForgeLens/image-20251110110219713.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ForgeLens:Data-Efficient Forgery Focus for Generalizable Forgery Image Detection</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">191</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><a href="/code">代码页面</a>：收罗图像取证安全领域已公布/待公布的代码 <br>,<a href="/competition">比赛页面</a>：收罗图像取证安全领域的比赛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">2.相关工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">3.准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-text">3.1.定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-text">3.2.目标</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">4.方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A3%80%E6%B5%8B%E5%99%A8%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-text">4.1生成检测器的泛化能力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%83%BD%E9%87%8F%E7%9A%84%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="toc-text">4.2 基于能量的检测器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%B7%AE%E5%BC%82%E8%83%BD%E9%87%8F%E7%9A%84%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="toc-text">4.3 基于差异能量的检测器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dend%E7%9A%84%E6%B3%9B%E5%8C%96%E6%80%A7"><span class="toc-text">4.4 DEnD的泛化性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">5.实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE"><span class="toc-text">5.1.设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C"><span class="toc-text">5.2.主要结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%B2%81%E6%A3%92%E6%80%A7%E8%AF%84%E4%BC%B0"><span class="toc-text">5.3.鲁棒性评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%90%E5%88%B6"><span class="toc-text">6.限制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">7.结论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'all'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
  //- console.log('MathJax loaded')
} else {
  // 重置 TeX 状态并重新渲染
  MathJax.startup.promise.then(() => {
    MathJax.texReset();  // 重置 TeX 编号等状态
    MathJax.typesetPromise();
  });

  //- MathJax.startup.document.state(0)
  //- MathJax.texReset()
  //- MathJax.typesetPromise()
  //- console.log('MathJax reset')
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>