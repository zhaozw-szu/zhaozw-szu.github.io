<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning | zhaozw后院</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning   Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE, Xinyu Zhang ,">
<meta property="og:type" content="article">
<meta property="og:title" content="Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning">
<meta property="og:url" content="https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/index.html">
<meta property="og:site_name" content="zhaozw后院">
<meta property="og:description" content="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning   Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE, Xinyu Zhang ,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/img/coverImage/cover5.jpg">
<meta property="article:published_time" content="2024-09-02T12:16:56.000Z">
<meta property="article:modified_time" content="2024-09-02T15:14:57.104Z">
<meta property="article:author" content="Zhaozw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/img/coverImage/cover5.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-02 23:14:57'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-chart-simple"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-video"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/coverImage/cover5.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="zhaozw后院"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">zhaozw后院</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-chart-simple"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-video"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-02T12:16:56.000Z" title="发表于 2024-09-02 20:16:56">2024-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-02T15:14:57.104Z" title="更新于 2024-09-02 23:14:57">2024-09-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><center>
Robust Camera Model Identification Over Online Social Network Shared
Images via Multi-Scenario Learning
</center>
<center>
Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE,
Xinyu Zhang ,
</center>
<br/>
<center>
Jinyu Tian , Member, IEEE, and Weiwei Sun
</center>
<h1 id="摘要">摘要</h1>
<p>​  相机模型识别（CMI，Camera model
identification）可广泛应用于图像取证的真实性鉴定、版权保护、伪造检测等领域。同时，随着互联网的蓬勃发展，在线社交网络（OSNs，
online social
networks）已成为图像共享和传输的主导渠道。然而，在OSNs上不可避免的损耗操作，如压缩和后处理，给现有的CMI方案带来了巨大的挑战，因为它们严重破坏了被调查图像中留下的相机痕迹。在这项工作中，我们提出了一种新的CMI方法，它对各种OSN平台的有损操作具有鲁棒性。具体来说，可以观察到，一个相机跟踪提取器可以很容易地训练在一个单一的退化场景（例如，一个特定的OSN平台）；而在混合退化场景中（例如，多个OSN平台）要困难得多。受此启发，我们设计了一种新的多场景学习（MSL，
multi-scenario
learning）策略，使我们能够在不同的osn中提取鲁棒的摄像机痕迹。此外，注意到图像平滑区域由OSN引起的失真更少，而图像信号本身的干扰更小，我们提出了一种平滑感知的痕迹提取器（STATE，SmooThness-Aware
Trace
Extractor），它可以根据输入图像的平滑度自适应地提取相机痕迹。通过与四种先进方法的比较实验，验证了该方法的优越性，特别是在各种OSN传输场景下。特别是在开放集摄像机模型验证任务中，我们在FODB数据集上的AUC大大超过第二名15.30%；而在闭集相机模型分类任务中，我们在SIHDR数据集的F1中显著领先第二名34.51%。我们所提出的方法的代码可在https://github.com/HighwayWu/CameraTraceOSN上找到。<br/>​  稿件于2022年11月24日收到；分别于2023年8月4日和2023年9月18日修订；2023年9月18日接受。出版日期为2023年9月25日；当前版本的日期为2023年11月20日。澳门科技发展基金2021-2023、0072/2020/AMJ、0022/2022/2/A1、0014/2022/AFJ；部分由澳门大学研究委员会MYRG2020-00101-FST和MYRG2022-00152-FST；中国自然科学基金61971476；部分由阿里巴巴集团通过阿里巴巴创新研究项目。协调审查这份手稿并批准其出版的副主编是Dr.
Benedetta Tondi。（通讯作者：Jiantao Zhou）<br/>​  Haiwei Wu、 Jiantao
Zhou、Xinyu
Zhang就职于智慧城市物联网国家重点实验室和澳门大学科技部计算机与信息科学系，中国澳门999078（电子邮件：
yc07912@umac.mo；jtzhou@umac.mo；mc14958@umac.mo）。<br/>​  Jinyu
Tian就职于澳门科技大学创新工程学院，中国澳门999078（电子邮件：
jytian@must.edu.mo）。<br/>​  Weiwei
Sun在阿里巴巴集团工作，位于中国，杭州311100（电子邮件：
sunweiwei.sww@alibaba-inc.com）。<br/>​  数字对象标识符
10.1109/TIFS.2023.3318968<br/>​  索引术语：相机模型识别，在线社交网络，深度神经网络，鲁棒性。</p>
<h1 id="引言">引言</h1>
<p>我们的主要贡献如下：</p>
<ul>
<li>据我们所知，是我们首次将MSL策略用于CMI，并证明了该策略对OSN传输具有令人满意的鲁棒性。<br/>-
我们提出了STATE根据区域平滑度，灵活有效地学习相机的痕迹。<br/>-
与最先进的方法[5]，[6]，[10]，[11]方法相比，我们的方法获得了更好的鲁棒性性能，特别是在OSN传输的场景中。<br/>-
我们基于现有的相机数据集FODB [12]和SIHDR
[23]，构建了9个流行的osn（Twitter、Telegram、Whatsapp、Instagram、
Facebook、Weibo、QQ、Dingding和WeChat）的OSN传输数据集，不仅可以评估CMI算法的鲁棒性，而且有利于不同的取证应用。</li>
</ul>
<h1 id="照相机模型识别的基线方案">照相机模型识别的基线方案</h1>
<p>​  在深入研究CMI的鲁棒设计之前，我们首先介绍了基于学习的基线方案的架构，该方案包括两个网络，即提取器和分类器，如图4所示。</p>
<figure>
<img
src="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902202942944.png"
alt="image-20240902202942944" />
<figcaption aria-hidden="true">image-20240902202942944</figcaption>
</figure>
<p>​  与现有的[6]、[10]、[13]方案类似，提取器的目的是提取摄像机的痕迹，而分类器则监督提取器的训练。在我们的基线方案中，提取器的特定体系结构采用了EfficientNet-b0。该选择是基于一个初步的实验，比较了不同候选架构的相机痕迹提取性能，包括ResNet
[40]、VGG [41]、XceptionNet[42]、EfficientNet[43]、ViT
[44]和SwinTransformer[45]等。对于分类器架构，我们简单地将其设计为线性层和SoftMax变换的组合。</p>
<p>​  一旦确定了提取器和分类器网络的体系结构，另一个关键问题是如何在测试阶段训练和使用它们。在图4中，我们说明了基线CMI方案的训练和测试过程。在训练阶段，给定一个由Y个不同相机模型捕获的图像组成的数据集<span
class="math inline">\(\cal{D}\)</span>，<span
class="math inline">\((\mathbf{X},y)\)</span>表示一对训练数据，其中<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{H\times W\times
C}\)</span>为输入图像，<span
class="math inline">\(y\in\{1,2,\cdots,Y\}\)</span>为相机的标签。具有可训练参数<span
class="math inline">\(\theta\)</span>的提取器<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>旨在提取能够表征相机痕迹的高级特征<span
class="math inline">\(\mathbf{T}\)</span>，<span
class="math inline">\(\mathbf{T}=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X})\)</span>。为了监督<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，分类器<span
class="math inline">\(\mathcal{C}_{\phi}\)</span>将<span
class="math inline">\(\mathbf{T}\)</span>转换为<span
class="math inline">\(\mathbf{\hat{y}}=\mathcal{C}_{\phi}(\mathbf{T})\)</span>来计算损失<span
class="math inline">\(\ell(\mathbf{\hat{y}},y)\)</span>，其中<span
class="math inline">\(\ell\)</span>是广泛使用的交叉熵损失，即：<br/><span
class="math display">\[\ell(\hat{\mathbf{y}},y)=-\sum_{i=1}^Y\mathcal{I}[y=i]\cdot\log(\hat{\mathbf{y}}^{&lt;i&gt;}).\]</span><br/>​  这里<span
class="math inline">\(\mathbf{\hat{y}}^{&lt;i&gt;}\)</span>表示<span
class="math inline">\(\mathbf{\hat{y}}\)</span>的第<span
class="math inline">\(i\)</span>个条目，<span
class="math inline">\(\mathcal{I}[y=i]\)</span>是一个二进制指标函数，如果是y
= i，则取1，否则取0。<br/>​  经过训练，训练良好的<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>可以根据[6]用于两个测试用例：
1)开集验证，目的是推断两个测试图像是否被同一相机模型捕获；2)闭集分类，目的是在有限的相机模型池中识别测试图像的源相机模型。<br/>​  请注意，在这两种情况下，分类器<span
class="math inline">\(\mathcal{C}_{\phi}\)</span>都被丢弃了。具体来说，在前一种情况下，给定两个图像<span
class="math inline">\(\mathbf{X_1}\)</span>和<span
class="math inline">\(\mathbf{X_2}\)</span>，它们的痕迹<span
class="math inline">\(\mathbf{T_1}\)</span>和<span
class="math inline">\(\mathbf{T_2}\)</span>首先由训练过的<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取。然后，通过余弦相似度计算这些痕迹被同一相机拍摄的概率：<br/><span
class="math display">\[S(\mathbf{T}_1,\mathbf{T}_2)=\cos\Big(\frac{\mathbf{T}_1}{||\mathbf{T}_1||},\frac{\mathbf{T}_2}{||\mathbf{T}_2||}\Big).\]</span><br/>​  对于闭集分类案例的测试过程，给出一组已知标签的<span
class="math inline">\(y_i\in\{1,2,\cdots,Y\}\)</span>的图像，<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>首先提取它们的痕迹<span
class="math inline">\(\mathbf{T_i}\)</span>。然后，用相同的相机模型平均痕迹，可以形成一个痕迹池<span
class="math inline">\(\{\mathbf{\bar{T}}_{i}\}_{i=1}^{Y}\)</span>。换句话说，池中的每个元素代表一个特定相机模型的平均痕迹。当一个测试图像<span
class="math inline">\(\mathbf{X_t}\)</span>出现时，其预测的相机类型<span
class="math inline">\(y_t\)</span>可以从痕迹池中搜索最大的相似性，即：<br/><span
class="math display">\[y_t=\underset{i}{\mathrm{argmax}}S(\mathbf{T}_t,\bar{\mathbf{T}}_i)\]</span><br/>​  其中，<span
class="math inline">\(\mathbf{T_t}\)</span>为通过<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的测试图像的痕迹。</p>
<p>​  虽然我们的基线CMI可以用于提取摄像机痕迹，并最终在上述两个测试用例中被采用，但在有损传输上的性能，如各种OSN传输场景，可能会严重降低。这些场景所引入的扭曲很可能会破坏相机的痕迹，这在本质上是脆弱的。在表I中，我们简要展示了FODB
[12]数据集上不同的osn造成的畸变，包括平均分辨率和文件大小的减少，以及平均采用的JPEG质量因子（QFs）。</p>
<figure>
<img
src="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902205601762.png"
alt="image-20240902205601762" />
<figcaption aria-hidden="true">image-20240902205601762</figcaption>
</figure>
<p>由FODB
[12]数据集上不同的OSNS造成的失真。在这里，“SCALE”和“SIZE”分别表示分辨率减少和文件大小减少的百分比。另外，“JPEG
QF”表示qf值的平均值。</p>
<p>​  可以看出，最严重的畸变是由Dingding造成的，导致分辨率降采样84.60%，文件大小减少93.93%，从最低的平均QF值69.3也可以观察到。其中考虑的最友好的OSN平台是微博，导致分辨率降采样57.77%，文件大小减少60.19%。可以清晰地得到如下结论，这些OSN失真将严重影响CMI算法，因此设计一个稳健的CMI方案至关重要，能够可靠地提取OSN传输中摄像机的痕迹。</p>
<h1 id="鲁棒相机模型识别">鲁棒相机模型识别</h1>
<p>​  在有了基线之后，我们提出了一种新的方法来设计一个强大的CMI来对抗各种osn上的传输，其中关键的创新是双重的：MSL和STATE。正如预期的那样，并将通过实验验证，STATE为不同的输入提取了更多的自适应痕迹，而MSL策略更好地监督了状态的训练，共同有助于鲁棒提取摄像机痕迹的目标。<br/>​  所提出的鲁棒CMI的训练过程如图5所示。</p>
<figure>
<img
src="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png"
alt="image-20240902210022345" />
<figcaption aria-hidden="true">image-20240902210022345</figcaption>
</figure>
<p>图5。我们提出的鲁棒CMI的训练过程。STATE从给定的图像中提取摄像机的痕迹，而MSL策略利用一组分类器，在逐个场景的基础上监督STATE的训练。</p>
<p>​  具体来说，给定一个训练图像<span
class="math inline">\(\mathbf{X}\)</span>，我们首先收集其在N个场景下的传输变体。为了达到令人满意的鲁棒性，在本工作中，我们定义了由NoTrans组成的场景。（原始），两种类型的OSN传输：Facebook和Whatsapp，这也被考虑在VISION数据集[17]。此外，考虑到训练和测试场景之间的差异，我们手工制作了一个增强场景来改进对未知（新的）osn的泛化，其中增强包括常用的后处理操作，如缩放、压缩、模糊和噪声添加。更多关于场景影响的分析，例如，不同数量的场景及其组合被推迟到第五章中的消融研究G.4。对于每个变体，STATE
<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取相应的痕迹<span
class="math inline">\(\mathbf{T}\)</span>，其中嵌入的平滑注意模块引导<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更多地关注<span
class="math inline">\(\mathbf{X}\)</span>中的平滑区域。根据观察I，使用N个分类器<span
class="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>来监督<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，其中每个分类器处理每个单独场景的训练。例如，<span
class="math inline">\(\mathcal{C}_{\phi_1}\)</span>处理NoTrans场景，<span
class="math inline">\(\mathcal{C}_{\phi_2}\)</span>处理Facebook场景等。接下来，将在<span
class="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>上生成的总损失<span
class="math inline">\(\sum_{n=1}^{N}\mathcal{L}_{n}\)</span>进行反向传播，以更新与STATE
<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>相关的可学习参数<span
class="math inline">\(\theta\)</span>和与N个分类器相关的<span
class="math inline">\(\{\boldsymbol{\phi}_n\}_{n=1}^N\)</span>。</p>
<p>​  在测试阶段，该过程类似于基线CMI，其中只需要训练好的状态<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>，而N个分类器<span
class="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>被丢弃。在下面，我们将提供更多关于我们提出的MSL策略和STATE
的更多细节。</p>
<h2 id="a.-多场景学习mslmulti-scenario-learning">A.
多场景学习（MSL，Multi-Scenario Learning）</h2>
<p>​  如前所述，受观察I启发，我们的MSL策略使用了多个具有非共享权重的分类器来监督多个场景的训练。多分类器产生的一个隐式问题是训练过程可能不稳定。这是因为训练的复杂性因不同的场景不同，导致梯度方向上的冲突。为了弥补这一缺陷，我们建议在MSL的逆向过程中集成一个动量掩膜操作来减轻梯度冲突。我们现在详细介绍我们提出的MSL的前向和反向过程的细节。</p>
<h3 id="前向过程">1)前向过程</h3>
<p>​  设<span class="math inline">\(\mathbf{X}\)</span>为输入图像，<span
class="math inline">\(\{\mathbf{X}_{n}\}_{n=1}^{N}\)</span>是其在N个场景下的N个变体，<span
class="math inline">\(\{\mathbf{T}_{n}\}_{n=1}^{N}\)</span>是它们由STATE
<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的痕迹。有关<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的细节将推迟到下一小节。同时，利用每个分类器<span
class="math inline">\(\mathcal{C}_{\phi_n}\)</span>，根据第n个场景中的训练数据来监督训练过程，损失如下：<br/><span
class="math display">\[\mathcal{L}_n=\ell(\mathcal{C}_{\boldsymbol{\phi}_n}(\mathbf{T}_n),y),\]</span><br/>​  其中，<span
class="math inline">\(\ell\)</span>为(1)中给出的交叉熵损失。<br/>​  值得注意的是，损失函数(4)和损失函数(1)之间的关键区别在于前者涉及多个非共享分类器，而后者只使用一个。另外，请注意，<span
class="math inline">\(\mathbf{T}_{n}\)</span>和<span
class="math inline">\(\mathcal{C}_{\phi_n}\)</span>应该有相同的下标n，以便逐场景实现MSL训练场景。</p>
<h3 id="反向过程">2)反向过程</h3>
<p>​  在反向过程中，分类器<span
class="math inline">\(\mathcal{C}_{\phi_n}\)</span>的参数更新为：<br/><span
class="math display">\[\phi_n=\phi_n-r\nabla_{\phi_n}\mathcal{L}_n,\]</span><br/>​  其中，r是学习率。同样，提取器<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的反向更新可以表示为：<br/><span
class="math display">\[\theta=\theta-r\sum_{n=1}^N\nabla_{\theta}\mathcal{L}_n.\]</span><br/>​  然而，这里的<span
class="math inline">\(\sum_{n=1}^N\nabla_{\boldsymbol{\theta}}\mathcal{L}_n\)</span>由N项组成，对应于N个场景，其中不一致的梯度方向可能导致冲突，导致<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>
[18]的次优训练。此外，通过<span
class="math inline">\(\mathbf{T}_n=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X}_n)\)</span>和使用链规则，(6)可以重写为：<br/><span
class="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span><br/>​  其中，<span
class="math inline">\(\partial\mathbf{T}_n/\partial\boldsymbol{\theta}\)</span>为<span
class="math inline">\(\mathbf{T}_n\)</span>的雅可比矩阵。为了减轻梯度冲突对(6)或(7)中<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更新的影响，一种解决方案是设计非冲突梯度<span
class="math inline">\(\mathbf{L}_{n}\)</span>作为<span
class="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>的替代品。<br/>​  根据[20]，可以通过基于一致性水平逐元素掩膜<span
class="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>来形成非冲突梯度<span
class="math inline">\(\mathbf{L}_{n}\)</span>。具体来说，<br/><span
class="math display">\[\mathbf{L}_n=\mathbf{M}_n\odot\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span><br/>​  其中，<span
class="math inline">\(\odot\)</span>表示元素级乘法。这里的<span
class="math inline">\(\mathbf{M}_n\)</span>是一个与<span
class="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>具有相同维数的二进制矩阵，定义为：<br/><span
class="math display">\[\begin{aligned}\mathbf{M}_{n}&amp;=\mathcal{I}[\mathbf{P}\succcurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\succcurlyeq\mathbf{0}]\\&amp;+\mathcal{I}[\mathbf{P}\preccurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\preccurlyeq\mathbf{0}],\end{aligned}\]</span><br/>​  其中<span
class="math inline">\(\mathcal{I}\)</span>为标准指标函数，适当尺寸的<span
class="math inline">\(\mathbf{U}\)</span>表示一个从均匀分布<span
class="math inline">\(U(0,1)\)</span>中抽样的随机矩阵，<span
class="math inline">\(\geq(\preccurlyeq)\)</span>为元素不等式。此外，<span
class="math inline">\(\mathbf{P}\)</span>测量了给定梯度中包含的正符号的纯度（一致性），其表述为：<br/><span
class="math display">\[\mathbf{P}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{G}_n}{\sum_n|\mathbf{G}_n|}\big),\]</span><br/>​  其中<span
class="math inline">\(\mathbf{G}_{n}=\mathrm{sign}(\mathbf{T}_{n})\odot\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>在批处理维度上合并了梯度贡献，所有的计算包括除值和绝对值操作都是逐元素进行。<br/>​  然而，由（10）计算的<span
class="math inline">\(\mathbf{P}\)</span>依赖于单个反向的特定梯度<span
class="math inline">\(\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>（或输入<span
class="math inline">\(\mathbf{X}\)</span>），限制了其在局部描述梯度一致性的能力。这可能导致不稳定的训练或在某些情况下较差的局部最小值，例如当批中的冲突相互抵消时。因此，我们建议通过动量平均[46]来考虑历史梯度，而不是只涉及当前反向的梯度。这样，<span
class="math inline">\(\mathbf{P}\)</span>就可以全局计算不同场景的一致性，稳定了随机梯度下降（SGD，stochastic
gradient descent）中的训练更新。<br/>​  为了将动量的概念应用于<span
class="math inline">\(\mathbf{P}\)</span>的生成，我们重新定义了第t个反向过程中的纯度为：<br/><span
class="math display">\[\mathbf{P}^{(t)}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{g}_n^{(t)}}{\sum_n|\mathbf{g}_n^{(t)}|}\big),\]</span><br/>​  其中<br/><span
class="math display">\[\begin{aligned}\mathbf{g}_n^{(t)}&amp;
=\mu\cdot\mathbf{g}_{n}^{(t-1)}+(1-\mu)\mathbf{G}_{n}^{(t)}
\\&amp;=\mu^{t-1}\mathbf{g}_{n}^{(1)}+\sum_{i=1}^{t-2}\mu^{i}(1-\mu)\mathbf{G}^{(t-i)}+(1-\mu)\mathbf{G}_{n}^{(t)}\end{aligned}\]</span><br/>​  在之前的t−1个历史反向过程上累积梯度，<span
class="math inline">\(\mathbf{g}_n^{(1)}\)</span>被初始化为<span
class="math inline">\(\mathbf{G}_{n}^{(1)}\)</span>。这里的<span
class="math inline">\(\mu\)</span>是控制最近梯度的权重的衰减因子。在实践中，我们根据经验设置<span
class="math inline">\(\mu=0.95\)</span>。显然，当<span
class="math inline">\(\mu=0\)</span>时，动量<span
class="math inline">\(\mathbf{P^{(t)}}\)</span>退化到原来的<span
class="math inline">\(\mathbf{P}\)</span>。<br/>​  在得到动量纯度<span
class="math inline">\(\mathbf{P^{(t)}}\)</span>后，可以通过用<span
class="math inline">\(\mathbf{P^{(t)}}\)</span>代替<span
class="math inline">\(\mathbf{P}\)</span>来相应地计算出(9)中的掩模<span
class="math inline">\(\mathbf{M}_{n}\)</span>和(8)中的非冲突梯度<span
class="math inline">\(\mathbf{L}_{n}\)</span>。最后，使用<span
class="math inline">\(\mathbf{L}_{n}\)</span>通过以下方式更新<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的参数：<br/><span
class="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\mathbf{L}_n.\]</span></p>
<h3 id="msl训练算法">3) MSL训练算法</h3>
<p>​  我们总结了算法1中的整个MSL训练过程。</p>
<figure>
<img
src="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902220503880.png"
alt="image-20240902220503880" />
<figcaption aria-hidden="true">image-20240902220503880</figcaption>
</figure>
<p>​  更具体地说，前向过程在行5∼7中描述，而其余行专门用于反向过程。在第5行中，我们收集了N个场景中的输入变体，这也可以提前离线进行。然后，STATE
<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取第6行中的摄像机痕迹，并利用分类器<span
class="math inline">\(\mathcal{C}_{\phi_n}\)</span>来计算第7行中的损失。为了减轻损失中的梯度冲突，第8行∼20主要用于产生动量掩模<span
class="math inline">\(\mathbf{M}_n\)</span>，然后在第21行中用于更新<span
class="math inline">\(\theta\)</span>。最终，经过训练的<span
class="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>在第24行产生。<br/>​  备注：一种简单的替代训练策略是在不同的场景下重新标记数据，并通过一个单一的分类器来计算预测。例如，一个包含29个摄像机和4个场景的数据集可以被表示为一个包含29个×4=116个类别的单一分类任务。一个潜在的关键问题是，这种替代方案假定不同场景之间有足够的可变性；否则，某些类别在某种程度上是无法区分的。然而，这个假设并不总是正确的，例如，对于NoTrans，在场景和裁剪场景中，摄像机的痕迹或多或少是相同的。换句话说，一个痕迹实际上可能对应于多个标签，这可能会导致训练过程中的不稳定。在我们提出的MSL策略中，这种困境可以通过N个分类器很自然地避免。<br/>​  我们现在将介绍STATE提取器的架构的细节。</p>
<h2 id="b.-清晰识别痕迹提取器statesmooth-aware-trace-extractor">B.
清晰识别痕迹提取器（STATE，SmooTh-Aware Trace Extractor）</h2>
<p>​  STATE的目的是根据给定图像的局部平滑度进行专注的相机跟踪提取。在这项工作中，我们使用著名的香农熵[47]来表示一个图像块的平滑性。显然，越小的熵值表示越光滑的区域，活动越少，反之亦然。在我们提出的STATE下，我们利用交叉注意[48]层来实现注意提取，其中平滑矩阵被转换为一个注意映射并作为指导。我们想强调的是，我们提出的STATE明确地应用了根据观察II之前的平滑性，因此和交叉注意层的简单采用有显著的不同。<br/>​  STATE的过程如图5所示。<br/><img
src="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902221142472.png"
alt="image-20240902221142472" /></p>
<p>​  具体地说，我们首先将大小为<span class="math inline">\(H\times
W\)</span>的输入图像分割为不重叠的块<span
class="math inline">\(\{\mathbf{R}_{i}\}\)</span>，每一个块行长<span
class="math inline">\(\frac{H}{\hat{H}}\)</span>、列长<span
class="math inline">\(\frac{W}{\hat{W}}\)</span>，总共得到<span
class="math inline">\(\hat{H}\times\hat{W}\)</span>个块。对于第<span
class="math inline">\(i\)</span>个块<span
class="math inline">\(\mathbf{R}_{i}\)</span>，我们计算相关的平滑度指标，即香农熵<span
class="math inline">\(E_i\)</span>：<br/><span
class="math display">\[E_i=-\sum_{v=0}^{255}p_v\mathrm{log}(p_v),\]</span><br/>​  其中<br/><span
class="math display">\[p_{v}=\frac{\hat{H}\hat{W}}{HW}\sum_{h=0}^{H/\hat{H}}\sum_{w=0}^{W/\hat{W}}\mathcal{I}[\mathbf{R}_{i}^{&lt;h,w&gt;}=v]\]</span><br/>​  为<span
class="math inline">\(\mathbf{R}_{i}\)</span>内的像素值为v的概率。然后，通过将<span
class="math inline">\(\{E_i\}\)</span>分组和重塑为维数<span
class="math inline">\(\hat{H}\times\hat{W}\)</span>，可以得到平滑矩阵<span
class="math inline">\(\textbf{S}\in\mathbb{R}^{\hat{H}\times\hat{W}}\)</span>。在实际实现中，我们对输入图像进行灰度处理以降低复杂性，并使用单热编码来进行高效的批处理。<br/>​  得到平滑矩阵<span
class="math inline">\(\textbf{S}\)</span>后，我们首先通过基线提取器提取内部特征<span
class="math inline">\(\textbf{X}^f\)</span>，然后根据<span
class="math inline">\(\textbf{S}\)</span>在<span
class="math inline">\(\textbf{X}^f\)</span>上执行交叉注意力，特别注意的是，<span
class="math inline">\(\textbf{X}^f\)</span>和<span
class="math inline">\(\textbf{S}\)</span>分别通过压扁化得到<span
class="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<span
class="math inline">\(\hat{\mathbf{s}}\)</span>，其大小为<span
class="math inline">\(\hat{H}\hat{W}\times\hat{C}\)</span>，其中<span
class="math inline">\(\hat{C}\)</span>是标记化后的通道数。然后通过计算每个<span
class="math inline">\(\hat{C}/K\)</span>通道的注意力，对展平的<span
class="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<span
class="math inline">\(\hat{\mathbf{s}}\)</span>计算K头注意力，结果是k头特征<span
class="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>，其中：<br/><span
class="math display">\[\hat{\mathbf{X}}_k^a=\text{Attention}(\hat{\mathbf{S}}\mathbf{Q}_k,\hat{\mathbf{X}}^f\mathbf{K}_k,\hat{\mathbf{X}}^f\mathbf{V}_k),k=1,\ldots,K,\]</span><br/>​  <span
class="math inline">\(\mathbf{Q}_k\)</span>、<span
class="math inline">\(\mathbf{K}_k\)</span>、<span
class="math inline">\(\mathbf{V}_k\in\mathbb{R}^{\hat{C}^{2}/K}\)</span>是交叉注意函数[48]的第k个投影的查询、键和值矩阵。这里，交叉注意力的执行是：<br/><span
class="math display">\[\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{SoftMax}(\frac{\mathbf{QK}^{T}}{\sqrt{\hat{C}/K}})\mathbf{V}.\]</span><br/>​  接下来，所有头<span
class="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>的拼接输出用来线性预测，得到展平的注意力<span
class="math inline">\(\mathbf{\hat{X}}^{a}\)</span>：<br/><span
class="math display">\[\mathbf{X}^a=\mathrm{MLP}(\mathrm{Concat}(\hat{\mathbf{X}}_1^a,\hat{\mathbf{X}}_2^a,\ldots,\hat{\mathbf{X}}_K^a)),\]</span><br/>​  其中<span
class="math inline">\(\mathrm{MLP}(\cdot)\)</span>代表一个具有GELU激活[49]的MLP。最后，通过将其重塑为<span
class="math inline">\(\hat{H}\times\hat{W}\times\hat
C\)</span>分辨率，获得细化的注意力特征<span
class="math inline">\(\mathbf{X}^{a}\)</span>。<br/>​  考虑到<span
class="math inline">\(\mathbf{X}^{a}\)</span>中的维数冗余性，我们遵循传统的[6]，[10]，采用全局平均池化和线性映射将<span
class="math inline">\(\mathbf{X}^{a}\)</span>编码为低维空间<span
class="math inline">\(\mathbb{R}^d\)</span>，从而得到最终的相机痕迹<span
class="math inline">\(\mathbf{T}\)</span>。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/">https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">zhaozw后院</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/coverImage/cover5.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/experimet/" title="experimet"><img class="cover" src="/img/coverImage/cover3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">experimet</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">可以用表情包和匿名评论了</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%85%A7%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E8%AF%86%E5%88%AB%E7%9A%84%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%A1%88"><span class="toc-number">3.</span> <span class="toc-text">照相机模型识别的基线方案</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%B2%81%E6%A3%92%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E8%AF%86%E5%88%AB"><span class="toc-number">4.</span> <span class="toc-text">鲁棒相机模型识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#a.-%E5%A4%9A%E5%9C%BA%E6%99%AF%E5%AD%A6%E4%B9%A0mslmulti-scenario-learning"><span class="toc-number">4.1.</span> <span class="toc-text">A.
多场景学习（MSL，Multi-Scenario Learning）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E8%BF%87%E7%A8%8B"><span class="toc-number">4.1.1.</span> <span class="toc-text">1)前向过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E8%BF%87%E7%A8%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">2)反向过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#msl%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.3.</span> <span class="toc-text">3) MSL训练算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#b.-%E6%B8%85%E6%99%B0%E8%AF%86%E5%88%AB%E7%97%95%E8%BF%B9%E6%8F%90%E5%8F%96%E5%99%A8statesmooth-aware-trace-extractor"><span class="toc-number">4.2.</span> <span class="toc-text">B.
清晰识别痕迹提取器（STATE，SmooTh-Aware Trace Extractor）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/" title="Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning"><img src="/img/coverImage/cover5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning"/></a><div class="content"><a class="title" href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/" title="Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning">Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning</a><time datetime="2024-09-02T12:16:56.000Z" title="发表于 2024-09-02 20:16:56">2024-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/experimet/" title="experimet"><img src="/img/coverImage/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="experimet"/></a><div class="content"><a class="title" href="/experimet/" title="experimet">experimet</a><time datetime="2024-09-02T01:42:21.000Z" title="发表于 2024-09-02 09:42:21">2024-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/" title="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods"><img src="/postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods"/></a><div class="content"><a class="title" href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/" title="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods">Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</a><time datetime="2024-08-26T01:26:05.000Z" title="发表于 2024-08-26 09:26:05">2024-08-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/" title="Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization"><img src="/postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization"/></a><div class="content"><a class="title" href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/" title="Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization">Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</a><time datetime="2024-08-24T07:47:02.000Z" title="发表于 2024-08-24 15:47:02">2024-08-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/DH-GAN/" title="DH-GAN"><img src="/postimages/DH-GAN/image-20240824153543322.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DH-GAN"/></a><div class="content"><a class="title" href="/DH-GAN/" title="DH-GAN">DH-GAN</a><time datetime="2024-08-24T07:26:52.000Z" title="发表于 2024-08-24 15:26:52">2024-08-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>