<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mesoscopic Insights:Orchestrating Multi-Scale &amp; Hybrid Architecture for Image Manipulation Localization | 喵</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="发表于AAAI2025，该论文同时构建IML所需的微观与宏观信息介观表征，并提出Mesorch架构，一种融合卷积神经网络和Transformer模型优势的混合模型，通过动态调整尺度权重，能高效捕捉介观层面的伪影特征。">
<meta property="og:type" content="article">
<meta property="og:title" content="Mesoscopic Insights:Orchestrating Multi-Scale &amp; Hybrid Architecture for Image Manipulation Localization">
<meta property="og:url" content="https://zhaozw-szu.github.io/Mesoscopic-Insights/index.html">
<meta property="og:site_name" content="喵">
<meta property="og:description" content="发表于AAAI2025，该论文同时构建IML所需的微观与宏观信息介观表征，并提出Mesorch架构，一种融合卷积神经网络和Transformer模型优势的混合模型，通过动态调整尺度权重，能高效捕捉介观层面的伪影特征。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/postimages/Mesoscopic-Insights/image-20250903224303989.png">
<meta property="article:published_time" content="2025-08-06T12:24:34.000Z">
<meta property="article:modified_time" content="2025-09-10T01:15:52.498Z">
<meta property="article:author" content="Zhaozw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/postimages/Mesoscopic-Insights/image-20250903224303989.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/Mesoscopic-Insights/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mesoscopic Insights:Orchestrating Multi-Scale & Hybrid Architecture for Image Manipulation Localization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-10 09:15:52'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/postimages/Mesoscopic-Insights/image-20250903224303989.png')"><nav id="nav"><span id="blog-info"><a href="/" title="喵"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">喵</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Mesoscopic Insights:Orchestrating Multi-Scale &amp; Hybrid Architecture for Image Manipulation Localization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">创建于</span><time class="post-meta-date-created" datetime="2025-08-06T12:24:34.000Z" title="创建于 2025-08-06 20:24:34">2025-08-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-10T01:15:52.498Z" title="更新于 2025-09-10 09:15:52">2025-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-star fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><span class="post-rank">A类会议,AAAI,2025</span></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/IML/">IML</a></span><span class="post-meta-separator">|</span><a target="_blank" rel="noopener" href="https://github.com/scu-zjz/Mesorch"><img src="https://img.shields.io/github/stars/scu-zjz/Mesorch?style=flat" alt="GitHub Stars: scu-zjz/Mesorch" loading="lazy"></a></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Mesoscopic Insights:Orchestrating Multi-Scale &amp; Hybrid Architecture for Image Manipulation Localization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div id="article-description">发表于AAAI2025，该论文同时构建IML所需的微观与宏观信息介观表征，并提出Mesorch架构，一种融合卷积神经网络和Transformer模型优势的混合模型，通过动态调整尺度权重，能高效捕捉介观层面的伪影特征。<div class="disclaimer">博主观点不代表文章作者观点</div></div><article class="post-content" id="article-container"><p>Mesoscopic Insights: Orchestrating Multi-Scale &amp; Hybrid
Architecture for Image Manipulation Localization</p>
<p>Xuekang Zhu1,2<em>，Xiaochen Ma1,2</em>， Lei Su1,2，Zhuohang
Jiang3，Bo Du1,2，Xiwen Wang1,2，Zeyu Lei1,2，Wentao Feng1,2，Chi-Man
Pun4，Ji-Zhe Zhou1,2†</p>
<p>1计算机学院，四川大学<br/>2机器学习与产业智能工程研究中心，中国教育部<br/>3香港理工大学<br/>4澳门大学科技学院计算机与资讯科学系</p>
<h1 id="摘要">摘要</h1>
<p>​  介观层面作为宏观与微观世界的桥梁，填补了两者长期忽视的空白。图像操控定位（IML）作为从虚假图像中探寻真相的关键技术，长期以来依赖于微观层面的痕迹信息。然而在实际应用中，大多数篡改行为旨在通过改变图像语义来欺骗观众。因此，图像操控通常发生在物体层面（宏观层面），这与微观痕迹同样重要。将这两个层面整合到介观层面，为IML研究提供了全新视角。受此启发，本文探索如何同时构建IML所需的微观与宏观信息介观表征，并提出Mesorch架构来协调二者。具体而言，该架构具有两大核心特点：首先，它将Transformer与CNN进行并行融合——Transformer负责提取宏观信息，CNN则捕捉微观细节；其次，它能在不同尺度间灵活切换，实现微观与宏观信息的无缝衔接。基于介观表示架构，本文还提出了两种基线模型，专门用于解决中观表示任务。通过在四个数据集上的大量实验验证，我们的模型在性能表现、计算复杂度和鲁棒性方面均超越了现有最优方案。</p>
<p>代码—https://github.com/scu-zjz/Mesorch<br/>扩展版本—https://arxiv.org/abs/2412.13753</p>
<h1 id="引言">1.引言</h1>
<p>​  介观系统存在于宏观与微观尺度之间。介观尺度的物体足够大，能够表现出宏观特性，同时又展现出与量子力学相位相关的干涉现象，这与微观系统相似。这种双重性正是它被称为“介观”的原因——<em>Yoseph
lmry</em>.</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250903222321662.png"
alt="image-20250903222321662" />
<figcaption aria-hidden="true">image-20250903222321662</figcaption>
</figure>
<blockquote>
<p>图1：三种篡改类型中的伪影示例。第三列中的红色虚线框表示第四列中放大区域的范围。第四列中的红色箭头指向被认为是篡改痕迹的伪影。</p>
</blockquote>
<p>​  多媒体篡改技术的快速发展使得检测和定位图像篡改更加具有挑战性。伪造逼真的图像变得容易，助长了篡改事件和错误信息，突显出需要有效的取证方法。如图1所示，在图像篡改领域，现有技术主要分为三大类(Pun,
Yuan, and Bi 2015; Bi, Pun, and Yuan 2016;Wu et al. 2019; Verdoliva
2020; Wei et al.
2023)：拼接（将不同图像片段组合生成新图）、复制移动（在同幅图像内复制粘贴区域）以及修复（移除瑕疵后用合理内容填补）。尽管复杂的手工篡改可能逃过人眼察觉，但每种操作在微观层面仍会留下可辨识痕迹。因此，当前图像篡改定位（IML）技术普遍将微观特征提取视为关键任务——通过捕捉图像RGB噪声(Zhou
et al. 2018; Bayar and Stamm 2018)、边缘信号(Zhou et al. 2020; Chen et
al. 2021)或高频特征(Li and Huang 2019; Wang et al.
2022a)等微观特征来识别篡改痕迹。这些微观特征在揭示瑕疵和定位篡改区域方面通常效果显著。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250903222417325.png"
alt="image-20250903222417325" />
<figcaption aria-hidden="true">image-20250903222417325</figcaption>
</figure>
<blockquote>
<p>图2：CASIAv2数据集的随机样本。红色线条标出了篡改区域的清晰边界。第一列显示与物体完全无关的篡改行为，其余四列则展示了与物体相关的篡改行为。</p>
</blockquote>
<p>​  然而，如图2所示，大多数篡改行为通常通过改变或模糊图像语义来欺骗观众。例如，在CASIAv2数据集中，约80%的样本涉及物体操控。此外，物体是否被篡改的可能性会根据其对整体图像语义的贡献度而变化——比如前景中的人类和动物比背景中的树木山峦更容易被篡改。因此我们认为，理解物体层面（宏观层面）的语义特征对于识别篡改区域和划定可疑范围至关重要。不过仅凭宏观层面的语义特征还不足以生成篡改掩膜，因为缺乏必要的细节来检测复杂的伪影。<br/>​  因此，探索将微观与宏观信息整合至介观层面的路径，可能成为提升中介模型（IML）研究的新突破口。DiffForensics(Yu
et
al.2024)指出，伪影存在于介观层面——即介于微观与宏观之间的中间层级，兼具两者的特征。但该研究未深入探讨如何从技术层面刻画这一层级。基于我们前期的深度分析，我们提出通过同步捕捉微观特征与提取宏观语义来定义介观层面的伪影捕获，这为中介模型任务提供了新范式。基于此范式，我们创新性地提出了Mesorch（介观编排，<strong>Meso</strong>scopic-<strong>Orch</strong>estration）架构，该架构采用专门设计的并行编码器和解码器结构来表征介观层面，从而实现两个层级间的高效协调，更精准地捕捉介观层面的伪影特征。<br/>​  在编码器阶段，当前研究(Huang
et al. 2016;Zhang et al. 2018; Yuan et al.
2021)表明，卷积神经网络（CNN）和Transformer模型分别擅长处理微观特征与宏观语义。然而，现有大多数图像微解构方法(Wu
et al. 2019; Hu et al. 2020;Guillaro et al.
2023)仍完全依赖单一架构进行决策。尽管部分模型如ObjectFormer(Wang et al.
2022a)，已认识到CNN与Transformer结合的优势，但其设计仍采用线性连接的顺序架构。这种顺序架构常导致单一模型占据主导地位——由于大部分计算资源或参数集中在该模型上，反而弱化了另一模型的优势。因此，其性能无法超越单架构模型，也未能充分融合微观与宏观层面的优势。为解决这一局限，我们采用并行架构同时运用CNN和Transformer。这种设计有效整合了两种方法的优势，特别针对介观层面的伪影捕捉问题。<br/>​  在此基础上，浅层特征图提供微观特征，而深层特征图则呈现宏观语义。因此，在解码阶段采用多尺度方法同步解码不同尺度的特征图，有助于模型更精准捕捉介观层面的伪影特征。类似地，诸如MVSS-Net(Chen
et al. 2021)和Trufor(Guillaro et al.
2023)等现有模型，在此阶段也采用了多尺度解码方法。然而，这些模型假设所有尺度的权重相等且未进行显式调整，可能导致忽略尺度间的差异。这既可能造成关键特征利用率不足，也可能过度强调次要特征，从而影响整体性能。为有效解决这一问题，我们引入自适应权重模块，动态调整各尺度的重要性。此外，通过剪枝非关键尺度，模型在保持性能的同时显著降低计算成本和参数量。<br/>​  综上所述，我们提出了一种名为Mesorch的混合模型架构，该架构融合了卷积神经网络（CNN）和Transformer模型，通过动态调整尺度权重来高效表征介观层级。通过剪枝低重要性尺度，我们有效降低了参数量和计算成本，最终构建出两个基准模型。在四个数据集上的测试表明，该模型在F1分数、鲁棒性和运算量（FLOPs）方面均达到业界领先水平。<br/>​  我们的贡献有三个方面；</p>
<ul>
<li>我们提出Mesorch架构，这是一种融合卷积神经网络与Transformer模型优势的混合架构。该架构采用多尺度协同机制，有效整合微观与宏观层面的信息，从而精准捕捉图像医学学习任务中的介观特征。</li>
<li>我们提出了一种自适应加权模块，能够动态调整每个尺度的重要性。此外，通过选择性地剪枝影响较小的尺度，我们的方法在保持鲁棒性和性能的同时，显著降低了计算成本和参数量。</li>
<li>我们基于Mesorch架构开发了两个基准模型。在基准数据集上的全面实验表明，我们的方法在F1分数、鲁棒性和浮点运算量方面达到了业界领先水平。</li>
</ul>
<h1 id="相关工作">2.相关工作</h1>
<h1 id="篡改定位的体系结构">2.1.篡改定位的体系结构</h1>
<p>​  <strong>基于CNN的模型</strong><br/>​  在图像篡改检测领域，基于卷积神经网络（CNN）的模型凭借其强大的特征提取能力长期占据主流地位，尤其擅长捕捉表明图像被篡改的局部纹理异常。诸如ManTra-Net
(Wu et al.2019)和SPAN (Hu et al. 2020)等代表性方案，通过采用VGG
(Simonyan and Zisserman 2015)和ResNet-50 (He et al.
2016)等架构，有效捕捉局部纹理异常特征。近年来，研究者们开始整合对比学习技术，例如NCL
(Zhou et al.2023)，进一步提升了检测系统的定位能力。</p>
<p>​  <strong>基于Transformer的模型</strong><br/>​  基于对更广泛上下文理解的需求，Transformer架构的IML-ViT
(Ma et al. 2023)率先将该架构引入IML领域，而TruFor (Guillaro et al.
2023)则采用了SegFormer (Xie et al.
2021)。这些模型擅长综合多维度上下文信息，通过动态聚焦潜在操作区域来提升定位精度，标志着该领域的重大突破。类似地，MGQFormer
(Zeng et al.
2024)采用基于查询的Transformer架构来精确定位操作区域。通过聚焦上下文相关特征，该模型显著提升了操作定位精度，充分展现了Transformer技术在提供稳健、上下文感知解决方案方面的强大优势，超越了以往方法。</p>
<p>​  <strong>基于混合CNN-Transformer模型</strong><br/>​  除了上述模型之外，像ObjectFormer
(Wang et al.
2022a)这类将卷积神经网络与transformer进行序列式结合的混合方法也备受关注。ObjectFormer采用EfficientNet架构作为编码器的初始部分，将输入数据下采样为特定特征块，随后这些特征块依次输入到双流视觉transformer中。</p>
<h2 id="多尺度应用">2.2.多尺度应用</h2>
<p>​  在图像处理与定位领域，多尺度技术已成为提升不同分辨率下特征分析效果的重要手段。以MVSSNet
(Chen et al.
2021)为例，该模型采用多视角、多尺度的监督策略来检测图像篡改行为。通过整合局部边缘信息与全局上下文特征，MVSSNet运用多尺度特征学习技术精准识别篡改区域，展现出在各类数据集和场景中高度泛化的解决方案。类似地，TruFor
(Guillaro et al. 2023)基于SegFormer (Xie et al.
2021)主干网络，有效整合了整幅图像的空间关联性。其多尺度方法通过结合细粒度局部特征与广义上下文信息，显著提升了篡改异常的定位精度，确保了操作痕迹的精准识别与鲁棒性。</p>
<h1 id="方法">3.方法</h1>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250903224303989.png"
alt="image-20250903224303989" />
<figcaption aria-hidden="true">image-20250903224303989</figcaption>
</figure>
<blockquote>
<p>图3：Mesorch框架示意图：输入的RGB图像首先在DCT模块中进行高频与低频处理，分别生成对应的高频和低频表征。局部特征模块通过结合原始图像与高频图像，专注于检测细粒度操作；全局特征模块则利用原始图像与低频图像捕捉物体级别的篡改线索。自适应加权模块通过为局部特征和全局特征分配像素级权重，动态整合这些图像信息。最终生成的组合特征用于预测任务，并与真实标签对比计算损失值。</p>
</blockquote>
<p>​  在本节中，我们介绍了如图3所示的Mesorch框架。该流程始于RGB图像，通过离散余弦变换（DCT）提取高频与低频特征(Gonzalez
and Woods
2018)。随后将这些特征与原始图像融合，生成高频增强图像和低频增强图像。高频增强图像被传递至局部特征模块，而低频增强图像则进入全局特征模块。两个模块分别输出四个不同尺度的特征图，对应的解码器处理这些特征图以生成操作区域的初始预测结果。通过加权融合这些多尺度预测结果，最终生成综合预测。为提高效率，在模型初步收敛后采用剪枝方法，通过剔除次要特征尺度来优化参数数量和运算量。</p>
<h2 id="整合局部和全局信息以增强预测">3.1
整合局部和全局信息以增强预测</h2>
<p>​  隐藏在介观层面的特征可能在RGB域中并不显著，但可以通过频域信息进行放大(Wang
et al.
2022a)。因此，我们从频域中提取高频和低频特征，以增强局部和全局特征编码器模块的能力。</p>
<h3 id="采用离散余弦变换增强功能">3.1.1 采用离散余弦变换增强功能</h3>
<p>​  最初，如图3所示，对尺寸为H×W×3的RGB图像x进行离散余弦变换（DCT）处理，将其分解为高频分量<span
class="math inline">\({\bf x}_{h}\)</span>和低频分量<span
class="math inline">\({\bf
x}_{l}\)</span>。这些分量保留了H×W×3的维度后，与原始图像x结合，形成增强表示以供后续处理：
<span class="math display">\[{\bf I}_{h}=\{\bf x,{\bf x}_{h}\},~~~{\bf
I}_{h}\in\mathbb{R}^{H\times W\times6}\]</span></p>
<p><span class="math display">\[{\bf I}_{l}=\{\bf x,{\bf
x}_{l}\},~~~{\bf I}_{l}\in\mathbb{R}^{H\times W\times6}\]</span></p>
<h3 id="特征编码和尺度解码">3.1.2 特征编码和尺度解码</h3>
<p>​  在进一步增强高频和低频特征后，高频增强图像<span
class="math inline">\({\bf
I}_{h}\)</span>由局部特征编码器处理，而低频增强图像<span
class="math inline">\({\bf
I}_{l}\)</span>由全局特征编码器处理。每个编码器随后输出四个不同尺度的特征：
<span
class="math display">\[\begin{matrix}\{L_{s_{1}},L_{s_{2}},L_{s_{3}},L_{s_{4}}\}=\mathrm{LocalFeatureEncoder}({\bf
I}_{h}),\\L_{s_{i}}\in\mathbb{R}^{\frac{H}{2^{(i+1)}}\times\frac{W}{2^{(i+1)}}\times{C}_{i_{local}}}\end{matrix}\]</span></p>
<p><span
class="math display">\[\begin{matrix}\{G_{s_{1}},G_{s_{2}},G_{s_{3}},G_{s_{4}}\}=\mathrm{GlobalFeatureEncoder}({\bf
I}_{l}),\\G_{s_{i}}\in\mathbb{R}^{\frac{H}{2^{(i+1)}}\times\frac{W}{2^{(i+1)}}\times{C}_{i_{global}}}\end{matrix}\]</span></p>
<p>​  这里，<span class="math inline">\({C}_{i_{local}}\)</span>和<span
class="math inline">\({C}_{i_{global}}\)</span>分别表示局部编码器和全局编码器在每个尺度i上的输出通道总数。<br/>​  由局部和全局特征编码器在尺度i
=
1、2、3、4生成的特征图随后通过解码器处理，解码器为每个尺度i输出形状为<span
class="math inline">\(\frac{H}{4}\times\frac{W}{4}\times1\)</span>的预测掩膜：
<span
class="math display">\[P_{l_{i}}=\mathrm{LocalFeatureDecoder}(L_{s_{i}})\]</span></p>
<p><span
class="math display">\[P_{g_{i}}={\mathrm{GlobalFeatureDecoder}}(G_{s_{i}})\]</span></p>
<p>​  将局部预测和全局预测相结合，生成形状为<span
class="math inline">\(\frac{H}{4}\times\frac{W}{4}\times1\)</span>的总和最终预测掩膜。然后将该掩膜调整为图像的原始尺寸（H×W），生成最终预测掩膜<span
class="math inline">\(P_{\mathrm{final}}\)</span>： <span
class="math display">\[P_{\mathrm{final}}=\mathrm{Resize}\left(\sum_{i=1}^{4}(P_{l_{i}}+P_{g_{i}}),H,W\right)\]</span>
​  模型性能的评估通过计算<span
class="math inline">\(P_{\mathrm{final}}\)</span>与真实掩模之间的交叉熵损失来实现，其中真实掩模用于标识图像中实际被篡改的区域。该真实掩模作为二值化图层突出显示被篡改区域，引导模型聚焦于预测操作与实际操作之间的差异：
<span
class="math display">\[\mathrm{Loss}=\mathrm{CrossEntropyLoss}(P_{\mathrm{final}},\mathrm{Mask})\]</span></p>
<h2 id="自适应尺度加权和模型剪枝">3.2 自适应尺度加权和模型剪枝</h2>
<p>​  为解决跨尺度权重均衡可能导致特征利用效率低下的问题，同时降低混合模型的参数数量，我们引入自适应加权模块和模型剪枝方法，从而提升预测准确性和计算效率。</p>
<h3 id="规模重要性">3.2.1 规模重要性</h3>
<p>​  权重网络接收原始RGB图像x、其高频分量和低频分量的拼接输入，最终得到尺寸为<span
class="math inline">\(\mathbb{R}^{H\times
W\times9}\)</span>的输入。随后网络生成归一化权重向量W，其中每个元素反映了各尺度预测在每个像素上的重要性：
<span
class="math display">\[\begin{matrix}W=\mathrm{WeightingModule}(\{\bf
x,{\bf x}_{h},{\bf
x}_{l}\}),\\W=\mathbb{R}^{\frac{H}{4}\times\frac{W}{4}\times8}\end{matrix}\]</span></p>
<h3 id="像素级预测融合">3.2.2 像素级预测融合</h3>
<p>​  最终的操控区域预测结果通过加权汇总各尺度预测掩膜得出。首先，将各尺度的局部和全局预测掩膜合并形成综合掩膜Pall，其表达式为<span
class="math inline">\(P_{\mathrm{all}}\in\mathbb{R}^{\frac{H}{4}\times\frac{W}{4}\times8}\)</span>。随后进行加权求和计算：
<span
class="math display">\[P_{\mathrm{imal}}=\mathrm{Resize}(\sum_{j=1}^{8}W_{j}\cdot
P_{a l l_{j}},H,W)\]</span> ​  这里，<span
class="math inline">\(P_{\mathrm{imal}}\)</span>是最终预测掩码，调整为原始图像尺寸（H×W）。与之前的预测一样，该掩码与真实掩码进行比较以计算交叉熵损失。</p>
<h3 id="二次修剪的依据">3.2.3 二次修剪的依据</h3>
<p>​  虽然初始训练阶段能让模型收敛并识别不同尺度下潜在的有用特征，但后续分析常会发现某些尺度可能包含冗余甚至噪声信息，这可能会削弱模型的整体效能。因此，有必要通过以下标准来评估每个尺度的贡献度：<br/>​  首先，通过计算该尺度i中所有像素或特征单元n的权重<span
class="math inline">\(W_{i,n}\)</span>的平均值，得到每个尺度i的平均权重<span
class="math inline">\(\bar{W}_{i}\)</span>： <span
class="math display">\[\bar{W}_{i}=\frac{1}{N}\sum_{n=1}^{N}W_{i,n}\]</span>
​  接下来，将修剪条件定义如下： <span
class="math display">\[\mathrm{Pnure\,Condition}\colon\bar{W}_{i}\lt
\epsilon\]</span> ​  在此，<span
class="math inline">\(\bar{W}_{i}\)</span>表示第i个尺度的平均权重，该权重是基于该尺度内所有N个像素单元计算得出的。如果这个平均权重低于预设阈值<span
class="math inline">\(\epsilon\)</span>，则认为对应的尺度对模型贡献微乎其微，随后会被剪枝。这种方法确保模型专注于提供有意义信息的尺度，同时剔除那些对整体性能贡献甚微的尺度。</p>
<h1 id="实验">4. 实验</h1>
<h2 id="实验设置">4.1 实验设置</h2>
<p>​  <strong>训练</strong><br/>​  我们的模型采用标准化的Protocol-CAT数据集进行训练，该数据集由代码库(Ma
et al.
2024)提供。该协议包含已建立的数据集和典型的数据增强方法。所有图像均被调整为512x512像素尺寸。我们在四块英伟达4090显卡上进行了150个训练周期的实验，采用12的批量大小。学习率遵循余弦曲线调度方案(Loshchilov
and Hutter
2017)，初始值为1e-4并逐步衰减至最低5e-7，同时设置了2个训练周期的预热期以平稳调整学习率。为防止过拟合，我们使用AdamW优化器并设置权重衰减系数0.05。此外，将累积迭代次数设为2次，通过动态调整批量大小来增强模型对多样化数据输入的泛化能力。</p>
<p>​  <strong>测试</strong><br/>​  我们采用国际公认的基准测试IMDLBenCo(Ma
et al. 2024)对模型进行评估，测试集包括四个主流数据集：CASIAv1 (Dong,
Wang and Tan 2013)、Coverage (Wen et al. 2016)、NIST16 (Guan et al.
2019)以及Columbia (Hsu and Chang
2006)。这些数据集因其多样化的挑战性而广受认可，是检验图像处理定位方法泛化能力的重要评估平台。</p>
<p>​  <strong>量度</strong><br/>​  与测试环节保持一致，我们沿用了业界公认的评估基准，采用标准像素级F1分数来衡量定位性能。所有结果均在0.5的默认阈值下计算得出，能够全面反映定位精度的优劣。</p>
<h2 id="与最新方法比较">4.2 与最新方法比较</h2>
<p>​  为确保评估的准确性，我们使用开源代码在各论文推荐的分辨率下训练模型，并采用CAT-Net(Kwon
et
al.2022)协议数据集。随后通过F1分数对模型性能进行跨多个权威数据集的基准测试。对比分析涵盖多种方法，包括PSCC-Net(Liu
et al. 2022a)、MVSS-Net(Chen et al.2021),、CAT-Net(Kwon et al.
2022)以及Trufor(Guillaroet al. 2023)。</p>
<p>​  <strong>定位结果</strong><br/>​  在表1中，我们用加粗字体标注了表现最佳的模型，并用下划线标注了次优模型，这些结果均基于所有评估数据集得出。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250903231407275.png"
alt="image-20250903231407275" />
<figcaption aria-hidden="true">image-20250903231407275</figcaption>
</figure>
<p>​  无论是剪枝前还是剪枝后的表现，我们提出的方法始终稳居前列，要么拔得头筹，要么位列第二。这种稳定的表现力充分证明了我们的方法在处理各类图像操作定位任务时具有精准性。此外，图4从定性角度展示了模型成功捕捉到物体布局和介观层面细节的能力，最终生成的被篡改掩膜具有极高的精确度。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250806203849712.png"
alt="image-20250806203849712" />
<figcaption aria-hidden="true">image-20250806203849712</figcaption>
</figure>
<p>​  <strong>鲁棒性能</strong><br/>​  为评估模型在不同条件下的鲁棒性，我们在CASIAv1数据集上进行了测试，并将结果汇总于表2。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250806210218523.png"
alt="image-20250806210218523" />
<figcaption aria-hidden="true">image-20250806210218523</figcaption>
</figure>
<p>​  我们引入了标准差各异的高斯噪声、不同核尺寸的高斯模糊处理，以及采用多种质量因子的JPEG压缩作为扰动手段。实验结果表明，我们的模型在这三种扰动方式下均保持了业界领先的鲁棒性表现。值得注意的是，即使经过剪枝处理，我们的方法仍比所有先前的模型保持了更高的鲁棒性，这证明了它在处理各种图像失真方面的有效性。</p>
<p>​  <strong>浮点运算和参数</strong><br/>​  所有测量的参数数量和浮点运算次数（FLOPs）均基于512x512分辨率和1批次大小进行计算。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250806210350812.png"
alt="image-20250806210350812" />
<figcaption aria-hidden="true">image-20250806210350812</figcaption>
</figure>
<p>​  如表3所示，我们的实验结果表明：该模型的FLOPs消耗量低于所有当前最先进的模型，其参数数量仅次于PSCC-Net。此外，通过应用我们的剪枝方法进一步降低了FLOPs和总参数数量，使得本模型相比现有顶尖模型具有显著优势。</p>
<h2 id="消融研究">4.3 消融研究</h2>
<p>​  <strong>对拟定方法的独立评价</strong><br/>​  为验证本文提出方法的有效性，我们首先独立评估了CNN的ConvNeXt架构和Transformer的Segformer架构作为基线模型，并在多尺度条件下对其性能进行测试。在确立基线性能后，我们进一步评估了混合模型，逐步整合多尺度方法、加权算法和DCT特征提取技术。同时对剪枝模型进行了测试（如表4所示）。实验结果表明，本文提出的每个组件对于精准定位图像篡改都具有关键作用。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250806210555288.png"
alt="image-20250806210555288" />
<figcaption aria-hidden="true">image-20250806210555288</figcaption>
</figure>
<p>​  <strong>模型体系结构的比较分析</strong><br/>​  我们通过两种CNN模型(Resnet-50
(He et al. 2016) and ConvNeXt-Tiny (Liu et al.
2022b))与四种Transformer模型(MAE-Base (He et al. 2022), PvT-B3 (Wang et
al. 2022b), Segformer-B3 (Xie et al. 2021),以及SwinTransformer-Base (Liu
et al.
2021))的不同组合，对Mesorch架构的性能进行了评估。各模型间的性能差异总结于第5部分。图5则通过定性分析展示了该架构在不同骨干网络下的表现。</p>
<figure>
<img src="../postimages/Mesoscopic-Insights/image-20250806210831745.png"
alt="image-20250806210831745" />
<figcaption aria-hidden="true">image-20250806210831745</figcaption>
</figure>
<p>​  研究结果表明，ConvNeXt与Segformer的组合在宏观定位和微观特征捕捉方面均表现出色，其综合性能超越了其他所有模型组合。</p>
<h1 id="结论">5. 结论</h1>
<p>​  受介观视角启发，本文重新定义了IML任务，旨在协调微观与宏观层面的研究。在此基础上，我们提出Mesorch架构，一种融合卷积神经网络和Transformer模型优势的混合模型，通过动态调整尺度权重，能高效捕捉介观层面的伪影特征。为降低参数量和计算成本，我们还基于该架构引入了两个基准模型。在大规模数据集上的广泛测试表明，我们的方法在F1分数、鲁棒性和运算量等指标上始终保持着业界领先水平。</p>
<h1 id="代码">6. 代码</h1>
<p>​  模型的参数量和FLOPS：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| module                         | #parameters or shape   | #flops     |</span><br><span class="line">|:-------------------------------|:-----------------------|:-----------|</span><br><span class="line">| model                          | 85.754M                | 0.122T     |</span><br></pre></td></tr></table></figure>
<p>​  按照其主要的成分：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| module                         | #parameters or shape   | #flops     |</span><br><span class="line">|:-------------------------------|:-----------------------|:-----------|</span><br><span class="line">| model                          | 85.754M                | 0.122T     |</span><br><span class="line">|  convnext                      |  28.594M               |  23.427G   |</span><br><span class="line">|  segformer                     |  44.081M               |  37.159G   |</span><br><span class="line">|  upsample                      |  12.29M                |  10.872G   |</span><br><span class="line">|  inverse                       |  0.648K                |  10.486M   |</span><br><span class="line">|  gate                          |  0.788M                |  47.916G   |</span><br><span class="line">|  low_dct                       |                        |  1.074G    |</span><br><span class="line">|  high_dct                      |                        |  1.074G    |</span><br><span class="line">|  resize                        |                        |  1.049M    |</span><br></pre></td></tr></table></figure>
<p>​  其中convnext、segformer为其特征提取器，gate为一个映射</p>
<h2 id="定义">6.1 定义</h2>
<p>​  不含可训练参数的高频和低频信息提取方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.low_dct = LowDctFrequencyExtractor()</span><br><span class="line">self.high_dct = HighDctFrequencyExtractor()</span><br></pre></td></tr></table></figure>
<p>​  局部特征提取器convnext（模型为convnext_tiny）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.convnext = ConvNeXt(conv_pretrain)</span><br></pre></td></tr></table></figure>
<p>​  全局特征提取器segformer：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.segformer = MixVisionTransformer(seg_pretrain_path)</span><br></pre></td></tr></table></figure>
<p>​  自适应权重模块gate：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        self.gate = ScoreNetwork()</span><br><span class="line"></span><br><span class="line">class ScoreNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(ScoreNetwork, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(9, 192, kernel_size=7, stride=2, padding=3)</span><br><span class="line">        self.invert = nn.Sequential(LayerNorm2d(192),</span><br><span class="line">                                    nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),</span><br><span class="line">                                    nn.Conv2d(192, 768, kernel_size=1),</span><br><span class="line">                                    nn.Conv2d(768, 192, kernel_size=1),</span><br><span class="line">                                    nn.GELU())</span><br><span class="line">        self.conv2 = nn.Conv2d(192, 8,  kernel_size=7, stride=2, padding=3)</span><br><span class="line">        self.softmax = nn.Softmax(dim=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        short_cut = x</span><br><span class="line">        x = self.invert(x)</span><br><span class="line">        x = short_cut + x</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.float()</span><br><span class="line">        x = self.softmax(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">| module                         | #parameters or shape   | #flops     |</span><br><span class="line">|:-------------------------------|:-----------------------|:-----------|</span><br><span class="line">|  gate                          |  0.788M                |  47.916G   |</span><br><span class="line">|   gate.conv1                   |   84.864K              |   5.549G   |</span><br><span class="line">|    gate.conv1.weight           |    (192, 9, 7, 7)      |            |</span><br><span class="line">|    gate.conv1.bias             |    (192,)              |            |</span><br><span class="line">|   gate.invert                  |   0.628M               |   41.134G  |</span><br><span class="line">|    gate.invert.0               |    0.384K              |    62.915M |</span><br><span class="line">|    gate.invert.1               |    0.332M              |    21.743G |</span><br><span class="line">|    gate.invert.2               |    0.148M              |    9.664G  |</span><br><span class="line">|    gate.invert.3               |    0.148M              |    9.664G  |</span><br><span class="line">|   gate.conv2                   |   75.272K              |   1.233G   |</span><br><span class="line">|    gate.conv2.weight           |    (8, 192, 7, 7)      |            |</span><br><span class="line">|    gate.conv2.bias             |    (8,)                |            |</span><br></pre></td></tr></table></figure>
<p>​  上采样网络，输入为八个特征：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class UpsampleConcatConv(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(UpsampleConcatConv, self).__init__()</span><br><span class="line">        self.upsamplec2 = nn.ConvTranspose2d(192, 96, kernel_size=4, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">        self.upsamples2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">        self.upsamplec3 = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(384, 192, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(192, 96, kernel_size=4, stride=2, padding=1)</span><br><span class="line">        )</span><br><span class="line">        self.upsamples3 = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(320, 128, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.upsamplec4 = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(768, 384, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(384, 192, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(192, 96, kernel_size=4, stride=2, padding=1)</span><br><span class="line">        )</span><br><span class="line">        self.upsamples4 = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(512, 320, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(320, 128, kernel_size=4, stride=2, padding=1),</span><br><span class="line">            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        # 上采样</span><br><span class="line">        c1,c2,c3,c4,s1,s2,s3,s4 = inputs</span><br><span class="line"></span><br><span class="line">        c2 = self.upsamplec2(c2)</span><br><span class="line">        c3 = self.upsamplec3(c3)</span><br><span class="line">        c4 = self.upsamplec4(c4)</span><br><span class="line">        s2 = self.upsamples2(s2)</span><br><span class="line">        s3 = self.upsamples3(s3)</span><br><span class="line">        s4 = self.upsamples4(s4)</span><br><span class="line">        </span><br><span class="line">        x = torch.cat([c1,c2,c3,c4,s1,s2,s3,s4 ], dim=1)</span><br><span class="line">        features = [c1,c2,c3,c4,s1,s2,s3,s4]</span><br><span class="line">        return x, features</span><br></pre></td></tr></table></figure>
<p>​  将通道数降低为1的网络，前4个卷积层将输入通道数从96减少到1，而后4个卷积层则将输入通道数从64减少到1：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.inverse = nn.ModuleList([nn.Conv2d(96, 1, 1) for _ in range(4)]+[nn.Conv2d(64, 1, 1) for _ in range(4)])</span><br></pre></td></tr></table></figure>
<h2 id="流程">6.2 流程</h2>
<p>​  输入的图片大小是B*3*512*512</p>
<p>​  计算高频和低频图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">high_freq = self.high_dct(image)</span><br><span class="line">low_freq = self.low_dct(image)</span><br><span class="line">input_high = torch.concat([image,high_freq],dim=1)</span><br><span class="line">input_low = torch.concat([image,low_freq],dim=1)</span><br><span class="line">input_all = torch.concat([image,high_freq,low_freq],dim=1)</span><br></pre></td></tr></table></figure>
<p>​  其中high_freq、low_freq大小是B*3*512*512；input_high、input_low大小是B*6*512*512；input_all大小是B*9*512*512；</p>
<p>​  分别提取对应的特征：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_,outs1 = self.convnext(input_high)</span><br><span class="line">_,outs2 = self.segformer(input_low)</span><br><span class="line">inputs = outs1 + outs2</span><br></pre></td></tr></table></figure>
<p>​  其中outs1、outs2都由四个特征构成，inputs由八个特征构成。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for out in outs1:</span><br><span class="line">    print(out.shape)</span><br><span class="line">[16:38:42.188030] torch.Size([8, 96, 128, 128])</span><br><span class="line">[16:38:42.188667] torch.Size([8, 192, 64, 64])</span><br><span class="line">[16:38:42.188887] torch.Size([8, 384, 32, 32])</span><br><span class="line">[16:38:42.189143] torch.Size([8, 768, 16, 16])</span><br><span class="line"></span><br><span class="line">for out in outs2:</span><br><span class="line">    print(out.shape)</span><br><span class="line">[16:39:18.365700] torch.Size([8, 64, 128, 128])</span><br><span class="line">[16:39:18.366133] torch.Size([8, 128, 64, 64])</span><br><span class="line">[16:39:18.366433] torch.Size([8, 320, 32, 32])</span><br><span class="line">[16:39:18.366609] torch.Size([8, 512, 16, 16])</span><br></pre></td></tr></table></figure>
<p>​  通过上采样网络：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x, features = self.upsample(inputs)</span><br></pre></td></tr></table></figure>
<p>​  其中，x是features的concat，大小是B*640*128*128</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([8, 96, 128, 128])     ---------&gt;    torch.Size([8, 96, 128, 128])  不变</span><br><span class="line">torch.Size([8, 192, 64, 64])      ---------&gt;    torch.Size([8, 96, 128, 128])  上采样1次</span><br><span class="line">torch.Size([8, 384, 32, 32])      ---------&gt;    torch.Size([8, 96, 128, 128])  上采样2次</span><br><span class="line">torch.Size([8, 768, 16, 16])      ---------&gt;    torch.Size([8, 96, 128, 128])  上采样3次</span><br><span class="line"></span><br><span class="line">torch.Size([8, 64, 128, 128])     ---------&gt;    torch.Size([8, 64, 128, 128])  不变</span><br><span class="line">torch.Size([8, 128, 64, 64])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样1次</span><br><span class="line">torch.Size([8, 320, 32, 32])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样2次</span><br><span class="line">torch.Size([8, 512, 16, 16])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样3次</span><br></pre></td></tr></table></figure>
<p>​  构建自适应多尺度聚合模块：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gate_outputs = self.gate(input_all)</span><br></pre></td></tr></table></figure>
<p>​  其中，gate_outputs的大小为B*8*128*128，为这8个特征的分数</p>
<p>​  自适应加权：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">pred_mask = torch.sum(gate_outputs * reduced, dim=1,keepdim=True)</span><br></pre></td></tr></table></figure>
<p>​  将各个通道数降低为1之后concat，得到reduced，大小是B*8*128*128，<br/>​  然后乘以特征分数，再求和得到最后的预测图pred_mask，大小是B*1*128*128，</p>
<p>​  得到对应的损失和预测：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pred_mask = self.resize(pred_mask)</span><br><span class="line">loss = self.loss_fn(pred_mask,mask)</span><br><span class="line">pred_mask = pred_mask.float()</span><br><span class="line">mask_pred = torch.sigmoid(pred_mask)</span><br></pre></td></tr></table></figure>
<p>​  使用的resize和损失方法为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.resize = nn.Upsample(size=(image_size, image_size), mode=&#x27;bilinear&#x27;, align_corners=True)</span><br><span class="line">self.loss_fn = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>
<h2 id="实验-1">6.3 实验</h2>
<p>​  首先是实验结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.975063915784834, &quot;IOU&quot;: 0.9610243780272704, &quot;AUC&quot;: 0.9937163561582565</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8467736605629914, &quot;IOU&quot;: 0.7918845663047374, &quot;AUC&quot;: 0.9872424296710802</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.6361199331736328, &quot;IOU&quot;: 0.5644224136507074, &quot;AUC&quot;: 0.9419156461954117</span><br></pre></td></tr></table></figure>
<p>​  如果我使用全0图代替输入的高频图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">high_freq = torch.zeros_like(high_freq, device=high_freq.device)</span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9749351575698441, &quot;IOU&quot;: 0.9604402530589924, &quot;AUC&quot;: 0.9940967261791229</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.842573461082324, &quot;IOU&quot;: 0.7872516940489385, &quot;AUC&quot;: 0.9870725175608759</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.6299422642888041, &quot;IOU&quot;: 0.5601868849901104, &quot;AUC&quot;: 0.9399141371250153</span><br></pre></td></tr></table></figure>
<p>​  如果我使用全0图代替输入的低频图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">low_freq = torch.zeros_like(low_freq, device=low_freq.device)</span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9562238987200731, &quot;IOU&quot;: 0.9350766544966727, &quot;AUC&quot;: 0.9875582307577133</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.7672053679245786, &quot;IOU&quot;: 0.7023817675984425, &quot;AUC&quot;: 0.9660183445267055</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.4984013343756983, &quot;IOU&quot;: 0.4087074449978039, &quot;AUC&quot;: 0.8918066124121348</span><br></pre></td></tr></table></figure>
<p>​  如果我使用全0图代替输入的高频图和低频图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">high_freq = torch.zeros_like(high_freq, device=high_freq.device)</span><br><span class="line">low_freq = torch.zeros_like(low_freq, device=low_freq.device)</span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.7122947562652161, &quot;IOU&quot;: 0.6040474035694013, &quot;AUC&quot;: 0.8484685838222503</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.40061715458990993, &quot;IOU&quot;: 0.2999635728291686, &quot;AUC&quot;: 0.8234983024389847</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.3969410685703992, &quot;IOU&quot;: 0.28453696076957763, &quot;AUC&quot;: 0.7860066642363867</span><br></pre></td></tr></table></figure>
<p>​  如果我不使用由convnext提取的局部特征分数，只取后四个由segformer提取的全局特征：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">        middle = gate_outputs * reduced</span><br><span class="line">        middle =  middle[:, 4:, :, :]</span><br><span class="line">        pred_mask = torch.sum(middle, dim=1, keepdim=True)</span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9751713696549196, &quot;IOU&quot;: 0.9615665894299884, &quot;AUC&quot;: 0.9937943428754806</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8472495981799928, &quot;IOU&quot;: 0.7910594238105628, &quot;AUC&quot;: 0.9871177999869637</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.646685848826351, &quot;IOU&quot;: 0.5761123446776952, &quot;AUC&quot;: 0.9423835227886835</span><br></pre></td></tr></table></figure>
<p>​  由此可知由convnext提取的局部特征基本上没用，主要靠segformer提取的多尺度特，接下来对segformer提取的多尺度特征进行测试：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([8, 64, 128, 128])     ---------&gt;    torch.Size([8, 64, 128, 128])  不变</span><br><span class="line">torch.Size([8, 128, 64, 64])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样1次</span><br><span class="line">torch.Size([8, 320, 32, 32])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样2次</span><br><span class="line">torch.Size([8, 512, 16, 16])      ---------&gt;    torch.Size([8, 64, 128, 128])  上采样3次</span><br></pre></td></tr></table></figure>
<p>​  首先去除 torch.Size([8, 64, 128, 128]) ---------&gt; torch.Size([8,
64, 128, 128]) 特征：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">        middle = gate_outputs * reduced</span><br><span class="line">        middle =  middle[:, 5:, :, :]</span><br><span class="line">        pred_mask = torch.sum(middle, dim=1, keepdim=True)</span><br><span class="line">        </span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9751710238930708, &quot;IOU&quot;: 0.961565841133373, &quot;AUC&quot;: 0.9937941581010818</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8472489722454933, &quot;IOU&quot;: 0.7910582403295584, &quot;AUC&quot;: 0.9871176211730294</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.6301504204199593, &quot;IOU&quot;: 0.5585220724029004, &quot;AUC&quot;: 0.9365634173154831</span><br></pre></td></tr></table></figure>
<p>​  去除前两个</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">        middle = gate_outputs * reduced</span><br><span class="line">        middle =  middle[:, 6:, :, :]</span><br><span class="line">        pred_mask = torch.sum(middle, dim=1, keepdim=True)</span><br><span class="line">        </span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.975395223852875, &quot;IOU&quot;: 0.9617374403554283, &quot;AUC&quot;: 0.9937337577342987</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8466569268999872, &quot;IOU&quot;: 0.7903226268924685, &quot;AUC&quot;: 0.9870902134024578</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.6411504408461812, &quot;IOU&quot;: 0.5703995004449652, &quot;AUC&quot;: 0.9402613987525305</span><br></pre></td></tr></table></figure>
<p>​  只保留torch.Size([8, 512, 16, 16]) ---------&gt; torch.Size([8, 64,
128, 128])</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">        middle = gate_outputs * reduced</span><br><span class="line">        middle =  middle[:, 7:, :, :]</span><br><span class="line">        pred_mask = torch.sum(middle, dim=1, keepdim=True)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.925702966623224, &quot;IOU&quot;: 0.8679496830037973, &quot;AUC&quot;: 0.9911561876535415</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.6967012119329747, &quot;IOU&quot;: 0.5692755726570369, &quot;AUC&quot;: 0.9718539403832477</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.5928905045668099, &quot;IOU&quot;: 0.4834324950086559, &quot;AUC&quot;: 0.9056356102228165</span><br></pre></td></tr></table></figure>
<p>​  只保留torch.Size([8, 320, 32, 32]) ---------&gt; torch.Size([8, 64,
128, 128])</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        reduced = torch.cat([self.inverse[i](features[i]) for i in range(8)], dim=1)</span><br><span class="line">        middle = gate_outputs * reduced</span><br><span class="line">        middle =  middle[:, 6:7, :, :]</span><br><span class="line">        pred_mask = torch.sum(middle, dim=1, keepdim=True)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9515924033639355, &quot;IOU&quot;: 0.9232780167625284, &quot;AUC&quot;: 0.9862031370401383</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8255711606730806, &quot;IOU&quot;: 0.7593634591044593, &quot;AUC&quot;: 0.9813148197920426</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.5732263977708664, &quot;IOU&quot;: 0.4835896231602484, &quot;AUC&quot;: 0.9347871392965317</span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ul>
<li>基于高频信息的全局特征网络（convnext）对于最后的贡献几乎没有</li>
<li>多尺度特征中，低等级的大尺度特征[(128, 64, 64), (64, 128,
128)]对于最后的贡献几乎没有</li>
<li>基于最后两个尺度[(320, 32, 32), (512, 16,
16)]的结果是最好的，而且(320, 32, 32)的结果要明显好于 (512, 16, 16)</li>
<li>对于结果的大部分贡献来源于segformer的(320, 32,
32)尺度的特征，小部分来源于segformer的(512, 16, 16)尺度的特征</li>
</ul>
<p>ps：不知道是因为交叉熵导致的特征集中还是其他原因。</p>
<p>​  尝试拿出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        f7 = gate_outputs[6] * self.inverse[6](self.upsample.upsamples3(outs2[2]))</span><br><span class="line">        f8 = gate_outputs[7] * self.inverse[7](self.upsample.upsamples4(outs2[3]))</span><br><span class="line">        pred_mask = torch.sum(torch.cat([f7, f8], dim=1), dim=1, keepdim=True)</span><br><span class="line"></span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9717471111299997, &quot;IOU&quot;: 0.9526561456793263, &quot;AUC&quot;: 0.9940605223178863, &quot;epoch&quot;: 1&#125;</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.8239552571959295, &quot;IOU&quot;: 0.7511917445195816, &quot;AUC&quot;: 0.9852089664210444, &quot;epoch&quot;: 1&#125;</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.623248095222164, &quot;IOU&quot;: 0.5433238030902899, &quot;AUC&quot;: 0.9288397679726282, &quot;epoch&quot;: 1&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        ff = torch.cat([self.inverse[6](self.upsample.upsamples3(outs2[2])), </span><br><span class="line">                        self.inverse[7](self.upsample.upsamples4(outs2[3]))], dim=1)</span><br><span class="line">        pred_mask = torch.sum(gate_outputs[:, 6:8, :, :] * ff, dim=1, keepdim=True)</span><br><span class="line">        </span><br><span class="line">&quot;Columbia&quot;, &quot;F1&quot;: 0.9753952238528754, &quot;IOU&quot;: 0.9617374403554286, &quot;AUC&quot;: 0.9937337577342987,</span><br><span class="line">&quot;CASIA1&quot;, &quot;F1&quot;: 0.846656926899987, &quot;IOU&quot;: 0.7903226268924689, &quot;AUC&quot;: 0.9870902056279389,</span><br><span class="line">&quot;COVERAGE&quot;, &quot;F1&quot;: 0.6339982163199723, &quot;IOU&quot;: 0.5639438125356415, &quot;AUC&quot;: 0.9364340404669443,</span><br></pre></td></tr></table></figure>
<p>​  在完全去除convnext和没有使用的权重之后：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| module                         | #parameters or shape   | #flops     |</span><br><span class="line">|:-------------------------------|:-----------------------|:-----------|</span><br><span class="line">| model                          |  56.733M               |  90.31G    |</span><br><span class="line">|  segformer                     |  44.081M               |  37.159G   |</span><br><span class="line">|  upsample                      |  11.864M               |  3.087G    |</span><br><span class="line">|   upsample.upsamplec3          |   1.475M               |            |</span><br><span class="line">|    upsample.upsamplec3.0       |    1.18M               |            |</span><br><span class="line">|    upsample.upsamplec3.1       |    0.295M              |            |</span><br><span class="line">|   upsample.upsamples3          |   0.787M               |   1.208G   |</span><br><span class="line">|    upsample.upsamples3.0       |    0.655M              |    0.671G  |</span><br><span class="line">|    upsample.upsamples3.1       |    0.131M              |    0.537G  |</span><br><span class="line">|   upsample.upsamplec4          |   6.194M               |            |</span><br><span class="line">|    upsample.upsamplec4.0       |    4.719M              |            |</span><br><span class="line">|    upsample.upsamplec4.1       |    1.18M               |            |</span><br><span class="line">|    upsample.upsamplec4.2       |    0.295M              |            |</span><br><span class="line">|   upsample.upsamples4          |   3.408M               |   1.879G   |</span><br><span class="line">|    upsample.upsamples4.0       |    2.622M              |    0.671G  |</span><br><span class="line">|    upsample.upsamples4.1       |    0.655M              |    0.671G  |</span><br><span class="line">|    upsample.upsamples4.2       |    0.131M              |    0.537G  |</span><br><span class="line">|  inverse                       |  130                   |   0.0021G  |</span><br><span class="line">|   inverse.6                    |   65                   |   1.049M   |</span><br><span class="line">|    inverse.6.weight            |    (1, 64, 1, 1)       |            |</span><br><span class="line">|    inverse.6.bias              |    (1,)                |            |</span><br><span class="line">|   inverse.7                    |   65                   |   1.049M   |</span><br><span class="line">|    inverse.7.weight            |    (1, 64, 1, 1)       |            |</span><br><span class="line">|    inverse.7.bias              |    (1,)                |            |</span><br><span class="line">|  gate                          |  0.788M                |  47.916G   |</span><br><span class="line">|  low_dct                       |                        |  1.074G    |</span><br><span class="line">|  high_dct                      |                        |  1.074G    |</span><br><span class="line">|  resize                        |                        |  1.049M    |</span><br></pre></td></tr></table></figure>
<p>​  这比剪枝狠啊！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ff = torch.cat([self.inverse[6](self.upsample.upsamples3(outs2[2])), </span><br><span class="line">                self.inverse[7](self.upsample.upsamples4(outs2[3]))], dim=1)</span><br><span class="line">pred_mask = torch.sum(gate_outputs[:, 6:8, :, :] * ff, dim=1, keepdim=True)</span><br><span class="line">pred_mask = self.resize(pred_mask)</span><br><span class="line">loss = self.loss_fn(pred_mask,mask)</span><br><span class="line">pred_mask = pred_mask.float()</span><br><span class="line">mask_pred = torch.sigmoid(pred_mask)</span><br></pre></td></tr></table></figure>
<p>​  这样的话其实就是使用segformer提取特征后，对segformer提取的最后两个特征首先上采样然后将通道数降低为1的网络，再乘上一个gate分数网络，最后再相加sigmoid得到预测结果</p>
<p>​  以上的实验中，并不知道提取的low_freq作用还是segformer网络的作用。</p>
<h2 id="segformer网络与官方网络的区别">6.4
segformer网络与官方网络的区别</h2>
<p>​  Mlp类、Block类、OverlapPatchEmbed类与原来一样<br/>​  Attention类在计算完成注意力之后加了一句（attn
= attn.float()）<br/>​  MixVisionTransformer类区别：</p>
<ol type="1">
<li>输入参数的变化，以mit_b3作为对比：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pretrain_path参数：原版没有  ---------&gt;  pretrain_path=None</span><br><span class="line">img_size参数：img_size=224  ---------&gt;  img_size=512</span><br><span class="line">num_classes参数：num_classes=1000 ---------&gt;  去除</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>代码变化：</li>
</ol>
<p>​  插入了预训练参数的加载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if pretrain_path is not None:</span><br><span class="line">    print(&quot;Load segformer pretrain pth.&quot;)</span><br><span class="line">    self.load_state_dict(torch.load(pretrain_path),</span><br><span class="line">                        strict=False)</span><br><span class="line">original_first_layer = self.patch_embed1.proj</span><br><span class="line">new_first_layer = nn.Conv2d(6, original_first_layer.out_channels,</span><br><span class="line">                                kernel_size=original_first_layer.kernel_size, stride=original_first_layer.stride,</span><br><span class="line">                                padding=original_first_layer.padding, bias=False)</span><br><span class="line">new_first_layer.weight.data[:, :3, :, :] = original_first_layer.weight.data.clone()[:, :3, :, :]</span><br><span class="line">    </span><br><span class="line">new_first_layer.weight.data[:, 3:, :, :] = torch.nn.init.kaiming_normal_(new_first_layer.weight[:, 3:, :, :])</span><br><span class="line">self.patch_embed1.proj = new_first_layer</span><br></pre></td></tr></table></figure>
<p>​  去除了原有的初始化代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.apply(self._init_weights) ---------&gt;  去除</span><br></pre></td></tr></table></figure>
<p>​  forward_features方法对输出做了小改变：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">return outs ---------&gt;  return x,outs</span><br></pre></td></tr></table></figure>
<p>​  forward方法变化（为了输出多尺度特征）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        # x = self.head(x)</span><br><span class="line">        return x</span><br><span class="line">变化为：</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x,outs = self.forward_features(x)</span><br><span class="line">        return x,outs </span><br></pre></td></tr></table></figure>
<p>​  以上便是该文章的segformer网络与官方网络的区别，现在对其区别进行分析：</p>
<p>​  该文章的segformer网络的输入是input_low =
torch.concat([image,low_freq],dim=1)，大小是B*6*512*512，所以在加载预训练权重之后对第一层的输入代码进行了更改以适配6通道数的输入。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/Mesoscopic-Insights/">https://zhaozw-szu.github.io/Mesoscopic-Insights/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">喵</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/postimages/Mesoscopic-Insights/image-20250903224303989.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/examine/" title="examine"><img class="cover" src="/img/coverImage/cover4.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">examine</div></div></a></div><div class="next-post pull-right"><a href="/LSNet-See-Large-Focus-Small/" title="LSNet:See Large, Focus Small"><img class="cover" src="/img/coverImage/cover5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LSNet:See Large, Focus Small</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><a href="/code">代码页面</a>：收罗图像取证安全领域已公布/待公布的代码 <br>,<a href="/competition">比赛页面</a>：收罗图像取证安全领域的比赛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">2.相关工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AF%A1%E6%94%B9%E5%AE%9A%E4%BD%8D%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-text">2.1.篡改定位的体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%BA%94%E7%94%A8"><span class="toc-text">2.2.多尺度应用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">3.方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E5%B1%80%E9%83%A8%E5%92%8C%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%E4%BB%A5%E5%A2%9E%E5%BC%BA%E9%A2%84%E6%B5%8B"><span class="toc-text">3.1
整合局部和全局信息以增强预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E7%A6%BB%E6%95%A3%E4%BD%99%E5%BC%A6%E5%8F%98%E6%8D%A2%E5%A2%9E%E5%BC%BA%E5%8A%9F%E8%83%BD"><span class="toc-text">3.1.1 采用离散余弦变换增强功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81%E5%92%8C%E5%B0%BA%E5%BA%A6%E8%A7%A3%E7%A0%81"><span class="toc-text">3.1.2 特征编码和尺度解码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%B0%BA%E5%BA%A6%E5%8A%A0%E6%9D%83%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D"><span class="toc-text">3.2 自适应尺度加权和模型剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E6%A8%A1%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">3.2.1 规模重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%83%8F%E7%B4%A0%E7%BA%A7%E9%A2%84%E6%B5%8B%E8%9E%8D%E5%90%88"><span class="toc-text">3.2.2 像素级预测融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E6%AC%A1%E4%BF%AE%E5%89%AA%E7%9A%84%E4%BE%9D%E6%8D%AE"><span class="toc-text">3.2.3 二次修剪的依据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">4. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-text">4.1 实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E6%96%B0%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83"><span class="toc-text">4.2 与最新方法比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6"><span class="toc-text">4.3 消融研究</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">5. 结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">6. 代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-text">6.1 定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B"><span class="toc-text">6.2 流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="toc-text">6.3 实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#segformer%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%AE%98%E6%96%B9%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">6.4
segformer网络与官方网络的区别</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'all'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
  //- console.log('MathJax loaded')
} else {
  // 重置 TeX 状态并重新渲染
  MathJax.startup.promise.then(() => {
    MathJax.texReset();  // 重置 TeX 编号等状态
    MathJax.typesetPromise();
  });

  //- MathJax.startup.document.state(0)
  //- MathJax.texReset()
  //- MathJax.typesetPromise()
  //- console.log('MathJax reset')
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>