<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Neural Clustering based Visual Representation Learning | 喵</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="发表于CVPR2024，认为现有图像视觉提取器基于图片是平滑的这一假设设计了基于网格式的架构，因此提出了聚类特征提取FEC，在图像处理中，FEC算法通过两种交替操作实现：首先将像素分组为独立簇以提取抽象特征，随后利用当前特征向量更新像素的深度特征。这种迭代机制通过多层神经网络实现，最终生成的特征向量可直接应用于下游任务。各层间的聚类分配过程可供人工观察验证，使得FEC的前向计算过程完全透明化，并赋">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Clustering based Visual Representation Learning">
<meta property="og:url" content="https://zhaozw-szu.github.io/Neural-Clustering-based-Visual-Representation-Learning/index.html">
<meta property="og:site_name" content="喵">
<meta property="og:description" content="发表于CVPR2024，认为现有图像视觉提取器基于图片是平滑的这一假设设计了基于网格式的架构，因此提出了聚类特征提取FEC，在图像处理中，FEC算法通过两种交替操作实现：首先将像素分组为独立簇以提取抽象特征，随后利用当前特征向量更新像素的深度特征。这种迭代机制通过多层神经网络实现，最终生成的特征向量可直接应用于下游任务。各层间的聚类分配过程可供人工观察验证，使得FEC的前向计算过程完全透明化，并赋">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png">
<meta property="article:published_time" content="2025-07-22T03:43:44.000Z">
<meta property="article:modified_time" content="2025-08-27T08:24:17.490Z">
<meta property="article:author" content="Zhaozw">
<meta property="article:tag" content="Clustering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/Neural-Clustering-based-Visual-Representation-Learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Neural Clustering based Visual Representation Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-27 16:24:17'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png')"><nav id="nav"><span id="blog-info"><a href="/" title="喵"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">喵</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/competition/"><i class="fa-fw fas fa-database"></i><span> 比赛</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 日记</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Neural Clustering based Visual Representation Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">创建于</span><time class="post-meta-date-created" datetime="2025-07-22T03:43:44.000Z" title="创建于 2025-07-22 11:43:44">2025-07-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-27T08:24:17.490Z" title="更新于 2025-08-27 16:24:17">2025-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-star fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><span class="post-rank">A类会议,CVPR,2024</span></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%81%9A%E7%B1%BB/">聚类</a></span><span class="post-meta-separator">|</span><a target="_blank" rel="noopener" href="https://github.com/guikunchen/FEC/"><img src="https://img.shields.io/github/stars/guikunchen/FEC?style=flat" alt="GitHub Stars: guikunchen/FEC" loading="lazy"></a></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Neural Clustering based Visual Representation Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div id="article-description">发表于CVPR2024，认为现有图像视觉提取器基于图片是平滑的这一假设设计了基于网格式的架构，因此提出了聚类特征提取FEC，在图像处理中，FEC算法通过两种交替操作实现：首先将像素分组为独立簇以提取抽象特征，随后利用当前特征向量更新像素的深度特征。这种迭代机制通过多层神经网络实现，最终生成的特征向量可直接应用于下游任务。各层间的聚类分配过程可供人工观察验证，使得FEC的前向计算过程完全透明化，并赋予其出色的自适应可解释性。<div class="disclaimer">博主观点不代表文章作者观点</div></div><article class="post-content" id="article-container"><p>Neural Clustering based Visual Representation Learning</p>
<p>Guikun Chen1 , Xia Li2 , Yi Yang1 , Wenguan Wang1*</p>
<p>1 ReLER, CCAI, 浙江大学 <br/>2 ETH Zurich</p>
<p>https://github.com/guikunchen/FEC/</p>
<h1 id="摘要">摘要</h1>
<p>​  我们深入探究机器视觉的核心课题——特征测量，通过重新审视机器学习与数据分析领域经典方法中的聚类技术展开研究。现有视觉特征提取器（包括卷积神经网络、视觉图卷积网络和多层感知机）通常将图像表示为矩形区域。尽管这种网格化范式应用广泛，但其构建基于工程实践，缺乏对数据分布的显式建模。本研究提出“聚类特征提取”（FEC）框架，这是一个概念精妙却出人意料地具有自适应可解释性的神经聚类系统。该框架将特征提取视为从数据中筛选代表元素的过程，从而自动捕捉数据的底层分布规律。在图像处理中，FEC算法通过两种交替操作实现：首先将像素分组为独立簇以提取抽象特征，随后利用当前特征向量更新像素的深度特征。这种迭代机制通过多层神经网络实现，最终生成的特征向量可直接应用于下游任务。各层间的聚类分配过程可供人工观察验证，使得FEC的前向计算过程完全透明化，并赋予其出色的自适应可解释性。针对多种视觉识别模型和任务的大量实验验证了FEC的有效性、普适性和可解释性。我们期待这项研究能促使学界重新审视当前主流的网格式架构。</p>
<h1 id="引言">1.引言</h1>
<p>​  特征测量，探索如何从高维图像数据中提取抽象、有意义的特征，是机器视觉历史上一个持久感兴趣的话题[1–3]。这种追求最初由人工设计的描述符[4–9]主导，在深度学习范式的影响下，从卷积景观[10,11]发展到注意力驱动机制[12,13]和基于MLP的方法[14,15]的前沿。卷积神经网络（ConvNets，图1a）将图像视为矩形区域，并采用滑动窗口的方式进行处理。基于注意力机制的方法（图1b)通常将图像分割为多个不重叠的块，并使用额外的[CLS]标记来表示整个图像。基于多层感知机（MLP）的骨干网络（图1c)同样遵循网格式架构，但在特征提取过程中不使用卷积或注意力机制。</p>
<figure>
<img
src="../postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722162252128.png"
alt="image-20250722162252128" />
<figcaption aria-hidden="true">image-20250722162252128</figcaption>
</figure>
<p>​  在观察图1所示的视觉主干网络阵列后，自然会产生以下疑问：❶这些网络之间存在何种关联？更重要的是，❷如果这些神经网络确实隐含地捕捉到了图像数据的某些内在属性，是否可能存在一种更透明、更易解释的方式来测量视觉特征？<br/>​  对问题❶的探索揭示了图像数据分析领域中对网格中心观点的持续坚持[16–18]。具体来说，<strong>现有骨干网络在前向处理过程中涉及的基本元素是矩形图像区域</strong>，例如基于卷积的骨干网络中的内核（滤波器）、滑动窗口和感受野，以及视觉Transformer（ViTs）和MLP中的图像块。这种广泛采用的范式虽然在卷积网络及其后续发展过程中起到了关键作用，但似乎更多是基于工程惯例而非对自然图像结构的模仿。现有大多数研究预计会随着网络层数的增加生成更抽象的特征，但没有人知道它们是如何实现的[19]。因此，问题❷变得更加根本：❸这种网格式架构存在哪些固有局限性？❹我们能否超越基于网格的统一假设，该假设无法体现图像的有机结构？<br/>​  在问题❸的驱动下，我们发现了两个关键的限制：</p>
<ul>
<li>首先，网格模型与像素组织的真实性质不一致，因此无法把握数据分布的复杂性[20]。</li>
<li>其次，深度特征提取器的黑盒特性阻碍了可解释性，掩盖了特征选择和显著性背后的原理。</li>
</ul>
<p>​  这引出了两个核心问题：❹探究当前方法论的不透明性及其与人类感知认知的差异[21–23]，后者具有将视觉场景分解为语义成分的独特能力。我们的目标是构建能够更精准捕捉像素数据分布特征、并模拟人类视觉认知过程的特征提取器，从而提升模型的可解释性和透明度。为弥合已发现的差距，必须进行根本性的范式转变：i）从图像表征的网格视图转向更灵活的模型，以包容视觉数据的动态特性；ii)从黑盒模型转向雄心勃勃的混合模型，整合强大的表征学习和可解释的特征编码。</p>
<p>​  在此脉络中，我们引入了FEC（第3节），这是一种基于聚类原理的机制可解释主干网络。其核心始于基于窗口的池化操作，生成作为初始元素的像素块。随后，FEC通过两个关键流程迭代运行：</p>
<p>​  i）基于聚类的特征池化。利用神经聚类算法对输入数据（像素块或先前聚类）进行建模，从而形成更抽象（不断扩大的）聚类结构。由于具有聚类特性，每个代表（聚类）能明确表征任意位置的像素集合，这正是FEC区别于网格式架构的关键所在。</p>
<p>​  ii)基于聚类的特征编码。在此阶段首先估计代表点，再根据像素与其代表点之间的相似度，将特征重新分配至各像素。</p>
<p>​  在这样的聚类框架下，FEC前向传播过程中的基础单元正是逐步扩大的聚类结构。</p>
<p>​  FEC具有几个引人注目的特点：</p>
<ul>
<li>首先，增强的简洁性和透明度。精简的设计，结合特征提取过程中聚类的语义意义，使得FEC在概念上既优雅又易于实现。代表模型的构建机制确保了FEC的前向过程完全透明。</li>
<li>其次，实现底层数据分布的自动化分配。确定性聚类方法能够揭示图像数据像素间的潜在关联，捕捉到标准主干网络可能忽略的语义粒度差异。如图1d所示，FEC算法无需人工监督即可自主学习区分非网格结构的语义区域。</li>
<li>最后，<strong>ad-hoc</strong>可解释性。通过深入分析各特征池化过程中的聚类分配并整合这些结果，FEC算法能在前向处理阶段基于聚合后的聚类结果进行预测解释，让用户直观掌握语义组件的构成。这种<strong>ad-hoc</strong>可解释性在安全敏感场景中具有重要价值，为人类理解特征提取的前向过程提供了切实可行的解决方案。</li>
</ul>
<p>​  通过回答问题❶-❹，我们在基于神经聚类的全透明框架中实现了视觉特征提取的理论化，弥合了经典聚类算法与神经网络可解释性之间的鸿沟。我们在第4节进行了文献综述及相关讨论。FEC作为直观且多功能的特征提取器，无需任何修改即可无缝兼容现有的视觉识别模型和任务。第5.1节的实验结果表明，仅需550万个参数，FEC在ImageNet
[24]数据集上就能达到72.7%的前1名准确率。第5.2节通过建模代表展示了FEC如何捕捉数据分布特征。第5.3和5.4节则通过三个基础识别任务验证了FEC的迁移性和通用性。最后，我们在第6节总结了研究结论。</p>
<h1
id="现有视觉特征提取器作为固定网格式解析器">2.现有视觉特征提取器作为固定网格式解析器</h1>
<p>​  <strong>问题陈述</strong><br/>​  本文研究标准分类场景。设X为输入空间（即视觉识别的图像空间），Y={猫、狗等}表示语义类别集合，例如ImageNet-1K[24]的类别数为|Y|=
1000。</p>
<p>​  <strong>标准流程</strong><br/>​  当前常用的分类方法是将深度神经网络<span
class="math inline">\(h:{\mathcal X}\mapsto{\mathcal
Y}\)</span>分解为特征提取器<span class="math inline">\(f:{\mathcal
X}\mapsto{\mathcal F}\)</span>和分类器<span
class="math inline">\(g:{\mathcal F}\mapsto{\mathcal
Y}\)</span>，满足<span class="math inline">\(h = g\circ
f\)</span>。其中，f和g分别表示特征提取器和分类器。给定输入图像X时，特征提取器f将其映射到d维表示空间<span
class="math inline">\({\mathcal F}\in{\mathbb R}^{C}\)</span>，即<span
class="math inline">\(f=f(x)\in{\mathbb
R}^{C}\)</span>；而分类器g则基于中间特征f预测类别结果<span
class="math inline">\(\hat y\)</span>，即<span
class="math inline">\(\hat y=g(f)\in{\mathcal
Y}\)</span>。本研究重点聚焦于特征提取器f的优化。</p>
<p>​  <strong>ConvNets</strong><br/>​  基于卷积的特征提取器多年来一直主导着学术界和工业界，其详细架构如下所述。形式上，给定输入图像<span
class="math inline">\(X\in{\mathbb
R}^{3×H×W}\)</span>，卷积神经网络（ConvNets）会提取特征嵌入<span
class="math inline">\(\{F^{l}\}^4_{l=1}\)</span>，其中分辨率分别为原始图像的1/4、1/8、1/16、1/32。这四个特征嵌入由四个独立阶段生成，每个阶段都包含网格式特征池化和编码。以ResNet18
[11]的第二阶段为例，给定第一阶段输出的<span
class="math inline">\(F^1\in{\mathbb
R}^{64×56×56}\)</span>，将生成如下低维特征图： <span
class="math display">\[\hat{F}^{2}={\mathrm {grid\_pool}}
(F^{1})\in\mathbb{R}^{128\times28\times28}\]</span>
​  其中网格池表示步长为2的卷积层，也可以用最大池化、平均池化等实现，之后进行特征编码得到该阶段的输出：
<span class="math display">\[{F}^{2}={\mathrm {encode}}
(\hat{F}^{2})\in\mathbb{R}^{128\times28\times28}\]</span>
​  其中，encode表示多个卷积层，这些层能保持输出分辨率的一致性。这一步骤是区分不同骨干网络的关键所在，具体实现方式包括ViTs中的自注意力机制和MLP中的标记混合器。ViTs
[13]和MLPs[14]都通过为图像中所有非重叠区域生成视觉标记嵌入来启动运算流程：
<span class="math display">\[E=\mathrm{token\_emb}(X)\]</span>
​  在此之后，ViTs使用[CLS]标记来表示整个图像，而MLP则通过计算所有图像块嵌入的平均值来实现这一目标。由于在整个特征提取的前向过程中都使用了图像块序列，我们也将其归类为网格式范式。<br/>​  总体而言，现有视觉模型主要基于刚性网格的计算建模构建，这种模式通过规则区域来呈现图像。然而，该范式低估了视觉场景的动态特性，其假设的空间均匀性与像素数据的实际分布存在矛盾。此外，它还忽视了人类感知的本质——人类感知并不局限于刚性网格，而是能够灵活地在语义上下文中进行导航[25]。<br/>​  在解决了问题❸之后，我们将在下一节详细阐述基于聚类的透明视觉特征提取器，这将是对问题❹的有力回应。</p>
<h1
id="基于聚类的特征提取fecfeature-extraction-with-clustering">3.基于聚类的特征提取（FEC，Feature
Extraction with Clustering）</h1>
<figure>
<img
src="../postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png"
alt="image-20250722173244493" />
<figcaption aria-hidden="true">image-20250722173244493</figcaption>
</figure>
<p>​  <strong>算法概述</strong><br/>​  FEC是一种基于神经聚类的视觉特征提取框架，其核心思想是采用分层选择代表的方法。具体来说，当输入图像进入FEC时，系统首先使用标准卷积层进行处理，其核尺寸和步长均设为4。随后基于生成的4×4像素块进行特征提取。接下来，FEC会针对每个输入特征交替执行以下步骤：</p>
<ul>
<li>基于聚类的特征<strong>编码</strong>，即<span
class="math inline">\({\mathbb R}^{C\times W\times H}\mapsto{\mathbb
R}^{C\times W\times
H}\)</span>。它通过将像素特征投影到相似性空间，并使用自适应（步长和核尺寸自动选择以适应所需的分辨率）平均池化来初始化聚类中心，将特征图中的像素划分为多个不重叠的簇。因此，可以根据像素与中心点之间的相似度进行聚类分配。随后，通过聚合像素特征构建聚类表征。接着采用特征调度技术——利用聚合后的中心点重新分配聚类内的像素特征，从而实现像素级特征编码，即信息传递过程。这样一来，同一聚类内的像素元素在特征空间中就会呈现出更高的一致性。</li>
<li>基于聚类的特征<strong>池化</strong>，即<span
class="math inline">\({\mathbb R}^{C\times W\times H}\mapsto{\mathbb
R}^{C^{\prime}\times W/2\times
H/2}\)</span>。与特征编码过程类似，该模块通过聚类获得簇分配。其核心区别在于直接输出簇表示，无需进行特征编码即可生成低维特征图。这种策略不仅保留了不同语义层级的组合结构，还能将聚类概念无缝融入前馈特征提取流程。</li>
</ul>
<p>​  简而言之，我们将目标任务——为视觉输入提取深度特征——形式化为表示选择。通过这种方式，中间代表可以自然替代<span
class="math inline">\(\;\mathrm
{grid\_pool}\;\)</span>。由于这些代表是根据每个输入的上下文计算得出的，因此也可以用于通过特征调度传递信息，这与<span
class="math inline">\(\;\mathrm
{encode}\;\)</span>操作具有相同的功能。然后，我们将详细阐述FEC核心模块的具体操作流程。<br/>​  <strong>中心初始化阶段</strong><br/>​  给定输入特征图<span
class="math inline">\({\boldsymbol F}\in{\mathbb R}^{N\times
C}\)</span>（其中N=W×H），我们首先通过1×1卷积层将其投影到键空间和值空间，分别得到<span
class="math inline">\({\boldsymbol K}\in{\mathbb R}^{N\times
C^{\prime}}\)</span>和<span class="math inline">\({\boldsymbol
V}\in{\mathbb R}^{N\times C^{\prime}}\)</span>。这里<span
class="math inline">\(C^{\prime}\)</span>是用于控制维度的超参数。随后，我们使用这些键和值特征来初始化聚类中心：
<span class="math display">\[\begin{aligned}&amp;[C_1^k;\cdots;C_O^k]
=\mathrm{ada\_pool_O}(\boldsymbol{K})\in\mathbb{R}^{O\times C^{\prime}},
\\&amp;[C_1^v;\cdots;C_O^v]
=\mathrm{ada\_pool_O}(\boldsymbol{V})\in\mathbb{R}^{O\times
C^{\prime}},\end{aligned}\]</span> ​  其中<span
class="math inline">\(\mathrm{ada\_pool_O}\)</span>表示在投影空间中使用自适应平均池化生成O特征中心。因此，这些中心会根据每个输入进行自适应初始化，并且梯度可以传递到所有索引。</p>
<p>​  <strong>表示建模</strong><br/>​  要将<strong>每个元素都分配到一个集群</strong>中，计算相似性矩阵M：
<span
class="math display">\[M=\langle\boldsymbol{K},[C_{1}^{k};\cdot\cdot\cdot\cdot
C_{O}^{k}]\rangle\in\mathbb{R}^{N\times O}\]</span>
​  其中⟨·，·⟩表示余弦相似度。每个元素根据<span
class="math inline">\(\mathrm{arg
max}(M)\)</span>被唯一分配到一个簇中，从而生成包含N个独热向量的分配矩阵A。通过<strong>簇分配</strong>，第o个代表（簇）的深度特征将通过以下方式聚合：
<span class="math display">\[R_{o}=\left(C_{o}^{v}+\Sigma_{n=1}^{N}A_{n
o}\,V_{n}\right)/\left(1+\Sigma_{n=1}^{N}A_{n
o}\right)\in\mathbb{R}^{C^{\prime}}.\]</span>
​  到目前为止，我们已经获得了低维特征R=[R1，…，RO]（即表示），它可以无缝地替代网格式范式中的网格式架构。</p>
<p>​  <strong>特征匹配</strong><br/>​  基于“同一簇内的元素应具有相似属性”这一认知，我们提出通过在各簇内部传播信息来增强这种特性。具体而言，我们选择采用与对应中心[26,27]的相似度相关的调制传播方式来实现这一目标。对于簇o中的元素n，其特征函数<span
class="math inline">\({\boldsymbol F}_n \in {\mathbb
R}^{C}\)</span>的更新公式为： <span class="math display">\[{\boldsymbol
F}_{n}^{\prime}={\boldsymbol F}_{n}+\mathrm{MLP}(\sigma(\alpha
{\boldsymbol M}_{n o}+\beta){\boldsymbol
R}_{o})\in\mathbb{R}^{C},\]</span>
​  其中σ表示S型函数。参数α和β是可学习的参数，用于调整相似度的缩放和偏移量。更新后的特征<span
class="math inline">\([{\boldsymbol F}_1^{\prime}；··；{\boldsymbol
F}_N^{\prime}]\)</span>是FEC <span class="math inline">\(\;{\mathrm
encode}\;\)</span>操作的输出（该过程可重复多次）。由于中心特征是从一组元素中自适应采样得到的，这种调度机制实现了簇内元素与簇中心元素之间的有效通信，从而整体理解图像底层数据分布及上下文信息。从更高维度来看，FEC可视为自注意力机制（非重叠簇）的专属变体，例如：中心初始化与键值矩阵、代表性建模与注意力分数、特征调度与加权聚合的对比。更多实现细节详见附录。</p>
<p>​  <strong>潜在数据分布的自动发现</strong><br/>​  聚类分配不仅阐明了元素与其表示之间的关系，而且还阐明了特征图中潜在的数据分布。在第l层<span
class="math inline">\(\{n|{\boldsymbol
A}_{no}^l=1\}\)</span>中分配了第o个质心的像素会聚合成一个簇（分割）<span
class="math inline">\({\boldsymbol
S}_o^l\)</span>，从而将整个特征图分解为O个可识别的段，这些段位于第l层。通过以下方式将连续层中的集群链接起来：
<span
class="math display">\[\bar{S}_{h}^{l}=\mathrm{Uninon}(\{S_{o}^{l-1}\mid
{\boldsymbol A}_{o h}^{l}=1\}),\]</span> ​  我们构建了一个分层金字塔<span
class="math inline">\([\bar S^1，\bar S^2，···，\bar
S^L]\)</span>，它将像素合并为越来越大的片段，并明确揭示了底层数据分布。</p>
<p>​  <strong>Ad-hoc可解释性</strong><br/>​  该配置通过直接前向处理过程<span
class="math inline">\(l = 1→L\)</span>，生成链式空间分解<span
class="math inline">\([\bar S^1，\bar S^2，···，\bar
S^L]\)</span>，直观地将图像解析呈现给观察者。相比之下，早期技术（如Grad-CAM
[28])需要通过回溯过程来突出激活区域。这些方法通常需要复杂的后处理才能揭示隐藏的解析机制。然而，FEC具有机制可解释性，因为其基于逐步增长的聚类（片段）的前向处理过程完全透明。详见文献[29]获取更详细讨论。</p>
<p>​  <strong>通用性</strong><br/>​  在用特征代表建模特征提取过程后，人们可能会质疑这种新范式在密集预测任务中的适用性。例如，在检测任务中，YOLO
[30]和Faster-RCNN等常用模型的训练过程依赖于基于网格的标签分配（锚点）。为了保留必要的网格信息，我们在特征代表的特征中引入了残差连接[11]：
<span
class="math display">\[R_{i}=\mathrm{Rescorn}(F_{i})+R_{i}\in\mathbb{R}^{C^{\prime}}.\]</span>
​  这一改进使得每个<span
class="math inline">\(R_{i}\in\mathbb{R}^{C^{\prime}}\)</span>既能表示网格式范式中的矩形区域，也能代表我们基于聚类范式中选定的代表性区域。如图2a所示，我们在特征提取过程中采用了与现有标准骨干网络相同的四个阶段，从而确保输出特征具有相同的分辨率。简而言之，FEC标志着视觉特征提取领域的一次根本性范式转变，同时完全兼容以往的研究成果。计算流程的具体细节将在第5.2节中详细阐述。</p>
<p>​  <strong>适应下游任务</strong><br/>​  如前所述，通过引入残差连接机制，FEC可以无缝集成到检测、分割等密集预测任务中，无需对架构进行任何修改。在分类任务方面，我们在最终特征图<span
class="math inline">\(F^L\)</span>上使用标准分类头，即单层多层感知机（MLP），该模型采用所有代表点的平均值作为输出。更多细节详见附录。我们预计近期的集合预测架构（例如DETR
[32])能够更有效地利用建模代表，这将作为未来的研究方向。</p>
<h1 id="相关工作">4.相关工作</h1>
<p>​  <strong>聚类</strong><br/>​  聚类作为机器学习领域的基础技术，其核心在于根据数据点的内在特征进行分组。面对海量数据时，聚类的目标是构建具有实际意义的集群模型（无论是否预设数量），这些集群可视为原始数据的综合呈现。通过量化分析，能够准确衡量每个数据点与集群表征之间的相似度。该技术已广泛应用于多个领域，例如场景理解[33–36]、点云分析[37-39]、图像分割[40–44]以及人工智能在科学领域的应用[45–48]。<br/>​  与以往将聚类机制作为辅助手段来实现特定任务的研究不同，我们的方法开创性地提出了从聚类视角学习通用视觉表征的创新思路。所提出的基于聚类的特征提取方法与经典视觉技术中相似像素聚类[49]存在共性，但其创新点在于能够捕捉数据分布的底层规律，并为下游任务生成连续表征。通过将整个特征提取过程重构为聚类方式选择代表元素，FEC在保留聚类固有透明特性的同时，充分发挥了端到端表征学习的鲁棒性。受生物系统同时处理多模态输入的启发，感知器[50,51]模型构建了一组与输入相关的潜在向量。FEC的理论基础源于经典聚类思想，其中间元素具有明确含义，即分段（公式8）。</p>
<h1 id="实验">5.实验</h1>
<h1 id="结论与讨论">6.结论与讨论</h1>
<p>​  在机器视觉领域，如何在保持数据分布可解释性和显式建模的同时，为视觉数据提取强大的分布式表征，始终是业界面临的重大挑战。尽管视觉主干网络已取得显著进展，但主流解决方案仍受限于处理矩形图像块的计算能力——这与人类感知中像素的组织方式形成鲜明对比。本研究通过将特征提取重构为代表性选择，实现了突破性进展，从而构建出透明且可解释的特征提取器。我们的目标是为视觉系统开辟新路径：这些系统不仅性能卓越，更能深入理解视觉场景底层的数据分布规律，从而提升其应用的可信度和清晰度。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/Neural-Clustering-based-Visual-Representation-Learning/">https://zhaozw-szu.github.io/Neural-Clustering-based-Visual-Representation-Learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">喵</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Clustering/">Clustering</a></div><div class="post_share"><div class="social-share" data-image="/postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Loupe/" title="Loupe:A Generalizable and Adaptive Framework for Image Forgery Detection"><img class="cover" src="/postimages/Loupe/image-20250908153551133.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Loupe:A Generalizable and Adaptive Framework for Image Forgery Detection</div></div></a></div><div class="next-post pull-right"><a href="/Mind-marginal-non-crack-regions/" title="Mind marginal non-crack regions:Clustering-inspired representation learning for crack segmentation"><img class="cover" src="/postimages/Mind-marginal-non-crack-regions/image-20250721225401970.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Mind marginal non-crack regions:Clustering-inspired representation learning for crack segmentation</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/" title="A Lightweight Clustering Framework for Unsupervised Semantic Segmentation"><img class="cover" src="/postimages/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/image-20250215220915193.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-15</div><div class="title">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</div></div></a></div><div><a href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/" title="A Survey on Deep Clustering:From the Prior Perspective"><img class="cover" src="/postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107223309527.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-07</div><div class="title">A Survey on Deep Clustering:From the Prior Perspective</div></div></a></div><div><a href="/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/" title="Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning"><img class="cover" src="/postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165700880.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-23</div><div class="title">Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning</div></div></a></div><div><a href="/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/" title="Deep Clustering for Unsupervised Learning of Visual Features"><img class="cover" src="/postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217154520065.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-17</div><div class="title">Deep Clustering for Unsupervised Learning of Visual Features</div></div></a></div><div><a href="/DivClust/" title="DivClust:Controlling Diversity in Deep Clustering"><img class="cover" src="/postimages/DivClust/image-20250217101153060.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-16</div><div class="title">DivClust:Controlling Diversity in Deep Clustering</div></div></a></div><div><a href="/Efficient-Deep-Embedded-Subspace-Clustering/" title="Efficient Deep Embedded Subspace Clustering"><img class="cover" src="/postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004113308304.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-04</div><div class="title">Efficient Deep Embedded Subspace Clustering</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><a href="/code">代码页面</a>：收罗图像取证安全领域已公布/待公布的代码 <br>,<a href="/competition">比赛页面</a>：收罗图像取证安全领域的比赛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E8%A7%86%E8%A7%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E4%BD%9C%E4%B8%BA%E5%9B%BA%E5%AE%9A%E7%BD%91%E6%A0%BC%E5%BC%8F%E8%A7%A3%E6%9E%90%E5%99%A8"><span class="toc-text">2.现有视觉特征提取器作为固定网格式解析器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%81%9A%E7%B1%BB%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96fecfeature-extraction-with-clustering"><span class="toc-text">3.基于聚类的特征提取（FEC，Feature
Extraction with Clustering）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">4.相关工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">5.实验</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="toc-text">6.结论与讨论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'all'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
  //- console.log('MathJax loaded')
} else {
  // 重置 TeX 状态并重新渲染
  MathJax.startup.promise.then(() => {
    MathJax.texReset();  // 重置 TeX 编号等状态
    MathJax.typesetPromise();
  });

  //- MathJax.startup.document.state(0)
  //- MathJax.texReset()
  //- MathJax.typesetPromise()
  //- console.log('MathJax reset')
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>