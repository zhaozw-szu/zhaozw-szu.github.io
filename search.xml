<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Discriminative fuzzy 𝐾-meansclustering withlocalstructure preservation for high-dimensional data</title>
      <link href="/Discriminative-fuzzy-means-clustering-withlocalstructure-preservation-for-high-dimensional-data/"/>
      <url>/Discriminative-fuzzy-means-clustering-withlocalstructure-preservation-for-high-dimensional-data/</url>
      
        <content type="html"><![CDATA[<p>Discriminative fuzzy 𝐾-meansclustering withlocalstructurepreservation for high-dimensional data</p><p>Yu-Feng Yu,a,∗， Peiwen Wei,a， Xiaoling Wu,a， Qiying Feng,b，Chuanbin Zhang,c</p><p>a广州大学统计学系<br/>b广州大学网络空间先进技术研究院<br/>c肇庆大学计算机科学与软件学院</p><h1 id="摘要">摘要</h1><p>​  𝐾均值聚类通过将数据点分配到𝐾个不同簇中，是数据挖掘和机器学习领域的重要技术。然而在实际应用中，多个因素会影响其聚类效果。其中，高维数据中冗余特征的混杂问题尤为突出。传统硬核或模糊型𝐾均值聚类方法往往难以有效处理这类高维数据。本文提出了一种创新模型——判别式模糊𝐾均值聚类，该模型将判别投影与𝑝-拉普拉斯图正则化整合到统一框架中。具体而言，该模型在将原始数据投影至低维空间时，既能有效抑制冗余特征的负面影响，又能显著提升判别能力。该方法通过在隶属度矩阵上应用𝑝-拉普拉斯图正则化，同时保留了数据的结构局部性特征。因此，对于高维数据，聚类性能得到显著提升。最后，我们提出了一种有效算法来解决所构建的优化问题。在基准数据集上获得的实验结果令人鼓舞，充分证明了所提方法的优越性。</p><h1 id="引言">1.引言</h1><p>​  聚类是数据挖掘与机器学习领域的重要课题，作为通用性强、效果显著的技术手段，已在人脸识别[1,2]、车辆识别[3]、特征选择/降维[4,5]及图像分割[6-8]等多个领域得到广泛应用。其核心目标在于揭示数据的内在结构，并将样本智能地分配到不同类别[9]。过去数十年间，各类聚类算法在学术界和工业界持续引发广泛关注，涌现出多种改进方案：包括经典𝐾均值聚类（KM）[10]、模糊𝐾均值聚类（FKM）[11]、最大间隔聚类[12]以及谱聚类[13]等。其中，KM算法通过最小化数据点与质心之间的距离，实现对数据的𝐾聚类划分。模糊𝐾均值聚类是KM算法的改进版本，通过学习隶属度将每个数据点分配到特定聚类中。尽管KM和FKM算法简单实用，但传统硬核或模糊版的𝐾均值聚类对离群值和噪声具有较差的鲁棒性[14]。因此，学界针对这一问题提出了多种相关研究方法。例如，Zhanget al.[15]在岭回归聚类框架中引入了非相关约束条件，以避免可能出现的简单解。Guoet al.[16]则在模糊聚类模型中添加了隶属度亲和套索项，从而显著提升了聚类效果。Rezaeeet al.[17]将聚类问题视为博弈论中的讨价还价过程，并提出了一种基于博弈的𝐾均值聚类算法。这些算法在处理含异常值和噪声的数据聚类时展现出显著优势。然而，该方法在聚类过程中未能有效区分高维数据中的判别性特征与冗余特征。<br/>​  随着科技发展，现实世界中的数据往往以高维特征呈现。来自不同领域的高维数据集包含大量属性，其中部分特征存在冗余。如何对具有冗余特征的高维数据进行聚类分析，始终是业界面临的重大挑战。Borleaet al.[18]提出了一种统一算法，将FCM和KM聚类方法相结合，并通过BigTim平台的分布式计算技术，在处理大规模数据时显著提升性能。Songet al.[19]则开发出一种混合特征选择算法，巧妙融合了相关性引导聚类与粒子群优化技术，为高维数据分析提供了更有效的解决方案。Pehivenet al.[20]提出了一种乘性模糊回归函数（MFRF），该函数采用一种新的乘性模糊聚类算法，用于改进模糊系统建模。Reformatet al.[21]提出了一种针对知识图谱实体的聚类方法，这些实体以带有不确定性等级的命题形式呈现，并采用余弦相似度来确定实体间的相似性。此外，许多算法通过联合应用聚类与降维技术，有效应对高维数据问题并降低冗余特征的影响。一种直观的方法是先对高维数据进行降维处理，再在低维子空间中开展聚类分析。PCAKM是一个典型代表，它首先采用主成分分析（PCA）[22]来降低数据维度，并在低维空间中进行KM聚类。Ding等人[23]提出了LDA-KM方法，该方法首先使用𝐾均值聚类生成类别标签，然后通过线性判别分析（LDA）[24]来学习子空间。与文献[23]不同，Yinet al.[25]在模糊𝐾均值聚类中引入最大熵方法进行类别标签学习，随后采用广义LDA进行投影操作。Houet al.[26]则通过模式收缩技术先学习子空间再进行数据聚类。这些方法将降维和聚类处理割裂为独立步骤，即降维过程与后续聚类环节互不关联[27]。<br/>​  近年来，针对高维数据处理，学界提出了多种将降维与聚类相结合的创新方法。这些研究突破了传统方法的局限，通过构建联合框架实现降维与聚类的协同优化。例如，Houet al.[27]提出了一种统一优化目标函数，该函数通过最大化主成分分析模型与𝐾均值聚类之间的差异来实现，并开发出迭代算法，可同步优化投影矩阵、指示矩阵和质心参数。类似地，Nie等人[9]采用主成分分析（PCA）学习子空间，并提出了一种名为模糊𝐾均值聚类与判别性嵌入（EFKM）的聚类框架。EFKM通过最小化模糊𝐾均值聚类与PCA之间的比率来实现子空间学习和聚类。局部保持投影（LPP）[28]能够保留低维空间中的局部邻域特征。为此，Zhouet al.[29]提出了一种投影模糊𝐾均值聚类算法，该算法结合了局部保持投影与模糊𝐾均值聚类方法来处理高维数据。Chenet al.[30]则将数据集划分为若干子集以学习降维器，并提出了一种投影最小二乘回归子空间聚类方法。Zhouet al.[31]提出了自适应多重聚类集成方法（CEAM），这是一种通过扩散增强算法提升性能的创新技术。Oskoueiet al.[32]开发了一种新型半监督模糊聚类算法，通过特征融合与聚类权重优化显著提升了聚类效果。Chenet al.[33]则运用核心块识别技术和快速密度可达性评估机制，有效增强了DBSCAN算法在大规模数据集上的表现。Dinget al. [35]提出了一种创新的高维流数据聚类算法。该算法通过将基于窗口的主成分分析与反馈控制相结合，能够自适应调整参数并最小化概念漂移，从而提升聚类准确性和效率。此外，部分研究者还提出了基于特征选择的子空间聚类算法。Longet al.[36]将特征选择与𝐾均值聚类整合到统一框架中，提出了一种灵活的子空间聚类算法，并采用<spanclass="math inline">\(l_{2,p}\)</span>范数来获得稳健性能。Wang et al.[37]则引入特殊选择矩阵而非特征值分解进行特征选择，提出了一种快速自适应𝐾均值子空间聚类方法，能有效筛选最具代表性的子空间。这些方法虽然将子空间学习与聚类整合到联合框架中，但未能充分考虑数据的局部信息特征。<br/>​  在本文中，我们提出了一种称为判别性模糊𝐾均值聚类（DFKC）的新型模型，该模型将判别性投影和模糊𝐾均值聚类整合到一个联合框架中。<br/>​  具体而言，DFKC在将原始数据投影到低维空间时，能够有效抑制冗余特征的负面影响并增强判别能力。同时，在学习投影矩阵过程中还能实现聚类分析。近年来，基于图的正则化技术作为保留数据结构局部性特征的有效手段已得到广泛应用[38,39]。例如，Guoet al.[16]提出了一种模糊聚类模型，通过在通用目标函数中引入基于隶属度亲和性的正则化项来保持局部信息；Wanget al.[40]则构建了基于随机傅里叶特征的模糊聚类框架，并在模型中加入𝑝-拉普拉斯正则化项[41]以维持局部性特征。受这些研究启发，我们在提出的联合框架中嵌入了𝑝-拉普拉斯图正则化项，从而有效保留数据的局部特性。本研究的主要贡献包括：</p><ul><li>我们直接将判别投影技术嵌入模糊𝐾均值聚类框架，有效抑制冗余特征的负面影响，同时增强对高维数据的判别能力。此外，在联合框架中引入了𝑝-拉普拉斯图正则化项，有助于保留局部信息。</li><li>在本文提出的框架中，将降维、模糊𝐾均值聚类和局部性保持等思想有机地结合在一起。</li><li>开发一种有效的算法来解决所提出的框架，并讨论所提出方法在参数敏感性方面的效率。</li><li>在各种聚类任务上的实验表明，与最先进的方法相比，提出的方法可以实现有竞争力的性能。</li></ul><p>​  本文结构安排如下：第二章将定义本文使用的符号体系并综述相关研究；第三章提出DFKC模型及其高效求解算法；第四章通过实验验证方法的有效性并分析参数敏感性；最后在第五章对全文进行总结。</p><h1 id="准备工作">2.准备工作</h1><p>​  本节将回顾相关研究。向量用粗体小写字母表示，例如𝐱。矩阵用粗体大写字母表示，例如𝐗，其中矩阵𝐗的第𝑖th列记作<spanclass="math inline">\({\mathbfx}_i\)</span>，第𝑖th行第𝑗th列元素记作<spanclass="math inline">\({x}_{i,j}\)</span>。符号<spanclass="math inline">\({\mathbf X}^T\)</span>表示矩阵𝐗的转置矩阵。</p><h2 id="k-means聚类">2.1. K-means聚类</h2><p>​  给定一个包含𝑁个样本的数据集<span class="math inline">\({\mathbfX}=[{\mathrm x}_1,{\mathrm x}_2,\cdot\cdot\cdot,{\mathrm x}_N]\inR^{D\times N}\)</span>，其中特征数量为𝐷，样本数为<spanclass="math inline">\({\mathrm x}_i\inR^D\)</span>。聚类算法的目标是通过最小化以下目标函数，将数据集𝐗划分为𝐾个互不相交的簇：<spanclass="math display">\[J(\mathbf{C},\mathbf{F})=\sum_{i=1}^{N}\sum_{k=1}^{K}c_{ik}\|\mathbf{x}_{i}-\mathbf{f}_{k}\|_{2}^{2}.\]</span> ​  其中<spanclass="math inline">\(\mathbf{C}=[c_{i k}]_{N\timesk}\)</span>是聚类指示矩阵，其元素表示为：当且仅当<spanclass="math inline">\({\mathrm x}_i\)</span>属于第𝑘个聚类时，<spanclass="math inline">\(c_{i k}=1\)</span>，否则<spanclass="math inline">\(c_{i k}=0\)</span>。<spanclass="math inline">\(\mathbf{F}=[\mathbf{f}_{1},\mathbf{f}_{2},\ldots,\mathbf{f}_{K}]\inR^{D\times K}\)</span>是聚类中心矩阵，而<spanclass="math inline">\(\mathbf{f}_{k}\inR^{D}\)</span>则是第𝑘个聚类的质心。</p><h2 id="模糊k-means聚类">2.2. 模糊K-means聚类</h2><p>​  模糊𝐾均值聚类[11]旨在通过学习隶属度将每个样本分配到特定的聚类中。其目标函数可表述如下：<spanclass="math display">\[\begin{aligned}\operatorname*{min}J(\mathbf{U},\mathbf{F})&amp;=\sum_{i=1}^{N}\sum_{k=1}^{K}u_{ik}^{m}\|\mathbf{x}_{i}-\mathbf{f}_{k}\|_{2}^{2}\\&amp;s.t.\quad\sum_{k=1}^{K}u_{ik}=1,0\leq u_{i k}\leq1,i=1,\dots,N\end{aligned}\]</span> ​  其中<spanclass="math inline">\(\mathrm{U}=\left[u_{i k}\right]_{N\timesK}\)</span>是隶属度矩阵，<spanclass="math inline">\(𝑢_{𝑖𝑘}\)</span>表示样本属于第𝑘个聚类的隶属度。参数𝑚（取值范围大于1）是模糊化参数，用于控制模糊程度的高低。<br/>​  通过使用拉格朗日乘数，簇中心<spanclass="math inline">\(\mathbf{f}_{K}\)</span>和隶属度<spanclass="math inline">\(𝑢_{𝑖𝑘}\)</span>根据以下方程交替更新： <spanclass="math display">\[u_{i k}=\left[\sum_{j=1}^{K}\left({\frac{d_{ik}}{d_{j k}}}\right)^{\frac{2}{m-1}}\right]^{-1}\]</span> ​  和 <spanclass="math display">\[\mathbf{f}_{k}={\frac{\sum_{i=1}^{N}u_{ik}^{m}\mathbf{x}_{i}}{\sum_{i=1}^{N}u_{i k}^{m}}},\]</span> ​  其中<spanclass="math inline">\(d_{ik}=\|\mathbf{x}_{i}-\mathbf{f}_{k}\|_{2}^{2}\)</span>。</p><h2 id="判别式嵌入式聚类">2.3.判别式嵌入式聚类</h2><p>​  判别式嵌入聚类（Discriminative embeddedclustering，简称DEC）[27]是一种将主成分分析（PCA）与𝐾均值算法相结合的联合框架，专门用于高维数据聚类。假设投影矩阵为<spanclass="math inline">\({\textbf{P}}\in{\mathit{R}}^{D\timesd}\)</span>，低维样本可表示为<spanclass="math inline">\(\mathbf{y}_{i}=\mathbf{P}^{T}\mathbf{x}_{i}\)</span>。在低维空间中，𝐾均值聚类的数学表达式可表示为<spanclass="math display">\[J(\mathbf{C},{\tilde{\mathbf{F}}})=\sum_{i=1}^{N}\sum_{k=1}^{K}c_{ik}\Vert\mathbf{y}_{i}-{\tilde{\mathbf{f}}}_{k}\Vert_{2}^{2},\]</span>​  其中<span class="math inline">\(\mathbf{\tilde F}=[\mathbf{\tildef}_{1},\mathbf{\tilde f}_{2},\ldots,\mathbf{\tilde f}_{K}]\in R^{d\timesK}\)</span>是聚类中心矩阵。<br/>​  通过将(5)与PCA相结合并进行简单的代数运算，DEC的目标函数可表示为：<spanclass="math display">\[\begin{aligned}\operatorname*{max}J(\mathbf{P},\mathbf{C},{\bar{\mathbf{F}}})&amp;=Tr(\mathbf{P}^{I}\mathbf{S}_{t}\mathbf{P})-\lambda||\mathbf{P}^{I}\mathbf{X}-{\bar{\mathbf{F}}}\mathbf{C}^{I}||_{F}^{2}\\&amp;s.t.\quad\mathbf{P}^{T}\mathbf{P}=\mathbf{I}\end{aligned}\]</span> ​  其中<spanclass="math inline">\({\bf S}_{t}=\textstyle{\sum_{i=1}^{N}({\bfx}_{i}-{\overline{\bf x}})({\bf x}_{i}-{\overline{\bfx}})^{T}}\)</span>表示总方差矩阵，样本均值为<spanclass="math inline">\(\overline{\bfx}\)</span>。由于式(6)中的优化问题不具备联合凸性，因此需要采用迭代算法交替优化投影矩阵、指示矩阵和质心。</p><p>（PS：拉普拉斯算子到𝑝-拉普拉斯算子：<spanclass="math inline">\(\Delta_{2}f=\nabla\cdot(\nabla f)\)</span>到<spanclass="math inline">\(\Delta_{p}f=\,\nabla\cdot(|\nabla f|^{p-2}\nablaf)\)</span>）</p><h1 id="所提出的模型">3.所提出的模型</h1><h2 id="动机">3.1. 动机</h2><p>​  如前所述，某些因素会降低现实应用中的聚类效果，而包含冗余特征的高维数据正是聚类过程中的关键问题。虽然多数传统聚类方法在低维数据中表现良好，但高维数据因特征冗余会导致性能下降。为解决这一难题，部分算法采用降维与聚类分离的策略[23,26]，但所得子空间可能无法满足后续聚类任务的需求[27]。因此，有研究将子空间学习与聚类整合到联合框架中[36,37]。然而这些算法存在两大局限：其一，部分方法仅对样本点进行投影操作，忽视了簇中心的作用；其二，在保持局部信息方面存在不足。<br/>​  由于数据的复杂性和结构特性，保留局部信息是聚类任务的核心要素。这有助于捕捉样本的局部结构特征，并在聚类结果中保留相关信息。基于图的正则化技术是保留局部信息的有效手段，已在聚类[42]和模式识别[41,43]领域得到应用。𝑝-拉普拉斯算子作为图拉普拉斯算子的非线性推广形式，为更好地保留局部结构提供了强有力的理论支撑[41]。实验表明，𝑝-拉普拉斯图正则化项能显著提升机器学习和聚类任务中的模型性能。<br/>​  针对上述缺陷，我们提出了一种名为判别式模糊𝐾均值聚类（DFKC）的创新模型。首先，我们将判别式投影与模糊𝐾均值聚类整合到联合框架中，对样本点和聚类中心同时进行投影操作。其次，通过将𝑝-拉普拉斯图正则化项嵌入该联合框架，有效保留了数据的局部性特征。</p><h2 id="目标函数构造">3.2. 目标函数构造</h2><p>​  受DEC[27]的启发，我们在模糊𝐾均值聚类框架中嵌入投影矩阵，以在低维子空间中进行聚类。目标函数定义如下：<spanclass="math display">\[\begin{aligned}\operatorname*{min}J(\mathbf{P},\mathbf{U},\mathbf{F})&amp;=\sum_{i=1}^{N}\sum_{k=1}^{K}u_{ik}^{m}\|\mathbf{P}^{T}\mathbf{x}_{i}-\mathbf{P}^{T}\mathbf{f}_{k}\|_{2}^{2}\\&amp;s.t.\quad\sum_{k=1}^{K}u_{ik}=1,0\leq u_{i k}\leq1,i=1,\dots,N\end{aligned}\]</span>​  沿用文献[16]的设定，我们在后续讨论中固定模糊化参数𝑚=2。如公式(7)所示，投影操作同时作用于样本点和聚类中心。通过映射公式<spanclass="math inline">\(\mathbf{P}^{T}\mathbf{x}_{i}\)</span>将样本点从原始空间<spanclass="math inline">\(𝑅^𝐷\)</span>映射到降维空间<spanclass="math inline">\(𝑅^𝑑\)</span>，通常满足条件：当投影到低维空间时，样本点与对应聚类中心的距离更接近。这种设计有效降低了特征冗余对数据降维的负面影响，同时确保样本在低维空间中的分布更加集中。<br/>​  接下来，我们探讨如何在聚类过程中实现降维。主成分分析（PCA）作为经典降维技术，在图像分类[44]、面部表情识别[45]和高维数据聚类[9,27]等领域得到广泛应用。该方法通过将原始变量重组为一组相互独立的综合变量，同时利用优化变量尽可能完整地反映原始数据信息。从数学角度出发，我们将PCA与模糊𝐾均值聚类相结合，构建如下模型：<spanclass="math display">\[\begin{aligned}J(\mathbf{P},\mathbf{U},\mathbf{F})&amp;=\sum_{i=1}^{N}\sum_{k=1}^{K}u_{ik}^{2}\|\mathbf{P}^{T}\mathbf{x}_{i}-\mathbf{P}^{T}\mathbf{f}_{k}\|_{2}^{2}-\mu\mathrm{Tr}(\mathbf{P}^{T}\,\mathrm{S}_{t}\mathbf{P})\\&amp;s.t.\quad\sum_{k=1}^{K}u_{ik}=1,0\leq u_{i k}\leq1,i=1,\dots,N\end{aligned}\]</span>​  从公式(8)可以看出，本文提出的模型是一个用于聚类和降维的统一框架。</p><h2id="具有局部结构保留的判别性模糊𝐾均值聚类">3.3.具有局部结构保留的判别性模糊𝐾均值聚类</h2><p>​  通过应用图正则化技术，前一节提出的聚类与降维统一框架得到了进一步优化。该方法能确保样本点之间相互关联时，其相似性隶属度的学习过程得到有效保障。同时，该方法能够保留数据的局部特性。受文献[40]的启发，我们在提出的统一框架中引入了𝑝-拉普拉斯图正则化技术。<br/>​  我们首先定义一个最近邻图。对于该图，设相似度矩阵为𝐖=[𝑤𝑖𝑗]𝑁×𝑁。其中第𝑖th行第𝑗th列的元素表示样本𝐱𝑖和样本𝐱𝑗之间的距离。本文采用热核相似度方案[46]，相似度矩阵𝑤𝑖𝑗的计算公式如下：<spanclass="math display">\[w_{ij}=\begin{cases}e^{-{\frac{|\mathbf{x}_{i}-\mathbf{x}_{j}|}{\sigma}}}&amp;j\in N\,B_{i}\\0 &amp;otherwise\end{cases}\]</span>​  其中𝜎用于调节相似度衰减速度，我们将其设定为0.5。𝑁𝐵𝑖是𝑖th样本的邻域集合。随后我们得到对称相似度矩阵<spanclass="math inline">\(\mathbf{W}=(\mathbf{W}+\mathbf{W}^T)/2\)</span>。最后，我们构建了一个具有局部结构保留功能的判别式模糊𝐾均值聚类框架，具体如下：<spanclass="math display">\[\begin{aligned}J(\mathbf{P},\mathbf{U},\mathbf{F})&amp;=\sum_{i=1}^{N}\sum_{k=1}^{K}u_{ik}^{2}\|\mathbf{P}^{T}\mathbf{x}_{i}-\mathbf{P}^{T}\mathbf{f}_{k}\|_{2}^{2}-\mu\mathrm{Tr}(\mathbf{P}^{T}\,\mathrm{S}_{t}\mathbf{P})\\&amp;+\lambda\sum_{k=1}^{K}\sum_{i=1}^{N}\sum_{j\inNB_{i}}\omega_{i j}(u_{i k}-u_{jk})^{p}\\\&amp;s.t.\quad\sum_{k=1}^{K}u_{i k}=1,0\leq u_{ik}\leq1,i=1,\dots,N\end{aligned}\]</span>​  其中𝜆和𝜇是两个正的权衡参数，而𝑝是一个不小于1的标量。我们在实验部分讨论了这些参数在方法中的作用。公式（10）右侧包含三个组成部分：第一项旨在最小化由样本点与低维空间中聚类中心构成的聚类误差；第二项是主成分分析，用于学习投影矩阵𝐏。通过约束条件<spanclass="math inline">\(\mathbf{P}^{T}\mathbf{P}=\mathbf{I}\)</span>，数据结构在投影过程中得以保持不变；第三项则确保邻近样本点的隶属度保持相近。</p><h2 id="优化算法">3.4.优化算法</h2><h1 id="实验结果与分析">4.实验结果与分析</h1><h1 id="结论">5.结论</h1><p>​  本文提出了一种名为判别式模糊𝐾均值聚类（DFKC）的新框架，用于学习投影矩阵并实现模糊聚类。我们摒弃了在原始数据上直接进行聚类的传统做法，而是将原始高维数据映射到低维空间，从而减少冗余特征的负面影响并提升判别能力。此外，我们将𝑝-拉普拉斯图正则化方法融入该统一框架，以保持数据的局部性特征。针对该框架，我们推导出了一种高效的优化算法。通过在真实数据库中开展多种聚类任务的实验验证，我们获得了令人信服的结果，充分证明了所提出的DFKC方法具有显著优势。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Omni-IML:Towards Unified Image Manipulation Localization</title>
      <link href="/Omni-IML-TowardsUnified-Image-Manipulation-Localization/"/>
      <url>/Omni-IML-TowardsUnified-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<p>Omni-IML: Towards Unified Image Manipulation Localization</p><p>Chenfan Qu，Yiwu Zhong，Fengjun Guo，Lianwen Jin</p><p>1华南理工大学<br/>2香港中文大学<br/>3Intsig Information Co.,Ltd<br/>4华南理工大学</p><h1 id="摘要">摘要</h1><p>​  现有的图像处理定位（IML）方法大多依赖特定任务设计，导致其仅在目标IML任务中表现优异。然而当跨多个IML任务进行联合训练时，性能会出现显著下降，严重影响实际应用效果。为此，我们提出Omni-IML——首个旨在统一不同任务IML的通用模型。具体而言，该模型通过三大核心组件实现泛化能力：<br/>​  (1)模态门编码器，其根据每个样本自适应地选择最佳编码模态，<br/>​  (2)动态权重解码器，其根据当前任务动态调整解码器滤波器，<br/>​  (3)一个异常增强模块，利用盒级别的监督来突出篡改区域，并促进任务无关特征的学习。<br/>​  除了定位任务，我们还构建了Omni-273k这一大型高质量数据集，用于辅助篡改图像的解读。该数据集包含对篡改痕迹的自然语言描述，并通过我们自主研发的自动思维链标注技术完成标注。同时，我们设计了一个简洁高效的解释模块，以更好地利用这些描述性注释。大量实验表明，我们的单个模型OmniIML在四大图像取证任务中均取得业界领先表现，不仅为实际应用提供了有效解决方案，更展现了通用型模型在图像取证领域的广阔前景。相关代码和数据集将向公众开放共享。</p><figure><imgsrc="../postimages/Omni-IML-TowardsUnified-Image-Manipulation-Localization/image-20250815211540814.png"alt="image-20250815211540814" /><figcaption aria-hidden="true">image-20250815211540814</figcaption></figure><figure><imgsrc="../postimages/Omni-IML-TowardsUnified-Image-Manipulation-Localization/image-20250815211628939.png"alt="image-20250815211628939" /><figcaption aria-hidden="true">image-20250815211628939</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LEGION:Learning to Ground and Explain for Synthetic Image Detection</title>
      <link href="/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/"/>
      <url>/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/</url>
      
        <content type="html"><![CDATA[<p>LEGION: Learning to Ground and Explain for Synthetic ImageDetection</p><p>Hengrui Kang1,2<em>， Siwei Wen3,2</em>， Zichen Wen1,2*， JunyanYe4,2， Weijia Li4,2†，Peilin Feng3,2，Baichuan Zhou2，Bin Wang2，DahuaLin2,5，Linfeng Zhang1， Conghui He2,5†</p><p>1上海交通大学<br/>2上海人工智能实验室<br/>3BeihangUniversity<br/>4中山大学<br/>5SenseTime Research</p><h2 id="摘要">摘要</h2><p>​  生成技术的快速发展是一把双刃剑。它提供了强大的工具来提高便利性，但也带来了重大的社会问题。作为<strong>图像检测领域</strong>的从业者，现有的合成图像检测方法往往缺乏人工制品级别的文本可解释性，且过度侧重图像篡改检测。当前数据集普遍存在生成器版本过时、标注细节不足等问题。本文提出<strong>SynthScars</strong>数据集，这是一个包含12,236张全合成图像的高质量多样化数据集，并配有专家人工标注。该数据集包含4种不同图像内容类型、3类人工制品类别，以及涵盖像素级分割、详细文字说明和人工制品分类标签的细粒度标注体系。此外，我们提出名为LEGION（学习生成并解释合成图像检测，(<strong>LE</strong>arningto <strong>G</strong>round and explain<br/>for Synthetic<strong>I</strong>mage<strong>d</strong>etectiON)）的多模态大型语言模型（MLLM）图像伪造分析框架，该框架集成了伪影检测、分割与解释功能。基于这一能力，我们将LEGION作为控制器集成到图像优化流程中，用于指导生成更高品质、更逼真的图像。大量实验表明，LEGION在多个基准测试中均优于现有方法，尤其在SynthScars数据集上，其mIoU值比第二名传统专家模型高出3.31%，F1分数提升达7.75%。此外，在其指导下生成的优化图像更符合人类审美偏好。相关代码、模型及数据集将对外公开。</p><h1 id="引言">1.引言</h1><p>​  从生成对抗网络[10,21]到扩散模型[13,14,32,36,37]，再到自回归模型[49,51]，图像生成技术发展迅猛，已能产出多样化且细节丰富的合成图像。虽然这项技术提升了创意表达、简化了设计流程并缓解了数据短缺问题，但也带来了隐私泄露、版权纠纷和虚假信息传播等风险。这种矛盾性既彰显了其变革力量，也暴露出伦理困境。众多研究者聚焦图像生成技术的风险，开发了检测合成图像的方法与基准[27,61]以降低社会危害。然而，对合成图像检测研究的全面梳理显示，现有方法仍存在显著局限性。</p><p>​  (I)合成图像检测数据集的挑战。<br/>​  虽然OpenForensics[24]提供的数据集包含海量信息，但其主要由早期生成对抗网络技术（GAN）生成的过时合成图像构成。这些图像质量低劣、瑕疵明显，且多采用卡通或动漫风格，人工痕迹极易辨识。正因如此，基于此类数据集训练的模型，在检测逼真合成图像时往往表现欠佳。RichHF-18K数据集[28]使用点注释来描述合成图像中的伪影，但其定位精度较低且边缘划分效果较差。与此同时，基于篡改的数据集（如SID-Set[17]）虽然能提供完整的物体轮廓标注，但其方法的泛化能力较弱，难以适应现代系统的需求。</p><p>​  (II)合成图像检测和伪影定位方法的局限性。<br/>​  传统方法如PAL4VST[67]主要依赖低层次结构线索，虽然能有效识别纹理损伤，但在处理需要全局推理的伪影时却力不从心——比如违反光照与阴影物理定律的情况。部分研究[17,18,26,58]尝试引入多模态大型语言模型（MLLMs）来解决这一难题。然而，由于研究方向趋于同质化，这些方法的发展受到限制：它们大多聚焦于篡改后的图像，而对完全合成图像的探索却十分有限。这类合成图像不仅包含更复杂的伪影特征，其现实参照物也较少受约束，且鲜少从可解释性角度进行深入研究。</p><p>​  (III)合成图像检测方法能促进图像生成吗？<br/>​  当前合成图像检测技术通过识别和定位合成图像中的伪影，有效缓解了图像生成技术带来的社会风险，从而将检测技术定位为“防御者”。然而，图像生成如同双刃剑——若只关注其负面影响，则无法充分发挥合成图像检测的潜力。基于此，我们旨在推动合成图像检测方法的设计范式转变——从构建防御者转向培养控制者。这不仅需要检测和定位合成图像中的伪影，更要引导图像生成器产出更真实自然的图像。通过这种转变，我们能够促进图像生成技术的可控发展。</p><p>​  为解决现有数据集的局限性，我们开发了SynthScars合成图像检测数据集。该数据集通过排除过时、低质量及卡通风格的图像，构建了精细标注体系：采用不规则多边形精准勾勒伪影轮廓，并提供详细的分类说明与技术解析。<strong>这种双重注释——空间性和可解释性——提高了数据集对推进合成图像检测研究的价值。</strong>为实现深度可解释性，我们提出LEGION——一个专为全合成图像设计的综合性图像伪造分析框架。该框架通过利用多层语言模型（MLLMs）强大的先验知识、推理能力和表达能力，在不同领域展现出强大的泛化能力，并对各类干扰表现出卓越的鲁棒性。此外，我们还探索了将伪造解释作为反馈机制来提升生成更高质量、更逼真图像的潜力。具体而言，我们没有将LEGION定位为防御型模型，而是将其作为控制型模型使用，并分别通过图像再生和修复技术构建了两个迭代优化流程。在图像再生环节，模型生成的伪影解释会持续优化提示词；在图像修复环节，检测到的伪影掩膜及其对应解释将指导逐区域选择性优化，逐步缩小伪影范围并提升图像真实性。与先前方法的整体对比如图1所示。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815194050429.png"alt="image-20250815194050429" /><figcaption aria-hidden="true">image-20250815194050429</figcaption></figure><blockquote><p>图1.与现有图像伪造检测方法的对比。LEGION不仅作为防御者，支持多任务伪造分析，还作为控制器，促进高质量图像生成。</p></blockquote><p>​  本文的主要贡献如下：</p><ul><li>我们提出了SynthScars数据集，这是一个具有挑战性的合成图像检测数据集，具有高质量的合成图像和多种内容类型，以及细粒度的像素级伪影注释和详细的文本解释。</li><li>我们提出了LEGION，一个综合的图像伪造分析框架，用于人工制品定位、解释生成和伪造检测，有效地帮助人类专家检测和理解图像伪造。</li><li>大量实验表明，LEGION在四个极具挑战性的基准测试中均取得了卓越性能。与现有19种方法的对比显示，该方法在绝大多数指标上均达到业界领先水平，展现出强大的鲁棒性和泛化能力。</li><li>我们不仅将LEGION定位为对抗不断演进的生成技术的防御者，更将其视为引导更高品质、更逼真图像生成的控制器。通过图像再生和修复的定性与定量实验，充分展现了LEGION在提供渐进式伪影优化反馈方面的卓越价值。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="合成图像检测与定位">2.1.合成图像检测与定位</h2><p>​  传统检测方法基于卷积神经网络和Transformer模型，将合成图像检测视为二分类任务，主要利用空间或频域特征[5,8,20,47,55,59]。然而这些方法普遍存在两大问题：一是难以适应不同生成器的泛化能力，二是对各类扰动缺乏鲁棒性。更重要的是，这类方法还存在可解释性不足的缺陷，针对特定异常原因生成自然语言解释的解决方案至今仍未得到充分探索。<br/>​  近期，多项研究[11,12,65,67]将最初的二分类任务拓展至更复杂的伪影定位场景。例如，部分学者采用梯度图[41,42,44]或注意力图[15]来识别潜在异常区域；另一些研究者则专注于构建带有详细伪影分割标注的数据集。然而，现有方法主要聚焦于从图像修复[65]或人为篡改[16,34,52]中检测伪造痕迹，却忽视了更具挑战性的AI生成痕迹识别任务。这类痕迹在图像内容、结构特征、风格表现等多维度均存在显著差异，展现出更强的灵活性、多样性和复杂性。</p><h2 id="多模态大型语言模型">2.2.多模态大型语言模型</h2><p>​  在大型语言模型（LLMs）[1,50]取得成功的基础上，多模态大型语言模型（MLLMs）[25,30,70]通过整合视觉与文本处理能力，显著提升了综合任务的性能表现。诸如FakeBench[27]和LOKI[61]等基准测试表明，MLLMs在合成图像检测领域展现出巨大潜力，能够生成更具可解释性和情境感知性的检测结果。此外，基于MLLM的通用视觉分割模型[23,40]也取得了长足进步，这些模型能通过语义信息实现精准的目标定位。<br/>​  近期研究探索了可解释合成图像分析领域，通过提供文本解释来辅助人类判断。例如，FFAA[18]通过其提出的多答案智能决策系统提升了鲁棒性，但仅限于面部数据且缺乏对定位任务的支持。Fakeshield[58]精心设计了若干模块以适应不同篡改类型，但未对完整合成图像进行分析。ForgeryGPT[26]提出定制化的语言模型架构和创新框架，能够从不同特征空间捕捉伪造图像的高阶取证知识关联，实现可解释生成与交互式对话。尽管SIDA同时探索了篡改图像和完整合成图像，但仅针对篡改图像提供伪影分割结果。总体而言，大多数方法主要聚焦于篡改分析，尚未深入探索更复杂的AI生成伪影定位任务。</p><h2 id="引导图像优化">2.3.引导图像优化</h2><p>​  现有的图像生成模型支持多模态条件生成，使得基于分割掩膜和解释的文本引导再生及伪影感知修复成为可能。早期研究[3,13]采用文本驱动方法进行条件图像生成。其他方法如ControlNet[66]则扩展了这一思路，支持包含掩膜和边缘图等多模态条件输入，从而实现更精准可控的优化过程。近年来，基于语言模型的方法[43,63]崭露头角。例如，Idea2Img[60]构建了一个智能体系统，利用GPT-4V的重构能力逐步优化文本到图像（T2I）模型的提示词，并通过迭代指导图像生成，显著提升了图像质量与文本-图像对齐效果。<br/>​  已有研究通过区域级图像伪造分析来指导修复模型修正异常区域。例如，PAL4Inpaint[65]和PAL4VST[67]将分割掩膜作为条件输入注入修复模型[45,69]或类似SDXL[39]的优化器。然而由于缺乏对文本伪影的解释，这些方法往往采用对象移除等简单手段，导致语义信息丢失。我们通过将图像伪造分析框架LEGION整合到流程中，为生成模型提供精准的伪影提示，从而拓展了这一思路。</p><h1 id="synthscars数据集">3.SynthScars数据集</h1><h2 id="动机">3.1.动机</h2><p>​  最近生成式人工智能的进步使创建复杂的合成和篡改内容变得更容易，但现有的检测数据集面临重大限制：(i)内容过时：像ProGAN[9]这样的基准模型依赖早期的生成对抗网络，生成的低保真图像很容易与StableDiffusion3.51和FLUX2等现代模型的逼真输出区分开来。(ii)领域不匹配：部分数据集专注于动漫风格或合成卡通图像，与现实应用中至关重要的自然摄影内容存在差距。(iii)注释缺陷：像RichHF-18K[28]这类数据集采用稀疏点标注方式，不仅牺牲了空间精度，还会在训练过程中引入模糊性——尤其在处理细微或复杂的篡改操作时更为明显。(iv)边界依赖问题：近期出现的数据集（如SID-Set[17]）主要针对边界清晰的篡改案例，却无法准确表征现实世界中那些边界模糊、上下文混合的伪造场景，导致模型泛化能力受限。</p><h2 id="数据集构建">3.2.数据集构建</h2><p>​  为解决第3.1节所述的局限性，我们通过一套严格的数据处理流程构建了综合数据集SynthScars，该流程旨在最大限度提升现实相关性和标注精度（如图2所示）。具体而言，我们整合并筛选了来自多个公开数据集的样本，包括RichHF-18K[28]、Chameleon [59]、FFAA [18]等，并按照以下规范进行整理。</p><p>​  <strong>数据预处理和质量控制。</strong><br/>​  为构建平衡且高质量的合成数据集，我们首先通过聚类预训练ResNet-50模型[22]的潜在特征表示进行源样本采样，随后从每个聚类中均匀采样，既避免了数据集特有的偏差，又保持了操作类型和语义内容的多样性。接着，我们采用Qwen2-VL-72B-Instruct[53]的多阶段过滤流程解决(i)和(ii)两个限制条件：剔除低质量样本（如模糊或压缩伪影）、非写实内容（如动漫风格图像），以及具有明显合成特征的样本。</p><p>​  <strong>双重细粒度标注。</strong><br/>​  为解决(iii)和(iv)类限制，我们采用不规则多边形掩膜技术对合成图像中的伪影进行标注。这种细粒度标注方法能精准识别图像中任意形状、大小或位置的伪影，更符合当前合成数据的特点——完整物体极少出现伪影。相较于基于点位的标注方式，不规则多边形掩膜在定位伪影区域时具有更高精度。参考文献[33]将合成图像伪影分为物理伪影、畸变伪影和结构伪影三类，我们借鉴这一分类体系，在标注过程中融入细粒度伪影分类机制，确保各类伪影都能获得系统且精准的标注。该方法不仅提升了数据集的清晰度与组织性，更让我们对合成伪影特性有了更深入的理解，从而为后续的针对性分析和模型训练提供了有力支持。</p><h2 id="数据集统计">3.3.数据集统计</h2><p>​  与现有合成图像数据集相比，表1突显了SynthScars的独特优势：所有样本均配备像素级遮罩、文字说明及伪影类型标注，凭借100%的有效标注率，在合成图像分析领域树立了全新标杆。如需获取包含图像分类和伪影标注等详细数据的统计信息，请参阅附录A。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815195033711.png"alt="image-20250815195033711" /><figcaption aria-hidden="true">image-20250815195033711</figcaption></figure><blockquote><p>表1.与现有图像伪造数据集的对比。最后一列显示了由通用生成器完全合成的样本数量，这些样本具有逼真的风格和有效的掩膜。乄表示仅提供篡改图像的掩膜。</p></blockquote><h1 id="方法">4.方法</h1><p>​  近期研究（如LISA[23]和GLaMM[40]）已证明多模态大型语言模型（MLLMs）在通用图像分割任务中的潜力。受此启发，我们将类似思路应用于全合成图像伪造分析任务，以解决第2节所述的局限性。为此，我们提出LEGION——一个多任务图像伪造分析框架，支持深度伪造检测、伪影定位及解释生成功能。此外，为提升现有图像生成技术的质量与真实感，我们基于LEGION分别构建了无需训练的图像再生和修复两个专用流程。</p><h2 id="legion架构">4.1.LEGION架构</h2><p>​  LEGION系统由四个核心组件构成：(i)全局图像编码器、(ii)大语言模型（LLM）、(iii)基底图像编码器和(iv)像素解码器，如图3(a)所示。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815195354693.png"alt="image-20250815195354693" /><figcaption aria-hidden="true">image-20250815195354693</figcaption></figure><blockquote><p>图3.架构概述。(a)我们提出的图像伪造分析框架LEGION。(b)和(c)展示了两种图像生成流程。其中(b)中的T2I是文本转图像的缩写，(c)中的Loca和Expla分别代表定位和解释。</p></blockquote><p>​  各组件的具体实现将在后续章节中详细阐述。首先，为从输入图像中提取全局特征，我们采用ViT-H/14CLIP作为全局图像编码器（<span class="math inline">\(\mathcalE_g\)</span>）。</p><p>​  <strong>深度伪造检测</strong><br/>​  针对该任务，我们采用常规方法将其建模为二分类问题，即将图像划分为真实或伪造两类。我们利用全局特征表示中的CLS标记，并通过双层多层感知机（MLP）进行预测。具体而言，给定输入图像<spanclass="math inline">\(x_i\)</span>时，首先将其编码为特征向量<spanclass="math inline">\(I_x={\mathcal E}_g(x_i)\in{\mathbbR}^{D_v}\)</span>。我们从该特征表示中提取CLS标记（记作<spanclass="math inline">\(CLS(\cdot)\)</span>），并将其输入MLP分类器（记作<spanclass="math inline">\(MLP(\cdot)\)</span>），从而获得真实类别与虚假类别的概率分布<spanclass="math inline">\(y_d\)</span>： <spanclass="math display">\[y_d=MLP(CLS(I_x)).\]</span>​  <strong>解释生成</strong><br/>​  为了进一步提升用户友好型自然语言解释的体验，我们引入了基于vicuna的大型语言模型（LLM）来实现视觉-语言对齐。我们设计了一个提示模板："The&lt;image&gt; provides an overview of the image."+ <spanclass="math inline">\(x_p\)</span>，其中<spanclass="math inline">\(x_p\)</span>代表专为图像伪造分析设计的定制化提示。具体而言，我们首先将CLIP全局图像编码器输出的剩余256个特征向量（排除CLS标记，记作<spanclass="math inline">\(I_x^{\prime}\)</span>）输入视觉-语言（V-L）投影层（<spanclass="math inline">\({\mathcalP}_{vl}\)</span>），该层会替换提示模板中的&lt;image&gt;标记。处理后的提示随后与图3(a)所示的伪造分析提示<spanclass="math inline">\(x_p\)</span>拼接，形成最终引导LLM生成文本解释<spanclass="math inline">\(y_e\)</span>的完整提示： <spanclass="math display">\[y_e={\mathcal L}(x_p,{\mathcalP}_{vl}(I_x^{\prime})).\]</span> <imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815202457947.png"alt="image-20250815202457947" /></p><p>​  <strong>定位伪影</strong><br/>​  为了获得像素级伪影掩膜，我们使用预训练的SAM编码器作为基础图像编码器（<spanclass="math inline">\({\mathcalE}_l\)</span>），并参考SAM解码器设计了像素解码器(<spanclass="math inline">\(\mathcal D\)</span>)。LLM的输出会在每个伪影de位置描述（例如猫的耳朵）后附加一个特殊标记&lt;SEG&gt;，随后使用语言到提示（L-P）投影层（<spanclass="math inline">\({\mathcalP}_{lp}\)</span>），将与&lt;SEG&gt;标记相关的文本嵌入向量（<spanclass="math inline">\(v_{seg}\)</span>）转换至解码器的特征空间。最终，<spanclass="math inline">\(\mathcalD\)</span>通过以下方程式生成二进制掩模(M)： <spanclass="math display">\[M={\mathcal D}({\mathcal E}_l(x_i),{\mathcalP}_{lp}(v_{seg})),s.t.,M_i\in\{0,1\}.\]</span>​  <strong>训练</strong><br/>​  我们采用两阶段独立训练策略。第一阶段，首先通过二元交叉熵（BCE）与Dice损失的加权组合优化分割性能，同时使用交叉熵（CE）损失评估预测解释与真实标注之间的差异，从而完成伪影定位和解释生成任务的训练。第二阶段则聚焦于通过典型交叉熵损失进行分类，以增强模型的伪造检测能力。各阶段损失函数可表示为：<spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{s1}&amp;=\lambda_{\mathrm}{\mathcal{L}}_{\mathrm}(M,{\hat{M}})+\lambda_{\mathrm{dice}}{\mathcal{L}}_{\mathrm{Dice}}(M,{\hat{M}})+\lambda_{\mathrm{ce}}{\mathcal{L}}_{\mathrm}(y_e,{\hat{y_e}})\\{\mathcal{L}}_{s2}&amp;={\mathcal{L}}_{\mathrm}(y_e,{\hat{y_e}})\end{aligned}\]</span></p><h2 id="图像修复流程">4.2.图像修复流程</h2><p>​  生成与检测技术相互促进，共同推动着它们的协同进化。受文献[67]启发，我们将LEGION从防伪系统升级为生成控制器，无需额外训练数据即可实现无伪影优化。通过为生成模型提供指导和反馈，LEGION能逐步消除潜在伪影，从而提升最终输出的质量与真实感。为此，我们探索了两种优化策略：一种是利用提示词修正进行图像再生，另一种则是运用修复技术对伪影区域进行选择性修正。</p><p>​  <strong>图像再生</strong><br/>​  我们采用一种迭代生成方法，将提示修订与文本到图像（T2I）模型相结合，如图3(b)所示。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815202518376.png"alt="image-20250815202518376" /><figcaption aria-hidden="true">image-20250815202518376</figcaption></figure><p>​  给定初始图像<span class="math inline">\(I_0\)</span>和提示<spanclass="math inline">\(P_0\)</span>，我们将该提示输入T2I模型（记作Regen<spanclass="math inline">\((\cdot)\)</span>）以重新生成更新后的图像<spanclass="math inline">\(I_1\)</span>，其可能包含生成不良的伪影区域。随后使用我们的LEGION框架对<spanclass="math inline">\(I_1\)</span>进行分析，该框架返回伪影异常的详细解释（例如手指变形）。这些解释将被记录在记忆库(M)中。随后，文本优化器（标记为Revise<spanclass="math inline">\((\cdot)\)</span>）通过重新审视记忆库中的历史记录，对文物相关区域的描述进行丰富，从而对初始提示<spanclass="math inline">\(P_0\)</span>进行优化。经过优化后的提示<spanclass="math inline">\(P_1\)</span>将用于生成图像的下一轮迭代。这一迭代过程会持续多次，逐步提升图像与提示的质量。具体而言，在第t次迭代时（t为0到N-1范围内的正整数），其递推公式可表示为：<span class="math display">\[\begin{aligned}{\mathcalM}_t&amp;={\mathcalM}_{t-1}+\mathrm{LEGION}(I_{t}),\\P_{t+1}&amp;=\mathrm{Revise}(P_{t},\mathcal{M}_{t}),\\I_{t+1}&amp;=\mathrm{Resen}(P_{t+1}).\end{aligned}\]</span>​  <strong>图像修复</strong><br/>​  我们还构建了一个如图3(c)所示的修复流程，通过使用图像修复模型迭代去除伪影并逐步提升图像质量。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815203107358.png"alt="image-20250815203107358" /><figcaption aria-hidden="true">image-20250815203107358</figcaption></figure><p>​  与完全再生相比，这种方法能更好地保留非伪影区域，因为图像并非完全再生，而是仅对异常区域进行选择性优化。具体来说，给定输入图像<spanclass="math inline">\(I_0\)</span>时，我们首先使用LEGION框架进行图像伪造分析，并将反馈信息组织成区域三元组集(A)，其中每个区域表示为<spanclass="math inline">\((L_i，M_i，E_i)\)</span>，L、M、E分别代表位置、掩膜和解释。随后通过输入修复模型（标记为Inpaint<spanclass="math inline">\((\cdot)\)</span>）处理每个识别出的伪影区域，该模型接收对应的掩膜和文本解释作为输入。这一过程逐区域进行直至完成所有区域，最终生成优化后的图像。通过多次迭代上述流程，可获得更高品质且更逼真的图像。对于第t次迭代的图像<spanclass="math inline">\(I_t\)</span>，该过程可表述为： <spanclass="math display">\[\begin{aligned}{\mathcalA}_{t}=\mathrm{LEGION}(I_{t})=\{(L_{i},M_{i},E_{i})\midi=1,2,\cdot\cdot\cdot,k\}\\I_{t+1}=I_{t}+\sum_{i=1}^{k}M_{i}\cdot(\mathrm{Inpaint}(I_{t},M_{i},E_{i})-I_{t}).\end{aligned}\]</span></p><h1 id="实验">5.实验</h1><h2 id="实验设置">5.1.实验设置</h2><p>​  <strong>实施细节</strong><br/>​  我们采用GLaMM的预训练权重，因其在提出的基于对话生成任务中展现出强大的能力。在第一阶段训练中，我们使用LoRA在8块NVIDIAA100GPU上对GLaMM进行微调，每块设备的α值为8，批量大小为2。初始学习率设为1e-4，公式4中平衡三种损失类型的权重超参数λce、λdice和λbce分别设置为1.0、0.2和0.4。在第二阶段训练中，我们直接在ProGAN上使用8块NVIDIAA100GPU训练多层感知机（MLP），初始学习率设为1e-3，每块设备的批量大小为64。</p><p>​  <strong>评估指标</strong><br/>​  在伪影定位任务中，我们通过报告前景与背景区域的平均交并比（mIoU）以及整体F1分数来评估分割性能，具体方法参考文献[11,12,67]。此外，为评估生成文本解释与真实标注的一致性，我们采用两项指标：基于n-gram重叠和序列匹配的ROUGE-L指标，以及基于预训练语言模型生成的文本嵌入计算余弦相似度的余弦相似度分数（CSS），该方法源自FakeShield[58]（详见附录B.2)。在图像修复环节，我们采用文献[56]提出的人类偏好评分（HPS）来评估再生图像和修复图像的质量与真实感。</p><h2 id="定位评估">5.2.定位评估</h2><p>​  为评估模型在人工标注数据集上的定位性能，我们采用SynthScars数据集的训练集作为训练样本，并通过其测试集进行域内评估。此外，我们使用经过筛选的LOKI[61]和RichHF-18K[28]数据集来检验模型对未知领域的泛化能力，这些数据集仅保留具有真实风格的图像。我们将LEGION与全合成伪影定位领域的顶尖模型（SOTAs）进行对比。基准测试包括传统专家模型，如HiFi-Net[12]、TruFor [11]和PAL4VST[67]，以及用于物体定位的视觉语言模型，例如Ferret [62]、Griffon[64]和LISA[23]。此外，我们还对在各类任务中表现优异的通用多语言语言模型进行了评估，包括InternVL2[7]、Qwen2-VL [53]和DeepSeek-VL2 [57]。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815203803904.png"alt="image-20250815203803904" /><figcaption aria-hidden="true">image-20250815203803904</figcaption></figure><blockquote><p>表2.在未见过的领域中SynthScars和两个基准上的伪影定位性能比较。<br/>&gt;∗表示在SynthScars数据集上进行微调的模型，其余模型因缺乏训练代码而采用预训练权重。☆和□分别代表输出分割掩膜和边界框的模型。灰度处理方法将图像大部分区域预测为伪影，仅作参考用途，未纳入本次对比。</p></blockquote><p>​  表2的测试结果表明，尽管LEGION在RichHF-18K数据集上的F1分数略低于LISA-v1-7B和TruFor，但它在全部三个评估数据集上均达到了业界顶尖水平。与传统专家模型相比，LEGION在SynthScars数据集的对象类别中，F1分数比最强专家模型PAL4VST高出10.65分，并且在另外两个数据集中也持续保持优势。针对对象定位的视觉语言模型（VLMs）和通用多语言学习模型（MLLLMs），我们发现这些模型因缺乏预训练和特定任务的先验知识，在伪影定位方面存在明显短板。这导致两种极端现象：部分模型完全无法识别前景区域（如DeepSeek-VL2)，而另一些模型则过度估计伪影，将大部分图像视为巨大伪影（如Ferret、Griffon和Qwen2-VL)，导致平均交并比（mIoU）偏低，但因对前景的过度召回而人为抬高了F1分数。在SynthScars数据集上微调的InternVL2和LISA虽表现稍好，但LEGION仍以多数指标优势碾压两者。各方法的可视化对比详见图4。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815204219726.png"alt="image-20250815204219726" /><figcaption aria-hidden="true">image-20250815204219726</figcaption></figure><p>​  <strong>鲁棒性研究</strong><br/>​  我们还系统比较了LEGION与PAL4VST在三种扰动类型下的定位性能。相关结果详见附录B表10，充分证明了本模型在强干扰环境下的鲁棒性——这是传统方法难以企及的关键优势。</p><h2 id="解释性性能">5.3.解释性性能</h2><h2 id="检测性能">5.4.检测性能</h2><p>​  基于前人研究[4,38,47,48,68]，我们在ProGAN模型[9]上训练LEGION，并通过UniversalFakeDetect基准测试[38]评估其跨生成器泛化能力。如表5所示，LEGION在GAN、CRN和IMLE数据集上取得最高准确率，在SITD数据集保持具有竞争力的第二名表现，并在其他生成器上维持了相当的检测性能。</p><figure><imgsrc="../postimages/LEGION-Learning-to-Ground-and-Explain-for-Synthetic-Image-Detection/image-20250815204419277.png"alt="image-20250815204419277" /><figcaption aria-hidden="true">image-20250815204419277</figcaption></figure><h2 id="图像修复案例">5.5.图像修复案例</h2><h1 id="结论">6.结论</h1><p>​  在本研究中，我们通过构建SynthScars数据集，探索了完全合成图像伪造分析这一极具挑战性的任务。该数据集包含多种高难度伪影实例，这些实例不受局部轮廓限制，需要全局理解能力。为实现更精细的伪影分析，我们提出基于大模型语言模型（MLLM）的LEGION框架，相比传统方法显著提升了可解释性。实验结果表明，该框架在多个基准测试和评估指标中均展现出卓越性能与强大鲁棒性。该研究在定性与定量层面均展现出作为引导式图像生成和修复控制器的强大潜力。尽管我们提出的数据集和方法弥补了图像伪造分析中的若干关键缺陷，但完全合成伪像固有的多样性和灵活性仍为未来改进提供了广阔空间。我们期待更多研究者深入探索这一领域，共同推动生成式人工智能的负责任与伦理化应用。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Imbalance-Aware Discriminative Clustering for Unsupervised Semantic Segmentation</title>
      <link href="/Imbalance-Aware-Discriminative-Clustering-for-Unsupervised-Semantic-Segmentation/"/>
      <url>/Imbalance-Aware-Discriminative-Clustering-for-Unsupervised-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<p>Imbalance-Aware Discriminative Clustering for Unsupervised SemanticSegmentation</p><p>Mingyuan Liu1,2 · Jicong Zhang1 · Wei Tang2</p><h1 id="摘要">摘要</h1><p>​  无监督语义分割（USS）旨在通过学习未标注图像集合，将图像划分为具有语义意义的片段。当前方法的有效性受到表征学习与像素聚类协调、不同类别特征分布建模、异常值与噪声处理以及像素类别不平衡问题等挑战的困扰。<br/>​  本文提出了一种名为“不平衡感知密集判别聚类（IDDC，Imbalance-AwareDense DiscriminativeClustering）”的创新方法，为USS提供了一个统一框架来解决上述难题。与现有方法分阶段学习伪掩码生成与更新、嵌入优化与聚类等步骤不同，IDDC通过新颖的目标函数实现了端到端自监督学习——该目标函数将视觉Transformer（ViT）嵌入空间中的像素流形结构迁移至标签空间，同时有效抑制像素亲和度中的噪声干扰。<br/>​  在推理过程中，训练好的模型会直接输出基于图像条件的每个像素分类概率。<br/>​  此外，本文提出了一种基于Weibullfunction的新正则化方法，用于处理单次训练中的像素类别不平衡和聚类退化问题。实验结果表明，IDDC在COCO-Stuff-27、COCO-Stuff-171和Cityscapes三个真实数据集上的表现显著优于所有先前的USS方法。大量消融研究验证了各设计的有效性。我们的代码可在https://github.com/MY-LIU100101/IDDC平台获取。</p><h1 id="引言">1.引言</h1><p>​  USS旨在从未标注图像集合中学习语义分割。该方法无需依赖任何形式的人工标注，不仅大幅降低了标注成本，还能轻松应用于新数据或新应用场景。此外，USS还能发现先验未知的新型视觉模式与结构(Choet al.,2021)，这对分析新兴领域的图像具有重要价值。尽管其重要性不言而喻，但由于缺乏语义监督、类别内差异显著以及严重类别不平衡等问题，USS仍是一个极具挑战性的课题。</p><p>​  现有方法通常采用两阶段学习框架来处理超大规模数据集（USS）。这些方法可分为两大类：</p><p>​  第一类通过K均值算法对预训练模型的像素嵌入进行聚类生成伪标签，随后迭代优化分割结果(Caronet al., 2018; Cho et al., 2021; Gao et al., 2022; Yin et al.,2022)。但这类方法的性能容易受到初始生成伪掩码的影响。此外，如何确定两阶段交替处理的频率也颇具挑战：若频繁更换伪标签会干扰特征学习过程并导致结果不稳定(Zhanet al., 2020)，而若更新速度过慢又会使特征学习过度拟合初始标签猜测。</p><p>​  第二类方法以STEGO(Hamilton et al.,2022)为代表，其核心思路是先学习低维且适合聚类的像素嵌入特征，再通过USS(Liet al., 2023; Melas-Kyriazi et al., 2022; Pang et al., 2022; Seong etal., 2023; Van Gansbeke et al., 2021;Zadaianchuk et al., 2022; Ziegler&amp; Asano,2022)将像素嵌入特征进行聚类分组。但这种方法存在若干局限性。</p><ol type="1"><li>首先，特征学习与像素聚类过程被割裂开来，而非采用端到端的同步训练方式（即所有模块并非同时进行优化）。由于特征学习完全不考虑后续聚类任务的具体目标（包括聚类数量和目标函数），模型优化可能无法达到最佳的聚类效果。<br/>2.其次，聚类算法（最常用的是K均值算法）具有生成性特征，对聚类形态存在较强假设。相较于判别式聚类方法（Ng和Jordan，2001），它在处理高维特征时表现欠佳且易受异常值干扰。<br/>3.最后，在超声图像分割（USS）中，严重的像素类别不平衡问题往往被忽视。直接将表征学习与聚类方法结合常导致退化解：部分聚类为空(Caronet al., 2018; Ji et al., 2019)。一种直接的解决方案(Ji et al., 2019;Krause et al., 2010; Van Gansbeke et al.,2020)是强制要求类别标签均匀分布，但这与实际像素高度偏斜的类别分布存在矛盾。</li></ol><p>​  本文提出了一种名为ImbalanceAware密集判别式聚类（IDDC）的新型方法，用于解决USS问题。该方法在一个统一框架下解决了上述所有限制。<br/>​  首先，IDDC的独特之处在于其判别式设计，能够直接输出基于输入图像的像素级分类概率。与广泛使用的K均值算法不同，IDDC无需预设聚类的生成分布或形态特征。因此，该方法能更灵活地处理不同类别和数据集的特征分布差异，对异常值更具鲁棒性，并能在高维特征空间中实现更精准的像素分类。<br/>​  其次，IDDC通过一种创新的目标函数，以端到端和自监督的方式学习像素级特征表示与密集判别聚类。该目标函数将视觉Transformer（ViT）嵌入空间中的像素流形结构迁移至标签空间。IDDC实现了表征学习与像素聚类的无缝衔接，无需经过生成伪标签、更新伪标签或学习降维且适合聚类的嵌入等中间学习目标。<br/>​  最后但同样重要的是，IDDC引入了一种基于Weibull函数(Murthy等人，2004年）的新型正则化器，可在单次操作中有效处理像素类别失衡和聚类退化问题。熵值作为深度聚类方法中的常用指标(Barber&amp; Agakov, 2005; Bridle et al., 1991;Krause et al., 2010; VanGansbeke et al.,2020)，通过促进像素均匀分配到各聚类来避免空聚类现象。然而现实世界中物体大小差异显著、出现频率各异，导致像素类别分布呈现严重偏态。我们将证明，这种新型正则化器不仅能精准建模偏态分布，还能有效规避空聚类问题。<br/>​  我们的贡献总结如下：</p><ul><li>我们提出了一种名为“不平衡感知密集判别聚类（IDDC）”的创新方法，专门针对图像超尺度聚类（USS）任务。该方法通过直接从图像中预测像素级分类概率，以判别式、端到端且自监督的方式联合学习密集特征表示与像素标签。相较于现有USS方法普遍采用的两阶段学习框架和生成式聚类技术，IDDC有效解决了这些方法在表征学习与像素聚类协同处理、类别分布不平衡建模以及异常值处理等方面的不足。</li><li>我们设计了一种创新性目标函数，通过将ViT嵌入空间中像素的流形结构迁移至标签空间，有效实现了IDDC模型的学习。针对由不同类别像素在ViT嵌入空间中存在相似性所引发的噪声训练信号问题，我们进行了深入研究。这一技术突破对实现自监督密集表征学习及像素标注具有关键作用。</li><li>我们提出了一种基于威布尔函数的新型正则化方法。该方法不仅能有效避免聚类退化（即空簇现象），还能解决超声图像分析中的像素类别不平衡问题——这一关键问题在以往研究中往往被忽视。</li><li>在COCO-Stuff-27、COCO-Stuff-171和Cityscapes三大真实数据集的实验中，IDDC方法以显著优势超越现有最先进算法。该成果为新兴且极具挑战性的超大规模数据集（USS）任务确立了全新基准。我们通过大量消融实验验证了IDDC各设计方案的有效性。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="无监督语义分割uss">2.1.无监督语义分割（USS）</h2><p>​  USS的目标是无需人工标注即可完成图像中每个像素的自动标注。早期研究将USS建模为补丁级聚类问题，通过最大化互信息将不同像素排列方式的补丁（Ouali等人，2020）、多种增强视角的补丁（Mirsadeghi等人，2021）或空间相邻补丁（Ji等人，2019）进行聚类。然而这些方法容易产生不准确的分割结果，且由于忽视了不同图像间补丁的相关性以及类别分布的不平衡特性，主要应用于分割天空、道路等固定元素。<br/>​  随后，研究者提出了基于跨图像学习的像素级解决方案。这些方案采用两阶段学习策略：先生成伪掩码再进行优化，或先学习像素级嵌入再进行聚类。前一种方法通过迭代生成并优化伪掩码作为监督信息来实现优化。PiCIE(Choet al., 2021)受深度聚类(Caron et al.,2018)启发，通过对比学习描述性像素嵌入，并迭代更新K均值聚类生成伪掩码。PASS(Gaoet al., 2022)构建自监督模型，在像素注意力图辅助下生成伪掩码。Yin et al.(2022)采用K均值聚类预训练像素嵌入作为伪标签，并通过自助法进行优化。后续研究中，由Hamilton团队提出的STEGO(Hamiltonet al.,2022)尝试从预训练模型中提取低维像素特征，并利用下游K均值聚类进行分组优化。Hamiltonet al.(2022)通过提取预训练网络的密集对应关系，先学习低维像素特征再进行聚类。Panget al. (2022)通过提取视频帧的不变性特征，获得更具描述性的特征。Seong etal. (2023)基于预训练嵌入和局部邻接关系，提取正向像素对进行对比学习。Liet al.(2023)将图像分割为多个区域，通过K均值聚类生成区域表征用于无监督语义分割。<br/>​  IDDC与STEGO(Hamilton et al.,2022)在建模、学习和解决类别不平衡问题方面存在显著差异。<br/>​  (1)建模方面。STEGO首先通过蒸馏方法提取低维特征，随后采用K均值算法进行像素聚类，该算法假设输入特征呈球形分布、各簇方差相等且簇大小均匀。相比之下，IDDC具有判别性特征，能够直接输出基于输入图像条件的每个像素分类概率，且无需满足上述任何假设。因此，IDDC在处理不同类别和数据集间特征分布差异时更具灵活性，对外部异常值的抗干扰能力更强，并能更高效地对高维特征空间中的像素进行分类。<br/>​  (2)学习机制。STEGO采用分阶段学习策略，分别对蒸馏学习和K均值聚类进行训练。这两个阶段在不同学习目标下进行优化，这种设计存在局限性。例如，蒸馏学习阶段完全不了解聚类目标或聚类数量，因此无法根据聚类阶段的实际需求进行动态调整。相比之下，IDDC可以在统一的学习目标下实现端到端训练。这种设计使得表征学习与像素聚类能够无缝协作，从而提升学习效率。<br/>​  (3)解决类别不平衡问题。STEGO采用K均值算法提取特征进行聚类，但该方法常生成的均衡聚类结果(Luet al., 2019; Xiong et al.,2006)与类别分布失衡的实际情况存在矛盾。针对这一问题，IDDC创新性地引入基于Weibull函数的正则化项，通过单次训练即可有效解决像素类别不平衡和聚类质量下降的问题。</p><h2 id="无监督对象中心分割">2.2.无监督对象中心分割</h2><p>​  部分研究(Van Gansbeke et al., 2021, 2022; Zadaianchuk et al., 2022;Melas-Kyriazi et al., 2022; Ziegler &amp;Asano,2022)聚焦于图像中物体的分割任务。这些研究采用了两阶段自监督学习框架：第一阶段提取前景区域或物体部件，并学习其视觉表征；第二阶段通过K均值聚类将这些区域归类为不同物体类别。具体而言，VanGansbeke等人（2021）通过监督显著性检测网络提取前景区域，并利用对比学习技术学习其特征。随后，他们对每个显著物体区域的特征向量进行平均池化处理，再通过K均值聚类确定其类别。COMUS(Zadaianchuk et al., 2022))和MaskDistill(Van Gansbeke et al.,2022)则分别采用无监督显著性检测、像素嵌入和注意力机制提取前景区域。这些提取的区域随后被聚类为伪标签，用于训练能够分割多个物体的分割网络。Leopart(Ziegler &amp; Asano,2022)通过聚类自监督密集特征提取前景区域，再利用社区检测将其划分为多个物体。DSM(Melas-Kyriazi etal.,2022)采用谱分段方法选取前k个特征向量识别物体部件，并对每个部件的特征向量进行平均池化处理后，通过K均值聚类获得语义标签。<br/>​  这些研究分别独立学习前景区域、特征和聚类，且仅对图像中的物体进行标注。相比之下，IDDC采用端到端的判别式方法学习密集特征和像素聚类，能够对每个像素（包括物体和背景）进行标注。此外，IDDC在自监督学习中明确处理噪声问题，并解决像素类别不平衡问题——这些都是前人研究中未曾涉及的。</p><h2id="判别式聚类discriminative-clustering">2.3.判别式聚类(DiscriminativeClustering)</h2><p>​  判别式聚类旨在从未标注数据中学习判别分类器，即确定不同聚类之间的分界线。传统方法主要采用互信息优化（Barber与Agakov，2005；Bridle等，1991；Krause等，2010）和最大间隔优化（Xu等，2004；Zhao等，2008）。这些方法通过平衡类别分布或寻找数据中的最大间隔超平面来实现目标。深度学习方法（VanGansbeke等，2020；Ji等，2019；Ouali等，2020；Schmarje等，2021；GhasediDizaji等，2017；Chang等，2017）则基于连通性聚类理念，将视觉相似的图像对归为同一类别。为避免聚类退化，这些方法假设类别标签在数据集中均匀分布。<br/>​  与之不同，IDDC专注于USS（密集像素标注任务）。Cho等人（2021）的研究表明，直接将图像聚类方法应用于像素层面效果欠佳。此外，处理密集标注带来的计算开销并非易事。我们还深入研究了像素不平衡问题以及由嵌入空间中相似但属于不同类别的像素所引发的噪声训练信号——这些因素在先前研究中常被忽视。</p><h2 id="自监督学习">2.4.自监督学习</h2><p>​  自监督学习致力于从无标注图像中挖掘具有意义的视觉表征。早期研究通过解决预设任务实现这一目标，例如图像修复、拼图游戏和旋转预测（Alexey等人，2015；Bojanowski与Joulin，2017；Doersch等人，2015；He等人，2020；Komodakis与Gidaris，2018；Noroozi与Favaro，2016）。近年来的研究主要基于对比学习方法（Chen等人，2020；Chen与He，2021；Grill等人，2020；He等人，2020）。这类方法通过提取同一图像的不同视角特征，同时消除不同图像间的相似性。经过训练后，所获得的图像表征可应用于下游任务。<br/>​  由于全局表征难以满足密集预测需求（何等人，2019；普鲁什瓦卡姆与古普塔，2020)，部分研究通过建立跨视角的密集对应关系，将图像级方法扩展到像素级（洪等人，2019；李等人，2021；罗等人，2021；王等人，2021)。另一类方法则利用视觉变换器（ViT）的优势（图沃龙等人，2021)，因其能建模长距离特征交互，相比卷积神经网络能更好地保留块级相似性（卡隆等人，2021；汉密尔顿等人，2022；周等人，2021)。我们的方法采用自监督ViT预训练的密集特征作为像素嵌入。</p><h1 id="方法">3.方法</h1><h2 id="概观">3.1.概观</h2><p>​  无监督语义分割（USS）旨在从一组未标注的图像中学习语义分割模型。对于新图像，它能够为每个像素分配一个类别标签。<br/>​  以往的USS方法通常采用两阶段的顺序或迭代学习框架。第一阶段是将图像映射为密集特征的神经网络，第二阶段则是使用传统聚类算法（通常是K均值算法）对特征空间中的像素进行聚类。如第1节所述，该框架存在若干难题：由于表示学习与像素聚类这两个过程被割裂而非端到端衔接，导致二者难以协调；由于K均值算法假设每个聚类呈球形，难以建模物体和背景的多样性并处理异常值；此外，现实世界中像素类别不平衡的问题完全被忽视，难以有效应对。<br/>​  我们提出了一种名为“不平衡感知密集判别聚类（IDDC）”的创新方法，用于解决超分辨率图像处理中的数据不平衡问题。该方法通过统一框架全面应对上述挑战。</p><figure><imgsrc="../postimages/Imbalance-Aware-Discriminative-Clustering-for-Unsupervised-Semantic-Segmentation/image-20250815163719782.png"alt="image-20250815163719782" /><figcaption aria-hidden="true">image-20250815163719782</figcaption></figure><blockquote><p>图1不平衡感知密集判别聚类（IDDC）在无监督语义分割（USS）中的应用架构。该方法通过直接建模图像中每个像素的分类概率来实现无监督语义分割。其网络架构包含两个核心组件：主干网络（即基于未标注图像预训练的ViT模型）负责提取图像密集特征，头部网络则负责预测像素级分类概率。这两个组件通过端到端、判别式且自监督的方式进行学习，其核心在于一种创新的目标函数——该函数由三个损失项构成。L+和L−通过将ViT嵌入空间中像素的流形结构迁移至标签空间，并允许像素亲和度存在噪声，为语义分割学习提供了有效的训练信号。<spanclass="math inline">\(L^{Weibull}\)</span>是一种基于Weibull函数的新正则化方法，既能避免聚类退化问题，又能有效解决像素类别不平衡的挑战。</p></blockquote><p>​  图1展示了IDDC的整体架构：IDDC直接基于图像条件建模每个像素的分类概率，无需假设生成分布或聚类形态。其网络架构包含两个核心组件——主干网络（即基于未标注图像预训练的ViT模型）负责提取密集特征，分割头则通过Softmax函数预测像素级分类概率。这两个组件采用端到端、判别式且自监督的学习方式，通过将ViT嵌入空间中的像素流形结构迁移至标签空间，有效缓解像素亲和力噪声的影响，并解决聚类退化与像素类别不平衡问题。具体而言，我们充分利用了自监督ViT模型的最新进展：其图像密集嵌入能良好保留像素间的语义相似性（第3.2节）。基于这种像素流形结构，我们设计了IDDC的监督学习机制（第3.3节）。然而，成对像素亲和力存在显著噪声干扰：在ViT嵌入空间中邻近的像素可能属于不同类别。我们深入研究了噪声处理方法（第3.3节）。最后，我们引入基于Weibull函数的新正则化项，用于建模像素的偏态分布并避免空聚类（第3.4节）。</p><h2 id="密集嵌入作为分割提示">3.2.密集嵌入作为分割提示</h2><p>​  我们证明了自监督视觉Transformer（ViT）学习到的图像密集嵌入具有语义意义。尽管存在噪声，但在ViT嵌入空间中距离更近的两个像素更有可能属于同一类别。<br/>​  给定输入图像时，基于ViT的主干网络会生成密集特征图。设<spanclass="math inline">\(u_i\)</span>表示第i个像素的二维归一化嵌入向量，我们通过计算两个像素i和j的嵌入向量余弦相似度来确定它们之间的亲和力：<spanclass="math inline">\(u_i·u_j\)</span>。图2展示了在Cityscapes训练集（Cordts等人，2016）上计算的像素对亲和力分布情况。</p><figure><imgsrc="../postimages/Imbalance-Aware-Discriminative-Clustering-for-Unsupervised-Semantic-Segmentation/image-20250815164039985.png"alt="image-20250815164039985" /><figcaption aria-hidden="true">image-20250815164039985</figcaption></figure><blockquote><p>图2展示了同类别（红色）与不同类别（蓝色）像素之间的成对亲和力分布。两个像素间的亲和力通过它们在ViT嵌入空间中的余弦相似度进行计算。</p></blockquote><p>​  其中红色和蓝色分别对应同类别像素对和不同类别像素对的分布。该主干网络采用ViT小模型架构，补丁尺寸为16，由Zhou等人（2021）基于ImageNet（Deng等人，2009）无标签数据进行预训练。<br/>​  通过观察图2可以得出两个重要结论。首先，同类像素对的亲和力通常高于异类像素对。当两个像素的亲和力超过0.2（即图2中两条曲线的交叉点）时，它们更可能属于同一类别。这一发现启发我们：在ViT嵌入空间中，通过分析像素间的亲和力关系，可以作为判断标签空间内像素关联性的关键线索，从而为语义分割模型的学习提供有效依据。其次，不同类别的像素仍然可能具有较高的亲和性，这使得前面提到的提示噪声。处理噪声对于有效的自监督学习至关重要。</p><h2 id="像素流形的抗噪学习">3.3.像素流形的抗噪学习</h2><p>​  我们引入了一个基本的目标函数，通过将ViT嵌入空间中像素的流形结构转移到标签空间（第3.3.1节）以及缓解像素亲和力中的噪声的不利影响（第3.3.2节）来学习USS。</p><h3 id="像素流形迁移学习">3.3.1像素流形迁移学习</h3><p>​  第3.2节计算的像素对亲和度特征，刻画了ViT嵌入空间中像素的流形结构。我们的基础目标函数通过在标签空间中保持这种流形结构来学习USS（UnsupervisedSupervisedSampling）。具体来说，当两个像素在ViT嵌入空间中距离较近或较远时，它们在标签空间中也应保持相近或相距较远。该目标函数由正项和负项构成：正项使每个正像素对的分类概率向同一方向拉近，而负项则使每个负像素对的分类概率朝相反方向分散。这里，正像素对指亲和度超过阈值t+的像素对，负像素对指亲和度低于阈值t−的像素对。<br/>​  为了获取正样本对，我们首先在同一图像中选取两个部分重叠的裁剪区域，然后从每个裁剪区域中各选取一个像素形成正样本对。两个裁剪区域之间的重叠部分确保存在同类像素对。通过随机几何变换扰动像素嵌入并扩充数据，这有助于提升学习稳健性并避免过拟合。设<spanclass="math inline">\(u_i\)</span>和<spanclass="math inline">\(u_j^+\)</span>分别表示两个裁剪区域中像素的l2归一化嵌入向量，其对应的分类概率分别为<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(v_j^+\)</span>。正样本损失项的公式为： <spanclass="math display">\[L^{+}=\frac{1}{N^{+}}\sum_{\foralli,j}w^{+}(u_{i}\cdot u_{j}^{+};\,t^{+})h^{+}(v_{i}\cdotv_{j}^{+})\]</span> ​  其中<span class="math inline">\(w^{+}(u_{i}\cdotu_{j}^{+};\,t^{+})\)</span>通过返回<spanclass="math inline">\(u_{i}\cdot u_{j}^{+}\)</span>（当其值大于<spanclass="math inline">\(t^+\)</span>时）来筛选正像素对，否则返回零；N+统计正像素对的总数，h+是单调递减函数（因为目标函数需要最小化）。<spanclass="math inline">\(v_{i}\cdotv_{j}^{+}\)</span>可理解为两个像素在独立情况下属于同一类别的概率。除了筛选非正像素对外，<spanclass="math inline">\(w^{+}(u_{i}\cdotu_{j}^{+};\,t^{+})\)</span>还利用ViT嵌入空间中正像素对的亲和力来加权它们在标签空间中的相似度。这解释了亲和力较大的像素对更可能属于同一类别的观察结果。h+的设计对于处理噪声至关重要，将在下一节详细说明。<br/>​  为了获得负样本对，我们首先分别从两张不同的图像中各采样两个裁剪区域，然后从每个裁剪区域中各采样一个像素点来形成负样本对。负损失项的公式表示为：<span class="math display">\[L^{-}=\frac{1}{N^{-}}\sum_{\foralli,j}w^{-}(u_{i}\cdot u_{j}^{-};\,t^{-})h^{-}(v_{i}\cdotv_{j}^{-})\]</span> ​  其中，<span class="math inline">\(w^{-}(u_{i}\cdotu_{j}^{-};\,t^{-})\)</span>通过返回<spanclass="math inline">\(1-u_{i}\cdot u_{j}^{-}\)</span>（当<spanclass="math inline">\(u_{i}\cdotu_{j}^{-}\)</span>小于t−时）来筛选负像素对，否则返回零；N−统计负像素对的总数，而h−是一个单调递增函数。最小化<spanclass="math inline">\(L^{-}\)</span>会使两个像素在标签空间中进一步分离，前提是它们在ViT嵌入空间中距离更远。</p><h3 id="抗噪">3.3.2抗噪</h3><p>​  像素对之间的亲和力存在噪声干扰，因为不同类别的两个像素可能具有较高的亲和性，如图2所示。我们探索了h+（·）和h−（·）的不同表达形式，并分析了它们对噪声的容忍能力。<br/>​  对数函数广泛应用于概率相关的损失函数中，例如交叉熵损失函数。我们考虑：<span class="math display">\[\begin{array}{l}{h^{+}(v_{i}\cdotv_{j}^{+})=-\log(v_{i}\cdot v_{j}^{+})}\\ {h^{-}(v_{i}\cdotv_{j}^{-})=-\log(1-v_{i}\cdot v_{j}^{-})}\end{array}\]</span>​  对数函数之所以适用于有良好标注数据的监督学习，是因为它会对错误分类施加渐近无限大的惩罚。但在USS（无监督学习）场景中，噪声是无法避免的。同一类别的像素对可能具有较小的相似度，而不同类别的像素对则可能具有较大的相似度。在这种情况下使用对数函数会导致梯度过大且方向错误。由于惩罚项和梯度值过高，网络将无法有效忽略噪声，从而严重干扰其学习过程。<br/>​  上述分析促使我们寻找一个定义域为[0,1]且值域有限的函数。满足该条件的最简单函数就是线性函数。我们考虑：<span class="math display">\[\begin{array}{l}{h^{+}(v_{i}\cdotv_{j}^{+})=1-v_{i}\cdot v_{j}^{+}}\\ {h^{-}(v_{i}\cdotv_{j}^{-})=v_{i}\cdot v_{j}^{-}}\end{array}\]</span>​  线性函数具有恒定的斜率，无论损失值如何变化，其梯度始终保持一致，这意味着两个像素点的亲和力与其类别之间存在不一致性。与鲁棒回归中使用的l1-范数（徐等人，2008年）类似，只要噪声量不过大，线性函数就能容忍噪声的存在。但潜在的问题在于，由于损失函数对所有案例一视同仁，优化简单案例反而更容易，这可能导致学习过程过度关注简单案例而忽视复杂案例，从而阻碍有效学习。<br/>​  最后，我们考虑一个指数函数：<span class="math display">\[\begin{array}{c}{h^{+}(v_{i}\cdotv_{j}^{+})=\mathrm{exp}(1-v_{i}\cdot v_{j}^{+})}\\ {h^{-}(v_{i}\cdotv_{j}^{-})=\mathrm{exp}(v_{i}\cdot v_{j}^{-})}\end{array}\]</span>​  与线性函数类似，指数函数在定义域[0,1]内具有有限的输出值范围和梯度。但与线性函数不同的是，指数函数在损失值较大时具有更大的梯度，因此对困难案例会施加更大的压力。<br/>​  总体而言，线性函数和指数函数相比对数函数都具有更强的噪声容忍度，这得益于它们在输出值范围和[0,1]定义域内的梯度分布特性。相较于线性函数，指数函数更侧重于处理困难案例而非简单案例。在USS算法中，我们无法区分困难案例与噪声干扰。但实验表明，指数函数的表现优于线性函数，两者均以显著优势超越对数函数。究其原因，可能在于噪声容忍度与困难案例处理能力对有效学习都至关重要，而指数函数在这两方面实现了良好的平衡。</p><h2id="解决集群退化和像素类别不平衡问题">3.4.解决集群退化和像素类别不平衡问题</h2><p>​  虽然目前提出的模型可以用于学习USS，但仍有两个关键问题亟待解决。首先是聚类退化问题：经常会出现空簇现象。其次是像素类别不平衡问题：现实世界中物体大小和出现频率不同，导致像素类别分布严重失衡。<br/>​  先前的判别聚类方法（Ji等人，2019；Krause等人，2010；Ouali等人，2020；VanGansbeke等人，2020）通过基于熵的正则化器来解决聚类退化问题： <spanclass="math display">\[\begin{array}{c}{L^{\mathrm{enropy}}=\sum_{c}p_{c}\log(p_{c}),}\\{\mathrm{where}\;p_c=\frac{1}{N}\sum_{\foralli}v_{i,c}}\end{array}\]</span>​  其中，i和c分别表示样本和类别的索引，<spanclass="math inline">\(v_{i,c}\)</span>是第i个样本上第c类的预测概率，N是样本总数，pc是训练数据中第c类出现的频率。该方法通过强制类别均匀分布来避免空聚类。<br/>​  然而现实世界中像素的类别分布往往呈现严重偏斜而非均匀状态。以城市景观数据集为例，道路类像素占比极高，其数量比稀有类别多出上千倍。这种类别失衡问题在USS研究中长期未被重视。<br/>​  为了解决类别退化和类别分布不均的问题，我们引入了一个正则化项来规范学习过程。这个正则化项有两个作用。首先，它对存在空簇的情况施加了重大惩罚。其次，它容忍了像素类分布的偏斜。</p><figure><imgsrc="../postimages/Imbalance-Aware-Discriminative-Clustering-for-Unsupervised-Semantic-Segmentation/image-20250815170544306.png"alt="image-20250815170544306" /><figcaption aria-hidden="true">image-20250815170544306</figcaption></figure><blockquote><p>图3展示了基于Weibull函数（WB）和熵两种正则化方法的对比分析。横轴表示罕见类别与常见类别像素数量的比值，该比值范围在0.001到1之间。两种正则化方法均经过归一化处理，确保其上限值均为单位量级。</p></blockquote><p>​  如图3所示，当x轴表示两类像素的分布比例时，L形函数能完美契合上述双重目标。一方面，“L”的垂直部分会产生较大损失值，有效抑制空簇的出现；另一方面，“L”的水平部分在类别分布不均衡时会产生接近零的损失值，使像素能够根据其在嵌入空间中的距离进行聚类。我们将正则化项建模为韦布尔函数（Murthy等，2004；Weibull，1951），因其形状参数<spanclass="math inline">\(k\in(0,1)\)</span>可灵活调整且具有可微性。k值越小，Weibull分布越接近L形曲线。具体而言，本文提出的正则化项可表示为：<span class="math display">\[{L}^{\mathrm{Weibull}}=\sum_{c}kp_{c}^{k-1}\exp(-p_{c}^{k})\]</span>​  从图3可以观察到，与聚类中广泛使用的熵正则化器（范·甘斯贝克等人，2020年）类似，当出现空簇时（即比例接近零的情况），威布尔正则化器会产生较大的惩罚。但与熵正则化器不同的是，当类别不平衡时（即比例远低于1的情况），Weibull正则化器产生的惩罚会比熵正则化器小得多。</p><h2 id="整体目标函数">3.5.整体目标函数</h2><p>​  IDDC的总体目标函数结合了第3.3节中的基本目标函数和第3.4节中的Weibull正则化项：<spanclass="math display">\[L=L^{+}+\lambda_{1}L^{-}+\lambda_{2}L^{\mathrm{weisull}}\]</span>​  其中λ1和λ2是权衡超参数。L+和L−通过将ViT嵌入空间中像素的流形结构迁移至标签空间，并容忍像素亲和力中的噪声，为语义分割学习提供了有效的训练信号。<spanclass="math inline">\({L}^{\mathrm{Weibull}}\)</span>方法有效避免了聚类退化问题，并解决了像素类别不平衡的问题。</p><h1 id="实验">4.实验</h1><h2 id="数据集和评估指标">4.1.数据集和评估指标</h2><h2 id="实施细节">4.2.实施细节</h2><h3 id="网络结构">4.2.1网络结构</h3><p>​  所有数据集的骨干网络均采用基于ViT的模型（Touvron等人，2021）。这些模型通过无监督预训练方法进行训练，包括DINO（Caron等人，2021)和iBoT（Zhou等人，2021)在ImageNet数据集上进行无标签预训练（Deng等人，2009）。分割头由两个ReLU激活的卷积层和一个Softmax激活的卷积层组成。</p><h3 id="训练">4.2.2训练</h3><p>​  我们的方法基于Pytorch框架实现。使用Adam优化器训练模型，批量大小设为64。头网络初始学习率设为5e−4，骨干网络初始学习率设为5e−7。采用多项式学习率策略：初始学习率乘以<spanclass="math inline">\((1−iter/max_iter)^{power}\)</span>，其中power取值为0.9。针对COCO-Stuff-27、COCO-Stuff-171和Cityscapes数据集的训练轮次分别为5、20和50轮，批量大小保持64。在非端到端训练中，骨干网络固定而头网络可调；在端到端设置中，骨干网络在前2/5和最后1/5训练轮次保持固定。前者避免了随机初始化头网络反向传播对预训练骨干网络的干扰，后者确保监督信号稳定以保证学习过程。输入图像会随机调整尺寸（比例在0.8至1.2之间）、随机翻转，并裁剪为224×224尺寸。遵循先前研究（VanGansbeke等人，2021；Hamilton等人，2022；Ji等人，2019；Cho等人，2021），我们将聚类数量设为真实类别数。单张NVIDIAV100 GPU卡上的训练耗时不足两小时，因此IDDC算法具有较高的运行效率。</p><h3 id="超参数">4.2.3超参数</h3><p>​  在COCO-Stuff-27数据集的ViT-S/8实验中，目标函数等式(8)中的λ1和λ2参数分别设置为1.4和0.25。用于筛选正负像素对的两个阈值t+和t−则设定为0.2和0.12。ViT-S/16实验中，这三个参数的取值分别为1.4、0.4、0.2和0.12。对于COCO-Stuff-171数据集，对应的参数配置为9.0、0.61、0.15和0.15。在Cityscapes数据集的ViT-B/8版本中，这些参数取值为0.38、0.22、0.2和0.28；而在ViT-S/8版本中则调整为0.25、0.3、0.15和0.25。我们已通过大量消融实验（详见第4.4节）验证了这些超参数的影响。</p><h2 id="与最先进方法的比较">4.3.与最先进方法的比较</h2><h2 id="消融研究">4.4.消融研究</h2><h2 id="可视化">4.5.可视化</h2><h1 id="结论">5.结论</h1><p>​  本文提出了一种名为“不平衡感知密集判别聚类（IDDC）”的创新方法，用于无监督语义分割（USS）。该方法通过端到端自监督学习，直接建模每个像素在图像条件下的分类概率，同时实现像素级特征表示与密集判别聚类的同步优化。我们创新性地提出一个目标函数，通过将ViT嵌入空间中像素的流形结构迁移至标签空间来实现IDDC学习，该方法不仅能容忍像素亲和力中的噪声干扰，还通过引入新型韦布尔正则化器有效解决像素类别不平衡问题。IDDC成功突破了传统方法在表征学习与像素聚类协调、异常值处理、噪声抑制及类别分布偏斜建模等方面的局限。在两个大规模真实数据集上，IDDC的性能显著优于所有现有最优方法。大量消融实验验证了各设计要素的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 无监督语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DFPD</title>
      <link href="/DFPD/"/>
      <url>/DFPD/</url>
      
        <content type="html"><![CDATA[<p>DFPD: Dual-Forgery Proactive Defense against Both Deepfakes andTraditional Image Manipulations</p><h1 id="摘要">摘要</h1><p>​  主动防御人脸伪造旨在通过将难以察觉的对抗扰动嵌入到要保护的人脸图像中来破坏伪造模型的输出。然而，现有的方法主要侧重于深度伪造，往往忽视了传统的图像处理。这限制了它们的实际适用性，因为当deepfake尝试失败时，攻击者可能会诉诸于传统的操作。为了弥合这一差距，提出了一种双重伪造主动防御（DFPD）方法，用于打击深度伪造和传统图像处理。为了抵抗深度伪造，DFPD设计了一种基于梯度的集成对抗攻击，有效地破坏了多个深度伪造模型的输出。为了克服传统的操作，还设计了一种基于可逆神经网络（INN）的脆弱水印算法，能够准确定位篡改区域。此外，为了减轻扰动注入和水印嵌入之间的相互干扰，一方面，DFPD采用了一种串行流水线，从水印嵌入开始，然后进行扰动注入，这确保了在基于INN的嵌入过程中，注入的扰动不会移位到残差图像中。另一方面，引入形态学后处理模块来消除篡改定位结果中的对抗噪声。大量实验验证了DFPD的有效性，表明在PSNR方面，深度伪造干扰比最佳基线提高了20.25%，在ACC方面，传统篡改定位提高了9.67%，同时保持了高感知质量（32.75dB PSNR）。</p><p>code：<ahref="https://github.com/imagecbj/DFPD">imagecbj/DFPD</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks</title>
      <link href="/fasterRCNN/"/>
      <url>/fasterRCNN/</url>
      
        <content type="html"><![CDATA[<p>Faster R-CNN: Towards Real-Time Object Detection with Region ProposalNetworks</p><p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun</p><h1 id="摘要">摘要</h1><p>​  最先进的对象检测网络依赖于区域建议算法来假设对象位置。SPPnet和FastR-CNN等技术的进步缩短了这些检测网络的运行时间，暴露出区域建议计算是一个瓶颈。在这项工作中，我们引入了一个区域建议网络（RPN），它与检测网络共享完整的图像卷积特征，从而实现了几乎免费的区域建议。RPN是一个完全卷积的网络，可以同时预测每个位置的对象边界和对象性得分。RPN经过端到端训练，生成高质量的区域建议，由FastR-CNN用于检测。我们通过共享它们的卷积特征，将RPN和FastR-CNN进一步合并为一个网络——使用最近流行的具有“注意力”机制的神经网络术语，RPN组件告诉统一网络该去哪里看。对于非常深的VGG-16模型，我们的检测系统在GPU上的帧速率为5fps（包括所有步骤），同时在PASCALVOC 2007、2012和MSCOCO数据集上实现了最先进的对象检测精度，每张图像只有300个建议。在ILSVRC和COCO2015比赛中，FasterR-CNN和RPN是几个赛道获得第一名的基础。代码已公开。</p><p>​  Faster RCNN论文原文中算法整体结构如下：</p><figure><img src="../postimages/fasterRCNN/image-20250810172419156.png"alt="image-20250810172419156" /><figcaption aria-hidden="true">image-20250810172419156</figcaption></figure><p>​  如图，Faster R-CNN算法流程主要包括四个部分，分别是卷积层（ConvLayers）、区域建议网络（RPN）、感兴趣区域池化（RoIPool）和检测网络（Classifier）。各部分功能如下：</p><ol type="1"><li>卷积层：卷积层是输入图像的特征提取器，作用是提取输入图像的全图特征，用于RPN推荐区域生成和RoI区域池化。卷积层可以采用多种网络结构来实现，比如Vgg/ResNet，本文代码实现部分采用的是Vgg16部分结构；<br/>2.RPN：RPN作用是生成候选框。RPN在特征图上滑动，对特征图上每个位置生成一定数量的先验框anchors，再结特征图分类和回归结果对先验框位置进行校正，从中选取符合要求的候选区域proposals用于区域池化和目标检测；<br/>3.区域池化：区域池化的作用是根据proposals区域位置在特征图上进行特征截取，并将截取的特征缩放到固定大小以便于检测网络进行分类和回归；<br/>4.检测网络：检测网络是最终对proposals进行分类和回归的全连接网络，用于对proposals进行更为精确的分类和位置回归。</li></ol><h1 id="faster-rcnn算法实现">Faster RCNN算法实现</h1><p>​  图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像：</p><ul><li>首先缩放至固定大小MxN，然后将MxN图像送入网络；</li><li>而Conv layers中包含了13个conv层+13个relu层+4个pooling层；</li><li>RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding boxregression偏移量，然后计算出proposals；</li><li>而Roi Pooling层则利用proposals从feature maps中提取proposalfeature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。</li></ul><figure><img src="../postimages/fasterRCNN/64ce08be6eb7e00429fdd07d43e87199.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="region-proposal-networksrpn">Region Proposal Networks(RPN)</h2><p>​  经典的检测方法生成检测框都非常耗时，如OpenCVadaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(SelectiveSearch)方法生成检测框。而FasterRCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是FasterR-CNN的巨大优势，能极大提升检测框的生成速度。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>examine</title>
      <link href="/examine/"/>
      <url>/examine/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, 密码错了，检查一下好吗～" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c27a164a2298a77153f1e1a2f92d05ba7dc783ddfec9de738e1d6effd62438b6">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, 这篇文章被加密了，请输入密码！</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mesoscopic Insights:Orchestrating Multi-Scale &amp; Hybrid Architecture for Image Manipulation Localization</title>
      <link href="/Mesoscopic-Insights/"/>
      <url>/Mesoscopic-Insights/</url>
      
        <content type="html"><![CDATA[<p>Mesoscopic Insights: Orchestrating Multi-Scale &amp; HybridArchitecture for Image Manipulation Localization</p><p>Xuekang Zhu1,2<em>，Xiaochen Ma1,2</em>， Lei Su1,2，ZhuohangJiang3，Bo Du1,2，Xiwen Wang1,2，Zeyu Lei1,2，Wentao Feng1,2，Chi-ManPun4，Ji-Zhe Zhou1,2†</p><p>1计算机学院，四川大学<br/>2机器学习与产业智能工程研究中心，中国教育部<br/>3香港理工大学<br/>4澳门大学科技学院计算机与资讯科学系</p><h1 id="摘要">摘要</h1><p>​  介观层面作为宏观与微观世界的桥梁，填补了两者长期忽视的空白。图像操控定位（IML）作为从虚假图像中探寻真相的关键技术，长期以来依赖于微观层面的痕迹信息。然而在实际应用中，大多数篡改行为旨在通过改变图像语义来欺骗观众。因此，图像操控通常发生在物体层面（宏观层面），这与微观痕迹同样重要。将这两个层面整合到介观层面，为IML研究提供了全新视角。受此启发，本文探索如何同时构建IML所需的微观与宏观信息介观表征，并提出Mesorch架构来协调二者。具体而言，该架构具有两大核心特点：首先，它将Transformer与CNN进行并行融合——Transformer负责提取宏观信息，CNN则捕捉微观细节；其次，它能在不同尺度间灵活切换，实现微观与宏观信息的无缝衔接。基于介观表示架构，本文还提出了两种基线模型，专门用于解决中观表示任务。通过在四个数据集上的大量实验验证，我们的模型在性能表现、计算复杂度和鲁棒性方面均超越了现有最优方案。</p><p>代码—https://github.com/scu-zjz/Mesorch<br/>扩展版本—https://arxiv.org/abs/2412.13753</p><h1 id="方法">方法</h1><figure><img src="../postimages/Mesoscopic-Insights/image-20250806203021731.png"alt="image-20250806203021731" /><figcaption aria-hidden="true">image-20250806203021731</figcaption></figure><p>​  输入的RGB图像在DCT模块中分别进行高频和低频处理，生成对应的高频与低频表征。局部特征模块通过结合原始图像与高频图像，专注于检测细粒度操作；而全局特征模块则利用原始图像与低频图像捕捉对象级别的篡改线索。自适应加权模块通过为局部特征和全局特征分配像素级权重，动态整合这些图像信息。最终的组合特征用于预测，并与真实标签进行比较以计算损失。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>​  <strong>训练</strong><br/>​  我们的模型采用标准化的Protocol-CAT数据集进行训练，该数据集由代码库(Maet al.2024)提供。该协议包含已建立的数据集和典型的数据增强方法。所有图像均被调整为512x512像素尺寸。我们在四块英伟达4090显卡上进行了150个训练周期的实验，采用12的批量大小。学习率遵循余弦曲线调度方案(Loshchilovand Hutter2017)，初始值为1e-4并逐步衰减至最低5e-7，同时设置了2个训练周期的预热期以平稳调整学习率。为防止过拟合，我们使用AdamW优化器并设置权重衰减系数0.05。此外，将累积迭代次数设为2次，通过动态调整批量大小来增强模型对多样化数据输入的泛化能力。</p><p>​  <strong>测试</strong><br/>​  我们采用国际公认的基准测试IMDLBenCo(Maet al. 2024)对模型进行评估，测试集包括四个主流数据集：CASIAv1 (Dong,Wang and Tan 2013)、Coverage (Wen et al. 2016)、NIST16 (Guan et al.2019)以及Columbia (Hsu and Chang2006)。这些数据集因其多样化的挑战性而广受认可，是检验图像处理定位方法泛化能力的重要评估平台。</p><p>​  <strong>量度</strong><br/>​  与测试环节保持一致，我们沿用了业界公认的评估基准，采用标准像素级F1分数来衡量定位性能。所有结果均在0.5的默认阈值下计算得出，能够全面反映定位精度的优劣。</p><h2 id="与最新方法比较">与最新方法比较</h2><p>​  为确保评估的准确性，我们使用开源代码在各论文推荐的分辨率下训练模型，并采用CAT-Net(Kwonetal.2022)协议数据集。随后通过F1分数对模型性能进行跨多个权威数据集的基准测试。对比分析涵盖多种方法，包括PSCC-Net(Liuet al. 2022a)、MVSS-Net(Chen et al.2021),、CAT-Net(Kwon et al.2022)以及Trufor(Guillaroet al. 2023)。</p><p>​  <strong>定位结果</strong><br/>​  在表1中，我们用加粗字体标注了表现最佳的模型，并用下划线标注了次优模型，这些结果均基于所有评估数据集得出。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806203215466.png"alt="image-20250806203215466" /><figcaption aria-hidden="true">image-20250806203215466</figcaption></figure><p>​  无论是剪枝前还是剪枝后的表现，我们提出的方法始终稳居前列，要么拔得头筹，要么位列第二。这种稳定的表现力充分证明了我们的方法在处理各类图像操作定位任务时具有精准性。此外，图4从定性角度展示了模型成功捕捉到物体布局和介观层面细节的能力，最终生成的被篡改掩膜具有极高的精确度。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806203849712.png"alt="image-20250806203849712" /><figcaption aria-hidden="true">image-20250806203849712</figcaption></figure><p>​  <strong>鲁棒性能</strong><br/>​  为评估模型在不同条件下的鲁棒性，我们在CASIAv1数据集上进行了测试，并将结果汇总于表2。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806210218523.png"alt="image-20250806210218523" /><figcaption aria-hidden="true">image-20250806210218523</figcaption></figure><p>​  我们引入了标准差各异的高斯噪声、不同核尺寸的高斯模糊处理，以及采用多种质量因子的JPEG压缩作为扰动手段。实验结果表明，我们的模型在这三种扰动方式下均保持了业界领先的鲁棒性表现。值得注意的是，即使经过剪枝处理，我们的方法仍比所有先前的模型保持了更高的鲁棒性，这证明了它在处理各种图像失真方面的有效性。</p><p>​  <strong>浮点运算和参数</strong><br/>​  所有测量的参数数量和浮点运算次数（FLOPs）均基于512x512分辨率和1批次大小进行计算。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806210350812.png"alt="image-20250806210350812" /><figcaption aria-hidden="true">image-20250806210350812</figcaption></figure><p>​  如表3所示，我们的实验结果表明：该模型的FLOPs消耗量低于所有当前最先进的模型，其参数数量仅次于PSCC-Net。此外，通过应用我们的剪枝方法进一步降低了FLOPs和总参数数量，使得本模型相比现有顶尖模型具有显著优势。</p><h2 id="消融研究">消融研究</h2><p>​  <strong>对拟定方法的独立评价</strong><br/>​  为验证本文提出方法的有效性，我们首先独立评估了CNN的ConvNeXt架构和Transformer的Segformer架构作为基线模型，并在多尺度条件下对其性能进行测试。在确立基线性能后，我们进一步评估了混合模型，逐步整合多尺度方法、加权算法和DCT特征提取技术。同时对剪枝模型进行了测试（如表4所示）。实验结果表明，本文提出的每个组件对于精准定位图像篡改都具有关键作用。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806210555288.png"alt="image-20250806210555288" /><figcaption aria-hidden="true">image-20250806210555288</figcaption></figure><p>​  <strong>模型体系结构的比较分析</strong><br/>​  我们通过两种CNN模型(Resnet-50(He et al. 2016) and ConvNeXt-Tiny (Liu et al.2022b))与四种Transformer模型(MAE-Base (He et al. 2022), PvT-B3 (Wang etal. 2022b), Segformer-B3 (Xie et al. 2021),以及SwinTransformer-Base (Liuet al.2021))的不同组合，对Mesorch架构的性能进行了评估。各模型间的性能差异总结于第5部分。图5则通过定性分析展示了该架构在不同骨干网络下的表现。</p><figure><img src="../postimages/Mesoscopic-Insights/image-20250806210831745.png"alt="image-20250806210831745" /><figcaption aria-hidden="true">image-20250806210831745</figcaption></figure><p>​  研究结果表明，ConvNeXt与Segformer的组合在宏观定位和微观特征捕捉方面均表现出色，其综合性能超越了其他所有模型组合。</p><h1 id="结论">结论</h1><p>​  受介观视角启发，本文重新定义了IML任务，旨在协调微观与宏观层面的研究。在此基础上，我们提出Mesorch架构，一种融合卷积神经网络和Transformer模型优势的混合模型，通过动态调整尺度权重，能高效捕捉介观层面的伪影特征。为降低参数量和计算成本，我们还基于该架构引入了两个基准模型。在大规模数据集上的广泛测试表明，我们的方法在F1分数、鲁棒性和运算量等指标上始终保持着业界领先水平。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LSNet:See Large, Focus Small</title>
      <link href="/LSNet-See-Large-Focus-Small/"/>
      <url>/LSNet-See-Large-Focus-Small/</url>
      
        <content type="html"><![CDATA[<p>LSNet: See Large, Focus Small</p><p>Ao Wang1 Hui Chen2* Zijia Lin1 Jungong Han3 Guiguang Ding1</p><p>1清华大学软件学院<br/>2清华大学BNRist<br/>3清华大学自动化系</p><h1 id="摘要">摘要</h1><p>​  视觉网络设计，包括卷积神经网络和视觉变换器，显著推动了计算机视觉领域的发展。然而，其复杂的计算过程给实际应用带来了挑战，特别是在实时场景中。为解决这一问题，研究人员探索了多种轻量级高效网络架构。但现有轻量模型主要依赖自注意力机制和卷积操作进行特征融合，这种过度依赖导致轻量网络在感知与聚合过程中效率受限，在计算资源有限的情况下难以实现性能与效率的平衡。本文从高效人类视觉系统中固有的动态异尺度视觉能力中获得启发，提出一种“广域感知+局部聚焦”的轻量级视觉网络设计策略。我们创新性地引入LS(Large-Small)卷积结构，该结构融合了大核感知与小核聚合特性。这种设计能高效捕捉多维度感知信息，并对动态复杂视觉表征实现精准特征聚合，从而实现视觉信息的高效处理。基于LS卷积架构，我们提出了LSNet这一全新轻量级模型家族。大量实验表明，LSNet在多种视觉任务中展现出比现有轻量级网络更优的性能和效率。相关代码和模型可在https://github.com/jameslahm/lsnet获取。</p><h1 id="引言">1. 引言</h1><p>​  视觉网络设计始终是计算机视觉领域的研究热点[17,22,24,50,51,96]。其中，卷积神经网络（CNN）[24,29,38,39,51]和视觉变换器（ViTs）[17,50,63,74,88,93]两大主流架构，在各类计算机视觉任务中不断突破边界[3,4,23,70,82,84,92]。然而，这两种架构在传统计算上都存在显著瓶颈，给实际应用带来巨大挑战——特别是在实时场景中，其性能表现尤为突出[44,49]。<br/>​  近期，科研人员正致力于探索视觉网络的轻量化与高效化设计[7,34,55,57,60,76]以满足实际应用需求。尽管这些轻量级模型效果显著，但其底层架构仍需依赖自注意力机制[17,77,86]和卷积层[38,39]等基础模块来实现特征融合技术。这种对基础模块的过度依赖，往往会影响网络感知与聚合过程的效率与效果，导致架构表达能力受限或推理速度下降。<br/>​  本质上，上下文感知与特征聚合是实现标记混合的核心机制[19,73,91]，能够有效促进空间信息融合。感知模型通过分析标记间的上下文关联，而聚合机制则基于这些关联整合特征。在现有的轻量级模型中，两种主流的标记混合方法——自注意力机制和卷积网络——分别采用不同的感知与聚合方式。具体而言，自注意力机制通过全局特征交互实现整体感知，并借助特征加权求和完成特征聚合。卷积方法通过感知标记之间的相对位置关系，并使用静态核权重聚合特征。然而，如图1(a)和(b)所示，这两种方法都存在局限性。</p><figure><imgsrc="../postimages/LSNet-See-Large-Focus-Small/image-20250806114142065.png"alt="image-20250806114142065" /><figcaption aria-hidden="true">image-20250806114142065</figcaption></figure><p>​  (1)自注意力机制容易对缺乏显著连接的区域过度关注，导致关键信息的聚合效果降低，例如在信息量较少的背景[46,65]中。此外，其感知与聚合过程共享相同的混合范围。自注意力机制及其变体在扩展上下文时，[19,33,49]会显著增加计算复杂度。这种特性使得轻量级模型难以在有限的计算资源下实现高表征能力。</p><p>​  (2)在卷积神经网络中，感知模型所建模的标记间关系（即聚合权重）由固定核权重决定。因此，尽管卷积网络效率高，但对上下文邻域的变化缺乏敏感性。这给轻量级模型的表达能力带来了限制，特别是考虑到这类网络本身存在固有局限性。在此背景下，探索一种能在有限计算成本下实现更高效感知与聚合处理的轻量级模型特征融合方案显得尤为重要。</p><p>​  为此，我们首先对感知和聚合过程背后的直觉进行了彻底的检验。我们发现，它们与高效人类视觉系统中的动态异尺度视觉能力现象高度吻合。</p><figure><imgsrc="../postimages/LSNet-See-Large-Focus-Small/image-20250806114559985.png"alt="image-20250806114559985" /><figcaption aria-hidden="true">image-20250806114559985</figcaption></figure><p>​  具体来说，如图1(c)所示，人类视觉系统遵循双阶段机制：<br/>​  (1)首先通过周边视觉的大视野感知[62,69]捕捉场景的宏观概览，即“看大”；<br/>​  (2)随后将注意力转向场景中的特定元素，借助中央视觉的小视野聚合能力[59,69]实现细节理解，即“聚焦小”。<br/>​  这种视觉特性源于视网膜[36,62]中两种感光细胞——杆状细胞和锥状细胞——独特的空间分布与视觉能力，如图1(d)所示。杆状细胞广泛分布于视网膜边缘区域[59]，生成的图像相对模糊且空间细节有限[78]。不过它们能对可见光谱产生广泛响应，与视网膜边缘的锥状细胞协同工作，共同实现大视野周边视觉能力[72]，从而让人“看清楚全局”。而锥状细胞则主要集中在中央凹区域，这个小范围负责中央视觉功能[87]。黄斑区密集分布着视锥细胞，这些细胞构成了捕捉精细细节和复杂特征的最敏锐区域[35,75,78]，从而实现“聚焦微小”。在周边感光细胞高效的大视野感知引导下，黄斑区能够通过小范围集中聚焦，精准捕捉细微特征的影像[62]。这种“放眼全局，聚焦细节”的方法使人类视觉系统能够快速有效地处理视觉信息[78]，从而促进准确和高效的视觉理解。</p><p>​  这些研究需求促使我们设计出既能感知大范围视觉场景又能整合局部细节的高效视觉网络。为此，我们首创了“大-小”（LS，<strong>L</strong>arge-<strong>S</strong>mall）卷积运算机制，该机制模拟人类视觉系统中“广角捕捉+微距聚焦”的核心策略，从而有效提取具有区分度的视觉特征模式。LS卷积通常采用大核静态卷积实现大视野感知，小核动态卷积用于小视野聚合。该方法并非简单叠加两种卷积方式，而是通过大核深度卷积捕捉广泛上下文信息来建模空间关系。随后，基于这些参数构建具有群组机制的小核动态卷积操作，从而在高度相关的视觉域内实现特征融合。通过这种方式，大核静态卷积能有效捕捉扩展的邻域信息，从而提升关系建模能力，类似于周边视觉系统。此外，得益于这一特性，小核动态卷积可自适应地整合微小环境中的复杂视觉特征，实现精细的视觉理解，如同中心视觉系统般精准。与此同时，我们巧妙地结合深度卷积与分组机制，对LS卷积进行了高效设计。聚合范围被限制在一个小区域内。这种设计有效确保了感知和聚合过程的低复杂度。因此，我们的LS卷积在性能和效率之间取得平衡，使得轻量级模型能够在低计算成本下充分发挥表征能力。<br/>​  我们将LS卷积视为标记混合的核心运算单元，并将其与其他主流架构设计相结合，构建出LS模块。在此基础上，我们提出了一类新型轻量级模型——LSNet。大量实验表明，在多种视觉任务中，LSNet相比现有最先进轻量级模型展现出更优的性能和效率[11,47,99]。我们期待LSNet能成为强有力的基准模型，为轻量化高效模型领域的发展注入新动力。</p><h1 id="相关工作">2. 相关工作</h1><p>​  <strong>EfficientCNNs.</strong><br/>​  过去十年间，卷积神经网络（CNN）已成为各类视觉任务的基础架构[2,15,16,52,66,79]。为提升实际应用效果，研究者们投入大量精力设计轻量级高效网络[12,13,30,31,53,71,81]。例如，MobileNet[31]和Xception [8]采用深度可分离卷积架构。MobileNetV2[67]通过引入带线性瓶颈的反向残差块来提升效率。ShuffleNet[98]和ShuffleNetV2[53]则结合通道混洗与通道分割操作以增强组信息交互。针对硬件特性，研究者还探索了硬件感知神经架构搜索（NAS）以获得紧凑型视觉网络[30,71]。同时，考虑到有限的感受野范围，部分研究致力于增强轻量CNN对长程依赖关系的建模能力[34,61,95]。例如，ParC-Net[95]引入位置感知循环卷积以实现全局感受野，AFFNet[34]则通过循环填充实现全局卷积的自适应频率滤波。</p><p>​  <strong>EfficientViTs.</strong><br/>​  自VisionTransformer[17]问世以来，基于Transformer架构在计算机视觉领域迅速崛起。ViTs已成功应用于多种视觉任务，并展现出卓越性能[18,97]。与此同时，研究者们不断优化其效率，开发出适用于实际部署的轻量级ViTs模型[44,58,76,80]。例如，MobileViT[57]通过融合MobileNet模块与MHSA模块，构建出混合架构；EdgeViT[60]创新性地将自注意力机制与卷积运算相结合，实现了成本效益型信息交互；为缓解推理瓶颈，EfficientFormer[44]提出维度一致性设计范式，有效平衡了延迟与性能的权衡关系；而FastViT[76]则通过结构化参数重配置和大核卷积技术，进一步提升了混合ViTs的性能表现。</p><p>​  <strong>Efficient TokenMixing.</strong><br/>​  CNN和ViT分别采用不同的特征混合方式，即卷积与自注意力机制，并结合独特的感知与聚合过程。基于这些方法，为开发轻量级视觉网络，研究者探索了多种高效的空间信息交互特征混合方式。例如：卷积层方面，反卷积[41]通过多层感知机（MLP）获取单像素条件下的聚合权重；CondConv[90]提出基于全局上下文的逐例路由策略，实现多卷积核的线性组合；自注意力机制中，EdgeNeXt[55]创新性地采用分层深度转置注意力（SDTA）混合多尺度特征；PVTv2[85]通过线性空间归一化注意力（LSRA，linear spatial reductionattention）实现注意力层的线性计算复杂度；而EfficientViT[49]则设计了级联群注意力机制，显著提升网络性能。</p><h1 id="方法">3. 方法</h1><h1 id="重新审视自注意力和卷积">3.1.重新审视自注意力和卷积</h1><p>​  自注意力机制与卷积操作是现有轻量级网络中用于建模视觉特征的两种主流标记混合方法[93]。对于输入图像，其特征图<spanclass="math inline">\(X\in{\mathbb R}^{H\times W\timesC}\)</span>（其中H×W表示空间分辨率，C为通道数），标记混合通过以下方式，基于上下文区域<spanclass="math inline">\(\mathcal N(x_i)\)</span>，为每个标记<spanclass="math inline">\(x_i\in{\mathbb R}^C\)</span>生成特征表示<spanclass="math inline">\(y_i\in{\mathbb R}^C\)</span>： <spanclass="math display">\[y_i={\mathcal A}({\mathcal P}(x_i,{\mathcalN}(x_i)),{\mathcal N}(x_i))\]</span> ​  其中，<spanclass="math inline">\({\mathcalP}\)</span>表示感知，涉及提取上下文信息并捕捉标记之间的关系；<spanclass="math inline">\({\mathcalA}\)</span>表示聚合，基于感知结果整合特征，并允许整合来自其他标记的信息。</p><figure><imgsrc="../postimages/LSNet-See-Large-Focus-Small/image-20250806115739709.png"alt="image-20250806115739709" /><figcaption aria-hidden="true">image-20250806115739709</figcaption></figure><p>​  在<strong>自注意力机制</strong>中，感知层<spanclass="math inline">\({\mathcalP}_{attn}\)</span>通过softmax归一化后的成对相关性计算<spanclass="math inline">\(x_i\)</span>与X之间的注意力分数。聚合层<spanclass="math inline">\({\mathcalA}_{attn}\)</span>则根据这些注意力分数对X的特征进行加权处理，最终得到<spanclass="math inline">\(y_i\)</span>。如图2(a)所示，整个过程可概括为：<span class="math display">\[y_{i}=\mathcal{A}_{a t t n}(\mathcal{P}_{at t n}(x_{i},X),X)=\mathcal{P}_{a t t n}(x_{i},X)(X W_{v});\]</span></p><p><span class="math display">\[\mathcal{P}_{a t tn}(x_{i},X)=\mathrm{sofmax}((x_{i}W_{q})(X W_{k})^{T}),\]</span></p><p>​  其中Wq、Wk和Wv是投影矩阵。可以观察到，<spanclass="math inline">\(\mathcal{P}_{a t t n}\)</span>和<spanclass="math inline">\(\mathcal{A}_{a t tn}\)</span>在信息量较少的区域[46,65]中存在冗余注意力机制和过度聚合现象，这限制了轻量级模型的效能。此外，它们对xi采用相同的上下文尺度进行操作。这种同尺度特性会导致当混合范围<spanclass="math inline">\(\mathcalN(x_i)\)</span>增加时计算复杂度显著上升，在低计算预算下扩展感知上下文面临挑战。因此，现有轻量级模型中的自注意力机制及其变体[19,49]难以在有限计算成本[34]下实现表征能力与效率之间的最佳平衡。</p><p>​  对于<strong>卷积</strong>核大小为K的情况，上下文区域是围绕xi的K×K邻域，记作<spanclass="math inline">\(\mathcal N_K(x_i)\)</span>。感知器<spanclass="math inline">\(\mathcal{P}_{c o n v}\)</span>通过计算<spanclass="math inline">\(x_i\)</span>与<span class="math inline">\(\mathcalN_K(x_i)\)</span>之间的相对位置关系，来确定聚合权重。对于每个<spanclass="math inline">\(x_j\in\mathcalN_K(x_i)\)</span>，其聚合权重是固定卷积核权重<spanclass="math inline">\(W_{conv}\)</span>中对应相对位置的数值。聚合层<spanclass="math inline">\({\mathcal A}_{conv}\)</span>随后利用权重对<spanclass="math inline">\(\mathcalN_K(x_i)\)</span>中的特征进行卷积。如图2(b)所示，整个过程可表述为：<span class="math display">\[\begin{aligned}y_{i}&amp;=\mathcal{A}_{c on v}(\mathcal{D}_{c o nv}(x_{i},\mathcal{N}_{K}(x_{i})),\mathcal{N}_{K}(x_{i}))\\&amp;=\mathcal{P}_{c o nv}(\mathcal{x}_{i},\mathcal{N}_{K}(\mathcal{x}_{i}))\circledast\mathcal{N}_{K}(\mathcal{x}_{i})\\\end{aligned}\]</span></p><p><span class="math display">\[\mathcal{P}_{c o nv}(\mathcal{x}_{i},\mathcal{N}_{K}(\mathcal{x}_{i}))=W_{c o nv},\]</span></p><p>​  其中<spanclass="math inline">\(\circledast\)</span>表示卷积运算。可以观察到，卷积中token的混合范围由核尺寸K决定——对于轻量级模型而言，核尺寸通常较小，这导致感知范围受限。此外，感知<spanclass="math inline">\(\mathcal{P}_{c o nv}\)</span>建模的词元间关系（即聚合权重）仅取决于相对位置，因此所有词元共享且固定不变。这种机制阻碍了token对相关上下文的自适应能力，限制了表达潜力。考虑到轻量级网络本身建模能力有限，这种局限性显得尤为突出。</p><h2 id="ls-large-small卷积">3.2.<strong>LS(Large-Small)</strong>卷积</h2><p>​  受人类视觉系统[59,62,72]展现的动态异尺度视觉能力启发，我们提出了一种名为“大视域聚焦小细节”的创新策略，用于感知与聚合过程，旨在实现轻量级模型中高效且有效的标记混合（如图2(c)所示）。该方法通过大范围感知有效收集全面的上下文信息并建立关系模型，同时借助小范围聚合在高度相关环境中实现精细视觉表征。具体而言，对于标记<spanclass="math inline">\(x_i\)</span>，其感知上下文区域<spanclass="math inline">\(\mathcal{N}_{P}(\mathcal{x}_{i})\)</span>与聚合上下文区域<spanclass="math inline">\(\mathcal{N}_{A}(\mathcal{x}_{i})\)</span>的范围存在差异——<spanclass="math inline">\(\mathcal{N}_{P}(\mathcal{x}_{i})\)</span>的空间覆盖范围比<spanclass="math inline">\(\mathcal{N}_{A}(\mathcal{x}_{i})\)</span>更广，整个过程可表述为：<spanclass="math display">\[y_{i}=\mathcal{A}(\mathcal{P}(x_{i},\mathcal{N}_{P}(\mathcal{x}_{i})),\mathcal{N}_{A}(\mathcal{x}_{i}));\]</span>​  可以观察到：(1)感知模块<spanclass="math inline">\(\mathcal{P}\)</span>与聚合模块<spanclass="math inline">\(\mathcal{A}\)</span>分别对应不同的上下文范围（即<spanclass="math inline">\(\mathcal{N}_{P}(\mathcal{x}_{i})\)</span>和<spanclass="math inline">\(\mathcal{N}_{A}(\mathcal{x}_{i})\)</span>，这种设计既支持异尺度上下文信息的利用，又能同时捕捉整体环境特征与细节特征。(2)对于空间覆盖范围较大的感知模块，可采用大核深度卷积等成本效益高的操作，从而以最小开销实现感知上下文的扩展。(3)针对邻域较小的聚合模块，我们可采用自适应加权特征求和方法。由于聚合范围有限，该方案既能保证计算效率，又可有效降低运算成本，还能缓解自注意力机制中权重不足的问题。<br/>​  基于这些研究，我们提出了一种新型的LS(Large-Small)卷积结构。</p><figure><imgsrc="../postimages/LSNet-See-Large-Focus-Small/image-20250806144434890.png"alt="image-20250806144434890" /><figcaption aria-hidden="true">image-20250806144434890</figcaption></figure><p>​  如图3(a)所示，针对每个标记，该结构引入了两个步骤：<br/>​  (1)大核感知模型通过大核静态卷积，以扩展的感受野来建模邻域关系。(2)小核聚合模型则通过小核动态卷积，自适应地整合周围特征。<br/>​  大核感知（LKP，<strong>Large-KernelPerception</strong>）<br/>​  大核感知采用了大核瓶颈块的设计方案。给定视觉特征图X∈RH×W×C，我们首先通过逐点卷积（PW）将特征向量投影到较低的通道维度（默认为<spanclass="math inline">\(\frac C2\)</span>），以此降低计算成本，使模型尽可能轻量化。对于xi，我们采用核尺寸为KL×KL的大核深度卷积（DW），以高效捕捉<spanclass="math inline">\(\mathcal{N}_{K_L}(\mathcal{x}_{i})\)</span>的广域空间上下文信息，其中<spanclass="math inline">\(\mathcal{N}_{K_L}(\mathcal{x}_{i})\)</span>表示以xi为中心、尺寸为KL×KL的邻域区域。这种大核深度卷积能有效扩展感受野，同时以最小成本提升上下文感知能力。随后我们利用逐点卷积（PW）来建模标记间的空间关系，即生成用于聚合步骤的上下文自适应权重W∈RH×W×D。整个过程可表述为： <spanclass="math display">\[\begin{aligned}w_{i}&amp;={\mathcal P}_{ls}(x_{i},\mathcal{N}_{K_L}(\mathcal{x}_{i}))\\&amp;={\mathrmPW}({\mathrm DW}_{K_L\times K_L}({\mathrmPW}(\mathcal{N}_{K_L}(\mathcal{x}_{i}))))\end{aligned}\]</span>​  其中wi∈RD表示为xi生成的权重。</p><p>​  小核聚合（SKA，<strong>Small-KernelAggregation</strong>）<br/>​  小核聚合采用分组动态卷积的设计方案。对于视觉特征图X∈R_H×W×C，我们将其通道划分为G个组，每个组包含<spanclass="math inline">\(\frac CG\)</span>个通道，组内通道共享聚合权重，从而降低轻量级模型的内存开销和计算成本。对于每个xi，我们将其对应的权重wi∈R_D（由大核感知生成）重塑为<spanclass="math inline">\(w_i^∗\in{\mathbb R}^{G\times K_S\timesK_S}\)</span>，其中KS_×KS表示小核尺寸。随后利用wi∗聚合其高度相关的上下文NKS（xi），其中NKS（xi）表示以xi为中心、尺寸为KS_×KS的邻域。具体而言，我们将xi的第c个通道记作xic，该通道属于第g组。通过NKS（xic）与wig∈R_KS×KS的卷积运算，我们获得其聚合特征表示yic。这种方式能有效表征自适应细粒度特征，使模型对多样化上下文中的动态复杂变化保持敏感。整个过程可表述为：</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Multi-view Pixel Contrast for General and Robust Image Forgery Localization</title>
      <link href="/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/"/>
      <url>/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/</url>
      
        <content type="html"><![CDATA[<p>Exploring Multi-view Pixel Contrast for General and Robust ImageForgery Localization</p><p>Zijie Lou1,2， Gang Cao1,2,∗，Kun Guo1,2，Haochen Zhu1,2， LifangYu3</p><p>1中国传媒大学媒体融合与传播国家重点实验室<br/>2中国传媒大学计算机与网络科学学院<br/>3北京平面设计学院信息工程系</p><h1 id="摘要">摘要</h1><p>​  图像伪造定位作为数字取证领域的基础性任务，其核心目标在于识别图像中的篡改区域。尽管现有深度学习方法已取得显著成果，但这些方法仅通过像素到标签的直接映射关系进行学习，未能充分挖掘特征空间中像素间的内在关联。针对这一缺陷，我们提出多视角像素对比算法（MPC）用于图像伪造检测。具体而言，我们首先采用监督对比损失对主干网络进行预训练，从图像内部、跨尺度和跨模态三个维度构建像素间关系模型，旨在提升同类图像的紧凑性并增强不同类图像的可区分性。随后，我们通过交叉熵损失对定位头进行微调，从而获得更精准的像素定位器。为与现有图像伪造定位算法进行全面公平的对比，我们使用三种不同规模的训练数据集对MPC进行训练。在小、中、大规模训练数据集上开展的大量实验表明，相较于现有最先进方法，本文提出的MPC不仅具有更强的泛化性能，还展现出更出色的抗后处理能力。相关代码将在https://github.com/multimediaFor/MPC平台开放获取。</p><h1 id="引言">1 引言</h1><p>​  得益于图像处理技术与工具的迅猛发展，数字图像已变得极易被篡改。诸如假新闻、学术造假和犯罪活动等恶意图像篡改行为，对社会可能造成严重负面影响。更棘手的是，由于篡改痕迹难以察觉，这些被篡改区域往往肉眼难辨。因此，开发可靠的图像篡改取证技术显得尤为重要。<br/>​  早期方法主要采用人工设计的特征来区分原始区域和篡改区域，例如颜色滤波器阵列[12]和局部噪声特征[38]。然而，由于依赖于先验统计模型，这些人工特征在面对未见过的篡改类型时难以有效泛化。近年来，基于深度学习（DL）的方法[2,3,8,14,16-18,21,25,27-30,32-35,43,46,47,51,54,57,59-61,63-67]在图像伪造定位领域取得了显著成果。凭借丰富的训练样本和强大的特征表示能力，这类深度学习方法在定位性能上明显优于传统方法。典型的图像伪造定位模型通常由深度特征提取器和像素级softmax/sigmoid分类器组成，并采用像素级交叉熵（CE）损失函数进行训练。为了有效区分篡改区域与真实区域，部分取证方法采用了专门的网络设计，例如多层特征融合[8,14,16,21,25,29,34,35,47]和注意力机制[8,16-18,21,25,28,34,35,51,67]。这些伪造定位模型的核心原理是利用深度网络将图像像素映射到高度非线性的特征空间。然而，它们通常直接在标签空间中学习像素到标签的映射关系，却忽视了特征空间中像素间的关联性。理想情况下，有效的伪造定位特征空间不仅需要1）考虑单个像素嵌入的分类能力，还需2）具备良好的组织结构，既能保证同类像素间的紧凑性，又能实现不同类别的可分离性。<br/>​  为解决这一问题，我们提出了一种多视角像素级对比算法以实现更高效的图像伪造定位。具体而言，我们首先通过三种视角（图像内部、跨尺度和跨模态）的监督对比损失对主干网络进行预训练，从而构建像素特征空间。随后使用CE损失对定位头进行微调，以解决类别间判别问题。该方法通过对比损失机制，促使同一类别的像素特征（原始与原始、未篡改与篡改）相互靠近，而不同类别的像素特征（原始与篡改）彼此远离，从而增强类别内紧凑性和类别间可分离性。这种机制自然促进了微调阶段中定位预测的准确性提升。<br/>​  近期如[41,60]等研究也采用对比学习方法进行图像伪造定位任务。但这些研究仅聚焦于图像内部像素对比度，同时使用对比损失和CE损失进行训练，并在有限测试数据集上验证其方案。与这些研究相比，我们通过对比损失从三个维度探索标注像素嵌入的结构信息：图像内部、跨尺度和跨模态。此外，我们采用两阶段训练策略，并在多个测试数据集上验证了所提方法的有效性。总体而言，我们的贡献包括：</p><ul><li>我们提出了一种多视角像素级对比算法（MPC）用于图像伪造定位。该方法采用两阶段训练策略，不仅能在标签空间中建模像素关系，还能在特征空间中进行建模。</li><li>对比损失用于从三个维度塑造像素特征空间：图像内部、跨尺度和跨模态。通过多视角像素对比，可以获得结构良好的像素特征空间，从而提升定位性能。</li><li>我们对现有的图像伪造定位方法进行了全面而公平的比较。广泛的实验表明，与最先进的方法相比，我们提出的MPC具有显著的性能优势。</li></ul><h1 id="相关工作">2 相关工作</h1><h2 id="图像伪造定位">2.1 图像伪造定位</h2><p>​  传统图像伪造定位方法[1]、[2]、[38]、[44]主要依赖人工设计的特征来捕捉篡改操作产生的统计异常。早期研究[1]通过颜色滤波器阵列伪影检测图像中被篡改区域的不一致性。成像传感器引入的局部噪声[2]以及光照颜色和方向[38]、[44]也是图像拼接定位的重要线索。尽管这些人工特征具有可解释性，但其检测范围仅限于少数几种伪造类型，在其他类型上泛化效果较差。<br/>​  近年来，基于深度学习的架构被开发用于自适应提取法证特征，从而在面对各种伪造操作时展现出更强的泛化能力。所采用的骨干网络包括卷积神经网络（CNN)[3]，[4]，[9]，[11]，[13]，[15]，[18]，[21]，[22]，[25]，[26]，[27]，）、长短期记忆网络（LSTM）和全卷积网络（FCN)[5]，[7]，[45]），以及Transformer[10]，[14]，[16]，[17]，[20]，[23].Siamese网络（[40]，[46]），后者通过探索补丁一致性来识别篡改区域。这些方法依赖于不同的相机属性，因此需要包含相机元信息的图像。相比之下，我们仅使用像素级信息处理篡改图像。本研究不设计更复杂的网络架构，而是专注于构建更具结构化的特征空间以实现更优的定位性能。<br/>​  值得注意的是，现有方法采用不同规模的训练数据集，导致难以实现公平比较。常见的训练数据集规模可分为三类：小规模（约5000个样本）、中等规模（约10万个样本）和大规模（超过80万样本）。例如，MVSS-Net++[11]，PSCC-Net [4]和TruFor[17]分别使用了约5000、10万和80万张篡改图像进行网络训练。因此，除非使用相同或等效规模的数据集进行训练，否则应避免直接对比。为了与现有图像伪造定位算法进行全面公平的比较，我们在三种实验设置中使用相同（协议1和协议3）或等效规模的数据集（协议2）训练网络，具体如表I所示。此外，由于许多先前定位方法缺乏公开代码[12]，[15]，[20]，[21]，[22]，[24]，[26]，，我们直接引用了经过数据集对齐后的对应文献结果。</p><h2 id="对比学习">2.2 对比学习</h2><p>​  近年来，对比学习在无监督自监督学习领域取得了重要进展[47]、[48]。该方法通过对比相似（正样本）数据对与不相似（负样本）数据对来获取有效表征。具体而言，通过增强实例生成正样本对，而负样本通常采用随机抽样方式获得。此外，研究者还提出了监督对比损失函数[49]以提升分类准确率。<br/>​  由于对比学习能够捕捉局部特征的丰富空间关系，越来越多的研究开始将对比学习应用于取证领域。例如，[50]通过对比损失来增强CE损失，用于人脸伪造检测。鉴于广泛使用的CE损失存在局限性，近期研究开始引入对比损失来辅助网络训练图像伪造定位[27]、[31]。但与仅关注图像内部像素对比度的研究不同，我们创新性地从三个维度构建像素特征空间：图像内部对比、跨尺度对比和跨模态对比。此外，现有研究通常采用对比损失和CE损失同时训练，而我们提出的MPC方案则采用了两阶段训练策略。参考文献[27]、[31]未能在多个测试数据集上验证对比学习的有效性，本研究通过大量实验充分证明了对比学习在图像篡改定位中的巨大潜力。</p><h1 id="方法">3 方法</h1><h2 id="网络结构">3.1 网络结构</h2><p>​  我们的算法包含两个主要组件，包括主干网络和定位头，如图1所示。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801155942589.png"alt="image-20250801155942589" /><figcaption aria-hidden="true">image-20250801155942589</figcaption></figure><p>​  该模型将输入图像𝑋映射为密集嵌入<spanclass="math inline">\(\{X_i，\hatX_i\}=𝑓_{BAC}(X)\)</span>。我们首次在取证任务中采用HRFormer，因其能在整个过程中保持高分辨率表征。这种高分辨率表征能够保留细粒度的取证线索，这对准确识别伪造行为至关重要。此外，我们的跨尺度与跨模态对比损失函数的设计依赖于HRFormer的架构设计。定位头<spanclass="math inline">\(f_{LOC}\)</span>通过1×1卷积层与ReLU激活函数实现，将拼接后的<spanclass="math inline">\(\{X_i\}\)</span>投影为评分图<spanclass="math inline">\(Y=f_{L O C}(C o n c at(X_{1},X_{2},X_{3},X_{4}))\)</span>。<br/>​  需要说明的是，我们采用了两阶段训练策略。具体来说，首先使用对比损失（公式4）对主干网络进行训练，随后冻结主干网络的权重。接着，我们使用CE损失（公式5）对定位头进行微调。</p><h2 id="用于取证的多视图像素对比">3.2 用于取证的多视图像素对比</h2><p>​  在本研究中，我们开发了一种多视角像素级对比学习算法，以增强伪造像素检测的表征学习效果。我们将图像伪造定位任务转化为像素级二分类问题，即图像X的每个像素x必须被归类到类别y∈{0,1}中，其中“0”表示“原始”，“1”表示“篡改”。具体而言，给定伪造图像<spanclass="math inline">\(X\in\mathbb{R}^{H\timesW\times3}\)</span>时，主干网络会生成一系列多尺度特征<spanclass="math inline">\(\{X_{1},X_{2},X_{3},X_{4}\}\)</span>和<spanclass="math inline">\(\{\hat X_{1},\hat X_{2},\hat X_{3},\hatX_{4}\}\)</span>。由于主干网络存在<em>dropout</em>机制，<spanclass="math inline">\(X_i\)</span>与<span class="math inline">\(\hatX_i\)</span>存在差异。因此可以推导出x的像素嵌入向量<spanclass="math inline">\(x\in\mathbb{R}^{C}\)</span>。</p><p>​  <strong>图像内损失</strong><br/>​  我们首先探究图像中像素之间的结构关系。对于具有真实标签“1”的像素嵌入<spanclass="math inline">\(x\in X_1\)</span>，正样本<spanclass="math inline">\(x^+\)</span>是该嵌入在<spanclass="math inline">\(X_1\)</span>中其他标记为“1”的像素嵌入，而负样本<spanclass="math inline">\(x^-\)</span>则是标记为“0”的像素嵌入。图像内对比损失函数定义如下：<spanclass="math display">\[{\mathcal{L}}_{x}^{1}=-\log{\frac{\frac{1}{|{\mathcal{P}}_{x}|}\sum_{x^{+}\in{\mathcal{P}}_{x}\inX_{1}}\exp(x\cdot x^{+}/\tau)}{\sum_{x^{-}\in{\mathcal{N}}_{x}\inX_{1}}\exp(x\cdot x^{-}/\tau)}}\]</span> ​  其中<spanclass="math inline">\({\mathcal{P}}_{x}\)</span>和<spanclass="math inline">\({\mathcal{N}}_{x}\)</span>分别表示从<spanclass="math inline">\(X_1\)</span>中随机采样的正负像素嵌入集合。‘·’表示内积，其中温度超参数为正数。这种图像内对比损失旨在学习判别性特征表示，有助于区分图像中的原始像素与伪造像素。通过将同一类像素嵌入拉近并推动不同类别的像素嵌入分开，可以提高类内紧凑性和类间可分离性[41,52,62]。</p><p>​  <strong>跨尺度损失</strong><br/>​  由于篡改区域通常具有不同尺寸特征，因此在不同尺度上融合特征对于增强定位算法的鲁棒性至关重要。与直接将低级特征整合到高级特征的[16,25]方法不同，我们的跨尺度对比损失是通过计算不同尺度特征之间的差异来实现的。具体而言，对于像素嵌入<spanclass="math inline">\(x\in X_1\)</span>，正负样本像素嵌入集合<spanclass="math inline">\({\mathcal{P}}_{x}\)</span>和<spanclass="math inline">\({\mathcal{N}}_{x}\)</span>分别从图像的三个部分（即图像块、区域块和像素块）中采样获得。<spanclass="math display">\[{\mathcal{L}}_{x}^{2}=-\log{\frac{\frac{1}{|{\mathcal{P}}_{x}|}\sum_{x^{+}\in{\mathcal{P}}_{x}\inX_{2}\cup X_{3}\cup X_{4}}\exp(x\cdotx^{+}/\tau)}{\sum_{x^{-}\in{\mathcal{N}}_{x}\in X_{2}\cup X_{3}\cupX_{4}}\exp(x\cdot x^{-}/\tau)}}\]</span>​  因此，不同尺度之间的密集连接能够有效地交换信息，这对于提取抗尺度变化的稳健的特征很有帮助。</p><p>​  <strong>跨模态损失</strong><br/>​  受自然语言处理领域自监督学习的启发，本研究进一步整合跨模态对比学习以增强法医鉴别能力。具体而言，图像𝑋被依次输入主干网络两次，生成两个特征图版本<spanclass="math inline">\(X_i\)</span>和<span class="math inline">\(\hatX_i\)</span>。由于随机<em>dropout</em>机制的作用，这两个特征图具有相似性但不完全相同。这种双重编码方式在不增加网络复杂度的前提下，通过扩展可训练样本数量，形成了两种不同模态的训练样本。对于像素嵌入<spanclass="math inline">\(x\in X_1\)</span>，正负像素嵌入集合<spanclass="math inline">\({\mathcal{P}}_{x}\)</span>，<spanclass="math inline">\({\mathcal{N}}_{x}\)</span>从数据集<spanclass="math inline">\(\hatX_i\)</span>中采样获得。然后采用跨模态对比损失函数 <spanclass="math display">\[{\mathcal{L}}_{x}^{3}=-\log{\frac{\frac{1}{|{\mathcal{P}}_{x}|}\sum_{x^{+}\in{\mathcal{P}}_{x}\in\hat X_{1}}\exp(x\cdot x^{+}/\tau)}{\sum_{x^{-}\in{\mathcal{N}}_{x}\in\hat  X_{1}}\exp(x\cdot x^{-}/\tau)}}\]</span>​  考虑到计算复杂度，我们采用分辨率最高的特征<spanclass="math inline">\(X_1\)</span>来计算<spanclass="math inline">\({\mathcal{L}}_{x}^{1}\)</span>和<spanclass="math inline">\({\mathcal{L}}_{x}^{3}\)</span>。总体而言，在训练主干网络时使用的总多视角对比损失函数<spanclass="math inline">\({\mathcal{L}}_{x}^{Contra}\)</span>定义为 <spanclass="math display">\[{\mathcal{L}}_{x}^{Contra}={\mathcal{L}}_{x}^{1}+{\mathcal{L}}_{x}^{2}+{\mathcal{L}}_{x}^{3}\]</span></p><h2 id="监督式像素级伪造检测">3.3 监督式像素级伪造检测</h2><p>​  完成训练后，将冻结主干网络的权重。随后对定位头进行微调，将投影<spanclass="math inline">\(\{X_{1},X_{2},X_{3},X_{4}\}\)</span>转换为评分图<spanclass="math inline">\(Y\)</span>。设𝑦为像素𝑥的评分向量（通过sigmoid激活），即.，<spanclass="math inline">\(𝑦\in Y\)</span>.。给定像素𝑥相对于其真实标签<spanclass="math inline">\(\hat𝑦\in\{0,1\}\)</span>，的𝑦值，优化改进后的交叉熵损失函数[31]。 <spanclass="math display">\[\begin{aligned}\mathcal{L}_{x}^{\mathrm{CE}}(\hat{y},y)=&amp;-\alpha\,(1-y)^{\gamma}\times\hat{y}\log\left(y\right)\\&amp;-(1-\alpha)y^{Y}\times(1-\hat{y})\log\left(1-y\right)\end{aligned}\]</span>​  其中𝛼和𝛾是超参数，通常经验性地设置为0.5和2。虽然对比学习会促使图像中的像素根据其标签聚类，但基于CE损失的微调会重新排列这些聚类，使其落在决策边界正确的那一侧。</p><h1 id="实验">4 实验</h1><h2 id="实验设置">4.1 实验设置</h2><p>​  <strong>数据集</strong><br/>​  为了与现有算法进行全面、公平的比较，我们在不同规模的训练数据集上进行了三组实验，如表1所示。各实验的配置如下：</p><ul><li>实验一<br/>基于前期研究[8,25]，我们的模型在包含5123张篡改图像的CASIAv2数据集[9]上进行训练。随后我们使用另外10个数据集对模型进行测试，包括CASIAv1[9]、NIST [15]、Columbia[20]、IFC [1]、IMD [42]、Coverage [53]、DSO[4]、DEF-test [39]、Wild [22]和Korus[26]。在实验1中，所有模型均仅在CASIAv2数据集上进行训练。</li><li>实验二<br/>在完成[17,28,34,59,61,65]阶段后，我们还从CAT-Net数据集[27]中随机选取了60,000张篡改图像进行模型预训练。在后续微调MPC时，我们沿用了与NIST、Coverage和CASIA实验相同的训练-测试比例配置。最后，我们在四个数据集（即CASIAv1[9]、NIST [15]、IMD [42]和Coverage[53])上进行测试。实验2中所有模型均在等比例缩放的数据集上进行训练，以确保公平比较。</li><li>实验三<br/> 我们的模型使用与CAT-Net[27]和TruFor[16]相同的训练数据集进行训练，并在其他10个公开数据集上进行测试，包括CASIAv1[9]、NIST [15]、Columbia[20]、Coverage [53]、DSO [4]、Wild [22]、Korus[26]、MISD [23]、CoCoGlide [16]和FF++[45]。由于计算资源的限制，我们仅将我们的方法与在相同数据集上训练的CAT-Net和TruFor进行对比。</li></ul><p>​  <strong>指标</strong><br/>​  与先前研究[16,25,54]类似，我们通过F1值和交并比（IoU）来评估像素级伪造检测的准确度。采用固定阈值0.5对定位概率图进行二值化处理。各测试数据集的平均F1值和IoU值被用作伪造检测算法的统计性能指标。考虑到不同数据集间图像样本数量存在明显不平衡，所有测试数据集的平均F1值和IoU值均按以下方式计算：<spanclass="math display">\[\mathrm{Average}={\frac{\sum_{i=1}^{i=N}{\mathrm{Metric}}_{D_i}\times\mathrm{Num}_{D_{i}}}{\sum_{i=1}^{i=N}\mathrm{Num}_{D_{i}}}}\]</span>​  其中<span class="math inline">\({\mathrm{Metric}}_{D_i}\)</span>表示第i个数据集的平均指标（F1或IoU），<spanclass="math inline">\({\mathrm{Num}}_{D_i}\)</span>表示该数据集包含的图像数量。这种指标本质上是样本级别的平均值，而非简单的数据集级算术平均值。等式6中的计算方法适用于现有文献中仅提供各数据集指标<spanclass="math inline">\({\mathrm{Metric}}_{D_i}\)</span>的场景，用于重新计算样本级别的平均值。</p><p>​  <strong>具体实施细节</strong><br/>​  所提出的MPC算法由PyTorch实现。我们在单个A800GPU上分两个阶段训练网络：第一阶段学习率从1e-4开始，采用“平台期降速策略”逐步降低；第二阶段学习率同样从1e-4起始，但改用“余弦退火策略”进行调整。优化器选用Adam，批量大小设为4，所有训练图像均调整为512×512像素尺寸。作为数据增强手段，我们采用了包括翻转、模糊、压缩、加噪和缩放在内的常见处理方式。</p><h2 id="与state-of-the-arts的比较">4.2与<strong>State-of-the-Arts</strong>的比较</h2><p>​  <strong>实验一</strong><br/>​  我们将MPC的性能与16种现有的图像伪造定位或语义分割算法进行比较，所有这些算法都是在CASIAv2数据集上训练的。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163131701.png"alt="image-20250801163131701" /><figcaption aria-hidden="true">image-20250801163131701</figcaption></figure><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163202176.png"alt="image-20250801163202176" /><figcaption aria-hidden="true">image-20250801163202176</figcaption></figure><p>​  表2和表3展示了不同方法在像素级定位性能上的评估结果。总体而言，我们的方法取得了最高的平均定位精度，在F1值和交并比（IoU）方面分别领先第二名PIM[25]3.7%和2.9%。此外，我们的方法在6个数据集上获得了最佳的F1值，在7个数据集上则在交并比（IoU）方面表现最优。尽管这10个测试数据集的分布形态各异，但我们的MPC方法在定位性能上仍超越了所有先前研究。这些亮眼的成绩充分展现了该方法在各类伪造图像中展现出的高精度定位能力和强大的泛化能力。这些优势对于推动法医定位技术向现实应用领域发展具有重要意义。<br/>​  此外，我们发现仅在小规模CASIAv2数据集上训练时，基于Swin-ViT[36]构建的简单语义分割网络定位性能，已超越大多数专门设计的取证算法[8,21,27,33,34,55]。这些结果表明，当训练数据集规模不足时，现有多数取证算法可能无法有效学习其依赖的篡改痕迹特征。相比之下，我们的多视角对比学习（MPC）方法不依赖特定取证线索，而是通过多视图对比学习构建结构化特征空间，从而实现原始像素与篡改像素的有效区分。</p><p>​  <strong>实验二</strong><br/>​  如文献[8,17,18,21,28,29,32,34,35,47,51,57,59-61,63-65,67]所述，这类实验通常采用中等规模数据集（样本量6万∼10万）进行预训练，随后在测试集子集上进行微调。由于缺乏额外的测试数据集，此类实验无法准确反映模型的泛化能力。不过为确保公平比较，我们仍沿用现有19种图像伪造定位方法的实验设置。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163414873.png"alt="image-20250801163414873" /><figcaption aria-hidden="true">image-20250801163414873</figcaption></figure><p>​  表5为实验2中微调模型的对比结果，标记为“-”的未获得结果的情况是由于文献中未公开源代码或未给出测试结果。相较于现有方法，我们的MPC模型在平均F1分数上表现最佳，并且比排名第二的TBFormer[35]高出3.8%。需要特别说明的是，TBFormer未在IMD数据集上进行测试——这对评估伪造检测算法的泛化能力至关重要。与同样采用对比学习的PCL[60]相比，MPC模型的改进幅度可超过24.5%。总体而言，我们的MPC模型能够适应不同分布的数据集，并展现出具有竞争力的定位性能。</p><p>​  <strong>实验三</strong><br/>​  在本系列实验中，我们将方法与CAT-Net[27]和TruFor[16]进行对比。所有模型均使用包含80万余张篡改图像的CAT-Net数据集[27]进行训练。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163558907.png"alt="image-20250801163558907" /><figcaption aria-hidden="true">image-20250801163558907</figcaption></figure><p>​  表4展示了不同方法在10个测试数据集上的F1值和交并比表现。我们的MPC方法在哥伦比亚、CASIAv1、Coverage、MISD和CoCoGlide数据集上超越TruFor，但在其余数据集上稍逊一筹。平均而言，我们的方法在F1值和交并比上分别比当前最佳的TruFor高出1.4%和0.9%。总体而言，我们的MPC方法在伪造定位性能上与TruFor旗鼓相当，并且在所有数据集上均优于CAT-Net。此外，我们的方法具有参数更少的优势。虽然CAT-Net（1.143亿参数）和TruFor（6870万参数）分别采用基于HRNet[50]和SegFormer[56]的双分支架构，但我们的MPC（4180万参数）仅采用单分支HRFormer架构。我们的方案避免了额外模块的设计，以更低的计算成本实现了相当甚至更优的定位性能。</p><h2 id="鲁棒性评估">4.3 鲁棒性评估</h2><p>​  我们首先评估图像伪造定位方法对社交网络（OSNs）复杂后处理操作的鲁棒性。参照先前研究[54]，我们测试了通过Facebook、Weibo、Wechat和Whatsapp平台传输的四个伪造数据集。表6显示，我们的方法在各社交网络平台的四个数据集中均保持最高准确率。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163704753.png"alt="image-20250801163704753" /><figcaption aria-hidden="true">image-20250801163704753</figcaption></figure><p>​  在对比方法中，MPC因OSNs的影响而产生的性能损失最小。值得注意的是，CAT-Net由于专门学习JPEG压缩伪影，在处理Columbia数据集时表现更优。但该方法在其他数据集上明显性能下降——例如在Facebook版Columbia数据集上F1=91.8%，而在Wechat版CASIAv1数据集上F1=13.9%。相比之下，我们的MPC在所有社交网络传输中始终保持高定位准确率。这些结果验证了MPC对社交网络处理的鲁棒性。<br/>​  在先前研究[8,34]的基础上，我们还在Columbia数据集上评估了模型对JPEG压缩、高斯模糊、高斯噪声和图像缩放等后处理的鲁棒性。如图2所示结果验证了我们的MPC模型对这些后处理具有高度鲁棒性。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801163858679.png"alt="image-20250801163858679" /><figcaption aria-hidden="true">image-20250801163858679</figcaption></figure><p>​  MPC在所有测试场景中均表现最佳。特别值得注意的是，模糊处理和噪声干扰会显著降低CAT-Net和TruFor的性能，但对我们的方法影响微乎其微。以噪声干扰为例，虽然CAT-Net和TruFor的F1值分别从0.8骤降至0和0.5，但MPC在不同噪声强度下的表现始终稳定在0.9以上。<br/>​  我们进一步验证了MPC算法在组合后处理操作中的鲁棒性。具体而言，在OSN传输完成后，我们在Columbia数据集上应用了额外的JPEG压缩、高斯模糊、高斯噪声和图像缩放处理。为模拟真实世界的后处理流程，所有操作顺序均采用随机化处理。表7展示了相应的统计测试结果，图3则呈现了示例测试图像的定性评估结果。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801164022849.png"alt="image-20250801164022849" /><figcaption aria-hidden="true">image-20250801164022849</figcaption></figure><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801164043501.png"alt="image-20250801164043501" /><figcaption aria-hidden="true">image-20250801164043501</figcaption></figure><p>​  可以观察到，CAT-Net算法无法抵御任何组合后的处理方式。TruFor的定位性能也严重受损，几乎无法使用。相比之下，我们的MPC展现出显著更强的鲁棒性，以明显优势超越CAT-Net和TruFor。这种结果归因于CAT-Net和TruFor依赖特定篡改痕迹（如JPEG伪影和噪声不一致性）。我们的方法致力于通过对比学习增强原始像素与篡改像素之间的类内紧凑性和类间可分离性。此外，训练过程中采用的数据增强策略也能进一步提升我们MPC的鲁棒性。</p><h2 id="消融研究">4.4 消融研究</h2><p>​  我们进行了大量消融实验来验证所提出的MPC的有效性。相关子实验的总结如表8所示。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801164142032.png"alt="image-20250801164142032" /><figcaption aria-hidden="true">image-20250801164142032</figcaption></figure><p>​  <strong>多视图像素对比度的有效性</strong><br/>​  我们移除三种对比损失中的其中一种，并通过评估定位性能的下降幅度来验证每种损失带来的性能提升。在MPC框架中，图像内损失<spanclass="math inline">\({\mathcal{L}}_{x}^{1}\)</span>最为关键，其缺失会导致定位性能出现灾难性崩溃。这是因为通过图像内对比损失来提升类间紧凑性和类内可分离性，正是我们研究的核心目标。此外，跨模态损失<spanclass="math inline">\({\mathcal{L}}_{x}^{3}\)</span>带来的性能提升效果优于跨尺度损失<spanclass="math inline">\({\mathcal{L}}_{x}^{2}\)</span>。这一结果归因于所采用的<em>dropout</em>机制：该机制引入具有挑战性的样本，从而增强了模型识别被篡改像素区域的能力。</p><p>​  <strong>主干网络的影响</strong><br/>​  我们还研究了网络规模对性能的影响。尽管HRFormer-base（41.8M）相比HRFormer-small（8.2M）的网络复杂度有所增加，但性能表现仍显著提升。这种改进归因于参数更丰富的大型模型可能具备更强的表征学习能力。不过与CAT-Net（114.3M）和TruFor（68.7M）相比，我们的MPC（41.8M）依然保持了轻量化优势。</p><h2 id="定性结果">4.5 定性结果</h2><p>​  我们还对不同方法的定位性能进行了定性对比。从测试数据集中选取了涵盖三种常见篡改类型（即拼接、复制移动和删除）的样本进行分析。图4展示了各方法在示例图像上的像素级伪造定位结果。</p><figure><imgsrc="../postimages/Exploring-Multi-view-Pixel-Contrast-for-General-and-Robust-Image-Forgery-Localization/image-20250801164444941.png"alt="image-20250801164444941" /><figcaption aria-hidden="true">image-20250801164444941</figcaption></figure><p>​  可以看出，我们的方法对各类伪造图像都能生成更精准的定位结果。在大多数情况下，其他定位方法只能检测到篡改区域的部分特征，且存在不同程度的误报。<br/>​  除了传统图像伪造案例，我们还对定位算法在新型伪造图像上的表现进行了定性对比。具体而言，我们分别从FF++[45]和CoCoGlide[16]数据集中选取了深度伪造图像和本地AI生成图像进行测试。FF++中的合成人脸图像是通过Face2Face[49]等面部交换算法生成的，而CoCoGlide中的本地AI生成图像则由GLIDE扩散模型[40]生成。如图5所示，尽管训练过程中未包含此类图像类型，我们的方法仍能精准识别出被篡改区域。相比之下，TruFor算法容易产生更多误判和漏检，而CAT-Net几乎完全失效。这些结果充分证明了我们提出的多视图像素级对比学习方法具有强大的泛化能力。</p><h1 id="结论">5 结论</h1><p>​  本文提出了一种名为MPC的新型可信图像伪造定位方案。首先通过像素级监督对比损失训练主干网络，从图像内部、跨尺度和跨模态三个维度建模特征空间中的像素关系。随后利用CE损失对定位头进行微调，从而获得更精准的像素定位器。我们通过三组主流实验与现有图像伪造定位方法进行全面公平的对比验证。大量实验表明，相较于现有技术，我们的方法展现出更强的泛化能力和鲁棒性。MPC在应对在线社交网络的复杂后处理及操作链路时表现出优异的抗干扰能力。未来研究将致力于开发更强大的取证算法，以应对低光环境图像和新型AI生成图像等极具挑战性的伪造场景。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GIM:A Million-scale Benchmark for Generative Image Manipulation Detection and Localization</title>
      <link href="/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/"/>
      <url>/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<p>GIM: A Million-scale Benchmark for Generative Image ManipulationDetection and Localization</p><p>Yirui Chen1,3,<em>， Xudong Huang3,</em>，Quan Zhang 2,3,*， WeiLi3，Mingjian Zhu3，Qiangyu an3<br/>，Simiao Li 3， Hanting Chen3，Hailin Hu3，Jie Yang1， Wei Liu1,†，Jie Hu3,</p><p>1 上海交通大学<br/>2 清华大学<br/>3 Huawei Noah’s Ark Lab</p><h1 id="摘要">摘要</h1><p>​  生成模型的非凡能力正成为图像编辑与生成逼真图像领域的新趋势，这对多媒体数据的可信度构成严峻挑战，并推动了图像操作检测与定位（IMDL）研究的发展。然而，由于缺乏大规模数据基础，IMDL任务难以实现。本文构建了一个整合了结构化注意力模型（SAM）、语言模型（LLM）和生成模型强大功能的定位篡改数据生成流程。在此基础上，我们提出了GIM数据集，该数据集具有以下优势：<br/>​  1)大规模，GIM包含超过100万对人工智能操作的图像和真实图像。<br/>​  2)丰富的图像内容，GIM包含广泛的图像类别。<br/>​  3)多种生成式操作，这些图像是使用最先进的生成器和各种操作任务进行处理的图像。<br/>​  上述优势使得IMDL方法的评估更加全面，拓展了其在各类图像中的应用范围。我们通过两种设置构建了GIM基准测试平台来评估现有IMDL方法，并提出了一种名为GIMFormer的创新IMDL框架，该框架包含影子追踪器、频域空间模块（FSB）和多窗口异常建模（MWAM）模块。大量实验表明，GIMFormer在两个不同基准测试中均超越了先前的最先进方法。</p><h1 id="引言">1. 引言</h1><h1 id="相关工作">2. 相关工作</h1><h1 id="数据集和基准构建">3. 数据集和基准构建</h1><p>​  在本节中，我们提出了一种从未标注数据生成处理图像的自动化流程。通过这一流程，我们构建了一个全面的大规模GIM数据集。为构建合理的基准测试体系，我们重点围绕数据规模与图像退化两大核心维度开展前期实验。首先，通过分析训练数据规模对模型性能的影响，确定GIM基准测试的合适规模参数。其次，为还原真实场景特征，对处理过和原始图像分别进行三种随机退化处理。最终，GIM基准测试包含超过32万张经过处理的图像及其真实对应图像，用于算法训练与评估。示例图像及其原始版本与篡改掩码如图1所示。最后，我们详细阐述了IMDL方法评估所采用的评判标准与参数设置方案。</p><figure><imgsrc="../postimages/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/image-20250801151431894.png"alt="image-20250801151431894" /><figcaption aria-hidden="true">image-20250801151431894</figcaption></figure><h1 id="方法">4. 方法</h1><p>​  为应对生成式操控的挑战，我们提出采用双编码器与解码器架构的GIMFormer模型。该框架包含多个组件：ShadowTracer、频域空间块（Frequency-SpatialBlock，简称FSB）以及多窗口异常建模模块（Multi Windowed AnomalousModeling，MWAM）。图3展示了该框架的整体架构。</p><figure><imgsrc="../postimages/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/image-20250801151443340.png"alt="image-20250801151443340" /><figcaption aria-hidden="true">image-20250801151443340</figcaption></figure><p>​  对于输入的RGB图像x，我们首先提取其学习得到的轨迹图t。随后，x和t被输入到一个双分支网络中，通过四阶段结构提取金字塔特征Fi（i∈[1,4]）。<br/>​    RGB分支由FSB、TransformerBlock（谢等人，2021)和WMAM组成；<br/>​    ShadowTracer分支则包含TransformerBlock和WMAM。<br/>​  在融合步骤中，采用特征整流模块（FRM）和特征融合模块（FFM）（张等人，2023)进行特征融合。经过四阶段融合的特征被传递至解码器，最终完成检测yˆ和定位Mˆ。</p><h2 id="shadowtracer">4.1 ShadowTracer</h2><p>​  传统图像篡改检测方法主要针对“廉价伪造”，依赖可见痕迹。这类痕迹包括因图像结构被篡改而产生的失真和突变现象。然而生成式篡改会对内容进行深度修改，且不会产生明显的频率变化或结构异常。如图4所示，这些细微痕迹会呈现独特的内在规律，而可见痕迹则具有不规则的边缘特征。</p><figure><imgsrc="../postimages/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/image-20250801152540674.png"alt="image-20250801152540674" /><figcaption aria-hidden="true">image-20250801152540674</figcaption></figure><p>​  ShadowTracer致力于捕捉生成模型的内在特征与细微痕迹。针对经过处理的图像，我们的目标是学习一个映射<spanclass="math inline">\({\mathcal{g}}_{\phi}\)</span>，将篡改后的图像映射到其潜在扰动像素值，其中<spanclass="math inline">\({\mathcal{g}}_{\phi}\)</span>代表具有可训练参数ϕ的神经网络。我们发现生成模型在数据分布中引入的差异具有内在规律，深度神经网络能够尝试重构这些变化。在训练阶段，我们会生成图像xi与篡改后的图像<spanclass="math inline">\(G(x_i)\)</span>这对样本，通过<spanclass="math inline">\(t_i = G(x_i)−x_i\)</span>计算出操作痕迹。训练<spanclass="math inline">\({\mathcal{g}}_{\phi}\)</span>的目标函数可表述为：<spanclass="math display">\[\operatorname*{min}_{\phi}\{\mathcal{L}_{r}(g_{\phi}(G({\mathbfx}_{i})),t_{i})\}\]</span> ​  其中<spanclass="math inline">\(\mathcal{L}_{r}(x,y)=\|x-y\|_2\)</span>。此外，映射网络需要能够检测细微篡改痕迹，并对现实世界中的各种图像退化具有鲁棒性。为此，我们通过混合原始图像与篡改图像，并在训练阶段引入多样化的退化操作来生成图像对。具体而言，给定输入图像I后，我们首先分割出目标区域并进行生成式处理以获得Im。采用混淆（Zhang等人，2017）策略对原始与篡改图像进行处理，以掩盖明显的篡改痕迹。随后，我们将图像置于上述退化处理中以生成最终的篡改图像。网络在从数据集中随机抽取的64×64像素块上进行训练，并采用等式1损失函数。</p><h2 id="频率-空间块fsb">4.2 频率-空间块FSB</h2><p>​  在进行退化处理时，被篡改图像中的伪影往往难以察觉。为提升图像局部特征的表达能力并提取判别线索，我们设计了频域-空间块（FSB）算法，通过在频率和空间维度同步提取伪造特征。<br/>​  受近期研究（Rao等人，2021；Lee-Thorp等人，2021；Zhang等人，2022）启发，FSB系统包含两个分支：频率分支和空间分支，如图3所示。</p><figure><imgsrc="../postimages/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/image-20250801153557573.png"alt="image-20250801153557573" /><figcaption aria-hidden="true">image-20250801153557573</figcaption></figure><p>​  在频率分支中，输入信号X通过二维快速傅里叶变换（FFT）转换为频域傅里叶变换（FT）信号X。可学习滤波器Gi与信号相乘以调制频谱并捕捉频率信息，随后通过逆快速傅里叶变换将特征还原至空间域，从而提取出频率感知特征Xf。在空间分支中，输入信号X经过卷积层和LeakyReLU函数处理以增强特征表达能力，获得精细的空间特征Xs。接着将Xf与Xs拼接后，通过卷积层和LeakyReLU函数进一步增强信息量，并通过逐元素求和的方式与原始输入X结合。整个处理流程可表示为<spanclass="math display">\[\begin{array}{l}{X_{\mathrm{f}}=\widehat{\mathcalF}_{T}(\mathcal{F}_{T}(X)\odot G_{\mathrm{i}})}\\{X_{\mathrm{s}}=\mathrm{Conv_{L}}(\mathrm{Conv}(X))}\\ {X_{o ut}=\mathrm{Conv_{L}}([X_{\mathrm{f}},X_{\mathrm{s}}])+X,}\end{array}\]</span>​  其中<spanclass="math inline">\(\odot\)</span>表示Hadamard积，ConvL表示使用LeakyReLU的卷积，[·]表示拼接。</p><h2 id="多窗口异常建模模块mwam">4.3 多窗口异常建模模块MWAM</h2><p>​  图像处理会导致像素层面的差异。真实像素应与邻近像素保持一致性，而经过处理的像素可能出现偏差并呈现异常现象。受先前研究(Wu,AbdAlmageed,and Natarajan 2019; Kong et al.2023)探索局部不一致性的启发，我们引入多窗口异常建模（MWAM）模块，通过在多个尺度上对这些差异进行建模，从而捕捉处理区域与真实区域之间的像素级差异，实现细粒度特征的精准刻画。</p><figure><imgsrc="../postimages/GIM-A-Million-scale-Benchmark-for-Generative-Image-Manipulation-Detection-and-Localization/image-20250801153523113.png"alt="image-20250801153523113" /><figcaption aria-hidden="true">image-20250801153523113</figcaption></figure><p>​  如图3所示，对于输入特征F∈H×W×C，我们通过等式3在两个分支中计算像素与其局部窗口内周围区域的差异。<span class="math display">\[\begin{array}{l cr}{D_{u}^{k}[i,j]=(F[i,j]-F_{u}^{k}[i,j])/\sigma^{*},}\\{\sigma^{*}=\operatorname*{maximum}(\sigma(F),1e^{-5}+w_{\sigma})}\end{array}\]</span>​  其中，<spanclass="math inline">\(u\in\left\{a,m\right\}\)</span>表示平均值或最大值分支，σ(F)是F的标准差，wσ是一个与σ长度相同的可学习非负权重向量。<spanclass="math inline">\(F_a^k\)</span>和<spanclass="math inline">\(F_m^k\)</span>分别通过计算每个像素处k_×_k窗口的平均值和最大值得出。通过选择不同尺寸的k来建模不同尺度的不一致性特征。随后，将获得的N= 3个不同尺度的<span class="math inline">\(D_a^k\)</span>和<spanclass="math inline">\(D_m^k\)</span>拼接后输入卷积网络，生成与原始输入相同尺寸的异常图Ma和Mm。此外，特征的异常评分掩码也<spanclass="math inline">\(\hat S_{u}\in H\timesW\)</span>是通过该方法计算得出。 <spanclass="math display">\[\begin{array}{l}{\hat{f}_{u}=\mathrm{DConv}\left(f\right),}\\{\hat{S}_{u}=\mathrm{Sigmoid}\left(\mathrm{Conv}(C,1)\left(\hat{f}_{u}\right)\right).}\end{array}\]</span>​  其中DConv表示一个3×3的深度向量卷积层。通过将异常分数<spanclass="math inline">\(\hatS_{u}\)</span>与异常图Mu进行逐元素相乘，可以捕捉异常信息。随后，我们计算生成的异常感知图与输入特征图X之间的逐元素求和，从而获得异常敏感特征图。整个过程可描述为：<span class="math display">\[\hat{X}=X+\hat{S}_{a}\timesM_{a}+\hat{S}_{m}\times M_{m}\]</span></p><h2 id="损失函数">4.4 损失函数</h2><p>​  在检测任务中，我们采用(Wang et al.2020)提出的轻量级主干网络，并基于第四阶段特征进行二分类预测yˆ。定位任务则使用多层感知机解码器(Xieetal.2021)作为分割头，生成预测掩膜ˆM。给定真实标签y和掩膜M，我们通过以下目标函数训练GIMFormer模型：<span class="math display">\[{\mathcal{L}}={\mathcal{L}}_{c ls}(y,{\hat{y}})+{\mathcal{L}}_{s e g}(M,{\hat{M}}),\]</span>​  其中Lcls和Lseg均为二元交叉熵损失函数。</p><h2 id="实施细节">4.5 实施细节</h2><p>​  我们的方法包含两个独立的训练步骤。首先，我们使用ImageNet生成的数据集训练ShadowTracer模型。该训练过程采用了与前一章所述类似的数据生成方法。接着，根据GIM框架中描述的两种配置方案（如前一节所述），我们对模型的编码器和解码器进行训练。模型在8个V100GPU上运行，初始学习率设为6e−5，采用幂策略（功率0.9）进行20轮周期的调度。优化器选用AdamW（Loshchilov和Hutter2017），参数设置为epsilon 1e−8、权重衰减1e−2，每个GPU的批量大小为4。</p><h1 id="结论">5. 结论</h1><p>​  我们针对生成式操控检测与定位的挑战，为人工智能生成内容（AIGC）安全领域构建了可靠的GIM数据库。该数据集通过整合多个生成器提供多样化操控数据，并基于此设计了IMDL方法的基准测试框架，包含两种实验场景。同时创新性地提出基于Transformer的GIMFormer框架。大量实验数据表明，该框架实现了当前最先进的性能表现。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Loupe:A Generalizable and Adaptive Framework for Image Forgery Detection</title>
      <link href="/Loupe/"/>
      <url>/Loupe/</url>
      
        <content type="html"><![CDATA[<p>Loupe: A Generalizable and Adaptive Framework for Image ForgeryDetection</p><p>Yuchu Jiang1,2∗， Jiaming Chu2,3∗， Jian Zhao2,5∗， Xin Zhang2,4，XuYang1†， Lei Jin3， Chi Zhang2， Xuelong Li2†</p><p>1 东南大学<br/>2 EVOL Lab, TeleAI of China Telecom<br/>3北京邮电大学<br/>4 兰州大学<br/>5 西北工业大学</p><h1 id="摘要">摘要</h1><p>​  生成模型的迅猛发展引发了人们对视觉内容伪造的严重担忧。现有的深度伪造检测方法主要针对图像级分类或像素级定位。虽然部分方法能达到高精度，但往往存在泛化能力不足的问题，或是依赖复杂的架构设计。本文提出名为<strong>Loupe</strong>的轻量级框架，该框架能有效实现深度伪造的联合检测与定位。Loupe通过整合补丁感知分类器与带条件查询的分割模块，实现了全局真实性分类与细粒度掩码预测的同步处理。为增强对测试集分布偏移的鲁棒性，该模型创新性地采用伪标签引导的测试时自适应机制，利用补丁级预测结果对分割头进行监督学习。基于DDL数据集的大量实验表明，Loupe模型在IJCAI2025深度伪造检测与定位挑战赛中以0.846的综合得分稳居榜首。我们的研究结果验证了提出的补丁级融合与条件查询设计方案，在不同伪造模式下均能有效提升分类准确率和空间定位精度。相关代码可在https://github.com/Kamichanw/Loupe获取。</p><h1 id="引言">1 引言</h1><p>​  生成式人工智能领域的最新突破[Croitoru <em>et al.</em>, 2023;Shuai<em>et al.</em>, 2024; Zhan <em>et al.</em>,2023]显著提升了生成逼真高质量图像的能力，使得创作高度还原现实世界的数字内容成为可能。然而这些技术进步也引发了重大隐忧——特别是针对制造欺骗性内容的恶意应用风险，这类内容往往旨在误导公众或篡改历史叙事。针对这些风险，计算机视觉领域正积极研发先进的深度伪造检测技术。现有方法[Lin<em>et al.</em>, 2024a; Yan <em>et al.</em>,2023]主要侧重于评估整幅图像的真实性（即真实或伪造），同时也有新兴研究方向专注于定位篡改区域[Guo<em>et al.</em>, 2023;Li <em>et al.</em>,2024]。<br/>​  具体而言，早期方法主要依赖视觉网络（如卷积神经网络CNN和视觉变换器ViT）[Pei<em>et al.</em>, 2024; Guo <em>et al.</em>, 2023;Li <em>et al.</em>,2024]或频域分析[Pei <em>et al.</em>,2024; Kwon <em>et al.</em>, 2022;Tan <em>et al.</em>,2024]来提取生成对抗网络或扩散模型生成图像的特征，以实现伪造检测与定位。然而，这些方法通常架构复杂且领域特定，对不同生成技术产生的图像普遍性较差[Pei<em>et al.</em>, 2024; Lin <em>et al.</em>,2024b]。另一方面，近期研究采用视觉-语言模型（VLMs）[Huang <em>etal.</em>, 2024; Kang <em>et al.</em>,2025]，这类模型在提供可解释性的同时实现了伪造检测与定位，并展现出优异性能。但VLMs所需的庞大计算资源限制了其实际应用。因此，开发一种计算效率高、结构简洁且能跨多种伪造技术泛化的解决方案至关重要。<br/>​  本文提出了一种名为Loupe的创新框架，用于图像伪造检测和伪造区域定位，旨在同时进行真实性分类和精确的篡改区域定位。Loupe集成了图像编码器、分类器和分割器，共同建模了真实性验证和伪造区域定位任务。在测试阶段引入监督信号以实现动态自适应，是我们研究的核心目标。需要指出的是，图像级分类通常比像素级分割复杂度更低，这意味着分类网络往往能取得更优性能。值得庆幸的是，随着大规模视觉预训练技术的突破，当前最先进的视觉骨干网络已可直接应用于密集预测任务，无需构建复杂的分割网络架构[Bolya<em>et al.</em>, 2025;Tschannen <em>et al.</em>, 2025; Oquab <em>etal.</em>, 2023; Kerssies <em>etal.</em>,2025]。因此，在我们的分类器中，除了传统的全图像预测外，我们还结合了分块预测，从而得到低分辨率的掩膜预测。这种掩码预测结果在测试时可作为伪标签使用，起到监督信号的作用来指导分割器。此外，将分块级预测结果与传统的全图预测结果相结合，即可得到最终结果。这种融合策略增强了图像级预测的鲁棒性和可靠性。<br/>​  我们在DDL数据集[Organizers]上评估了Loupe的有效性。在验证集上，分类AUC达到0.946，而分割IoU和F1分数分别达到0.880和0.886。在测试集上，分类AUC达到0.963，分割IoU和F1分数分别为0.756和0.819。值得注意的是，测试集存在轻微的分布偏移，包含训练集中未出现的伪造技术。尽管如此，Loupe仍展现出稳健的性能表现，这证实了所提出的框架本身及测试时自适应方法的有效性。</p><h1 id="方法">2 方法</h1><p>​  图1展示了Loupe的整体架构，包含三大核心组件：图像编码器、分类器和分割器（由条件像素解码器与掩码解码器构成）。</p><figure><img src="../postimages/Loupe/image-20250726223355057.png"alt="image-20250726223355057" /><figcaption aria-hidden="true">image-20250726223355057</figcaption></figure><p>​  训练过程分为两个阶段：第一阶段冻结图像编码器，训练分类头；第二阶段则在保持编码器冻结的情况下训练分割头。各阶段的具体方法详见第2.1节和第2.2节。第2.3节将详细说明如何运用Loupe实现测试时的自适应调整。</p><h2 id="第一阶段分类">2.1 第一阶段：分类</h2><p>​  在第一阶段，我们训练分类头来判断输入图像是否真实或伪造。给定一张图像<spanclass="math inline">\(I\in{\mathbb R}^{3\times H\timesW}\)</span>（其中H和W分别表示图像的高度和宽度），首先通过图像编码器处理生成特征表征<spanclass="math inline">\(F_{16}\in{\mathbb R}^{H/16\times W/16\timesD}\)</span>（其中D表示图像编码器的输出维度，假设其块尺寸为16）。随后将生成的特征图F传递给块感知分类器进行分析。</p><figure><img src="../postimages/Loupe/image-20250726223724034.png"alt="image-20250726223724034" /><figcaption aria-hidden="true">image-20250726223724034</figcaption></figure><p>​  如图2a所示，该补丁感知分类器的架构设计包含三个核心模块：首先通过池化层提取全图像全局特征，随后将这些特征输入多层感知机（MLP）进行全局预测；接着独立运行的MLP模块对每个图像像素进行局部特征分析，生成局部预测结果；最终通过简单的线性层融合全局与局部预测信息，输出最终分类结果<spanclass="math inline">\(\haty\)</span>。<br/>​  在图像真实性分类任务中，伪造样本的数量通常远少于真实样本。为缓解类别不平衡问题（即多数类可能主导学习过程），我们采用多焦点损失函数$ {L}_{patch} $ [Leng <em>et al.</em>,2022]作为样本预测的监督目标：<spanclass="math display">\[{\mathcal{L}}_{\mathrm{patch}}={\frac{1}{N}}\sum_{i=1}^{N}\left[-\alpha(1-p_{i})^{\gamma}\log(p_{i})+\epsilon(1-p_{i})^{\gamma+1}\right].\]</span>​  其中，N = H/16×W/16表示总补丁数量，<spanclass="math inline">\(p_i\)</span>代表第i个补丁中伪造类别的预测概率，α和γ是focal损失系数，而ϵ则是多项式项的缩放因子。这种公式设计能促使模型更关注难样本或代表性不足的样本。<br/>​  此外，全局预测采用标准二元交叉熵损失<spanclass="math inline">\({\mathcal{L}}_{\mathrm{global}}\)</span>进行监督。最终分类损失由补丁级损失与全局损失之和构成：<spanclass="math display">\[{\mathcal{L}}_{\mathrm{cls}}={\mathcal{L}}_{\mathrm{patsh}}+{\mathcal{L}}_{\mathrm{global}}.\]</span></p><h2 id="第二阶段分割">2.2 第二阶段：分割</h2><p>​  在第二阶段，我们训练分割头生成像素级掩码。参照DetVit[Li <em>etal.</em>, 2022]的方法，我们在图像编码器输出的<spanclass="math inline">\(F_{16}\)</span>特征图上应用轻量级特征金字塔网络（FPN），提取1/4、1/8、1/16和1/32分辨率的多尺度特征，最终得到{F4，F8，F16，F32}四个层级的特征流<spanclass="math inline">\(F_i\in{\mathbb R}^{D\times H_i\timesW_i}\)</span>。在分割预测环节，我们采用Mask2Former[Cheng <em>etal.</em>,2022]架构。作为第一步，为增强特征信息，我们引入了改进版的条件像素解码器。在第i层中，特征图<spanclass="math inline">\(F_{i}\ \in\\{F_{4},F_{8},F_{16},F_{32}\}\)</span>通过多尺度可变形注意力机制（MSDA，multi-scaledeformable attention）进行优化，输出处理后的特征流<spanclass="math inline">\(\tildeF_i\)</span>。该过程实现了跨空间分辨率的信息自适应聚合，同时保持计算效率。<br/>​  为支持第2.3节提出的伪标签引导式测试时自适应机制，MSDA生成的特征通过交叉注意力层进行深度处理。</p><figure><img src="../postimages/Loupe/image-20250726223724034.png"alt="image-20250726223724034" /><figcaption aria-hidden="true">image-20250726223724034</figcaption></figure><p>​  如图2b所示，这些特征在与条件查询交互的过程中，不仅实现了空间聚合的精准控制，还整合了高层次语义信息，使后续掩码解码器能够生成兼具语义深度与空间精度的掩码。在此过程中，多尺度特征被转化为既保持分辨率一致性又富含语义内涵的表征形式。<br/>​  掩码解码器的结构和训练流程与Mask2Former保持一致。与第2.1节所述的补丁分类方法类似，我们采用多焦点损失函数替代标准二元交叉熵进行监督分割分类。为有效缓解模型过度预测真实区域（如漏检）的问题，我们引入Tversky损失函数$ {L}_{tversky} $ [Salehi <em>et al.</em>, 2017]作为辅助目标函数：</p><p><spanclass="math display">\[{\mathcal{L}}_{\mathrm{twrsky}}=1-{\frac{\mathbf{TP}}{\mathrm{TP}+\alpha\cdot\mathbf{FP}+\beta\cdot\mathbf{FN}}},\]</span>​  其中TP、FP和FN分别表示真阳性、假阳性和假阴性的数量。系数α和β用于调节假阳性和假阴性的惩罚值，从而在精确度与召回率之间进行权衡。在实验中，我们设定α= 0.3和β =0.7，以优先保证召回率并减少对伪造区域的漏检。<br/>​  总损失函数的表达式为：<spanclass="math display">\[\mathcal{L}_{\mathrm{seg}}=\lambda_{1}{\mathcalL}_{\mathrm{mask}}+\lambda_{2}{\mathcalL}_{\mathrm{twesky}}+\lambda_{3}{\mathcal L}_{\mathrm{box}},\]</span>​  其中，<span class="math inline">\({\mathcalL}_{\mathrm{mask}}\)</span>是使用多焦点损失来处理类别不平衡的分类损失函数，该函数会赋予少数类（伪造区域）更高的权重。<spanclass="math inline">\({\mathcalL}_{\mathrm{box}}\)</span>是边界框损失函数，其中使用匈牙利匹配将预测的边界框最优地分配到真实伪造区域，确保预测伪造区域的空间一致性。</p><h2 id="伪标签引导的自适应">2.3 伪标签引导的自适应</h2><p>​  如第1节所述，传统伪造检测方法普遍存在泛化能力不足的问题，导致在面对分布外（OOD，out-of-distribution）数据的真实场景中难以有效应用。因此，在测试阶段如何有效利用已训练模型成为关键挑战。针对这一难题，我们提出了一种创新方案——通过在测试阶段向分割框架注入监督信号，为模型训练提供持续优化的反馈机制。<br/>​  为实现这一目标，在第二阶段训练过程中，我们定义了两个可学习的嵌入向量来表征“真实”与“伪造”两种语义类别。根据真实标签，系统会选取对应的嵌入向量与条件像素解码器中的图像特征Fi进行交互。测试时，我们将分类器的最终输出作为伪标签，通过在两个语义嵌入向量之间进行插值处理，为像素解码器提供额外约束条件。将补丁级别的预测结果视为低分辨率掩膜，随后将其输入掩膜解码器进行监督学习。</p><h1 id="实验">3 实验</h1><h2 id="设置">3.1 设置</h2><p>​  <strong>数据集和评估</strong><br/>​  我们在DDL数据集上对Loupe模型进行了训练和评估，该数据集包含真实/伪造分类和空间定位任务。数据集包含超过150万张图像，涵盖61种操控技术，如单人脸和多人脸篡改场景。在评估中，我们采用ROC曲线下面积（AUC）作为检测指标，F1分数和交并比（IoU）作为空间定位指标（其中IoU仅针对伪造样本计算）。</p><p>​  <strong>实施细节</strong><br/>​  我们采用感知编码器[Bolya等人，2025]作为图像编码器。在分割器部分，像素解码器和掩码解码器的大部分架构参数与Mask2Former[Cheng等人，2022]保持一致，但将可学习查询数量设置为20。每个训练阶段运行一个周期，使用AdamW优化器进行训练。为调整学习率，我们采用了预热稳定衰减调度器[胡等人，2024]，其中前10%的训练步骤用于预热，最后10%用于学习率衰减。更多超参数详见附录A。</p><h2 id="结果">3.2 结果</h2><p>​  Loupe在IJCAI2025深度伪造检测与定位挑战赛中勇夺冠军。排行榜前五名的具体数据详见表1。我们的方法总分比亚军高出0.03分，而亚军至季军的分差更是仅有不到0.001分。</p><figure><img src="../postimages/Loupe/image-20250731194829520.png"alt="image-20250731194829520" /><figcaption aria-hidden="true">image-20250731194829520</figcaption></figure><p>​  在验证集上，Loupe实现了0.947的分类AUC值，以及0.880和0.886的分割IoU和F1分数。尽管测试集与训练集及验证集相比存在轻微的数据分布偏移，但Loupe——尤其是分类AUC指标——基本保持稳定，这表明我们的方法具有较强的鲁棒性。</p><h2 id="消融研究">3.3 消融研究</h2><p>​  我们在DDL数据集的验证集上进行了一系列消融研究，以评估我们提出的方法的有效性。</p><p>​  <strong>补丁感知分类器</strong><br/>​  我们通过移除局部预测来验证其重要性。如表2所示，局部预测相比传统的全局方法有显著提升，证明了局部-全局融合策略的有效性。</p><figure><img src="../postimages/Loupe/image-20250731194947369.png"alt="image-20250731194947369" /><figcaption aria-hidden="true">image-20250731194947369</figcaption></figure><p>​  <strong>条件像素解码器</strong><br/>​  表3表明，Loupe模型通过我们提出的条件查询方法获得了显著优势。在将图像特征输入掩码解码器前，先用语义嵌入进行条件化处理，这种方法不仅实现了测试时的自适应调整，还增强了预测掩码与底层伪造类型之间的语义一致性，从而实现更精准且具有上下文感知能力的定位。</p><figure><img src="../postimages/Loupe/image-20250731195016187.png"alt="image-20250731195016187" /><figcaption aria-hidden="true">image-20250731195016187</figcaption></figure><h1 id="结论">4 结论</h1><p>​  在本工作中，我们引入了Loupe，这是一个统一和高效的框架，用于深度伪造检测和伪造区域定位。通过将补丁感知分类器与条件像素解码器相结合，Loupe在保持架构复杂度极低的同时，实现了全局与局部预测的鲁棒性。此外，我们提出了一种伪标签引导的测试时自适应机制，有效提升了模型在分布偏移下的泛化能力。在DDL数据集上的大量实验表明，Loupe取得了业界领先的表现，在IJCAI2025深度伪造检测与定位挑战赛中超越了所有竞争对手。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Neural Clustering based Visual Representation Learning</title>
      <link href="/Neural-Clustering-based-Visual-Representation-Learning/"/>
      <url>/Neural-Clustering-based-Visual-Representation-Learning/</url>
      
        <content type="html"><![CDATA[<p>Neural Clustering based Visual Representation Learning</p><p>Guikun Chen1 , Xia Li2 , Yi Yang1 , Wenguan Wang1*</p><p>1 ReLER, CCAI, 浙江大学 <br/>2 ETH Zurich</p><p>https://github.com/guikunchen/FEC/</p><h1 id="摘要">摘要</h1><p>​  我们深入探究机器视觉的核心课题——特征测量，通过重新审视机器学习与数据分析领域经典方法中的聚类技术展开研究。现有视觉特征提取器（包括卷积神经网络、视觉图卷积网络和多层感知机）通常将图像表示为矩形区域。尽管这种网格化范式应用广泛，但其构建基于工程实践，缺乏对数据分布的显式建模。本研究提出“聚类特征提取”（FEC）框架，这是一个概念精妙却出人意料地具有自适应可解释性的神经聚类系统。该框架将特征提取视为从数据中筛选代表元素的过程，从而自动捕捉数据的底层分布规律。在图像处理中，FEC算法通过两种交替操作实现：首先将像素分组为独立簇以提取抽象特征，随后利用当前特征向量更新像素的深度特征。这种迭代机制通过多层神经网络实现，最终生成的特征向量可直接应用于下游任务。各层间的聚类分配过程可供人工观察验证，使得FEC的前向计算过程完全透明化，并赋予其出色的自适应可解释性。针对多种视觉识别模型和任务的大量实验验证了FEC的有效性、普适性和可解释性。我们期待这项研究能促使学界重新审视当前主流的网格式架构。</p><h1 id="引言">1.引言</h1><p>​  特征测量，探索如何从高维图像数据中提取抽象、有意义的特征，是机器视觉历史上一个持久感兴趣的话题[1–3]。这种追求最初由人工设计的描述符[4–9]主导，在深度学习范式的影响下，从卷积景观[10,11]发展到注意力驱动机制[12,13]和基于MLP的方法[14,15]的前沿。卷积神经网络（ConvNets，图1a）将图像视为矩形区域，并采用滑动窗口的方式进行处理。基于注意力机制的方法（图1b)通常将图像分割为多个不重叠的块，并使用额外的[CLS]标记来表示整个图像。基于多层感知机（MLP）的骨干网络（图1c)同样遵循网格式架构，但在特征提取过程中不使用卷积或注意力机制。</p><figure><imgsrc="../postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722162252128.png"alt="image-20250722162252128" /><figcaption aria-hidden="true">image-20250722162252128</figcaption></figure><p>​  在观察图1所示的视觉主干网络阵列后，自然会产生以下疑问：❶这些网络之间存在何种关联？更重要的是，❷如果这些神经网络确实隐含地捕捉到了图像数据的某些内在属性，是否可能存在一种更透明、更易解释的方式来测量视觉特征？<br/>​  对问题❶的探索揭示了图像数据分析领域中对网格中心观点的持续坚持[16–18]。具体来说，<strong>现有骨干网络在前向处理过程中涉及的基本元素是矩形图像区域</strong>，例如基于卷积的骨干网络中的内核（滤波器）、滑动窗口和感受野，以及视觉Transformer（ViTs）和MLP中的图像块。这种广泛采用的范式虽然在卷积网络及其后续发展过程中起到了关键作用，但似乎更多是基于工程惯例而非对自然图像结构的模仿。现有大多数研究预计会随着网络层数的增加生成更抽象的特征，但没有人知道它们是如何实现的[19]。因此，问题❷变得更加根本：❸这种网格式架构存在哪些固有局限性？❹我们能否超越基于网格的统一假设，该假设无法体现图像的有机结构？<br/>​  在问题❸的驱动下，我们发现了两个关键的限制：</p><ul><li>首先，网格模型与像素组织的真实性质不一致，因此无法把握数据分布的复杂性[20]。</li><li>其次，深度特征提取器的黑盒特性阻碍了可解释性，掩盖了特征选择和显著性背后的原理。</li></ul><p>​  这引出了两个核心问题：❹探究当前方法论的不透明性及其与人类感知认知的差异[21–23]，后者具有将视觉场景分解为语义成分的独特能力。我们的目标是构建能够更精准捕捉像素数据分布特征、并模拟人类视觉认知过程的特征提取器，从而提升模型的可解释性和透明度。为弥合已发现的差距，必须进行根本性的范式转变：i）从图像表征的网格视图转向更灵活的模型，以包容视觉数据的动态特性；ii)从黑盒模型转向雄心勃勃的混合模型，整合强大的表征学习和可解释的特征编码。</p><p>​  在此脉络中，我们引入了FEC（第3节），这是一种基于聚类原理的机制可解释主干网络。其核心始于基于窗口的池化操作，生成作为初始元素的像素块。随后，FEC通过两个关键流程迭代运行：</p><p>​  i）基于聚类的特征池化。利用神经聚类算法对输入数据（像素块或先前聚类）进行建模，从而形成更抽象（不断扩大的）聚类结构。由于具有聚类特性，每个代表（聚类）能明确表征任意位置的像素集合，这正是FEC区别于网格式架构的关键所在。</p><p>​  ii)基于聚类的特征编码。在此阶段首先估计代表点，再根据像素与其代表点之间的相似度，将特征重新分配至各像素。</p><p>​  在这样的聚类框架下，FEC前向传播过程中的基础单元正是逐步扩大的聚类结构。</p><p>​  FEC具有几个引人注目的特点：</p><ul><li>首先，增强的简洁性和透明度。精简的设计，结合特征提取过程中聚类的语义意义，使得FEC在概念上既优雅又易于实现。代表模型的构建机制确保了FEC的前向过程完全透明。</li><li>其次，实现底层数据分布的自动化分配。确定性聚类方法能够揭示图像数据像素间的潜在关联，捕捉到标准主干网络可能忽略的语义粒度差异。如图1d所示，FEC算法无需人工监督即可自主学习区分非网格结构的语义区域。</li><li>最后，<strong>ad-hoc</strong>可解释性。通过深入分析各特征池化过程中的聚类分配并整合这些结果，FEC算法能在前向处理阶段基于聚合后的聚类结果进行预测解释，让用户直观掌握语义组件的构成。这种<strong>ad-hoc</strong>可解释性在安全敏感场景中具有重要价值，为人类理解特征提取的前向过程提供了切实可行的解决方案。</li></ul><p>​  通过回答问题❶-❹，我们在基于神经聚类的全透明框架中实现了视觉特征提取的理论化，弥合了经典聚类算法与神经网络可解释性之间的鸿沟。我们在第4节进行了文献综述及相关讨论。FEC作为直观且多功能的特征提取器，无需任何修改即可无缝兼容现有的视觉识别模型和任务。第5.1节的实验结果表明，仅需550万个参数，FEC在ImageNet[24]数据集上就能达到72.7%的前1名准确率。第5.2节通过建模代表展示了FEC如何捕捉数据分布特征。第5.3和5.4节则通过三个基础识别任务验证了FEC的迁移性和通用性。最后，我们在第6节总结了研究结论。</p><h1id="现有视觉特征提取器作为固定网格式解析器">2.现有视觉特征提取器作为固定网格式解析器</h1><p>​  <strong>问题陈述</strong><br/>​  本文研究标准分类场景。设X为输入空间（即视觉识别的图像空间），Y={猫、狗等}表示语义类别集合，例如ImageNet-1K[24]的类别数为|Y|=1000。</p><p>​  <strong>标准流程</strong><br/>​  当前常用的分类方法是将深度神经网络<spanclass="math inline">\(h:{\mathcal X}\mapsto{\mathcalY}\)</span>分解为特征提取器<span class="math inline">\(f:{\mathcalX}\mapsto{\mathcal F}\)</span>和分类器<spanclass="math inline">\(g:{\mathcal F}\mapsto{\mathcalY}\)</span>，满足<span class="math inline">\(h = g\circf\)</span>。其中，f和g分别表示特征提取器和分类器。给定输入图像X时，特征提取器f将其映射到d维表示空间<spanclass="math inline">\({\mathcal F}\in{\mathbb R}^{C}\)</span>，即<spanclass="math inline">\(f=f(x)\in{\mathbbR}^{C}\)</span>；而分类器g则基于中间特征f预测类别结果<spanclass="math inline">\(\hat y\)</span>，即<spanclass="math inline">\(\hat y=g(f)\in{\mathcalY}\)</span>。本研究重点聚焦于特征提取器f的优化。</p><p>​  <strong>ConvNets</strong><br/>​  基于卷积的特征提取器多年来一直主导着学术界和工业界，其详细架构如下所述。形式上，给定输入图像<spanclass="math inline">\(X\in{\mathbbR}^{3×H×W}\)</span>，卷积神经网络（ConvNets）会提取特征嵌入<spanclass="math inline">\(\{F^{l}\}^4_{l=1}\)</span>，其中分辨率分别为原始图像的1/4、1/8、1/16、1/32。这四个特征嵌入由四个独立阶段生成，每个阶段都包含网格式特征池化和编码。以ResNet18[11]的第二阶段为例，给定第一阶段输出的<spanclass="math inline">\(F^1\in{\mathbbR}^{64×56×56}\)</span>，将生成如下低维特征图： <spanclass="math display">\[\hat{F}^{2}={\mathrm {grid\_pool}}(F^{1})\in\mathbb{R}^{128\times28\times28}\]</span>​  其中网格池表示步长为2的卷积层，也可以用最大池化、平均池化等实现，之后进行特征编码得到该阶段的输出：<span class="math display">\[{F}^{2}={\mathrm {encode}}(\hat{F}^{2})\in\mathbb{R}^{128\times28\times28}\]</span>​  其中，encode表示多个卷积层，这些层能保持输出分辨率的一致性。这一步骤是区分不同骨干网络的关键所在，具体实现方式包括ViTs中的自注意力机制和MLP中的标记混合器。ViTs[13]和MLPs[14]都通过为图像中所有非重叠区域生成视觉标记嵌入来启动运算流程：<span class="math display">\[E=\mathrm{token\_emb}(X)\]</span>​  在此之后，ViTs使用[CLS]标记来表示整个图像，而MLP则通过计算所有图像块嵌入的平均值来实现这一目标。由于在整个特征提取的前向过程中都使用了图像块序列，我们也将其归类为网格式范式。<br/>​  总体而言，现有视觉模型主要基于刚性网格的计算建模构建，这种模式通过规则区域来呈现图像。然而，该范式低估了视觉场景的动态特性，其假设的空间均匀性与像素数据的实际分布存在矛盾。此外，它还忽视了人类感知的本质——人类感知并不局限于刚性网格，而是能够灵活地在语义上下文中进行导航[25]。<br/>​  在解决了问题❸之后，我们将在下一节详细阐述基于聚类的透明视觉特征提取器，这将是对问题❹的有力回应。</p><h1id="基于聚类的特征提取fecfeature-extraction-with-clustering">3.基于聚类的特征提取（FEC，FeatureExtraction with Clustering）</h1><figure><imgsrc="../postimages/Neural-Clustering-based-Visual-Representation-Learning/image-20250722173244493.png"alt="image-20250722173244493" /><figcaption aria-hidden="true">image-20250722173244493</figcaption></figure><p>​  <strong>算法概述</strong><br/>​  FEC是一种基于神经聚类的视觉特征提取框架，其核心思想是采用分层选择代表的方法。具体来说，当输入图像进入FEC时，系统首先使用标准卷积层进行处理，其核尺寸和步长均设为4。随后基于生成的4×4像素块进行特征提取。接下来，FEC会针对每个输入特征交替执行以下步骤：</p><ul><li>基于聚类的特征<strong>编码</strong>，即<spanclass="math inline">\({\mathbb R}^{C\times W\times H}\mapsto{\mathbbR}^{C\times W\timesH}\)</span>。它通过将像素特征投影到相似性空间，并使用自适应（步长和核尺寸自动选择以适应所需的分辨率）平均池化来初始化聚类中心，将特征图中的像素划分为多个不重叠的簇。因此，可以根据像素与中心点之间的相似度进行聚类分配。随后，通过聚合像素特征构建聚类表征。接着采用特征调度技术——利用聚合后的中心点重新分配聚类内的像素特征，从而实现像素级特征编码，即信息传递过程。这样一来，同一聚类内的像素元素在特征空间中就会呈现出更高的一致性。</li><li>基于聚类的特征<strong>池化</strong>，即<spanclass="math inline">\({\mathbb R}^{C\times W\times H}\mapsto{\mathbbR}^{C^{\prime}\times W/2\timesH/2}\)</span>。与特征编码过程类似，该模块通过聚类获得簇分配。其核心区别在于直接输出簇表示，无需进行特征编码即可生成低维特征图。这种策略不仅保留了不同语义层级的组合结构，还能将聚类概念无缝融入前馈特征提取流程。</li></ul><p>​  简而言之，我们将目标任务——为视觉输入提取深度特征——形式化为表示选择。通过这种方式，中间代表可以自然替代<spanclass="math inline">\(\;\mathrm{grid\_pool}\;\)</span>。由于这些代表是根据每个输入的上下文计算得出的，因此也可以用于通过特征调度传递信息，这与<spanclass="math inline">\(\;\mathrm{encode}\;\)</span>操作具有相同的功能。然后，我们将详细阐述FEC核心模块的具体操作流程。<br/>​  <strong>中心初始化阶段</strong><br/>​  给定输入特征图<spanclass="math inline">\({\boldsymbol F}\in{\mathbb R}^{N\timesC}\)</span>（其中N=W×H），我们首先通过1×1卷积层将其投影到键空间和值空间，分别得到<spanclass="math inline">\({\boldsymbol K}\in{\mathbb R}^{N\timesC^{\prime}}\)</span>和<span class="math inline">\({\boldsymbolV}\in{\mathbb R}^{N\times C^{\prime}}\)</span>。这里<spanclass="math inline">\(C^{\prime}\)</span>是用于控制维度的超参数。随后，我们使用这些键和值特征来初始化聚类中心：<span class="math display">\[\begin{aligned}&amp;[C_1^k;\cdots;C_O^k]=\mathrm{ada\_pool_O}(\boldsymbol{K})\in\mathbb{R}^{O\times C^{\prime}},\\&amp;[C_1^v;\cdots;C_O^v]=\mathrm{ada\_pool_O}(\boldsymbol{V})\in\mathbb{R}^{O\timesC^{\prime}},\end{aligned}\]</span> ​  其中<spanclass="math inline">\(\mathrm{ada\_pool_O}\)</span>表示在投影空间中使用自适应平均池化生成O特征中心。因此，这些中心会根据每个输入进行自适应初始化，并且梯度可以传递到所有索引。</p><p>​  <strong>表示建模</strong><br/>​  要将<strong>每个元素都分配到一个集群</strong>中，计算相似性矩阵M：<spanclass="math display">\[M=\langle\boldsymbol{K},[C_{1}^{k};\cdot\cdot\cdot\cdotC_{O}^{k}]\rangle\in\mathbb{R}^{N\times O}\]</span>​  其中⟨·，·⟩表示余弦相似度。每个元素根据<spanclass="math inline">\(\mathrm{argmax}(M)\)</span>被唯一分配到一个簇中，从而生成包含N个独热向量的分配矩阵A。通过<strong>簇分配</strong>，第o个代表（簇）的深度特征将通过以下方式聚合：<span class="math display">\[R_{o}=\left(C_{o}^{v}+\Sigma_{n=1}^{N}A_{no}\,V_{n}\right)/\left(1+\Sigma_{n=1}^{N}A_{no}\right)\in\mathbb{R}^{C^{\prime}}.\]</span>​  到目前为止，我们已经获得了低维特征R=[R1，…，RO]（即表示），它可以无缝地替代网格式范式中的网格式架构。</p><p>​  <strong>特征匹配</strong><br/>​  基于“同一簇内的元素应具有相似属性”这一认知，我们提出通过在各簇内部传播信息来增强这种特性。具体而言，我们选择采用与对应中心[26,27]的相似度相关的调制传播方式来实现这一目标。对于簇o中的元素n，其特征函数<spanclass="math inline">\({\boldsymbol F}_n \in {\mathbbR}^{C}\)</span>的更新公式为： <span class="math display">\[{\boldsymbolF}_{n}^{\prime}={\boldsymbol F}_{n}+\mathrm{MLP}(\sigma(\alpha{\boldsymbol M}_{n o}+\beta){\boldsymbolR}_{o})\in\mathbb{R}^{C},\]</span>​  其中σ表示S型函数。参数α和β是可学习的参数，用于调整相似度的缩放和偏移量。更新后的特征<spanclass="math inline">\([{\boldsymbol F}_1^{\prime}；··；{\boldsymbolF}_N^{\prime}]\)</span>是FEC <span class="math inline">\(\;{\mathrmencode}\;\)</span>操作的输出（该过程可重复多次）。由于中心特征是从一组元素中自适应采样得到的，这种调度机制实现了簇内元素与簇中心元素之间的有效通信，从而整体理解图像底层数据分布及上下文信息。从更高维度来看，FEC可视为自注意力机制（非重叠簇）的专属变体，例如：中心初始化与键值矩阵、代表性建模与注意力分数、特征调度与加权聚合的对比。更多实现细节详见附录。</p><p>​  <strong>潜在数据分布的自动发现</strong><br/>​  聚类分配不仅阐明了元素与其表示之间的关系，而且还阐明了特征图中潜在的数据分布。在第l层<spanclass="math inline">\(\{n|{\boldsymbolA}_{no}^l=1\}\)</span>中分配了第o个质心的像素会聚合成一个簇（分割）<spanclass="math inline">\({\boldsymbolS}_o^l\)</span>，从而将整个特征图分解为O个可识别的段，这些段位于第l层。通过以下方式将连续层中的集群链接起来：<spanclass="math display">\[\bar{S}_{h}^{l}=\mathrm{Uninon}(\{S_{o}^{l-1}\mid{\boldsymbol A}_{o h}^{l}=1\}),\]</span> ​  我们构建了一个分层金字塔<spanclass="math inline">\([\bar S^1，\bar S^2，···，\barS^L]\)</span>，它将像素合并为越来越大的片段，并明确揭示了底层数据分布。</p><p>​  <strong>Ad-hoc可解释性</strong><br/>​  该配置通过直接前向处理过程<spanclass="math inline">\(l = 1→L\)</span>，生成链式空间分解<spanclass="math inline">\([\bar S^1，\bar S^2，···，\barS^L]\)</span>，直观地将图像解析呈现给观察者。相比之下，早期技术（如Grad-CAM[28])需要通过回溯过程来突出激活区域。这些方法通常需要复杂的后处理才能揭示隐藏的解析机制。然而，FEC具有机制可解释性，因为其基于逐步增长的聚类（片段）的前向处理过程完全透明。详见文献[29]获取更详细讨论。</p><p>​  <strong>通用性</strong><br/>​  在用特征代表建模特征提取过程后，人们可能会质疑这种新范式在密集预测任务中的适用性。例如，在检测任务中，YOLO[30]和Faster-RCNN等常用模型的训练过程依赖于基于网格的标签分配（锚点）。为了保留必要的网格信息，我们在特征代表的特征中引入了残差连接[11]：<spanclass="math display">\[R_{i}=\mathrm{Rescorn}(F_{i})+R_{i}\in\mathbb{R}^{C^{\prime}}.\]</span>​  这一改进使得每个<spanclass="math inline">\(R_{i}\in\mathbb{R}^{C^{\prime}}\)</span>既能表示网格式范式中的矩形区域，也能代表我们基于聚类范式中选定的代表性区域。如图2a所示，我们在特征提取过程中采用了与现有标准骨干网络相同的四个阶段，从而确保输出特征具有相同的分辨率。简而言之，FEC标志着视觉特征提取领域的一次根本性范式转变，同时完全兼容以往的研究成果。计算流程的具体细节将在第5.2节中详细阐述。</p><p>​  <strong>适应下游任务</strong><br/>​  如前所述，通过引入残差连接机制，FEC可以无缝集成到检测、分割等密集预测任务中，无需对架构进行任何修改。在分类任务方面，我们在最终特征图<spanclass="math inline">\(F^L\)</span>上使用标准分类头，即单层多层感知机（MLP），该模型采用所有代表点的平均值作为输出。更多细节详见附录。我们预计近期的集合预测架构（例如DETR[32])能够更有效地利用建模代表，这将作为未来的研究方向。</p><h1 id="相关工作">4.相关工作</h1><p>​  <strong>聚类</strong><br/>​  聚类作为机器学习领域的基础技术，其核心在于根据数据点的内在特征进行分组。面对海量数据时，聚类的目标是构建具有实际意义的集群模型（无论是否预设数量），这些集群可视为原始数据的综合呈现。通过量化分析，能够准确衡量每个数据点与集群表征之间的相似度。该技术已广泛应用于多个领域，例如场景理解[33–36]、点云分析[37-39]、图像分割[40–44]以及人工智能在科学领域的应用[45–48]。<br/>​  与以往将聚类机制作为辅助手段来实现特定任务的研究不同，我们的方法开创性地提出了从聚类视角学习通用视觉表征的创新思路。所提出的基于聚类的特征提取方法与经典视觉技术中相似像素聚类[49]存在共性，但其创新点在于能够捕捉数据分布的底层规律，并为下游任务生成连续表征。通过将整个特征提取过程重构为聚类方式选择代表元素，FEC在保留聚类固有透明特性的同时，充分发挥了端到端表征学习的鲁棒性。受生物系统同时处理多模态输入的启发，感知器[50,51]模型构建了一组与输入相关的潜在向量。FEC的理论基础源于经典聚类思想，其中间元素具有明确含义，即分段（公式8）。</p><h1 id="实验">5.实验</h1><h1 id="结论与讨论">6.结论与讨论</h1><p>​  在机器视觉领域，如何在保持数据分布可解释性和显式建模的同时，为视觉数据提取强大的分布式表征，始终是业界面临的重大挑战。尽管视觉主干网络已取得显著进展，但主流解决方案仍受限于处理矩形图像块的计算能力——这与人类感知中像素的组织方式形成鲜明对比。本研究通过将特征提取重构为代表性选择，实现了突破性进展，从而构建出透明且可解释的特征提取器。我们的目标是为视觉系统开辟新路径：这些系统不仅性能卓越，更能深入理解视觉场景底层的数据分布规律，从而提升其应用的可信度和清晰度。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mind marginal non-crack regions:Clustering-inspired representation learning for crack segmentation</title>
      <link href="/Mind-marginal-non-crack-regions/"/>
      <url>/Mind-marginal-non-crack-regions/</url>
      
        <content type="html"><![CDATA[<p>Mind marginal non-crack regions: Clustering-inspired representationlearning for crack segmentation</p><p>Zhuangzhuang Chen, Zhuonan Lai, Jie Chen, Jianqiang Li *</p><p>中国深圳，深圳大学</p><h1 id="摘要">摘要</h1><p>​  裂缝分割数据集在获取真实裂缝或非裂缝标签时，始终致力于实现最大程度的清晰度。然而观察发现，在处理边缘非裂缝区域时，由于对比度不足和纹理异质性，模糊性问题依然难以避免。为解决这一难题，我们提出了一种基于聚类启发的表征学习框架，该框架包含自动裂缝分割的双阶段策略。第一阶段通过预处理步骤实现边缘非裂缝区域的精确定位。为此，我们提出了一种模糊感知分割损失（AsegLoss，<em>ambiguity-aware segmentationloss</em>），通过学习分割方差来捕捉上述区域的模糊特征，从而帮助裂纹分割模型更精准地定位这些区域。在第二阶段，为学习这些区域的判别性特征，我们设计了聚类启发式损失（CILoss，<em>clustering-inspiredloss</em>），将监督学习模式转变为无监督聚类方式。我们证明了所提出的方法可以在各种数据集和我们构建的CrackSeg5k数据集上超越现有的裂纹分割模型。</p><h1 id="引言">1.引言</h1><p>​  混凝土结构健康监测在工业场景中具有关键作用[24,29,32,45]，其中裂缝分段检测是最后也是不可或缺的环节。随着混凝土结构老化，维护需求日益增加，若处理不当将导致结构健康状况恶化或出现缺陷[20,22]。因此我们认为，在严重劣化发生前及时修复裂缝至关重要，这能有效减轻人工维护负担[5]。然而，由于对比度低、纹理异质性以及没有领域知识的分割区域的不确定性，边缘非裂纹区域的模糊性仍然是不可避免的。考虑到这一点，裂缝的像素级分割仍然是一个挑战[3,7,63]。<br/>​  当前，深度学习技术的突飞猛进也推动了裂缝分割任务的研究[8,32,41]。例如，DeepCrack[67]提出了一种基于多阶段融合的裂缝分割方法，该方法源自常用的编码器-解码器架构（SegNet[2]）。受ViT [51]启发，Crackformer[32]开发出裂缝transformer网络，通过捕捉长程交互来实现细粒度裂缝分割。JTFN[8]则巧妙利用边界信息，将裂缝边界作为额外监督项应用于裂缝分割任务。然而，这些方法有一个关键的限制：由于边缘非裂纹区域的模糊性，无法提取出区分特征。也就是说，与自然图像中的物体不同，由于裂纹与非裂纹区域之间的对比度较低，可能不存在明显的结构边界。因此，当遇到边缘非裂纹区域的模糊标签时，裂纹分割模型很难提取出判别性特征。为验证这一结论，图1(b)直观展示了裂纹区域与边缘非裂纹区域的像素特征。可以观察到，在特征空间中投影的各类像素分布范围较广，且裂纹/非裂纹特征相互交织并靠近类别边界。因此，与以往仅关注裂纹边界学习的方法不同，我们认为通过解决边缘非裂纹区域的模糊性问题，仍有提升裂纹分割性能的空间。<br/>​  本文提出了一种基于聚类的表征学习框架（CIRL），采用两阶段方法解决上述问题。具体而言，第一阶段首先在预处理中定位边缘非裂纹区域。随后，为捕捉这些区域的模糊性特征，我们创新性地提出了基于Wasserstein距离[1]的Aseg损失函数，用于学习分割方差。借助这种方差信息，我们能够更精准地识别出存在模糊性的区域。直观来看，这些模糊区域往往带有模糊标签，这使得裂缝分割模型难以学习到区分性特征。为此，在第二阶段我们提出了新思路：将现有监督学习方法转变为无监督聚类方式，并借鉴共识理论——特征空间中的局部邻近点更可能属于同一簇，其预测结果应比其他特征[10,59]更具相似性。基于上述研究，我们提出了一种受聚类启发的损失函数——CI损失函数，其核心目标是让邻近特征产生相似预测结果，而远距离特征则呈现差异性预测。值得注意的是，该方法可便捷地集成到大多数裂纹分割模型中。此外，通过整合Aseg和CI损失函数，该方法的灵活性显著提升了裂纹分割模型对裂纹区域的精准分割能力。综上所述，本研究的主要贡献包括：</p><ul><li>据我们所知，这是首个针对边缘非裂纹区域模糊性问题的研究。为此，本文提出了一种受两阶段聚类启发的表征学习（CIRL）框架，旨在摆脱这些模糊标签带来的干扰。</li><li>本文提出了一种模糊感知分割损失（AsegLoss），通过学习分割方差，使裂缝分割模型能够捕捉边缘非裂缝区域的模糊性。</li><li>一种聚类启发式损失函数（CILoss）通过将现有监督学习转换为模糊区域的无监督聚类方式，实现了CIRL的目标。</li><li>大量实验验证了所提出的方法在公共数据集和我们构建的CrackSeg5k数据集上的优越性。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="裂缝分割">2.1裂缝分割</h2><h2 id="裂缝分割损失">2.2.裂缝分割损失</h2><h2 id="深度聚类学习">2.3.深度聚类学习</h2><p>​  鉴于我们的方法旨在对模糊区域进行无监督聚类学习，我们在此简要回顾相关研究进展。当前深度聚类方法大致可分为两类：一类是交替或同步学习特征表示与聚类分配。第一类方法中，DAC[4]和DCCM[56]可作为典型范例，它们交替更新聚类分配与样本间相似度。第二类方法则致力于最大化样本与其增强数据之间的互信息[10]。受对比学习启发，许多无监督聚类方法[9,62]通过结合信息非对称编码（InfoNCE）[42]来构建更优的特征空间。值得注意的是，NNCLR[13]提出了一种创新方案——在对比学习中利用潜在空间中的最近邻作为正样本。但该方法存在一个缺陷：负样本可能包含同类别样本。此外，由于其增强处理仅在图像层面进行，因此难以直接应用于像素级裂缝分割任务。<br/>​  我们提出的CIRL方法与先前专注于裂纹分割模糊性的方法具有相同的研究目标。但本研究在三个方面独具特色：<br/>​  (1)本文聚焦于边缘非裂纹区域而非裂纹边界，因此在处理标注错误时，相比基于边缘或边界关键点的方法，我们的方法对误差的敏感度更低；<br/>​  (2)创新设计的Aseg损失函数能帮助网络学习边缘非裂纹区域的分割差异，从而更精准地定位模糊区域；<br/>​  (3)CI损失函数专门针对像素级裂纹分割任务中的模糊区域问题，通过构建两组特征集进行无监督聚类学习，突破了现有对比聚类学习中仅使用正负样本的传统框架。</p><h1 id="方法">3.方法</h1><p>​  本节将介绍我们提出的两阶段CIRL方法，该方法包含两个连续阶段。具体而言，我们首先在第3.1节阐述CIRL的动机。接下来的第一阶段专注于学习边缘非裂纹区域的模糊性特征，从而准确定位这些模糊区域（第3.2节）。随后第二阶段将监督学习转变为无监督聚类学习来处理模糊区域（第3.3节）。图3展示了我们方法的整体框架。</p><figure><imgsrc="../postimages/Mind-marginal-non-crack-regions/image-20250721225401970.png"alt="image-20250721225401970" /><figcaption aria-hidden="true">image-20250721225401970</figcaption></figure><p>(1)我们的网络首先在模糊感知分割损失（LAseg）的监督下，引入一个标准头来估计标准差并进行分割。随后，学习到的方差图帮助我们在边缘非裂纹区域精确定位模糊区域。(2)我们进一步将现有监督学习问题中这些模糊区域的处理方式转变为无监督聚类模式。随后，我们提出了一种基于聚类启发的损失函数（LCI），帮助网络摆脱模糊标签的干扰并学习判别性特征。移除了标准头模块后，仅需主干网络和分割头即可完成预测任务。因此，我们的框架在推理阶段不会引入额外的计算量和内存占用。</p><h2 id="动机">3.1.动机</h2><p>​  CIRL从一个直观的想法开始：由于裂缝和非裂缝区域之间的对比度较低，因此在边缘的非裂缝区域中存在歧义。因此，这些区域的标签很可能不明确，使得现有的裂缝分割模型很难学习判别性特征。<br/>​  为验证这一直觉，我们在CrackSeg5k数据集上开展实验。具体而言，我们首先在二元交叉熵（BCE）损失函数[11]的监督下训练Crackformer[32]模型，并从随机选取的训练样本中提取裂缝区域及边缘非裂缝区域的像素特征。在预处理阶段，基于真实裂缝分割图<spanclass="math inline">\(y^{gt}\)</span>，通过OpenCV中的膨胀操作生成边缘非裂缝区域图M，具体流程如下：<span class="math display">\[M=d i l a t e(y^{g t})-y^{g t},\]</span>​  其中，dilate（·）表示使用5×5核大小的膨胀操作。随后，我们采用t-SNE[50]算法对图1(b)中裂纹像素与非裂纹像素的特征进行可视化处理，并用不同颜色区分显示。可以观察到，在特征空间中部分裂纹像素与非裂纹像素存在纠缠现象。究其原因，部分处于边缘非裂纹区域的像素具有与裂纹像素相似的外观特征。因此，在训练裂纹分割模型时，直接利用这些区域的标签信息是不明智的做法。<br/>​  在上述讨论的基础上，我们通过额外实验验证了裂缝分割任务中边缘非裂纹区域需要精心设计的解决方案。首先，我们将CrackSeg5k的训练集随机划分为四个子训练集。然后，我们利用上述子训练集以两种方式在BCE损失的监督下训练Crackformer：1）利用边缘非裂纹区域的标签进行训练。2）通过等式1，将边缘非裂纹区域排除在训练之外。这种现象的根源在于：某些模糊区域若直接用于裂缝分割模型训练，可能会产生负面影响；而其他区域则能通过提供像素级的更多训练样本来提升模型性能。为更直观理解这一现象，我们局部模糊区域的可视化结果可参考图2。从直观判断来看，边缘非裂缝区域中的模糊区域需要进一步精确定位，并与其他区域进行区分处理。</p><h2id="第一阶段学习边缘非裂纹区域的模糊性">3.2.第一阶段：学习边缘非裂纹区域的模糊性</h2><p>​  根据前文讨论，在第一阶段，我们的目标是同时估计分割置信度和分割结果，从而捕捉边缘非裂纹区域的模糊性。为此，对于输入图像中位置（i，j）处的像素，我们的网络会预测一个概率分布<spanclass="math inline">\(P_{i,j}^{\Theta}(y)\)</span>，而非单一标签。在此我们假设每个像素级别的预测标签（即单变量）服从独立高斯分布。由此可得以下公式：<spanclass="math display">\[P_{i,j}^{\Theta}(y)=\frac{1}{\sqrt{2\pi\hat{\sigma}_{i,j}^{2}}}e^{-\frac{\left(y-y_{i,j}^{p}\right)^{2}}{2\hat{\sigma}_{i,j}^{2}}}\]</span>​  其中<spanclass="math inline">\(\Theta\)</span>是包含主干、seg头和std头的可学习参数集合，如图3所示。其中，<spanclass="math inline">\(y_{i,j}^{p}\)</span>和<spanclass="math inline">\({\hat{\sigma}}_{i,j}\)</span>分别表示输入图像中位置（i，j）处的预测标签和估计标准差。当<spanclass="math inline">\({\hat{\sigma}}_{i,j}\)</span>非常接近0时，表明我们的网络对当前预测标签具有高度置信度。需要说明的是，如图3所示，我们的标准头是通过在主干网络上添加一个全连接层来实现的。因此，相应的真值标签<spanclass="math inline">\(y_{i,j}^{gt}\)</span>也可以被表述为高斯分布<spanclass="math inline">\({\mathcalN}(y_{i,j}^{gt},\sigma_{i,j}^{2})\)</span>，其标准差为<spanclass="math inline">\(\sigma_{i,j}^{2}\rightarrow0\)</span>。然后，这个高斯分布可以看作：<spanclass="math inline">\(P_{i,j}^{g t}(y)\;=\;\delta\,\left(y-y_{i,j}^{gt}\right)\)</span>，其中δ（·）表示Diracdelta函数。然后，所提出的Aseg损失可表述如下： <spanclass="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{Aseg}}&amp;=\sum_{i=1}^{H}\sum_{j=1}^{W}\frac{D_{W}\left(P_{i,j}^{\Theta}(y)||P_{i,j}^{gt}(y)\right)}{\lambda+\hat{\sigma}_{i,j}^{2}}\\&amp;=\sum_{i=1}^{H}\sum_{j=1}^{W}\frac{\|y_{i,j}^{p}-y_{i,j}^{gt}\|_{2}^{2}+\hat{\sigma}_{i,j}^{2}}{\lambda+\hat{\sigma}_{i,j}^{2}}\end{aligned}\]</span>​  其中H和W分别表示输入图像的高度和宽度。超参数λ的作用将在后文详细说明。本文采用Wasserstein距离作为距离度量<spanclass="math inline">\(D_W(\cdot)\)</span>，通过最小化<spanclass="math inline">\(P_{i,j}^{\Theta}(y)\)</span>与<spanclass="math inline">\(P_{i,j}^{gt}(y)\)</span>之间的距离来实现优化。同时，<spanclass="math inline">\(D_{W}\left(P_{i,j}^{\Theta}(y)||P_{i,j}^{gt}(y)\right)\)</span>可以通过以下假设展开：</p><p>​  <strong>假设1</strong><br/>​  如前所述，由于将<spanclass="math inline">\(P_{i,j}^{g t}(y)\)</span>视是<em>Diracdelta</em>函数，其表达式为： <spanclass="math display">\[\delta\left(y-y_{i,j}^{gt}\right)=\operatorname*{lim}_{\mu\to y_{i,j}^{gt},\Sigma\to0}\mathcal{N}(\mu,\Sigma),\]</span> ​  因此<spanclass="math inline">\(D_{W}\left(P_{i,j}^{\Theta}(y)||P_{i,j}^{gt}(y)\right)\)</span>可推导为以下方程式： <spanclass="math display">\[D_{W}\left(P_{i,j}^{\Theta}(y)||P_{i,j}^{gt}(y)\right)=\|y_{i,j}^{p}-y_{i,j}^{gt}\|_{2}^{2}+\hat{\sigma}_{i,j}^{2}.\]</span>​  <strong>证明1</strong><br/>​  假设我们在<spanclass="math inline">\(\mathcal R^n\)</span>空间中有两个多元高斯分布<spanclass="math inline">\({\mathcal{N}}_{1}(\mu_{1},\Sigma_{1})\)</span>和<spanclass="math inline">\({\mathcal{N}}_{2}(\mu_{2},\Sigma_{2})\)</span>，那么这两个分布之间的<em>Wasserstein</em>距离可以推导如下：<spanclass="math display">\[W_{2}^{2}({\mathcal{N}}_{1}(\mu_{1},\Sigma_{1}),{\mathcal{N}}_{2}(\mu_{2},\Sigma_{2}))=\|\mu_{1}-\mu_{2}\|^{2}+\left(\Sigma_{1}+\Sigma_{2}-2\left(\sqrt{\Sigma_{1}}\Sigma_{2}\sqrt{\Sigma_{1}}\right)^{\frac{1}{2}}\right)\]</span>​  现在，通过将上述方程式中的变量<spanclass="math inline">\(\delta\left(y-y_{i,j}^{g t}\right)\)</span>和<spanclass="math inline">\({\mathcalN}(y_{i,j}^{gt},\sigma_{i,j}^{2})\)</span>代入，我们可以得到假设1。</p><p>​  值得注意的是，之所以采用Wasserstein距离而非KL散度[17]，是因为后者严重依赖于两个分布[1]之间不可或缺的交集。此外，当<spanclass="math inline">\(y_{i,j}^{p}\)</span>被准确预测时，即，当<spanclass="math inline">\(\|y_{i,j}^{p}-y_{i,j}^{gt}\|_{2}^{2}\rightarrow0\)</span>时，我们的网络预计会产生更小的方差。基于这一考量，我们在公式等式3中新增了项<spanclass="math inline">\(\lambda+\hat{\sigma}_{i,j}^{2}\)</span>，其作用机制如下：由于边缘非裂纹区域存在模糊性，当预测标签与真实标签不一致时（即<spanclass="math inline">\(\|y_{i,j}^{p}-y_{i,j}^{gt}\|_{2}^{2}&gt;\lambda\)</span>时），<spanclass="math inline">\({\mathcal{L}}_{\mathrm{Aseg}}\)</span>的最小化过程将促使网络生成更大的方差参数<spanclass="math inline">\(\hat{\sigma}_{i,j}^{2}\)</span>。为确保数值稳定性，我们实际采用对数方差参数<spanclass="math inline">\(\hat{s}=\log\hat{\sigma}_{i,j}^{2}\)</span>进行预测，并据此重新构建公式等式3如下：<spanclass="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{Aseg}}&amp;=&amp;\sum_{i=1}^{H}\sum_{j=1}^{W}\frac{D_{W}\left(P_{i,j}^{\Theta}(y)||P_{i,j}^{gt}(y)\right)}{\lambda+\exp({\hat{s}})}\\&amp;=&amp;\sum_{i=1}^{H}\sum_{j=1}^{W}\frac{\|y_{i,j}^{p}-y_{i,j}^{gt}\|_{2}^{2}+\exp({\hat{s}})}{\lambda+\exp({\hat{s}})}\end{aligned}\]</span>​  现在，由于已学习的方差<spanclass="math inline">\(\sigma_{i,j}^{2}=\exp(\hat{s})\)</span>，位于（i，j）位置的输入像素将通过以下公式判断是否归类为模糊区域：<span class="math display">\[A_{i,j}=\left\{\begin{array}{ll}{1,}&amp;{\mathrm{case}\:\hat{\sigma}_{i,j}^{2}\gt\gamma\;\;\mathrm{and}\;\;M_{i,j}==1}\\{0,}&amp;{\mathrm{case}\:\hat{\sigma}_{i,j}^{2}\leq\gamma\;\;\mathrm{or}\;\;M_{i,j}==0}\end{array}\right.,\]</span>​  其中<spanclass="math inline">\(\gamma\)</span>是一个超参数，用作阈值。根据公式等式1，<spanclass="math inline">\(M_{i,j}=1\)</span>表示当前像素属于边缘非裂纹区域。</p><h2id="第二阶段从监督学习到模糊区域的无监督学习">3.3.第二阶段：从监督学习到模糊区域的无监督学习</h2><p>​  借助前一阶段的辅助，我们能够定位那些标签存在模糊性的区域。若直接在训练过程中使用这些标签，将会产生不可控的影响。为此，在第二阶段我们提出了聚类启发式损失函数（CILoss），其采用无监督特征聚类的方式，其核心思想在于：在特征空间中位置相近/相距较远的特征，其预测结果应呈现一致性/差异性。<br/>​  给定输入图像<spanclass="math inline">\(\mathcal I\)</span>，设<spanclass="math inline">\(S_{\mathcalI}\)</span>表示通过等式8算法确定的模糊区域所有像素的特征集合。<spanclass="math inline">\({\mathcal F}_m\)</span>和<spanclass="math inline">\({\mathcal F}_n\)</span>分别代表<spanclass="math inline">\(S_{\mathcalI}\)</span>表中两个像素的特征值，并对应着预测概率<spanclass="math inline">\(p_m\)</span>和<spanclass="math inline">\(p_n\)</span>。受文献[14,59]启发，我们定义<spanclass="math inline">\(p_{m,n}\)</span>为<spanclass="math inline">\({\mathcal F}_m\)</span>和<spanclass="math inline">\({\mathcal F}_n\)</span>具有相同预测结果的概率：<spanclass="math display">\[p_{m,n}=\frac{e^{p_{m}^{T}p_{n}}}{\sum_{\mathcal{F}_{q}\inS_{\overline{r}}}e^{p_{m}^{T}p_{q}}}.\]</span> ​  对于<spanclass="math inline">\(S_{\mathcal I}\)</span>中的每个特征<spanclass="math inline">\({\mathcalF}_m\)</span>，我们定义了两个集合：邻近特征集<spanclass="math inline">\({\mathcal C}_m\)</span>和远端特征集<spanclass="math inline">\({\mathcalO}_m\)</span>。前者通过余弦相似度作为距离度量，从<spanclass="math inline">\(S_{\mathcal I}\)</span>中选取<spanclass="math inline">\({\mathcalF}_m\)</span>的K个最近邻；后者则是通过排除<spanclass="math inline">\({\mathcal C}_m\)</span>和<spanclass="math inline">\({\mathcal F}_m\)</span>后剩余的<spanclass="math inline">\(S_{\mathcalI}\)</span>特征构建而成。回到研究初衷，对于每个特征<spanclass="math inline">\({\mathcal F}_m\)</span>而言，其在<spanclass="math inline">\({\mathcal O}_m\)</span>中的预测结果应比在<spanclass="math inline">\({\mathcalC}_m\)</span>中更显著地表现出不一致性。基于此，我们定义了<spanclass="math inline">\({\mathcal F}_m\)</span>与<spanclass="math inline">\({\mathcal C}_m\)</span>之间的似然函数： <spanclass="math display">\[P\left({\mathcalC}_m\mid{\mathcal{F}}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)=\prod_{\mathcal{F}_{n}\in{\mathcal C}_{m}}p_{m,n}=\prod_{\mathcal{F}_{n}\in {\mathcalC}_{m}}{\frac{e^{p_{m}^{T}p_{n}}}{\sum_{\mathcal{F}_{q}\in {S}_{\mathcalI}}e^{p_{m}^{T}p_{q}}}},\]</span> ​  其中<spanclass="math inline">\(\theta_{\mathcal{B}}\)</span>和<spanclass="math inline">\(\theta_{\mathcal{S}}\)</span>分别表示网络中主干和seg头部的参数。类似地，<spanclass="math inline">\({\mathcal{F}}_{m}\)</span>与<spanclass="math inline">\({\mathcal{O}}_{m}\)</span>之间的似然函数可定义如下：<span class="math display">\[P\left({\mathcalO}_m\mid{\mathcal{F}}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)=\prod_{\mathcal{F}_{n}\in{\mathcal O}_{m}}p_{m,n}=\prod_{\mathcal{F}_{n}\in {\mathcalO}_{m}}{\frac{e^{p_{m}^{T}p_{n}}}{\sum_{\mathcal{F}_{q}\in {S}_{\mathcalI}}e^{p_{m}^{T}p_{q}}}},\]</span>​  现在，我们可以通过最小化以下负对数似然函数来实现我们的聚类启发式损失的目标：<span class="math display">\[\psi({\mathcal C}_m,{\mathcal O}_m)=-l og{\frac{P\left({\mathcalC}_m\mid{\mathcal{F}}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)}{P\left({\mathcalO}_m\mid{\mathcal{F}}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)}}.\]</span>​  注意到当<span class="math inline">\(S_{\mathcalI}\)</span>非常大时，计算上述方程是低效的甚至是不现实的，考虑到这一点，我们通过以下命题推导出一个上界作为替代方案。</p><p>​  <strong>假设2</strong><br/>​  假设<spanclass="math inline">\(|{\mathcal O}_m|\)</span>显著大于<spanclass="math inline">\(|{\mathcal C}_m|\)</span>，那么我们得到一个由<spanclass="math inline">\(\psi({\mathcal C}_m,{\mathcalO}_m)\)</span>给出的上界。 <span class="math display">\[\begin{aligned}&amp;\psi(\mathcal{C}_{m},\mathcal{O}_{m})=-log\frac{P\left(\mathcal{C}_{m}\mid\mathcal{F}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)}{P\left(\mathcal{O}_{m}\mid\mathcal{F}_{m},\theta_{\mathcal{B}},\theta_{\mathcal{S}}\right)}\\ &amp;\leq-\sum_{\mathcal{F}_{n}\in\mathcal{C}_{m}}p_{m}^{T}p_{n}+\frac{|\mathcal{C}_{m}|}{|\mathcal{O}_{m}|}\sum_{\mathcal{F}_{k}\in\mathcal{O}_{m}}p_{m}^{T}p_{k}+(|\mathcal{C}_{m}|-|\mathcal{O}_{m}|)\log|S_{\mathcal{I}}|\\ &amp;=\overline{\psi}(\mathcal{C}_{m},\mathcal{O}_{m})\end{aligned}\]</span>​  <strong>证明2</strong><br/>​  根据等式10-12，我们有<spanclass="math inline">\(\psi({\mathcal C}_m,{\mathcal O}_m)\)</span>：<span class="math display">\[\begin{aligned} &amp; \psi({\mathcalC}_m,{\mathcal O}_m)\\ &amp;=-\sum_{\mathcal{F}_{n}\in\mathcal{C}_{m}}p_{m}^{T}p_{n}+\sum_{\mathcal{F}_{k}\in\mathcal{O}_{m}}p_{m}^{T}p_{k}+(|\mathcal{C}_{m}|-|\mathcal{O}_{m}|)\log\left(\sum_{\mathcal{F}_{q}\inS_{\mathcal{I}}}e^{p_{m}^{T}p_{q}}\right) \\ &amp;\leq\sum_{\mathcal{F}_{n}\in\mathcal{C}_{m}}p_{m}^{T}p_{n}+\sum_{\mathcal{F}_{k}\in\mathcal{O}_{m}}p_{m}^{T}p_{k}+(|\mathcal{C}_{m}|-|\mathcal{O}_{m}|)\left(\sum_{\mathcal{F}_{q}\inS_{\mathcal{I}}}\frac{p_{m}^{T}p_{q}}{|S_{\mathcal{I}}|}+\log|S_{\mathcal{I}}|\right)\\ &amp;\approx\sum_{\mathcal{F}_{n}\in\mathcal{C}_{m}}p_{m}^{T}p_{n}+\sum_{\mathcal{F}_{k}\in\mathcal{O}_{m}}p_{m}^{T}p_{k}+(|\mathcal{C}_{m}|-|\mathcal{O}_{m}|)\left(\sum_{\mathcal{F}_{q}\in\mathcal{O}_{m}}\frac{p_{m}^{T}p_{q}}{|\mathcal{O}_{m}|}+\log|S_{\mathcal{I}}|\right)\\ &amp;=\sum_{\mathcal{F}_{n}\in\mathcal{C}_{m}}p_{m}^{T}p_{n}+\frac{|\mathcal{C}_{m}|}{|\mathcal{O}_{m}|}\sum_{\mathcal{F}_{k}\in\mathcal{O}_{m}}p_{m}^{T}p_{k}+(|\mathcal{C}_{m}|-|\mathcal{O}_{m}|)\log|S_{\mathcal{I}}|\\ &amp;=\overline{\psi}(\mathcal{C}_{m},\mathcal{O}_{m}),\end{aligned}\]</span>​  其中第一个不等式通过满足Jensen不等式成立，因为对数函数log（·）是凹函数。<br/>​  由于我们有<spanclass="math inline">\(S_{\mathcal{I}}\approx \mathcal{C}_{m}\cup\mathcal{O}_{m}\)</span>且<span class="math inline">\(|{\mathcalO}_{m}|\gg|\mathcal C_{m}|\)</span>，因此在假设<spanclass="math inline">\(S_{\mathcal{I}}\)</span>可近似为<spanclass="math inline">\({\mathcalO}_{m}\)</span>的情况下，我们得到了第三个方程。最后，考虑到整个特征集<spanclass="math inline">\(S_{\mathcal{I}}\)</span>，我们定义了如下基于聚类的损失函数：<spanclass="math display">\[{\mathcal{L}}_{\mathrm{CI}}={\frac{1}{|S_{\mathcal{I}}|}}\sum_{\mathcal{L}_{m}\inS_{\mathcal{I}}}\overline\psi({\mathcal C}_m,{\mathcal O}_m).\]</span>​  借助<spanclass="math inline">\({\mathcal{L}}_{\mathrm{CI}}\)</span>，我们能够对模糊区域进行无监督聚类学习。同时，对于剩余区域，我们采用常用的BCE损失函数进行监督训练。通过这种方式，裂缝分割模型能够摆脱模糊标签的干扰，学习到具有区分性的特征。最终，我们的整体损失函数可表示为：<spanclass="math display">\[\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{BCE}}+\beta\mathcal{L}_{\mathrm{Cl}},\]</span>​  其中标量β用于平衡两个损失函数。</p><h1 id="实验">4.实验</h1><p>​  核心实验方案详见第4.1节。为确定CIRL模型中λ、γ和β参数的最佳取值，我们在第4.2节通过多组实验进行验证。为验证方法有效性，我们不仅在第4.3节与现有裂纹分割模型展开对比，还在第4.4节与其他前沿分割损失函数展开横向比较。特别值得关注的是，第4.5节的消融实验首先验证了我们提出的模糊感知分割损失相较于现有不确定性方法的优势，随后通过实证表明，这种受聚类启发的损失函数在裂纹分割任务中显著优于现有的无监督聚类学习方法。</p><h2 id="实验设置">4.1.实验设置</h2><p>​  在本文中，我们基于我们的CrackSeg5k数据集、两个公共的裂缝分割数据集、一个血管分割数据集以及相应的实现细节进行了广泛的实验。</p><p>​  <strong>实施细节</strong><br/>​  我们在PyTorch2框架下使用单块NVIDIARTX3090显卡进行实验。参照前人研究[8]，我们采用水平翻转、随机裁剪和随机旋转（90◦、180◦、270◦）作为数据增强策略。同时，我们沿用了[8]的训练设置——所有样本在训练阶段均被裁剪为256×256尺寸。优化器选用Adam[21]，初始学习率设为10−3，权重衰减系数为5×10−4，小批量大小为2。针对这四个数据集，模型总训练周期为2000个epoch。此外，在训练初期，若标准头未正确学习，所学方差信息量不足。因此，前1000个epoch作为第一阶段，之后模型采用LBCE与LCI联合监督机制（参考等式16)。</p><p>​  <strong>评估指标</strong><br/>​  为评估裂纹分割的像素级准确率，我们参照现有研究[8,19,47]采用F1分数、精确率和召回率作为评估指标。需要特别说明的是，精确率和召回率是通过逐像素比对预测掩膜与真实掩膜来计算得出的。F1分数的计算公式为：<spanclass="math inline">\(F_{1}=2\frac{P r e c t s i o n\times R e c a ll}{P r e c t s i o n+R e c a l l}\)</span>。</p><h2 id="超参数灵敏度研究">4.2.超参数灵敏度研究</h2><p>​  本文引入了四个超参数：λ用于帮助模型捕捉模糊区域，γ用于在第一阶段定位模糊区域，K用于定义邻近区域集合的大小，β则用于在第二阶段平衡BCE损失和CI损失。图4展示了基于JTFN[8]作为基础分割模型，在CrackSeg5k数据集上的超参数敏感性研究结果。</p><figure><imgsrc="../postimages/Mind-marginal-non-crack-regions/image-20250722112013085.png"alt="image-20250722112013085" /><figcaption aria-hidden="true">image-20250722112013085</figcaption></figure><p>​  可以观察到，四个超参数λ、γ、K和β在较大范围内仅分别比最高值降低了1.5%、0.8%、2.0%和1.8%的F1分数，这表明我们提出的方法在实际应用中具有潜力。根据这些观察结果，我们在后续实验中设定λ= 0.3、γ = 0.4、K=4×5和β = 0.2。</p><h2 id="裂缝分割模型的比较">4.3.裂缝分割模型的比较</h2><h2 id="分割损失比较">4.4.分割损失比较</h2><h2 id="消融研究">4.5.消融研究</h2><h1 id="结论">5.结论</h1><p>​  综上所述，边缘非裂纹区域的模糊性会制约当前最先进的裂纹分割模型性能。本文提出了一种基于两阶段聚类启发式表征学习（CIRL）框架，旨在实现更精准的像素级裂纹分割。该框架包含模糊感知分割损失（AsegLoss），促使网络学习预测每个像素的分割方差，从而进一步精确定位模糊区域。随后引入聚类启发式损失（CILoss），用于学习上述区域的判别特征。实验结果表明，我们的方法在裂纹分割任务中超越了现有最优方案和传统损失函数。鉴于先前研究在频域学习判别特征方面表现优异[39,57]，我们计划进一步探索基于特征的数据增强技术[6]和神经元脉冲网络架构[27,28]在裂纹分割中的应用潜力。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 聚类启发式损失 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoCoOp:Conditional Prompt Learning for Vision-Language Models</title>
      <link href="/CoCoOp/"/>
      <url>/CoCoOp/</url>
      
        <content type="html"><![CDATA[<p>CoCoOp: Conditional Prompt Learning for Vision-Language Models</p><p>Kaiyang Zhou Jingkang Yang Chen Change Loy Ziwei Liu</p><p>S-Lab, Nanyang Technological University, Singapore</p><h1 id="摘要">摘要</h1><p>​  随着CLIP等强大预训练视觉语言模型的兴起，研究如何将这些模型适配到下游数据集变得至关重要。最近提出的一种名为上下文优化（CoOp）的方法，将自然语言处理领域的新趋势——提示学习引入到视觉领域，用于适配预训练的视觉语言模型。具体来说，CoOp技术通过将提示中的上下文词汇转化为可学习的向量集合，并仅需少量标注图像即可实现显著提升，这比人工调优的密集式提示效果更为突出。在我们的研究中发现CoOp存在一个关键缺陷：所学上下文无法泛化到同一数据集内的更广泛未见类别，这表明CoOp在训练过程中过度拟合了基础类别的特征。为解决这一问题，我们提出条件上下文优化（CoCoOp）方案。该方案通过引入轻量级神经网络，为每张图像生成输入条件标记（向量），从而扩展了CoOp的功能。与CoOp的静态提示不同，我们的动态提示能根据具体实例进行自适应调整，因此对类别偏移具有更强的鲁棒性。大量实验表明，CoCoOp在处理未知类别时展现出远超CoOp的泛化能力，不仅在单一数据集上表现出色，更在跨领域迁移性能上更具优势。代码可在https://github.com/KaiyangZhou/CoOp处获得。</p><figure><img src="../postimages/CoCoOp/image-20250721215909226.png"alt="image-20250721215909226" /><figcaption aria-hidden="true">image-20250721215909226</figcaption></figure><h1 id="方法">3方法</h1><p>​  图2展示了我们方法的总体框架。首先，我们将简要回顾本文使用的基础模型CLIP[40]和协作模型CoOp[63]。随后，我们将详细阐述本研究的技术细节及其设计原理。与协作模型类似，我们的方法同样适用于更广泛的CLIP类视觉语言模型。</p><h2 id="对clip和coop的回顾">3.1.对CLIP和CoOp的回顾</h2><p>​  <strong>对比语言图像预训练</strong><br/>​  对比语言-图像预训练模型CLIP[41]已充分展现了学习开放集视觉概念的潜力。如图2所示，该模型采用双编码器架构：图像编码器可选用ResNet[18]或ViT[9]进行特征提取，将图像转化为特征向量；文本编码器则采用Transformer[48]，通过词元序列输入生成向量化表征。<br/>​  在训练过程中，CLIP采用对比损失函数来学习两种模态的联合嵌入空间。具体而言，对于图像-文本对的小批量数据，CLIP会针对每个图像最大化其与匹配文本的余弦相似度，同时最小化与其他未匹配文本的余弦相似度，而每个文本的损失计算方式也与此类似。完成训练后，CLIP即可用于零样本图像识别任务。设x为图像编码器生成的图像特征，{wi}Ki=1为文本编码器产生的权重向量集合（假设总共有K个类别）。特别地，每个wi都是根据提示词生成的，例如“一张{class}的照片”，其中“{class}”标记由第i个类别名称填充。随后预测概率将通过以下公式计算：<spanclass="math display">\[p(y|x)={\frac{\exp(\sinh(x,w_{y})/\tau)}{\sum_{i=1}^{K}\exp(\sin(x,w_{i})/\tau)}},\]</span>​  其中sim(.,.)表示余弦相似度，τ是学习得到的温度参数。</p><p>​  <strong>上下文优化(CoOp)</strong><br/>​  上下文优化（CoOp）旨在解决提示工程中的效率问题，使预训练的视觉-语言模型能更好地适配下游应用[63]。其核心思想是通过端到端学习从数据中提取连续向量来建模每个上下文标记。具体而言，CoOp不采用“一张照片”作为上下文，而是引入M个可学习的上下文向量{v1，v2，...，vM}，这些向量与词嵌入具有相同维度。第i类的提示词ti现在表示为ti={v1，v2，...，vM，ci}，其中ci是类别名称的词嵌入(s)。所有类别共享这些上下文向量。（CoOp有一个学习特定类上下文的替代版本，这里没有考虑它，因为将特定类上下文转移到未见过的类别并不简单。）设g（·）表示文本编码器，预测概率则为<spanclass="math display">\[p(y|x)=\frac{\exp(\mathrm{sim}(x,g(t_{y}))/\tau)}{\sum_{i=1}^{K}\exp(\mathrm{sim}(x,g(t_{i}))/\tau)}.\]</span>​  为了将CLIP模型适配到下游图像识别数据集，可以采用交叉熵损失作为学习目标函数。由于文本编码器g（·）具有可微性，梯度可以完整回传以更新上下文向量。需要特别说明的是，在整个训练过程中，CLIP的基础模型（包括我们提出的模型）始终保持冻结状态。</p><h2 id="cocoop条件上下文优化">3.2. CoCoOp：条件上下文优化</h2><p>​  CoOp是一种数据高效方法，仅需少量标注图像即可训练上下文向量。但正如讨论所示，该方法无法将泛化能力扩展到同一任务中的更广泛未见过类别。我们认为，实例条件上下文具有更好的泛化能力，因为它将关注点从特定类别集合——即减少过拟合——转移到每个输入实例上，从而适用于整个任务。<br/>​  实现CoCoOp的直接方法是构建M个神经网络来获取M个上下文标记。但这种设计需要M×个神经网络单元，其规模远超CoOp中仅需M个上下文向量的架构。为此我们提出了一种参数效率更高的设计方案，该方案在实际应用中表现优异。具体而言，在M个上下文向量基础上，我们进一步学习了一个轻量级神经网络（称为Meta-Net），用于为每个输入生成条件标记（向量），随后将其与上下文向量结合。图2展示了该架构的示意图。</p><figure><img src="../postimages/CoCoOp/image-20250721215909226.png"alt="image-20250721215909226" /><figcaption aria-hidden="true">image-20250721215909226</figcaption></figure><p>​  令<spanclass="math inline">\(h_{\theta}(\cdot)\)</span>表示由参数θ参数化的Meta-Net，此时每个上下文标记通过<spanclass="math inline">\(v_{m}(x)=v_{m}+\pi\)</span>获得，其中<spanclass="math inline">\(\pi=h_{\theta}(x)\)</span>，<spanclass="math inline">\(m\in\{1,2,...,M\}\)</span>。第i类的提示词ti(x)即根据输入条件生成，具体形式为<spanclass="math inline">\(t_{i}(x)=\{v_{1}(x),v_{2}(x),\cdot\cdot\cdot,v_{M}(x),c_{i}\}\)</span>。预测概率的计算方式如下：<spanclass="math display">\[p(y|\alpha)=\frac{\exp(\mathrm{sim}(x,g(t_{y}(x)))/\tau)}{\sum_{i=1}\exp(\mathrm{sim}(x,g(t_{i}(x)))/\tau)}.\]</span>​  在训练过程中，我们会同步更新上下文向量<spanclass="math inline">\(\{v_{m}\}_{m=1}^{M}\)</span>和Meta-Net的参数θ。本研究采用双层瓶颈结构（线性-ReLU-线性）构建Meta-Net，其中隐藏层将输入维度缩减16×倍。Meta-Net的输入直接取自图像编码器生成的特征输出，至于更复杂的设计方案，我们将在后续研究中继续探索。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CoOp:Learning to Prompt for Vision-Language Models</title>
      <link href="/CoOp/"/>
      <url>/CoOp/</url>
      
        <content type="html"><![CDATA[<p>CoOp: Learning to Prompt for Vision-Language Models</p><p>Kaiyang Zhou · Jingkang Yang · Chen Change Loy · Ziwei Liu</p><p>S-Lab<br/>南洋理工大学<br/>新加坡</p><h1 id="摘要">摘要</h1><p>​  像CLIP这样的大型预训练视觉语言模型，在学习可迁移的表征方面展现出巨大潜力，这些表征能够跨多种下游任务有效迁移。与传统基于离散标签的表征学习不同，视觉语言预训练通过在共同特征空间中对齐图像和文本，使得零样本任务也能通过提示实现迁移——即通过自然语言描述目标类别来合成分类权重。在这项工作中，我们表明在实践中部署此类模型的主要挑战是提示工程，这需要领域专业知识和非常耗时——需要花费大量的时间在词语调优上，因为措辞的轻微变化可能会对性能产生巨大的影响。受自然语言处理（NLP）领域提示学习研究的最新进展启发，我们提出了一种名为“上下文优化”（ContextOptimization，简称CoOp）的创新方法。该方案专门针对CLIP类视觉-语言模型在下游图像识别任务中的应用进行了优化设计。具体而言，CoOp通过可学习向量对提示词的上下文词汇进行建模，同时保持所有预训练参数不变。为应对不同图像识别任务，我们为CoOp提供了两种实现方案：统一上下文和类别特定上下文。通过在11个数据集上的大量实验，我们证明CoOp仅需一到两次样本就能以明显优势超越人工设计的提示词，并且在更多样本（如16次）时能显著提升效果——平均增益约15%，最高可达45%以上。尽管采用的是学习型方法，但与使用人工设计提示词的零样本模型相比，CoOp仍展现出卓越的领域泛化能力。</p><figure><img src="../postimages/CoOp/image-20250721213523437.png"alt="image-20250721213523437" /><figcaption aria-hidden="true">image-20250721213523437</figcaption></figure><h1 id="方法">3方法</h1><h2 id="视觉-语言预训练">3.1视觉-语言预训练</h2><p>​  我们简要介绍了视觉语言预训练方法，重点聚焦于CLIP模型（Radford等人，2021）。我们的方法适用于更广泛的CLIP类视觉语言模型。</p><p>​  <strong>模型</strong><br/>​  CLIP包含两个编码器，分别用于图像和文本处理。图像编码器旨在将高维图像映射到低维嵌入空间，其架构可采用类似ResNet-50(Heet al., 2016)或ViT (Dosovitskiy et al.,2021)的卷积神经网络模型。而文本编码器则基于Transformer(Vaswani et al.,2017)构建，专门用于从自然语言中生成文本表示。<br/>​  具体来说，给定一个词序列（标记），例如“一张狗的照片”，CLIP首先将每个标记（包括标点符号）转换为小写字节对编码（BPE）表示（Sennrich等人，2016），本质上是一个唯一的数字ID。CLIP的词汇表大小为49,152。为实现小批量处理，每个文本序列均添加[SOS]和[EOS]标记，并限制在77个字符以内。随后将ID映射到512维词嵌入向量，再输入Transformer模型。最后对[EOS]标记位置的特征进行层归一化处理，并通过线性投影层进一步优化。</p><p>​  <strong>训练</strong><br/>​  CLIP经过专门训练，能够将分别针对图像和文本学习的两个嵌入空间进行对齐。具体而言，其学习目标被定义为对比损失函数：当处理图像-文本配对数据时，CLIP会最大化匹配对的余弦相似度，同时最小化所有未匹配对的余弦相似度。为了学习更具迁移能力的多样化视觉概念，CLIP团队构建了一个包含4亿个图像-文本配对的大型训练数据集。</p><p>​  <strong>Zero-Shot推理</strong><br/>​  CLIP经过预训练，能够预测图像是否与文本描述匹配，因此天然适用于零样本识别任务。该方法通过将图像特征与文本编码器生成的分类权重进行对比实现，其中文本编码器以指定目标类别的文本描述作为输入。严格来说，设f为图像编码器提取的图像x的特征向量，{wi}是文本编码器生成的K个权重向量（i=1）。其中K表示类别数量，每个权重向量wi源自类似“一张[CLASS]的照片”的提示语句。这里的类标记会被替换为具体的类别名称，例如“猫”、“狗”或“汽车”。然后，预测概率计算如下：<spanclass="math display">\[p(y=i|x)={\frac{\exp(\cos(w_{i},f)/\tau)}{\sum_{j=1}^{K}\exp(\cos(w_{j},f)/\tau)}},\]</span>​  其中<spanclass="math inline">\(\tau\)</span>是CLIP学习到的温度参数，<spanclass="math inline">\(cos(\cdot,\cdot)\)</span>表示余弦相似度。<br/>​  与传统分类器学习方法通过随机向量学习闭集视觉概念不同，视觉语言预训练允许通过高容量文本编码器探索开放集视觉概念，从而获得更宽泛的语义空间，并使学习到的表征更适用于下游任务。</p><h2 id="上下文优化">3.2上下文优化</h2><p>​  我们提出上下文优化（CoOp）技术，通过端到端学习从数据中提取连续向量来建模上下文词汇，同时冻结大量预训练参数，从而避免手动调整提示词。图2展示了该技术的概览。下文我们将介绍几种不同的实现方式。</p><p>​  <strong>统一上下文</strong><br/>​  我们首先介绍统一上下文方法，该方法与所有类别共享相同的上下文。具体而言，提供给文本编码器g（·）的提示采用以下形式设计：<spanclass="math display">\[t=[\mathrm{V}]_{1}[\mathrm{V}]_{2}\cdot\cdot\cdot[\mathrm{V}]_{M}[\mathrm{CLASS}],\]</span>​  其中每个<span class="math inline">\(\lbrack\mathbf{V}]_{m}\ \(m\in\{1,\ldots,M\})\)</span>是一个与词嵌入维度相同的向量（例如CLIP的512维），而M是超参数，用于指定上下文标记的数量。<br/>​  通过将提示词t传递给文本编码器g（·），我们可以获得一个表示视觉概念的分类权重向量（仍来自[EOS]标记位置）。预测概率的计算方式如下：<spanclass="math display">\[p(y=i|x\rangle=\frac{\exp({\mathrm{cos}}(g(t_{i}),f)/\tau)}{\sum_{j=1}^{K}\exp({\mathrm{cos}}(g(t_{j}),f)/\tau)},\]</span>​  其中，每个提示ti中的类别标记被替换为第i个类别名称对应的词嵌入向量(s)。<br/>​  除了像公式(2)那样将类标记放在序列的末尾，我们还可以将其放在中间位置。<spanclass="math display">\[t=[\mathrm{V}]_{1}\ldots[\mathrm{V}]_{\frac{M}{2}}[\mathrm{CLASS}][\mathrm{V}]_{\frac{M}{2}+1}\ldots[\mathrm{V}]_{M},\]</span>​  这增加了学习的灵活性——提示可以用来补充描述，也可以通过句号等终止信号提前切断句子。</p><p>​  <strong>特定类的上下文</strong><br/>​  另一种选择是设计类特定上下文（CSC），其中上下文向量与每个类别相互独立，即<spanclass="math inline">\([\mathbf{V}]_{1}^{i}[\mathbf{V}]_{2}^{i}\cdot\cdot\cdot[\mathbf{V}]_{M}^{i}\neq[\mathbf{V}]_{1}^{j}[\mathbf{V}]_{2}^{j}\cdot\cdot\cdot[\mathbf{V}]_{M}^{j}\)</span>其中i，j∈{1，...，K}。相较于统一上下文，我们发现CSC在某些细粒度分类任务中具有显著优势。</p><p>​  <strong>训练</strong><br/>​  训练过程旨在通过交叉熵最小化标准分类损失，同时利用参数中编码的丰富知识，使梯度能够反向传播至整个文本编码器g（·）以优化上下文。连续表示的设计还允许在词嵌入空间中进行充分探索，这有助于学习与任务相关的上下文信息。</p><h1 id="讨论">3.3讨论</h1><p>​  我们的方法专门针对近期提出的大型视觉语言模型如CLIP（Radford等人，2021)在适应性训练方面的新挑战。与自然语言处理领域开发的语言模型提示学习方法例如GPT-3（Brown等人，2020）相比，我们的方法存在两个显著差异：首先，CLIP类模型与语言模型的主干架构截然不同——前者同时接收视觉和文本数据作为输入，并生成用于图像识别的对齐分数；后者则专为处理纯文本数据而设计。其次，预训练目标也存在差异：对比学习与自回归学习。这种差异将导致模型行为的不同表现，从而需要采用不同的模块设计。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CLIP:Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/CLIP/"/>
      <url>/CLIP/</url>
      
        <content type="html"><![CDATA[<p>CLIP: Learning Transferable Visual Models From Natural LanguageSupervision</p><p>Alec Radford * 1, Jong Wook Kim * 1 , Chris Hallacy 1 , Aditya Ramesh1 , Gabriel Goh 1 , Sandhini Agarwal 1, Girish Sastry 1 , Amanda Askell1 , Pamela Mishkin 1 , Jack Clark 1 , Gretchen Krueger 1, Ilya Sutskever1</p><h1 id="摘要">摘要</h1><p>​  当前最先进的计算机视觉系统通常被训练用于预测一组预设的固定物体类别。这种受限的监督方式限制了系统的通用性和实用性，因为要定义其他视觉概念还需要额外的标注数据。而直接从图像的原始文本中学习则是一种有前景的替代方案，它利用了更广泛的数据来源进行监督。我们证明，通过预测图像与对应文字描述的简单预训练任务，能够从互联网收集的4亿张（图像+文本）数据集零基础学习到最先进的图像表征方法，且该方法具有高效性和可扩展性。完成预训练后，自然语言被用来参照已学习的视觉概念（或描述新概念），从而实现模型在下游任务中的零样本迁移。我们通过在30多个现有计算机视觉数据集上进行基准测试，研究了该方法的性能表现。这些数据集涵盖OCR、视频动作识别、地理定位以及多种细粒度物体分类等任务。该模型能够轻松迁移至大多数任务，并且无需任何特定数据集训练，其性能通常可与全监督基线模型相媲美。例如，我们在ImageNet零样本上匹配原始ResNet-50的准确性，而不需要使用它所训练的128万个训练示例中的任何一个。我们在https://github.com/OpenAI/CLIP上发布我们的代码和预训练模型权重。</p><figure><img src="../postimages/CLIP/CLIP.png" alt="CLIP" /><figcaption aria-hidden="true">CLIP</figcaption></figure><p>​  给定一批N（图像，文本）对，CLIP被训练来预测在N×N可能的（图像，文本）配对中实际发生的配对。为此，CLIP通过联合训练图像编码器和文本编码器来学习多模态嵌入空间，以最大化批次中N对真实图像和文本嵌入的余弦相似度，同时最小化N2−N个错误配对的嵌入的余弦相似度。我们对这些相似性分数进行了对称交叉熵损失的优化。图3展示了CLIP算法核心实现的伪代码。据我们所知，这种批量构建技术及其目标函数最初由Sohn（2016）在深度度量学习领域提出，作为多类别N对损失函数；随后被Oord等人（2018）推广应用于对比表示学习领域，命名为InfoNCE损失；最近张等人（2020）则将其改编为医学影像领域的对比（文本/图像）表示学习模型。</p><figure><img src="../postimages/CLIP/image-20250721212128336.png"alt="image-20250721212128336" /><figcaption aria-hidden="true">image-20250721212128336</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust Watermarking Using Generative Priors Against Image Editing:From Benchmarking to Advances</title>
      <link href="/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/"/>
      <url>/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/</url>
      
        <content type="html"><![CDATA[<p>Robust Watermarking Using Generative Priors Against Image Editing:From Benchmarking to Advances</p><p>Shilin Lu1, Zihan Zhou1, Jiayou Lu1, Yuanzhi Zhu2, Adams Wai-KinKong</p><p>1 南洋理工大学<br/>2 ETH Zurich</p><h1 id="摘要">摘要</h1><p>​  当前的图像水印技术容易受到基于大规模文本转图像模型的高级图像编辑技术的攻击。这些模型在编辑过程中会扭曲嵌入的水印，给版权保护带来重大挑战。在本工作中，我们介绍了W-Bench，这是第一个全面的基准，旨在评估水印方法对广泛的图像编辑技术的鲁棒性，包括图像再生、全局编辑、局部编辑和图像到视频生成。通过对11种代表性水印方法与主流编辑技术的广泛对比测试，我们发现多数方法在遭遇图像编辑后难以有效识别水印。针对这一技术瓶颈，我们创新性地提出VINE水印方案——该方案在保持图像质量优异的同时，显著提升了对各类图像编辑技术的抗干扰能力。我们的方法包含两项核心创新：(1)通过分析图像编辑的频率特征，发现模糊失真具有相似的频率特性，这使我们能够在训练过程中将其作为替代攻击手段来增强水印的鲁棒性；(2)采用大规模预训练扩散模型SDXL-Turbo，并针对水印任务进行适配，从而实现更隐蔽且稳定的水印嵌入。实验结果表明，本方法在多种图像编辑技术下均展现出卓越的水印性能，在图像质量与鲁棒性方面均优于现有方法。代码可在https://github.com/Shilin-LU/VINE上找到。</p><h1 id="引言">1引言</h1><p>​  图像水印的核心功能在于保护版权或验证真实性。其设计的关键在于确保对各类图像处理具有强大的抗干扰能力。早期基于深度学习的水印技术(Buiet al., 2023; Tancik et al., 2020; Zhu,2018)已证明能有效抵御压缩、加噪、缩放和裁剪等常规处理。然而，近年来大规模文本到图像（T2I）模型的突破性进展(Changet al., 2023;Ramesh et al., 2022; Rombach et al., 2022; Saharia et al.,2022)显著提升了图像编辑能力，提供了丰富的用户友好型操作工具(Brooks etal., 2023;Zhang et al.,2024b)。这些基于T2I的编辑方法能生成高度逼真的修改效果，使得水印在编辑后的版本中几乎难以察觉。这对版权和知识产权保护构成了挑战，因为恶意用户即使在作品中嵌入了水印，也能轻松篡改艺术家或摄影师的作品，创作出未注明出处的新内容。<br/>​  在本研究中，我们提出了W-Bench基准测试平台——这是首个整合四种图像编辑技术的综合评估体系，用于检验水印方法的鲁棒性（如图1(a)所示）。该平台共评估了十一种代表性水印技术，涵盖图像再生、全局编辑、局部编辑及图像转视频生成（I2V）四大类。</p><p>​  (1)图像再生技术通过将原始图像扰动为含噪版本后进行重建，可分为随机型(Menget al., 2021; Zhao et al., 2023b)和确定型（亦称图像反转）(Mokady et al.,2022; Song et al.,2020a)。<br/>​  (2)在全局编辑方面，我们采用Instruct-Pix2Pix（Brooks等人，2023）和MagicBrush（Zhang等人，2024b）等模型，这些模型以图像和文本提示作为输入来实现图像编辑。<br/>​  (3)对于局部编辑，我们使用ControlNet-Inpainting（Zhang等人，2023）和UltraEdit（Zhao等人，2024c）等模型，它们允许通过额外的遮罩输入指定需要修改的区域。<br/>​  (4)此外，我们还利用StableVideoDiffusion（SVD）（Blattmann等人，2023）在图像到视频生成的背景下评估水印模型，以确定生成的视频帧中水印是否仍可被检测到。尽管这不是传统图像编辑方法，但我们将其视为一种特殊情况，以便识别生成的视频是否使用了受版权保护的图像。</p><p>​实验结果（图1(b))表明，大多数先前的水印模型在使用这些方法编辑图像后难以提取水印。StegaStamp（Tancik等人，2020）和MBRS（Jia等人，2021）在某些情况下能够保留水印，但代价是牺牲了图像质量。</p><figure><imgsrc="../postimages/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/image-20250718162455780.png"alt="image-20250718162455780" /><figcaption aria-hidden="true">image-20250718162455780</figcaption></figure><p>​  每种方法都用一个方块和四个长条进行可视化呈现。方块的面积代表该方法的编码容量，其中心y轴坐标表示归一化图像质量，通过计算水印图像与原始图像在归一化PSNR、SSIM、LPIPS和FID指标上的平均值得出。x轴坐标反映鲁棒性，以0.1%假阳性率（TPR@0.1%FPR）下的真阳性率作为衡量标准，综合四种图像编辑方法的测试结果，涵盖七种不同模型和算法。四根长条的排列方向对应不同编辑任务：左为图像再生，上为全局编辑，右为局部编辑，下为图像转视频生成。柱体长度反映了各编辑类型后的归一化TPR@0.1%FPR值——柱体越长，说明性能越优。</p><p>​  为此，我们提出VINE，一种专为抵御图像编辑而设计的隐形水印模型。我们的改进重点在于两个关键组件：噪声层和水印编码器。对于噪声层而言，直接将编辑过程纳入训练流程是提升水印模型抗编辑能力的常规方法。然而，这种做法在大规模基于T2I模型的图像编辑场景中几乎不可行，因为需要对整个采样过程进行反向传播，可能导致内存问题（Salman等人，2023）。为此，我们通过频率分析视角研究图像编辑行为，采用替代攻击策略。核心发现在于：图像编辑倾向于消除高频带中的特征模式，而低频带特征则受影响较小。这一特性同样体现在模糊失真（如像素化和散焦模糊）中。实验表明，在噪声层中融入多种模糊失真可显著增强水印对图像编辑的抗干扰能力。<br/>​  然而，这种鲁棒性是以牺牲水印图像质量为代价的，其质量受限于水印编码器的能力。为解决这一问题，我们采用大规模预训练生成模型（如SDXL-Turbo，Sauer等人，2023年）作为强大的生成先验，并针对水印任务进行专门适配。在此框架下，水印编码器作为条件生成模型运作，以原始图像和水印作为输入，生成具有独特分布特征的水印图像，这些图像能被对应的解码器可靠识别。通过利用这种强大的生成先验，水印得以更有效地嵌入，从而在提升感知图像质量的同时，也增强了系统的鲁棒性。<br/>​  我们的贡献总结如下：</p><ul><li>我们提出了W-Bench，这是第一个全面的基准，旨在评估11个代表性的水印模型在各种图像编辑方法中的表现：图像再生、全局编辑、局部编辑和图转视频生成，该评估涵盖了7种广泛使用的编辑模型和算法，并证明了当前的水印模型很容易受到它们的影响。</li><li>我们发现图像编辑主要针对高频带的水印模式进行消除，而低频带的水印则受影响较小。这种现象在某些类型的模糊失真中同样存在。这类失真可作为替代攻击手段，既能有效应对训练过程中使用T2I模型带来的挑战，又能增强水印的抗干扰能力。</li><li>我们将水印编码器视为条件生成模型，并引入两项技术将SDXL-Turbo（一种预训练的一步式文本到图像模型）适配于水印任务。这种强大的生成先验不仅提升了水印图像的感知质量，还增强了其对各类图像编辑的鲁棒性。实验结果表明，我们的模型VINE在保持高画质的同时，对多种图像编辑方法表现出色，性能超越现有水印模型。</li></ul><h1 id="相关工作">2相关工作</h1><p>​  <strong>水印基准</strong><br/>​  据我们所知，WAVES(An et al.,2024)目前是评估基于深度学习的水印方法在大规模生成模型驱动图像篡改场景下鲁棒性的唯一综合性基准。然而，该基准仅涵盖主流图像编辑技术中的图像再生(Zhaoet al.,2023b)，未包含其他基于T2I的编辑模型。相比之下，W-Bench不仅包含图像再生，还囊括全局编辑(Brookset al., 2023)、局部编辑(Zhang et al.,2023)以及图像转视频生成 (Blattmannet al.,2023)，从而拓宽了对图像编辑方法的评估范围。此外，WAVES仅评估三种水印方法——StegaStamp(Tanciket al., 2020)、Stable Signature(Fernandez et al., 2023)和TreeRing(Wen etal., 2023)。值得注意的是，StableSignature和TreeRing仅适用于生成图像，无法应用于真实图像。而W-Bench的设计初衷是评估能适配各类图像的水印模型，从而提升其版权保护效果。</p><p>​  <strong>鲁棒水印</strong><br/>​  图像水印技术长期被用于知识产权追踪与保护等领域（Al-Haj，2007；Cox等，2007；Navas等，2008）。近年来，基于深度学习的方法（Bui等，2023；Chen与Li，2024；Fang等，2022；2023；Jia等，2021；Kishore等，2021；Luo等，2020；2024；Ma等，2022；Tancik等，2020；Wu等，2023；Zhu，2018；Zhang等，2019；2021）展现出对各类图像变换的强健防御能力。然而，这些方法仍难以抵御基于大规模生成模型的图像编辑攻击。近期三项突破性研究——EditGuard（Zhang等，2024d)、RobustWide（Hu等，2024）和JigMark（Pan等，2024）——已开始研发能有效抵御此类图像编辑的水印模型。</p><h1 id="方法">3方法</h1><p>​  给定原始图像xo和水印w，我们的目标是通过编码器E（·）将水印以不可察觉的方式嵌入图像中，从而获得带水印的图像$ x_w = E(x_o, w) $ 。即使图像经过编辑<spanclass="math inline">\(\epsilon(\cdot)\)</span>，对应的解码器D（·）也应能准确提取水印，即<spanclass="math inline">\(w^{\prime}=D(x_w)\)</span>。</p><p>​  在第3.1节中，我们研究了各种图像编辑方法的频率特性，并确定了增强水印对它们鲁棒性的替代攻击。在第3.2节中，我们通过采用一步式文本转图像模型作为水印编码器，进一步提升了水印图像的鲁棒性和质量。此外，我们还引入了多种技术手段来促进这种适配过程。第3.3节详细阐述了实验中采用的训练损失函数、策略以及分辨率缩放方法。</p><h2 id="图像编辑的频率特性">3.1图像编辑的频率特性</h2><p>​  要开发一种能有效抵御图像编辑的鲁棒水印模型，最直接的方法是在训练过程中将图像编辑模型整合到编码器与解码器之间的噪声层。然而，当前主流的图像编辑方法大多基于扩散模型，这类模型通常需要经过多次采样步骤才能生成编辑后的图像。这可能导致在反向传播去噪过程时出现内存问题。其他替代方案如梯度截断（Hu等人，2024；Yuan等人，2024）效果欠佳，而直流估计器（Bengio等人，2013）在从头训练时甚至无法收敛。因此，我们尝试在训练过程中引入替代攻击机制。<br/>​  我们首先研究图像编辑方法对图像频谱的影响机制。具体而言，我们设计了三组实验方案，分别在低频、中频和高频频段嵌入对称图案。</p><figure><imgsrc="../postimages/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/image-20250718164210873.png"alt="image-20250718164210873" /><figcaption aria-hidden="true">image-20250718164210873</figcaption></figure><p>​  图2展示了将图案嵌入低频段的分析流程：首先在原始图像<spanclass="math inline">\(x_o\)</span>的RGB通道傅里叶频谱低频区域嵌入一个恒定值环形图案w（即<spanclass="math inline">\({\mathcal{F}}(x_{o})+w\)</span>），随后通过逆傅里叶变换得到水印后的图像<spanclass="math inline">\(x_w\)</span>。接着应用图像编辑模型<spanclass="math inline">\(\epsilon(\cdot)\)</span>对原始图像<spanclass="math inline">\(x_o\)</span>和水印图像<spanclass="math inline">\(x_w\)</span>进行处理，分别生成编辑后的图像<spanclass="math inline">\(\epsilon(x_o)\)</span>和<spanclass="math inline">\(\epsilon(x_w)\)</span>。最后通过计算两者的傅里叶频谱差异<spanclass="math inline">\(|{\mathcal{F}}(\epsilon(x_{w}))-{\mathcal{F}}(\epsilon(x_{o}))|\)</span>，评估图案在编辑过程中的变化情况。具体使用的图像编辑方法详见第4.1节。</p><figure><imgsrc="../postimages/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/image-20250718164657337.png"alt="image-20250718164657337" /><figcaption aria-hidden="true">image-20250718164657337</figcaption></figure><p>​  实验结果基于1000张图像的平均值。图像编辑方法通常会消除中高频段的频率特征，而低频特征基本不受影响。这种现象在像素化、失焦模糊等模糊失真中同样存在。相比之下，JPEG压缩和饱和度等常见失真方式在频域中不会呈现类似特征。由于SVD会消除所有特征模式，使其对人眼不可见，因此未纳入分析范围。关于SVD的详细讨论可参见第4.3节。</p><p>​  图3显示，图像编辑方法通常会移除中高频段的特征模式，而低频特征则相对不受影响。这表明基于T2I的图像编辑方法往往难以复现复杂的中高频细节特征。我们推测，这是因为T2I模型在训练时优先捕捉图像的整体语义内容和结构（即主要关注低频成分），以匹配文本提示。因此，在生成过程中高频特征会被过度平滑处理。<br/>​  要开发出能有效抵御图像编辑的水印模型，其核心在于学会将信息嵌入低频带。为识别有效的替代性攻击手段，我们研究了多种图像失真方法（记作<spanclass="math inline">\(\calT(\cdot)\)</span>），这些方法在某种程度上模拟了图像编辑行为。虽然图像编辑和失真技术都能保持整体布局和大部分内容，但失真处理通常会导致图像感知质量下降。值得注意的是，如图3所示，某些模糊失真（例如像素化和散焦模糊）呈现出与图像编辑相似的特征。相比之下，广泛使用的失真效果（如JPEG压缩和饱和度调整）则不会出现这种现象。由于这些模糊失真具有较高的计算效率，我们在训练过程中将其以不同严重程度融入噪声层。这鼓励模型在低频带中嵌入信息（参见附录B中每种水印方法的频率模式）。因此，如表2的消融研究所示，增强了对图像编辑的鲁棒性。噪声层中应用的完整失真效果包含多种常见失真类型，例如饱和度调整、对比度调节、亮度调整、JPEG压缩、高斯噪声、散粒噪声、脉冲噪声和斑点噪声，这些都能有效抵消透射导致的图像劣化。此外，我们还融入了像素化模糊、虚焦模糊、缩放模糊、高斯模糊和运动模糊等多种模糊效果，以增强图像抗编辑能力。</p><h2 id="水印编码的生成式先验">3.2水印编码的生成式先验</h2><p>​  虽然在噪声层中引入图像失真可以增强对图像编辑的鲁棒性，但这种改进是以牺牲水印图像质量为代价的，而图像质量受限于水印编码器的能力。水印编码器可视为一个条件生成模型，其条件包含水印和详细图像，而非深度图、Canny边缘或涂鸦等简单表征。我们推测，强大的生成先验机制既能更隐蔽地嵌入信息，又能提升鲁棒性。因此，我们计划将大规模T2I模型适配为水印编码器。目前存在两种大规模T2I模型：多步骤型和单步骤型。多步骤T2I模型会增加水印提取损失的反向传播复杂度，导致推理速度较慢。为此，我们采用预训练的单步骤文本到图像模型SDXL-Turbo（Sauer等人，2023）。<br/>​  要将SDXL-Turbo转化为水印编码器，关键在于找到一种能同时整合输入图像与水印信息的有效策略。扩散模型中常见的条件集成方法是引入额外的适配分支（Mou等人，2024；Zhang等人，2023）。但在单步生成模型中，噪声图——即UNet的输入——直接决定了生成图像的最终布局（Sauer等人，2023）。这与多步扩散模型形成鲜明对比，后者在早期采样阶段会逐步构建图像布局。若在单步模型中添加额外条件分支，UNet将接收两组残差特征，每组分别对应不同的结构特征。这使得训练过程更具挑战性，导致性能表现欠佳，如表2的消融实验所示。为此，我们采用条件适配器融合输入图像与水印信息（条件适配器架构详见图11)，如图4所示。将融合后的数据输入VAE编码器提取潜在特征后，再通过UNet和VAE解码器生成最终水印图像。我们还尝试通过文本提示输入水印并同步微调文本编码器，但该方案未能收敛。因此在训练过程中，我们直接将文本提示设置为空提示。</p><figure><imgsrc="../postimages/Robust-Watermarking-Using-Generative-Priors-Against-Image-Editing-From-Benchmarking-to-Advances/image-20250718165536854.png"alt="image-20250718165536854" /><figcaption aria-hidden="true">image-20250718165536854</figcaption></figure><p>​  我们采用预训练的一步式文本到图像模型SDXL-Turbo作为水印编码器，并在将信息传递给VAE编码器前，通过条件适配器将水印与图像融合。为增强感知相似性，我们添加了零卷积层（Zhang等人，2023）和跳跃连接。在解码水印时，采用ConvNeXt-B（Liu等人，2022b）作为解码器，并额外增加全连接层输出100比特水印。整个训练过程中，SDXL-Turbo的文本提示始终设置为空提示。图11展示了条件适配器的架构设计。</p><p>​  尽管SDXL-Turbo的变分自编码器（VAE）整体效果良好，但其架构并不完全适合水印任务。该模型旨在平衡重建能力和压缩性能，因此在重建保真度与更平滑的潜在空间及更好的可压缩性之间进行了权衡。然而在水印的背景下，重建能力对于确保带有水印的图像在感知上与输入图像相同是至关重要的。为此，我们通过在编码器与解码器之间引入跳跃连接（图4）来增强变分自编码器（VAE）。具体而言，我们在编码器的每个下采样块后提取四个中间激活值，将其通过零卷积层(Zhang等人，2023年）处理后，输入到解码器对应的上采样块中。如表2所示，这一改进显著提升了水印图像与输入图像之间的感知相似度。为解码水印，我们采用卷积神经网络解码器ConvNeXt-B（Liu等人，2022b），并在其后添加全连接层以输出100比特水印信息。</p><h2 id="目标函数和训练策略">3.3目标函数和训练策略</h2><p>​  <strong>目标函数</strong><br/>​  我们采用标准训练方案，在不同图像处理条件下平衡水印图像质量与水印提取效果，总损失函数如下：<span class="math display">\[{\mathcal L}_{\mathrm{AL}}={\mathcalL}_{\mathrm{IMG}}\left(x_{o},x_{w}\right)+\alpha{\mathcalL}_{\mathrm{BCE}}\left(w,w^{\prime}\right),\]</span>​  其中α是权衡超参数，<span class="math inline">\({\mathcalL}_{\mathrm{BCE}}\)</span>是基于提取的水印与真实值计算的标准二元交叉熵损失函数。图像质量损失<spanclass="math inline">\({\mathcal{L}}_{\mathrm{IHG}}\)</span>定义为：<spanclass="math display">\[{\mathcal{L}}_{\mathrm{IHG}}=\beta_{\mathrm{MSE}}{\mathcal{L}}_{\mathrm{MSE}}\left(\gamma(x_{o}),\gamma(x_{w})\right)+\beta_{\mathrm{LPPS}}{\mathcal{L}}_{\mathrm{LPIPS}}\left(x_{o},x_{w}\right)+\beta_{\mathrm{GAN}}{\mathcal{L}}_{\mathrm{GAN}}\left(x_{o},x_{w}\right),\]</span>​  其中<span class="math inline">\(\beta_{\mathrm{MSE}}\)</span>、<spanclass="math inline">\(\beta_{\mathrm{LPIPS}}\)</span>和<spanclass="math inline">\(\beta_{\mathrm{GAN}}\)</span>分别代表各损失项的权重。这里，<spanclass="math inline">\(\gamma(\cdot)\)</span>是一个可微的非参数映射函数，用于将输入图像从RGB色彩空间转换为感知上更均匀的YUV色彩空间；<spanclass="math inline">\({\mathcal{L}}_{\mathrm{LPIPS}}(x_o,x_w)\)</span>是感知损失项，而<spanclass="math inline">\({\mathcal{L}}_{\mathrm{GAN}}(x_o,x_w)\)</span>则是来自GAN判别器<spanclass="math inline">\({\mathcalD}_{\mathrm{disc}}\)</span>的标准对抗损失项： <spanclass="math display">\[{\mathcal{L}}_{\mathrm{GAN}}={\mathbb{E}}_{x_{o}}\left[\logD_{\mathrm{disc}}(x_{o})\right]+{\mathbb{E}}_{x_{o},w}\left[\log\left(1-D_{\mathrm{disc}}(E(x_{o},w))\right)\right].\]</span>​  <strong>训练策略</strong><br/>​  在第一阶段训练中，我们优先采用水印提取损失函数，将α设为10，同时将<spanclass="math inline">\(\beta_{\mathrm{MSE}}\)</span>、<spanclass="math inline">\(\beta_{\mathrm{LPIPS}}\)</span>和<spanclass="math inline">\(\beta_{\mathrm{GAN}}\)</span>各设为0.01。为保持生成先验特性，SDXL-Turbo的UNet和VAE解码器以及新增的零卷积层均被冻结。当比特准确率超过0.85时，我们将进入第二阶段训练，解冻所有参数进行后续优化。此时调整损失权重因子为α= 1.5、βMSE = 2.0、βLPIPS = 1.5、βGAN =0.5。经过前两个阶段训练后的模型作为基础模型，命名为VINE-B。在第三阶段，我们通过将代表性指令驱动图像编辑模型Instruct-Pix2Pix（Brooks等人，2023）整合到噪声层中，对VINE-B进行微调。梯度通过直流估计器（Bengio等人，2013）进行反向传播。需要注意的是，如果在早期训练阶段直接应用该方法，将无法收敛。经过微调的模型被称为VINE-R。更多实现细节请参见附录F。</p><p>​  <strong>分辨率缩放</strong><br/>​  不同的水印模型通常采用固定输入分辨率进行训练，这限制了它们在测试时只能接受固定分辨率的输入。然而在实际应用中，保持原始分辨率的水印处理对维护图像质量至关重要。Bui等人（2023）提出了一种方法（详见附录D.1)，可使任何水印模型适应任意分辨率而不影响水印图像的质量及其固有鲁棒性，如附录D.2和D.3所示。在我们的实验中，我们将这种分辨率缩放方法应用于所有模型，使其能够在统一的512×512分辨率下运行，这与图像编辑模型的兼容性完全一致。</p><h1 id="实验">4实验</h1><p>​  在W-Bench测试中，我们评估了十一种代表性水印模型对多种图像编辑方法的鲁棒性，包括图像再生、全局编辑、局部编辑以及图像转视频生成。第4.1节和第4.2节分别概述了所采用的图像编辑方法及基准测试设置。第4.3节分析了基准测试结果。第4.4节通过消融实验研究，深入探究了关键组件的影响机制。</p><h2 id="图像编辑方法">4.1图像编辑方法</h2><h2 id="实验设置">4.2实验设置</h2><h2 id="基准测试结果与分析">4.3基准测试结果与分析</h2><h2 id="消融研究">4.4消融研究</h2><h1 id="结论">5结论</h1><p>​  在本研究中，我们推出了W-Bench，首个整合四大图像编辑功能的综合基准测试平台，通过大规模生成模型评估水印模型的鲁棒性。我们从海量样本中筛选出11种代表性水印方法进行测试，并揭示了图像编辑对傅里叶频谱的普遍影响机制，同时开发出高效的模拟替代方案。自主研发的VINE模型在对抗各类图像编辑技术时展现出卓越性能，在图像质量与鲁棒性方面均超越现有方法。研究结果表明：预训练模型可作为水印技术的通用框架，而强大的生成式先验机制能以更隐蔽且稳健的方式增强信息嵌入效果。</p><p>​  <strong>限制</strong><br/>​  虽然我们的方法在基于生成模型的常见图像编辑任务中表现出色，但在I2V生成方面的效果仍有限。此外，我们的模型比基线模型更大，导致内存需求增加，推理速度略有下降，具体如表7所示。</p>]]></content>
      
      
      <categories>
          
          <category> 无监督语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ForensicHub:A Unified Benchmark &amp; Codebase for All-Domain Fake Image Detection and Localization</title>
      <link href="/ForensicHub/"/>
      <url>/ForensicHub/</url>
      
        <content type="html"><![CDATA[<p>ForensicHub: A Unified Benchmark &amp; Codebase for All-Domain FakeImage Detection and Localization</p><p>Bo Du1†, Xuekang Zhu1,2†, Xiaochen Ma3,4†, Chenfan Qu5, 2†, KaiwenFeng1†,Zhe Yang1, Chi-Man Pun6, Jian Liu2‡, Jizhe Zhou1‡</p><p>1四川大学<br/>2蚂蚁集团<br/>3MBZUAI<br/>4北京大学<br/>5华南理工大学<br/>6澳门大学</p><h1 id="摘要">摘要</h1><p>​  伪造图像检测与定位（FIDL）领域目前仍处于高度碎片化的状态，主要涵盖四大方向：深度伪造检测（Deepfake）、图像篡改检测与定位（IMDL）、人工智能生成图像检测（AIGC）以及文档图像篡改定位（Doc）。尽管部分领域已形成独立的基准测试，但针对FIDL所有领域的统一标准至今仍未建立。由于缺乏统一基准测试标准，各领域形成了明显的数据孤岛现象——不同机构各自为政地构建数据集、模型和评估方案，缺乏跨平台兼容性。这种孤立状态不仅阻碍了跨领域对比研究，更严重制约了FIDL技术的全面发展。为打破这种数据壁垒，我们推出了ForensicHub，这是首个面向全领域伪造图像检测与定位的统一基准测试平台及代码库。考虑到所有领域中数据集、模型和评估配置的剧烈变化，以及开源基准模型的稀缺性和某些领域的缺乏单独的基准，ForensicHub：i)提出了一种模块化和配置驱动的架构，将取证管道分解为跨数据集、转换、模型和评估器的可互换组件，允许在所有领域灵活组合；ii)通过基于适配器的设计，全面实现10个基线模型（其中3个为从头复制），6个主干网络，2个AIGC和文档新基准，并整合了DeepfakeBench和IMDLBenCo两个现有基准；iii)建立图像取证融合协议评估机制，支持不同任务中各类取证模型的统一训练与测试；iv)基于取证中心开展深度分析，针对FIDL模型架构、数据集特征及评估标准提供8项关键可操作性建议。具体来说，ForensicHub包含4个取证任务、23个数据集、42个基准模型、6个骨干网络、11项GPU加速的像素级与图像级评估指标，并实现了16种跨领域评估。该平台在打破FIDL领域壁垒方面实现了重大突破，为未来创新提供了重要启示。相关代码可在以下地址获取：https://github.com/scu-zjz/ForensicHub。</p><h1 id="概述">1 概述</h1><p>"<em>The whole is more than the sum of its parts</em>" -<strong>Aristotle</strong></p><p>​  近年来，随着各种数字图像编辑技术的快速发展，假图像变得越来越普遍。篡改图片的检测和定位（FIDL，FakeImage Detection andLocalization）旨在区分部分篡改和完全生成的图像与真实图像。在FIDL中，术语“检测”指的是图像级别的分类，而“定位”则针对像素级别上更精细的篡改像素分割。<br/>​  然而，随着时间的推移，FIDL的研究工作逐渐分裂为四个相对独立的研究领域。</p><p>​  1）深度伪造检测（Deepfake）[37,26,25,8,38,4,66,42,28,33,54,71,49,65,67]：检测诸如面部交换、表情编辑或特征替换等操作。<br/>​  2）图像篡改检测/定位（IMDL）[5,17,29,35,70,53]：检测和定位自然图像中的篡改。<br/>​  3）AI生成图像检测（AIGC）[40,18,63,59]：检测由StableDiffusion[46]等深度生成模型完全生成的图像。<br/>​  4）文档图像处理定位（文档）[48,43,6,12,52]：定位各种形式的文档图像伪造，包括收据、证书和识别材料，特别关注检测打印文本的修改。</p><p>​  尽管这四个领域因应用场景、操作类型和检测方法的差异而逐渐分化，但它们之间仍存在诸多共性与相似之处。作为视觉任务，这些领域普遍采用最先进检测或分割模型作为预训练骨干网络。此外，由于伪造图像制作者通常致力于保留语义合理且逼真的内容，所有领域都高度重视设计低级视觉特征提取器，以捕捉细微的非语义差异实现可靠检测。诸如对比学习等研究方法在这些领域中被广泛采用，用于挖掘判别性特征。我们在表1中总结了各领域中最先进的技术：骨干网络架构、伪影策略、输出类型及贡献度。虽然差异导致四个FIDL领域呈现碎片化特征，但共性要求我们通过统一视角来系统理解其内在联系。</p><p>表1：四个法医学领域的代表性方法总结，详细说明模型设计、主干、人工制品策略、输出格式和核心贡献。</p><table><colgroup><col style="width: 13%" /><col style="width: 8%" /><col style="width: 13%" /><col style="width: 27%" /><col style="width: 18%" /><col style="width: 19%" /></colgroup><thead><tr class="header"><th>Task</th><th>Model</th><th>Backbone</th><th>Artifact Strategy</th><th>Output Type</th><th>Contribution</th></tr></thead><tbody><tr class="odd"><td>Deepfake</td><td>Capsule-Net [37] (ICASSP19)</td><td>VGG [50]</td><td>动态路由</td><td>标签</td><td>提出了一种具有动态路由和VGG19主干的胶囊网络</td></tr><tr class="even"><td></td><td>RECCE [4] (<em>CVPR22</em>)</td><td>Xception [7]</td><td>重构</td><td>标签</td><td>提出了一种基于图的框架，利用重建差异</td></tr><tr class="odd"><td></td><td>SPSL [28] (<em>CVPR21</em>)</td><td>Xception [7]</td><td>相位谱</td><td>标签</td><td>提出了一种利用Xception进行相位谱融合的面部伪造检测方法</td></tr><tr class="even"><td></td><td>UCF [66] (<em>ICCV23</em>)</td><td>Xception [7]</td><td>多任务解耦</td><td>标签</td><td>提出使用Xception进行深度伪造泛化的多任务解耦</td></tr><tr class="odd"><td></td><td>SBI [49] (<em>CVPR22</em>)</td><td>EfficientNet [55]</td><td>频率，混合边界</td><td>标签</td><td>提出自混合图像以提高深度造假检测的泛化能力</td></tr><tr class="even"><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>IMDL</td><td>MVSS-Net [62] (<em>ICCV21</em>)</td><td>Resnet [19]</td><td>BayarConv,Sobel</td><td>标签、掩膜</td><td>利用多视图学习来检测篡改，以利用噪声和边界伪影</td></tr><tr class="even"><td></td><td>CAT-Net [22] (<em>IJCV22</em>)</td><td>HRNet [56]</td><td>DCT</td><td>掩膜</td><td>融合RGB和DCT流，以学习用于剪接定位的压缩伪影</td></tr><tr class="odd"><td></td><td>PSCC-Net [29] (<em>TCSVT22</em>)</td><td>HRNet [56]</td><td>多分辨率卷积</td><td>标签、掩膜</td><td>通过空间通道相关性逐步细化掩模，以实现高分辨率定位</td></tr><tr class="even"><td></td><td>Trufor [17] (<em>CVPR23</em>)</td><td>Seformer [64]</td><td>高分辨率、多尺度、边缘</td><td>标签、掩膜</td><td>融合RGB和学习到的噪声指纹，将篡改检测为异常</td></tr><tr class="odd"><td></td><td>IML-ViT [35] (<em>Arxiv</em>)</td><td>ViT [13]</td><td>BayarConv,SRM 滤波器</td><td>掩膜</td><td>使用高分辨率、多尺度边缘感知设计的ViT进行篡改定位</td></tr><tr class="even"><td></td><td>Mesorch [70] (<em>AAAI25</em>)</td><td>Conv. [31],Segfor. [64]</td><td>DCT</td><td>掩膜</td><td>融合微观和宏观线索，用于介观图像操作定位</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td>AIGC</td><td>Dire [59] (<em>ICCV23</em>)</td><td>Resnet [19]</td><td>扩散重建</td><td>标签</td><td>利用扩散的重建误差来检测扩散生成的图像</td></tr><tr class="odd"><td></td><td>DualNet [63] (<em>APSIPA23</em>)</td><td>CNN</td><td>SRM，低频</td><td>标签</td><td>融合SRM残留和低频内容流用于AIGC检测</td></tr><tr class="even"><td></td><td>HiFiNet [18] (<em>CVPR23</em>)</td><td>HRNet [56]</td><td>多分支特征提取器</td><td>标签、掩膜</td><td>学习伪造属性的层次精细表征</td></tr><tr class="odd"><td></td><td>Synthbuster [2] (<em>OJSP23</em>)</td><td>None</td><td>傅里叶变换</td><td>标签</td><td>利用频域中的频谱伪影进行扩散检测</td></tr><tr class="even"><td></td><td>UnivFD [40] (<em>CVPR23</em>)</td><td>CLIP-ViT [45]</td><td>None</td><td>标签</td><td>使用预训练的视觉语言模型特征进行统一检测</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td>文档</td><td>CAFTB [52] (<em>TOMM24</em>)</td><td>Resnet [19]</td><td>SRM</td><td>掩膜</td><td>提出具有双分支和交叉注意力的CAFTB-Net</td></tr><tr class="odd"><td></td><td>TIFDM [12] (<em>TCE24</em>)</td><td>Resnet [19]</td><td>None</td><td>掩膜</td><td>提出一个多尺度注意力的鲁棒网络</td></tr><tr class="even"><td></td><td>DTD [43] (<em>CVPR23</em>)</td><td>Conv. [31], Swin.[30]</td><td>频率</td><td>掩膜</td><td>提出一种带有频率头和多视图解码器的DTD</td></tr><tr class="odd"><td></td><td>FFDN [6] (<em>ECCV24</em>)</td><td>ConvNext [31]</td><td>波形，频率</td><td>掩膜</td><td>提出了一种结合视觉增强和频率分解的FFDN方法</td></tr></tbody></table><p>​  尽管在深度伪造领域存在DeepfakeBench [67]、IMDL领域有IMDLBenCo[36]等独立基准测试，但FIDL框架下所有领域的统一基准测试仍是一片空白。这种缺乏统一标准的局面导致各领域形成明显的数据孤岛——每个领域都在独立构建自己的数据集、模型和评估协议，彼此之间缺乏互操作性。这种孤立状态不仅造成现有FIDL领域研究的重复与不均衡，更使得建立通用统一的FIDL方法论变得困难重重，严重阻碍了整个FIDL领域的创新发展。<br/>​  因此，为所有领域建立统一基准至关重要。但该基准面临两大挑战：首先，由于各领域数据集、模型和评估配置存在巨大差异，基准设计必须具备足够的扩展性和灵活性以覆盖所有领域；其次，既要确保与现有基准的兼容性以减少重复研究，又要解决开源基准模型稀缺以及某些领域缺乏独立基准的问题。<br/>​  为此，我们提出了ForensicHub：<br/>​  1)提出了一种模块化和配置驱动的架构，将取证管道分解为跨数据集、转换、模型和评估器的可互换组件，允许在所有领域灵活组合；<br/>​  2）通过适配器设计，完整实现10个基线模型（其中3个为从头复制），6个骨干网络，2个AIGC和文档新基准，并整合了DeepfakeBench和IMDLBenCo两个现有基准。<br/>​  通过以上工作，ForensicHub成为全领域假图像检测与定位的首个统一基准和代码库。基于ForensicHub平台，我们开发了图像取证融合协议（IFF协议）评估机制，支持跨任务的多样化取证模型统一训练与测试。针对研究者关注但尚未深入探讨的八大核心问题展开深度分析，为FIDL模型架构、数据集特征及评估标准提供全新视角。ForensicHub的推出实现了FIDL各领域的无缝衔接，打破领域壁垒，为未来技术突破注入新动能。</p><h1 id="相关工作">2 相关工作</h1><p>​  假图像检测与定位包含四个子任务，详见附录C。尽管取得了快速进展，但目前仍缺乏统一基准——每个任务都使用独立的流程，限制了跨任务比较。<br/>​  DeepfakeBench[67]是专为解决数据处理流程不统一问题而设计的深度伪造检测基准测试，该问题会导致检测模型的数据输入存在差异。IMDLBenCo[36]作为IMDL技术的基准测试平台和代码库，致力于通过统一的训练与评估协议对IMDL模型进行横向对比。AIGCDetectBenchmark[68]则是一个专门用于测试9种AI生成图像检测方法的实验资源库。<br/>​  这些基准测试在各自任务中提供了模型、数据集和评估指标，但其底层设计缺乏跨任务考量，导致难以在不同检测场景中实现整合。例如，DeepfakeBench与特定于Deepfake的数据预处理步骤（如面部特征点）存在紧密耦合，而IMDLBenCo要求数据集和模型都输出像素级掩码。AIGCDetectBenchmark在多GPU指标计算方面表现欠佳。此外，它们均未包含完整的图像级和像素级指标体系。这些局限性迫切需要一个全新、统一且灵活的跨任务基准测试。</p><h1 id="forensichub">3 ForensicHub</h1><p>​  在本节中，我们提出了我们的ForensicHub，它是一个统一的基准，用于所有域的假图像检测和定位，如图1所示，设计灵活和可扩展。</p><figure><img src="../postimages/ForensicHub/image-20250717231040099.png"alt="image-20250717231040099" /><figcaption aria-hidden="true">image-20250717231040099</figcaption></figure><p>​  <strong>模块结构</strong><br/>​  为满足不同法证任务需求，ForensicHub采用模块化架构设计，包含四大核心组件：数据集、转换器、模型和评估器。数据集负责数据加载流程，必须返回符合ForensicHub规范的字段。转换器承担数据预处理与增强功能，适配各类任务需求。模型通过与数据集精准匹配并统一输出格式，可集成多种前沿图像取证模型。评估器涵盖了不同任务中常用的图像和像素级指标，并通过GPU加速来提高训练和测试期间的评估效率。</p><p>​  <strong>可配置工作流</strong><br/>​  ForensicHub采用无代码开发模式，用户仅需配置YAML文件即可直接构建训练或测试流程。基于模块化架构设计，用户可灵活选择不同评估器，在任意数据集上对模型进行训练和测试。该平台还提供代码自动生成工具，支持用户通过简单编程实现与基准测试的无缝对接。</p><p>​  <strong>ForensicHub的构建</strong><br/>​  为实现跨平台互操作性并减少重复劳动，ForensicHub采用基于适配器的设计方案[14]，确保与深度伪造基准测试平台DeepfakeBench[67]和IMDL基准库[36]两大主流工具无缝对接。该机制让用户无需大幅修改即可复用现有模型和数据集，同时支持在ForensicHub统一协议框架下定义新模型和基准测试。这种标准化架构简化了跨任务基准测试流程，保障结果可复现性，并实现不同领域评估的一致性。|​  具体而言，ForensicHub支持IMDLBenCo提供的全部10个模型进行多领域和跨领域评估。在DeepfakeBench中，34个图像级检测器中有27个兼容，包括5个通用主干网络和9个不适用于跨任务评估的领域专用模型。剩余13个模型支持跨不同法医领域的训练或推理，因此DeepfakeBench共包含22个模型。ForensicHub完整实现了AIGC的5个基准模型和文档的5个基准模型，详见第4节。此外，ForensicHub还包含6个常用主干网络。总体而言，ForensicHub覆盖4项任务、23个数据集、42个模型、6个主干网络，并实现了11种常用的图像级和像素级评估指标。</p><p>​  <strong>数据集</strong><br/>​  本文使用的数据集包括：<br/>​    用于深度伪造的工具包括：Faceforencs++[47]、Celeb-DF [27]、DFD [9]、FaceShifter [23]和UADFV[26]；<br/>​    用于IMDL的工具有：CASIA [11]、COVERAGE[60]、Columbia[20]、IMD2020 [39]、NIST16 [16]、CocoGlide[17]和Autosplice [21]；<br/>​    用于AIGC的工具包含：DiffusionForensics[59]和GenImage [69]；<br/>​    用于文档处理的工具则有：Doctamper[43]、T-SROIE [58]、OSTF [44]、TPIC-13 [57]和RTM[32]。<br/>​  各数据集的简要总结见表2，更多详细信息见附录D.1。</p><p>​  <strong>模型</strong><br/>​  本文使用的模型为：<br/>​    深度伪造检测的有Capsule-Net[37]、RECCE [4]、SPSL [28]、UCF [66]和SBI[49]；<br/>​    图像处理与定位领域的有MVSS-Net [5]、CAT-Net[22]、PSCC-Net [29]、Trufor [17]、IML-ViT [35]和Mesorch[70]；<br/>​    生成式人工智能检测领域的有Dire [59]、DualNet[63]、HiFiNet [18]、Synthbuster [2]和UnivFD[40]；<br/>​    文档检测方面的有DTD [43]、FFDN [6]、CAFTB [52]和TIFDM[12]。<br/>​  这些方法均来自官方资源库及我们的重新实现版本。此外，ForensicHub还精选了视觉任务中常用的6种骨干网络：Resnet[19]、Xception [7]、EfficientNet [55]、Segformer [64]、Swin Transformer[30]以及ConvNext [31]。具体模型参数详见附录D.2。</p><p>​  <strong>指标</strong><br/>​  本文使用的指标为：AP、MCC、TNR、TPR、AUC、ACC、F1和IOU等指标，其像素级和图像级实现方式如图1所示。各指标的具体说明详见附录D.3。在评估过程中，所有指标的阈值（若适用）均设为0.5，以确保比较的公平性。</p><h1 id="基准">4 基准</h1><p>​  除了与现有基准测试DeepfakeBench [67]和IMDLBenCo[36]完全兼容外，ForensicHub还通过引入针对AIGC和文档领域的统一评估协议，进一步推动了标准化进程——这两个领域此前一直缺乏广泛认可的基准测试和代码库。我们为这两个领域提出了两种评估协议，用于衡量模型的泛化能力。</p><h2 id="ai生成图像检测基准">4.1 AI生成图像检测基准</h2><p>​  <strong>数据集</strong><br/>​  在AIGC检测领域，数据集构建的难点通常不在于获取足够数量的样本——因为现有模型已能轻松生成这些样本，而在于确保对各类生成模型的全面覆盖。为此，我们仅选取两个常用公开数据集：DiffusionForensics[59]和GenImage[69]。前者仅包含基于扩散生成的图像，后者则涵盖由八种顶尖生成模型构建的百万级数据集。模型在扩散取证数据集上进行训练，并通过基因图像中的不同生成模型进行评估以检验泛化能力，因为检测方法通常已在同源生成模型的样本上取得良好表现[69]。具体数据划分详见表D.1.3。</p><p>​  <strong>模型</strong><br/>​  ForensicHub在AIGC检测中实现了五种最先进方法：Dire[59]、DualNet [63]、HiFiNet [18]、Synthbuster [2]和UnivFD[40]。其中Synthbuster没有官方开源代码，我们对其进行了完全重实现。更多关于模型和训练设置的详细信息，请参见附录E.1。</p><p>​  <strong>结果</strong><br/>​  表3中的绿色背景部分展示了AIGC基准测试中图像级分类的AUC分数，具体分为扩散取证[59]测试集的域内结果、不同生成模型的跨域结果以及GenImage[69]总数据集的结果。结果显示，AIGC的顶尖模型在与训练集同源的扩散取证测试集中表现优异，同时在由基于扩散生成的图像（如ADM、VQDM和GLIDE）构成的数据集上也表现出色。然而，这些模型对Midjourney和Wukong等生成模型的泛化能力相对较弱，这不仅指出了改进方向，也为未来模型开发提供了重要参考。</p><figure><img src="../postimages/ForensicHub/image-20250717231959957.png"alt="image-20250717231959957" /><figcaption aria-hidden="true">image-20250717231959957</figcaption></figure><h2 id="文档图像处理定位基准">4.2文档图像处理定位基准</h2><p>​  <strong>数据集</strong><br/>​  目前用于文档图像操作定位的现有数据集主要分为两大类：一类是高保真非切片数据集，包括T-SROIE[58]、OSTF [44]、TPIC-13 [57]和RTM[32]；另一类则是切片数据集，以Doctamper[43]为代表。两者的根本区别在于是否对图像进行了分块切片预处理。<br/>​  为确保后续评估的一致性，我们采用了Doctamper的切片策略，并将其应用于四个未切片的数据集，从而形成统一格式。每个数据集均保留原有的训练集与测试集划分。值得注意的是，Doctamper提供了一个训练集和三个不同的测试集——Doctamper-Test、Doctamper-FCD和Doctamper-SCD，分别针对不同操作场景设计。具体数据分布详见表D.1.4。</p><p>​  <strong>模型</strong><br/>​  我们采用了两个开源模型，DTD [43]和FFDN[6]，并复现了两个闭源模型，CATFB [52]和TIFDM[12]。所有细节见附录D.2.4和E.2。</p><p>​  <strong>结果</strong><br/>​  根据原始协议[43,6]，每个检测器都在指定的训练集上进行训练，并在对应的测试集上进行评估。如表4所示，有三种模型始终保持着最佳性能：专为文档取证设计的FFDN和DTD，以及基于IMDL架构的Cat-Net模型。值得注意的是，这三种方法都采用了JPEG特有的先验特征，例如离散余弦变换系数和量化表，这凸显了压缩伪影在定位文档图像篡改时的鉴别价值。</p><figure><img src="../postimages/ForensicHub/image-20250717232315889.png"alt="image-20250717232315889" /><figcaption aria-hidden="true">image-20250717232315889</figcaption></figure><p>​  然而，这种评估设置存在一个关键缺陷：所有模型均在相同数据分布下进行训练和测试，这限制了跨域泛化能力的评估。为解决这一问题，我们专门设计了文档协议（DocProtocol），让模型在Doctamper数据集上训练，并在另外四个文档级测试集上进行评估。如表5所示，PSCC-Net展现出更优的泛化性能，充分证明了渐进式空间建模在文档操作定位中的优势。</p><figure><img src="../postimages/ForensicHub/image-20250717232431850.png"alt="image-20250717232431850" /><figcaption aria-hidden="true">image-20250717232431850</figcaption></figure><h1 id="图像取证融合协议">5图像取证融合协议</h1><p>​  <strong>协议</strong><br/>​  为统一不同模型在法证协议下的性能评估，我们基于CAT-Net的训练数据构建策略，开发了图像取证融合协议（IFF-协议,<em>mageforensic fusionprotoco</em>）。该协议将训练集定义为Deepfake、IMDL、AIGC和文档数据的组合，每个训练周期从各领域随机抽取等量样本。在训练过程中，我们选择了来自Deepfake任务的FaceForensics++[47]，来自IMDL任务的CASIAv2 [11]，来自AIGC任务的GenImage[69]以及来自文档任务的OSTF[44]、RealTextManipulation、T-SROIE和Tampered-IC13数据集。我们采用最小数据集CASIAv2（包含12,641个样本），作为每个训练周期的采样量。在测试阶段，我们直接在不同领域的数据集上评估模型性能，无需进行微调。</p><p>​  <strong>实施细节</strong><br/>​  我们将图像尺寸统一调整为256×256（UnivFD、DTD和FFDN模型除外，具体参数详见附录F），仅采用基础数据增强手段：包括图像翻转、亮度与对比度调节、压缩处理以及高斯模糊。所有模型均采用余弦衰减学习率策略进行20轮训练，初始值设为1e−4，训练过程中逐步递减至1e−5。针对输出掩码的模型（IMDL和Doc），我们在最后一层特征图上应用最大池化操作以获取预测标签，并仅基于该标签计算损失函数。</p><p>​  <strong>模型效率</strong><br/>​  我们在表6中测试了各领域模型主干网络和超大规模架构的参数设置与浮点运算量。可以发现，模型效率往往与其应用场景密切相关。例如，深度伪造模型通常采用轻量化设计以支持实时视频检测，而专注于像素级分类的IMDL模型则常采用更复杂且体积更大的架构。这些效率偏好会对IFF协议下的实验结果产生显著影响。</p><figure><img src="../postimages/ForensicHub/image-20250717233135309.png"alt="image-20250717233135309" /><figcaption aria-hidden="true">image-20250717233135309</figcaption></figure><p>​  <strong>基准结果</strong><br/>​  表7展示了基于IFF协议下四个领域数据集的主干网络与领域特定SoTA方法的AUC分数，其中DFD代表DeepFakeDetection[15]，DF指DiffusionForensics [59]，RTM则对应RealTextManipulation[32]。详细结果详见附录F。</p><figure><img src="../postimages/ForensicHub/image-20250717233248267.png"alt="image-20250717233248267" /><figcaption aria-hidden="true">image-20250717233248267</figcaption></figure><p>​  研究结果表明，令人惊讶的是，诸如ConvNeXt [31]和Swin Transformer[30]等视觉主干网络的表现几乎超越了所有领域特定的最先进方法，这说明当在更统一的伪造图像数据上训练时，主干网络展现出更大的潜力。与此同时，领域特定的最先进方法并不总能在其任务中保持优势。例如，基于CLIP微调的AIGC检测模型UnivFD[40]，在IMDL提供的IMD2020[39]数据集上表现出色，揭示了跨任务方法可迁移性的宝贵见解。</p><p>​  从任务层面来看，尽管IMDL的目标分类已从像素级提升至图像级，但由于不同数据集在规模和操作类型上存在显著差异，该任务仍具挑战性。相比之下，AIGC得益于基于多样化生成模型的充足训练数据，从而实现了更高的检测精度。这一现象提醒我们：不仅要确保训练数据涵盖丰富的操作类型，更要着力提升模型的泛化能力。</p><h1 id="实验">6实验</h1><p>​  基于ForensicHub，我们进行了交叉任务实验，这在以前的研究中很少被探索。不同任务检测方法的异同性引出了以下问题：1）低级特征提取器是否在所有任务中都有效？2）一个任务的检测方法在转移到另一个任务时是否仍然有效？我们通过大量实验回答了上述问题。</p><h2 id="低级特征提取器的有效性">6.1低级特征提取器的有效性</h2><p>​  鉴于各领域已提出特定特征提取器，为统一评估其在特定领域的有效性，我们在上述IFF协议框架下，采用6种骨干网络与4种不同浅层提取器进行实验。具体包括：用于噪声伪影的BayarConv[3]、处理边缘伪影的Sobel [5]、消除频率伪影的DCT [1]以及滤波伪影的FPH[43]。各提取器的具体参数详见附录G.1。</p><figure><img src="../postimages/ForensicHub/image-20250717233639185.png"alt="image-20250717233639185" /><figcaption aria-hidden="true">image-20250717233639185</figcaption></figure><p>​  表8展示了各任务测试数据集上，使用四种不同特征提取器与未使用特征提取器的各主干网络版本之间的AUC差异平均值。除EfficientNet[55]外，其他主干网络在使用特征提取器后均出现性能下降，这表明在IFF协议框架下（训练数据包含充足的操控类型和图像数量），模型无需依赖特征提取器提供的额外信息。不过由于EfficientNet本身轻量级的优势，仍能从特征提取器中获益。研究结果表明，特征提取器可能仅对小规模数据集、有限操控类型或轻量级模型的检测任务有益。各领域测试数据集的AUC分数详情可参见附录G.2。</p><h2 id="任务专用检测器的可转移性">6.2任务专用检测器的可转移性</h2><h3 id="imdl与文档基准之间的交叉评估">6.2.1IMDL与文档基准之间的交叉评估</h3><p>​  当前文档级检测器的输入输出格式与图像处理检测定位模型完全兼容。基于这种一致性，我们进行了双向评估：将IMDL检测器应用于文档基准测试，同时将文档检测器用于IMDL基准测试。这种交叉测试不仅扩大了各基准的有效模型库规模，还使我们能够突破检测器原有任务范围，深入探究其通用性。</p><p>​  <strong>IMDL→Document</strong><br/>​  表4展示了原始文档基准划分下域内评估结果，而表5则呈现了我们新引入的泛化协议在跨域测试中的表现。在这两种场景中，IMDL检测器在文档取证场景中均展现出强劲竞争力。在常规划分中，Cat-Net[22,43,6]家族以最佳平均F1值脱颖而出，印证了其分层“CatNet”架构的优势。在更具挑战性的跨域评估中，PSCC-Net[29]展现出显著更强的泛化能力，表明渐进式空间建模能有效捕捉文档篡改定位线索。我们期待未来研究能进一步揭示PSCC-Net背后的潜在机制。</p><figure><img src="../postimages/ForensicHub/image-20250717232315889.png"alt="image-20250717232315889" /><figcaption aria-hidden="true">image-20250717232315889</figcaption></figure><figure><img src="../postimages/ForensicHub/image-20250717232431850.png"alt="image-20250717232431850" /><figcaption aria-hidden="true">image-20250717232431850</figcaption></figure><p>​  <strong>Document →IMDL</strong><br/>​  根据MVSS训练协议[36]，所有文档导向模型均在CASIAv2数据集[11]上进行训练，并在五个标准IMDL测试集上进行评估。如表9所示，CAFTB[52]的双分支架构在迁移至IMDL任务时，于文档模型中展现出最佳整体性能——这一结果与当前最先进模型Mesorch[70]的设计理念相契合，该模型同样强调双分支学习。</p><figure><img src="../postimages/ForensicHub/image-20250717234116217.png"alt="image-20250717234116217" /><figcaption aria-hidden="true">image-20250717234116217</figcaption></figure><h3id="将imdl检测器扩展到aigc和deepfake基准">6.2.2将IMDL检测器扩展到AIGC和Deepfake基准</h3><p>​  IMDL模型旨在生成像素级掩码和图像级标签，其架构大多同时包含分类头和分割分支。这种双输出设计使其能够直接应用于AIGC和Deepfake检测等任务。对于未配备标签头的模型，图像级评分是通过在预测掩码上进行最大池化操作获得的。</p><p>​  <strong>IMDL→AIGC</strong><br/>​  我们在AIGC基准测试的训练数据集上对代表性IMDL检测器进行微调，并在表3中报告了跨生成器性能。训练设置及其他配置与前述AIGC基准测试设置保持一致。结果显示，IMDL技术中的噪声特征（TruFor）和多尺度分析（IML-ViT）等方法，在AIGC检测中依然有效。</p><figure><img src="../postimages/ForensicHub/image-20250717231959957.png"alt="image-20250717231959957" /><figcaption aria-hidden="true">image-20250717231959957</figcaption></figure><p>​  <strong>IMDL →Deepfake</strong><br/>​  我们在FF++-c23训练集上训练每个IMDL检测器，并在所有剩余的深度伪造测试集上进行评估，具体分数详见表10。与deepfakebench[67]中的所有基线模型相比，Cat-Net在域内设置中表现最佳，而Mesorch则在跨域评估中取得最高平均准确率，在两个场景下均创下新的最佳成绩。</p><figure><img src="../postimages/ForensicHub/image-20250717234428051.png"alt="image-20250717234428051" /><figcaption aria-hidden="true">image-20250717234428051</figcaption></figure><h1 id="结论">7.结论</h1><p>​  本文提出ForensicHub，这是首个面向全领域假图像检测与定位的统一基准测试平台及代码库。该平台不仅对现有基准测试进行了优化，还拓展至其他领域。通过大量跨领域实验，我们总结出8项对未来研究具有指导意义的关键洞见：</p><p>​  1)在文档领域，PSCC-Net表现出强大的泛化能力，而Cat-Net能有效适应人工合成操作，为文档模型设计提供了有价值的参考。<br/>​  2)在IMDL中，CAFTB和Mesorch等并行架构模型取得了领先性能，表明多分支建模的有效性。<br/>​  3)像Cat-Net和Mesorch这样的频率策略模型始终表现良好，突出了频率特征在FIDL中的潜力。<br/>​  4)在IFF协议下，像ConvNeXt和SwinTransformer这样的较少被探索的主干网络的表现优于几乎所有领域的SoTA。<br/>​  5)当数据集较大且包含多种操作类型时，浅层特征提取器的串联往往会降低性能，而像EfficientNet这样的轻量级模型可以从这种做法中受益。<br/>​  6）当前AIGC和文档评估往往忽视泛化能力，导致性能被高估。我们建议将本文提出的AIGC和文档协议用于后续工作，该协议明确鼓励设计具有泛化意识的模型。<br/>​  7）现有的AIGC和Deepfake数据集往往过于简单，缺乏多样性，限制了有意义的比较，未来的基准应该以更高的复杂性和真实性为目标。<br/>​  8）对于全域场景，我们推荐使用我们的IFF协议来实现更全面的评估。</p><p>​  总之，ForensicHub是打破四个领域领域孤岛的重要一步，为FIDL模型架构、数据集特征和评估标准的未来研究提供了新的见解。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TruFor:Leveraging all-round clues for trustworthy image forgery detection and localization</title>
      <link href="/TruFor/"/>
      <url>/TruFor/</url>
      
        <content type="html"><![CDATA[<p>TruFor: Leveraging all-round clues for trustworthy image forgerydetection and localization</p><p>Fabrizio Guillaro1 ，Davide Cozzolino1 ，Avneesh Sud2 ，NicholasDufour2 ，Luisa Verdoliva1</p><p>1University Federico II of Naples<br/>2Google Research</p><p><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2023-red" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2212.10957"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/grip-unina/TruFor"><imgsrc="https://img.shields.io/github/stars/grip-unina/TruFor?style=flat"alt="GitHub" /></a> <a href="https://grip-unina.github.io/TruFor/"><imgsrc="https://img.shields.io/badge/project-blue" alt="project" /></a></p><h1 id="摘要">摘要</h1><p>​  本文提出TruFor取证框架，可应用于从传统廉价伪造到基于深度学习的新型图像篡改技术。该框架通过基于Transformer的融合架构，同时提取高阶特征与低阶特征：前者整合RGB图像与自适应学习的噪声敏感指纹，后者则通过仅使用真实数据进行自监督训练，精准捕捉相机内外部处理产生的伪影特征。系统通过检测原始图像中规律性特征的异常偏离，实现对伪造图像的有效识别。通过异常检测机制，该方法能有效识别各类局部篡改行为，确保结果具有普适性。除像素级定位图和全图像完整性评分外，我们还生成了可靠性评估图，精准标注定位预测可能出现误判的区域。这一特性在取证应用中尤为重要，既能减少误报风险，又支持大规模分析。基于多个数据集的大量实验表明，我们的方法在检测和定位廉价伪造与深度伪造篡改方面表现优异，其准确率已超越现有最先进方案。代码可在https://grip-unina.github.io/TruFor/上公开获取。</p><figure><img src="../postimages/TruFor/image-20250715225930456.png"alt="image-20250715225930456" /><figcaption aria-hidden="true">image-20250715225930456</figcaption></figure><h1 id="结果">4.结果</h1><h2 id="实验设置">4.1.实验设置</h2><p>​  <strong>训练</strong><br/>​  我们的方法包含三个独立的训练步骤。首先，我们使用两个热门图片分享网站——Flickr（www.flickr.com）和DPReview（www.dpreview.com）上公开的大量原始图像数据集来训练Noiseprint++提取器。该数据集包含来自43个品牌的1475款不同相机型号（每款8至92张图像）拍摄的24,757张原始图像。接着，我们采用与CAT-Netv2[24]提出的相同数据集（包含原始图像、伪造图像及其对应真实标签），训练异常定位网络的编码器和解码器。最后，基于同一数据集，我们训练置信度图解码器和伪造检测器。更多数据集详情可参见补充材料。</p><p>​  <strong>测试</strong><br/>​  我们通过七个公开数据集和一个基于扩散模型开发的本地处理数据集对模型进行了基准测试。具体而言，我们采用了文献中广泛使用的CASIAv1 [15]、Coverage [40]、Columbia [20]、NIST16 [19]、DSO-1 [13]和VIPP[7]等数据集，这些数据集包含剪辑、复制移动和修复等低成本伪造操作。总体而言，这些数据集共包含1530张伪造图像和1412张真实图像。此外，我们还加入了OpenForensics[26]——一个使用GAN模型生成的面部伪造大数据集，并从中抽取了2000张图像；同时引入了CocoGlide数据集，其中包含512张由GLIDE扩散模型（基于COCO2017验证集中的图像生成）生成的图像。</p><p>​  <strong>指标</strong><br/>​  与以往大多数研究类似，我们在像素级评估中采用F1指标，并同时展示最佳阈值和默认0.5阈值的对比结果。而在图像级分析中，我们改用无需设定决策阈值的AUC指标，以及综合考虑误报与漏检的平衡准确率指标——此时阈值同样被重新设定为0.5。</p><h2 id="最新技术水平比较">4.2.最新技术水平比较</h2><p>​  为确保公平比较，我们仅选取在线公开代码和/或预训练模型的方法，并在选定的测试数据集上进行验证。此外，为避免偏差，我们仅纳入在与测试数据集不重叠的数据集上训练的方法。最终纳入的模型方法包括：基于JPEG伪影的ADQ[6]、利用噪声伪影的Splicebuster[10]；以及11种深度学习方法：EXIF自洽性[22]、约束R-CNN [44]、RRU-Net[5]、ManTraNet [42]、SPAN [21]、AdaCFA [2]、端到端[31]、CAT-Net v2[24]、IF-OSN [41]、MVSS [8]、PSCC-Net [29]、Noiseprint[12]。这些方法的简要概述详见表3。</p><figure><img src="../postimages/TruFor/image-20250715230158856.png"alt="image-20250715230158856" /><figcaption aria-hidden="true">image-20250715230158856</figcaption></figure><p>​  <strong>定位结果</strong><br/>​  在表1中我们展示了像素级定位性能。</p><figure><img src="../postimages/TruFor/image-20250715230235435.png"alt="image-20250715230235435" /><figcaption aria-hidden="true">image-20250715230235435</figcaption></figure><p>​  我们的方法平均F1值表现最佳，在所有测试数据集上均位列第一或第二，充分证明了其在各类操作场景下的卓越泛化能力。事实上，该方法在OpenForensics（基于生成对抗网络的局部操作）和CocoGlide（扩散模型驱动的局部操作）等场景中同样表现出色——除CAT-Netv2外，其他多数方法在此类任务中均遭遇惨败。得益于采用Noiseprint++算法（基于数字历史记录的训练方法），我们的方法在所有测试数据集上始终保持优异表现。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pixel-Inconsistency Modeling for Image Manipulation Localization</title>
      <link href="/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/"/>
      <url>/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<p>Pixel-Inconsistency Modeling for Image Manipulation Localization</p><p>Chenqi Kong <em>, Member, IEEE</em>, Anwei Luo , Shiqi Wang <em>,Senior Member, IEEE</em>, Haoliang Li <em>, Member, IEEE</em>,AndersonRocha <em>, Fellow, IEEE</em>, and Alex C. Kot <em>, Life Fellow,IEEE</em></p><h1 id="摘要">摘要</h1><p>​  数字图像取证在图像认证与篡改定位领域发挥着关键作用。尽管深度神经网络技术取得了显著进展，但现有伪造定位方法在处理未知数据集和受干扰图像时仍存在局限性（即缺乏普适性和对实际应用的鲁棒性）。为解决这些问题并保障图像完整性，本文通过分析像素不一致伪影，提出了一种通用且稳健的篡改定位模型。这一理论依据源于一个观察：大多数图像信号处理器（ISP，<strong>imagesignalprocessors</strong>）都会进行去马赛克处理，这会导致原始图像中出现像素相关性。此外，诸如拼接、复制移动和修复等操作会直接影响这种像素规律性。因此，我们首先将输入图像分割成多个区块，并设计了掩码自注意力机制来模拟输入图像中的全局像素依赖关系。与此同时，我们优化了另一条局部像素依赖流，用于挖掘输入伪造图像中的局部操作线索。此外，我们设计了新型学习加权模块（LWM，<strong>Learning-to-WeightModules</strong>）来整合两条信息流的特征，从而提升最终的伪造定位性能。为优化训练过程，我们提出了一种创新的像素不一致性数据增强（PIDA，<strong>Pixel-InconsistencyDataAugmentation</strong>）策略，促使模型专注于捕捉固有的像素级伪影特征，而非单纯挖掘语义层面的伪造痕迹。本研究构建了一个综合性的基准测试框架，整合了12个数据集中的16种代表性检测模型。大量实验表明，我们的方法能够有效提取像素不一致性特征的固有特征，在图像篡改定位任务中展现出最先进的泛化能力和鲁棒性表现。</p><h1 id="i.引言">I.引言</h1><p>​  图像伪造技术自摄影诞生之初便已存在[3]。近几十年来，拼接、复制移动和修复等图像处理技术取得重大突破，这三种技术虽普遍却臭名昭著[87]（如图1所示）。这些技术能生成高度逼真的伪造内容，模糊真伪图像的界限。其痕迹极其微妙，肉眼几乎难以察觉。随着数字图像在互联网上的广泛传播，恶意攻击者利用现成的强大图像编辑工具（如Photoshop、AfterEffectsPro、GIMP以及最新推出的Firefly）发起伪造攻击变得愈发容易。这些精心制作的内容可用于实施欺诈、制造假新闻及敲诈勒索。图像伪造无疑动摇了公众对媒体内容的信任，而伪造品的泛滥更引发了社会对网络安全的深切担忧。因此，设计有效的图像伪造定位模型以应对这些问题至关重要。<br/>​  早期图像篡改定位技术主要依赖先验知识进行特征提取，例如镜头畸变[28]，[32]，[46]，[67]，[96]，[96]，色彩滤波阵列（CFA）伪影[10]，[25]，[29]，[38]，[76]，噪声模式[17]，[22]，[48]，[64]，[65]，[75]，压缩伪影[6]，[9]，[15]，[23]，[27]，[43]，[73].。然而这些传统方法存在精度和泛化能力不足的问题。随着深度学习与人工智能的突破，基于学习的检测器应运而生，在域内图像伪造定位方面展现出优异性能。但数据驱动方法普遍存在过拟合训练数据的缺陷，导致鲁棒性和泛化能力受限——具体表现为对图像扰动异常敏感，且难以应对未知的图像篡改数据集。<br/>​  提取通用且稳健的图像伪造特征以实现精准定位仍是业界面临的重要挑战。本文重构了传统图像处理流程，提出一种新型伪造检测框架，通过捕捉图像中的像素不一致性实现精准识别。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715214046399.png"alt="image-20250715214046399" /><figcaption aria-hidden="true">image-20250715214046399</figcaption></figure><p>​  图2展示了典型的伪造图像生成链：滤光片与镜头将非目标光线聚焦至传感器，随后应用色彩滤光阵列（CFA）提取单色分量。在相机内部处理阶段，系统执行去马赛克（即色彩插值）等软件操作，通过周边单色像素重建全彩图像。后续进行色彩校正、降噪和压缩等内部处理步骤，最终生成RGB图像。值得注意的是，恶意攻击者可利用图像编辑工具在后期处理中篡改原始图像，这些操作会破坏去马赛克操作引入的像素相关性（即，扰动周期性图案），从而在取证分析中留下明显的像素不一致性痕迹[10]、[76]、[87]。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715214203039.png"alt="image-20250715214203039" /><figcaption aria-hidden="true">image-20250715214203039</figcaption></figure><p>​  图3展示了四种典型的色彩滤波算法类型：(a)BayerCFA、(b)RGBE、(c)CMY、(d)CMYG。色彩滤波技术能够实现每个像素点对特定颜色的精准捕捉。因此，在生成的RAW图像中，每个像素点仅保留单一颜色信息，而去马赛克处理则会重建缺失的颜色样本。现有用于伪造指纹提取的法证分析技术主要聚焦于不同图像规律性的数学建模。例如，Popescu等人[76]量化了CFA插值引入的特定相关性，并描述了如何自动检测这些相关性。Ferrara等人[25]提出了一种新颖特征，通过最小2×2块级测量这些图像规律性的存在与否，从而预测伪造概率图。在[11]和[55]的研究中，则采用线性回归方法对块内指纹进行建模。尽管这些像素相关性建模方法在法证分析中效果显著，但多数仍需将CFA类型作为先验信息。此外，这些方法难以充分捕捉现代AI相机中智能图像信号处理器（ISP）引入的更复杂规律性。<br/>​  与现有技术不同，我们基于这一洞见提出了一种基于学习的方法，用于捕捉伪造图像中的固有像素不一致性。为此，我们设计了一个双流像素依赖建模框架来实现图像操作定位。受近期自回归模型（如PixelCNN[85]、[86])在各类计算机视觉任务中取得的成功启发，我们开发了掩码自注意力机制来建模输入图像中的全局像素依赖关系。此外，我们还设计了差异卷积（DC，DifferenceConvolution）流以捕捉局部图像区域内的像素不一致性伪影。同时，我们引入了创新的权重学习模块（LWM，Learning-toWeightModules），将这两个流中的全局与局部像素不一致性特征进行融合。<br/>​  我们设计了三种解码器来预测潜在篡改区域、伪造边界及重建图像。最终引入像素不一致性数据增强（PIDA，Pixel-InconsistencyDataAugmentation）策略，用于探索像素级伪造痕迹。PIDA是一种仅依赖真实图像进行数据增强的有效方法，它引导模型专注于捕捉像素不一致性伪影而非语义伪造痕迹。该框架采用端到端训练方式，通过二值掩膜和边界标签的联合监督实现优化。<br/>​  我们的工作主要贡献是：</p><ul><li>我们构建了一个全面的基准测试框架，用于评估16种代表性图像伪造定位方法在12个数据集上的泛化能力。该框架进一步扩展至六种未知图像扰动类型（每种包含九个严重程度等级）的鲁棒性评估。此外，我们还针对现代人工智能生成内容（AIGC）技术产生的复杂高级操作，对设计模型进行了性能验证。</li><li>我们设计了一个双流图像篡改定位框架，包含局部像素依赖编码器、全局像素依赖编码器、四个特征融合模块和三个解码器。该模型能够有效提取像素不一致性伪造特征，从而实现更通用且鲁棒的篡改定位性能。</li><li>我们提出了一种像素不一致性数据增强策略，该策略完全利用真实图像来生成数据。这种数据增强方法促使模型专注于捕捉固有的像素级伪影，而非语义伪造线索，从而显著提升了伪造定位的性能表现。</li><li>大量定量和定性实验结果表明，我们提出的方法在泛化能力和鲁棒性评估中始终优于现有最先进方法。全面的消融实验进一步验证了设计组件的有效性。</li></ul><p>​  第二部分概述了图像伪造定位与像素依赖建模领域的前期研究成果。第三部分详细阐述了本文设计的框架体系。第四部分在多种实验场景下展示了全面的评估结果。最后，第五部分对全文进行总结，并探讨了当前存在的局限性及未来可能的研究方向。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  本节将对图像伪造检测与定位领域的现有研究进行综述，包括人工设计方法和基于学习的方法。同时，我们还将回顾像素依赖性建模及其应用的相关研究。</p><h2id="a.使用低等级痕迹的操纵检测和定位方法">A.使用低等级痕迹的操纵检测和定位方法</h2><p>​  图像篡改检测并非新问题。早期方法主要针对相机内部处理痕迹产生的低级伪影进行识别。例如，复杂光学系统缺陷导致的镜头畸变[28]，[32]，[46]，[67]，[96]，[96]，可视为取证领域的独特指纹。色差作为典型镜头畸变特征，已被广泛应用于防伪检测[46]，[67]，[96].。此外，许多方法[10]，[11]，[25]，[76]通过捕捉滤色阵列（CFA）伪影来检测操作痕迹。这些技术表明，篡改行为会破坏去马赛克处理过程中形成的周期性图案。同时，由于光响应不均匀性（PRNU）具有机型特异性，部分方法[48]，[64]，[65]通过提取查询图像中的噪声模式来检测数字篡改痕迹。此外，学界已投入大量研究致力于分析JPEG压缩伪影在离散余弦变换（DCT）域[9]、[15]、[23]、[24]、[73]中的残留特征，用于图像伪造检测。尽管这些传统图像篡改检测方法具有可解释性和计算效率优势，但多数仍存在检测精度不足和泛化能力有限的问题。为了实现准确、通用和可解释的图像伪造定位，我们在本文中引入了一个基于学习的框架，旨在捕捉低级像素不一致的伪影。</p><h2id="b.基于学习的操纵检测与定位方法">B.基于学习的操纵检测与定位方法</h2><p>​  近年来，图像取证领域取得显著进展。为解决伪造定位难题，各类基于学习的方法层出不穷，有效提升了检测性能。这些方法大多依托丰富的先验知识，例如噪声特征[18]，[34]，[103]，CFA伪影特征[4]、JPEG特征[52]，[78]，[90]等进行检测。高频（HF）滤波器如隐写分析富模型（SRM）滤波器[94]，[103]和拜耳滤波器[19]，[94]也被用于捕捉大量高频伪造伪影。此外，通过检测伪造边界线[19]，[81]，像素级伪造检测效果得到显著提升。更有甚者，部分方法采用多尺度学习技术从不同层级提取伪造特征，从而大幅提升检测精度。与SPAN[40]通过金字塔形局部自注意力模块堆叠来捕捉多尺度图像块或像素间关系不同，我们的方法创新性地采用了三种核心模块：用于捕捉局部像素差异的掩码式自注意力全局像素依赖编码器、模拟长程像素关联的掩码式自注意力编码器，以及整合伪造特征的融合模块。这些组件的设计旨在更精准捕捉伪造图像中固有的像素不一致性伪影。得益于视觉变换器（ViT）的问世，基于ViT的检测器[58]、[89]凭借其长程交互特性与无归纳偏置的优势，在包括取证在内的多种场景中展现出卓越的检测性能。然而，这些数据驱动方法仍存在泛化能力和鲁棒性不足的问题。本文认为，伪造图像中的像素不一致性是各类篡改操作和数据集中普遍存在的一种伪影特征。因此，我们设计了一个新的图像伪造定位框架，通过捕捉像素不一致的伪影来实现更通用和鲁棒的伪造定位性能。</p><h2 id="c.像素依赖性建模">C.像素依赖性建模</h2><p>​  自回归（AR）模型[14]，[16]，[31]，[53]，[72]，[80]，[85]在图像生成[31]，[49]，[85]，、补全[14]，[44]，[72]，和分割[71]等各类计算机视觉任务中取得了显著成效。这些AR方法旨在通过以下方式对每个像素的联合概率分布进行建模：<span class="math display">\[\hat{a}_{i}\simp_{\theta}(a_{i}|a_{1},\cdot\cdot\cdot,a_{i-1}).\]</span>​  这些模型采用特定的掩码卷积或掩码自注意力策略，使得当前像素的概率分布取决于生成序列中所有先前像素。像PixelCNN[85]和PixelRNN[86]这样的开创性AR模型，在图像生成领域展现了其在模拟自然图像长程像素依赖关系方面的有效性。为了进一步提高图像生成性能，引入了后续变化，例如PixelCNN++[80]。此外，掩码自注意力机制还能辅助依赖关系建模，例如图像变换器[72]和稀疏变换器[16]。像素螺旋[14]通过将因果卷积与自注意力机制相结合，显著提升了图像生成效果。受像素依赖建模在各类生成任务中取得突破性进展的启发，我们尝试将这一概念拓展至法医分析领域。本文创新性地提出像素差异卷积与掩码自注意力机制，旨在捕捉局部与全局像素不一致性伪影。</p><h1 id="iii.提出的方法">III.提出的方法</h1><p>​  本节将详细介绍我们提出的图像操作定位方法。首先阐述整体框架架构，接着深入解析各核心模块的设计原理与技术逻辑，包括全局像素依赖建模模块、局部像素依赖建模模块以及学习加权模块。最后重点介绍创新的像素不一致性数据增强策略及其独特优势。</p><h2 id="a.总体框架">A.总体框架</h2><p>​  如图4所示，本文设计了一种双流图像操作定位框架。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715215031753.png"alt="image-20250715215031753" /><figcaption aria-hidden="true">image-20250715215031753</figcaption></figure><p>​  该框架的设计灵感源自一个观察发现：诸如拼接、复制移动和修复等图像处理过程，会不可避免地破坏去马赛克操作引入的像素规律性。该框架通过局部像素依赖编码器和全局像素依赖编码器，共同探索像素不一致性及上下文信息以实现操作定位。具体而言，输入图像首先被分割成多个块，随后由两个编码器进行并行处理。在块嵌入过程中，我们将输入图像分割为4×4像素的块。将每个补丁的原始像素RGB值展平为4×4×3=48的维度，然后将每个补丁标记投影到嵌入维度。直观来看，将单个RGB像素值嵌入单一标记有助于建模像素间的依赖关系。但这种方法会显著增加计算成本，因为标记数量将与图像的高度(H)和宽度(W)相等（本研究中H=W=512）。为解决这一问题并实现合理平衡，我们将补丁尺寸放宽至4×4，从而在计算效率与有效捕捉处理图像中的像素不一致性之间取得平衡。相较于单像素，4×4像素标记能提供更具表现力的表征。实验表明，采用这种补丁嵌入策略的方法成功捕捉了处理图像中的全局和局部像素不一致性。此外，我们在设计的transformer模块中引入多层感知机（MLP）层，以增强每个标记内部像素依赖关系的学习效果[56]、[84]。<br/>​  为探索长程相互作用与无感应偏置特性，我们采用transformer架构作为两条信息流的主干网络。上层局部像素依赖编码器包含四个差分卷积（DC，DifferenceConvolution）模块，专门用于捕捉局部区域的像素不一致性。下层全局像素依赖编码器则引入了四个创新的掩码自注意力模块，该机制能建模输入图像中的全局像素依赖关系。此外，我们设计了四个学习加权模块（LWM，Learning-to-WeightModules），在多层级结构中协同整合全局特征[fg1，fg2，fg3，fg4]与局部特征[fl1，fl2，fl3，fl4]。整个框架还集成了边界解码器、伪造解码器和图像解码器三大核心组件。<br/>​  值得注意的是，像素不一致性在边界区域表现最为明显。为此，我们整合了边界辅助监督机制以提升最终伪造检测的定位精度。伪造解码器以组合特征[f1，f2，f3，f4]作为输入，用于预测图像中可能被篡改的区域；而图像解码器则以[fg1，fg2，fg3，fg4]为输入，致力于还原原始图像。最后，我们提出了一种新型的像素不一致性数据增强（PIDA）策略，该策略专注于像素不一致性而非语义伪造痕迹。这一策略进一步增强了模型的泛化能力和鲁棒性。</p><h2 id="b.全局像素依赖性建模">B.全局像素依赖性建模</h2><p>​  本部分的目标是通过光栅扫描顺序对每个标记进行条件化处理，从而建模图像块间的全局像素依赖关系。因此，每个输出标记都会高度依赖所有先前“已观测”的像素。相较于单独处理单个像素，这种设计在计算效率上更具优势。考虑到图像中的空间冗余特性[36]，该方法还能有效建模全局像素依赖关系。<br/>​  受[14]和[72]模型的启发，这些模型通过注意力机制建模长期像素依赖关系，我们在像素全局依赖建模中引入了掩码自注意力（MaskedSA）模块，其设计风格与自注意力机制类似。图4展示了全局像素依赖编码器与图像解码器的组合结构，共同构成自动编码器。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715215920246.png"alt="image-20250715215920246" /><figcaption aria-hidden="true">image-20250715215920246</figcaption></figure><p>​  图5(a)详细说明了所提出的掩码自注意力机制及其对应的掩码设计，其中Q、K和V分别表示查询、键和值（为简洁起见，省略了归一化层和MLP层）。<spanclass="math inline">\(z^{in}\)</span>和<spanclass="math inline">\(z^{out}\)</span>分别表示输入和输出特征，<spanclass="math inline">\(\otimes\)</span>表示矩阵乘法运算符。掩码自注意力机制可表述为：<spanclass="math display">\[z^{out}=\mathrm{Mask}\left[\mathrm{softmax}\left(\frac{y_{qu e r y}(z^{i n})y_{k e y}(z^{i n})^{\top}}{\sqrt{d im}}\right)\right]y_{v a l u e}(z^{i n}),\]</span> ​  其中<spanclass="math inline">\(y_{q u e r y}(\cdot)\)</span>、<spanclass="math inline">\(y_{k e y}(\cdot)\)</span>和<spanclass="math inline">\(y_{v a l ue}(\cdot)\)</span>表示可学习参数，而<span class="math inline">\(y_{q u er y}(z^{i n})\)</span>、<span class="math inline">\(y_{k e y}(z^{in})\)</span>和<span class="math inline">\(y_{v a l u e}(z^{in})\)</span>分别对应Q、K和V。如图5(b)所示，我们采用光栅扫描掩码来建模全局像素依赖关系，这与输入图像的光栅扫描采样顺序相对应[71]。若将输入向量<spanclass="math inline">\(z^{i n}\in\mathbb{R}^{N\times d im}\)</span>定义为<span class="math inline">\(z^{i n}=[\bar{z}_{1}^{in},z_{2}^{i n},...,\ z_{N}^{i n}]^{\top}\)</span>，则每一行<spanclass="math inline">\(z^{in}\)</span>代表一个输入标记。像[14]，对于本文提出的掩码注意力机制输出的<spanclass="math inline">\(z^{o u t}\in\mathbb{R}^{N\times d im}\)</span>，每个输出标记<span class="math inline">\(z_{m}^{o ut}\)</span>可表示为： <span class="math display">\[z_{m}^{o ut}=\sum_{n\leq m}\gamma_{m n}y_{v a l u e}(z_{n}^{i n}),\]</span>​  其中，行m中的元素<span class="math inline">\(\gamma_{mn}\)</span>可表示为： <span class="math display">\[\gamma_{mn}=softmax[y_{k e y}(z_{1}^{i n})^{\top}y_{q u e r y}(z_{m}^{in}),\cdot\cdot\cdot,y_{k e y}(z_{m}^{i n})^{\top}y_{q u e r y}(z_{m}^{in})],\]</span> ​  在公式(3)中，我们可以清晰观察到每个输出标记<spanclass="math inline">\(z_{m}^{o u t}\)</span>都基于输入<spanclass="math inline">\(z_{n}^{i n}\,(n\leqm)\)</span>中区域的先前标记<spanclass="math inline">\(z^{in}\)</span>进行条件化处理，且扫描顺序遵循光栅扫描模式。这种机制还能有效建模现实场景中的复杂像素依赖关系，例如现代AI相机智能图像信号处理器所引入的依赖关系。因此，每个条件句都能通过注意力算子访问其上下文中的任意像素，这由对所有可用上下文的求和运算所体现，记作<spanclass="math inline">\(\sum_{n\leqm}\)</span>。<br/>​  该设计的模块能够访问远处像素，从而增强长距离统计建模。因此，提取的特征[fg1、fg2、fg3、fg4]可携带丰富的全局像素依赖信息。实验结果表明，真实图像与篡改图像之间的像素相关性特征具有显著差异，这为图像伪造定位提供了有效依据。</p><h2 id="c.局部像素依赖性建模">C.局部像素依赖性建模</h2><p>​  根据去马赛克算法的特性，给定像素的关联规律性主要取决于其邻近像素[10]、[11]。此外，像素规律性可通过线性去马赛克公式[10]、[55]进行建模。然而，这些传统方法在伪造检测性能方面存在局限性。受[59]、[82]、[101]等研究的启发，我们提出通过将传统去马赛克思路整合到卷积运算中，来构建局部像素依赖关系模型。<br/>​  在局部像素依赖编码器中，我们在每个transformer模块顶部部署差异卷积（DC）头，以学习方式建模局部图像区域的像素依赖关系。我们设计的差异卷积（DC）在标记层面执行，每个标记代表一个非常小的图像块。相较于处理单个像素，4×4图像块能提供更具表现力的表征来执行差异卷积。我们的方法显著降低了计算成本，同时有效捕捉局部图像区域的像素不一致性。此外，我们在每个transformer模块中采用MLP层，进一步增强各模块内部局部像素依赖关系的学习效果。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715222052521.png"alt="image-20250715222052521" /><figcaption aria-hidden="true">image-20250715222052521</figcaption></figure><p>​  图6(a)展示了设计的差分卷积（DC）头部架构。输入特征fl首先被前馈至两个差分卷积模块：中心差分卷积（CDC）和径向差分卷积（RDC）。通过利用CDC和RDC，可以有效建模局部像素依赖关系，从而提升最终伪造检测性能。图6(b)详细展示了CDC和RDC的工作原理：输入标记（即本地像素依赖编码器中transformer模块的输出）被重塑为二维特征fl。首先计算给定输入特征图在局部特征图区域内的差异，然后分别用对应的卷积权重对两个像素差异特征图进行卷积运算，最终生成CDC和RDC特征图。其中CDC操作可表示为：<spanclass="math display">\[f_{l}^{C}=\sum_{(x_{i},x_{c})\in\Omega^{C}}w_{i}^{C}(x_{i}-x_{c}).\]</span>​  其中，xc表示局部区域ΩC中的中心元素，xi则代表其对应的周边元素。如图6(b)所示，ΩC和ΩR中的每个元素都对应一个标记。wi_C值表示可学习的卷积权重。类似地，RDC操作可表示为：<spanclass="math display">\[f_{l}^{R}=\sum_{(x_{i},x_{i}^{\prime})\in\Omega^{R}}w_{i}^{R}(x_{i}-x_{i}^{\prime}),\]</span>​  其中xi和<spanclass="math inline">\(x_{i}^{\prime}\)</span>是区域ΩR中的元素对，如图6(b)所示。<br/>​  我们通过学习权重模块（LWM）将CDC特征flC与RDC特征flR进行互补组合，该模块将在第三节D部分详细阐述。本模型旨在提取局部像素依赖特征。相较于传统卷积网络，CDC和RDC通过差异运算的优势，能更充分地揭示像素不一致性伪影，从而显著提升图像伪造定位的最终性能。</p><h2 id="d.学习加权模块lwm">D.学习加权模块LWM</h2><p>​  如图6(a)所示，由CDC和RDC生成的特征fl C和flR被合并后依次输入到学习权重模块（LWM）。设计的LWM通过学习权重融合这两个输入特征，从而实现更有效的特征整合。图6(c)展示了局部CDC特征flC与RDC特征flR的学习权重过程，其中FC表示全连接层，EltMul代表逐元素乘法。在此过程中，拼接后的特征需先经过一个平均池化层和一个全连接层。随后，学习权重A1⊕A2通过逐元素乘法依次作用于拼接后的特征flC⊕flR。最终，通过将拼接后的特征与加权特征相加，即可获得融合后的特征fl。<br/>​  如图4所示，我们进一步采用局部权重混合技术（LWM）融合局部像素依赖特征[fl1，fl2，fl3，fl4]与全局像素依赖特征[fg1，fg2，fg3，fg4]。融合后的特征向量[f1，f2，f3，f4]随后被输入边界与伪造解码器，用于生成边界预测和伪造图谱。</p><h2 id="e.像素不一致性数据增强">E.像素不一致性数据增强</h2><p>​  现有方法[89]主要聚焦于发现伪造图像中的语义层面（或对象层面）不一致。部分方法[19]、[104]还提出通过随机粘贴原始真实图像中的物体来实现数据增强。然而，随着图像处理技术的不断进步，伪造内容的复杂程度也在同步提升。因此，原本用于捕捉语义层面不一致的方法在应对高级操作时难以有效泛化。我们提出了一种像素不一致性数据增强（PIDA）策略，旨在捕捉像素级不一致而非语义伪造痕迹。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715222613465.png"alt="image-20250715222613465" /><figcaption aria-hidden="true">image-20250715222613465</figcaption></figure><p>​  图7(a)展示了我们提出的PIDA流程。➀对于给定的真实原始图像Ip，我们通过施加图像扰动（如压缩、噪声和模糊处理）生成受损图像Ic；➁可以直接调用OpenCV内置函数提取Ip的前景掩膜M。融合模块以Ip、Ic和M为输入参数，生成自融合伪造图像Ib；➂操纵图像的边界标签B可通过M轻松推导得出。图7(b)详细展示了融合模块的工作原理：我们将源图像的前景与目标图像的背景相结合，最终生成自融合伪造样本。</p><h2 id="f.目标函数">F.目标函数</h2><p>​  整个框架采用端到端的方式进行训练，总体目标函数由以下四个部分组成：掩码预测损失LM、边界预测损失LB、紧凑性损失LC和图像重建损失LR：<spanclass="math display">\[L=L_{M}+\lambda_{B}L_{B}+\lambda_{C}L_{C}+\lambda_{R}L_{R},\]</span>​  其中，<span class="math inline">\(L_{M}\)</span>和<spanclass="math inline">\(L_{B}\)</span>分别表示预测结果与对应标签之间的交叉熵损失。边界损失<spanclass="math inline">\(L_{B}\)</span>可视为辅助监督机制，有助于提升伪造检测的定位精度。基于观察到大多数篡改区域较为紧凑的特性，我们进一步在预测掩膜上施加了紧凑性约束<spanclass="math inline">\(L_{C}\)</span>： <spanclass="math display">\[L_{C}={\frac{1}{N_{i m g}}}\sum_{i=1}^{N_{i mg}}\frac{P e r^{2}}{4\pi S}={\frac{1}{N_{i m g}}}\sum_{i=1}^{N_{i mg}}\frac{\sum_{j\in\hat{B}}{\hat{B}_{j}}^{2}}{4\pi(\sum_{k\,\in\hat{M}}|\hat{M}_{k}|+\epsilon)}.\]</span>​  在该公式中，P Peri和S分别表示预测伪造区域的周长和面积，Nimg代表图像数量。B̂和M̂则对应预测边界与掩膜。因此，式(8)的分子计算的是预测边界图B̂中像素值平方之和B̂j²，分母则与预测掩膜图M̂中像素绝对值之和M̂k成正比。此处被设定为极小数值。采用LC方法可使预测图像伪造图更加紧凑，并提升操作定位性能。<br/>​  图像重建损失LR计算重建图像ˆIi与对应输入图像Ii之间的差值的l1−范数：<spanclass="math display">\[L_{R}=\frac{1}{N}\sum_{i=1}^{N}||I_{i}-\hat{I}_{i}||_{1}.\]</span>​  通过使用LR，可以在[fg1、fg2、fg3、fg4]中对全局像素依赖性进行建模，该模型被用于LWMs中的伪造图和边界图预测。</p><h1 id="iv.实验与结果">IV.实验与结果</h1><p>​  本文首先介绍本研究涉及的数据集、评估指标及基线模型。随后，我们在不同实验场景下评估模型的泛化能力和鲁棒性，并通过可视化伪造定位结果来展示本方法的优势。最后，我们开展消融实验以验证各设计组件的有效性。</p><h2 id="a.数据集">A.数据集</h2><p>​  本研究选取了12个具有不同属性、分辨率和质量的图像处理数据集。如表I所示，其中CM（复制-移动）、SP（拼接）和IP（修复）分别代表三种常见的图像处理类型。与[19]、[81]、[103]等先前研究一致，我们选用包含超过12,000张多样化内容图像的CASIAv2[20]数据集作为训练集。此外，我们采用DEF-12kval[66]作为验证集，该数据集包含6000张具有三种伪造类型的挑战性假图像，以及从MS-COCO[57]数据集中采集的6000张真实图像。在测试阶段，我们选取了11个具有挑战性的数据集进行评估，包括哥伦比亚[39]、IFC[1]、CASIAv1+1 [20]、WildWeb [102]、COVER [91]、NIST2016 [33]、Carvalho[12]、Korus [51]、In-the-wild [42]、DEF-12k-test [66]和IMD2020[70]，并按发布时间排序。所有数据集中，我们都将伪造区域统一标记为‘1’，真实区域标记为‘0’。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715223559371.png"alt="image-20250715223559371" /><figcaption aria-hidden="true">image-20250715223559371</figcaption></figure><h2 id="b.评估指标">B.评估指标</h2><p>​  本文使用F1、MCC、IoU和AUC四个指标对最先进的模型的像素级伪造检测性能进行评估。</p><p>​  <strong>F1分数</strong><br/>​  F1分数是二元分类中常用的指标，广泛应用于图像伪造检测和定位领域。该指标通过计算精确率与召回率的调和平均值来实现：<span class="math display">\[F1=2\cdot\frac{P r e c i s i o n\times R ec a l l}{P r e c i s i o n + R e c a l l}=\frac{2\times T P}{2\times TP+F P+F N_{.}},\]</span> ​  其中，T P、T N、F P和FN分别表示真阳性、真阴性、假阳性和假阴性。</p><p>​  <strong>马修斯相关系数（MCC）</strong><br/>​  马修斯相关系数（MCC）衡量预测值与真实值之间的相关性，MCC的取值范围在-1到1之间，MCC值越大表示性能越好，其计算公式如下：<span class="math display">\[MCC=\frac{T P\times T N-F P\times FN}{\sqrt{(T P+F P)(T P+F N)(T N+F P)(T N+F N)}}.\]</span>​  <strong>交并比（IoU）</strong><br/>​  交并比（IoU）是语义分割领域中广泛应用的评估指标。该指标的分子表示预测区域P与真实区域G的交集面积，而分母则计算两者并集的面积：<span class="math display">\[I o U={\frac{P\cap G}{P\cup G}}.\]</span>​  <strong>曲线下面积（AUC）</strong><br/>​  曲线下面积（AUC）衡量接收者操作特征（ROC）曲线下的面积。与其他指标不同，AUC不需要选择阈值。它量化了模型在所有可能阈值下的整体性能。</p><h2 id="c.基准模型">C.基准模型</h2><p>​  本研究整合了来自顶级期刊和会议的16个代表性基准检测器，包含5种数据驱动架构和11种最先进图像伪造检测器。其目标是评估不同网络架构的检测性能并促进直接对比。基准测试包括三种普适性CNN架构（FCN[63]、U-Net [79]和DeepLabv3[13]）以及两个视觉模型：CASIAv1+和训练集CASIAv2共享782张相同的真实图像。为防止数据泄露，CASIAv1+用等量的COREL[88]图像替换了这些真实图像。此外，该基准还纳入了十种最先进的图像伪造检测模型：</p><p>​  MFCN[81]将图像拼接定位问题建模为多任务学习任务，通过采用双分支卷积神经网络VGG-16，实现伪造图与边界图的同步预测。</p><p>​  RRU-Net[8]是一种专为图像拼接检测设计的端到端环形残差U-Net架构。该架构通过残差传播机制有效解决了深度网络中的梯度扰动问题，从而显著增强了伪造线索的学习效果。</p><p>​  MantraNet[94]是一个端到端的图像伪造检测与定位框架，其训练数据集包含385种图像篡改类型。为实现强大的图像篡改检测能力，曼特拉网创新性地引入了一种长短期记忆解决方案，专门用于检测局部异常。</p><p>​  HPFCN[54]通过整合ResNet模块和一个可学习的高通滤波器，实现像素级的修复定位。</p><p>​  H-LSTM[5]是一种融合了CNN编码器和LSTM网络的伪造检测模型。这种组合使模型能够捕捉并分析伪造图像中的空间和频率域伪影。</p><p>​  SPAN[40]是一个构建金字塔注意力网络的框架，用于捕捉图像块在多个尺度上的相互依赖关系。它基于预训练的MantraNet构建，并提供了在特定训练集上微调参数的灵活性。</p><p>​  PSCC[61]是一种渐进式空间通道相关网络，通过密集的交叉连接在多个尺度上提取局部和全局特征。其渐进式学习机制使模型能够以从粗到细的方式预测伪造掩码，从而提升最终检测性能。</p><p>​  MVSS-Net++[19]设计了一个双流网络，利用多尺度特征捕捉边界和噪声伪影。通过结合两个流，该网络能够有效分析图像的不同方面，从而在像素级别和图像级别检测操作痕迹。</p><p>​  CAT-NET[52]是一种基于卷积神经网络（CNN）的模型，通过利用离散余弦变换（DCT）系数来捕捉图像处理过程中产生的JPEG压缩伪影。</p><p>​  EVP[60]提供了一个统一的低级结构检测框架，适用于图像处理。ViT适配器和视觉提示功能使EVP模型能够实现卓越的伪造定位精度。</p><p>​  TruFor 2[34]通过基于学习到的噪声敏感指纹的变压器融合架构，同时捕获高级RGB伪影和低级噪声伪造痕迹。</p><p>​  在本研究中，为了进行公平且可重复的比较，我们遵循MVSS-Net++[19]，选择满足以下三个标准之一的基线模型：(1)官方训练代码公开；(2)该模型采用与我们相同的训练协议，即使用CASIAv2作为训练数据集；(3)发布官方预训练模型。<br/>​  在测试阶段，我们遵循MVSS-Net++[19]和JPEG-SSDA[78]的协议规范，对训练完成的模型进行伪造图像检测，并针对所有测试数据集报告图像级检测结果。所选的篡改方法涵盖了多种伪造特征：边界伪影（MFCN[81]、MVSS-Net++[19]）、多尺度特征（PSCC[61]、MVSS-Net++ [19]、TruFor [34]）、高频伪影（HPFCN [54]、MVSS-Net++[19]、MantraNet [94]）以及压缩伪影（CATNET [52]）。</p><h2 id="d.实施细节">D.实施细节</h2><p>​  我们的模型基于PyTorch [74]框架开发，并在两块Quadro RTX 8000GPUs上进行训练。输入图像尺寸为512×512像素，采用Adam优化器（参数设置为[47]，β1=0.9，β2=0.999）进行训练，批量大小设为28，学习率和权重衰减分别为6×10<sup>-5和1×10</sup>-5。模型训练周期为20个epoch，每完成1600个全局步骤即进行验证。实验设置参照[19]方法，在CASIAv2[20]数据集上训练模型，并在DEF-12k-val[66]数据集上进行验证。除提出的像素不一致性数据增强技术外，我们还遵循[19]方法采用翻转、模糊、压缩、噪声、拼贴及修复等常规数据增强手段进行训练。</p><h2 id="e.跨数据集评估">E.跨数据集评估</h2><p>​  像素级检测：<br/>​  在伪造图像中定位被篡改区域至关重要，因为这能提供已被篡改区域的证据。预测的伪造区域可揭示攻击者的潜在意图[50]。然而，由于训练集和测试集之间的域差距很大，大多数检测器在跨数据集评估中都存在定位性能较差的问题。本研究通过像素级伪造检测（即操作定位）评估不同检测器的泛化能力。遵循[19]跨数据集评估协议，我们在CASIAv2[20]数据集上训练模型，并在DEF-12kval[66]数据集上验证模型。为便于全面解读结果，我们在表II和表III中报告了F1和IoU这两个图像伪造定位领域广泛应用的关键指标。附录（在线提供）还提供了AUC和MCC结果。我们用粗体突出最佳定位结果，并用下划线标注次优结果。与[19]中为每个模型和数据集单独确定最优阈值不同，我们统一将F1、MCC和IoU的默认决策阈值设为0.5，主要基于以下两点考虑：(1)在实际应用场景中，不可能为每个测试样本预先设定不同的最优阈值；(2)统一0.5作为决策阈值可确保所有基线模型的公平比较。不同阈值下的像素级评估结果详见附录（在线提供）。<br/>​  F1分数是该领域[20]、[77]、[78]、[97]中使用最广泛的度量标准。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715224517628.png"alt="image-20250715224517628" /><figcaption aria-hidden="true">image-20250715224517628</figcaption></figure><p>​  在表II中，我们的方法在六个数据集上取得了最佳检测F1分数，在两个数据集上表现次优。与当前最先进的TruFor[34]方法相比，我们提出的像素不一致性建模（PIM）方法在九个数据集上展现出更优的伪造定位F1分数，平均提升幅度达2.3%，从31.0%提升至33.3%。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715224751154.png"alt="image-20250715224751154" /><figcaption aria-hidden="true">image-20250715224751154</figcaption></figure><p>​  表III显示，我们的像素不一致性建模（PIM）方法在未见过的测试数据集中始终保持着最佳或次优的检测性能。尽管这11个未见过的数据集分布多样，但我们的方法平均交并比得分仍以显著优势超越所有先前方法。所提出方法的优越性可归因于其能够捕捉像素不一致伪影，这在不同的伪造数据集中是一个共同的指纹。</p><p>​  图像级评价：<br/>​  在本小节中，我们进一步通过跨数据集评估对图像级伪造检测进行验证。理想情况下，原始真实图像的篡改概率图应全部为零。为此，我们在篡改概率图上应用最大池化操作，并将生成的输出分数作为输入图像[78]的整体预测结果。表IV展示了关键指标F1分数，其中最佳结果以粗体标出，次优结果则用下划线标注。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715224812177.png"alt="image-20250715224812177" /><figcaption aria-hidden="true">image-20250715224812177</figcaption></figure><p>​  值得注意的是，我们的方法在NIST、CASIAv1+、DEF-12k、IMD、Carvalho、In-the-Wild、Korus和WildWeb等八个数据集中均取得前两名的图像级检测性能。即使在COVER数据集排名第六、IFC数据集排名第五的情况下，我们的方法仍能逼近最佳检测结果(COVER：我们的方法得分为655分，而最佳方法得分为671分；IFC：我们的方法得分为458分，而最佳方法得分为472分）。该方法展现出卓越的泛化能力，取得了最佳平均性能，充分证明了其出色的伪造检测能力。</p><h2 id="f.交叉操作评价">F.交叉操作评价</h2><p>​  为评估模型对未知修复技术的泛化能力，我们在CASIAv2数据集上训练模型，并在未知修复（IP）技术上进行测试。表V展示了10种修复技术的跨技术F1分数，交并比性能数据详见在线附录。这10个典型且具有挑战性的修复数据集包括CA[98]、EC [68]、GC [99]、LB [93]、LR [35]、NS [7]、PM [37]、RN [100]、SG[41]、SH [95]和TE[83]，这些数据集在以往修复检测研究中被广泛采用[92]。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715224924770.png"alt="image-20250715224924770" /><figcaption aria-hidden="true">image-20250715224924770</figcaption></figure><p>​  从表V可以看出，CAT-NET和TruFor得益于其海量训练数据和捕捉低级伪造特征的能力，在平均伪造定位性能方面表现亮眼。然而，我们提出的方法PIM在八个修复数据集上取得了最高F1值（平均0.649），以显著优势超越了先前方法。<br/>​  我们的方法之所以能有效应对不可预见的操控技术，主要得益于两大核心设计：(1)像素不一致性数据增强（PIDA）策略，该方案能让模型捕捉到更普遍且微妙的伪影特征，从而有效缓解训练过程中的过拟合问题；(2)精心设计的网络架构能同时捕捉全局与局部像素不一致性特征，使模型能够揭示更多本质性的像素级伪影，而非仅捕捉语义层面的痕迹。</p><h2 id="g.对复杂操作的泛化性">G.对复杂操作的泛化性</h2><p>​  为检验模型在复杂图像合成任务中的泛化能力，我们选取了Dall-E2（DE2）和StableDiffusion（SD）两个数据集进行测试。DE2包含60张高仿伪造图像，SD则包含328张。这些伪造图像具有高度协调性特征：伪造区域的光照效果一致、尺寸比例合理、语义内容连贯且位置精准。两套数据集的具体生成流程详见在线附录。表VI展示了针对未见过的复杂操控技术的伪造定位性能(F1、IoU、AUC和MCC分数）。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715225033165.png"alt="image-20250715225033165" /><figcaption aria-hidden="true">image-20250715225033165</figcaption></figure><p>​  虽然最先进的TruFor在这些指标上表现尚可，但我们的方法——像素不一致性建模（PIM）——在大多数指标上都超越了其他所有方法。在DE2和SD数据集上，PIM均取得了最高的F1值、交并比（IoU）和马修斯相关系数（MCC），这表明其在图像伪造定位中对复杂操作具有更强的泛化能力。PIM在应对先进AI生成内容技术（AIGC）制造的复杂伪造行为时表现出色，这说明该模型能有效适应未知且复杂的操控手段，为实际应用提供了可靠的解决方案。</p><h2 id="h.对高级操作的泛化性">H.对高级操作的泛化性</h2><p>​  随着AI生成图像（AIGC）技术的迅猛发展，伪造图像正变得愈发逼真，使用AIGC工具的门槛也大幅降低。因此，检测这些新兴的高级图像篡改手段至关重要。我们针对两个由先进AIGC方法生成的图像篡改数据集——Autosplice[45]和CocoGlide [69]进行了模型优化。其中，Autosplice[45]是由强大视觉语言模型生成的文本提示图像数据集，包含2,273张真实图像和3,621张经过处理的图像，每张伪造图像均采用三种JPEG压缩质量等级：75、90和100（数值越高表示图像质量越好）。CocoGlide包含512张基于COCO2017验证集生成的逼真伪造图像，这些图像采用文本引导的GLIDE扩散模型生成。表VII展示了图像伪造定位的AUC和MCC指标。</p><figure><imgsrc="../postimages/Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization/image-20250715225316624.png"alt="image-20250715225316624" /><figcaption aria-hidden="true">image-20250715225316624</figcaption></figure><p>​  我们的PIM方法在Autosplice 100、Autosplice90和CocoGlide数据集上持续保持最佳AUC和MCC表现。主流方法TruFor得益于其基于海量额外数据训练的Noiseprint++提取器，在低质量的Autosplice75数据集上取得最高分。尽管如此，PIM在所有先进AI生成图像数据集上均展现出更优的平均AUC和MCC指标。</p><h2 id="i.鲁棒性评价结果">I.鲁棒性评价结果</h2><h2 id="j.定性实验结果">J.定性实验结果</h2><h2 id="k.打乱图像的可视化结果">K.打乱图像的可视化结果</h2><h2 id="l.消融实验">L.消融实验</h2><h1 id="v.结论和今后的工作">V.结论和今后的工作</h1><p>​  本文提出了一种通过捕捉伪造图像中像素不一致性实现的通用型图像篡改定位模型。该方法基于双流像素依赖建模框架构建图像篡改定位系统，创新性地采用掩码自注意力机制有效建模输入图像中的全局像素依赖关系。同时，通过设计中央差分卷积（CDC）和径向差分卷积（RDC）两种定制化卷积模块，能更精准捕捉局部区域内的像素不一致性特征。研究发现，通过建模像素间关联关系可有效挖掘图像篡改的内在线索。为提升整体性能，学习加权模块（LWM）将全局与局部特征互补融合。动态权重方案的应用实现了更优的特征融合效果，从而构建出更具鲁棒性和普适性的图像篡改定位模型。<br/>​  此外，我们提出了一种新型像素不一致性数据增强（PIDA）技术，该技术专门利用原始图像生成增强伪造样本，将研究重点聚焦于像素级伪影。所提出的PIDA策略为未来取证研究的泛化能力提升提供了重要启示。大量实验结果表明，该框架在图像篡改检测与定位方面展现出最先进水平，在泛化能力和鲁棒性评估中均表现优异。我们设计的模型在未见过的、高级且复杂的篡改图像上也表现出色，充分证明了其在复杂真实场景中的应用潜力。消融实验进一步验证了各组件的有效性。<br/>​  尽管我们的方法对未见过的图像扰动具有鲁棒性，但仍存在易受重捕获攻击的缺陷。这一漏洞源于该框架的核心目标：识别因处理过程中破坏CFA规律性而产生的像素不一致伪影。重捕获操作会重新引入去马赛克化过程中建立的像素依赖关系，掩盖了这些伪影，导致伪造检测失败。未来研究中，开发有效的重捕获检测模块将成为关键方向，以确保更安全的篡改检测。</p><p>（以下仅代表个人观点：首先作为发表在IEEE Transactions on PatternAnalysis and Machine Intelligence（TPAMI）2025的文章，只与2023的方法对比，其次比较论文实验的结果和原本论文在相同数据集相同指标下的结果相差太多，对实验结果持怀疑态度，对此文的阅读暂且搁置）</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>M2SFormer:Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization</title>
      <link href="/M2SFormer/"/>
      <url>/M2SFormer/</url>
      
        <content type="html"><![CDATA[<p>M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-AwareDifficulty Guidance for Image Forgery Localization</p><p>Ju-Hyeon Nam1 ，Dong-Hyun Moon1 ，Sang-ChulLee1,2<br/>1仁荷大学电子与计算机工程系<br/>2DeepCardio</p><h1 id="摘要">摘要</h1><p>​  图像处理技术发展迅猛，既催生了创新应用场景，也催生了数字图像的恶意篡改。基于深度学习的方法在像素级伪造定位方面已取得高精度，但在计算开销和表征能力方面仍存在局限，尤其在应对细微或复杂的篡改时表现欠佳。本文提出M2SFormer，这是一种基于Transformer编码器的创新框架，旨在攻克现有技术难题。与传统方法分别处理空间和频率线索不同，M2SFormer通过在跳跃连接中统一多频段和多尺度注意力机制，借助全局上下文信息，能更精准捕捉各类伪造特征。此外，我们的框架通过采用全局先验图（一种反映伪造检测难度的曲率度量指标）来解决上采样过程中细节丢失的问题。该指标能引导难度导向注意力模块更有效地保留细微操作痕迹。在多个基准数据集上的大量实验表明，M2SFormer模型不仅超越了现有最先进模型，在跨领域检测和定位伪造行为时展现出更强的泛化能力。</p><figure><img src="../postimages/M2SFormer/image-20250715212113564.png"alt="image-20250715212113564" /><figcaption aria-hidden="true">image-20250715212113564</figcaption></figure><h1 id="引言">引言</h1><p>​  我们能相信媒体上的信息吗？<br/>​  随着图像编辑技术的飞速发展，数字图像处理能力日益精进，既催生了创意应用，也滋生了恶意篡改[32,36,40,70]。这种现象引发重大社会关切——伪造图像可能导致虚假信息传播、法律纠纷频发、公众信任危机加剧，进而助长社会不稳定[2,49,63]。因此，图像伪造检测技术备受瞩目。早期方法主要采用基于基础手工特征的传统手段，例如JPEG压缩痕迹[54,71]、传感器模式噪声（SPN）[10,35]以及色彩滤光阵列（CFA）插值模式[12,16]。然而这些方法主要聚焦于图像级检测（二元分类），对新型伪造类型存在泛化能力不足的问题[28,67]。<br/>​  随着深度学习时代的到来，像素级伪造检测（二值分割）技术已取得显著进展，曼陀罗网络[67]和SPAN[28]等模型在自动提取伪造痕迹方面表现优异。然而，尽管这些方法成效显著，但其计算成本高昂且表征能力不足，导致对新型伪造类型难以有效泛化[8,77]。近年来，注意力机制的引入[11,27,66]缓解了部分挑战，提升了基于UNet架构编码器-解码器模型的伪造检测效率与性能[23,43,77]。不过，当伪造内容较为隐蔽或与原图高度相似时，现有方法仍存在鉴别困难的问题，导致实际应用场景中的检测效果有所下降[18,41,69]。<br/>​  近年来，频域方法在图像分类[9,38,56,75]、语义分割[4,37,72]和目标检测[39,78]等多个领域得到广泛应用，旨在提升模型对未知领域的泛化能力和抗干扰能力。值得注意的是，相较于依赖空间域线索进行伪造定位的方案，频域信号分析往往能更有效地揭示细微的篡改痕迹[18,21,42,47,55,69]。然而现有研究因效率考量，通常将空间域与频域割裂处理，未能构建统一框架。这导致目前缺乏能同时整合空间与频率信息的统一式注意力机制，主要源于对计算效率的顾虑。由此引出一个关键研究问题：“如何在高效整合空间与频率注意力的同时，有效捕捉细微的伪造特征？”<br/>​  为解决上述问题，我们提出了一种新颖的频率-空间统一注意力机制——在跳跃连接中引入多光谱-多空间（M2S，<em>SpectralandMulti-Spatial</em>）注意力模块，用于伪造检测定位。我们研发的M2S注意力模块包含两大核心机制：频率域与空间域。在频率域方面，我们采用二维离散余弦变换（2DDCT）[1]的基图像生成通道级注意力图。通过选择性地加权相关频率分量，该方法既能精准提取关键频谱信息，又能从多光谱视角完整保留空间上下文特征。在空间域处理方面，我们采用受SIFT启发的特征金字塔结构，精准捕捉剪切粘贴操作产生的细微异常边界线索。通过逐层应用空间注意力机制，有效突显这些伪造特征。同时，我们对每个输入样本进行量化评估，并开发出融合难度引导注意力（DGA）与Transformer模块的解码器——该解码器将难度等级转化为文本表征，并结合通道级注意力机制，显著提升模型应对复杂样本的能力。通过将M2S注意力模块与基于边缘感知DGA的Transformer解码器相结合，我们提出了一种名为M2SFormer的Transformer架构——这是一种新型伪造检测模型，能够充分利用全局依赖关系，并能稳定捕捉不同操作类型、形状和尺寸下的伪造掩码。在多个公开基准数据集上的大量实验表明，M2SFormer不仅超越了现有模型，在跨领域伪造检测中显著提升了泛化性能，还具有更强的适应性。此外，我们在附录（第8节）中详细阐述了M2SFormer在人工智能领域的应用价值。本文的贡献可归纳如下：</p><ul><li>我们提出了一种基于Transformer的新型伪造定位框架M2SFormer，该框架将M2S注意力模块与边缘感知的DGAbasedTransformer解码器高效集成，相比现有方法显著提升了伪造定位性能。</li><li>通过在跳跃连接中整合多光谱与多尺度注意力机制，能更精准捕捉伪造痕迹。此外，全局先验图可量化伪造定位难度，在上采样后引导难度导向注意力模块，从而在复杂区域保留细节特征。</li><li>在多个基准数据集上的大量实验表明，M2SFormer优于现有模型，在跨未见领域的伪造定位中显著提高了泛化性能。</li></ul><p>​  为评估各模型性能，我们选取了图像分割领域常用的三个指标：Dice分数系数（DSC）[48]和平均交并比（mIoU）。</p><figure><img src="../postimages/M2SFormer/image-20250726215243569.png"alt="image-20250726215243569" /><figcaption aria-hidden="true">image-20250726215243569</figcaption></figure><figure><img src="../postimages/M2SFormer/image-20250726215303108.png"alt="image-20250726215303108" /><figcaption aria-hidden="true">image-20250726215303108</figcaption></figure><p>​  </p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 边缘感知 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ImgEdit</title>
      <link href="/ImgEdit/"/>
      <url>/ImgEdit/</url>
      
        <content type="html"><![CDATA[<p>ImgEdit: A Unified Image Editing Dataset and Benchmark</p><p>Yang Ye1,3,<em>， Xianyi He1,3,</em>， Zongjian Li1,3,<em>， BinLin1,3,</em>， Shenghai Yuan1,3,<em>， Zhiyuan Yan1,</em>， Bohan Hou1，Li Yuan1,2,†，</p><p>1北京大学深圳研究生院<br/>2 程鹏实验室<br/>3 Rabbitpre AI</p><h1 id="摘要">摘要</h1><p>​  近年来生成模型的突破性进展，使得高保真度的文本转图像技术得以实现。但开源图像编辑模型仍难以与商业版一较高下，这主要源于其优质数据资源匮乏和测试平台建设不足。为克服这些限制，我们引入了ImgEdit，这是一个大规模、高质量的图像编辑数据集，包含120万个精心策划的编辑对，其中既包含新颖和复杂的单回合编辑，也包含具有挑战性的多回合任务。为确保数据质量，我们采用多阶段处理流程，整合了前沿的视觉语言模型、检测模型、分割模型，以及针对具体任务的修复流程和严格的后处理步骤。ImgEdit在任务创新性和数据质量方面均超越现有数据集。通过使用ImgEdit，我们训练出ImgEdit-E1，一个利用视觉语言模型处理参考图像和编辑提示的编辑模型，在多项任务中表现优于现有开源模型，充分彰显了ImgEdit及其模型设计的价值。为实现全面评估，我们推出了ImgEdit基准测试平台。该平台通过指令遵循度、编辑质量与细节保留三个维度，系统评估图像编辑性能。平台包含基础测试套件、高难度单轮测试套件以及专用多轮测试套件三大模块。我们不仅对开源模型和专有模型进行评估，还特别针对ImgEdit-E1版本展开深度分析，为当前图像编辑模型的运行机制提供可操作性的洞见。</p><p>源数据可在https://github.com/PKU-YuanGroup/ImgEdit上公开获取。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge_Distillation</title>
      <link href="/Knowledge-Distillation/"/>
      <url>/Knowledge-Distillation/</url>
      
        <content type="html"><![CDATA[<h1 id="通用的知识蒸馏方法">通用的知识蒸馏方法</h1><p><strong>第一步</strong>是训练Net-T；<br/><strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S</p><figure><imgsrc="../postimages/Knowledge-Distillation/knowledge_distillation.png"alt="Knowledge Distillation" /><figcaption aria-hidden="true">Knowledge Distillation</figcaption></figure><p>https://intellabs.github.io/distiller/knowledge_distillation.html#hinton-et-al-2015</p><p>高温蒸馏过程的目标函数由distill loss(对应soft target)和studentloss(对应hard target)加权得到。 <span class="math display">\[L=\alphaL_{s o f t}+\beta L_{h a r d}\\\]</span> 其中 <spanclass="math display">\[{\cal L}_{s o f t}=-\sum_{j}^{N}\p_{j}^{T}\log(q_{j}^{T})\\p_{i}^{T}\,=\,\frac{\exp(v_{i}/T)}{\sum_{k}^{N}\exp(v_{k}/T)}\,\,,\,q_{i}^{T}\,=\,\frac{\exp(z_{i}/T)}{\sum_{k}^{N}\exp(z_{k}/T)}\]</span>随后： <span class="math display">\[{\cal L}_{h a rd}=-\sum_{j}^{N}c_{j}\log(q_{j}^{1})\\q_{i}^{1}\,=\,\frac{\exp(z_{i})}{\sum_{k}^{N}\exp(z_{k})}\]</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection</title>
      <link href="/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/"/>
      <url>/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/</url>
      
        <content type="html"><![CDATA[<p>Unlocking the Capabilities of Large Vision-Language Models forGeneralizable and Explainable Deepfake Detection</p><p>Peipeng Yu 1 Jianwei Fei 2 Hui Gao 1 Xuan Feng 1 Zhihua Xia 1Chip-Hong Chang 3</p><p>1 济南大学网络安全学院<br/>2 澳门大学<br/>3南洋理工大学电气与电子工程学院</p><h1 id="摘要">摘要</h1><p>​  目前的大型视觉语言模型（LVLMs）在理解多模态数据方面表现出显著的能力，但由于其知识和取证模式的错位，它们在深度造假检测方面的潜力仍未得到充分探索。为此，我们提出了一种新框架，旨在挖掘语言模型在深度伪造检测中的潜力。该框架包含知识引导的伪造检测器（KFD,Knowledge-guided Forgery Detector）、伪造提示学习器（FPL, Forgery PromptLearner）和大型语言模型（LLM, Large LanguageModel）。KFD用于计算图像特征与原始或深度伪造图像描述嵌入之间的相关性，从而实现伪造的分类和定位。</p><h1 id="引言">1.引言</h1><p>​  生成式人工智能的快速发展显著加速了深度造假技术的发展，促进了逼真的面部操作和重演。尽管这些技术在娱乐和艺术领域有着显著的应用，例如稳定扩散（Esser等，2024）和DALL·E（Ramesh等，2021)，但它们的滥用对社会构成了严重的安全威胁（Wang等，2024b）。这些工具允许用户通过输入精心设计的提示，合成出逼真但不存在的内容，使得深度伪造生成比以往任何时候都更加容易获取且潜在危险。</p><p>​  大型视觉-语言模型（LVLMs）为解决这一问题提供了有希望的解决方案。这些模型在广泛多样的数据集上进行了预训练，能够捕捉大量关于自然物体的知识，从而显著提高识别被篡改内容的泛化能力。通常，LVLM会使用图像编码器提取图像特征，然后将这些特征与文本提示结合，输入到大型语言模型（LLM）中生成响应。例如，输入一张面部图像，并附带这样的提示：“这是一张用于深度伪造检测的面部图像，不应出现局部颜色差异或明显的拼接痕迹。这是一张深度伪造图像吗？”LVLM可以评估图像是否可能被篡改。然而，现有的LVLM主要针对通用图像理解任务进行了优化，可能无法有效捕捉深度伪造检测所需的细节特征。直接进行微调存在挑战，因为LVLM在处理“颜色差异”或“视觉伪影”等专业术语时，可能难以准确地解释这些术语在伪造检测中的含义。因此，设计一个能够更好地理解这些术语的模型至关重要。</p><p>​  本文旨在利用大型视觉-语言模型的能力，解决深度伪造检测任务。人们通常通过特定的描述符，如细微的视觉瑕疵、局部光照不一致和过度平滑的纹理，来识别被篡改的内容。然而，这些特征难以仅通过数据模拟或特征增强准确复制，这限制了现有方法对被篡改内容的全面解读(Zhanget al.,2024)。为了解决这一局限性，我们提出探索图像与文本描述之间的校正，以辅助深度伪造检测。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707192635895.png"alt="image-20250707192635895" /><figcaption aria-hidden="true">image-20250707192635895</figcaption></figure><p>​  如图1所示，我们提出了一种基于预训练知识的新型LVLM深度伪造检测框架，该框架能够实现通用且可解释的检测(Lesteret al., 2021; Liu et al.,2022)。我们首先整合外部知识来训练一个伪造检测器，然后将该检测器的特征融入语言模型（LLM）中以生成响应。<br/>​  具体来说，该框架包含两个关键阶段：知识引导的伪造检测器训练和语言模型提示调优。<br/>​  在知识引导的伪造检测器训练阶段，我们的目标是训练出一个高精度的深度伪造检测器。利用预训练的多模态编码器，我们从图像中提取图像特征，并从原始图像和伪造图像的描述中提取文本特征。通过计算这些图像特征与描述文本嵌入之间的相关性，我们生成了一致性图，这些图展示了视觉内容与文本描述之间的对齐情况。这些地图随后由伪造定位器和分类器处理，生成伪造分割图和伪造评分。<br/>​  在语言模型提示调优阶段，我们将深度伪造检测的知识融入语言模型中，生成伪造检测描述。为了确保深度伪造检测的准确性，我们使用专门为本任务定制的模拟伪造图像-文本对来训练语言模型。我们的贡献总结如下：</p><ul><li>我们提出了一种基于LVLM的深度伪造检测新框架，通过提示调优集成细粒度伪造提示嵌入，显著增强了模型的泛化能力和可解释性。</li><li>我们引入了一种知识引导的伪造检测器，生成伪造一致性图，将原始图像和深度伪造图像的描述文本嵌入与图像特征对齐，以增强泛化能力。</li><li>在FF++、CDF1、CDF2、DFD、DFDCP、DFDC和DF40等多个基准上的大量实验表明，我们的方案在泛化性能上优于现有方法，并且具有支持多轮对话的能力。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="深度造假检测方法">2.1深度造假检测方法</h2><p>​  传统的分类架构在早期深度伪造内容的伪造线索检测方面取得了显著成效。近年来，为了提高深度伪造检测模型的泛化能力，研究者们探索了多种策略，包括数据增强(Liet al., 2020a; Shiohara &amp; Yamasaki, 2022; Nguyen et al.,2024)、特征一致性分析 (Zhao et al., 2021b; Yan et al.,2023)以及频域异常检测(Jeong et al., 2022; Liu et al.,2021; Wang et al.,2023a),。尽管这些方法提高了检测精度，但它们主要依赖于数据增强或增强特征提取（Yan等，2024），并且往往忽略了外部人类知识的整合。然而，许多深度伪造的特征嵌入在人类知识中，仅靠数据或特征增强难以捕捉这些特征。这一局限性显著限制了现有算法的泛化能力。本文提出了一种基于LVLM的深度伪造检测框架，该框架通过将图像特征与真实/伪造描述对齐，增强了模型检测未知深度伪造的能力。</p><h2 id="大型视觉语言模型">2.2大型视觉语言模型</h2><p>​  近期，大型视觉-语言模型（LVLMs）在多模态任务中的应用潜力得到了显著展示（Gunjal等，2024；Leng等，2024；Gu等，2024）。典型的LVLM架构包括图像编码器、投影器和语言模型（LLM）。图像编码器从输入图像中提取视觉特征，这些特征随后通过投影器转换为视觉提示嵌入。这些视觉嵌入与文本提示嵌入结合，输入到语言模型中生成响应。基于这一架构，诸如BLIP-2（李等，2023)、LLaVA（刘等，2024a)和MiniGPT-4（朱等，2024)等模型，在自然场景的语言教学（苏等，2023；杨等，2024)和视觉推理（陈等，2024)方面取得了显著进展。一些研究还探讨了低维语言模型（LVLMs）在伪造检测中的应用。FakeShield（徐等，2024)构建了一个大规模的图像-文本数据集，并引入了一种专门用于伪造检测的基于LVLM的框架。FKA-Owl（刘等，2024b)提出了一种新的假新闻检测框架，该框架利用特定于伪造的知识来增强LVLMs，使其能够对操纵行为进行推理。同样，FFAA（黄等，2024)提出了一种多模态LVLM方法，用于可解释的、开放世界的面部伪造分析，突显了LVLMs在伪造检测任务中的潜力。</p><p>​  尽管取得了这些进展，当前的基于语言模型的深度伪造检测方法（LVLMs）主要集中在通用语言处理和视觉理解上，往往忽略了对深度伪造检测任务至关重要的细节。这一局限性限制了它们在伪造定位和分类上的效果。为了解决这一问题，我们开发了一种新的深度伪造检测框架，该框架基于语言模型，通过构建细粒度的伪造提示嵌入来指导语言模型检测细微的篡改行为。通过在预训练的语言模型中整合丰富的外部知识，我们的方案不仅增强了模型对各种伪造类型的泛化能力，还保留了其原有的对话功能。</p><h1 id="拟提出的方法">3.拟提出的方法</h1><p>​  我们的目标是使低维线性模型（LVLMs）能够准确地区分真实与伪造的面孔。尽管LVLMs是在大规模数据集上训练的，但它们主要针对通用图像理解任务，往往缺乏对伪造细节的敏感度。为了解决这一问题，我们提出了一种基于LVLM的新深度伪造检测框架，通过构建精细的伪造提示，提高了对深度伪造伪影的敏感度。如图2所示，我们的方案基于传统的语言模型框架，该框架包括图像编码器、投影器和语言模型。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png"alt="image-20250707193258253" /><figcaption aria-hidden="true">image-20250707193258253</figcaption></figure><p>​  图像编码器从输入图像中提取内容特征，这些特征随后由投影器转换为视觉提示嵌入<spanclass="math inline">\(E_{visual}\)</span>。此外，用户查询被编码为问题提示嵌入<spanclass="math inline">\(E_{question}\)</span>。为了训练用于伪造检测的模型，我们采用了两阶段流程。<br/>​  在第一阶段，我们训练了一个知识引导型伪造检测器（Knowledge-GuidedforgeryDetector，简称KFD），通过计算图像内容特征与原图/深度伪造图像描述之间的相关性来进行伪造检测和定位。这一阶段确保KFD能够通过学习精细的视觉-文本关联，有效分类和定位伪造物。<br/>​  在第二阶段，我们通过LLM提示调优，将KFD的知识整合到LVLM框架中。具体来说，我们设计了一个伪造提示学习器，用于将与伪造相关的特征转换为伪造提示嵌入。这些嵌入，连同问题和视觉提示的嵌入，随后被输入到LLM中，以生成文本检测结果。<br/>​  为了进一步提高模型的可解释性，我们采用了交替训练策略，同时使用深度伪造数据集和通用视觉问答（VQA,Visual QuestionAnswering）数据集。这使得模型不仅能够准确检测深度伪造，还能保持多轮对话的能力。</p><h2 id="知识引导型伪造检测器">3.1.知识引导型伪造检测器</h2><p>​  <strong>伪造的视觉文本对齐</strong><br/>​  为了获取伪造检测的相关知识，我们受到（Jeong等，2023）的启发，将图像内容特征与预定义的文本描述特征进行对齐，以获得细粒度的伪造特征。这一过程如图3所示。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193606348.png"alt="image-20250707193606348" /><figcaption aria-hidden="true">image-20250707193606348</figcaption></figure><p>​  具体来说，该过程涉及一个预训练的图像编码器和一个预训练的文本编码器。图像和文本编码器均来自ImageBind（Girdhar等，2023），这是一个大规模的多模态预训练模型，拥有广泛的跨模态知识。我们首先定义了真实图像描述（<spanclass="math inline">\(D_{real}\)</span>）和虚假图像描述（<spanclass="math inline">\(D_{fake}\)</span>），并使用文本编码器提取它们的语义特征。这些特征与一个可学习的嵌入层连接，生成特定任务的文本嵌入<spanclass="math inline">\(F_{t e x t}\in\mathbb{R}^{2\times C_{t e xt}}\)</span>，其中<spanclass="math inline">\(C_{text}\)</span>表示文本嵌入的维度。对于视觉特征，我们从图像编码器中选取l层，获取每层提取的中间特征。这些中间特征通过线性层处理后，生成视觉特征<spanclass="math inline">\(F_{v i s}^{i}\in\mathbb{R}^{H_{i}\timesW_{i}\times C_{t e xt}}\)</span>，其中i表示第i层。计算视觉特征与文本特征之间的相似度图，并将这些相似度图连接起来形成一致性图。计算一致性图的公式如下：<span class="math display">\[\rho_{t e x t}=\{F_{v i s}^{i}F_{t e xt}^{\mp}\}.\]</span>​  此外，为了优化提取的图像特征，我们计算了参考图像（原始图像）<spanclass="math inline">\(F_{v r e f}^{i}\)</span>与输入图像特征<spanclass="math inline">\(F_{v is}^{i}\)</span>之间的余弦相似度。这种相似度优化提高了图像编码器提取特征的鲁棒性。需要注意的是，参考图像仅在训练阶段使用，在推理阶段则不使用。相似度计算如下：<span class="math display">\[\rho_{r e f}=\{C o s(F_{v r e f}^{i},F_{v is}^{i})\}.\]</span>​  <strong>伪造定位器和分类器</strong><br/>​  为了提高模型对深度伪造内容的敏感度，我们引入了伪造定位器和伪造分类器，用于识别伪造区域并区分原始图像和深度伪造图像。伪造定位器由三个分支组成，每个分支分别对相应的连贯图进行下采样和上采样处理，随后通过插值、连接和卷积层生成分割图。伪造分类器同样包含三个分支。首先，三个特征图通过卷积和池化操作进行处理，然后将这些特征图连接起来，形成统一的特征表示。接下来，我们使用两个全连接层来计算图像的真实或伪造概率。为了提高伪造分割的准确性，我们采用了Dice损失函数。此外，我们还通过优化文本一致性图（<spanclass="math inline">\(\rho_{t e x t}\)</span>）与参考一致性图（<spanclass="math inline">\(\rho_{r ef}\)</span>）之间的匹配度，进一步增强了提取的伪造特征的鲁棒性。这两个图都旨在准确地定位伪造区域。定位损失的公式如下：<span class="math display">\[\mathcal{L}_{l o c}=D i ce\big(\phi(\rho_{t e x t}),g t\big)+\lambda D i c e\big(\phi(\rho_{r ef}),g t\big),\]</span> ​  其中，<spanclass="math inline">\(\phi\)</span>代表伪造定位器，gt代表真实掩码。Dice损失用于优化预测分割与真实掩码之间的重叠度。λ是用于平衡这两种损失的权重参数。<br/>​  此外，我们采用二元交叉熵损失来优化伪造分类任务的性能，其公式如下：<span class="math display">\[\mathcal{L}_{c ls}=-\left(c\log({\hat{c}})+(1-c)\log(1-{\hat{c}})\right),\]</span>​  其中<spanclass="math inline">\(\hat{c}\)</span>是预测的伪造分数，表示该图像是否为假；c是真实标签（0代表真实，1代表伪造）。</p><h2 id="伪造提示学习和llm">3.2.伪造提示学习和LLM</h2><p>​  <strong>伪造提示学习</strong><br/>​  为了有效将伪造特征转化为语言模型的输入，我们提出了一种伪造提示学习器，该学习器能够将伪造分割图、伪造分数和一致性图转换为伪造提示嵌入。同时，我们还为伪造提示学习器添加了可学习的提示嵌入，以在深度伪造检测任务中融入额外信息。伪造提示学习器由两个卷积神经网络、一个全连接层以及可学习的提示嵌入<spanclass="math inline">\(E_{b a s e}\in\mathbb{R}^{n_1\times C_{e mb}}\)</span>组成，其中<span class="math inline">\(C_{e mb}\)</span>表示嵌入向量的维度。具体来说，两个卷积网络将伪造分割图和一致性图转换为向量表示，分别为<spanclass="math inline">\(E_{l o c}\in\mathbb{R}^{n_2\times C_{e mb}}\)</span>和<span class="math inline">\(E_{c o ns}\in\mathbb{R}^{n_3\times C_{e m b}}\)</span>。伪造得分扩展为<spanclass="math inline">\(E_{c l s}\in\mathbb{R}^{1\times C_{e mb}}\)</span>。这些嵌入被连接并输入到卷积层，生成伪造提示嵌入<spanclass="math inline">\(E_{forgery}\in\mathbb{R}^{n_f\times C_{e mb}}\)</span>。最后，伪造提示嵌入、视觉提示嵌入和问题提示嵌入被输入到语言模型中。</p><p>​  <strong>LLM</strong><br/>​  LLM通过处理提示嵌入来解读上下文，并准确识别伪造区域。通过结合视觉细节（来自<spanclass="math inline">\(E_{forgery}\)</span>和<spanclass="math inline">\(E_{visual}\)</span>)与用户查询（来自<spanclass="math inline">\(E_{question}\)</span>)，LLM生成的响应不仅提供了伪造检测的判断，还能精确定位被篡改的区域（如眼睛、嘴巴）。在此过程中，我们利用提示调优和LoRA技术，使用专门为深度伪造检测任务定制的模拟图像-文本对来微调LLM。为了确保LLM生成的响应准确无误，我们采用交叉熵损失来衡量预测响应与目标标签之间的差异。公式如下：<span class="math display">\[\mathcal{L}_{l lm}=-\sum_{j}y_{j}\log(\hat{y_{j}}),\]</span> ​  其中，<spanclass="math inline">\(\hat{y_{j}}\)</span>表示第j个标记的预测概率，而<spanclass="math inline">\(y_{j}\)</span>则是对应的真值标签。</p><h2 id="llm提示调优的数据">3.3.LLM提示调优的数据</h2><p>​  <strong>伪造数据模拟</strong><br/>​  我们计划让LLM能够识别原始图像和深度伪造图像，并且还能定位伪造区域。这需要对专门描绘被篡改区域的图像-文本对进行训练，但目前这类数据尚不可用。为了解决这一问题，我们借鉴了SBI（Shiohara&amp;Yamasaki，2022）的技术，利用现有的真实图像构建图像-文本对。首先，我们从真实图像<spanclass="math inline">\(I_{real}\)</span>中生成面部特征点，然后随机选择1到n个区域（如鼻子、嘴巴或眼睛）作为目标伪造区域。接着，我们对真实图像应用轻微的仿射变换，生成仿射变换后的图像<spanclass="math inline">\(I_{affine}\)</span>。原始的真实图像作为背景（目标面部），而仿射变换后的图像则作为前景（源面部）。根据Nguyen等人（2024）的方法，我们使用泊松混合技术将前景和背景图像结合在一起。混合过程如下：<span class="math display">\[{\bf I}_{M}={\bf M}\odot{\bf I}_{a f f i ne}+{\bf(1-M)\odot I_{r e a l}}\]</span>​  其中M是基于选定伪造区域构建的凸包掩膜，其取值范围为0到1。符号<spanclass="math inline">\(\odot\)</span>表示元素级乘法。</p><p>​  <strong>问答内容</strong><br/>​  训练LVLM需要大量的视觉问题与答案对。因此，我们为每张图像构建了相应的文本查询。为了确保与深度伪造检测任务的兼容性，我们在每个查询中加入背景描述，例如：“这是一张专为深度伪造检测设计的面部图像，不应出现局部颜色差异或明显的拼接痕迹。”这可以视为一种人类先验知识。此外，我们将知识引导伪造检测器（KFD）的预测结果融入提示中，例如：“根据KFD预测，伪造分数为0.93。”然后，我们会提出一个与图像内容相关的问题，例如：“这是一张深度伪造图像吗？”LVLM的响应会指出图像中是否存在伪造，并指出伪造的具体位置。例如，“是的，这是一张深度伪造的图片，伪造区域位于图像的中心面部。”在这里，伪造区域是根据在伪造数据模拟过程中选定的区域来定义的。通过定义查询和响应，我们可以训练LVLM来区分原始图像和深度伪造图像。输入到LVLM的提示格式如下：</p><p>​  <span class="math inline">\(\#\#\#Human:&lt;Img&gt;E_{visual}&lt;/Img&gt;E_{f o r g e r y}[Task description][KFDresult] Is this a deepfakeimage?&lt;br/&gt;\#\#\#Assistant:,\)</span></p><p>​  其中，<spanclass="math inline">\(E_{visual}\)</span>表示视觉提示嵌入，<spanclass="math inline">\(E_{forgery}\)</span>表示伪造提示学习器学习到的伪造提示嵌入，KFD结果表示伪造分类结果，Taskdescription提供了深度伪造检测任务的文本描述。</p><h1 id="实验">4.实验</h1><h2 id="实验设置">4.1.实验设置</h2><p>​  数据集。FaceForensics++（FF++）数据集（Rossler等人，2019）包含1000段真实视频和5000段伪造视频，涵盖五类深度伪造类别，是深度伪造检测领域应用最广泛的基准数据集之一。DFD（Dufour与Gully，2019）、CDF1、CDF2（Li等人，2020b）、DFDCP（Dolhansky，2019）、DFDC（Dolhansky等人，2020）以及DF40(Yan等人）等常用数据集，常用于评估深度伪造检测的泛化性能。所有图像均通过Dlib和RetinaFace进行裁剪处理。本研究仅使用FF++数据集的真实数据进行训练。</p><p>​  评估指标。根据现有方法（Shiohara &amp;Yamasaki，2022；Nguyen等，2024），我们采用视频级别的接收者操作特征曲线下的面积（AUC）和平均精度（AP）作为评估指标。此外，我们通过评估语言模型（LLM）文本输出的真实性（是或否）的二分类结果，来评估其性能，从而计算出LLM响应的视频级别AUC。</p><p>​  比较方法。我们根据几种最先进的深度伪造检测算法评估我们的方法(Rossleret al., 2019; Li et al., 2020a; Qian et al., 2020; Zhao et al.,2021a;Liu et al., 2021; Zhao et al., 2021b; Cao et al., 2022;Shiohara &amp;Yamasaki, 2022; Wang et al., 2023a;b; Dong et al., 2023; Yan et al.,2023; Xu et al., 2023; Yan et al.,2024; Nguyen et al., 2024; Cheng etal., 2024; Lin et al.,2025; Luo et al., 2024; Ba et al., 2024; Fu etal., 2025)和基于LVLM的方法 (Khan &amp; Dang-Nguyen, 2024; Su et al.,2023; Liu et al., 2024b; Wang et al., 2024a)</p><p>​  实现细节。我们的方法采用了PandaGPT架构，该架构中集成了ImageBindHuge模型作为图像和文本编码器。我们从编码器的第16层、第24层和第32层提取特征，与文本特征计算一致性图，然后将这些图传递给Vicuna-7B模型进行推理。为了实现多轮对话功能，我们在深度伪造数据集和PandaGPT数据集之间交替训练。伪造区域的数量n设定为3。所有图像均裁剪至224×224。训练在两块NvidiaRTX 4090GPU上进行，共50个周期，使用Adam优化器，学习率为1e-4，权重衰减为1e-5。损失参数λ设为1。</p><h2 id="与sota检测方法的比较">4.2.与SOTA检测方法的比较</h2><p>​  我们首先将我们的方法与几种最先进的深度伪造检测方法进行比较。(Li etal., 2020a; Shiohara &amp;Yamasaki, 2022; Cao et al., 2022; Huang etal., 2023; Yan et al., 2024; Tan et al., 2024)</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708152829213.png"alt="image-20250708152829213" /><figcaption aria-hidden="true">image-20250708152829213</figcaption></figure><h2 id="与基于lvlm的方法的比较">4.3.与基于LVLM的方法的比较</h2><p>​  LVLM检测性能<br/>​  我们对我们的框架进行了基准测试，与最先进的基于LVLM的分类方法（Khan&amp; Dang-Nguyen，2024；Lin et al.，2025；Fu etal.，2025）和VQA方法（Su et al.，2023；Wang et al.，2024a；Liu etal.，2024b）进行了对比。在这些评估中，图像和相应的查询都被作为输入提供给LVLM，模型需要判断图像的真实性（真实或伪造）。对于分类任务，我们的知识引导的伪造检测器（KFD）在检测性能上显著优于现有的基于LVLM的分类方法。对于VQA模型如PandaGPT、Qwen2-VL和FAKOwl，我们利用LLM的输出（“是”或“否”）来判断真实性，并计算AUC值进行评估。如表3所示，我们的方法在FF++、CDF2、DFD、CDF1和DFDCP数据集上始终优于现有的基于LVLM的VQA方法，表现更佳。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153116786.png"alt="image-20250708153116786" /><figcaption aria-hidden="true">image-20250708153116786</figcaption></figure><p>​  对话式可视化。与传统检测方法不同，我们的方案不仅支持深度伪造检测，还能实现多轮对话功能，让用户能更深入地了解图像内容。图5展示了数据集内和跨数据集评估中的部分对话示例。实验结果表明，我们提出的方案能精准识别图像中的伪造区域，而大语言模型则提供了精准且符合上下文的判断。更多多轮对话示例详见补充材料。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153136840.png"alt="image-20250708153136840" /><figcaption aria-hidden="true">image-20250708153136840</figcaption></figure><h2 id="分析">4.4.分析</h2><p>​  <strong>训练图像数量。</strong><br/>​  在真实场景中获取大规模人脸图像往往难以实现，因此我们通过调整训练图像数量来评估模型性能。具体而言，我们从FF++数据集中随机抽取50、100、200和500张真实图像，生成对应的假图像-文本对用于训练。训练步骤固定为500次。基于LLM的响应结果计算视频级AUC指标。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153225551.png"alt="image-20250708153225551" /><figcaption aria-hidden="true">image-20250708153225551</figcaption></figure><p>​  如表4所示，仅使用500张训练图像，我们的方法就达到了业界领先水平。尽管减少训练集的规模会导致精度略有下降，但我们的方法即使在仅有100张训练图像的情况下仍能保持竞争力。这突显了我们框架的稳健性，尤其是在数据稀缺的情况下。</p><p>​  <strong>即时调优机制的效能验证。</strong><br/>​  该机制通过将伪造检测知识转化为大模型输入，显著提升识别准确率。其核心架构包含伪造提示学习器（FPL）、大模型及LoRA策略。为评估该模块效能，我们在FF++、CDF2和DFDC数据集上开展消融实验。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153312148.png"alt="image-20250708153312148" /><figcaption aria-hidden="true">image-20250708153312148</figcaption></figure><p>​  通过计算大模型在鉴别真伪图像时的输出结果曲线下面积（AUC），如表5所示：配备伪造提示学习器的模型展现出更优的AUC值，表明其在深度伪造检测任务中具有更强的识别能力。值得注意的是，集成LoRA策略后性能进一步提升，在多个数据集上均取得优于未采用该策略的配置结果。</p><p>​  <strong>对训练数据集的通用性</strong><br/>​  深度造假检测的通用性与所使用的训练数据的多样性密切相关。为验证该方法在不同数据集上的有效性，我们在多个训练集上训练模型，并在FF++、CDF2、DFDCP和DFDC数据集上进行了跨数据集评估。</p><figure><imgsrc="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153440331.png"alt="image-20250708153440331" /><figcaption aria-hidden="true">image-20250708153440331</figcaption></figure><p>​  如表6所示，我们根据语言模型的响应计算了检测AUC值。结果表明，本方法在多样化数据集上展现出强大的鲁棒性，并能通过不同类型的伪造样本进行泛化训练，有效应对各类伪造类型。</p><p>​  <strong>不同LLM架构的影响</strong><br/>​  深度伪造检测性能与所采用的特定大模型架构密切相关，因为不同大模型具有独特的特性。为评估各类大模型架构的检测表现，我们选取了Llama-3.2-1B、Llama-3.2-3B和Vicuna-7B三个模型进行测试。实验在FF++、CDF1、DFD和DFDC数据集上展开。如表7所示，我们发现大规模架构始终展现出更优的检测效果。这一趋势表明，参数容量更大的模型更能精准捕捉细微的伪造痕迹。</p><p>​  <strong>基于参考优化的消融研究</strong><br/>​  为了验证基于参考的优化过程的有效性，我们在两种配置下训练了模型：一种是使用基于参考的优化过程（ROP），另一种是没有使用该过程。随后，我们评估了该方法在不同数据集上的泛化能力。表8总结了在CDF1、CDF2、DFDC和DFDCP数据集上的泛化性能。结果显示，即使不进行相似性优化，所提出的框架也优于现有方法。此外，引入相似性优化过程后，性能进一步提升，这进一步证明了其在增强深度伪造检测模型泛化能力方面的有效性。</p><h1 id="结论">5.结论</h1><p>​  在这项研究中，我们提出了一种新的深度伪造检测框架，该框架利用语言模型（LLM）来提升泛化能力和解释性。通过整合知识引导的伪造检测器，我们能够有效地将图像特征与原始图像和深度伪造图像的文本描述进行匹配，从而促进伪造分类和定位。此外，我们还引入了一个伪造提示学习器，能够将细粒度的伪造特征转化为语言模型的输入，确保了准确的伪造检测结果。在包括FF++、CDF1、CDF2、DFD、DFDCP和DFDC在内的多个基准测试中，我们的方案在泛化性能上超越了现有方法。值得注意的是，我们的框架还支持多轮对话，提供了交互式且可解释的检测结果。这些发现强调了基于语言模型的方法在提升深度伪造检测技术方面所具有的潜力。</p><h1 id="代码结构">6.代码结构</h1>]]></content>
      
      
      <categories>
          
          <category> deepfake </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Rethinking Image Forgery Detection via Soft Contrastive Learning and Unsupervised Clustering</title>
      <link href="/FOCAL/"/>
      <url>/FOCAL/</url>
      
        <content type="html"><![CDATA[<p>Rethinking Image Forgery Detection via Soft Contrastive Learning andUnsupervised Clustering</p><p>Haiwei Wu, Yiming Chen, Jiantao Zhou, <em>Senior Member, IEEE</em>,and Yuanman Li, <em>Senior Member, IEEE</em></p><h1 id="摘要">摘要</h1><p>​  图像伪造检测的目标是识别并定位图像中的伪造区域。现有的大多数伪造检测算法通过分类问题来区分伪造像素和原始像素。然而，伪造像素与原始像素的定义仅在单个图像内部相对，例如，图像A中的伪造区域在其原始图像B中可能是原始的（拼接伪造）。这种相对定义被现有方法严重忽视，导致不同图像中的伪造（或原始）区域被不必要地归为同一类别。为了解决这一难题，我们提出了基于软对比学习和无监督聚类的新型、简单而有效的图像伪造检测方法——法医对比聚类（FOCAL，FOrensicContrAstivecLustering）。具体来说，FOCAL<br/>​  1)设计了一种软对比学习（SCL，softcontrastivelearning），以图像为单位监督高级法医特征的提取，明确体现了上述相对定义；<br/>​  2)采用即时无监督聚类算法（而非训练好的算法）将学习到的特征聚类为伪造和原始类别，进一步减少了训练数据对不同图像的影响；<br/>​  3)通过简单的特征级连接来提升检测性能，无需重新训练。<br/>​  广泛的实验结果表明，在六个公开测试数据集上，我们提出的FOCAL方法在多个方面显著优于现有最先进方法：在<strong>Coverage</strong>上高出+24.8%，在<strong>Columbia</strong>数据集上高出+18.9%，在<strong>FF++</strong>数据集上高出+17.3%，在<strong>MISD</strong>数据集上高出+15.3%，在<strong>CASIA</strong>数据集上高出+15.0%，在<strong>NIST</strong>数据集上高出+10.5%（见图1）。</p><figure><img src="../postimages/FOCAL/image-20250703225034556.png"alt="image-20250703225034556" /><figcaption aria-hidden="true">image-20250703225034556</figcaption></figure><p>​  FOCAL方法不仅为图像伪造检测任务提供了新的视角，还可能成为该领域的新型基准。代码可在https://github.com/HighwayWu/FOCAL上获取。</p><h1 id="i.引言">I.引言</h1><p>​  随着图像编辑工具如Photoshop和美图的不断进步和普及，人们可以轻松地对数字图像进行处理，而无需具备专业知识。因此，图像的真实性问题最近受到了广泛关注，因为恶意篡改（伪造）的图像可能在谣言传播、经济欺诈、非法获利等多个领域造成严重的负面影响。<br/>​  许多法医方法[1]–[6]，[13]–[23]（及其参考文献）已经开发出来，用于检测和定位图像中的伪造区域。其中，基于深度学习的方案比依赖手工特征的方法表现更佳。一些法医方法专门用于检测特定类型的伪造，如拼接[24]、复制-移动[25]和修复[26]，而更强大且实用的解决方案则用于检测复杂的混合类型伪造，即使这些伪造伴随传输退化和各种后处理操作[1]–[4]。<br/>​  通常，现有的基于学习的图像伪造检测方法通过二分类问题来区分像素是伪造的还是原始的。需要注意的是，伪造和原始像素的定义仅在单个图像中相对。例如，在图2(a)中，与两个人相关的像素被认为是原始的，而在图2(b)中，这些相同的像素则被认为是伪造的，这可能导致标签冲突。</p><figure><img src="../postimages/FOCAL/image-20250703225354437.png"alt="image-20250703225354437" /><figcaption aria-hidden="true">image-20250703225354437</figcaption></figure><p>​  不幸的是，现有的基于分类的伪造检测方法严重忽视了这种相对定义，将不同图像中的伪造（原始）区域不必要地混为一类。实际上，图2中的α1、α2和α3区域虽然属于同一原始类别，但它们的法证特征并不一定相似（β1和β2也是如此）。因此，当看到同一组像素被错误地标记为伪造或原始时，分类器可能会受到误导，导致训练不稳定和检测性能下降。<br/>​  重新审视伪造像素与原始像素的相对定义，促使我们对以往流行的分类问题进行重新定义，引入对比学习和无监督聚类的新范式。具体而言，本研究提出了一种名为‘法医对比聚类’（FOCAL）的方法，这是一种新颖、简单且高效的图像伪造检测方法。首先，FOCAL的核心在于通过像素级真实伪造掩码直接监督高级特征，明确利用了上述相对定义。考虑到像素级的真实标签可能在监督高级特征学习时引发所谓的标签模糊性，我们提出了一种用于FOCAL训练的软对比学习（SCL）方法。具体而言，SCL通过引入可优化的系数，能够细致地调整原始类别与伪造类别特征权重之间的关系。此外，我们设计的SCL（图像级学习）模型还具有一个独特特点，即逐图像监督，这能有效避免同一批次中不同图像特征之间的相互影响（即标签冲突）。进一步地，FOCAL采用了一种即时无监督聚类算法，将学习到的特征分类为伪造或原始类别，从而进一步减少了训练数据对跨图像的影响。值得注意的是，所采用的聚类模块不包含任何可训练参数，因此不会参与训练过程。此外，研究还表明，通过直接在特征级别进行融合，无需重新训练即可实现性能的进一步提升。<br/>​  广泛的实验结果表明，在六个公开测试数据集上，我们提出的FOCAL算法在多个方面显著优于现有的最先进竞争算法[1]–[6]：对于IoU评价指标，在Columbia[7]上+24.8%，在Coverage[8]+上17.3%，在MISD [11]上+18.9%，在FF++[12]上+15.0%，在CASIA [9]上+15.3%，以及在NIST[10]上+10.5%。FOCAL的范式不仅为图像伪造检测任务提供了新的视角，还可能成为该领域的新型基准。我们的主要贡献可以概括如下：</p><ul><li>从伪造像素和原始像素的相对定义的角度出发，重新思考基于分类的图像伪造检测的固有限制。</li><li>我们设计了FOCAL，这是一种基于提出的SCL和无监督聚类的新型、简单而有效的图像伪造检测范式。</li><li>所提出的FOCAL在六个跨域数据集上显著优于几种最先进的图像伪造检测方法，平均增益在IoU上为20.2%，在F1上为10.8%。</li></ul><p>​  本文其余部分安排如下：第二部分介绍图像伪造检测的相关工作；第三部分详细介绍我们提出的FOCAL框架；第四部分给出实验结果和分析；第五部分进行总结。</p><h1 id="ii.图像伪造检测相关工作">II.图像伪造检测相关工作</h1><p>​  基于分类的图像伪造检测通过深度学习技术已经达到了最先进的性能[1]–[4]。CAT-Net[2]通过分类离散余弦变换（DCT）系数来定位伪造区域。PSCC-Net[3]利用多尺度特征进行伪造检测。Dong等人[1]引入了MVSSNet，通过多视角学习联合提取伪造特征。Wu等人[4]设计了一种基于对抗噪声建模的鲁棒训练框架，用于在线社交网络上的图像伪造检测。最近，Guillaro等人[5]提出了TruFor，该方法结合了RGB图像和学习到的噪声敏感指纹，以提取法医线索。鉴于广泛使用的交叉熵损失的局限性，一些近期的研究还引入了对比损失，以辅助网络训练，用于图像伪造检测[5]，[13]，[18]，[27]–[30]。<br/>​  从聚类[17]、[31]至[37]的角度来看，检测伪造的方法只有少数几种，但其性能远不及基于分类的方法。这类方法主要通过简单的聚类算法，将图像块（像素）分为伪造和原始两类，采用了多种噪声特征，如图像噪声水平[35]、相机噪声[31]、[38]以及JPEG量化噪声[32]。<br/>​  尽管上述图像伪造检测方法已经取得了相当不错的成果，但它们的设计原则与我们提出的FOCAL在以下几个方面存在显著差异：<br/>​  1）基于分类的方法忽略了伪造像素与原始像素之间的相对定义，因此未能充分利用无监督聚类的优势；<br/>​  2）涉及聚类的方法几乎都依赖于手工设计的特征，这些特征难以准确反映法医痕迹，并且在跨域测试中难以推广到未见过的伪造类型；<br/>​  3）我们的框架构建了一个特征空间，该空间能够明确地建模每个图像中局部区域之间的细微关系，使检测器能够在没有预设伪影的情况下识别出细微的不一致。</p><h1 id="iii.图像伪造检测的focal">III.图像伪造检测的FOCAL</h1><p>​  在深入探讨FOCAL的具体细节之前，我们先介绍传统基于分类的图像伪造检测的一般框架。该框架由两个神经网络组成：提取器和分类器，如图3(a)所示。</p><figure><img src="../postimages/FOCAL/image-20250703230604933.png"alt="image-20250703230604933" /><figcaption aria-hidden="true">image-20250703230604933</figcaption></figure><p>​  给定输入<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\)</span>，提取器首先提取出一个判别特征<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{H\times W\timesC}\)</span>，然后分类器根据这个特征生成一个预测的二进制伪造掩码<spanclass="math inline">\(\mathbf{P}\in\{0,1\}^{H\timesW}\)</span>。为了优化网络，通常采用交叉熵损失<spanclass="math inline">\({\mathcal{L}}_{\mathrm{CE}}(\mathbf{P},\mathbf{Y})\)</span>，其中<spanclass="math inline">\({\bf Y}\in\{0,1\}^{H\timesW}\)</span>是真实的伪造掩码（1表示伪造像素，0表示原始像素）。不同于传统的分类方法，我们构建了一个对比聚类框架FOCAL（见图3(b))，用于图像伪造检测，该框架通过利用图像中伪造与原始像素之间的相对定义来实现。接下来，我们将详细说明FOCAL的训练过程，通过SCL监督，以及FOCAL的测试过程，通过无监督聚类。</p><h2id="a.通过软对比学习scl进行focal训练">A.通过软对比学习（SCL）进行FOCAL训练</h2><p>​  FOCAL的训练过程如图3(b)上部所示。从给定的输入X中提取高级特征F后，我们期望来自相同（伪造或原始）区域的特征相互吸引，而来自不同区域的特征相互排斥。一个自然的想法是使用像素级别的真实伪造掩码来划分这些特征所属的区域（类别），然后通过传统的对比学习方法（如NCE[39]、[40])监督特征的更新。然而，此时可能会出现标签模糊的问题，因为一些高层次的特征（例如，与伪造区域边界相关的特征）可能同时出现在伪造和原始像素中。为了解决这一标签模糊问题，我们提出了一种SCL算法，该算法通过可优化的权重系数来确定特征属于伪造或原始类别的程度。因此，我们能够构建出分别代表伪造区域和原始区域特征的平均特征。最后，基于优化后的系数和平均特征，实现了精心设计的软对比监督机制。<br/>​  具体来说，我们使用了一个权重矩阵<spanclass="math inline">\(\mathbf{W}=[w_{ij}]\)</span>，其中每个权重系数<span class="math inline">\(w_{ij}\)</span>对应于位于扁平化坐标<spanclass="math inline">\(i(i\in\{1,...,HW\})\)</span>的特征<spanclass="math inline">\(\mathbf{F}_{i}\in\mathbb{R}^{C}\)</span>，表示<spanclass="math inline">\(\mathbf{F}_{i}\)</span>与第j类（j∈{0,1}，j =0表示原始数据，j =1表示伪造数据）的相关程度。根据地面实测数据Y，如果某个特征的索引i位于原始区域，则将其初始值设为wi0= 1；反之，则设为wi1 =1。显然，这些系数在0到1之间进行了归一化处理，并且满足<spanclass="math inline">\(\textstyle\sum_{j}w_{ij}\,=\,1\)</span>。需要注意的是，这些初始化的硬系数无法准确反映特征可能属于多个类别的软度。设<spanclass="math inline">\(\mathbf{M}=[\mathbf{M}_{j}]\)</span>，其中<spanclass="math inline">\(\mathbf{M}_j\in\mathbb{R}^{C}\)</span>表示第j类（无论是原始还是伪造）的特征中心。通过最小化<spanclass="math inline">\(\mathbf{F}_{i}\)</span>与<spanclass="math inline">\(\mathbf{M}_{i}\)</span>之间的距离，我们可以优化<spanclass="math inline">\(w_{ij}\)</span>。为此，我们构建了以下约束优化问题：</p><p><span class="math display">\[\begin{array}{cc}\mathrm{minimize}&amp;{J(\mathbf{W},\mathbf{M}),\mathrm{where}\J=\sum_{i,j}w_{ij}^{\boldsymbol{\mu}}\|\mathbf{F}_{i}-\mathbf{M}_{j}\|^{\dagger}}\\\mathrm{subject\;to}&amp;{\sum_{j}\mathbf{W}.j=1,}\end{array}\]</span></p><p>​  其中∥·∥表示欧几里得范数，W·j代表矩阵W的第j列向量，而参数ρ用于控制超参数[41]，通常设定为2。在处理约束问题时，拉格朗日乘数法是一种常用的优化策略，其核心步骤是构建拉格朗日函数：<span class="math display">\[{\cal L}({\bf W},{\bfM},\lambda)=\sum_{i,j}w_{i j}^{\rho}\|{\bf F}_{i}-{\bfM}_{j}\|^{2}+\lambda\cdot(\sum_{j}{\bf W}.j-1),\]</span> ​  其中<spanclass="math inline">\(\lambda=[\lambda_{i}]\)</span>是拉格朗日乘子向量。最小化<spanclass="math inline">\(J\)</span>相当于找到L的驻点，即L关于W、M和λ的梯度均为零的点。通过令L关于wij的偏导数为零，我们得到：<span class="math display">\[\begin{array}{l}{\frac{\partial L}{\partialw_{i j}}=\rho w_{ij}^{\rho-1}||\mathbf{F}_{i}-\mathbf{M}_{j}||^{2}+\lambda_{i}=0}\\{\Rightarrow w_{ij}=\left({\frac{-\lambda_{i}}{\rho||\mathbf{F}_{i}-\mathbf{M}_{j}||^{2}}}\right)^{\frac{1}{\rho-1}}.}\end{array}\]</span>​  将(3)式与约束条件<span class="math inline">\(\frac{\partialL}{\partial\lambda_{i}}\,=\,\sum_{j}\,w_{ij}\,-\,1\,=\,0\)</span>相结合，可得： <spanclass="math display">\[\begin{array}{l}{\sum_{j}\left(\frac{-\lambda_{i}}{\rho\|\mathbf{F}_{i}-\mathbf{M}_{j}\|^{2}}\right)^{\frac{1}{\rho-1}}=1}\\{\Rightarrow\left({\frac{-\lambda_{i}}{\rho}}\right)^{\frac{1}{\rho-1}}=({\sum_{j}\frac{-\lambda_{i}}{\|\mathbf{F}_{i}-\mathbf{M}_{j}\|^{2}}}})^{-\frac{1}{\rho-1}.}\end{array}\]</span>​  将(4)代入(3)以消除λi，从而可推导出wij： <spanclass="math display">\[w_{i j}=\left(\frac{||{\bf F}_{i}-{\bfM}_{j}|^{2}}{||{\bf F}_{i}-{\bf M}_{0}||^{2}}+\frac{||{\bf F}_{i}-{\bfM}_{j}||^{2}}{||{\bf F}_{i}-{\bfM}_{1}||^{2}}\right)^{-\frac{1}{\rho-1}}.\]</span>​  另一方面，将L相对于Mj的梯度设为0可以得到： <spanclass="math display">\[\nabla_{\mathrm{M}_{j}}L=\sum_{j}-2w_{ij}^{\rho}(\mathbf{F}_{i}-\mathbf{M}_{j})=0,\]</span></p><p>​  那么Mj可以写为：</p><p><span class="math display">\[\mathbf{M}_{j}={\frac{\sum_{i}w_{ij}^{\rho}\mathbf{F}_{i}}{\sum_{i}w_{i j}^{\rho}}}.\]</span>​  由于变量(5)和(7)相互交织，我们采用了交替迭代的方法，持续更新wij和Mj，直到目标函数J收敛。</p><figure><img src="../postimages/FOCAL/image-20250708230932732.png"alt="image-20250708230932732" /><figcaption aria-hidden="true">image-20250708230932732</figcaption></figure><p>​  图4直观地展示了wij的变化过程，其中(a)表示初始的wij值，而(b)则对应优化后的wij值。显然，wij反映了原始类别与伪造类别边界特征的标签模糊性。<br/>​  在解决了标签模糊问题后，我们基于改进的NCE损失提出了SCL方法，以实现FOCAL框架中的软对比监督机制，旨在增强同类间的相似性，同时降低不同类间的相似度。通过将M0设为查询对象，<spanclass="math inline">\(\mathcal{L}_{\mathrm{pristine}}\)</span>可定义为：<spanclass="math display">\[\mathcal{L}_{\mathrm{pristine}}=-\log\frac{\frac{1}{HW}\sum_{i}\exp(\mathrm{M}_{0}\cdotw_{i0}\mathrm{F}_{i}/\tau)}{\mathrm{exp}(\mathrm{M}_{0}\cdot\mathrm{M}_{1}/\tau)},\]</span>​  其中，τ是温度超参数[42]。在公式(8)中，分子部分用于衡量原始特征间的相似度，而分母部分则描述了原始与伪造中心特征之间的相似性。同样，我们也可以通过将查询设置为M1，从锻造特征的角度获取<spanclass="math inline">\(\mathcal{L}_{\mathrm{forged}}\)</span>： <spanclass="math display">\[\mathcal{L}_{\mathrm{forged}}=-\log{\frac{\frac{1}{HW}\sum_{i}\exp({\bf M_{1}\cdot w_{i1}{\bf F}_{i}}/\tau)}{\exp({\bfM_{1}\cdot M_{0}}/\tau)}}.\]</span> ​  总体的SCL训练损失公式变为<spanclass="math inline">\(\mathcal{L}_{\mathrm{SCL}}=\mathcal{L}_{\mathrm{pristine}}+\mathcal{L}_{\mathrm{forged}}\)</span>。与传统的NCE损失相比，我们改进的LSCL方法在每次损失计算中都考虑了所有正向键，通过计算Mj与加权特征wijFi的点积期望值。这不仅有助于优化过程，也如图5中的损失曲线所示。正如第四节所述，这种新的LSCL损失方法显著提升了性能，超越了传统的NCE损失。<br/>​  需要强调的是，训练阶段的监督是在真实伪造掩码Y和提取特征F之间隐式进行的，而没有生成预测的伪造掩码。此外，对于前向迷你批次中的每个图像，LSCL（局部软对比损失）是逐个计算的，而不是在整个批次中计算，然后将这些损失相加以计算总体损失。具体来说，给定一个迷你批次的特征<spanclass="math inline">\({\{\bf F}^{(1)},{\bf F}^{(2)},\cdot\cdot\cdot,{\bfF}^{(B)}\}\)</span>，该迷你批次的总体软对比损失<spanclass="math inline">\({\mathcal{L}}_{\mathrm{SCL-IBI}}\)</span>为：<spanclass="math display">\[{\mathcal{L}}_{\mathrm{SCL-IBI}}={\frac{1}{B}}\sum_{b=1}^{B}{\mathcal{L}}_{\mathrm{SCL}}(\mathbf{F}^{(b)}).\]</span>​  请注意，在公式（10）中，小批量特征未合并以计算总体的LSCL，从而避免了训练数据对不同图像的影响。这种总损失是在伪造像素与原始像素相对定义的指导下设计的，与[5]、[18]、[39]、[43]等方法中的损失计算在批次级别进行有显著差异。为了进一步证明（10）的合理性，我们在图5中绘制了传统批次处理和我们逐图像处理的对比损失曲线。</p><figure><img src="../postimages/FOCAL/image-20250704105334905.png"alt="image-20250704105334905" /><figcaption aria-hidden="true">image-20250704105334905</figcaption></figure><p>​  可以看出，逐图像设计的损失函数（绿色线）不仅使优化过程更加稳定，还显著加快了收敛速度。特别是，蓝色和橙色线中检测到的高幅度脉冲表明，相关批次图像之间可能存在严重冲突，例如图2(a)和(b)中出现的标签冲突情况。<br/>​  最终，经过培训的提取器将用于FOCAL测试阶段。正如预期的那样，并将通过实验验证，我们提出的带有逐图像监督的SCL损失显著提高了图像伪造检测性能。</p><h2 id="b.通过无监督聚类进行焦点测试">B.通过无监督聚类进行焦点测试</h2><p>​  我们现在准备详细介绍FOCAL测试阶段的细节。关键在于如何将提取的特征映射到预测的伪造掩模中。与传统框架使用训练好的分类器（见图3(a))相比，我们提出采用无监督在线学习算法（见图3(b)的下半部分）。如前所述，伪造像素与原始像素的定义仅在一个图像中相对，难以在不同图像间推广。这解释了为什么基于分类的方法未能提供令人满意的检测结果，因为从训练数据中训练出的分类器可能无法对未见过的数据进行有效推断。<br/>​  因此，将不同图像的特征分别映射到最终的伪造掩模上，会是一个更为明智的解决方案。为此，我们采用了即时聚类算法。具体来说，我们使用了HDBSCAN[44]，最小聚类大小经验性地设定为200，对F进行聚类，并将元素最多的聚类标记为原始（否则为伪造），隐含假设伪造像素只占相对较小的比例。通过我们的SCL损失函数在逐图像监督下提取的特征F可能已经非常具有区分度，使得无监督算法足以处理聚类任务。不同聚类算法的性能比较将在实验结果中展示。<br/>​  备注：一种可行的方法是将可训练的聚类（如可微分K-means[45])与对比损失结合，形成端到端的框架，从而实现中间特征F和聚类结果的联合优化。然而，实验发现这种方法不仅显著增加了训练时间，而且未能带来明显的性能提升。尽管如此，探索能够平衡端到端可区分性和聚类鲁棒性的混合框架仍然是未来研究的一个开放且有价值的方向。总之，由于性能提升不明显和额外的内存开销，端到端训练框架被放弃了。</p><h2 id="c.特征融合策略">C.特征融合策略</h2><p>​  我们现在展示，通过简单而有效的特征融合策略，可以进一步提升独立FOCAL的性能。图6展示了一个将两个具有不同骨干网络（例如，HRNet[48]或ViT [49])的FOCAL α和FOCAL β融合的例子。</p><figure><img src="../postimages/FOCAL/image-20250704105736093.png"alt="image-20250704105736093" /><figcaption aria-hidden="true">image-20250704105736093</figcaption></figure><p>​  融合后的特征可以通过直接连接轻松获得，即， <spanclass="math display">\[\hat{\mathbf{F}}=\mathrm{Concat}(\mathbf{F}^{\alpha},\mathbf{F}^{\beta}),\]</span>​  其中，<span class="math inline">\(\mathbf{F}^{\alpha}\)</span>和<spanclass="math inline">\(\mathbf{F}^{\beta}\)</span>分别是通过FOCALα和FOCALβ提取的特征，需要调整到相同的分辨率。随后，通过聚类和映射生成预测结果。实验验证表明，这种特征级融合显著优于简单的结果级融合[50]。此外，这种特征融合策略可以轻松扩展到包含两个以上FOCAL网络的情况，且无需重新训练。</p><h1 id="iv.-实验结果">IV. 实验结果</h1><p>​  在本节中，我们首先详细介绍了实验设置。接着，在六个公开测试数据集上报告了图像伪造检测/定位的结果，并与几种最先进算法的结果进行了对比。最后，进行了广泛的消融研究和进一步分析。</p><h2 id="a.设置">A.设置</h2><p>​  1)训练数据集：<br/>​  我们使用与[2]相同的训练数据集[5]来训练FOCAL。该数据集包含了超过80万张伪造图像，这些图像来自SP-COCO[2]、CM-COCO [2]、CM-RAISE [2]、CM-C-RAISE [2]、CASIA-v2 [46]和IMD2020[47]。特别是，CASIA-v2是一个广泛采用的数据集，包含多种多源拼接和复制移动伪造图像；而IMD2020则收集了互联网上的真实世界操纵图像。鉴于这两个数据集中的图像数量不足，Kwon等人[2]利用拼接和复制移动方法，基于原始数据集COCO[51]和RAISE[52]生成了大量伪造图像。为了更好地模拟真实世界图像的分布，还进行了包括调整大小、旋转和压缩在内的多种后处理操作。</p><p>​  2)测试数据集：<br/>​  根据[1]、[2]、[4]至[6]的分类，本研究采用了六个常用的测试数据集，具体包括：Columbia[7], Coverage [8], CASIA [9], NIST [10],MISD [11], and FF++[12]。这些测试数据集包含大量高度复杂的伪造样本，例如MISD（多源伪造数据集）和FF++（通过生成对抗网络[53]合成的面部数据集）。值得注意的是，训练数据集与测试数据集之间没有重叠，旨在模拟实际应用环境，评估伪造检测算法的泛化能力。表I中详细列出了所有涉及的数据集的统计信息。</p><figure><img src="../postimages/FOCAL/image-20250704110113430.png"alt="image-20250704110113430" /><figcaption aria-hidden="true">image-20250704110113430</figcaption></figure><p>​  3）对比方法：<br/>​  以下是最先进的基于学习的图像伪造检测算法：PSCC-Net[3]、MVSS-Net [1]、IF-OSN [4]、WSCL [6]、CAT-Net [2]和TruFor[5]，被选为对比方法。这些算法的官方代码链接分别为1 2 3 4 56。为了确保比较的公平性，我们不仅直接使用了它们发布的版本，还在CAT-Net的训练数据集上重新训练了PSCC-Net、MVSS-Net、IFOSN和WSCL。此外，我们还引入了两种著名的基于聚类的算法——Lyu-NOI[17]和PCA-NOI [35]作为对比方法。</p><p>​  4)评估指标：<br/>​  遵循[1]、[2]、[4]至[6]的惯例，我们采用像素级别的F1分数和交并比（IoU）作为固定阈值指标（数值越高越好），默认阈值设为0.5。具体而言，宏平均F1分数定义为<spanclass="math display">\[\mathrm{F1}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{2\times\mathrm{TP}_{y}}{2\times\mathrm{TP}_{y}+\mathrm{FP}_{y}+\mathrm{FN}_{y}},\]</span>​  其中TPy、FPy和FNy分别代表给定类别y（“原始”或“伪造”）的真阳性、假阳性和假阴性。<br/>​  IoU的计算方法如下：<spanclass="math display">\[\mathrm{IoU}={\frac{\mathrm{P}\cap\mathrm{Y}}{\mathrm{P}\cup\mathrm{Y}}},\]</span>​  其中P和Y分别代表预测掩码和真实掩码。对于不依赖阈值的度量，我们采用曲线下面积（AUC）作为标准，具体实现请参考我们的代码，该代码主要基于scikit-learn的[54]扩展包。</p><p>​  5)实现细节：<br/>​  我们使用PyTorch深度学习框架实现了FOCAL。FOCAL提取器采用了HRNet[48]和ViT [49]作为特定的骨干网络。优化器选择了Adam[55]，初始学习率为1e-4。训练时，批量大小设为4，使用4个NVIDIA A100GPU（40GB显存）进行训练。所有输入图像均被调整为1024×1024的尺寸，HRNet的特征空间为<spanclass="math inline">\(\mathbb{R}^{256\times256\times256}\)</span>，而ViT的特征空间为<spanclass="math inline">\(\mathbb{R}^{128\times128\times512}\)</span>。与现有竞争对手一样，训练过程中应用了包括随机压缩、缩放、模糊和加性噪声在内的数据增强技术，以模拟现实世界的变化并提高模型的鲁棒性。</p><h2 id="b.定量比较">B.定量比较</h2><p>​  表II列出了不同图像伪造检测方法在像素级F1和IoU分数上的定量对比。</p><figure><img src="../postimages/FOCAL/image-20250704110550316.png"alt="image-20250704110550316" /><figcaption aria-hidden="true">image-20250704110550316</figcaption></figure><p>​  此外，我们还报告了使用CAT-Net训练集重新训练的PSCC-Net、MVSS-Net、IF-OSN和WSCL的结果。总体而言，重新训练的MVSS-Net和IF-OSN的性能与官方发布的版本相当，而重新训练的PSCC-Net和WSCL则表现显著提升，PSCC-Net的F1分数提高了+9.5%，IoU分数提高了+5.0%。这表明不同的训练数据集对最终性能有显著影响。为了便于比较，我们在后续分析中采用了原始版本和重新训练版本中的较高性能。<br/>​  从表II可以看出，传统的聚类算法Lyu-NOI和PCA-NOI在F1值上表现不佳∼50%，在IoU值上表现不佳∼11%。这主要是因为这些算法的手工设计噪声特征在测试数据集的后处理过程中受到了严重破坏。相比之下，最新的基于分类的深度学习算法提供了显著提升的检测效果。在这些模型中，MVSS-Net和IF-OSN在Columbia数据集上的表现略胜一筹，IoU分别达到了78.4%和54.8%；而在CASIA数据集上，CAT-Net的表现更为出色，IoU达到了64.2%。最近发布的TruFor在其余三个数据集上也取得了优异的成绩。得益于训练框架中的SCL设计和测试时采用的无监督聚类方法，我们的FOCAL，无论是使用单个提取器（如HRNet或ViT)还是融合提取器（如HRNet+ViT），在所有测试数据集上均在F1和交并比标准上表现出色。特别是，FOCAL（Fusion）通过简单的特征级连接，无需重新训练，就能显著提升性能（例如，IoU在FF++和Coverage上分别提高了+10.6%和+4.8%）。<br/>​  表III还列出了像素级AUC的定量比较，以全面评估原始和伪造像素之间的阈值无关可分离性。</p><figure><img src="../postimages/FOCAL/image-20250704112701776.png"alt="image-20250704112701776" /><figcaption aria-hidden="true">image-20250704112701776</figcaption></figure><p>​  FOCAL的AUC得分报告为91.4%，与先前的方法[1]、[3]、[5]（76.7%∼85.3%）相比，显示出更优的区分能力，验证了我们的方法在所有决策阈值下都能保持强大的法医区分能力，而不仅仅是在特定的操作点上表现突出。<br/>​  鉴于上述评估均基于像素级别的指标，我们计算了图像级别的伪造检测得分，并在表IV中以AUC和准确率作为指标进行了报告。</p><figure><img src="../postimages/FOCAL/image-20250704112808087.png"alt="image-20250704112808087" /><figcaption aria-hidden="true">image-20250704112808087</figcaption></figure><p>​  具体而言，这一计算是通过分析我们的法医感知特征F的同质性来实现的，其中局部特征分布之间的归一化互信息作为全局真实性的指标。如表IV所示，FOCAL在AUC上比第二佳方法提高了5.3%，在准确率上提高了5.8%。这种能力自然地来源于我们的定位聚焦框架，因为伪造图像在被篡改区域和原始区域之间固有的特征异质性，使得无需修改架构即可同时优化像素级和图像级的检测。</p><h2 id="c.定性比较">C.定性比较</h2><p>​  图7展示了在一些代表性测试图像上的伪造检测结果。</p><figure><img src="../postimages/FOCAL/image-20250704112943958.png"alt="image-20250704112943958" /><figcaption aria-hidden="true">image-20250704112943958</figcaption></figure><p>​  可以看出，传统的基于聚类的方法，如Lyu-NOI和PCA-NOI，由于使用了手工设计的噪声特征，表现不佳；许多伪造区域未能被检测到，且存在大量误报。基于分类的方法PSCC-Net在这些跨域测试数据上也表现不佳，大多数伪造区域未能被检测到。同样，CAT-Net和MVSS-Net也错过了许多伪造区域，导致检测结果不准确。TruFor和IF-OSN在某些情况下表现稍好；但许多伪造区域仍无法准确识别，同时许多未被篡改的区域也被错误地检测为伪造。相比之下，我们的FOCAL（融合）不仅能够准确检测伪造区域，而且在跨域测试中表现稳定。此外，误报显著减少。<br/>​  FOCAL假设所有伪造区域在单个图像中具有相似的特征，尽管这些伪造可能采用不同的方法（如拼接和修复）。一个有趣的问题是，FOCAL是否能够同时检测多种类型的伪造。答案是肯定的。图7的最后两行展示了来自MISD数据集的例子，其中使用了多源拼接伪造。可以看出，FOCAL仍然能够产生令人满意的检测结果。在这一具有挑战性和实际应用的场景中，成功的原因可能是原始区域比伪造区域更多。数据量最大的聚类将直接被标记为原始，而数据量较少的聚类将被合并，并全部标记为伪造。<br/>​  在图11至16中，分别展示了对测试数据集的更多比较结果，包括Coverage[8]、Columbia [7]、NIST [10]、CASIA [9]、MISD [11]和FF++ [12]。</p><h2 id="d.消融研究">D.消融研究</h2><p>​  我们现就提取器骨干、损失函数和聚类算法这三个方面，分析每个组件如何为FOCAL框架做出贡献。</p><p>​  1）提取器主干：<br/>​  我们从选择FOCAL提取器开始消融研究，重点在于如何挑选骨干网络。由于我们的目标不是设计全新的骨干网络，而是直接采用了近年来最常用的几种骨干网络，即HRNet[48]、ConvNeXt [56]、ViT [49]和MiT[57]，进行对比。表V展示了相应的检测结果，其中第一行列出了传统分类框架的性能作为对比。</p><figure><img src="../postimages/FOCAL/image-20250704113231570.png"alt="image-20250704113231570" /><figcaption aria-hidden="true">image-20250704113231570</figcaption></figure><p>​  可以看出，ViT在这些对比的骨干网络中表现出更优的检测性能。这可能是因为ViT的注意力机制能够通过全局建模长距离依赖关系，提取出更丰富的伪造特征。此外，当有更先进的架构可用时，FOCAL中的提取器骨干可以灵活地被替换。进一步，在图8中，我们比较了使用不同骨干网络和融合策略时预测的伪造掩码的视觉效果。可以看出，特征融合显著优于简单的结果级融合（如并集或交集融合）。</p><figure><img src="../postimages/FOCAL/image-20250704113327321.png"alt="image-20250704113327321" /><figcaption aria-hidden="true">image-20250704113327321</figcaption></figure><p>​  2）损失函数：<br/>​  软对比学习模块在FOCAL中扮演着至关重要的角色。我们通过用现有的对比损失函数，如三元组[58]、DCL[59]、Circle [60]和原始的NCE[40]，替换LSCL，来评估FOCAL变体的性能。从表VI可以看出，并非所有这些损失函数都适用于伪造检测任务。</p><figure><img src="../postimages/FOCAL/image-20250704113547240.png"alt="image-20250704113547240" /><figcaption aria-hidden="true">image-20250704113547240</figcaption></figure><p>​  例如，三元组损失对每个查询正负对的距离得分施加了相同的惩罚强度[60]，这导致了模型的崩溃。通过在监督下重新加权每个距离得分，Circle提供了更加灵活的优化过程和明确的收敛目标，远超Triplet。尽管DCL和NCE采用了相同的监督机制，但DCL去除了正向约束，容易陷入局部最优解，导致其性能远逊于NCE。此外，通过引入可优化的系数wij来对特征进行加权，我们提出的LSCL能更好地处理标签模糊问题，与标准NCE相比，F1值提高了7.4%。请注意，当不使用wij提供的权重时，LSCL的监督效果退化为NCE，这突显了我们提出的SCL策略的重要性。最后，我们展示了在逐图像而非批量级别计算整体LSCL时的结果（表VI的最后两行）。正如预期，我们的逐图像整体损失设计显著优于批量级别的损失（F1值高出+5.5%）。这一显著的性能差距进一步表明，在单个图像中明确使用伪造像素和原始像素的相对定义的必要性。</p><p>​  3）聚类算法：<br/>​  除了对比学习之外，FOCAL的另一个关键模块是用于生成最终预测伪造掩模的聚类算法。为了探索最适合FOCAL框架的聚类算法，我们评估了最流行的几种聚类算法，包括K-means[61]、B-K-means [62]、BIRCH [63]、Hierarchical [64]和HDBSCAN[44]，并在表VII中报告了结果。对于K-means、B-Kmeans和BIRCH算法，将形成的聚类数量设置为2，而其他参数采用默认值。</p><figure><img src="../postimages/FOCAL/image-20250704113743958.png"alt="image-20250704113743958" /><figcaption aria-hidden="true">image-20250704113743958</figcaption></figure><p>​  可以看出，上述算法表现出相似的性能，这主要归因于提取器学习到的特征F具有区分性。其中，HDBSCAN的表现最佳，比第二名高出1.4%的F1分数。考虑到F可能包含256×256=65536个元素需要聚类。那些无法扩展到大规模元素的聚类算法，如谱聚类[65]和AP聚类[66]，由于速度极慢而被排除在外。我们还想要指出，在K均值、B-K均值和BIRCH算法中使用固定数量的聚类可能存在的局限性。对于完全原始的图像（无伪造区域），这些聚类方法仍会强制生成两个聚类，从而不可避免地产生误报（参见图10的最后一行）。</p><figure><img src="../postimages/FOCAL/image-20250704114017247.png"alt="image-20250704114017247" /><figcaption aria-hidden="true">image-20250704114017247</figcaption></figure><p>​  为了进一步评估原始图像中的误报（越低越好），我们在ImageNet[67]、COCO [51]和VISION[68]这几个原始数据集上进行了额外的实验，每组随机选取了2000张图像。</p><figure><img src="../postimages/FOCAL/image-20250704114052600.png"alt="image-20250704114052600" /><figcaption aria-hidden="true">image-20250704114052600</figcaption></figure><p>​  如表VIII所示，FOCAL采用的基于密度的算法HDBSCAN能够动态确定最终聚类的数量，有效减少了原始图像中的误报率4.1%。此外，即使使用K-means算法，我们的FOCAL仍然显著优于竞争对手[1]、[2]和[5]。</p><h2 id="e.鲁棒性评价">E.鲁棒性评价</h2><p>​  伪造的图像通常会经历一系列的后处理操作，如压缩、模糊、添加噪声等，以试图消除伪造痕迹或误导伪造检测算法。此外，作为图像的主要传输渠道，网络社交平台（OSNs）已被证明会对图像取证算法造成严重影响[4]。因此，评估所有竞争算法在后处理操作和社交网络传输中的稳健性至关重要。具体而言，我们将上述退化应用于原始测试数据集，并将结果展示在图9中。</p><figure><img src="../postimages/FOCAL/image-20250704114217762.png"alt="image-20250704114217762" /><figcaption aria-hidden="true">image-20250704114217762</figcaption></figure><p>​  可以看出，尽管CATNet[2]在原始数据集上表现出色，但对后处理和社交网络传输较为敏感。TruFor[5]、MVSS-Net [1]和IF-OSN[4]对这些干扰具有一定的抵抗力。相比之下，我们的FOCAL在这些竞争对手中始终表现出最佳的性能和稳定性。例如，FOCAL在Facebook或微博传输中的性能仅下降了∼0.2%。</p><h1 id="v.结论">V.结论</h1><p>​  我们已经明确指出了在图像中对伪造像素和原始像素进行相对定义的重要性，这已经被现有的伪造检测方法严重忽视。受此启发，我们提出了FOCAL——一种新颖、简单且高效的图像伪造检测框架，该框架基于SCL监督的逐图像处理和无监督聚类技术。通过大量实验验证了其卓越的性能。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cluster Triplet Loss for Unsupervised Domain Adaptation on Histology Images</title>
      <link href="/Cluster-Triplet-Loss-for-Unsupervised-Domain-Adaptation-on-Histology-Images/"/>
      <url>/Cluster-Triplet-Loss-for-Unsupervised-Domain-Adaptation-on-Histology-Images/</url>
      
        <content type="html"><![CDATA[<p>Cluster Triplet Loss for Unsupervised Domain Adaptation on HistologyImages</p><p>Ruby Wood1 ;Enric Domingo2 ;Viktor HendrikKoelzer2<em>,</em>3<em>,</em>4 ;Timothy S. Maughan2<em>,</em>5 ;JensRittscher 1</p><p>1牛津大学工程科学系，英国牛津<br/>2英国牛津大学肿瘤科，牛津<br/>3瑞士苏黎世大学医院病理学和分子病理学系，苏黎世大学，苏黎世，瑞士<br/>4瑞士巴塞尔大学医院医学遗传学和病理学研究所，巴塞尔，瑞士<br/>5英国利物浦大学分子和临床癌症医学系，利物浦，英国</p><h1 id="摘要">摘要</h1><p>​  用于从医学影像预测癌症患者治疗反应的深度学习模型，需要在不同患者群体中具有普适性。然而，由于患者群体的多样性，这是一项挑战。本研究关注从肿瘤活检扫描的组织学图像中预测结直肠癌患者对放疗的反应，并将这一预测模型应用于一个新且外观不同的患者群体。我们提出了一种新的无监督域适应方法，该方法采用聚类三元组损失函数，仅使用源域中的少量信息，从而将目标群体的AUC值从0.544提升至0.818。我们避免使用伪标签和类别特征中心，以防止对适应模型引入噪声和偏差，并通过实验验证我们的模型优于这些最先进方法。我们提出的方法适用于多种复杂的医学成像场景，包括基于图神经网络提取的图像的小型、内存可行表示的预测，用于大型全切片图像的预测。</p><h1 id="引言">1.引言</h1><p>​  将医学影像领域的深度学习模型从一组患者适应到另一组患者可能具有挑战性，因为患者之间存在广泛的个体差异。本研究中，我们利用深度学习技术，通过分析治疗前肿瘤组织的数字组织学图像，预测结直肠癌（CRC）患者对放疗的反应，并尝试将该模型应用于来自不同地理区域的全新患者群体。本研究特别关注无监督域适应（UDA），因为为了使该预测模型在临床实践中有效，我们需要在使用时无需了解患者的预后情况来调整模型。<br/>​  尽管在其他领域中，使用域适应技术的研究已经相当广泛，但将其应用于组织学图像则更具挑战性，因为这种成像方式的尺寸和异质性带来了复杂性[12]。<br/>​  组织学切片是从活检样本中切割出来的，经过苏木精和伊红（H&amp;E）染色并数字化扫描的肿瘤组织切片。这些切片以极高的分辨率扫描，导致文件体积极大。为了适应计算机内存，图像需要被分割成更小的部分，然后采用多实例学习（MIL）方法将这些预测整合为每张切片的一个最终预测。本文提出了一种方法，专注于模型内部的中间特征表示，从而保留了所有可选的MIL方法，用于最终输出。具体而言，我们利用图神经网络（GNN）方法，从自然分割的组织区域进行预测，通过GNN中的特征来帮助模型适应新领域。<br/>​  社会经济因素可能影响不同地区或国家的患者对癌症的体验[5]，而组织学图像中的批次效应通常源于肿瘤活检样本从患者体内取出后的处理过程。由于各医疗中心在组织样本处理上的方法略有差异，这导致数据中存在固有的领域偏移[29]。我们使用来自三个不同医疗中心的患者队列来训练和验证我们的方法，这些中心的组织处理实践各不相同。<br/>​  我们以一种通用的方法来解决二元预测问题，通过仅调整基础特征以适应新领域，同时保留原有的分类分支，从而避免使用伪标签。与许多其他统一数据适应（UDA）方法不同[19,30,35,40-43,45]，我们避免了从源模型中引入偏差和噪声到预测结果中。<br/>​  此外，我们避免使用基于类别的聚类方法来为每个类别标签找到一个代表，因为许多文献[10,15,19,30,43]已经采用了这种方法。通过一次性对整个特征集进行聚类，可以允许每个类别标签内部存在更多的变异，从而获得一个自然数量的聚类，而不受数据集中类别标签数量的限制。这种方法在处理二元结果数据时效果尤为显著，因为它能够用超过两个聚类来表示整个源数据集。<br/>​  本文中，我们开发了一种特征对齐的UDA技术，能够在不使用任何目标标签的情况下，将训练好的临床模型应用于未见过的目标群体。我们提出了一种创新的方法，定义了一个损失函数，用于‘源监督’训练，以实现领域适应。该损失函数仅需轻量级的源数据表示，即可指导新目标模型的学习，使其适应新的领域。我们的方法允许对队列调整的模型进行分布式训练，无需对原始模型进行任何训练或更新，从而提供了一种安全的联邦学习技术，能够保护不同地点之间的患者隐私。我们不将结果与所有数据集排列混淆，而是专注于最不相似的数据集作为目标数据集，因为这是最大的挑战。这在临床实践中也有类似的应用，即需要将预训练的模型转移到新的队列领域，以更好地预测患者的治疗结果，而无需事先了解患者对治疗的反应。这种方法不需要任何批次或队列假设，即使只有一个新数据点也可以应用。</p><h1 id="相关工作">2.相关工作</h1><h2 id="无监督域适应">2.1.无监督域适应</h2><h3 id="聚类">2.1.1 聚类</h3><p>​  尽管许多论文已经探索了使用聚类进行领域适应的方法，采用了对比或对抗损失方法[13,16,22,43]对齐源和领域分布，但据我们所知，还没有人使用我们建议的轻量级聚类方法。<br/>​  我们领域适应方法的灵感来源于吸引和分散[40]的概念，该方法旨在将相似特征聚集在一起，而将不相似特征分开。这一无监督方法通过使用k-最近邻算法和伪标签，最大化邻居之间的预测一致性，同时最小化不相似特征预测的相似性。一种类似的方法，结构化正则化深度聚类（SRDC）[30]，通过KMeans算法对目标数据的中间网络特征进行聚类，并最小化预测的目标标签与真实源标签分布之间的KL散度，以及学习到的源和目标聚类中心之间的KL散度。另一种利用KMeans的方法是Liang等人提出的源假设转移（SHOT）方法[19]。该方法通过冻结源模型的最终分类器层，其余部分作为目标模型的初始化。这种方法通过预测伪标签并最小化熵来实现无监督学习，类似于加权KMeans，找到目标类别的中心点，并根据最近邻类中心点（使用余弦距离测量）定义目标样本的伪标签。</p><h3 id="伪标签">2.1.2 伪标签</h3><p>​  大多数统一数据辅助（UDA）方法使用伪标签来训练模型[19,30,35,40-43,45]，这在多类分类中比二分类提供了更多的信息。这些伪标签通常用于掩码或作为指示器，以计算用于损失函数[40,45]的进一步统计信息。使用伪标签的方法很大程度上依赖于教师模型在目标领域具有合理的先验准确性，但这一点并不总是成立，正如Li等人[18]所指出的。重要的是，他们还发现没有通用的方法来评估这些伪标签的质量。尽管许多研究承认了这一问题，并提出了相应的解决方法[18,30,41,43]，但这种设计上的缺陷显然会引入不必要的偏差和噪声。张等人也意识到了这一点，在训练过程中通过测量与类别[41]特征中心的距离来对伪标签进行加权正则化[41]。分割与对比的方法将目标数据分为源样或非源样，并合理假设，来自源样目标数据的伪标签比来自特定目标样本的伪标签更准确[43]。SRDC的作者也承认，源模型在目标数据上的不可靠性可能导致一些错误的目标预测，因此他们在损失函数中加入了额外的项，使用伪标签作为预测标签的指示器[30]。</p><h3 id="三联体损失">2.1.3 三联体损失</h3><p>​  使用中心特征的三元组损失的概念最初由[10]在物体检索中提出，他们提出了三元组中心损失（TCL），旨在将同一类别的特征对齐到一个可学习的类别中心，并排斥不同类别的特征。他们使用欧几里得距离来衡量类别中心与样本特征之间的差异，这一点与我们的方法相同，不过在他们的三元组损失中，他们选择了最近的负样本中心。此外，他们还利用类别标签来确定相应的类别中心，因此该方法并非无监督的。其他研究也采用了类似的方法，通过特征中心的三元组损失来处理不同领域的问题。大多数研究集中在从伪标签中计算特征中心，以找到每个类别在分类问题中的代表中心。<br/>​  Wieczorek等人提出的质心三元组损失[37]用于图像检索，该方法在目标特征上采用传统的三元组损失，其中正例作为目标类别的中心，负例作为负类别的中心，这与我们提出的方法相似，但不同之处在于我们排除了任何假设或已知的类别信息。Lagunes-Fortiz等人[15]在他们的三元组损失中使用了不同的负样本，即从域本身中选取样本，而不是以特征为中心。此外，三元组损失还被用来定义目标和源集群作为类别引导的约束条件[35]，以实现域之间的更好类别对齐。</p><h2 id="组织学领域适应">2.2.组织学领域适应</h2><p>​  腐蚀斑<br/>​  在组织病理学的深度学习领域，不同医院和实验室之间的组织染色和处理方法存在显著差异，因此已经采取措施来减少这些批次染色的影响[8,14,25,44]超越传统的颜色标准化方法[20,31]。然而，有时仅靠这种方法还不足以确保模型的领域泛化能力。Lafarge等人[14]提出了一种领域对抗神经网络（DANN），用于预测样本是否来自特定领域，从而在保留对预测有用的特征的同时去除特定领域的特征。他们还尝试了传统的染色领域适应方法，发现当DANN与颜色增强或染色标准化结合使用时，效果最佳。</p><p>​  特征对齐<br/>​  在本研究中，我们专注于源域与目标域之间的特征对齐。在组织学领域，大多数基于聚类的方法使用伪标签来预测类别，这有助于更新类别特征中心[6,32]。Distill-SODA[32]是一种无源统一数据对齐方法，通过蒙特卡洛模拟聚类过程以提高鲁棒性。与我们的方法类似，他们计算聚类中心以在损失函数中与目标特征进行比较；然而，他们的聚类中心不是无标签的，而是每个类别仅限一个，而不是从源域自然推导而来。Jian等人提出了一种特征对齐方法[26]，通过训练卷积神经网络（CNN）将目标图像映射到源模型的特征空间，以最小化不同领域的差异。该方法进一步引入了孪生模型，鼓励来自同一全切片图像（WSI）的区域被赋予相同的标签，但这种方法未能考虑到组织样本中自然存在的异质性。Wang等人[36]专注于利用图神经网络（GNN）节点特征，通过对抗损失对结直肠癌（CRC）组织学图像进行核检测的对齐。.Abbet等人[1]则使用少量源标签训练模型，用于结直肠癌组织分类。</p><p>​  二分类<br/>​  大多数研究集中在多类分类或分割问题上，其中伪标签或类别中心可以提供更多的信息。一些研究则关注二分类问题，例如上皮-间质分类，有一篇论文同时在源域和目标域上训练单一模型，并通过简单地将目标域和源域的最大特征值对应的特征向量进行向量乘法，来调整卷积神经网络的内核以适应目标域[11]。Qi等人[24]也研究了上皮-间质分类，采用课程学习方法，通过测量样本与类别中心之间的余弦相似度来避免可能产生错误伪标签的样本，根据与源域的最大距离选择初始训练样本。<br/>​  Li等人[17]专注于在乳腺癌、肺癌和结肠癌的组织学切片上对肿瘤进行良性或恶性的分类。尽管没有明确的结果类别，但他们确实使用了多个数据集队列，因此他们的UDA方法在每个源域和目标域上分别训练了一个特征提取器，并利用源标签来学习特征分布的对齐。最优传输也被用于惩罚肿瘤与正常组织二元分类中的域预测[9]。我们未发现有关UDA在从组织学图像预测患者治疗反应模型方面的先前研究。</p><p>​  组织学下的三联体损失<br/>​  很少有研究将三元组损失应用于组织学的领域适应，尤其是在无监督方法方面。Sikaroudi等人[28]在他们的研究中使用了三联体损失，旨在学习不受医院限制的组织学表示，重点关注不同领域之间的类别条件变化。他们采用监督方法，使用交叉熵损失对目标预测进行优化，同时利用KL散度来对齐特征域，并通过度量损失来区分不同的类别。</p><h1 id="方法">3.方法</h1><p>​  本研究假设我们已经有一个预训练的源模型，希望将其适应到新的领域。下面我们将描述这个源模型，该模型基于[38]领域的类似先前模型构建，并解释如何使用源模型的权重初始化来训练新模型，以适应新领域的预测。此外，我们还将介绍在源数据上使用的聚类方法，以提取源数据的轻量级表示，这些表示随后被用于我们提出的聚类三元组损失函数中，以训练和适应新模型。<br/>​  我们首先介绍一些术语。源模型是我们应用任何领域适应之前所使用的初始模型。该源模型之前已在源数据<spanclass="math inline">\(x_s\)</span>上进行了训练和验证。目标数据<spanclass="math inline">\(x_t\)</span>是我们希望让源模型适应的新未见过的数据集。目标模型是源模型的更新版本，且已经适应了目标数据。</p><h2 id="源模型">3.1.源模型</h2><p>​  我们的源模型是一个图神经网络（GNN），包含三个图同构网络层，特征尺寸分别为64、32和16。我们没有直接将WSI输入到GNN中，而是先对WSI应用了超像素方法，然后从同一区域（大小[1,768])[38]的补丁特征中计算出超像素特征，这些补丁特征是通过自监督预训练的大组织学模型CTransPath[34]提取的。基于这些超像素特征，我们构建了每个WSI的图表示，其中节点和节点特征由超像素定义，图的边则通过最近邻关系使用Delaunay三角化定义。这些图随后作为GNN的输入，GNN以半监督方式训练，以预测患者对放疗的反应。<br/>​  在源验证数据集上，源模型达到了0.931的AUC、0.803的平衡准确率和0.885的加权F1分数。显然，我们的源模型在源队列上表现优异。尽管我们在训练过程中努力使模型具有更强的泛化能力，但当该模型应用于未见过的测试队列时，其泛化能力明显不足，AUC为0.544、平衡准确率为0.500、加权F1分数为0.840，具体数据见表2。为了防止训练队列上的过拟合，我们采取了多种措施，包括在提取特征前对训练图像进行大量数据增强，在图神经网络（GNN）和分类分支中引入高概率的dropout（p=0.5），在多个地理队列的患者上进行训练，并采用多任务学习方法，确保最终特征集不仅包含分子特征信息，还涵盖了空间组织结构等[38]。<br/>​  在本研究中，我们仅关注中间特征表示，而不涉及模型的最终预测阶段。在训练新的领域适应模型时，我们冻结了目标模型中的分类分支（由于采用了源模型的多任务学习方法，因此存在多个分类分支，其中一个分支用于预测患者对放射治疗的反应），并且仅在这些分类分支之前训练图神经网络层。因此，本研究的重点在于数据集的节点级特征，而非幻灯片级特征。我们将源模型中的节点特征提取器部分称为Fs，而该部分之后的分类器在源模型和目标模型之间保持不变。</p><h2 id="聚类-1">3.2.聚类</h2><p>​  我们通过在源数据上应用聚类技术，提取出源数据特征集的轻量级、高层次表示。图神经网络（GNNs）为我们提供了来自超像素节点的节点级预测，从而直观、自然地展现了肿瘤内部组织段的形态特征。我们从图神经网络的最终层中提取这些节点的特征，然后将其分为三个预测分支，以实现多任务学习方法。<br/>​  我们将聚类方法应用于训练中观察到的源数据队列的标准化节点特征向量的连接集合。这些连接的特征向量大小为[N，16]，其中N=134,132是节点总数，16是每个节点的特征数。为了确定最佳聚类数量kopt，我们计算了聚类数量k=2，...，20时的轮廓宽度[27]。我们选择该范围内轮廓宽度最高、Calinski-Harabasz指数[4]最低且DavidBouldin得分[7]最低的聚类作为最佳聚类数量，以确保在无监督设置中聚类的最显著性。由于样本量较大，我们采用了KMeansMiniBatch方法，该方法在Python库sklearn.cluster（版本1.1.3)[23]中实现。为了提高效率，我们在源数据集的一个子样本(n =10,000个节点特征）上拟合了MiniBatchKMeans，使用了最佳的聚类数量。我们提取了最终的聚类中心C，大小为[kopt，16]。</p><h2 id="聚类三元组损失">3.3.聚类三元组损失</h2><p>​  为了将我们的模型训练并适应目标数据集，我们提出了簇三元组损失，该方法利用了前一节中的源聚类。<br/>​  我们提出的聚类三元组损失函数以每个样本为基础，这意味着它可以适应任何规模的队列。对于每个提供的特征向量，它计算该特征向量与固定源聚类中心之间的均方误差损失，类似于传统KMeans算法的一次迭代。基于此，我们选取与输入特征向量最接近和最远的聚类中心，并将它们作为三元组损失计算中的正样本和负样本，以输入特征向量为锚点，将特征向量移动到聚类域中，同时对样本进行聚类。我们对模型训练批次中的所有特征向量同时进行向量化和应用该方法。在实现三元组损失时，我们采用了1的边界，并根据Balntas等人[3]的建议，将输入与负样本中心的距离与正样本与负样本中心的距离进行了交换。<br/>​  我们首先定义源模型<spanclass="math inline">\(\mathcal{M}_{s}=\mathcal{H}_{s}(\mathcal{F}_{s})\)</span>，其中<spanclass="math inline">\(\mathcal{H}_{s}\)</span>是模型的分类器部分，<spanclass="math inline">\(\mathcal{F}_{s}\)</span>是模型的特征部分，后者被调整以适应新领域。接着，我们定义目标模型<spanclass="math inline">\(\mathcal{M}_{t}=\mathcal{H}_{s}(\mathcal{F}_{t})\)</span>，这里我们使用源模型<spanclass="math inline">\(\mathcal{H}_{s}\)</span>是的相同分类器，但更新了源模型的特征部分以生成<spanclass="math inline">\(\mathcal{F}_{t}\)</span>。因此，源模型和目标模型具有完全相同的架构，但权重不同。<br/>​  在我们提出的聚类三元组损失函数中，我们从源数据的最佳聚类得到的聚类中心C出发。对于批量大小为b的输入目标数据<spanclass="math inline">\(x_t\)</span>，我们计算每个聚类中心与输入之间的欧氏距离<spanclass="math inline">\(d_{ij}\)</span>， <spanclass="math display">\[d_{i j}=\|x_{t_{i}}-C_{j}\|^{2},\]</span>​  其中，i∈[1，b]表示批次中的每个节点输入。<br/>​  我们利用这些距离来确定最近的（<spanclass="math inline">\(C_{j_{pos}}\)</span>）和最远的（<spanclass="math inline">\(C_{j_{neg}}\)</span>）聚类中心，使用 <spanclass="math display">\[j_{p o s_{i}}=\arg\operatorname*{min}_{j}d_{ij},\quad j_{n e g_{i}}=\arg\operatorname*{max}_{j}d_{i j}.\]</span>​  我们在调整后的三元组损失函数中使用这些正负聚类中心，如定义所示 <spanclass="math display">\[L_{i}(x_{t_{i}})=\operatorname*{max}\{||x_{t_{i}}-C_{j_{po s_{i}}}||^{2}-\|C_{j_{p o s_{i}}}-C_{j_{n eg_{i}}}||^{2}+\mu,0\}\]</span>​  使用边缘μ=1。<br/>​  最后，我们通过取批次的平均值来减少输出，并使用批次损失对模型进行反向传播。<spanclass="math display">\[L_{b}(x_{t};C,\mu)=\frac{1}{b}\sum_{i}L_{i}(x_{t_{i}},j_{po s_{i}},j_{n e g_{i}};C,\mu),\]</span>​  在簇中心C和边缘μ固定的情况下，<span class="math inline">\(j_{p os_{i}}\)</span>和<span class="math inline">\(j_{n eg_{i}}\)</span>会根据方程(1)和(2)的变化而变化。<br/>​  我们整个方法的算法详见算法1。步骤1-3只需执行一次，然后，给定源数据表示C，从步骤4开始，可以用于在不同领域训练任意数量的目标模型。</p><figure><imgsrc="../postimages/Cluster-Triplet-Loss-for-Unsupervised-Domain-Adaptation-on-Histology-Images/image-20250702204429106.png"alt="image-20250702204429106" /><figcaption aria-hidden="true">image-20250702204429106</figcaption></figure><hr /><p>算法1：使用聚类三元组损失进行训练</p><hr /><p>输入：源特征模型<spanclass="math inline">\({\mathcal{F}}_{s}\)</span>、源数据<spanclass="math inline">\(x_{s}\)</span>以及目标数据<spanclass="math inline">\(x_t\)</span>。<br/>1在分类前，从GNN的最终层中提取特征<spanclass="math inline">\({\mathcal{F}}_{s}(x_{s})\)</span>；<br/>2 对<spanclass="math inline">\({\mathcal{F}}_{s}(x_{s})\)</span>运行KMeans算法，设置k= 2，...，20个聚类，并通过轮廓宽度计算最优k值<spanclass="math inline">\(k_{opt}\)</span>；<br/>3 从最佳KMeans中提取<spanclass="math inline">\(k_{opt}\)</span>个聚类中心C；<br/>4 将源模型<spanclass="math inline">\({\mathcal{F}}_{s}\)</span>的权重应用于目标模型<spanclass="math inline">\({\mathcal{F}}_{t}\)</span>；<br/>5<strong>while</strong> <em>Training</em> <strong>do</strong><br/>6 |从目标模型中提取目标特征<spanclass="math inline">\({\mathcal{F}}_{t}(x_{t})\)</span>；<br/>7 |使用等式(1)计算<spanclass="math inline">\({\mathcal{F}}_{t}(x_{t})\)</span>与C中每个聚类中心之间的欧氏距离；<br/>8|使用等式(2)计算距离，找到与目标特征最接近（Cpos）和最远（Cneg）的聚类；<br/>9| 使用公式(3)和(4)，计算<spanclass="math inline">\(x_t\)</span>的平均三元组损失，并进行反向传播。<br/>10<strong>end</strong></p><p>输出：调整后的目标特征模型<spanclass="math inline">\({\mathcal{F}}_{t}\)</span></p><hr /><h1 id="实验">4.实验</h1><h2 id="数据">4.1.数据</h2><h2 id="结果">4.2.结果</h2><h2 id="与最先进水平sota的比较">4.3.与最先进水平（SOTA）的比较</h2><h2 id="消融研究">4.4.消融研究</h2><h1 id="讨论">5.讨论</h1><h2 id="当前限制与未来工作">5.1.当前限制与未来工作</h2><p>​  我们承认，我们的适应模型仅训练到特征提取阶段，这意味着用于从这些域转换特征预测结果的分类分支没有更新。由于我们将特征域转移到了原始源特征的域上，而现有的分类分支正是基于这些原始特征训练的，因此这部分模型应该无需进一步训练就能适应。然而，在这最后一步中，可能遗漏了一些有用的队列特定信息。<br/>​  正如我们在消融研究（第4.4节）中所展示的，找到源数据的最佳聚类是该方法取得最佳效果的关键。可以将这项工作扩展，测试这种方法是否能推广到多个目标群体。为了模拟实际应用，建议采用累积方法，在每个新的目标领域重新计算聚类中心，并评估这如何影响模型的适应性。此外，还可以探讨调整后的模型在原始源域中的表现变化。<br/>​  这种方法的效果在很大程度上取决于源数据中疾病变异对疾病空间的覆盖程度。如果我们确信源模型之前已经处理过特定的疾病变异，那么在调整特征时可以更加积极，同时对于异常值则可以采取较为保守的态度，引入某种加权异常检测方法。</p><h2 id="结论">5.2.结论</h2><p>​  我们提出了一种新方法，该方法利用图节点特征和源聚类中心，在聚类三元组损失函数中实现组织学深度学习模型的统一域适应。这种方法允许在全息切片图像（WSI）中进行局部域适应，使得一个目标图像中的不同组织切片不必以相同的方式‘移动’。<br/>​  尽管我们提出的方法并非完全无源，但只需要原始数据的密集表示，这不仅避免了存储内存密集型的数据集，而且在不同医院环境中实施时，能够保护患者数据的匿名性。该方法适用于多种结果类别，并可应用于多种深度学习和多实例学习（MIL）方法。</p><h2 id="致谢">5.3.致谢</h2>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 三联体损失 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>三元组损失</title>
      <link href="/%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1/"/>
      <url>/%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1/</url>
      
        <content type="html"><![CDATA[<h1 id="一什么是三元组损失">一、什么是三元组损失？</h1><p>三元组损失（TripletLoss）是深度学习中用于学习特征表示的重要损失函数，最初在FaceNet论文中提出，后被广泛应用于人脸识别、行人重识别（ReID）等任务。其核心思想是通过<strong>锚点样本（Anchor）</strong>、<strong>正样本（Positive）和负样本（Negative）</strong>的三元组，让同类样本的特征距离更近，不同类样本的特征距离更远。</p><h1 id="二代码结构解析">二、代码结构解析</h1><p>完整示例代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TripletLoss</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Triplet loss with hard positive/negative mining.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Reference:</span></span><br><span class="line"><span class="string">        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Imported from `&lt;https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py&gt;`_.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        margin (float, optional): margin for triplet. Default is 0.3.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, margin=<span class="number">0.3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TripletLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.margin = margin</span><br><span class="line">        <span class="variable language_">self</span>.ranking_loss = nn.MarginRankingLoss(margin=margin)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, targets</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).</span></span><br><span class="line"><span class="string">            targets (torch.LongTensor): ground truth labels with shape (num_classes).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n = inputs.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">&emsp;&emsp;<span class="comment">#步骤1：计算特征距离矩阵</span></span><br><span class="line">        <span class="comment"># Compute pairwise distance, replace by the official when merged</span></span><br><span class="line">        dist = torch.<span class="built_in">pow</span>(inputs, <span class="number">2</span>).<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>).expand(n, n)</span><br><span class="line">        dist = dist + dist.t()</span><br><span class="line">        dist.addmm_(inputs, inputs.t(), beta=<span class="number">1</span>, alpha=-<span class="number">2</span>)</span><br><span class="line">        dist = dist.clamp(<span class="built_in">min</span>=<span class="number">1e-12</span>).sqrt() <span class="comment"># for numerical stability</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For each anchor, find the hardest positive and negative</span></span><br><span class="line">        mask = targets.expand(n, n).eq(targets.expand(n, n).t())</span><br><span class="line">        dist_ap, dist_an = [], []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dist_ap.append(dist[i][mask[i]].<span class="built_in">max</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line">            dist_an.append(dist[i][mask[i] == <span class="number">0</span>].<span class="built_in">min</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line">        dist_ap = torch.cat(dist_ap)</span><br><span class="line">        dist_an = torch.cat(dist_an)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute ranking hinge loss</span></span><br><span class="line">        y = torch.ones_like(dist_an)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.ranking_loss(dist_an, dist_ap, y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="步骤1计算特征距离矩阵">步骤1：计算特征距离矩阵</h2><p><span class="math display">\[D_{i j}\,=\,{\sqrt{\vert\vertx_{i}\,-\,x_{j}\vert\vert^{2}}}\]</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Stacking Brick by Brick:Aligned Feature Isolation for Incremental Face Forgery Detection</title>
      <link href="/Stacking-Brick-by-Brick/"/>
      <url>/Stacking-Brick-by-Brick/</url>
      
        <content type="html"><![CDATA[<p>Stacking Brick by Brick: Aligned Feature Isolation for IncrementalFace Forgery Detection</p><p>Jikang Cheng1*, Zhiyuan Yan 2*, Ying Zhang3, Li Hao2, Jiaxin Ai1, QinZou1, Chen Li3, Zhongyuan Wang1†</p><p>武汉大学计算机学院1<br/>北京大学深圳研究生院电子与计算机工程学院2<br/>微信视觉，腾讯公司3</p><h1 id="摘要">摘要</h1><p>​  随着面部伪造技术的快速发展，出现了越来越多种类的伪造。增量式面部伪造检测（Incrementalface forgeryDetection，IFFD）通过逐步添加新的伪造数据来微调已训练的模型，被引入作为应对不断演变的伪造方法的一种有前景的策略。然而，一个未经充分训练的IFFD模型在处理新的伪造时容易出现灾难性遗忘。这是因为将所有伪造都视为单一的“假”类别，导致不同类型的伪造品相互覆盖，从而导致早期任务中独特特征的遗忘，限制了模型在学习伪造品特异性和普遍性方面的有效性。在本文中，我们提出了一种方法，通过将先前任务和新任务的潜在特征分布逐块堆叠，实现特征的对齐隔离。这样做的目的是保留已学习到的伪造信息，并通过最小化分布重叠来积累新知识，从而减轻灾难性遗忘。为了达到这一目标，我们首先引入了稀疏均匀回放（SUR），以获取可以视为先前全局分布的均匀稀疏版本的代表性子集。接着，我们提出了一个潜在空间增量检测器（LID），该检测器利用SUR数据来隔离和对齐分布。为了评估我们的方法，我们构建了一个更先进、更全面的基准测试，专门针对IFFD。实验结果证明了我们方法的优越性。代码可在https://github.com/beautyremain/SUR-LID上获取。</p><p><ahref="https://scisimple.com/en/articles/2025-05-19-incremental-face-forgery-detection-a-new-approach--a3ox7po">IncrementalFace Forgery Detection: A New Approach - Simple Science</a></p><h1 id="引言">1.引言</h1><p>​  随着人脸伪造技术的兴起，社会面临重大威胁，这引起了研究者们的高度关注，他们特别关注这些技术在身份盗用、虚假信息传播和隐私侵犯方面的潜在风险。因此，开发有效的检测方法对于保护个人安全和维护公众对数字互动的信任至关重要。现有的方法主要集中在使用有限的训练数据来训练通用的面部伪造检测器。然而，随着现实世界中面部伪造技术的日益多样化，仅依赖有限的训练数据期望通用模型能够有效检测所有类型的伪造，这显得有些不切实际[4]。与此同时，每当出现新的伪造品时，使用所有可用数据训练新模型会导致计算费用、存储限制和隐私影响的重大问题。因此，考虑到伪造数据量的不断增加，采用增量学习研究范式进行人脸识别检测可以解决更广泛的应用场景。<br/>​  迄今为止，只有少数方法探索了增量面部伪造检测（IFFD）[19,30,39,41]领域。这些方法通过多种重放策略来保留先前任务中的代表性信息，例如选择中心样本和困难样本[30]、生成代表性的对抗性扰动[39]以及考虑混合原型[41]。然而，由于IFFD始终致力于学习同一简单的二分类任务，骨干提取器更容易随意用新生成的特征覆盖前一任务的全局特征分布。这种情况下，IFFD中的灾难性遗忘问题尤为突出。尽管目前的方法提出了多种重放和正则化策略，但这些方法主要集中在保留一些特定的代表性样本（例如DFIL[30]中的中心样本和困难样本），并保持这些样本的特征一致性。因此，这些方法难以维持和组织之前学习到的全局特征分布，从而难以缓解分布覆盖的问题。<br/>​  在本文中，我们提出了一种方法，通过在潜在空间中逐块堆叠先前任务和新任务的特征分布，实现特征的对齐隔离。如图1所示，我们用“砖块”来描述这些特征分布，因为它们被设计为相互独立，而不是相互覆盖。而“逐块”则意味着逐步将新任务的二元决策边界与所有先前任务的边界对齐。实施‘逐块堆叠’方法的优势体现在两个方面。首先，通过特征隔离，可以减少新旧领域之间的特征分布重叠，从而更好地保留从前任务中获得的知识。其次，逐个决策对齐确保了在增量学习过程中，能够有效利用累积的多样化伪造信息，进行最终的二进制面部伪造检测。<br/>​  为了实现特征的对齐隔离，我们提出了一种新的IFFD方法，称为SUR-LID。具体来说，为了对齐和隔离所有特征分布，一个先决条件是获取能够代表先前全局分布的重放子集。因此，我们首先提出了一个基于稳定性和分布密度选择重放样本的稀疏均匀重放（SUR）策略。SUR子集的分布可以视为原始全局分布的均匀稀疏版本。借助SUR算法保持的分布特性，我们提出了一种潜在空间增量检测器（LID），以实现特征的对齐隔离。该检测器通过隔离损失来隔离每个分布，并通过重新填充分布的方式进一步增强效果，从而基于SUR数据恢复并模拟之前的全局分布。随后，引入了增量决策对齐机制，确保新任务的决策边界与所有先前任务保持一致。此外，我们还设计了两种精心设计的增量协议，以提升IFFD性能的实验评估。初步结果证明了所提方法的优越性。我们的贡献可以概括为：</p><ul><li>我们提出在潜在空间中，将先前任务与新任务的特征分布逐层叠加，实现特征对齐隔离。这种方法既能缓解特征覆盖问题，又能有效整合学习到的多样化伪造信息，从而提升面部伪造检测性能。</li><li>为了实现特征的对齐隔离，我们引入了SUR来存储之前的全局分布和LID，并利用SUR数据来实现特征的隔离和对齐。</li><li>我们精心构建了一个新的综合评估IFFD的基准，包括多种最新的伪造方法和两种与实际应用相关的协议。</li></ul><h1 id="背景">2.背景</h1><h2 id="iffd的初步情况">2.1 IFFD的初步情况</h2><p>​  训练范式<br/>​  在增量学习中，新数据会依次引入，以微调已经基于先前任务训练好的模型，而完整的先前数据则无法访问[8]。与从头开始使用所有可用数据重新训练模型相比，这种范式能够逐步利用新数据，同时减少计算开销和存储需求。</p><p>​  研究目的<br/>​  在[39]的基础上，我们旨在解决增量学习中的灾难性遗忘问题，即在增加新任务时，模型对先前学习任务的性能可能会显著下降，也就是遗忘已学知识。</p><p>​  重放集<br/>​  重放集指的是从已学习的训练集中存储一小部分数据。它在几乎不增加额外存储开销的情况下，能够显著提升模型保留先前学习知识的能力，同时为增强增量学习提供了设计灵活性。</p><h2 id="面部伪造检测器">2.2 面部伪造检测器</h2><p>​  现有的方法主要集中在检测器的泛化上，以应对面部伪造带来的严重威胁。例如，鉴于检测器中观察到的模型偏差，已经提出了多种方法[5,23,44]来减轻伪造样本中存在的模型偏差。在潜在空间中，也有方法[4,46]研究特征组织和融合，以挖掘和多样化伪造信息，从而提高通用伪造检测器的泛化能力。这些方法[4-6,13,15,23,24,44,46]旨在从有限的已知数据中提取通用伪造信息，并在少量未知数据中展现出良好的性能。<br/>​  然而，考虑到面部伪造技术的快速发展，仅凭有限的可见数据来训练理想的通用检测器是不切实际的。因此，增量学习模式可能成为适应多样化和不断演变的伪造技术的更优选择。</p><h2 id="增量式人脸伪造检测">2.3 增量式人脸伪造检测</h2><p>​  增量学习方法被广泛研究，主要分为参数隔离[8]、参数正则化[1,20,22]和数据重放[26,31]。然而，专注于构建有效框架以实现增量面部伪造检测的方法却不多。CoReD[19]通过蒸馏损失来保留先前任务的知识，而DFIL[30]则通过使用中心样本和困难样本进行重放，进一步增强了这一功能。HDP[39]将通用对抗扰动（UAP [28]）作为重放机制，用于获取早期任务的知识。DMP[41]利用混合原型创建重放集，以封装先前的任务。<br/>​  尽管现有的方法能够重放并维护来自少数代表性数据（如中心样本和困难样本）的知识，但它们无法维持和组织先前任务与新任务的全局分布。因此，先前的全局分布经常被新任务的分布所覆盖，导致遗忘问题，以及对伪造特异性和普遍性的学习不足。</p><h1 id="方法论">3.方法论</h1><h2 id="对齐特征隔离的原理">3.1.对齐特征隔离的原理</h2><p>​  在训练过程中，骨干提取器学会了将图像空间的输入映射到潜在空间中的代表性特征（即图像-特征映射）。因此，提取出的特征的全局分布能够反映出骨干提取器从训练任务中学到的知识。如果覆盖了之前的分布，可能会破坏之前学到的图像-特征映射，从而导致遗忘先前任务中的知识。此外，研究表明，潜在空间的组织对于模型的有效性至关重要[4,7,11]。现有的方法[19,30,39,41]虽然能够保留少数代表性数据点，但只能在这些特定点上保持性能，而无法维持全局分布。同时，在不保留全局分布的情况下，组织先前和新任务的潜在空间也是一项挑战。<br/>​  因此，我们提出了一种对齐特征隔离的方法，以通过三个步骤来改进IFFD：<br/>​  1)存储能够代表全局分布而非仅限于少数特定点的重放子集。<br/>​  2)隔离每个任务的全局分布，以减少覆盖，从而允许逐步积累更加多样化的伪造信息。<br/>​  3)利用通过决策对齐从隔离中获得的累积伪造信息，从而增强最终的二进制面部伪造检测。</p><h2 id="总体框架">3.2.总体框架</h2><p>​  本文提出了一种针对IFFD的对齐特征隔离方法，该方法包含两个核心组件：一种名为稀疏均匀重播（SUR，SparseUniformReplay）的重播策略，以及一个名为潜在空间增量检测器（LID，Latent-spaceIncrementalDetector）的检测模型。在完成某一任务的训练后，我们使用SUR来存储数据。随后，将SUR数据与下一个训练集合并，以训练LID进行增量面部伪造检测。整个框架如图2所示。</p><figure><imgsrc="../postimages/Stacking-Brick-by-Brick/image-20250702160852295.png"alt="image-20250702160852295" /><figcaption aria-hidden="true">image-20250702160852295</figcaption></figure><h2id="稀疏均匀重放sursparse-uniform-replay">3.3.稀疏均匀重放(SUR，SparseUniform Replay)</h2><p>​  为了实现所提出的对齐特征隔离，一个关键的前提是在增加新的（t+1）任务时，需要参考前t个任务的全局特征分布。因此，如图3所示，我们提出了稀疏均匀重放（SUR）策略，该策略旨在从先前的训练集中选择具有高维均匀性的稳定表示。（稳定表示指的是当输入中的无关内容发生[35,48]变化时，所提取的特征保持一致。）</p><figure><imgsrc="../postimages/Stacking-Brick-by-Brick/image-20250702211733301.png"alt="image-20250702211733301" /><figcaption aria-hidden="true">image-20250702211733301</figcaption></figure><p>​  具体来说，保持重播集的均匀性可以使其更接近全局分布，而不仅仅是原始分布中的局部区域。同时，通过采样这些稳定提取的特征，可以减少重播集中包含异常值的风险。<br/>​  考虑到一个任务通常包含真实和虚假的领域，为了简化符号表示，我们使用<spanclass="math inline">\(\mathbf{F}^{t}\in\mathbb{R}^{n\timesd}\)</span>和<spanclass="math inline">\(\mathbf{X}^{t}\in\mathbb{R}^{n\times 3\timesw\timesh}\)</span>来表示特征的一个特定领域及其对应的图像，这些图像在第t个任务中可能是真实的或虚假的。其中，n表示样本数量，d表示特征的维度，w和h分别表示图像的宽度和高度。对于第t个任务<spanclass="math inline">\({\mathcal{E}}^{t}\)</span>的训练好的骨干提取器，<spanclass="math inline">\(\mathbf{F}^{t}\)</span>可以通过<spanclass="math inline">\(\mathbf{F}^{t}={\mathcal{E}}^{t}(\mathbf{X}^{t})\)</span>生成。首先，我们利用质心作为参考，均匀地从重放集中采样，计算方法为<spanclass="math inline">\(\mathbf{c}^{t}=avg(\mathbf{F}^{t})\in\mathbb{R}^{d}\)</span>。在高维特征空间中均匀采样时，需要同时考虑幅度和角度。具体来说，从<spanclass="math inline">\(\mathbf{c}^{t}\)</span>到<spanclass="math inline">\(\mathbf{F}^{t}\)</span>中每个特征的<strong>幅度</strong>可以表示为：<span class="math display">\[{\mathbfM}^{t}=\|\mathbf{F}^{t}-\mathbf{c}^{t}\|_{2},\]</span> ​  其中<spanclass="math inline">\(\|\ast\|_2\)</span>表示计算欧几里得范数。随后，高维<strong>角矩阵</strong><spanclass="math inline">\(\mathbf{A}^{t}\)</span>可按以下方式计算： <spanclass="math display">\[\mathbf{A}^{t}={\frac{(\mathbf{F}^{t}-\mathbf{c}^{t})}{||\mathbf{F}^{t}-\mathbf{c}^{t}||_{2}}}.\]</span>​  随后，我们利用无论是否打乱都存在的一致性[5,29,38]来量化学习到的表示的稳定性。也就是说，由于伪造信息主要是细粒度的，并且不受打乱的影响，因此无论是否打乱都存在，伪造特征应该是一致的[5,29,38]。因此，我们对<spanclass="math inline">\(\mathbf{X}^{t}\)</span>执行网格洗牌[2]操作生成<spanclass="math inline">\(\mathbf{\tildeX}^{t}\)</span>，从而获得洗牌数据的特征<spanclass="math inline">\(\mathbf{\tildeF}^{t}={\mathcal{E}}^{t}(\mathbf{\tildeX}^{t})\)</span>。因此，稳定性矩阵<spanclass="math inline">\(\mathbf{S}^{t}\)</span>中的第i个元素<spanclass="math inline">\((s_i^t)\)</span>是通过使用<spanclass="math inline">\(\mathbf{\tilde F}^{t}\)</span>和<spanclass="math inline">\(\mathbf{F}^{t}\)</span>的第i个特征（<spanclass="math inline">\(\tilde f_i^t\)</span>和<spanclass="math inline">\(f_i^t\)</span>）来计算得出的： <spanclass="math display">\[s_{i}^{t}={\frac{\tildef_i^t\cdot(f_i^t)^{\mathrm{T}}}{\|\tildef_i^t\|_{2}\cdot\|f_i^t\|_{2}}},\]</span>​  其中上标T表示转置矩阵。直观来说，为了获得统一且稳定的表示，这三个因素（即<spanclass="math inline">\(\mathbf{M}^{t}\in\mathbb{R}^{n}\)</span>、<spanclass="math inline">\(\mathbf{A}^{t}\in\mathbb{R}^{n\timesd}\)</span>和<spanclass="math inline">\(\mathbf{S}^{t}\in\mathbb{R}^{n}\)</span>)应同时考虑以获得均匀和稳定的表示。然而，实现一个理想的策略需要高维线性规划，它乘法考虑所有三个矩阵来决定最佳的重放集，从而导致不可接受的复杂计算。在此，我们提出了一种近似算法，该算法能够识别每个矩阵段内的局部最优数据点，并将所有三个因素以加法方式综合考虑，从而显著降低计算量。具体来说，设每个域的重放集大小为<spanclass="math inline">\(n_r\)</span>，我们首先根据幅度距离<spanclass="math inline">\(\mathbf{M}^{t}\)</span>将<spanclass="math inline">\(\mathbf{F}^{t}\)</span>按升序排列。接着，我们将<spanclass="math inline">\(\mathbf{F}^{t}\)</span>分割成<spanclass="math inline">\(\frac{n_r}{2}\)</span>个等长的段，每个段的长度为<spanclass="math inline">\(\mathbf{F}^{t}=\{\mathbf{F}^{t}_{1:\frac{2n}{n_r}},...,\mathbf{F}^{t}_{(n-\frac{2n}{n_r}):n}\}\in\mathbb{R}^{\frac{n_r}{2}\times\frac{2n}{n_r}\times d}\)</span>。在每段内，我们根据<spanclass="math inline">\(\mathbf{S}^{t}\)</span>识别出最稳定的特征<spanclass="math inline">\(f_s^t\)</span>，并将其对应的图像<spanclass="math inline">\(x_s^t\)</span>纳入重放集。为了同时考虑角度的均匀性（即<spanclass="math inline">\(\mathbf{A}^{t}\)</span>)，我们在每个段中寻找与<spanclass="math inline">\(f_s^t\)</span>具有最低归一化余弦相似度的特征，称为<spanclass="math inline">\(f_a^t\)</span>。随后，我们可以从所有段中选择n个<spanclass="math inline">\(f_s^t\)</span>和<spanclass="math inline">\(f_a^t\)</span>。这些特征对应的图像被存储起来，构成一个领域（真实或假）的第t个重放集。补充材料中提供了SUR算法的简要总结。</p><h2id="潜空间增量探测器lidlatent-space-incremental-detector">3.4.潜空间增量探测器（LID，Latent-spaceIncremental Detector）</h2><p>​  我们提出了一种潜在空间增量检测器（LID），该检测器在潜在空间中逐块堆叠先前和新的任务。LID包含两个关键要素：特征隔离和增量决策对齐。</p><h3 id="分配再填充下的特征隔离">3.4.1分配再填充下的特征隔离</h3><p>​  在此，我们旨在分离每个真实/伪造和先前/新领域的分布，并减轻覆盖以保留知识并积累从新任务和先前任务中学习到的伪造信息。</p><p>​  <strong>分配再填充(DR，DistributionRe-filling)</strong><br/>​  为了进一步促进不同分布的隔离，我们提出利用SUR稀疏均匀重放集的稀疏均匀性，在重放数据点和质心之间重新填充潜在空间分布。具体来说，由于SUR可以被看作是先前全局分布的均匀稀疏子集，因此SUR稀疏均匀重放特征与质心之间的空间也应属于相同的先前全局分布。因此，我们可以利用潜在空间混合来补充并进一步模拟之前的全局分布，从而增强特征隔离。所提出的分配再填充操作涉及来自同一重放集的两个随机特征（f₁和f₂）及其对应的质心(c)。该过程可表示为：<spanclass="math display">\[\mathbf{f}_{\mathrm{filled}}=\beta(\alpha\mathbf{f}_{1}+(1-\alpha)\mathbf{f}_{2})+(1-\beta)\mathbf{c},\]</span>​  其中<span class="math inline">\(\alpha,\beta\ \in\[0,1]\)</span>为随机混合比例。通过这种方式，我们能够有效填充由顶点f₁、f₂和c构成的三角区域，从而在新任务训练时进一步促进特征隔离。通过这样做，我们可以有效地重新填充由顶点f1、f2和c形成的三角形区域，从而在新任务的训练中进一步促进特征隔离。</p><p>​  <strong>隔离损失</strong><br/>​  通过使用SUR稀疏均匀重放和再填充数据，我们可以引入监督对比损失[18]来区分真实/虚假和先前/新分布的每个特征域。具体而言，隔离损失可表示为：<span class="math display">\[{\cal L}_{i so}=-{\frac{1}{N}}\sum_{i=1}^{N}\log(\frac{\exp(\mathbf f_{i}\cdot\mathbff_{j}/\tau)}{\sum_{k=1}^{N}{\mathbb{I}_{[y_{i}\neq y_{k}]}\exp({\mathbff_{i}\cdot{\mathbf f_{k}/\tau}})}})\]</span> ​  其中，<spanclass="math inline">\(\mathbf f_{i}\)</span>和<spanclass="math inline">\(\mathbf f_{j}\)</span>是来自同一领域的特征。<spanclass="math inline">\(y_i\)</span>表示<spanclass="math inline">\(\mathbff_{i}\)</span>的领域标签，并为每个真实/虚假和先前/新领域分配一个唯一的值。<spanclass="math inline">\(\mathbb{I}_{[y_{i}\neqy_{k}]}\)</span>表示一个指示函数，当<spanclass="math inline">\(y_i\)</span>等于<spanclass="math inline">\(y_k\)</span>时，该函数值为1；否则为0。值得注意的是，如果这些特征来自新的任务，则它们可能是当前训练数据的特征；如果这些特征来自先前的任务，则它们可能是由SUR或重填充数据生成的。同时，为了促进对不同真实领域的学习，来自不同任务的真实数据也被分配了不同的唯一yi值。<br/>​  特征隔离机制通过阻止增量任务被先前任务覆盖，有效缓解了灾难性遗忘问题。同时，该机制促使骨干提取器能够区分各任务的不同领域，从而增强了其对各类伪造信息的敏感度。</p><h3 id="增量决策一致性">3.4.2增量决策一致性</h3><p>​  虽然特征隔离技术能有效降低特征覆盖效应并提升主干网络对伪造信息的敏感度，但如何直接从任务独立的特征域中提取最终的二值检测结果仍面临挑战。为此，我们提出增量决策对齐（IncrementalDecisionAlignment，简称IDA）方法，通过整合多类别独立特征中积累的伪造信息，为最终的二值检测结果提供有效支持。<br/>​  IDA的目标是使所有任务中每个独立真实/虚假领域的决策边界保持一致。通过这种方式，我们既能促进特征隔离，又能优化统一的决策边界，从而在最终检测中有效区分真实与虚假领域。对于对齐任务，首先需要针对每个任务分别训练并获取真实/伪造的个体边界。因此，我们首先为同一任务中的真实样本和伪造样本分配并维护独立的分类器。这些分类器可视为每个任务的独立决策边界。第t个任务的分类器记作<spanclass="math inline">\({\cal C^{t}}(*;\theta^{t})\)</span>，其中<spanclass="math inline">\(\theta^{t}\)</span>是<spanclass="math inline">\({\calC^{t}}\)</span>的参数。为确保所有任务保持一致性，只需将递增的<spanclass="math inline">\({\calC^{t+1}}(*;\theta^{t+1})\)</span>与前一个<spanclass="math inline">\({\calC^{t}}(*;\theta^{t})\)</span>进行对齐即可，从而实现所有任务的递归对齐。由于区分真实/伪造的分类器属于线性层，对决策边界的对齐等同于确保线性参数的角度一致性。因此，针对<spanclass="math inline">\({\calC^{t+1}}(*;\theta^{t+1})\)</span>的决策对齐优化步骤可形式化表述为：<spanclass="math display">\[\theta^{t+1}\leftarrow\left\|\theta^{t+1}\right\|_{2}\cdot\frac{(1-\gamma)\tilde{\theta}^{t+1}+\gamma\tilde{\theta}^{t}}{\left\|(1-\gamma)\tilde{\theta}^{t+1}+\gamma\tilde{\theta}^{t}\right\|_{2}},\]</span>​  其中<span class="math inline">\(\tilde{\theta}=\frac{\theta}{\|\theta\|_{2}}\)</span>，γ表示学习率。在训练第（t+1）个任务时，分类器<spanclass="math inline">\({\calC^{t+1}}\)</span>会按照公式等式6进行优化以与<spanclass="math inline">\({\calC^{t}}\)</span>保持一致，而所有先前的分类器则被冻结，以维持原有的决策边界及其对齐状态。</p><h2 id="训练和推理">3.5.训练和推理</h2><p>​  <strong>训练</strong><br/>​  在训练第（t+1）个任务时，将把第1到第t个重放缓冲集与第（t+1）个训练数据合并为<spanclass="math inline">\({\bf X}=\{\hat{\bf X}^{1},\hat{\bfX}^{2},...,\hat{\bf X}^{t},{\bfX}^{t+1}\}\)</span>。随后通过特征提取公式<spanclass="math inline">\(\mathbf{F}\ =\{\cal{E}}^{t+1}(\mathbf{X})\)</span>，可获得其特征向量<spanclass="math inline">\(\mathbf{F}=\{\hat{\mathbf{F}}^{1},{\hat{\mathbf{F}}}^{2},...,{\hat{\mathbf{F}}}^{t},\mathbf{F}^{t+1}\}\)</span>。根据知识蒸馏损失函数的定义，我们还通过以下方式保留前序任务学习的信息：<span class="math display">\[\mathcal{L}_{d is}\Longrightarrow\sum_{i=1}^{t}(\hat{\bf F}^{i}-\mathcal{L}^{t}(\hat{\bfX}^{i}))^{2}\]</span> ​  需要说明的是，<spanclass="math inline">\({\cal{E}}^{t}\)</span>是基于前t次任务训练得到的冻结主干提取器。随后，我们通过引入带分配再填充的隔离损失函数（<spanclass="math inline">\({\calL}_{iso}\)</span>）来实现特征隔离。最终，二值检测损失函数可表示为：<span class="math display">\[\mathcal{L}_{d et}=\sum_{i=1}^{t}\mathrm{C}\mathrm{E}(\mathcal{C}^{i}(\hat{\mathrm{F}}^{i}),{\bfY}^{i})+\mathrm{C}\mathrm{E}(\mathcal{C}^{t+1}(\mathrm{F}^{t+1}),{\bfY}^{t+1}),\]</span> ​  其中，CE表示交叉熵损失函数，<spanclass="math inline">\({\bfY}^{t}\)</span>是第t个任务的二元检测标签。因此，整体损失函数可表示为：<spanclass="math display">\[\mathcal{L}_{\mathrm{overal}}=\mathcal{L}_{i so}+\mu_{1}\mathcal{L}_{d i s}+\mu_{2}\mathcal{L}_{d e t},\]</span>​  其中μ₁和μ₂是权衡参数。在通过反向传播优化<spanclass="math inline">\(\mathcal{L}_{\mathrm{overal}}\)</span>后，我们应用等式6来优化对齐的决策边界。</p><p>​  <strong>推理</strong><br/>​  在推理期间，输入图像x首先被<spanclass="math inline">\(\calE\)</span>处理为特征f。由于在实际应用的推理过程中，x的具体任务是未知的，因此我们无法确定具体的分类器用于推理。考虑到所有分类器都具有对齐的决策边界，我们采用它们的平均检测结果作为最终的推理结果，其表达式可表示为：<spanclass="math display">\[y_{\mathrm{infer}}=\sum_{i=1}^{t+1}\frac{C^{i}({\bff})}{t+1}.\]</span></p><h1 id="实验结果">4.实验结果</h1><h1 id="结论">5.结论</h1><p>​  在本文中，我们提出了一种新的对齐特征隔离技术，旨在提升增量面部伪造检测（IncrementalFace ForgeryDetection，IFFD）的性能。具体而言，我们将当前任务与之前任务的特征分布‘逐块’叠加，以减轻全局分布的覆盖效应，积累多样化的伪造信息，从而解决灾难性遗忘问题。随后，我们引入了一种新的稀疏均匀重播（SparseUniform Replay，SUR）策略和潜在空间增量检测器（Latentspace IncrementalDetector，LID），以实现对齐特征隔离。通过在新的高级IFFD评估基准上的实验，我们显著证明了所提出方法的优越性。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 三联体损失 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ClusterFomer:Clustering As A Universal Visual Learner</title>
      <link href="/clusterformer/"/>
      <url>/clusterformer/</url>
      
        <content type="html"><![CDATA[<p>ClusterFomer: Clustering As A Universal Visual Learner</p><p><strong>James C. Liang</strong>（罗彻斯特理工学院）、<strong>YimingCui</strong>（ 佛罗里达大学）、<strong>Qifan Wang</strong>（MetaAI）、<strong>Tong Geng</strong>（美国罗彻斯特大学）、<strong>WenguanWang</strong>（ 浙江大学）、<strong>DongfangLiu</strong>∗（罗彻斯特理工学院）</p><h1 id="摘要">摘要</h1><p>​  本文介绍了一种基于CLUSTERing范式与TransFORMER的通用视觉模型——CLUSTERFORMER。该模型包含两个创新设计：①循环交叉注意力聚类，重新定义了TransFORMER中的交叉注意力机制，通过递归更新聚类中心，促进强大的表示学习；②特征调度，利用更新后的聚类中心，通过基于相似性的度量重新分配图像特征，形成一个透明的处理流程。这种优雅的设计简化了可解释和可转移的工作流程，能够处理不同层次的聚类粒度（即图像、框和像素级别）的异构视觉任务（即图像分类、对象检测和图像分割）。实证结果表明，CLUSTERFORMER在图像分类任务中，对ImageNet-1K数据集的top-1准确率达到了83.41%；在对象检测和实例分割任务中，对MSCOCO数据集的mAP分别达到了54.2%和47.0%；在语义分割任务中，对ADE20K数据集的mIoU达到了52.4%；在全景分割任务中，对COCOPanoptic数据集的PQ达到了55.8%。我们希望通过这项研究，能够推动计算机视觉领域通用模型的范式转变。</p><h1 id="引言">1.引言</h1><p>​  计算机视觉领域已经出现了针对不同视觉任务的专门解决方案（例如，ResNet[34]用于图像分类，Faster RCNN [70]用于目标检测，Mask RCNN[33]用于实例分割），旨在实现更优的性能。然而，神经科学研究[73,65,82,5]指出，人类的感知系统在处理复杂视觉刺激时展现出卓越的解释能力，不受特定任务的限制。这一特点与当前计算机视觉技术[95,44,46]的设计大相径庭，后者通常采用多样化的架构设计。<br/>​  人类视觉具有独特的注意力机制，能够有选择性地聚焦于视觉场中的相关部分，同时忽略无关信息[81,40]。这可以类比为一种聚类方法[2,3,89]，其中单个像素点被分解并重组为相关概念，以应对各种任务。这实际上是一个层次化的过程，通过组合基本的视觉元素，如线条、形状和颜色，来构建更高级别的物体、场景和个人的抽象[79,59,66,27]。这项工作受到人类视觉系统卓越能力的启发，旨在开发一个能够复制这种无与伦比能力的通用视觉模型。<br/>​  为此，我们采用了一种基于聚类的策略，该策略在不同粒度级别上运行，以提高视觉理解。通过解决不同的视觉任务（如图像分类、目标检测和图像分割），我们考虑了视觉信息在图像、框和像素级别的具体分组方式。我们将这种方法命名为CLUSTERFORMER（第3.2节），因为它利用了集成在TransFORMER架构中的聚类机制，创建了一个通用网络。该方法首先将图像嵌入到离散的标记中，这些标记代表了被分组成不同集群的关键特征。然后，通过一个递归的聚类交叉注意力机制，考虑中心维度上的相关特征表示，递归地更新集群中心。一旦完成中心分配和更新，特征将根据更新后的集群中心进行重新分配，并将两者输入到目标任务的头部。</p><figure><img src="../postimages/clusterformer/image-20250701153931883.png"alt="image-20250701153931883" /><figcaption aria-hidden="true">image-20250701153931883</figcaption></figure><p>​  CLUSTERFORMER具有几个吸引人的特点。<br/>​  ❶灵活性：CLUSTERFORMER是一种基于聚类的方法，能够在一个框架下处理广泛的视觉任务，并展现出卓越的性能（见图1）。其核心理念是处理不同粒度级别的任务（如图像级分类、框级检测、像素级分割等），旨在实现一种通用的视觉解决方案。<br/>​  ❷可转移性：CLUSTERFORMER编码器生成的聚类中心直接作为任务头的初始查询，用于聚类，从而使整个架构能够将底层表示用于目标任务的预测（见表4）。这种设计巧妙地促进了从上游任务（即在ImageNet[72]上训练的编码器）学到的知识向下游任务（例如，在COCO[49]上训练的实例分割解码器）的迁移。<br/>​  ❸可解释性：无论目标任务如何，CLUSTERFORMER的决策过程都通过一个透明的管道，持续根据基于相似性的度量更新聚类中心。由于推理过程是自然可推导的，因此模型的推理行为可以即时解释（见第4.2节）。这使得CLUSTERFORMER与大多数现有统一模型[17,44,95]区别开来，后者往往无法精确解释其工作原理。</p><p>​  为了有效评估我们的方法，我们在实验中展示了：在第4.1.1节中，针对图像分类任务，CLUSTERFORMER在从头训练时，与SwinTransformer[53]相比，在ImageNet[72]数据集上的前1名准确率提高了0.13∼0.39%，在第4.1.2节中，使用我们预训练的ImageNet模型时，该方法可以扩展到目标检测任务，并且在COCO[49]数据集上，与Dino [96]相比，SwinTransformer的性能显著提升（mAP提高了0.8∼1.1%）。此外，我们的方法还可以适应更通用的像素级任务，如语义分割（见第4.1.3节）、实例分割（见第4.1.4节）和全景分割（见第4.1.5节）。例如，我们在ADE20K[101]数据集上的语义分割mIoU提高了0.6∼1.3%，在MS COCO[49]数据集上的实例分割mAP提高了1.0∼1.4%，在COCO Panoptic[42]数据集上的全景分割PQ提高了1.5∼1.7%，与Mask2Former[17]相比。我们的算法经过了广泛的测试，核心组件的有效性也通过第4.2节中概述的一系列消融研究得到了验证，</p><h1 id="相关工作">2.相关工作</h1><p>​  通用视觉模型<br/>​  Transformers[81]在推动全球雄心方面发挥了关键作用，通过相同的架构能够应对不同具体任务的模型，体现了这些最新发展[23,17,16,95,96,4,80,30,57,86]在该领域的潜力。在视觉领域，主流研究主要集中在开发编码器[53,88]或解码器[44,94]上。编码器的核心在于开发基础模型[4,53,24,22]，这些模型经过大量数据训练，能够适应并微调以应对各种下游任务。例如，SwinTransformer[53]通过采用由移位窗口组成的分层结构，成为计算机视觉领域的通用骨干；ViT-22B[22]将架构参数化至220亿，并通过学习大规模数据，在多种视觉任务上表现出色。通过学习大规模数据，该方法在多种视觉任务中表现出色。相反，[23,17,16,95,94,44,96,87,50,20,52,19,21,51,93,76,37,99,25,48]的解码器研究旨在解决同质目标任务，通过使用查询来描绘视觉模式。例如，Mask2Former[17]将掩码信息融入Transformer架构，统一了各种分割任务（如语义分割、实例分割和全景分割）；Mask-DINO[44]通过直接利用查询嵌入进行目标任务预测，将解码过程从检测扩展到分割。概念上不同，我们简化了一个基于聚类的优雅的系统工作流，并在不同的聚类粒度下处理异构视觉任务（例如，图像分类、对象检测和图像分割）。</p><p>​  视觉中的聚类<br/>​  传统视觉[39,28,29,55,91,1,10,61,6,58]中的聚类算法可分为分层模式和分割模式。分层方法[62,38]涉及像素层次结构的建模，并通过迭代分割和合并像素对形成聚类，直至达到饱和状态。这种方法避免了事先确定聚类数量的必要性，并解决了局部最优问题[98,12]。然而，它仅考虑每个阶段的相邻像素，无法利用关于聚类全局配置或维度的先验信息[69,64]。相比之下，部分聚类算法[78,36]直接生成具有预设数量的聚类结构，并将像素单独分配给一个聚类。这种设计展现出动态特性，使像素能够在簇[11,63]之间转换。通过采用适当的措施，这种方法能够有效地将复杂知识整合到簇中心。作为强大的系统，人类视觉融合了聚类模式[89,83,67]的优势。我们能够根据不同的尺度对相似的实体进行分组。同时，我们还能仅基于形状、颜色或纹理对物体进行有效分类，而无需依赖层次信息。基于上述见解，我们从聚类的角度重新设计了Transformer架构中的注意力机制（§3.2），以解析视觉复杂性的层次结构。</p><h1 id="方法">3.方法</h1><h2 id="前言">3.1前言</h2><p>​  聚类<br/>​  聚类的目标是将一组数据点（记为X∈Rn×d）根据它们的内在相似性划分为C个不同的簇，同时确保每个数据点仅属于一个簇。为了实现这一目标，需要优化数据点的分层，综合考虑其特征和位置信息，以形成连贯且有意义的分组。聚类方法通常采用高级相似性度量，如余弦相似性，来评估数据点与簇中心的距离。此外，这些方法还会考虑点的空间位置，以实现更精确的分组。</p><p>​  通用聚类的交叉注意力机制<br/>​  从Transformer解码器架构[81]中获得灵感，现代端到端架构[17,9]采用了一种基于查询的方法，其中一组K个查询，<spanclass="math inline">\(C=\left[c_{1};\cdot\cdot\cdot;c_{K}\right]\in\mathbb{R}^{K\timesD}\)</span>，通过一系列交叉注意力块进行学习和更新。在此背景下，我们重新定义了“C”这一术语，将其与每一层的聚类中心关联起来。具体而言，每层都使用交叉注意力机制来自适应地聚合图像特征，并随后更新查询：<span class="math display">\[C\leftarrow C+\mathrm{softmax}_{HW}(Q^{C}(K^{I})^{\top})V^{I},\]</span> ​  其中，<spanclass="math inline">\(Q^{C}{\in}\mathbb{R}^{K\timesD},V^{I}{\in}\mathbb{R}^{H W\times D},K^{I}{\in}\mathbb{R}^{H W\timesD}\)</span>分别表示查询、键和值的线性投影特征。上标“<spanclass="math inline">\(C\)</span>”和“<spanclass="math inline">\(I\)</span>”分别代表从中心和图像特征投影出的特征。受[95]的启发，我们重新解释了交叉注意力机制，将其视为聚类求解器，通过将查询视为聚类中心，并沿查询维度(K)而非图像分辨率（HW）应用softmax函数来实现：<span class="math display">\[C\leftarrowC+\mathrm{softmax}_{K}(Q^{C}(K^{I})^{\top})V^{I},\]</span></p><h2 id="clusterformer">3.2 CLUSTERFORMER</h2><p>​  在本小节中，我们介绍CLUSTERFORMER（见图2(a))。该模型通过一系列分层阶段，实现了多尺度表示学习，以适应各种场景。在每个阶段，图像块被转换为特征嵌入[81,53,24]，这些嵌入通过一个统一的流程——首先是循环交叉注意力聚类，然后是特征调度——被分组到不同的簇中。</p><p><img src="../postimages/clusterformer/image-20250701155045781.png"alt="image-20250701155045781" /> <spanclass="math display">\[\begin{array}{rrl}E-step:&amp;\hat{M}^{(t)}&amp;=\mathrm{softmax}_{K}(Q^{C^{(t)}}(K^{I})^{\top}),\\M-step:&amp;C^{(t+1)}&amp;=M^{(t)}V^{I}\in\mathbb{R}^{K\timesD},\end{array}\]</span></p><h3 id="循环交叉注意力聚类">3.2.1 循环交叉注意力聚类</h3><p>​  考虑到特征嵌入<span class="math inline">\(I~\in~\mathbb{R}^{HW\times D}\)</span>和初始中心<spanclass="math inline">\(C^{(0)}\)</span>，我们封装了迭代的期望最大化（EM，Expectation-Maximization）聚类过程，其中<spanclass="math inline">\(t\in\{1,\cdot\cdot\cdot,T\}\)</span>，<spanclass="math inline">\({\hat{M}}\in[0,1]^{K\times HW}\)</span>表示“软”聚类分配矩阵（即K个聚类的概率分布图）。根据第3.1节的定义，<spanclass="math inline">\(Q^{C}\in\mathbb{R}^{K\timesD}\)</span>表示从中心C投影出的查询向量，而<spanclass="math inline">\(V^{I},K^{I}\ \in\ \mathbb{R}^{H W\timesD}\)</span>则分别对应从图像特征<spanclass="math inline">\(I\)</span>投影出的值向量和键向量。循环交叉注意力方法通过迭代更新聚类成员<spanclass="math inline">\(\hat M\)</span>（即E-step）和中心<spanclass="math inline">\(C\)</span>（即M-step），这种动态更新策略体现了分区聚类的核心思想。该方法具有以下几个吸引人的特点：</p><ul><li><p>效率：虽然传统的自注意力机制的时间复杂度为<spanclass="math inline">\({\calO}(H^{2}W^{2}D)\)</span>，但循环交叉注意力方法的下限为<spanclass="math inline">\({\calO}(TKHWD)\)</span>。这主要是因为TK≪HW（例如，Swin[53]中的4165对比我们方法中的1200）。具体来说，在编码过程中，金字塔架构[88,53]的特性使得TK可以远小于HW，尤其是在早期阶段。值得注意的是，在每次迭代中，只有Q矩阵需要更新，而K和V矩阵只需进行一次计算。因此，整个模型具有系统效率（见表6c)。</p><figure><img src="../postimages/clusterformer/image-20250702151949092.png"alt="image-20250702151949092" /><figcaption aria-hidden="true">image-20250702151949092</figcaption></figure></li><li><p>透明：透明度的关键在于聚类中心在我们循环交叉注意力机制中所扮演的独特角色。通过聚类过程生成的聚类中心，作为特征的‘原型’。这些‘原型’作为每个聚类的代表性样本，反映了该聚类内数据点最显著或最具特征性的特点。此外，循环交叉注意力方法遵循了广泛认可的EM聚类算法，提供了一个清晰透明的框架。这种聚类中心分配在表示学习过程中以人类可理解的方式运作（见图3），促进了即时的可解释性，使人们能够更直观地理解潜在的关系。</p></li><li><p>非参数化的优雅：循环交叉注意力机制通过在迭代过程中共享查询、键和值的投影权重，实现了递归特性。这种方法有效地确保了递归性，而无需引入额外的学习参数（见表6b)。</p><figure><img src="../postimages/clusterformer/image-20250702152007163.png"alt="image-20250702152007163" /><figcaption aria-hidden="true">image-20250702152007163</figcaption></figure></li></ul><p>​  由于整体架构具有层次性，循环交叉注意力能够全面探索表示的粒度，这反映了层次聚类的过程：<spanclass="math display">\[C^{l}=\mathrm{RCA}^{l}(I^{l},C_{0}^{l}),\]</span>​  其中，RCA表示循环交叉注意力层。<spanclass="math inline">\(I^l\)</span>是通过标准池化操作在不同层上生成的图像特征图，分辨率为<spanclass="math inline">\(H/2^{l}\times W/2^{l}\)</span>。<spanclass="math inline">\(C^l\)</span>是第l层的聚类中心矩阵，而<spanclass="math inline">\(C_0^l\)</span>则是第l层的初始中心。不同层的循环交叉注意力参数<spanclass="math inline">\(\{\mathrm{RCA}^{l}\}_{l=1}^{L}\)</span>不共享。此外，我们从图像网格中初始化这些中心：<span class="math display">\[[c_{1}^{(0)};\cdot\cdot\cdot\cdotc_{K}^{(0)}]=\mathrm{FFN}(\mathrm{Adptive}_{-}\mathrm{Pooling}_{K}(I)),\]</span>​  其中，FFN代表位置前馈网络，这是Transformer架构的重要组成部分。它包含两个全连接层和一个用于隐藏层的激活函数。<spanclass="math inline">\(\mathrm{Adptive}_{-}\mathrm{Pooling}_{K}(I)\)</span>指的是从I个特征中心中选择K个特征中心，通过自适应采样计算出合适的窗口大小，以自适应地达到所需的输出大小，相比传统池化方法，这种方法提供了更高的灵活性和精度。</p><h3 id="特征调度">3.2.2 特征调度</h3><p>​  在完成聚类分配后，所提出的方法采用了一种自适应过程，根据相似性（见图2(c))来调度每个聚类内的补丁，从而对聚类内的整体结构和上下文形成更加连贯和全面的理解。对于每个补丁嵌入<spanclass="math inline">\(p_{i}\in I\)</span>，更新后的补丁嵌入<spanclass="math inline">\(p_{i}^{\prime}\)</span>计算如下： <spanclass="math display">\[p_{i}^{&#39;}=p_{i}+\mathrm{MLP}(\frac{1}{K}\sum_{k=0}^{K}si m(C_{k},p_{i})* C_{k})\]</span>​  该方程通过考虑特征嵌入与聚类中心(C)之间的相似性，并根据这些相似性的权重进行加权，来实现特征嵌入的自适应调度。通过利用聚类中心提供的内在信息，该方法能够优化特征嵌入，从而更好地理解图像的底层结构和背景。所有特征表示都被用于解码过程中的目标任务处理。在第3.3节中，我们将详细讨论结束任务的具体实现。</p><h1 id="实施细节">3.3实施细节</h1><p>​  CLUSTERFORMER的实现细节和框架如图2a所示。我们采用了Swin Transformer[53]的架构和配置，相关代码可在此处获取（https://github.com/ClusterFormer/ClusterFormer）。</p><ul><li>编码器<br/> 编码过程旨在为给定图像生成表示层次结构，用<spanclass="math inline">\(\{I^l\}\)</span>表示，其中<spanclass="math inline">\(l=\{1,2,3,4\}\)</span>。这一过程从特征嵌入开始，将图像转换成独立的特征标记。接下来，通过多头计算[81,53]技术，将这些嵌入的特征分配到不同的头部。中心初始化（公式5）作为初始化聚类中心的起点，而递归交叉注意力聚类（公式3）则用于递归地更新这些中心。一旦中心更新完成，特征将根据与更新后的中心的关联（公式6）进行重新分配。进一步的解码过程利用了中心和特征，确保了全面的学习效果。</li><li>图像分类适应<br/>分类头是一个单层多层感知机（MLP），它利用编码器的聚类中心进行预测。</li><li>检测和分割适应<br/>下游任务头包含六个Transformer解码器层，其核心设计为循环交叉注意力聚类（公式4）。每个层均执行三次迭代。</li></ul><h1 id="实验">4实验</h1><p>​  我们在四个基准上对我们的方法进行了五项视觉任务的评估，即图像分类、目标检测、语义分割、实例分割和全景分割。</p><p>​  用于图像分类的ImageNet-1K数据集<br/>​  ImageNet-1K[72]包含涵盖不同类别（如动物、植物和车辆）的高分辨率图像。按照常规流程，该数据集被划分为训练集、验证集和测试集，分别包含120万、5万和10万张图像。</p><p>​  用于目标检测和实例分割的MS COCO数据集<br/>​  COCO[49]数据集包含日常场景中80种常见物体的密集标注。按照[49]的标准做法，该数据集被划分为训练集（115,000张）、验证集（5,000张）和测试开发集（20,000张）。</p><p>​  用于语义分割的ADE20K数据集<br/>​  ADE20K[101]数据集提供了一个包含像素级标注的图像集合，涵盖了150个不同物体类别，涵盖室内和室外场景。该数据集分为训练集、验证集和测试集，分别包含20,000张、2,000张和3,000张图像。</p><p>​  用于全景分割的COCOPanoptic数据集<br/>​  COCO全景数据集[42]包含80个“物体”类别和53个“物品”类别的精心标注。按照[42]的标准做法，该数据集被划分为训练集、验证集和测试集，分别包含115,000张、5,000张和20,000张图像。</p><p>​  接下来的部分首先介绍了每个任务的主要结果（第4.1节），随后是一系列消融研究（第4.2节），旨在验证每种调节设计的有效性。</p><h2 id="主要结果">4.1主要结果</h2><h3 id="图像分类实验">4.1.1图像分类实验</h3><h3 id="目标检测实验">4.1.2目标检测实验</h3><h3 id="语义分割实验">4.1.3语义分割实验</h3><h3 id="实例分割实验">4.1.4实例分割实验</h3><h3 id="全景分割实验">4.1.5全景分割实验</h3><h2 id="消融研究">4.2消融研究</h2><h1 id="结论">5结论</h1><p>​  本研究采用了一种以聚类为基础的范式为视角的认识论方法，提出了一个名为CLUSTERFORMER的通用视觉框架。该框架旨在应对不同聚类粒度的多样化视觉任务。通过利用聚类的见解，我们为递归聚类定制了交叉注意力机制，并引入了一种新的特征调度方法。实证研究提供了大量证据，证明了这一系统方法的有效性。基于其有效性，我们推断，从聚类的角度来看，所提出的通用解决方案将对更广泛的视觉任务产生重大影响。这一问题仍是我们未来研究的重点。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 可微 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OmniGuard:Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking</title>
      <link href="/OmniGuard/"/>
      <url>/OmniGuard/</url>
      
        <content type="html"><![CDATA[<p>OmniGuard: Hybrid Manipulation Localization via Augmented VersatileDeep Image Watermarking</p><h1 id="摘要">摘要</h1><p>​  随着生成式AI的迅速发展及其在图像编辑领域的广泛应用，数字内容的真实性与完整性面临新的风险。现有的多种水印技术在篡改定位的精确度与视觉效果之间存在权衡。由于早期框架灵活性有限，这些水印在所有图像中必须保持固定。在AIGC编辑下，他们的版权提取准确度也不令人满意。为了解决这些问题，我们提出了一种新的增强型多功能水印技术——OmniGuard。该技术结合了主动嵌入与被动、盲目的提取方法，旨在实现强大的版权保护和篡改定位。OmniGuard采用了一种混合的取证框架，不仅支持灵活选择水印位置，还引入了一个退化感知的篡改检测网络，确保在复杂条件下也能实现精准的定位。此外，设计了一个轻量级的AIGC编辑模拟层，以增强全球和本地编辑的稳健性。广泛的实验表明，OmniGuard在保真度、稳健性和灵活性方面表现优异。与最近的最先进方法EditGuard相比，我们的方法在容器图像的PSNR（峰值信噪比）上提高了4.25dB，在噪声条件下的F1分数提高了20.7%，平均比特准确率提高了14.8%。</p><h1 id="引言">1.引言</h1><p>​  在海量数据集的支持和大规模模型技术的推动下，生成式AI已经赋能众多行业，开发了大量文本转图像模型[16,45,46]和图像编辑算法[7,32,40,65]。这种强大的生成和编辑能力是一把双刃剑，对信息的真实性和完整性构成了重大威胁。例如，非法侵权者可能会窃取他人生成的图像并声称拥有版权，这使得保护知识产权变得越来越困难。此外，欺诈者可能利用AI工具篡改在线图片，生成误导性内容，给法庭取证带来了新的挑战。<br/>​  作为一种经典且广泛采用的技术，水印技术因其在保护图像版权、检测未经授权的使用以及追踪篡改方面的重要作用而受到越来越多的关注。例如，强健的水印技术注重其韧性，旨在即使在显著的失真（如JPEG压缩[25]或屏幕截图[12]）后，也能保持嵌入的水印完整且可检测。相比之下，脆弱水印技术更注重敏感性，确保即使面对细微或特定的修改，嵌入的水印也能作出响应[27,34,35,44]。近年来，随着深度学习技术的发展，深度水印在容量[54]、鲁棒性[20]和保真度[26]方面展现出显著优势。它不仅能够抵御多种AI生成内容编辑方法，还能实现像素级别的篡改定位。值得注意的是，一些水印方法能够同时解决篡改定位和版权保护任务[23,27,49]。例如，AudioSeal[48]采用了零比特水印策略，实现了样本级音频篡改定位，并具备强大的位嵌入能力。SepMark[52]首次引入了用于稳健版权保护和深度伪造检测的深度可分离水印。EditGuard[66,68]利用图像嵌入图像隐写术的局部性，设计了一种串行编码和并行解码的水印框架。<br/>​  然而，现有的通用深度图像水印方法仍然面临一些挑战。<br/>​  1）准确度：<br/>​  多用途图像水印技术在篡改定位的准确性和水印图像的保真度之间面临不可避免的权衡。这是因为当前的方法，如[66,68,71]，依赖于预定义的定位水印与重建水印之间的残差来生成掩码。因此，为了确保定位的准确性，必须牺牲水印图像的保真度。<br/>​  2）灵活性：<br/>​  由于在解码阶段必须知道预定义的定位水印才能提取掩模，因此定位水印倾向于在所有图像中保持固定，这大大限制了信息嵌入的灵活性。<br/>​  3）鲁棒性：<br/>​  现有的多种水印技术在鲁棒性方面存在明显不足：局部水印恢复在严重退化条件下往往失效，而版权水印则可能被全局AIGC编辑算法清除。</p><p>​  为解决上述问题，我们提出了一种全面增强的通用水印方法，称为OmniGuard。<br/>​  首先，我们提出了一种结合主动水印嵌入与被动网络盲提取的混合取证方法。该方法能够在没有预先添加水印的情况下，从重建的局部水印中输出篡改的掩码，实现了编码与解码的分离。同时，我们训练了这种掩码提取器，即使在重建不准确的情况下也能实现精确提取，从而使水印网络能够专注于提高水印图像的质量。<br/>​  此外，我们还研究了局部水印的选择模式，并设计了一种自适应的水印变换，使网络能够以内容感知的方式嵌入局部信息。<br/>​  最后，我们提出了一种轻量级的AIGC编辑模拟层，该层提高了网络在版权提取方面的准确性，确保了在全球性编辑（如Instructpix2pix[7]）和局部修复（如StableDiffusionInpaint[46]）等场景下的稳健性能。<br/>​  图1展示了我们在最先进的多功能水印技术[66]上的显著改进。</p><figure><img src="../postimages/OmniGuard/image-20250701104337419.png"alt="image-20250701104337419" /><figcaption aria-hidden="true">image-20250701104337419</figcaption></figure><p>​  简而言之，我们的贡献可以概括如下。</p><ul><li>我们提出了一种混合操作定位和鲁棒版权保护框架OmniGuard，它包括一个主动的双水印网络和一个被动的提取器，改进了现有的多功能水印。</li><li>为了将主动水印与被动提取整合到一个统一的框架中，提出了一种深度退化感知的篡改提取器，该提取器融合了重建的局部水印与被篡改图像中的伪影，在严重退化条件下实现了更高的精度。</li><li>为了增强主动双水印网络，设计了自适应水印变换和轻量级AIGC编辑模拟器，分别提高容器图像保真度和版权提取精度。</li><li>实验验证，我们的OmniGuard在保真度、灵活性和鲁棒性方面均优于现有的多功能水印技术，并且在被动定位方法和鲁棒水印技术上也全面超越。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="图像篡改检测与定位">2.1.图像篡改检测与定位</h2><p>​  被动方法：<br/>​  被动图像篡改检测与定位网络主要关注识别异常区域，如伪影、噪声和分辨率不一致[21,24,28,29,47,51,53,55-58]。例如，MVSS-Net[11]通过多尺度监督和多视角特征学习来捕捉噪声和边界差异。TruFor[17]采用了一种对噪声敏感的指纹，并通过变压器驱动的融合机制结合了高级和低级痕迹提取。HiFi-Net[18]结合了多分支特征提取与专门的定位模块，有效检测来自CNN生成或编辑的图像的修改。同时，IMLViT[43]将Swin-ViT整合到其框架中，采用了FPN结构和边缘损失约束以提高准确性。受扩散模型启发，DiffForensics[64]采用了扩散去噪预训练，以提高对篡改图像中细微细节的检测能力。</p><p>​  主动计划：<br/>​  主动篡改检测与定位[3,5,6,62,71]通过在图像原始状态中嵌入信息，这有助于网络检测篡改或进行归属。尽管传统的脆弱水印方法[10,23,27,34,35,44]能够实现块级篡改定位，但其精确度和灵活性仍有待提高。最近，MaLP[4]通过利用两分支架构估计的局部和全局特征来学习模板，并从恢复的模板中检测出被篡改的区域。Imuge[61,63]采用了自嵌入机制和高效的攻击层，实现了篡改定位和自我恢复。在RAW图像中嵌入[21]水印，以增强被动篡改定位网络对JPEG压缩、模糊和重新缩放等有损操作的抗性。EditGuard[66]和V 2AMark[68]利用可逆网络嵌入双重水印，实现了像素级定位和版权保护。虽然这些主动机制已经显示出了良好的结果，但在保真度、稳健性和定位精度之间实现平衡仍然是一个开放的挑战。</p><h2 id="深度图像水印">2.2.深度图像水印</h2><p>​  水印技术是一种被广泛认可的经典版权保护手段[30,39,59,67-69]。传统方法通常将秘密信息嵌入空间域或自适应域[9]，往往选择数据位于不太显眼的位[15]或区域，这限制了隐藏信息的容量。近期，深度图像水印技术备受关注。例如，HiDDeN[72]通过引入深度编码器-解码器网络，实现了位流数据的隐藏与检索功能。此外，基于流模型的[13,42]技术因其固有的信息隐藏能力而受到青睐，进一步提升了水印图像的保真度。JPEG和截图[2,12,38,52]已被开发以增强其稳健性。SSL[14]在自监督潜在空间中对图像进行水印处理，通过学习到的图像表示来提升安全性和韧性。TrustMark[8]提出了一种适用于任意分辨率图像的通用水印技术。然而，这些方法无法有效支持AIGC编辑。最近，Robust-wide[20]提出了一种部分指令驱动的去噪采样指导模块，以增强对指令驱动图像编辑的鲁棒水印效果。VINE[41]引入了SDXL-Turbo[50]和解码器，实现了极高的比特精度。然而，这些方法需要在网络训练中加入多步骤迭代扩散过程，这显著降低了训练速度。</p><h1 id="方法">3.方法</h1><h2 id="多功能深度图像水印技术综述">3.1.多功能深度图像水印技术综述</h2><p>​  我们首先回顾了深度多功能水印[6,52,66,68,71]领域的现有方法。以EditGuard[66]为代表，当前的多功能水印采用串行编码与并行解码框架，使得局部和版权水印能够在同一图像中共存而不互相干扰。</p><figure><img src="../postimages/OmniGuard/image-20250701104815359.png"alt="image-20250701104815359" /><figcaption aria-hidden="true">image-20250701104815359</figcaption></figure><p>​  具体而言，如图2所示，给定二维局部水印<spanclass="math inline">\({\bf W}_{\mathrm{loc}}\in\mathbb{R}^{H\timesW\times3}\)</span>和一维版权水印<spanclass="math inline">\(\mathbf{w}_{\mathrm{cop}}\in\{0,1\}^{L}\)</span>，原始图像<spanclass="math inline">\({\bf I}_{\mathrm{ori}}\in\mathbb{R}^{H\timesW\times3}\)</span>输入到图像位联合嵌入网络中，生成容器图像<spanclass="math inline">\(\mathbf{I}_{\mathrm{con}}\)</span>。随后，若接收到的图像<spanclass="math inline">\(\mathbf{I}_{\mathrm{rec}}\)</span>已进行局部编辑或通道退化处理，可将其输入图像比特解码网络以恢复双重水印<spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\)</span>,<spanclass="math inline">\(\mathbf{\hatw}_{\mathrm{cop}}\)</span>。AIGC编辑与退化流程可建模如下。 <spanclass="math display">\[\mathrm{I_{rec}}={\calD}_{\mathrm{deg}}(\mathbf{I}_{\mathrm{con}}\odot(1-\mathbf{M}_{\mathrm{gt}})+{\mathcal{E}}_{\mathrm{edit}}(\mathbf{I}_{\mathrm{con}})\odot\mathbf{M}_{\mathrm{gt}})\]</span>​  其中，<spanclass="math inline">\({\mathcal{E}}_{\mathrm{edit}}(\cdot)\)</span>、<spanclass="math inline">\({\cal D}_{\mathrm{deg}}(\cdot)\)</span>和<spanclass="math inline">\(\mathbf{M}_{\mathrm{gt}}\)</span>分别代表编辑函数、退化函数和篡改掩码。需要注意的是，这些多功能方法[66,68,71]仅考虑局部编辑，即<spanclass="math inline">\(\mathbf{M}_{\mathrm{st}}\neq0\)</span>。恢复的版权<spanclass="math inline">\(\mathbf{\hat w}_{\mathrm{cop}}\)</span>预期与<spanclass="math inline">\(\mathbf{w}_{\mathrm{cop}}\)</span>保持一致。最后，我们通过残差减法比较<spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\)</span>和<spanclass="math inline">\(\mathbf{W}_{\mathrm{loc}}\)</span>，以计算篡改区域<spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\in\mathbb{R}^{H\timesW}\)</span>。 <spanclass="math display">\[\hat{\mathrm{M}}_{\mathrm{loc}}=\Theta_{\tau}(|\hat{\mathrm{W}}_{\mathrm{loc}}-\mathrm{W}_{\mathrm{loc}}|),\]</span>​  其中<spanclass="math inline">\(\Theta_{\tau}(z)=1(z\geq{\boldsymbol{\tau}})\)</span>。|·|表示绝对值运算。然而，我们总结指出，这种逐像素残差减法存在三大主要缺点。</p><ul><li><spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\)</span>的恢复与<spanclass="math inline">\(\mathbf{I}_{\mathrm{con}}\)</span>的生成存在相互制约关系。由于检测精度高度依赖于<spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\)</span>的恢复精度，因此<spanclass="math inline">\(\mathbf{I}_{\mathrm{con}}\)</span>的质量不得不大幅降低。</li><li><spanclass="math inline">\(\mathbf{\hat{M}}_{\mathrm{loc}}\)</span>的提取需要了解<spanclass="math inline">\(\mathbf{W}_{\mathrm{loc}}\)</span>，因此它不是盲的。为了便于解码端和编码端之间的协作，一个简单的方法是选择一个固定的<spanclass="math inline">\(\mathbf{W}_{\mathrm{loc}}\)</span>。然而，固定的局部水印无法为所有图像提供最佳保真度。</li><li>如果<spanclass="math inline">\(\mathbf{\hat{W}}_{\mathrm{loc}}\)</span>的未改变区域包含由于严重退化而产生的伪影，则这些区域也可能被错误地视为篡改，从而对检测的稳健性造成威胁。</li></ul><h2 id="omniguard总体框架">3.2.OmniGuard总体框架</h2><p>​  为了解决上述问题，我们提出了一种混合取证框架OmniGuard，该框架结合了主动的双水印网络和被动的、深度的、退化感知的篡改提取器（图3）。</p><figure><img src="../postimages/OmniGuard/image-20250701111018844.png"alt="image-20250701111018844" /><figcaption aria-hidden="true">image-20250701111018844</figcaption></figure><p>​  该框架利用主动水印的强大泛化能力，克服了被动提取网络的准确性限制，同时通过被动网络进一步增强定位的鲁棒性。此外，OmniGuard通过精心设计的定位水印提高了容器图像的保真度，并通过在退化层中有效模拟AIGC的全局和局部编辑，增强了版权提取的鲁棒性。</p><figure><img src="../postimages/OmniGuard/image-20250701111151567.png"alt="image-20250701111151567" /><figcaption aria-hidden="true">image-20250701111151567</figcaption></figure><p>​  具体来说，如图3和图4(a)所示，我们首先采用自适应前向变换（第3.4节）将原始图像<spanclass="math inline">\(\mathbf{I}_{\mathrm{ori}}\)</span>嵌入到局部水印<spanclass="math inline">\(\mathbf{W}_{\mathrm{loc}}\)</span>中，使变换后的水印<spanclass="math inline">\(\mathbf{\tildeW}_{\mathrm{loc}}\)</span>获得一些内容感知性和合理的纹理，这已被证明有助于有效隐藏。随后，将定位标签<spanclass="math inline">\(\mathbf{\tildeW}_{\mathrm{loc}}\)</span>和版权水印wcop嵌入到<spanclass="math inline">\(\mathbf{I}_{\mathrm{ori}}\)</span>中，生成容器图像<spanclass="math inline">\(\mathbf{I}_{\mathrm{con}}\)</span>。同样地，我们从Irec中提取w<sup>cop，并通过本地化水印解码模块和逆变换（第3.4节）获得伪影图W</sup>loc。这里的本地化水印隐藏和解码网络采用了[66]中的可逆网络结构。版权水印隐藏和解码网络的结构则受到了[8]的启发。最后，伪影图W<sup>loc和被篡改的图像Irec被输入到深度篡改提取器中，生成篡改掩模M</sup>loc。<br/>​  OmniGuard的主要改进集中在两个关键点上。<br/>​  首先，我们介绍了一种深度篡改提取器，该提取器结合了Irec和ˆWloc数据来分析篡改痕迹。通过将ˆWloc的重建任务重新定义为分类任务，提取器只需区分篡改痕迹与其他区域，从而大大简化了训练过程。<br/>​  其次，我们扩展了EditGuard支持的降级范围。在版权解码方面，我们设计了一个轻量级的AIGC模拟层，使OmniGuard能够支持全局编辑和局部篡改。对于本地水印解码，我们的检测技术能够应对JPEG（Q=50）、色彩抖动和严重噪声等条件。我们已在表1中列出了与EditGuard的对比。</p><figure><img src="../postimages/OmniGuard/image-20250701111650324.png"alt="image-20250701111650324" /><figcaption aria-hidden="true">image-20250701111650324</figcaption></figure><h2id="被动降解-防篡改提取器的动机">3.3.被动降解-防篡改提取器的动机：</h2><h2 id="自适应局部水印变换的动机">3.4.自适应局部水印变换的动机</h2><h2 id="轻量级aigc编辑模拟器">3.5.轻量级AIGC编辑模拟器</h2><h2 id="训练详情">3.6.训练详情</h2><h1 id="实验结果">4.实验结果</h1><h2 id="实验设置">4.1.实验设置</h2><p>​  我们使用AdamW优化器，在MIRFlickR数据集[22]上训练版权水印网络，初始学习率为4×10−6，批量大小为32，采用余弦退火策略。随后，我们在DIV2K[1]数据集上优化整个主动双水印网络，使用Adam优化器，学习率为1×10−5，批量大小为8。基于窗口的transformer主干网络是ViTB，该网络通过MAE[19]在ImageNet-1k上进行了预训练。窗口大小和填充大小分别设置为14和1024。篡改提取器在20000个构建的配对{W^loc，Irec，Mgt}上进行了训练。训练过程中使用的篡改类型为稳定扩散修复，但我们的方法可以适应几乎任何类型的局部修改。所有实验均在NVIDIAGTX 3090Ti服务器上完成。</p><h2 id="与定位方法的比较">4.2.与定位方法的比较</h2><p>​  为了评估我们的OmniGuard在防篡改定位上的精度，我们将其与一些最先进的被动方法进行了对比，包括PSCC-Net[37]、MVSS-Net [11]、CATNet [29]、HiFi-Net [18]、IML-ViT[43]，以及主动方法EditGuard[66]。我们使用F1分数和AUC作为评估指标。在COCO[36]测试集的1000个样本上，我们比较了我们的方法与其他竞争方法。我们选择了三种独特的局部编辑方法，即稳定扩散修复、ControlNet修复和图像拼接，以实现等式1中的<spanclass="math inline">\({\mathcal{E}}_{\mathrm{edit}}(\cdot)\)</span>功能。退化函数<spanclass="math inline">\({\calD}_{\mathrm{deg}}(\cdot)\)</span>从高斯噪声（σ=1-10）、JPEG（Q=70-85）、亮度、对比度和饱和度的色彩抖动，以及椒盐噪声中随机选取，以验证其鲁棒性。</p><figure><img src="../postimages/OmniGuard/image-20250701112139661.png"alt="image-20250701112139661" /><figcaption aria-hidden="true">image-20250701112139661</figcaption></figure><p>​  如表2所示，我们的方法在AIGC和简单篡改场景下均实现了最佳的定位性能，F1分数超过0.95，AUC接近1。尽管EditGuard在干净条件下与我们的表现类似，但我们观察到在嘈杂条件下EditGuard的定位精度显著下降，F1下降了约0.2。这是因为EditGuard依赖于像素级别的对比，这使得其定位结果高度依赖于水印恢复的性能。相比之下，OmniGuard在面对常见的退化情况时几乎不会影响性能。图5进一步展示了我们方法的优势。对于一些野外篡改样本，被动方法如PSCCNet和IML-ViT难以有效检测到篡改痕迹。主动方法如EditGuard会产生不准确的掩码，并且当篡改图像经历全局退化时，对阈值设置非常敏感。相比之下，我们的OmniGuard能够稳健且准确地识别篡改区域。需要注意的是，对于像Instructp2p这样的全局编辑，OmniGuard仍然能够稳健地恢复版权并检测到篡改；然而，它倾向于将整个图像视为篡改。这与我们的预期功能一致，因为我们的水印技术更侧重于解决像素级别的篡改问题，而不是语义上的修改。</p><h2 id="与水印技术的比较">4.3.与水印技术的比较</h2><h2 id="消融研究">4.4.消融研究</h2><h1 id="结论">5.结论</h1><p>​  OmniGuard提出了一种创新的混合取证框架，该框架结合了主动水印嵌入与被动提取技术。这一方法集成了深度退化感知的篡改检测器、自适应局部水印变换以及轻量级的AIGC编辑模拟器，使OmniGuard在保真度、灵活性和鲁棒性方面表现出色。实验结果表明，OmniGuard通过生成更高保真度的图像，减少伪影，并在严重退化和生成式AI编辑的情况下实现更准确的篡改定位和版权提取，超越了现有的最先进方法。作为对抗AIGC操纵的有效防御手段，OmniGuard对社区而言是一个重要的进步，为未来的版权保护和数字内容真实性发展做出了贡献。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Operation History Estimation and Its Application to Multi-Degraded Image Restoration</title>
      <link href="/Operation-History-Estimation-and-Its-Application-to-Multi-Degraded-Image-Restoration/"/>
      <url>/Operation-History-Estimation-and-Its-Application-to-Multi-Degraded-Image-Restoration/</url>
      
        <content type="html"><![CDATA[<p>Operation History Estimation and Its Application to Multi-DegradedImage Restoration</p><p>Yuanman Li , <em>Member, IEEE</em>, Minhua Liu, <em>Student Member,IEEE</em>, Jinyu Tian , <em>Member, IEEE</em>, Jie Du , <em>Member,IEEE</em>, and Xia Li , <em>Member, IEEE</em></p><h1 id="摘要">摘要</h1><p>​  从网络和社交平台获取的共享图像可能因多种操作而质量下降，影响其对消费者的质量和可靠性。现有的图像修复算法主要针对已知退化类型的单个图像，而对于未知退化类型的多退化图像修复研究较少。此外，现有方法通过分类模型检测图像是否经历了特定的单一操作来评估图像的可靠性。然而，准确估计图像的完整操作历史仍然具有挑战性。这些限制可能限制了这些技术在消费者应用中的实际使用。为了解决这些问题，本研究提出了一种新的框架，该框架利用操作历史估计来提高多退化图像的可靠性和质量。首先，我们基于机器翻译机制开发了一个深度网络，用于估计输入多退化图像的操作历史，包括操作类型、参数及其顺序。具体而言，设计了自注意力机制和交叉注意力机制，以捕捉操作与参数之间的内部关联及其交互关系。其次，我们提出将操作历史估计方案应用于多退化图像的恢复，利用估计的操作和相关参数作为先验信息，辅助恢复过程。广泛的实验结果表明，我们的框架在操作历史估计和多退化图像恢复方面表现出色，最终提高了图像在消费应用中的可靠性和质量。</p><h1 id="i.引言">I.引言</h1><p>​  这些方法包括检测操作链（即一系列连续的操作）[9]、[10]、[11]，以识别图像处理过程，并估计操作参数[12]、[13]，以确定操作链的相应参数。然而，现有的方法通常局限于特定任务，如估计操作链或参数，未能全面捕捉整个操作历史。此外，许多方法依赖分类模型来估计操作参数。由于操作参数通常是连续值，分类技术只能通过识别决策边界来提供近似的参数范围，而无法给出精确的参数值。因此，为了确保图像的可靠性，开发能够准确揭示整个操作历史的创新方法至关重要。<br/>​  为了解决上述问题，本文提出了一种模型，该模型能够全面揭示操作历史，并将其应用于多退化图像的恢复。与传统的基于分类的方法不同，我们采用了基于机器翻译的方法，考虑了操作历史中操作和参数的顺序性。具体而言，我们将每个操作和参数视为句子中的词语，构建了两个描述操作历史的句子，一个描述操作，另一个描述参数。通过在语言空间中建立图像与操作历史之间的映射关系，我们能够同时考虑操作和参数，从而估计出完整的操作历史。随后，我们将这一操作历史估计模型整合到一个多退化图像恢复框架中。具体而言，我们将退化历史视为操作历史的一个特例，并利用估计的退化历史作为多退化图像恢复的先验信息，即使没有已知的退化历史，也能实现图像的恢复。我们的主要贡献可总结如下：</p><ul><li>我们提出了一种操作历史估计模型，该模型充分利用了自注意力和交叉注意力机制，能够有效学习输入图像、操作及其相关参数之间的关联。与现有方法仅能估计操作或参数不同，我们的方法能够通过机器翻译机制逐步解码出完整的操作历史，包括操作、参数及其顺序。</li><li>我们利用回归技术来估计每个操作相关的参数。与基于分类的先前方法相比，这种方法通过直接预测参数的数值，而不是将其视为离散类别，从而实现更细致的估计。</li><li>我们进一步探索了该方法的实际应用，通过将其应用于多退化图像恢复框架来辅助恢复过程。与传统图像恢复方法假设退化历史已知不同，我们的恢复框架估计退化历史，并将其作为多退化图像恢复的先验信息。</li><li>实验结果表明，我们提出的方法在多退化图像的操作历史估计和恢复方面，比现有算法具有更好的性能。</li></ul><p>​  本文的结构安排如下：第二部分概述了相关工作，第三部分介绍了操作历史估计算法及其在多退化图像复原中的应用，第四部分给出了实验结果，第五部分对全文进行了总结。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  在本节中，我们对有关操作历史估计和图像恢复的相关工作进行了简要回顾。</p><h2 id="a.操作历史估计">A.操作历史估计</h2><p>​  实际的图像处理过程可能包含多个操作，这些操作共同构成了图像操作的历史记录。由于多个操作之间的相互作用，单个操作的取证方法难以满足对图像操作链的实际需求。为了全面揭示数字图像的操作过程，需要识别图像操作链中的操作类型、顺序和关键参数。操作链取证的概念最早由[14]提出，该研究以量化和加性高斯白噪声的操作链为例，验证了其理论分析的有效性。他们进一步分析了操作链的拓扑结构，其中包含更多的操作[15]。针对内容感知缩放和对比度增强的二元操作，Li等人[16]提出了一种方法，通过计算马尔可夫一步转移概率矩阵并提取DCT域的高维特征，来估计由内容感知缩放和对比度增强组成的链的操作历史。需要注意的是，上述方法基于手动特征。<br/>​  随着深度学习技术的发展，操作链检测的任务已从传统的特征提取转向了深度学习。Boroumand和Fridrich[17]专注于特定的二进制操作链，并在卷积神经网络中引入了全局平均池化层，以识别被篡改图像中的操作类型。Chen等人[18]开发了一种自动化的神经网络，利用强化学习生成高性能的神经网络，特别适用于多目标取证和历史检测。通过联合提取与域操作序列相关的条件指纹特征，[19]和[20]的研究实现了特定二进制操作序列的类型识别和顺序识别。Liao等人[9]引入了一种基于双流卷积神经网络的图像操作链取证框架，并为特定的操作组合设计了多种预处理技术。最近，You等人[10]提出了一种基于Transformer的图像操作链检测方法，该方法将操作链检测视为一个翻译任务，而非分类任务。为了进一步提升检测性能，他们提出了链反转策略和双向检测机制[11]。<br/>​  此外，操作历史中操作参数的估计也受到了关注。Feng等人[21]提出了一种利用归一化能量密度来估计重采样图像参数的方法。Zhu等人[22]介绍了一种基于归一化能量密度和矩阵特征的学习排序方法，用于估计图像缩放因子。Liu等人[23]提出了一种利用图像光谱和差分图像极值区间直方图来估计预JPEG压缩图像缩放因子的方法。Xue等人[12]提出了一种使用DCT系数直方图来估计双JPEG压缩图像因子的方法。Wang等人[24]通过分析零直方图区间数量与伽马变化之间的关系，来估计伽马变换参数。最近，Liao等人[13]通过对操作链中操作之间相关性的分析，进行了参数估计的研究。他们探讨了操作序列和参数变化对最终图像的影响，并根据操作间的耦合或非耦合相关性设计了不同的参数估计策略。值得注意的是，所有现有的方法都存在一个共同的局限性，即只能单独估计单个操作或参数，而无法全面捕捉整个操作历史。</p><h2 id="b.图像恢复">B.图像恢复</h2><p>​  图像恢复技术旨在解决因成像系统受多种因素影响而导致的图像质量下降问题，这些问题包括[8]、[25]、[26]、[27]和[28]。<br/>​  近年来，图像恢复领域取得了显著进展。例如，Gu等人[25]提出了一种名为IKC的迭代方法，用于估计模糊核。他们将这一问题视为基于模型的图像恢复任务，假设低分辨率图像从高分辨率图像中经历了多次模糊退化。Yu等人[29]开发了一个基于强化学习的图像恢复框架，将恢复过程视为一个决策过程，并自适应地选择工具来优化图像质量。在应对多重退化挑战时，Suganuma等人[30]提出了一种神经网络层结构，该结构通过基于注意力的权重执行并行操作，以根据输入选择最合适的处理方式。此外，He等人[8]解决了多退化图像的问题，提出了一种可控的恢复框架，能够根据不同类型的退化和不同程度调节输出效果，引入了多维调制的概念。</p><h1 id="iii.提出的操作历史估计算法">III.提出的操作历史估计算法</h1><p>​  在本节中，我们首先介绍问题的表述，然后详细阐述我们提出的基于机器翻译的操作历史估计算法。最后，我们将展示操作历史估计在多退化图像恢复任务中的应用。</p><h2 id="a.问题的定义">A.问题的定义</h2><h2 id="b.提出的操作历史估计算法">B.提出的操作历史估计算法</h2><figure><imgsrc="../postimages/Operation-History-Estimation-and-Its-Application-to-Multi-Degraded-Image-Restoration/image-20250608145112370.png"alt="image-20250608145112370" /><figcaption aria-hidden="true">image-20250608145112370</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Image Operation Chain Detection with Machine Translation Framework</title>
      <link href="/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/"/>
      <url>/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/</url>
      
        <content type="html"><![CDATA[<p>Image Operation Chain Detection with Machine TranslationFramework</p><p>Yuanman Li , Member, IEEE, Jiaxiang You, Student Member, IEEE,Jiantao Zhou , Senior Member, IEEE, Wei Wang , Member, IEEE, Xin Liao ,Senior Member, IEEE, and Xia Li , Member, IEEE</p><h1 id="摘要">摘要</h1><p>​  操作链检测的目的是揭示给定操作图像中涉及的操作及其应用顺序，这对于图像处理和多媒体取证至关重要。目前，所有现有方法都将图像操作链检测视为分类问题，并且仅考虑最多包含两个操作的链。考虑到操作与解决方案空间呈指数级增长之间的复杂相互作用，检测更长的操作链极具挑战性。为了解决这一问题，本研究提出了一种新的图像操作链检测方法。不同于现有的基于分类模型的方法，我们采用机器翻译框架进行操作链的检测。具体而言，我们的方法将操作链建模为目标语言中的句子，每个可能的操作用该语言中的一个词来表示。在执行链检测时，我们首先将输入图像转换成潜在源语言中的句子，通过学习到的深度特征实现。随后，我们提出在机器翻译框架下将潜在语言转换为目标语言，并最终按顺序解码所有操作。此外，我们开发了链反转策略和双向建模机制，以提升检测性能。我们还设计了一种加权交叉熵损失，旨在缓解链长度和链类别不平衡带来的问题。我们的方法可以检测包含多达7个操作的操作链，并在各种场景中对短链和长链的检测都取得了非常有希望的结果。</p><h1 id="i.引言">I.引言</h1><p>​  操作链检测在数字图像分析中扮演着至关重要的角色。许多数字图像因一系列操作而受损，如JPEG压缩、高斯噪声、重采样、增强等。了解给定图像所经历的操作链，可以提供其退化历史的更多细节，这对于许多图像处理任务[1]、[2]、[3]至关重要。以多退化图像恢复为例，退化历史可用于构建模型，合成大量原始与退化图像对，进而训练深度网络以实现更佳的图像恢复效果。此外，操作链的信息对于图像取证同样重要。如今，图像伪造在互联网谣言、假新闻和不诚实的学术文献中普遍存在，对人类生活的多个方面造成了负面影响。在制作图像伪造时，通常会使用多种常见的图像处理技术来隐藏操作痕迹或使生成的图像看起来更加逼真。基于这一背景，许多图像取证算法被设计出来，用于验证图像的真实性并揭示其操作链[4]、[5]。<br/>​  大多数先前的研究都集中在设计用于识别单一操作的取证方法上。这些方法大致可以分为两类：一是针对特定操作的检测[6]，[7]，二是通用操作检测[8]，[9]，[10]。然而，实际上伪造图像通常涉及多个操作。因此，调查人员开始考虑操作链检测。图1展示了两个例子，其中一张图像通过五种操作以不同的顺序进行了处理。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250606232032134.png"alt="image-20250606232032134" /><figcaption aria-hidden="true">image-20250606232032134</figcaption></figure><p>​  与传统的特定和通用操作检测方法不同，操作链检测需要同时识别应用于图像的多个操作，并确定这些操作的顺序，以揭示完整的图像处理历史[4]。操作链检测的任务超出了传统取证算法的能力，原因如下：首先，传统的取证方法只能识别一个潜在的操作，如何有效地将它们结合在一起揭示多个操作似乎并不直接。其次，确定检测操作的顺序仍然不容易，特别是当操作的数量很大时。第三，由于不同图像操作之间的相互作用，后序操作可能会影响和掩盖前序操作留下的痕迹，使某些操作难以被检测。考虑到上述困难，操作链检测方面取得的进展有限。例如，[11]的工作从理论上研究了使用信息论度量方法量化不同操作链之间可区分性的问题。[12]和[13]的研究将操作链检测问题视为多个假设检验的问题。在最近的论文中，[4]和[5]通过使用深度卷积神经网络（CNNs）尝试检测操作链，取得了显著进展。据我们了解，现有的所有算法都将操作链检测视为分类问题，面临以下挑战：</p><ul><li>所有先前的方法仅考虑由最多两个操作组成的链。随着链的长度增加，解空间的大小呈指数级增长。例如，当只有两个操作时，可能的链数为<spanclass="math inline">\(\textstyle\sum_{i=0}^{2}A_{2}^{i}=5\)</span>；而当有七个操作时，可能的解决方案数量则达到<spanclass="math inline">\(\textstyle\sum_{i=0}^{7}A_{7}^{i}=13700\)</span>。更糟糕的是，对于较长的操作链，操作之间的相互作用极其复杂，先前创建的指纹可能会受到后续操作的强烈影响，甚至被掩盖。由于这些挑战，如何检测长操作链仍然是一个未解决的问题。</li><li>所有现有的方法都将链检测建模为一个分类问题，通过直接为每个链分配一个独特的标签。尽管这种方法看似非常直接，但它无法充分利用操作的顺序（或部分顺序）信息，也无法充分利用不同链之间的强相关性。例如，由于链路的唯一标记，JPEG→AWGN→重采样和AWGN→重采样的关系变得模糊。</li></ul><p>​  由于这些问题的困扰，图像操作链检测仍处于起步阶段，基于分类的方法所取得的有限进展促使我们考虑新的方法，特别是针对长链检测。<br/>​  本文提出了一种新型的端到端深度框架，用于图像操作链检测，我们将其命名为TransDetect。与以往基于分类模型的方法不同，我们在机器翻译框架内解决了操作链检测问题，我们的方法继承了机器翻译解码超长句子的能力。具体来说，我们将操作链建模为目标语言中的一个句子，每个图像操作用该语言的一个单词表示。因此，图像操作链检测的目标是找到从处理后的图像域到目标语言域的映射关系。为此，首先将输入图像转换到深度特征空间，然后将这些特征转化为潜在源语言中的句子，以描述操作链的指纹。最后，我们提出使用机器翻译框架将潜在语言转换为目标语言，从而解码整个操作链。此外，我们还提出了分层特征提取、链反向、双向建模和加权交叉熵损失策略，以提高检测性能。<br/>​  据我们所知，这是首次将操作链检测作为通用机器翻译问题进行研究，并在统一框架下考虑包含多达七个操作的链。我们总结了本研究的主要贡献和创新点如下：</p><ul><li>我们采用策略性建模方法，将操作链作为时间序列（具体来说，目标语言中的一个句子）进行建模，而不是直接给它贴上简单的标签。与现有方法相比，我们的链模型保留了原始链的特性，例如操作顺序和相似链之间的强相关性。基于所提出的链模型，我们在机器翻译框架内设计了一种新颖的图像操作链检测方法。</li><li>与其它机器翻译任务类似，我们的方法逐步解码操作链，即逐个操作地进行。这一特性使我们的方法能够利用先前解码的操作作为先验信息，帮助解码下一个操作，从而大大缓解了长链检测中解决方案空间大的问题。我们进一步提出了两种策略，即链反转和双向建模，以提高检测性能。</li><li>为了检测未知长度的链，我们设计了一个加权交叉熵损失来处理链长度和链类别之间的不平衡问题。</li><li>大量的实验表明，我们的方法在检测短链和长链的各种场景中都取得了非常有希望的结果。</li></ul><p>​本研究的部分内容之前已作为会议版本发表在[14]上。与[14]相比，技术部分和实验部分都得到了显著的改进。我们总结了主要的改进如下：首先，我们从通用机器翻译的角度重新定义了操作链检测，并详细阐述了我们的翻译策略在保持相似操作链之间强相关性方面的优势。其次，我们针对链长和类别所导致的基本不平衡问题，设计了加权交叉熵损失函数（WCEL，WeightedCross-Entropy Lossfunction）来解决这一问题，并对参数的选择进行了论证。第三，我们提出了一种链反转策略和双向建模策略，以减轻早期解码子链（置信度较低）导致的错误传播，并提出了一个分层特征提取策略，通过融合多级特征来实现更丰富的语言表示。这两种策略在消融研究中得到了验证。第四，与[14]仅提供包含最多5个操作的链的平衡长度实验初步结果相比，我们进行了广泛的额外实验，以展示我们的优势。这些实验包括但不限于：新的评估指标——双语评估下研究（BLEU，BilingualEvaluationUnderstudy）分数、平衡类别实验、跨数据集验证、与更近期方法的对比，以及处理未知参数和由7个操作组成的链等挑战性案例。此外，与[14]相比，性能显著提升，例如，在未知长度的链检测上，准确率提高了12.69%。最后但同样重要的是，我们新增了第五部分A小节，专门讨论本研究的不足之处及未来发展方向。<br/>​  本文其余部分安排如下：第二部分回顾相关工作，第三部分详细介绍我们提出的用于操作链检测的TransDetect算法，第四部分给出了大量的实验结果，最后在第六部分给出结论。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  在本节中，我们简要回顾了操纵检测的一些相关工作和流行的机器翻译框架Transformer。</p><h2 id="a.操纵检测">A.操纵检测</h2><p>​  目标操作检测：<br/>​  目标操作检测方法旨在识别单一特定的操作。这些方法通常通过提取特定操作留下的特征，如重采样[6]、[15]、中值滤波[7]、[16]、对比度增强[17]和JPEG压缩[18]，来构建二分类问题。例如，Qiao等人[6]提出了一种统计框架，通过分析残差噪声的行为来检测重采样操作。Pasquini等人[19]基于假设检验理论，探讨了重采样操作的可检测性。Cao等人[17]设计了两种对比度增强检测器，分别基于零高度间隙指纹和块峰值/间隙箱，考虑了之前已进行JPEG压缩的图像。Chen等人[16]通过突出差异域中引入的统计伪影，提出了中值滤波操作检测器。Kang等人[7]研究了从中值滤波残差图计算出的自回归系数，并开发了一种算法，用于检测压缩图像中的中值滤波。除了上述方法外，还有许多其他常见的操作类型[20]、[21]、[22]、[23]、[24]、[25]。</p><p>​  通用操作检测：<br/>​  实际上，通常无法获得关于已应用了哪种操作的先前信息。在这种情况下，调查人员可能会运行多个独立的目标检测器来验证图像。然而，在实践中，如何在控制总体误报率的同时融合不同法医测试的结果是非常具有挑战性的。为了解决这一问题，近期的一些研究探讨了通用图像取证算法，旨在通过单一测试从多种可能的操作中识别出具体的篡改类型。与针对特定操作的检测不同，通用图像取证方法将篡改检测视为一个多类分类问题[8]，[9]，[10]，[26]。例如，邱等人[10]将各种图像处理操作建模为隐写术，并基于通用的隐写分析特征设计了一种通用的图像取证方法，实现了对六种典型操作的区分能力。李等人[9]研究了残差域中相邻像素之间的相关性，并设计了一种通用的取证算法，用于区分十一种图像操作和四种反取证操作。Bayar等人[5]开发了一种基于端到端CNN的通用图像操作检测器（MISLnet），该检测器通过设计一个约束卷积层来学习操作产生的特征，并抑制被操作图像的内容。最近，吴等人[8]提出了一种端到端的全卷积网络，用于通用操作检测，称为ManTra-Net，该网络考虑了来自七个操作家族的多达385种图像操作。然而，其最高检测准确率仅为约50%。</p><p>​  操作链检测：<br/>​  应该注意的是，针对性和通用操作检测方法只能从潜在操作池中揭示一个应用的操作。然而，在实践中，通常会顺序地对图像应用多个操作。最近，研究人员开始尝试检测操作链，这需要确定所涉及的操作及其顺序。Boroumand等人[27]提出了一种基于深度学习的图像处理历史检测算法，该算法假设在两个不同质量因子的JPEG压缩操作之间插入一个操作。Chen等人[28]设计了一种基于自动CNN架构的图像处理历史检测算法，考虑了调整大小等四种操作的组合。[12]的作者[13]将检测操作顺序的问题转化为多重假设检验问题，并通过信息理论框架研究了何时可以或不能检测到操作顺序。Bayar等人[5]提出了一种受约束的CNN，用于检测最多包含两个图像操作的图像操作链。最近，Liao等人[4]开发了一种双流CNN，用于基于从空间域和变换域提取的特征来检测两个图像操作的链。值得注意的是，所有现有方法都将操作链检测视为一个多类分类问题，并且仅考虑最多包含两个操作的链。</p><h2 id="b.-transformer">B. Transformer</h2><p>​  机器翻译的目标是将一种语言（源语言）转换成另一种语言（目标语言）。鉴于句子的顺序特性，许多传统的机器翻译模型采用了循环编码器-解码器框架，例如循环神经网络（RNN）、长短期记忆网络（LSTM）[29]和门控循环单元（GRU）网络[30]。最近提出了一种基于自注意力机制的简单网络架构——Transformer，旨在实现训练过程的并行化[31]。值得注意的是，Transformer摒弃了循环结构，能够建模全局依赖关系，而不考虑这些依赖关系在输入或输出序列中的距离。随着这种架构在机器翻译领域的巨大成功，研究人员开始将Transformer应用于计算机视觉任务。visionTransformer（ViT）[32]是一项开创性的研究，它实现了用于图像分类任务的纯Transformer编码器。ViT将输入图像序列化为一系列标记，然后将这些标记输入到具有多头自注意力机制的标准Transformer编码器中。受此启发，许多其他基于Transformer的视觉架构也被设计出来[33]，[34]，例如DeiT[33]和Swin Transformer [34]。</p><h1id="iii.用于操作链检测的transdetect算法">III.用于操作链检测的TRANSDETECT算法</h1><p>​  在本节中，我们讨论了提出的用于操作链检测的TransDetect算法。与现有算法不同，我们将操作链检测问题视为机器翻译问题，而非分类问题。</p><h2 id="a.问题的定义">A.问题的定义</h2><p>​  在我们的研究中，操作链Ti被定义为一系列连续应用于图像的操作的有序序列。我们假设这些操作是从操作池<spanclass="math inline">\({\mathcal{O}}=\{O_{1},O_{2},...,O_{M}\},\)</span>中选取的，因此可以将Ti表示为<span class="math display">\[T_{i}= O_{1}^{\prime}\rightarrowO_{2}^{\prime}\rightarrow\dots\rightarrow O_{m}^{\prime},~~O_{1}^{\prime}..., O_{m}^{\prime}\in{\cal O}.\]</span>​  对于链长为N的情况，所有可能的链总数为<spanclass="math inline">\(A^N_M\)</span>。假设链的最大长度为N(N≤M），那么，存在K条不同的链。 <spanclass="math display">\[K=A_{M}^{0}+A_{M}^{1}+A_{M}^{2}+\ldots+A_{M}^{N},\]</span>​  请注意，式(2)中的<spanclass="math inline">\(A^0_M\)</span>表示图像未被修改，即链的长度为0。设<spanclass="math inline">\(\mathcal{T}_{c h a in}=\{T_{1},T_{2},...,T_{K}\}\)</span>包含所有可能的链；因此，链检测操作旨在从<spanclass="math inline">\(\mathcal{T}_{c h a in}\)</span>中识别出给定处理图像的正确链。<br/>​  应该强调的是，揭示完整操作链中的一些长子链对图像取证分析也是有帮助的。因此，在本研究中，我们将给定链的子链定义为其连续的子序列。例如，链Ti=O₁→O₂→O₃的子链包括O₁、O₂、O₃、O₁→O₂、O₂→O₃以及O₁→O₂→O₃。<br/>​  我们可以推导出，随着M和N的增加，T链的规模将呈指数级增长，这使得问题变得极其复杂。以往的研究仅限于最多包含两个操作的链。为了应对长链检测的问题，我们在机器翻译框架中进行了操作链检测，其中每个操作由目标语言句子中的一个词表示。与现有方法相比，我们的基于机器翻译的方法能够保留原始链的顺序信息和相似链之间的强相关性。此外，我们的方法逐步检测链，其中先前解码操作的先验信息用于辅助下一个操作的检测，从而大大缓解了长链检测的大解决方案空间的问题。此外，如第三节D部分所示，通过操作链的顺序建模，成功检测到的子链也能有效引导我们框架的学习过程。<br/>​  本文考虑了多达七种图像运算，总共产生了13700种可能的运算链。所考虑的操作和相关参数总结见表I。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250606233708623.png"alt="image-20250606233708623" /><figcaption aria-hidden="true">image-20250606233708623</figcaption></figure><p>​  例如，经过链式MF→GB→HE→USM→AWGN→JPEG处理的图像可以表示为O2→O1→O6→O7→O4→O5。需要注意的是，O1至O5的操作也在[5]中使用。除了可能的图像操作外，表I还列出了三个辅助符号，这些符号将用于机器翻译。O0是Transformer模型中用于并行训练的填充符号。Os和Oe分别是解码的开始和结束符号。</p><h2 id="b.提出的transdetect算法">B.提出的TransDetect算法</h2><p>​  图2展示了我们提出的TransDetect算法框架。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250606233902052.png"alt="image-20250606233902052" /><figcaption aria-hidden="true">image-20250606233902052</figcaption></figure><p>​  该方法主要包含三个步骤：1）分层特征提取，2）分词及源句子构建，3）语言翻译。</p><h3 id="分层特征提取">1）分层特征提取</h3><p>​  由于操作之间的复杂交互，学习良好的特征表示对于检测操作链至关重要。在我们的研究中，我们提出了一种使用深度架构来提取混合操作留下的痕迹的层次特征的方法。具体来说，我们的特征提取器由以下三种类型的层组成：</p><ul><li>卷积层（Conv），第一层包含64个滤波器，核大小为3×3。我们对特征进行了批量归一化处理，并使用修正线性单元（ReLU）作为激活函数，以实现非线性映射。</li><li>ResBlock：为了提取混合操作的层次特征，我们构建了一组残差块，每个残差块包含两个卷积层和一个跳跃连接。</li><li>MaxPool：在每个ResBlock之后串联了三个MaxPooling函数，以生成更加紧凑的特征，每个函数将特征图的尺寸缩减至原来的四分之一。</li></ul><p>​  由于不同操作之间的相互作用，不同操作留下的痕迹强度差异显著；有些痕迹可能被严重削弱，而另一些则可能非常强烈。为了提取更丰富的特征表示，我们不仅提取了低级特征，还提取了高级特征，并在层次特征提取模块的末尾将这些特征连接起来，如图2所示。进一步地，自适应最大池化操作[35]应用于不同级别的特征，以生成相同大小的特征图。<br/>​  为了简化，我们将分层特征提取过程表示为<spanclass="math inline">\({\mathcal{F}}_{fe}(\cdot)\)</span>。对于给定的输入图像I，其深度表示计算如下： <spanclass="math display">\[{F}_{o}=\mathcal{F}_{f e}(I).\]</span> ​  <spanclass="math inline">\({F}_{o}\)</span>的尺寸为<spanclass="math inline">\({C}\times H^{\prime}\timesW^{\prime}\)</span>，，其中C表示通道数；<spanclass="math inline">\(W^{\prime}\)</span>和<spanclass="math inline">\(H^{\prime}\)</span>分别代表每个特征图的宽度和高度。</p><h3 id="分词与源句构建">2）分词与源句构建</h3><p>​  请注意，机器翻译的输入应为时间序列信号。因此，将深度特征表示Fo转换成时间序列，以携带链的特征，这一点非常重要，对于翻译过程来说是必要的。在基于机器翻译的图像处理任务中，大多数现有方法会将输入图像分割成空间域[32]、[34]中的序列标记。在我们的案例中，操作是全局进行的，因此不受空间位置的影响。基于这一特点，我们提出了一种通道级标记化策略，将特征转换为时间序列信号。设标记化函数为<spanclass="math inline">\(\mathcal{T}_{source}(\cdot)\)</span>，则我们将Fo转换为时间序列如下：<span class="math display">\[S_{o}=\,{\cal T}_{s o u r c e}({\calF}_{o}).\]</span> ​  为了简化，我们将<spanclass="math inline">\(\mathcal{T}_{source}(\cdot)\)</span>函数实现为两个Conv+BN+ReLU层与MaxPooling和通道展平的组合。在实验中，我们根据经验将第一个和第二个卷积层的滤波器数量分别设置为64和32，以平衡准确性和复杂度。直观来说，<spanclass="math inline">\(\mathcal{T}_{source}(\cdot)\)</span>将特征映射到一个潜在的源语言空间，在这个空间中，时间序列信号So可以被看作是源语言中的一个32词句子。需要注意的是，源语言空间中的词与表I中展示的图像操作不是一一对应的，而句子So则概括了目标链对图像作用的复杂行为。<br/>​  设<spanclass="math inline">\(S_{o}=[W_{1},\dots,W_{32}]\)</span>，其中每个词<spanclass="math inline">\(W_i\in\mathbb{R}^{H^{\prime}{W}^{\prime}}\)</span>。与其它词嵌入策略[32]类似，我们将每个词Wi嵌入到一个512维的向量中。数学上，<span class="math display">\[v_{i}^{s}=E m b e ds(W_{i}),\;\;\;i\in\{1,...,32\}.\]</span>​  在本文中，我们使用全连接层来实现词嵌入函数。<br/>​  综上所述，通过应用前两个步骤，图像可以转换为潜在源语言中的嵌入式句子，表示为<spanclass="math inline">\(V^{s}=[v_{1}^{s},v_{2}^{s},v_{3}^{s}...v_{32}^{s}]\in\mathbb{R}^{32\times512}\)</span>。</p><h3 id="语言翻译">3）语言翻译</h3><p>​  与现有的方法不同，这些方法为每个目标链分配一个独特的标签，我们的方法将链建模为目标语言空间中的句子，每个操作则由该句子中的一个词表示。句子的长度会随着操作链的长度而变化。从根本上说，我们的框架与任何机器翻译模型兼容。鉴于Transformer[31]在自然语言处理（NLP）和计算机视觉任务中的巨大成功，我们采用了Transformer[31]进行语言翻译。如图3所示，Transformer架构由编码器和解码器组成，类似于其他机器翻译框架。<br/>​  (i)在编码器端，Transformer为每个嵌入向量vi添加位置编码，以表示词在句子中的相对位置。与原始Transformer采用硬编码的位置嵌入策略不同，本文提出了一种可学习的方法来编码词的位置，正如[38]中所提出的。具体来说，<span class="math display">\[f_{i}=P o s i t i o n a l\;E n c o d i ng(i),\]</span> ​  其中，位置编码函数<span class="math inline">\(P o s i ti o n a l\;E n c o d i ng(\cdot)\)</span>将句子中的第i个位置映射到一个向量<spanclass="math inline">\(f_{i}\in\mathbb{R}^{1\times512}\)</span>。带有位置编码的嵌入词，<spanclass="math display">\[V_{0}^{s}=[v_{1}^{s}+f_{1},v_{2}^{s}+f_{2},...v_{32}^{s}+f_{32}]\,,\]</span>​  然后输入到Transformer编码器中。<br/>​  Transformer编码器的架构如图3(a)所示。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250607190658802.png"alt="image-20250607190658802" /><figcaption aria-hidden="true">image-20250607190658802</figcaption></figure><p>​  具体而言，我们构建了一个具有L层的Transformer编码器，其结构遵循[31]框架，其中每一层均包含一个前馈网络（FFN，feedforwardnetwork）和一个多头自注意力模块（MSA，multiheadself-attention）。编码程序可表述如下： <spanclass="math display">\[\begin{array}{l}Q_{\ell-1}^{s,i}=F C_{qi}(V_{\ell-1}^{s}),\\K_{\ell-1}^{s,i}=F C_{ki}(V_{\ell-1}^{s}),\\J_{\ell-1}^{s,i}=F C_{v i}(V_{\ell-1}^{s}),\\H e ad_{i}=SA(Q_{\ell-1}^{s,i},K_{\ell-1}^{s,i},J_{\ell-1}^{s,i}),\;\;\;\;\;\;i=1,...,U,\\T_{\ell-1}^{s}=LN\left(F C\left(c a t\left[H e a d_{1},...,H e ad_{U}\right]\right)+V_{\ell-1}^{s}\right),\\V_{\ell}^{s}=L N\left(F FN(T_{\ell-1}^{s})+T_{\ell-1}^{s}\right),\;\;\;\;\;\;\ell=1,...,L,\\[z_{1},z_{2},z_{3}...z_{32}]=Z^{s}=V_{L}^{s}.\end{array}\]</span>​  在此，U表示头的数量，<span class="math inline">\(F C_{xi}(\cdot)\)</span>是第i个全连接层的头，<spanclass="math inline">\(LN(\cdot)\)</span>表示层归一化操作，<spanclass="math inline">\(FFN(\cdot)\)</span>是由两个全连接层组成的块。<spanclass="math inline">\(SA(\cdot)\)</span>作为自注意力机制，其公式为 <spanclass="math display">\[S A(Q,K,J)=s o f t m a x\left({\frac{QK^{T}}{\sqrt{d_{k}}}}\right)J,\]</span>​  其中，Q、K和J分别对应查询矩阵、键矩阵和值矩阵。<spanclass="math inline">\(Z^{s}\in\mathbb{R}^{32\times512}\)</span>是编码器的输出结果。<br/>​  （ii）在解码器端，与其它转换方法类似，我们为每个句子分配一个起始符号（Os），以控制解码器的状态。针对目标操作链O1→O2→...→ON，每个操作都将被嵌入到向量空间中。具体如下：<span class="math display">\[\begin{array}{l}v_{s}^{t}=W o r d2V ec(O_{s}),\;\;\;\;\;\;v_{e}^{t}=W o r d2V e c(O_{e}),\\v_{i}^{t}=W^{}o rd2V e c(O_{i}^{t}),\;\;\;\;\;\;i=1,...,N.\end{array}\]</span>​  在我们的工作中，我们通过PyTorch深度学习框架的nn.embedding函数实现了Word2Vec（词向量）模型。具体而言，每个操作首先从操作字典中索引，然后将对应的索引映射到向量空间中。<br/>​  解码器的架构如图3(b)所示，由L层构成。每一层包含一个掩码多头自注意力机制（MSA）模块、一个标准多头自注意力机制（MSA）模块和一个带有跳过连接的全连接网络（FFN）模块。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250607191725710.png"alt="image-20250607191725710" /><figcaption aria-hidden="true">image-20250607191725710</figcaption></figure><p>​  Masked-MSA模块的作用是确保当前单词的解码不依赖于未解码的单词。与编码器类似，我们采用了可学习的位置嵌入策略来编码单词之间的相对位置。通过Masked-MSA（MMSA）模块，Transformer在训练阶段能够并行解码单词，从而同时输出目标操作链中所有操作的概率。数学上，整个解码过程可以表述如下：<spanclass="math display">\[\begin{array}{l}V_{0}^{t}=\left[v_{s}^{t}+f_{0},v_{1}^{t}+f_{1},v_{2}^{t}+f_{2},...v_{N}^{t}+f_{N}\right],\\He a d=M M SA\left(Q_{\ell-1},K_{\ell-1},J_{\ell-1}\right),\\G_{\ell-1}=L N\left({FC}(H e a d)+V_{\ell-1}^{t}\right),\\T_{\ell-1}^{t}=L N(M S A(FC_{q}(G_{\ell-1})\times F C_{k}(Z^{s}),FC_{v}(Z^{s}))+G_{\ell-1}),\\V_{\ell}^{t}=L N\left(F FN\left(T_{\ell-1}^{t}\right)+T_{\ell-1}^{t}\right),\ \ \ \ \\ell=1,...,L,\\\hat{y}=S o f t m a x\left(FC\left(V_{L}^{t}\right)\right).\end{array}\]</span>​  我们将Q−1定义为包含U个头查询矩阵的集合，其中<spanclass="math inline">\(Q_{\ell-1}^{i}=F C_{qi}(V_{\ell-1}^{t})\)</span>。同样地，我们也可以定义<spanclass="math inline">\(K_{\ell-1}\)</span>和<spanclass="math inline">\(J_{\ell-1}\)</span>。随后，式（11）中描述的MMSA（·）操作流程可表示为<span class="math display">\[\begin{array}{l}H e a d_{i}=S A_{m a s k ed}(Q_{\ell-1}^{i},K_{\ell-1}^{i},J_{\ell-1}^{i})\\H e a d=c a t\left[H ea d_{1},H e a d_{2},...,H e ad_{U}\right],\;i=1,...,U.\end{array}\]</span> ​  这里，<spanclass="math inline">\(SA_{masked}\)</span>表示Transformer中采用的掩码自注意力机制。在推理阶段，与其它机器翻译框架类似，Transformer会逐步解码输入句子，即逐词进行处理。这使得我们的TransDetect算法能够充分利用先前解码操作的信息，从而促进后续操作的识别。关于Transformer的更多细节，请参见[31]。在获得不同操作的概率后，我们选择概率最高的操作。</p><h2 id="c.链反转和双向建模">C.链反转和双向建模</h2><p>​  操作链检测面临的主要挑战之一是，早期操作留下的痕迹可能被后续操作严重削弱。如果使用早期低置信度子链的解码来辅助后期操作的解码，这将加剧错误的传播。本文提出了两种策略来解决这一问题，即链反转和双向建模。<br/>​  对于链反转策略，我们建议将链的顺序进行反转，即将原始目标链O1→O2→...→ON（从左到右，即L2R）转换为ON→ON−1→...→O1（从右到左，即R2L），然后再计算损失。这种反转方式确保了链中最后的操作，这些操作可能在图像上留下更明显的痕迹，首先被解码。由于Transformer的渐进解码特性，最早且高置信度解码的操作将作为先验信息，有助于识别那些痕迹较弱的前序操作。<br/>​  尽管链反转策略具有优势，但某些预测误差仍可能被传播，导致之前的操作难以检测。受[39]的启发，本研究进一步提出了一种双向建模策略，使解码器能够交互式地整合来自L2R和R2L方向的信息。来自相反方向的相互约束能够显著减少误差累积，从而提升最终检测性能。图4展示了我们提出的双向建模策略的具体流程。具体而言，类似于公式（11）中的第二个方程，我们首先计算L2R和R2L的注意力结果，通过<spanclass="math display">\[\begin{array}{l}\stackrel{\rightarrow}{H_{1}}\ =MM SA\left(\stackrel{\rightarrow}{Q}_{\ell-1},\stackrel{\rightarrow}{K}_{\ell-1},\stackrel{\rightarrow}{J}_{\ell-1}\right),\\\stackrel{\leftarrow}{H_{1}}\=M M SA\left(\stackrel{\leftarrow}{Q}_{\ell-1},\stackrel{\leftarrow}{K}_{\ell-1},\stackrel{\leftarrow}{J}_{\ell-1}\right),\end{array}\]</span>​  其中，MMSA（·）是在公式（12）中定义的Masked-MSA操作。在我们的研究中，我们使用→表示L2R方向，而←表示R2L方向。受跨注意力机制[40]的启发，我们提出了一种方法，通过交互方式整合两个相反方向的信息。<spanclass="math display">\[\begin{array}{l}\overset{\rightarrow}{H_{2}}\ =MM SA\left(\overset{\leftarrow}{Q}_{\ell-1},\overset{\rightarrow}{K}_{\ell-1},\overset{\rightarrow}{J}_{\ell-1}\right),\\\overset{\leftarrow}{H_{2}}\=M M SA\left(\overset{\rightarrow}{Q}_{\ell-1},\overset{\leftarrow}{K}_{\ell-1},\overset{\leftarrow}{J}_{\ell-1}\right),\end{array}\]</span>​  请注意，<spanclass="math inline">\(\stackrel{\rightarrow}{H_{1}}\)</span>和<spanclass="math inline">\(\stackrel{\leftarrow}{H_{2}}\)</span>包含了来自两个方向的信息。最终的注意力结果随后计算出来。<span class="math display">\[\begin{aligned}\overset{\leftarrow}{H}&amp;=\overset{\to}{\operatorname*{H_1}}+\lambda_1\cdot\overset{\to}{\operatorname*{H_2}},\\\overset{\leftarrow}{\operatorname*{H}} &amp;=\overset{\leftarrow}{\operatorname*{H_1}}+\lambda_2\cdot\overset{\leftarrow}{\operatorname*{H_2}},\\H &amp;=cat[\overset{\rightarrow}{H},\overset{\leftarrow}{H}]\end{aligned}\]</span>​  其中λ₁和λ₂是两个通过自适应学习得到的参数。<br/>​  对于双向建模，我们可直接用（15）中得到的H替换（11）中的Head，同时保持其他步骤不变。实验阶段将展示，我们的双向建模策略能显著提升检测精度。</p><h2 id="d.损失函数">D.损失函数</h2><p>​  当已知链长时，我们直接使用以下交叉熵损失进行网络训练： <spanclass="math display">\[\mathcal{L}=\frac{1}{KN}\sum_{k=1}^{K}\sum_{i=1}^{N}c r o s s e n t r o py\left(\hat{T}_{k,i},T_{k,i}\right),\]</span>​  其中K表示训练样本的数量，<spanclass="math inline">\(\hat{T}_{k,i}\)</span>，i代表估计链<spanclass="math inline">\(\hat{T}_{k}\)</span>的第i个操作，而<spanclass="math inline">\(T_{k,i}\)</span>，i则指目标链<spanclass="math inline">\(T_k\)</span>的第i个真实操作。在训练阶段，我们通过添加填充符号O0对所有链进行补全，使其长度固定为N。<br/>​  当链长度未知时，考虑到不同链类别和链长度之间的不平衡，我们有两种基本策略来训练TransDetect。<br/>​  1）平衡类别策略（BCS，BalancedCategoryStrategy）<br/>​  第一个策略是为每个类别分配相同数量的样本，这是分类问题中的自然设置。然然而，BCS在翻译语境中可能会造成严重的长度不平衡问题。由于随着链长(N)的增加，相应类别的数量<spanclass="math inline">\(K=A_{M}^{N}\)</span>呈指数增长，因此模型倾向于生成较长的链。如表II所示，在每个类别包含29个样本的情况下，BCS为长度为7和6的链分配了147153个样本，而长度为1的链仅分配了204个样本。我们的实证研究发现，即使真实链非常短，使用BCS训练的模型也更倾向于解码较长的链。<br/>​  2）平衡长度策略（BLS，BalancedLengthStrategy）<br/>​  另一种策略是确保每个链长对应相同数量的样本。然而，这会导致不同类别样本的数量严重失衡。如表II所示，在假设总样本数为400k且每个链长的样本数量相等的情况下，长度为1的链有7k个对应的样本，而长度为7的链只有10个样本。因此，模型在处理短链时容易过拟合，而在处理长链时表现不佳。<br/>​  3）加权交叉熵损失（WCEL）<br/>​  为了解决类别不平衡与长度不平衡之间的权衡问题，我们提出了一种用于网络训练的加权交叉熵损失函数（WCEL），其表达式如下：<span class="math display">\[{\mathcal{L}}={\frac{1}{KN}}\sum_{k=1}^{K}\gamma(N_{k})\sum_{i=1}^{N}c r o s s e n t r o py\left({\hat{T}}_{k,i},T_{k,i}\right).\]</span> ​  在这里，<spanclass="math inline">\(\gamma\left(N_{k}\right)\)</span>是一个与链长Nk相关的加权函数。在我们的实现中，我们根据BCS（贝叶斯条件概率）分配样本，并使用<spanclass="math inline">\(\gamma\left(N_{k}\right)\)</span>更加重视较短的链，从而解决了BCS引起的不平衡问题。由于链长增加时，链类别的数量呈指数增长，我们定义<spanclass="math inline">\(\gamma\left(N_{k}\right)\)</span>为 <spanclass="math display">\[\gamma(n)={\frac{e^{-p_{n}}}{\sum_{i=0}^{N}e^{-p_{i}}}},n=0,...,N,\]</span>​  其中 <spanclass="math display">\[p_{n}=\alpha\times\frac{A_{M}^{n}}{\sum_{i=0}^{N}A_{M}^{i}},\alpha\geq0.\]</span>​  在这里，N表示链的最大长度，M表示可能的操作数量。可以看出，γ(n)与链长n呈负相关，这使得模型更加关注较短的链。α是一个超参数，用于控制γ(n)与链长之间的斜率；相应的曲线如图5所示。如何选择合适的α将在第四部分中讨论。<br/>​  值得注意的是，通过操作链的顺序建模，正确检测到的子链不仅能减少公式（16）和（17）中定义的损失，还能明确指导网络训练。对于双向模型，公式（17）在两个方向上计算损失。</p><h1 id="iv.-实验结果">IV. 实验结果</h1><p>​  在本节中，我们介绍并分析了一组实验，这些实验旨在评估我们的TransDetect方法的有效性。我们使用PyTorch深度学习框架实现了该方法。在实验过程中，我们采用了以下设置：将ResBlock中的滤波器数量设为64，Transformer编码器和解码器的层数各设为6，MSA模块的头数设为U=8；训练过程中，每轮迭代150次，批量大小为24。用于这些实验的源代码将在https://github.com/YuanmanLi/github-TransDetect.上发布。</p><h2 id="a.数据集">A.数据集</h2><h2 id="b.对比方法和指标">B.对比方法和指标</h2><h2 id="c.检测两个操作的链">C.检测两个操作的链</h2><h2 id="d.检测已知长度的链">D.检测已知长度的链</h2><h2 id="e.检测未知长度的链">E.检测未知长度的链</h2><h2 id="f.跨数据集验证">F.跨数据集验证</h2><h2 id="h.更具挑战性的链条检测性能">H.更具挑战性的链条检测性能</h2><h1 id="v.图像修复检测的应用">V.图像修复检测的应用</h1><p>​  在本节中，我们通过一个简单的案例研究来进一步评估TransDetect的有效性，该案例研究涉及稳健的图像修复检测。图像修复是图像处理领域的一个基本任务，用于填补不完整图像中的缺失部分。同时，它也成为了伪造图像的强大工具，例如通过修改或删除图像内容来实现。在图像修复过程中，通常会应用一些后处理操作以减弱伪造痕迹。在我们的实验中，我们采用了HP-FCN[45]进行图像修复检测，并使用GC数据集[23]进行评估，该数据集包含48,000张训练图像和1,000张测试图像。为了简化，本案例研究仅考虑了四种后处理链：链A：GB→AWGN→JPEG，链B：GB→USM→JPEG，链C：GB→USM→AWGN→JPEG，以及链D：AWGN→GB→USM→JPEG。<br/>​  表十三展示了不同设置下的修复检测结果。</p><figure><imgsrc="../postimages/Image-Operation-Chain-Detection-with-Machine-Translation-Framework/image-20250607195059991.png"alt="image-20250607195059991" /><figcaption aria-hidden="true">image-20250607195059991</figcaption></figure><p>​  在首次实验中，我们通过随机应用上述四种操作链对训练图像进行数据增强，直接训练模型。最终模型的平均F1分数达到39.79%，AUC分数为81.84%。在第二次实验中，我们首先使用TransDect来估计给定测试图像的后处理链，然后利用这个估计的链对第一次实验中获得的模型进行一次微调。结果显示，通过TransDect估计的链微调后的模型，F1（AUC）提高了6.75%（即4.54%）。尽管案例研究较为简单，但这一显著的改进表明，有效揭示给定图像的退化历史对于实现稳健的伪造检测非常有帮助。值得注意的是，在实际应用中可能涉及许多其他后处理链，如何自动调整模型参数以适应这些估计的链，将是未来研究的一个有趣方向。</p><h2 id="a.讨论">A.讨论</h2><p>​  在我们的实验中，我们考虑了包含多达七个常用操作的链，以确保每个长度的ACC值保持在50%以上。感兴趣的读者可以将我们的方法应用于更长的链。尽管我们的方法已经比现有方法表现得更好，但要提高更长链的准确性，未来的研究仍需付出更多努力。<br/>​  类似于现有的操作链检测算法，如[4]和[5]，我们假设操作是在给定图像块的全局范围内进行的。有时，图像仅在局部被修改。在这种情况下，我们可以首先使用通用的图像取证方法来识别可能的操作区域，然后应用我们的算法来局部检测图像的操作链。此外，与现有方法类似，我们假设操作链中的操作来自一个已知的集合。实际上，还可能存在其他标准和定制的操作。如何最小化这些操作对检测方法的影响是一个挑战，也是一个未来值得探讨的有趣话题。<br/>​  实际上，当一个或几个操作被应用时，可以直接应用我们的方法来重建图像处理历史，从而获得足够的信息，以确定图像的真实性和来源。<br/>​  此外，实际上，一些常见的图像处理技术被广泛用于削弱或掩盖复杂数字伪造的指纹，例如复制-移动、拼接和图像修复。这些未知的操作会严重影响伪造检测器的性能[46]。在这种情况下，可以首先应用我们的框架来揭示后处理的历史。然后，利用这些信息，伪造检测器可以进行自适应调整，以提高对后处理操作负面影响的抵抗能力。如何将我们的方法与其它伪造检测算法有效结合，仍需未来更多的努力。</p><h1 id="vi.结论">VI.结论</h1><p>​  在本文中，我们提出了一种新的图像操作链检测方法。这是首次考虑包含多达七个操作的链，这导致了超过10,000种可能的解决方案。具体而言，我们在机器翻译框架内进行了操作链检测，这种方法与现有的基于分类模型的算法完全不同。在我们的框架中，每个操作用一个词表示，将操作链视为目标语言中的句子。图像操作链检测的目标是将原始图像空间映射到目标语言空间。为此，原始图像首先被转换成潜在的源语言空间，这些句子能够准确描述不同操作链留下的特征。<br/>​  接着，通过逐步将源语言中的句子转换为目标语言中的句子，来解码链。此外，我们提出了三种策略：分层特征提取、链反转、双向建模和加权交叉熵损失，以提升检测性能。广泛的实验已经证明了我们方案的优势。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Employing Reinforcement Learning to Construct a Decision-Making Environment for Image Forgery Localization</title>
      <link href="/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/"/>
      <url>/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/</url>
      
        <content type="html"><![CDATA[<p><strong>Employing Reinforcement Learning to Construct aDecision-Making Environment for Image Forgery Localization</strong></p><p>Rongxuan Peng , <em>Student Member, IEEE</em>, Shunquan Tan ,<em>Senior Member, IEEE</em>,</p><p>Xianbo Mo , <em>Student Member, IEEE</em>, Bin Li , <em>SeniorMember, IEEE</em>, and Jiwu Huang , <em>Fellow, IEEE</em></p><h1 id="摘要">摘要</h1><p>​  由于高级图像编辑工具和深度生成技术的广泛滥用，导致在现实场景中出现了大量内容被篡改的图像，通常没有任何明显的篡改痕迹。这给图像的安全性和可信度带来了潜在威胁。图像伪造定位是一项亟需的技术。在本文中，我们提出了一种基于强化学习的新框架CoDE（构建决策环境），该框架能够为伪造图像中的篡改区域提供可靠的定位结果。我们将伪造定位任务建模为马尔可夫决策过程（MDP），其中每个像素都配备了一个代理，该代理执行基于高斯分布的连续动作，以迭代更新相应的伪造概率，从而实现像素级别的图像伪造定位。为了在马尔可夫决策过程中构建状态转换，我们提出了一种双流状态编码器来处理更新后的状态，该状态包括伪造图像及其对应的伪造概率图。此外，考虑到在实际的图像篡改场景中，被篡改的区域通常较为稀疏，我们为此设计了一个专门针对这些稀疏区域的奖励函数。这种奖励机制能够引导智能体更高效地学习最大化累积奖励的最优策略。通过在多种基准数据集上进行的大量实验，CoDE在定位精度和对在线社交网络（OSNs）传输及各类后处理攻击导致的图像退化具有更强的鲁棒性方面表现出色。</p><h1 id="i.引言">I.引言</h1><p>​  图像编辑软件的普及使得数字图像伪造变得前所未有的容易。即使是那些没有专业知识的新手也能迅速学会如何篡改图像。因此，图像的真实性已成为一个重要问题，尤其是在需要高度准确性和可信度的情况下，如法庭证词、新闻报道、保险索赔、摄影比赛等。如今，不断涌现的篡改手段进一步加剧了这一问题。<br/>​  通常，如图1所示，篡改操作可分为以下三种类型：</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250604110835963.png"alt="image-20250604110835963" /><figcaption aria-hidden="true">image-20250604110835963</figcaption></figure><p>​    (1)剪辑：将一个图像的一部分叠加到另一个图像上；<br/>​    (2)复制移动：在同一图像中复制并粘贴特定区域；<br/>​    (3)删除：擦除图像的特定部分，然后进行修补。<br/>​  AdobePhotoshop（PS）是最常用的剪辑和复制移动操作软件。当然，还有其他相关工具，如GIMP和AfterEffectsPro等。随着深度学习的迅猛发展，图像修复技术[1]能够用于移除特定对象、水印、人脸和标签信息等。最近，基于生成对抗网络（GAN）的方法[2]，[3]，[4]以及基于扩散模型（DM）的方法[5]，[6]在图像修复领域展现了卓越的能力，能够有效恢复被移除的部分，这些部分在视觉上具有高度可信的纹理结构和语义内容。<br/>​  随着篡改技术日益先进，大多数伪造图像都会留下难以察觉的痕迹。因此，开发能够捕捉更细微篡改特征的方法变得尤为重要，以检测甚至定位图像伪造。传统方法[7]通常从图像中提取低级特征，如纹理、颜色和边缘信息，然后使用支持向量机（SVM）等机器学习算法来判断图像是否被伪造。然而，这些方法对于复杂的篡改操作可能效果不佳，并且需要一定的先验知识。近年来，深度学习技术在各种模式识别任务中的应用日益广泛，图像取证领域也不例外。在图像取证领域，深度学习技术被广泛应用于构建端到端的框架，这些框架能够预测伪造图像的伪造概率图，从而实现像素级别的图像伪造定位。早期的一些研究主要集中在解决特定类型的篡改操作，例如.[8]，[9]，[10]拼接，[11]，[12]复制-移动，[13]，[14]移除。然而，这些方法存在一个关键的弱点，即它们往往难以应对之前未见过的篡改类型。为了解决这一问题，最近的一些研究[15]，[16]，[17]，[18]，[19]，[20]，[21]，[22]，[23]，[24]采取了更为全面的方法，尽可能地考虑所有类型的篡改操作。<br/>​  MVSS-Net[25]通过结合多种特征提取方法和多尺度视觉注意力机制，提升了伪造定位的性能。<br/>​  PSCC-Net[26]提出了一种时空通道相关模块（SCCM），能够同时捕捉空间和通道的相关性。<br/>​  为了防御对抗性攻击，SAT-IFL[27]采用FGSM [28]生成的对抗样本进行对抗性训练。<br/>​  CAT-Net v2[29]提出了一个两阶段网络，用于捕捉RGB域中的图像采集伪影和DCT域中的压缩伪影。<br/>​  虽然最近的研究已取得重大进展，但仍有两大不足：(1)泛化能力有限；(2)鲁棒性不足。泛化能力主要指模型在多个数据集上的表现。如果测试集的数据分布与训练集有显著差异，模型的性能可能会大幅下降。为解决这一问题，多项研究[17]、[30]提出通过使用目标数据集的一个子集来微调预训练模型。需要注意的是，这种方法可能仅对用于微调的特定数据集有效，而对其他数据集则可能无效。鲁棒性主要指模型在面对图像退化时的表现。最近的研究[22]、[23]、[26]、[27]通过实施各种后处理攻击，如高斯噪声、调整大小、高斯模糊和JPEG压缩等，来评估模型在图像退化条件下的表现。然而，这些工作只考虑独立的而不是混合的图像后处理攻击，这可能不足以应对更复杂的场景。例如，Wu等人[31]发现，通过微信、Facebook等在线社交网络（OSNs）传输的图像会经历各种已知或未知的操作，这给定位篡改区域带来了更大的挑战，因为篡改痕迹在经过这些图像处理流程后可能会被严重削弱甚至完全消失。<br/>​  为了解决上述问题，我们提出了一种基于强化学习的新框架CoDE，用于像素级别的图像伪造定位。我们将伪造概率图的更新建模为马尔可夫决策过程（MDP）。在CoDE中，我们设计了一个基于A3C（AsynchronousAdvantage Actor-Critic，异步并行的 Actor-Critic方法）算法[32]的Actor-Critic网络来实现这一目标，其中actor是一个策略网络，根据当前状态选择行动；critic是一个价值网络，估计当前状态的预期价值。图像中的每个像素都有一个代理，这些代理并行学习以最大化累积奖励的最佳策略。与标准方法相比，这种方法的优势在于它允许代理进行探索，不断尝试新的行动，以更好地理解复杂环境，发现潜在的高奖励策略，从而提高泛化性能和鲁棒性。<br/>​  我们的主要贡献可总结如下：</p><ul><li>我们首先提出了一种基于强化学习的新框架CoDE，用于像素级图像伪造定位，其中代理在每个像素上在有限的步骤中迭代更新伪造概率图。</li><li>我们基于高斯分布定义了一个连续的动作空间，这显著提高了模型对各种图像后处理攻击的鲁棒性，即使没有相应的数据增强。</li><li>考虑到在实际图像篡改中常见的篡改区域分布稀疏，我们专门设计了一个奖励函数，使代理能够从反馈奖励中有效学习最优策略。</li><li>在多种基准数据集上进行的综合实验表明，CoDE显著优于现有最先进方法，并在抵抗在线社交网络传输引起的图像退化方面表现出优越的鲁棒性。</li></ul><p>​  本文其余部分安排如下：第II部分介绍如何利用强化学习方法对图像伪造定位任务进行建模；第III部分介绍实验设置的详细情况和全面的实验结果；最后第IV部分总结全文并展望未来的研究方向。</p><h1 id="ii.提出的方法">II.提出的方法</h1><p>​  在本节中，我们将详细阐述提出的像素级图像伪造定位的CoDE方法。首先，我们将简要介绍强化学习和A3C算法的概念。接着，我们将介绍CoDE的整体模型架构。然后，我们将描述如何定义强化学习中的关键要素，包括状态、动作、奖励函数、策略、演员和评论家。最后，我们将详细说明相应的优化算法。</p><h2 id="a.前言">A.前言</h2><p>​  强化学习是一种流行的机器学习范式，已被应用于机器人、游戏和控制等多个领域。近年来，一些研究开始将强化学习技术应用于图像取证领域。Jin等人[33]通过调整边界框的位置，利用预定义的离散动作，实现了篡改视频的粗略定位。Wei等人[34]和Chen等人[35]则在神经架构搜索（NAS）中应用了强化学习，专注于优化卷积神经网络，以提高图像取证的准确性。<br/>​  强化学习的核心在于MDP，它提供了一个数学框架，用于形式化环境中的顺序决策问题。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250604112108569.png"alt="image-20250604112108569" /><figcaption aria-hidden="true">image-20250604112108569</figcaption></figure><p>​  图2展示了概念性的交互过程，其中智能体通过采取行动、接收奖励并转移到新状态与环境互动。一个完整的MDP通常由多个交互过程组成，这些过程被称为一集。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250604112223716.png"alt="image-20250604112223716" /><figcaption aria-hidden="true">image-20250604112223716</figcaption></figure><p>​  如图3所示，我们为图像伪造定位任务设计了MDP。MDP由一个元组<spanclass="math inline">\(({\mathcal{S}},{\mathcal{A}},{\mathcal{P}},r,\gamma)\)</span>定义，其中<spanclass="math inline">\({\mathcal{S}}\)</span>表示可能的状态集合；<spanclass="math inline">\({\mathcal{A}}\)</span>表示可能的行动集合；<spanclass="math inline">\({\mathcal{P}}\)</span>表示智能体采用的策略。具体来说，<spanclass="math inline">\({\mathcal{P}}(a^{(t)}|{s^{(t)}})\)</span>表示在状态<spanclass="math inline">\(s^{(t)}\)</span>下采取行动<spanclass="math inline">\(a^{(t)}\)</span>的概率；<spanclass="math inline">\(r^{(t+1)}\)</span>表示从状态<spanclass="math inline">\(s^{(t)}\)</span>转移到<spanclass="math inline">\(s^{(t+1)}\)</span>时立即获得的奖励；<spanclass="math inline">\(\gamma\)</span>是折扣因子，用于平衡即时奖励与未来奖励之间的关系。<br/>​  异步优势Actor-Critic（A3C）是一种流行的强化学习框架。通过同时使用优势函数和时间差分误差，A3C能够有效平衡探索与开发，相比其他强化学习算法，可以实现更好的性能。A3C是actor-critic算法的扩展，它结合了基于策略和基于价值的方法，以实现更好的性能。actor负责学习在给定当前状态时选择动作的策略，而critic则估计从当前状态出发的预期价值。<br/>​  A3C中actor的公式更新定义为：<spanclass="math display">\[\mathcal{L}_{\theta_{a}}=\beta\mathcal{H}(\mathcal{P}_{\theta_{a}}(s^{(t)}))+log\mathcal{P}_{\theta_{a}}(a^{(t)}|s^{(t)})A(s^{(t)},a^{(t)})\]</span></p><p><spanclass="math display">\[\theta_{a}=\theta_{a}+\eta\nabla_{\theta_{a}}Z_{\theta_{a}}\]</span></p><p>​  其中，<spanclass="math inline">\(\theta_{a}\)</span>是actor网络的参数，<spanclass="math inline">\(\mathcal{L}_{\theta_{a}}\)</span>是相应的优化目标，该目标分为两部分。<spanclass="math inline">\(\mathcal{H}(\mathcal{P}_{\theta_{a}}(s^{(t)}))\)</span>表示在给定状态<spanclass="math inline">\(s^{(t)}\)</span>下所有可能动作的熵，作为正则化项以促进探索；<spanclass="math inline">\(\beta\)</span>是一个控制熵正则化强度的系数；<spanclass="math inline">\(\mathcal{P}_{\theta_{a}}(a^{(t)}|s^{(t)})\)</span>表示在当前策略下，处于状态<spanclass="math inline">\(s^{(t)}\)</span>下采取行动<spanclass="math inline">\(a^{(t)}\)</span>的的概率；<spanclass="math inline">\(A(s^{(t)},a^{(t)})\)</span>是优势函数，用于评估在状态<spanclass="math inline">\(s^{(t)}\)</span>时采取动作<spanclass="math inline">\(a^{(t)}\)</span>的优势，其计算方式如下： <spanclass="math display">\[A(s^{(t)},a^{(t)})=Q(s^{(t)},a^{(t)})-V(s^{(t)})\]</span>​  其中，<span class="math inline">\(V(s^{(t)})\)</span>表示状态<spanclass="math inline">\(s^{(t)}\)</span>的期望值；<spanclass="math inline">\(Q(s^{(t)},a^{(t)})\)</span>则表示在状态<spanclass="math inline">\(s^{(t)}\)</span>下采取动作<spanclass="math inline">\(a^{(t)}\)</span>的期望值，通常通过单步近似法进行估算，其公式如下：<span class="math display">\[Q(s^{(t)},a^{(t)})=\left\{\begin{array}{ll}{V(s^{(t)}),}&amp;{i f\ s t o p,}\\ {r^{(t+1)}+\gammaQ(s^{(t+1)},a^{(t+1)}),}&amp;{e l s e;}\end{array}\right.\]</span>​  这里“if stop”指的是状态<spanclass="math inline">\(s^{(t)}\)</span>是终状态。此时<spanclass="math inline">\(Q(s^{(t+1)},a^{(t+1)})\)</span>等于<spanclass="math inline">\(V(s^{(t)}\)</span>，因为没有发生状态转换。<br/>​  A3C中对critic的公式更新定义为：<spanclass="math display">\[\mathcal{L}_{\theta_{c}}=A(s^{(t)},a^{(t)})^{2}\]</span></p><p><spanclass="math display">\[\theta_{c}=\theta_{c}-\eta\nabla_{\theta_{c}}\mathcal{L}_{\theta_{c}}\]</span></p><p>​  其中<spanclass="math inline">\(\theta_{c}\)</span>代表critic网络的参数，而<spanclass="math inline">\(\mathcal{L}_{\theta_{c}}\)</span>则是相应的优化目标，即优势函数的平方。</p><h2id="b.在mdp中定义图像伪造定位的元素">B.在MDP中定义图像伪造定位的元素</h2><p>​  在下文中，我们将详细阐述如何在MDP中定义图像伪造定位任务的基本元素，如图3所示。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250604112223716.png"alt="image-20250604112223716" /><figcaption aria-hidden="true">image-20250604112223716</figcaption></figure><h3 id="状态空间">1）状态空间：</h3><p>​  状态表示应设计为包含任务中最相关的信息。对于图像伪造定位，它当然应包括伪造图像（<spanclass="math inline">\(I_{f}\in\mathbb{R}^{H\timesW\times3}\)</span>），因为我们需要捕捉到细微的篡改痕迹，以指导模型做出精准决策。其中H和W分别表示图像的高和宽。此外，由于我们的目标是预测二进制掩码<spanclass="math inline">\(M_{f}\in\mathbb{R}^{H\timesW}\)</span>，所以在某一事件的时间步t处的伪造概率图<spanclass="math inline">\(P_{f}^{(t)}\in\mathbb{R}^{H\timesW}\)</span>也是必不可少的。因此，根据等式(7)的表述，状态空间由两个主要部分构成：<br/>​    (1)伪造的图像，<spanclass="math inline">\(I_{f}\)</span>；<br/>​    (2)时间步长t处的伪造概率图<spanclass="math inline">\(P_{f}^{(t)}\)</span>。<br/>​  将<spanclass="math inline">\(I_{f}\)</span>和<spanclass="math inline">\(P_{f}^{(t)}\)</span>均除以255，以便在[0,1]范围内进行归一化。<span class="math display">\[s^{(t)}=(I_{f},P_{f}{}^{(t)})\]</span></p><h3 id="动作空间">2）动作空间：</h3><p>​  动作空间的设计基于状态变量的变化。由于我们的目标是迭代更新所有像素的伪造概率（即伪造概率图<spanclass="math inline">\(P_{f}^{(t)}\)</span>)，因此我们的动作自然涉及伪造概率的加减，如公式等式(8)所示。<span class="math display">\[P_{f}^{(t+1)}=P_{f}^{(t)}+a^{(t)}\]</span>​  在强化学习领域，动作空间主要分为离散型和连续型两种。本研究中，我们基于高斯分布构建了连续型动作空间，并为每个像素设置独立的高斯分布进行动作采样，具体公式见等式(9)。<spanclass="math display">\[a^{(t)}\sim\mathcal{N}(\mu^{(t)},(\sigma^{2})^{(t)})\]</span>​  其中，<span class="math inline">\(a^{(t)}\ \in\mathbb{R}^{H\timesW}\)</span>、<span class="math inline">\(\mu^{(t)}\in\mathbb{R}^{H\timesW}\)</span>和<span class="math inline">\((\sigma^{2})^{(t)} \ \in\ \\mathbb{R}^{H\timesW}\)</span>分别表示时间步长t时的动作图、均值图和方差图。与离散动作空间（例如[±0.5，±0.2，±0.1，±0.0]）不同，连续动作空间拥有更宽泛的数值范围，这使得智能体能够全面探索环境并优化策略。此外，我们发现使用其他具有可变分布形状的统计分布，如贝塔分布时，会导致像素决策出现显著波动。这种波动性使得代理难以学习有效的策略。相比之下，高斯分布是一个单峰、对称的钟形曲线，这意味着它在平均值附近具有较高的概率密度。这种稳定性使代理在训练过程中能够更有效地探索并学习到更好的策略。<br/>​  由于我们在一个回合中通过多个时间步长来采样动作以更新伪造概率图，因此需要控制均值图和方差图的值范围。这种控制机制通过限制极端或不切实际的动作，防止过度探索，从而提高智能体收敛到最优策略的效率。因此，我们引入了一个缩放因子<spanclass="math inline">\(\lambda\)</span>来限制<spanclass="math inline">\(\mu^{(t)}\)</span>的范围。<spanclass="math inline">\(\sigma^{2}\)</span>的范围则由actor网络中的激活函数控制（请参见第II-C.2节）。随后，动作图<spanclass="math inline">\(a^{(t)}\)</span>的公式如下： <spanclass="math display">\[a^{(t)}\sim\mathcal{N}(\lambda\mu^{(t)},(\sigma^{2})^{(t)})\]</span>​  然而，采样操作（公式（10）)不可导，导致反向传播在更新<spanclass="math inline">\(\mu^{(t)}\)</span>和<spanclass="math inline">\((\sigma^{2})^{(t)}\)</span>的梯度时失败。为了解决这一问题，采用了重参数化技术。如等式（11）所示，首先从标准正态分布<spanclass="math inline">\({\mathcal{N}}(0,1)\)</span>中抽取一个矩阵变量<spanclass="math inline">\(\xi\in\mathbb{R}^{H\timesW}\)</span>，然后通过等式（12）转换，得到动作图<spanclass="math inline">\(a^{(t)}\)</span>，这相当于直接从<spanclass="math inline">\(\mathcal{N}(\lambda\mu^{(t)},(\sigma^{2})^{(t)})\)</span>中抽样。这样，在训练过程中可以计算并反向传播<spanclass="math inline">\(\mu^{(t)}\)</span>和<spanclass="math inline">\((\sigma^{2})^{(t)}\)</span>相对于<spanclass="math inline">\(a^{(t)}\)</span>的梯度。 <spanclass="math display">\[\xi\sim{\mathcal{N}}(0,1)\]</span></p><p><spanclass="math display">\[a^{(t)}=\lambda\mu^{(t)}+\xi\odot\sqrt{(\sigma^{2})^{(t)}}\]</span></p><p>​  请注意，伪造概率图<spanclass="math inline">\(P_{f}^{(t)}\)</span>的范围在[0,1]内。然而，通过等式（12）计算得到的<spanclass="math inline">\(a^{(t)}\)</span>范围可能无限大。为了确保<spanclass="math inline">\(P_{f}^{(t)}\)</span>保持在有效范围内，我们对<spanclass="math inline">\(a^{(t)}\)</span>和<spanclass="math inline">\(P_{f}^{(t)}\)</span>在[0,1]区间内进行了截断处理。<br/>​  在测试阶段，代理执行确定性策略，即不再需要$(<sup>{2})</sup>{(t)}<span class="math inline">\(来进行探索，仅使用\)</span><sup>{(t)}<spanclass="math inline">\(作为动作图。因此，动作图\)</span>a</sup>{(t)}<spanclass="math inline">\(可以统一表示为：\)</span><spanclass="math inline">\(a^{(t)}=\left\{\begin{array}{l}{\lambda\mu^{(t)}+\xi\odot\sqrt{(\sigma^{2})^{(t)}},\i f\ t r a i n,}\\ {\lambda\mu^{(t)},}\end{array}\right.\)</span>$</p><h3 id="奖励函数">  3）奖励函数：</h3><p>​  奖励函数的设计至关重要，因为它在训练过程中为智能体提供了关键的反馈信息。因此，奖励函数应根据具体任务的特点进行定制。为了更新像素的伪造概率，我们需要定义一个映射函数，该函数能够给出伪造概率的映射值，从而确定智能体因更新伪造概率而获得的奖励。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250604115900146.png"alt="image-20250604115900146" /><figcaption aria-hidden="true">image-20250604115900146</figcaption></figure>​  图4a展示了一个单个像素伪造概率的线性映射函数，其公式如下： <spanclass="math display">\[\begin{aligned}f_{lin}(x,GT) &amp;=\left\{\begin{array}{ll}1-x, &amp; \mathrm{if~}GT=0, \\x, &amp;\mathrm{if~}GT=1.\end{array}\right. \\ &amp; =GT\cdotx+(1-GT)\cdot(1-x)\end{aligned}\]</span>​  其中x表示像素的伪造概率。而“GT”的含义如下：“GT=1”表示该像素为伪造，而“GT=0”表示该像素为原始。<br/>​  随后，我们提出了一种基础奖励函数<spanclass="math inline">\(r_{bs}^{(t)}\)</span>。该设计基于等式（14）的直观理念，直接将两次伪造概率的差异转化为奖励。为了在不同GT条件下区分正负奖励，等式（14）被设计为分段函数形式。<spanclass="math inline">\(r_{bs}^{(t)}\)</span>的具体公式如下： <spanclass="math display">\[\begin{aligned}r_{bs}^{(t)} &amp; = f_{l in}(x^{(t)},G T)-f_{l i n}(x^{(t-1)},G T)\\ &amp; =2(GT-0.5)(x^{(t)}-x^{(t-1)})\end{aligned}\]</span>​  在这里，t表示完整更新过程中的时间步。需要注意的是，初始伪造概率被设定为零（即<spanclass="math inline">\(x^{(0)}\ =\ 0\)</span>)。<spanclass="math inline">\(r_{bs}^{(t)}\)</span>表明奖励与伪造概率呈线性相关。例如，当<spanclass="math inline">\(x^{(0)}=0,x^{(1)}\ =0.2\)</span>时，<spanclass="math inline">\(r_{bs}^{(t)}\)</span>（<spanclass="math inline">\(x^{(0)}\)</span>，<spanclass="math inline">\(x^{(1)}\)</span>，GT=1）和<spanclass="math inline">\(r_{bs}^{(t)}\)</span>（<spanclass="math inline">\(x^{(0)}\)</span>，<spanclass="math inline">\(x^{(1)}\)</span>，GT=0）的奖励结果完全相反。然而，在伪造图像中，原始区域的比例通常大于篡改区域。同时，代理在训练初期采用的策略非常不理想。这导致代理在原始区域采取错误行动的比例远高于正确行动，从而遭受了较大的惩罚。随着时间的推移，持续的负面奖励促使代理倾向于采取微小的更新动作以避免重大惩罚，这反而不利于策略的优化。<br/>​  基于上述分析，我们提出了一种新的奖励函数设计原则，该原则主张在篡改区域采取正确行动时给予更高的奖励，同时减少在原始区域采取错误行动的惩罚。二元交叉熵与这一奖励函数设计原则非常契合，因此，我们首先介绍了一种基于二元交叉熵的映射函数<spanclass="math inline">\(f_{b c e}(x,G T)\)</span>，如图4b所示，其公式为：<span class="math display">\[\begin{aligned}f_{b c e}(x,G T) &amp;=\left\{\begin{array}{ll}-\log(1-x+\epsilon), &amp; \mathrm{if~}GT=0,\\-\log(x+\epsilon),, &amp; \mathrm{if~}GT=1.\end{array}\right. \\ &amp;-G T\cdot l o g(x+\epsilon)-(1-G T)\cdot l og(1-x+\epsilon)\end{aligned}\]</span> ​  考虑到<spanclass="math inline">\(f_{b c e}(x=0,\,GT=1)\)</span>和<spanclass="math inline">\(f_{b ce}(x=0,\,GT=0)\)</span>的无限值可能导致训练崩溃，我们通过引入常数<spanclass="math inline">\(\epsilon=1\times10^{-8}\)</span>来限制其值范围，从而实现简单的优化。<br/>​  随后，基于二进制交叉熵的奖励函数<spanclass="math inline">\(r_{bs}^{(t)}\)</span>被定义为： $$<span class="math display">\[\begin{array}{l}r_{b c e}^{(t)}&amp;=-(f_{bc e}(x^{(t)},G T)-f_{b c e}(x^{(t-1)},G T))\\&amp;=G T\cdot l og(\frac{x^{(t)}+\epsilon}{x^{(t-1)}+\epsilon})\\&amp;+(1-G T)\cdot l og(\frac{1-x^{(t)}+\epsilon}{1-x^{(t-1)}+\epsilon})\end{array}\]</span><p><span class="math display">\[​&amp;emsp;&amp;emsp;为了确保当$x^{(t)}$接近GT时给予正奖励，而当$x^{(t)}$离开GT时给予负奖励，在$f_{bc e}(x^{(t)},G T)-f_{b c e}(x^{(t-1)},GT)$的表达式前加上负号。例如，给定$x^{(0)}=0,x^{(1)}\=0.2$，正奖励$r_{bs}^{(t)}$（$x^{(0)}$，$x^{(1)}$，GT=1）相对于负奖励$r_{bs}^{(t)}$（$x^{(0)}$，$x^{(1)}$，GT=0）的绝对值要大得多。&lt;br/&gt;​&amp;emsp;&amp;emsp;上述关于两个奖励函数等式（15）和等式（17）的讨论，都是基于单个像素的。我们的任务目标是生成整个图像的伪造概率图$P_{f}\in\mathbb{R}^{H\timesW}$。为了使讨论更加全面，我们将单个像素的伪造概率x扩展到整个图像的伪造概率图$P_{f}$。因此，基本奖励函数$r_{bs}(t)\in\mathbb{R}^{H\timesW}$通过等式（18）定义，而基于BCE的奖励函数$r_{bc}e^{(t)}\in\mathbb{R}^{H\times W}$则通过等式（19）定义。\]</span>r_{b c e}^{(t)}=2(M_{gt}-0.5),,(P_{f}{}<sup>{(t)}-P_{f}{}</sup>{(t-1)})$$</p><p><span class="math display">\[\begin{array}{l}r_{b ce}{}^{(t)}=&amp;M_{g t}\odot l og(\frac{P_{f}{}^{(t)}+\epsilon}{P_{f}{}^{(t-1)}+\epsilon})\\&amp;+\left(1-M_{gt}\right)\odot l og\bigl(\frac{1-P_{f}(t)+\epsilon}{1-P_{f}(t-1)+\epsilon}\bigl)\end{array}\]</span></p><p>​  这里的粗体部分是矩阵变量，Mgt是真实值的掩码。</p><p>​  为了验证我们关于两个奖励函数（公式（18）和等式（19）)的设计假设，我们在第三部分B.1节中展示了它们对比的实验结果。</p><h2 id="c.网络架构">C.网络架构</h2><p>​  图5展示了模型，图6展示了各模块的结构图。整个模型由三部分组成：孪生流状态编码器、行为网络和评价网络。接下来我们将详细介绍：</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250606163728166.png"alt="image-20250606163728166" /><figcaption aria-hidden="true">image-20250606163728166</figcaption></figure><h3 id="双流状态编码器">1）双流状态编码器：</h3><p>​  状态编码器旨在提取状态特征并满足状态转换，分为两部分：(1)伪造编码器；(2)伪造概率图编码器（FPM-E）。如图5所示，伪造编码器通过五个伪造编码器模块，从伪造图像中提取层次特征，其形式为<spanclass="math inline">\(I_{f}\in\mathbb{R}^{H\times W\times3}\)</span>。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250606163425108.png"alt="image-20250606163425108" /><figcaption aria-hidden="true">image-20250606163425108</figcaption></figure><p>​  为了实现快速训练，我们采用了ImageNet预训练的EfficientNetb4[36]模型，并去除了其分类层。FPM-E设计用于编码伪造概率图<spanclass="math inline">\(P_{f}^{(t)}\in\mathbb{R}^{H\timesW}\)</span>，该图可以整合到actor-critic网络中，以指导actor做出相应的决策。FPM-E由两个FPMC（伪造概率图卷积）模块组成，如图6b所示。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250606164932650.png"alt="image-20250606164932650" /><figcaption aria-hidden="true">image-20250606164932650</figcaption></figure><p>​  W表示伪造概率图。</p><p>​  每个FPMC模块中，首先使用一个1×1的无偏卷积层将通道维度从1扩展到48，随后是两个3×3的卷积层，其中最后一个卷积层具有2×2的步幅，用于下采样。在伪造编码器和FPM-E中，每次下采样后特征图的大小都会减半。<br/>​  实际上，我们可以通过在通道维度上将<spanclass="math inline">\(I_f\)</span>和<spanclass="math inline">\(P_f^{(t)}\)</span>连接起来，使用一个编码器同时处理这两个信号，即<spanclass="math inline">\([I_f,P_f^{(t)}]\in\mathbb{R}^{H\timesW\times4}\)</span>。然而，这种方法需要较长的推理时间，因为状态需要在一个回合中多次更新。为了减少推理时间，我们提出了一种双流架构，该架构仅通过FPM-E即可实现状态转换，伪造编码器在一个回合中只需运行一次。我们在第III-B.2节中介绍了两种状态处理模式在伪造定位性能和推理时间上的比较实验。</p><h3 id="actor网络">2）Actor网络：</h3><p>​  Actor网络通过输出均值图<spanclass="math inline">\(\mu^{(t)}\in\mathbb{R}^{H\timesW}\)</span>和方差图<span class="math inline">\((\sigma^{2})^{(t)}\in\\mathbb{R}^{H\timesW}\)</span>，为每个像素构建独立的高斯分布模型。随后，代理可以根据这些数据生成动作图<spanclass="math inline">\(a^{(t)}\in\mathbb{R}^{H\timesW}\)</span>。演员网络通过四个基础解码器模块（如图6a所示）实现四倍2×上采样处理。此外，为了加速模型的收敛，我们采用了U-Net[37]架构，将前四个伪造编码器模块与上述四个基本解码器模块连接起来。在每个基本解码器模块中，首先使用一个非学习的上采样层，采用最近邻插值法，然后是一个3×3卷积层。连接后，再使用两个3×3卷积层。<br/>​  最后，为了输出<spanclass="math inline">\(\mu^{(t)}\)</span>和<spanclass="math inline">\((\sigma^{2})^{(t)}\)</span>，我们采用了双分支架构，即均值块和方差块。如图6c和图6d所示，这两个模块都以相同的特征图作为输入，该特征图由最终的基本解码器块和FPMC块的输出拼接而成。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250606165123913.png"alt="image-20250606165123913" /><figcaption aria-hidden="true">image-20250606165123913</figcaption></figure><p>​  X表示最终伪造编码器模块或之前的基解码器模块的输出。Z表示FPMC模块的输出。</p><p>​  在结构上，它们均包含一个上采样层和三个3×3卷积层。它们仅在最后一个3×3卷积层的激活函数上有所不同，其中均值块使用Softsign(<spanclass="math inline">\(S o f t s i g n(x)={\frac{x}{\vertx\vert+1}}\)</span>)来限制<spanclass="math inline">\(\mu^{(t)}\)</span>在(−1<em>,</em>1)的范围内。方差块则使用Softplus(<span class="math inline">\(S o f t p lu s(x)=\log(e^{x}+1)\)</span>)来确保在(0,+∞)的范围内。<br/>​  为了研究动作空间大小对定位性能的影响，我们引入了一个缩放因子λ来限定<spanclass="math inline">\(\mu^{(t)}\)</span>的值域。此外，我们还探讨了通过Sigmoid函数将<spanclass="math inline">\((\sigma^{2})^{(t)}\)</span>的值域限制在0到1之间的效果。比较实验的结果详见第三部分B.3节。</p><h3 id="critic网络">3）critic网络：</h3><p>​  critic网络旨在估计当前状态下的值图<spanclass="math inline">\(V^{(t)}\in\mathbb{R}^{H\timesW}\)</span>。如图5所示，该网络还使用了四个基本解码器块进行四倍2×上采样，并与前四个伪造编码器块采用相同的U-Net[37]连接方式。最后，值块将最终的基本解码器块和FPMC块的输出合并。如图6e所示，值块包含一个上采样层和两个3×3卷积层。最后一个3×3卷积层中的激活函数为Sigmoid(<spanclass="math inline">\(S i g m o id(x)={\frac{1}{1+e^{-x}}}\)</span>)，以确保<spanclass="math inline">\(V^{(t)}\)</span>的值保持在0到1之间。</p><h2 id="d.训练阶段">D.训练阶段</h2><p>​  训练阶段如算法1所示，可分为三个部分：</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250606174635568.png"alt="image-20250606174635568" /><figcaption aria-hidden="true">image-20250606174635568</figcaption></figure><h3 id="初始化">1）初始化：</h3><p>​  第一步是初始化网络的参数。每当新一轮开始时，初始伪造概率图<spanclass="math inline">\(Pf^{(0)}\)</span>被设为零。因此，初始状态<spanclass="math inline">\(s^{(0)}\)</span>被定义为<spanclass="math inline">\((I_f,Pf^{(0)})\)</span>。奖励图r(0)在开始时也被设置为零，因为还没有发生任何状态转换。</p><h3 id="迭代更新状态">2）迭代更新状态：</h3><p>​  在第t轮，当前状态s（t−1）被输入模型，以获取均值图µ（t−1）、方差图（σ2）（t−1）和价值图V（t−1）。随后，均值图µ（t−1）经过λ的缩放，记为λµ（t−1）。接着，从标准正态分布N（0,1）中抽取ξ∈RH×W，并重新参数化以获得动作图a（t−1），具体公式见等式（12）。代理根据Pf（t−1）执行a（t−1），以生成新的伪造概率图Pf(t)，该图稍后会被截断到范围[0,1]。奖励图r(t)可通过等式（19）计算得出。最后，这一过程会再次使用更新后的状态s(t)重复进行。</p><h3 id="累积损失和更新参数">3）累积损失和更新参数：</h3><p>​  在完成一轮后，计算并累积演员网络和评论家网络的损失<spanclass="math inline">\(\cal L_{\theta_a}\)</span>和<spanclass="math inline">\(\cal L_{\theta_c}\)</span>。首先将总损失<spanclass="math inline">\(\calL_{total}\)</span>重置为零。从最后一轮的反转开始，我们按以下方式累积<spanclass="math inline">\(\cal L_{total}\)</span>： <spanclass="math display">\[\mathcal{L}_{t o t a l}=\mathcal{L}_{t o t a l}+Av e r a g e(\frac{\calL_{\theta_{c}}}{2}-\mathcal{L}_{\theta_{a}})\]</span>​  需要注意的是，优化目标在于最大化行为者（公式(2))中的优势函数和熵，并最小化评估者（公式(6))中优势函数的均方误差。因此，参数更新采用以下公式，其中η表示学习率。<spanclass="math display">\[\theta_{a}=\theta_{a}-\eta\nabla_{\theta_{a}}\mathcal{L}_{to t al},\quad\theta_{c}=\theta_{c}-\eta\nabla_{\theta_{c}}\mathcal{L}_{t o ta l}\]</span>​  此外，由于我们的动作空间遵循高斯分布，等式(1)中的熵是通过以下公式计算的：<spanclass="math display">\[\mathcal{H}(P_{\theta_{a}}(s_{t}))=\frac{1}{2}\left(\ln(2\pi(\sigma^{2})^{(t)}+1)\right)\]</span></p><h2 id="e.测试阶段">E.测试阶段</h2><p>​  测试阶段如算法2所示。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701114433240.png"alt="image-20250701114433240" /><figcaption aria-hidden="true">image-20250701114433240</figcaption></figure><p>​  首先，初始伪造概率图Pf(0)被设为零。与训练阶段不同，代理无需进一步探索，而是采用确定性策略来完成任务。具体来说，在状态s(t)传递后，我们仅保留平均图μ(t)，而不是从高斯分布N（μ(t)，σ²(t)）中抽样，缩放后的平均图λμ(t)直接作为动作图a(t)使用。随后，通过执行(t)更新Pf(t)。一旦剧集结束，我们对最终的伪造概率图<spanclass="math inline">\(P_{f}^{(L_{e pt})}\)</span>应用二值化处理，使用固定的伪造阈值δ生成二进制掩码M_f。</p><h1 id="iii.实验">III.实验</h1><p>​  在本节中，我们评估了CoDE在各种基准数据集上的表现，并与最先进的方法进行了比较。我们还分析了CoDE对不同后处理攻击的鲁棒性，包括调整大小、高斯模糊、高斯噪声、JPEG压缩和裁剪等，并设置了不同的攻击强度。此外，我们展示了CoDE在来自不同主流在线社交网络的数据集上的泛化能力和鲁棒性，这些社交网络包括Facebook、微信、微博和WhatsApp。</p><h2 id="a.实验设置">A.实验设置</h2><p>​  1)数据集：<br/>​  我们在实验中采用了以下基准数据集。所有伪造图像均配有对应的真值掩码。</p><ul><li><p>COLUMBIA数据集[38]由160张伪造图像组成，这些图像完全通过拼接技术创建。</p></li><li><p>COVERAGE[39]是一个包含100张伪造图像的复制-移动数据集。所有伪造图像都经过了后期处理，以掩盖操作痕迹。</p></li><li><p>CASIA数据集[40]的v1版本包含960张伪造图像，而v2版本则有5123张。这些伪造图像主要通过剪辑和复制移动的方式进行篡改，部分图像还经过了滤镜处理和模糊等后期修饰，以掩盖篡改痕迹。</p></li><li><p>NIST16数据集[41]由564张伪造图像组成，包含三种篡改类型：拼接、复制移动和删除。</p></li><li><p>IMD2020数据集[42]包含2,010张伪造图像，其原始版本均来自互联网。所采用的篡改技术包括拼接、复制移动和删除。</p></li><li><p>TampCOCO数据集由CAT-Net v2[29]提供，包含80万张伪造图像，涉及两种篡改方式：拼接和复制移动。我们仅从该数据集中随机选取了0.5%的伪造图像用于训练，而CAT-Netv2则使用了全部数据。</p></li><li><p>DSO[43]是一个剪接数据集，专注于人类受试者操作，包含100张伪造图像。</p></li><li><p>由IF-OSN[31]提供的OSNs数据集包含了来自COLUMBIA、NIST16、DSO和CASIAv1的伪造图像。所有这些图像均通过Facebook、微博、微信和WhatsApp等社交媒体平台传播。</p></li><li><p>OpenForensics[44]是一个面部伪造数据集，具有极高的挑战性。测试集经过了多种处理，包括色调、亮度和饱和度的调整，JPEG压缩，锐化，添加噪声等，以更真实地模拟自然环境中的背景。我们随机选取了500张伪造图像进行评估。</p></li><li><p>CocoGlide数据集由TruFor [45]使用扩散模型GLIDE[46]提供，包含512张伪造图像。</p></li><li><p>FF++[47]是一个面部伪造数据集，旨在检测视频中的伪造行为。该数据集由多种基于生成对抗网络（GAN）的模型生成，如Face2Face、NeuralTextures和FaceSwap。在本研究中，我们使用了该数据集的中等压缩版本（c23）。从每个模型中随机选取了100个伪造视频，并从每个视频中随机抽取了两帧，最终生成了600张用于评估的伪造图像。</p></li></ul><p>​  2）训练模式：<br/>​  我们对不同的训练模式进行了实验，具体介绍如下：</p><ul><li>基准测试：模型分别在每个基准数据集上进行训练，并在相应的测试分割上进行测试。</li><li>预训练：模型仅在外部数据集上进行训练，然后直接在目标数据集的测试分割上进行评估。</li><li>微调：首先使用预训练权重初始化模型，然后在目标数据集的训练分割上进行进一步微调，并在目标数据集的测试分割上进行评估。</li></ul><p>​  在第三节C部分，对于基准训练，我们保持了与PSCC-Net[26]相同的训练与测试比例，即75%：25%。在微调阶段，我们首先在CASIAv2、0.5%TampCOCO和IMD2020数据集上预训练CoDE，然后在每个数据集的训练集上进行微调（训练与测试的比例与基准训练模式一致）。在第三节D部分，我们展示了预训练模式下的泛化性能。我们在CASIAv2、0.5%TampCOCO和IMD2020数据集上训练CoDE，然后直接在这些数据集上进行测试。需要注意的是，我们的训练集中仅包含大约11,000张伪造图像，而CAT-Netv2 [29]和TruFor [45]则使用了约850,000张图像。</p><p>​  3）评价指标：<br/>​  在我们的实验中，采用F1分数和交并比（IoU）作为评价指标。<br/>​  F1是精确度和召回率的调和平均值，是定位结果准确性和完整性的衡量指标，其取值范围在0到1之间，其中1表示定位性能达到完美。<span class="math display">\[F_{1}=\frac{2\times P r e c i s i o n\timesR e c a l l}{P r e c i s i o n+R e c a l l}\]</span>​  IoU是真实区域与预测篡改区域的交并比，其取值范围为0到1，其中1表示定位性能达到完美。<spanclass="math display">\[\mathrm{IoU}={\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}+\mathrm{FN}}}\]</span>​  此外，为了提供全面的评估，在表IV、表V和表VII中，我们计算了所有数据集上的F1和IoU的加权平均值，其中权重基于各自伪造图像的数量。<br/>​  实际上，在其他关于图像伪造定位的论文中，AUC（ROC曲线下面积）也经常被采用。然而，在实际的图像伪造定位场景中，篡改区域远小于原始区域，这导致了严重的类别不平衡问题。在这种情况下，ROC曲线可能会受到原始区域像素数量的影响，使得评估结果偏向于原始像素。因此，AUC可能会给出不可靠的结果。相比之下，F1和IoU则表现更好，因为它们同时考虑了伪造定位结果的准确性和完整性。</p><p>​  4)设置：<br/>​  所提出的CoDE基于PyTorch实现，所有图像均调整为512×512的尺寸，以便在单个TeslaA100GPU上进行端到端训练。小批量大小设为8。我们采用了Adam优化器[48]和多参数学习方案，其中学习率从1.0×10−4开始，并在每轮<spanclass="math inline">\(i\in[1,N_{e p i}]\)</span>时乘以<spanclass="math inline">\((1-(\frac{i}{N_{e pi}})^{0.9})\)</span>。我们将最大回合数Nepi设定为50,000。折扣因子γ和伪造阈值δ分别实验性地设定为0.95和0.2。<br/>​  源代码和辅助材料可从GitHub(https://github.com/tansq/CoDE)下载。</p><h2 id="b.消融研究">B.消融研究</h2><p>​  在本小节中，我们分别研究了不同奖励函数、状态处理模式、动作空间以及单个回合中的最大步数对结果的影响。所有实验均在基准训练模式下进行。<br/>​  1）奖励函数的影响<br/>​  2）状态处理模式的影响<br/>​  3）µ和σ²缩放的影响<br/>​  4）离散动作空间与连续动作空间的影响<br/>​  5）最大步长Lepi的影响</p><h2 id="c.定量比较">C.定量比较</h2><p>​  在本小节中，我们比较了基准训练和微调两种训练模式下不同方法的性能。<br/>​  如表V所示，在基准训练模式下，我们的CoDE在F1和IoU指标上分别比IF-OSN提升了12.5%和16.3%，表现更优。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701144344273.png"alt="image-20250701144344273" /><figcaption aria-hidden="true">image-20250701144344273</figcaption></figure><p>​  在微调模式下，CoDE的F1和IoU平均分别提高了9.1%和9.9%，优于IF-OSN。此外，即使没有外部数据集进行预训练，我们的基准训练CoDE仍能显著超越其他微调模型，例如，在F1和IoU上分别平均提高了4.6%和6.2%。上述两种训练模式的结果表明，我们的方法CoDE在图像伪造定位方面具有更强的潜力。通过对比CoDE在基准训练和微调模式下的表现，后者在性能上有了全面的提升，特别是在COVERAGE和CASIAv1数据集上，F1分别提高了14.3%和8.2%。加权平均F1和IoU分别提高了4.3%和3.4%。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701144521425.png"alt="image-20250701144521425" /><figcaption aria-hidden="true">image-20250701144521425</figcaption></figure><p>​  图9展示了使用我们提出的CoDE方法进行图像伪造定位的视觉结果。伪造概率图初始设为0（即Pf(0) = 0)。由于策略在训练过程中已完全优化，代理可以采取良好的行动a(0)，使Pf(1)接近Mgt。但这并不意味着任务已经结束，因为接下来的两个步骤将更加专注于精细的定位。</p><h2 id="d.泛化性能">D.泛化性能</h2><p>​  为了评估泛化性能，我们直接使用其他方法提供的预训练模型，在多个测试数据集上进行性能评估。需要注意的是，所有方法使用的训练集和测试集互不重叠。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701144616961.png"alt="image-20250701144616961" /><figcaption aria-hidden="true">image-20250701144616961</figcaption></figure><p>​  如表VI所示，与五种方法相比，我们的CoDE在平均F1值上分别提高了0.225（76.8%）、0.151（41.1%）、0.119（29.8%）、0.194（59.9%）和0.019（3.8%）。为了进一步揭示基于深度学习的数据对方法泛化性能的影响，我们还引入了基于生成对抗网络（GAN）的数据集（FF++）和基于扩散模型的数据集（CocoGlide）进行评估。即使与TruFor相比，我们提出的CoDE也表现出相当甚至更优的性能。值得注意的是，我们的训练集不包含任何来自GAN或扩散模型的数据。</p><h2 id="e.后处理攻击的鲁棒性分析">E.后处理攻击的鲁棒性分析</h2><p>​  在实际应用中，评估模型对不同水平的图像退化的鲁棒性是至关重要的。为此，我们对测试数据集进行了多种后处理攻击，包括JPEG压缩、高斯噪声、裁剪、高斯模糊和调整大小。在裁剪过程中，我们采用了中心裁剪，并引入了一个名为“裁剪率”（CR）的参数来控制裁剪区域的高度Hcr和宽度Wcr，具体公式见等式（25）。 <spanclass="math display">\[H_{c r}=C R*H_{o r i.}\ \ \ \ W_{c r}=C R*W_{o ri}\]</span>​  其中Hori和Wori分别代表原始图像的高度和宽度。我们使用预训练模型对这些数据集进行评估，并计算加权平均F1值。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701144815839.png"alt="image-20250701144815839" /><figcaption aria-hidden="true">image-20250701144815839</figcaption></figure><p>​  如图10所示，随着各种后处理攻击强度的增加，我们的CoDE相比其他方法表现出显著更低的性能下降。我们量化了图像退化前后性能的最大差距：TurFor在JPEG压缩、高斯噪声、裁剪、高斯模糊和调整大小下的性能分别下降了17.8%、46.1%、13.6%、43.1%和32.3%。相比之下，我们的CoDE在这些测试中的性能分别下降了19.3%、14.1%、1.0%、18.1%和8.7%。实验结果清楚地表明，我们的CoDE表现出显著较低的性能下降率，突显了其对不同强度的后处理攻击具有更好的稳定性。此外，与最先进的TruFor相比，我们的CoDE在JPEG压缩、高斯噪声、裁剪、高斯模糊和调整大小下的F1值平均提高了0.018（4.1%）、0.122（34.3%）、0.060（13.0%）、0.090（23.6%）和0.078（18.4%）。</p><h2 id="f.在线社交网络的鲁棒性分析">F.在线社交网络的鲁棒性分析</h2><p>​  在线社交网络（OSNs）带来了新的挑战性场景，其中图像经常同时被压缩和调整大小，并可能包含未知噪声。这一过程导致伪造图像中的篡改痕迹被稀释。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701145558695.png"alt="image-20250701145558695" /><figcaption aria-hidden="true">image-20250701145558695</figcaption></figure><p>​  根据表VII的数据，我们的CoDE在所有社交网络（OSNs）的平均F1值和IoU指标上表现最佳。具体而言，与MVSS-Net、PSCC-Net、CAT-Netv2、IF-OSN及TruFor相比，CoDE在所有社交网络上的平均F1值分别提升了0.311（91.6%）、0.154（34.8%）、0.238（66.5%）、0.170（39.9%）和0.050（9.2%）。</p><p>​  通过全面对比，我们发现CoDE在Facebook、微博和WhatsApp上的平均F1值几乎相同。微信对所有方法的性能影响最大，因为它可能同时进行高强度的JPEG压缩和图像调整。在微信平台上，与其它方法相比，CoDE分别提高了0.308（118.9%）、0.134（30.9%）、0.353（165.0%）、0.174（44.3%）和0.059（11.6%）。这些实验结果表明，CoDE能够有效抵御来自社交网络平台的混合后处理攻击，即使在篡改痕迹被显著削弱的情况下，也能保持较高的稳健性。</p><h2 id="g.计算复杂性">G.计算复杂性</h2><p>​  为了评估我们的CoDE算法的计算复杂度，我们在CASIAv1数据集上进行了实验，所有图像都被统一调整为512×512的尺寸，并作为输入在单个TeslaA100 GPU上进行处理。我们对比了CoDE与MVSS-Net、PSCC-Net、CAT-Netv2和IF-OSN的计算复杂度。此外，我们还使用单个TeslaA100重新计算了这些方法的计算复杂度。</p><figure><imgsrc="../postimages/Employing-Reinforcement-Learning-to-Construct-a-Decision-Making-Environment-for-Image-Forgery-Localization/image-20250701145717011.png"alt="image-20250701145717011" /><figcaption aria-hidden="true">image-20250701145717011</figcaption></figure><p>​  如表VIII所示，我们的CoDE模型参数量较低，仅为2.11M。这是因为我们设计了轻量级的编码器和解码器模块。由于CoDE在每个训练周期中需要多次更新，因此其FLOPs相对较高。然而，得益于我们设计的双流状态编码器、伪造编码器以及八个基本解码器模块，这些模块在每个训练周期只需运行一次，而FPM-E和均值、方差、值模块则运行Lepi= 3次。在CASIAv1数据集上，我们的CoDE模型平均每张图像的推理时间为65毫秒，这一速度超过了大多数模型。</p><h1 id="iv.结论">IV.结论</h1><p>​  本文提出了一种基于深度强化学习的新型端到端框架CoDE，用于图像伪造定位任务。CoDE的目标是部署一个智能体，通过基于像素级高斯分布的连续动作，在多个步骤中迭代更新伪造概率图。设计的基于BCE的奖励函数专门针对稀疏分布的篡改区域，能够更高效地引导代理学习，从而提升收敛性能。此外，我们设计的双流状态编码器和轻量级解码器不仅提升了性能，还显著缩短了推理时间。广泛的实验结果表明，CoDE在各种数据集上的泛化能力和对各种后处理攻击的鲁棒性方面，均优于现有的深度学习方法。此外，CoDE即使在面对高度压缩的图像和通过在线社交网络传输引入的未知噪声时，仍能提供可靠的伪造定位结果。<br/>​  在我们未来的工作中，我们将重点关注以下几个方面：定义更高效的图像伪造定位任务的状态表示、动作空间和奖励函数，以提高在半监督学习环境下的准确性和鲁棒性。此外，我们还计划将基于强化学习的框架扩展到更广泛的伪造定位场景，例如在视频中识别被剪辑或移除操作伪造的对象。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforced Multi-teacher Knowledge Distillation for Efficient General Image Forgery Detection and Localization</title>
      <link href="/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/"/>
      <url>/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<p>Reinforced Multi-teacher Knowledge Distillation for Efficient GeneralImage Forgery Detection and Localization</p><p>Zeqin Yu1 , Jiangqun Ni2,3*, Jian Zhang1 , Haoyi Deng4 , YuzhenLin4<br/>1中山大学计算机科学与工程学院<br/>2中山大学网络科学与技术学院<br/>3鹏程实验室新网络部<br/>4深圳大学广东智能信息处理重点实验室</p><h1 id="摘要">摘要</h1><p>​  图像伪造检测与定位（IFDL）至关重要，因为伪造的图像可能会传播虚假信息，对我们的日常生活构成潜在威胁。然而，以往的方法在处理现实场景中经过多种伪造操作的伪造图像时仍难以有效应对。在本文中，我们提出了一种针对IFDL任务的新型强化多教师知识蒸馏（Re-MTKD）框架，该框架围绕一个编码器-解码器ConvNeXt-UperNet以及边缘感知模块构建，该模块被命名为Cue-Net。首先，分别对三种主要类型的图像伪造进行Cue-Net模型的训练，即复制移动、拼接和修复，然后作为多教师模型，通过自我知识蒸馏，用Cue-Net训练目标学生模型。开发了一种强化动态教师选择（Re-DTS）策略，对涉及的教师模型进行动态分配权重，这有助于特定知识的转移，并使学生模型能够有效地学习不同篡改痕迹的共同和特殊性质。大量实验表明，与其它最先进方法相比，所提出的算法在最近出现的多种图像伪造数据集上具有优越的性能。</p><h1 id="引言">1引言</h1><p>​  近期在图像编辑和生成模型方面的进展（Karras、Laine和Aila2019；Rombach等2022）不仅提升了图像处理和合成的质量，还简化了其过程。然而，通过这些技术生成的篡改图像可能被滥用以传播虚假信息，对社会安全构成重大威胁。因此，迫切需要探索有效的图像取证方法，防止篡改图像的滥用。<br/>​  针对上述问题，图像伪造检测与定位（IFDL）任务旨在识别伪造图像并定位其被篡改区域。通常，图像伪造操作主要包括复制移动、拼接和修复。通常，开发特定的方法用于检测和定位特定的伪造图像，该方法利用了特定类型中图像篡改的伪造痕迹的特性，例如，（Wu，Abd-Almageed，andNatarajan 2018；Chen et al. 2020）用于复制移动，（Bi et al. 2019；Kwonet al. 2022）用于拼接和（Li and Huang 2019；Wu and Zhou2021）用于修复。由于训练数据仅限于特定的伪造行为，这些特定的伪造检测方法可能会过度拟合，导致泛化能力有限。这种局限性通常会导致性能不佳，尤其是在检测跨源数据时，如<del>图4</del><spanstyle="color:red">图1</span>第一部分所示，当面对复制移动或拼接伪造时，修复检测器的性能显著下降。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512225837843.png"alt="image-20250512225837843" /><figcaption aria-hidden="true">image-20250512225837843</figcaption></figure><p>​  为了解决各种篡改操作，通用的伪造检测方法包括混合数据，其中包括多种类型的篡改。该策略旨在捕获各种伪造中的通用篡改特征，例如，噪声分析（Bappy等人，2019；Zhuo等人，2022；Guillaro等人，2023）、表示学习（Hu等人，2020；Yu等人，2024）、多尺度监督（Dong等人，2022；Ma等人，2023）和多视图特征融合（Liu等人，2022；Guo等人，2023）。然而，同时学习多个任务本身也带来了挑战，如<del>图4</del><spanstyle="color:red">图1</span>第二部分所示。仅通过简单的联合训练来学习各种篡改痕迹的共同和特定性质是具有挑战性的，这通常会导致性能下降。此外，这些方法通常会增加复杂性、计算成本和额外参数，使得满足实际应用场景变得困难。因此，促进模型识别不同篡改痕迹的共同特征和具体差异的能力，提高其泛化性能，对于提升图像伪造检测和定位能力至关重要。<br/>​  在本文中，我们提出了一种新颖的强化多教师知识蒸馏（Re-MTKD）框架，用于IFDL任务。首先，我们提出了Cue-Net模型，该模型基于ConvNeXt-UperNet结构，并配备了新颖的边缘感知模块（EAM）。EAM的集成通过融合低级和高级特征，促进了伪造伪影检测。为了有效地训练Cue-Net，我们引入了一种新颖的知识蒸馏策略，该策略利用多个教师模型，并通过一种名为强化动态教师选择（Re-DTS）的强化学习策略在训练过程中激活它们。具体来说，我们首先在不同的数据集上训练多个教师模型，每个模型专注于一种特定类型的伪造。通过引入Re-DTS策略，这些训练良好的教师模型会根据篡改数据的类型动态选择，以进行知识蒸馏。这种方法允许将不同伪造类型预训练的教师模型的知识高效地转移到学生模型中，从而降低对任何特定类型伪造痕迹的过拟合风险。我们对此工作的贡献如下：</p><ul><li>我们提出了一种强化多教师知识蒸馏（Re-MTKD）框架，该框架由ConvNeXt-UPerNet结构和一种新颖的边缘感知模块（EAM）组成，该模块融合了低级和高级特征，显著增强了检测篡改痕迹的能力。</li><li>我们提出了一种强化动态教师选择（Re-DTS）策略，通过动态选择在特定篡改取证任务中表现出色的教师模型进行专门的知识转移，使学生模型能够学习多个篡改痕迹的共性和细节，从而最大化学生模型的伪造检测和定位能力。</li><li>综合实验表明，与其它最先进方法相比，该方法在多个近期的多类型篡改数据集上取得了优异的性能。</li></ul><h1 id="相关工作">2相关工作</h1><h2 id="图像伪造检测与定位">2.1图像伪造检测与定位</h2><p>​  IFDL任务有几种算法，包括针对复制移动、拼接、修复和通用伪造检测策略的特定伪造检测。</p><h3 id="特定伪造检测">2.1.1特定伪造检测</h3><p>​  特定的IFDL方法专注于识别特定类型的数据被篡改的具体痕迹。对于复制-移动检测，BusterNet（Wu，Abd-Almageed，和Natarajan2018）和CMSDSTRD（Chen等2020）分别设计用于并行和串行地定位复制-移动篡改图像中的源/目标区域。对于拼接检测，MFCN（Salloum、Ren和Kuo2018）侧重于突出篡改的边缘，RRU-Net（Bi等人2019）强调残余伪影，CAT-Net（Kwon等人2022）分析RGB和DCT域中的压缩伪影。对于修复检测，HP-FCN（Li和Huang2019）使用高通滤波器增强修复痕迹，IID-Net（Wu和Zhou2021）采用神经架构搜索进行自动特征提取，TLTF-LEFF（Li等2023）引入局部增强变换器架构。虽然这些方法在特定的伪造类型上取得了优异的性能，但它们经常在其他类型的篡改数据上挣扎，导致模型泛化的效率低下。</p><h3 id="通用伪造检测">2.1.2通用伪造检测</h3><p>​  随着大数据的增长，近期的通用IFDL方法旨在学习各种篡改任务的共性。H-LSTM(Bappy et al. 2019), SATL-Net (Zhuo et al. 2022), CFL-Net (Niloy,Bhaumik, and Woo 2023)以及 TruFor (Guillaro et al.2023)将空间域和噪声域整合起来，以检测多种操作。SPAN (Hu et al. 2020)和DiffForensics (Yu et al. 2024)利用自监督学习来识别细微篡改痕迹。MVSS-Net (Dong et al. 2022)和IML-Vit (Ma et al.2023)强调多尺度监督，而PSCC-Net (Liu et al. 2022) 和HiFi-Net (Guo etal.2023)则专注于多视角特征融合。尽管这些方法在各种伪造操作中取得了有希望的结果，但由于任务不兼容，它们在联合训练中经常遭受性能退化，并且通常引入额外的复杂性和计算成本。</p><h2 id="知识蒸馏">2.2知识蒸馏</h2><p>​  初始知识蒸馏（KD,knowledge distillation） (Hinton, Vinyals, andDean2015)通过教师-学生架构将知识从一个大型教师模型转移到一个较小的学生模型。这一概念被（Zhang等人，2019）扩展到自蒸馏中，其中相同的网络同时作为教师和学生，允许学生从自身学习。对于IFDL任务，（Yu等人，2023）提出了一种固定权重的多教师自蒸馏策略来处理智能手机截图图像中的五种篡改操作，使学生模型能够学习多个篡改痕迹。然而，为每个教师模型分配固定权重可能不是最佳的，因为它阻止了对不同数据批次的动态适应，这些数据批次可能包含不同的篡改类型，从而可能限制学生模型的IFDL性能。</p><h2 id="强化学习">2.3 强化学习</h2><p>​  强化学习在自动驾驶（Sinha等人，2020）、智能游戏（Silver等人，2016）和推荐系统（Feng等人，2018；Yuan等人，2021）等领域已经取得了令人鼓舞的结果。它的核心思想是通过使代理能够从与环境的交互中学习，并根据奖励信号调整其行为，从而最大化长期回报。在本文中，我们介绍了一种新颖的Re-MTKD框架，该框架融合了这些原则。该框架的核心是Re-DTS策略，动态选择最合适的教师模型，将专业知识转移到学生模型。这一策略增强了学生模型处理各种篡改痕迹的能力，并提高了IFDL性能。</p><h1 id="拟提出的re-mtkd框架">3拟提出的Re-MTKD框架</h1><h2 id="cue-net主干">3.1Cue-Net主干</h2><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512210419473.png"alt="image-20250512210419473" /><figcaption aria-hidden="true">image-20250512210419473</figcaption></figure><p>​  <strong>流程</strong><br/>​  图2(b)展示了Cue-Net的架构，这是一个编码器-解码器结构。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512212140458.png"alt="image-20250512212140458" /><figcaption aria-hidden="true">image-20250512212140458</figcaption></figure><p>​  对于编码器，假设输入图像为<spanclass="math inline">\(x\in\mathbb{R}^{h\times w\timesc}\)</span>，首先通过ConvNeXtv2（Woo等人，2023）进行特征提取，并从低到高层次获得特征图<spanclass="math inline">\(\{E_{j}\}_{i=1}^{4}\)</span>。此外，<spanclass="math inline">\(E_2\)</span>和<spanclass="math inline">\(E_4\)</span>用作本文提出的EAM的输入，将在下一小节中介绍。解码器采用UPerNet（Xiao等人，2018），包括两个组件：金字塔池化模块（PPM）（Zhao等人，2017）和特征金字塔网络（FPN）（Lin等人，2017）。FPN输出特征图<spanclass="math inline">\(\{D_{j}\}_{i=1}^{4}\)</span>，使用双线性插值将其调整为原始尺寸，然后合并以获得应用sigmoid激活函数后的定位分割结果。</p><p>​  <strong>边缘感知模块</strong><br/>​  为了进一步增强细粒度的篡改痕迹提取，我们设计了一个简单而有效的边缘感知模块（EAM），该模块能够准确地提取篡改区域的边缘信息，以便于后续的引导和融合。篡改区域的边缘通常只由几个像素组成，这使得仅使用最低分辨率的高级特征图难以实现精确的边缘监督。相反，仅使用高分辨率的低级特征图很难感知到被篡改的区域。在EAM中，我们在多级特征上执行边缘监督，这是通过在编码器中融合低级特征E2和高级特征E4得到的，如下所示，<span class="math display">\[f^{\epsilon}=\sigma(C B R(C a t(DR(E_{2}),U p(D R(E_{4})))),\]</span> ​  其中，<spanclass="math inline">\(\sigma(\cdot)\)</span>是sigmoid归一化，CBR模块（卷积+BN +ReLU)实现了低级和高级特征的融合，Cat（·）表示拼接操作，DR代表使用CBR模块进行降维，Up表示上采样。随后，获得的边缘预测图f e和边缘标签y e被用于损失迭代。</p><p>​  损失函数<br/>​  在以前的IFDL方法（Liu et al. 2022；Dong et al.2022；Guo et al.2023）的基础上，我们的方法包括三种类型的监督：定位分割（像素级）监督<spanclass="math inline">\(\mathcal{L}_{seg}\)</span>和检测分类（图像级）监督<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>作为标准，以及边缘检测监督<spanclass="math inline">\(\mathcal{L}_{edg}\)</span>作为特殊设计。<br/>​  对于本地化分割监督<spanclass="math inline">\(\mathcal{L}_{seg}\)</span>，根据（Guillaro等人，2023），我们使用加权<spanclass="math inline">\(\ell_{w b c e}\)</span>和<spanclass="math inline">\(\ell_{d i ce}\)</span>（Milletari、Navab和Ahmadi，2016)的组合。 <spanclass="math display">\[\mathcal{L}_{s e g}=\lambda_{0}^{s}\ell_{w b ce}(f^{s},y^{s})+\left(1-\lambda_{0}^{s}\right)\ell_{d i ce}(f^{s},y^{s}),\]</span> ​  其中<spanclass="math inline">\(\lambda_{0}^{s}\)</span>是分割平衡权重，<spanclass="math inline">\(f^{s}\)</span>和<spanclass="math inline">\(y^{s}\)</span>分别是定位结果和定位标签。<br/>​  在ConvNeXtv2（Woo等人，2023）之后，我们使用标准<span class="math inline">\(\ell_{bc e}\)</span>设计了E4上的检测分类损失<spanclass="math inline">\(\mathcal{L}_{c l s}\)</span>， <spanclass="math display">\[\mathcal{L}_{c l s}=\ell_{b ce}(f^{c},y^{c}),\]</span> ​  其中<spanclass="math inline">\(f^{c}\)</span>和<spanclass="math inline">\(y^c\)</span>分别是检测分类结果和分类标签。<br/>​  对于边缘监督<spanclass="math inline">\(\mathcal{L}_{e d g}\)</span>与EAM，我们使用<spanclass="math inline">\(\ell_{d i c e}\)</span>更好地关注微小的边缘区域，<span class="math display">\[\mathcal{L}_{e d g}=\ell_{d i ce}(f^{e},y^{e}).\]</span> ​  对于最终的组合损失，也就是“硬”损失<spanclass="math inline">\(\mathcal{L}_{h a rd}\)</span>，从三个角度加权，得到我们最近的监督损失函数， <spanclass="math display">\[\mathcal{L}_{\mathrm{hard}}=\alpha\cdot\mathcal{L}_{se g}+\beta\cdot(\mathcal{L}_{c l s}+\mathcal{L}_{e d g}),\]</span>​  其中α、β、∈、[0,1]。术语“硬”表示每个像素的二进制标签用于监督。</p><h2 id="强化动态教师选拔策略">3.2强化动态教师选拔策略</h2><p>​  在介绍Re-DTS策略之前，我们将首先构建单教师知识蒸馏KD。为了更好地提升学生模型的定位分割和检测分类性能，我们选择通过“软”损失<spanclass="math inline">\(\mathcal{L}_{s o ft}\)</span>进行专门知识的迁移，如下所示， <spanclass="math display">\[\mathcal{L}_{s o f t}=\mathcal{L}_{s eg}(f_{\bf{s}}^{s},f_{\bf{t}}^{s})+\mathcal{L}_{c ls}(f_{\bf{s}}^{c},f_{\bf{t}}^{c}),\]</span> ​  其中加粗<spanclass="math inline">\(\bf{t}\)</span>和<spanclass="math inline">\(\bf{s}\)</span>分别表示教师模型和学生模型。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512212239447.png"alt="image-20250512212239447" /><figcaption aria-hidden="true">image-20250512212239447</figcaption></figure><p>​  图2(a)展示了我们的Re-DTS策略概览。强化学习中的元素（即状态、动作和奖励）介绍如下。为了明确起见，由于所有教师模型共享相同的训练过程，我们仅介绍其中一个，三个教师模型分别表示为<spanclass="math inline">\(\{\Theta_{k}^{t}\}_{k=1}^{3}\)</span>。</p><p>​  <strong>状态</strong><br/>​  我们的Re-DTS策略包含一系列环境状态s1、s2，...，这些状态总结了待检测的篡改图像特征、学生模型特征和候选教师模型特征，以便做出合理的决策。为了构建状态<spanclass="math inline">\(s_i\)</span>，我们设计了一个状态特征向量<spanclass="math inline">\(\mathbf{F}\left(S_{i}\right)\)</span>，其中包含了三个特征的组合。<br/>​  第一个特征是第i个批次图像xi的向量表示<spanclass="math inline">\(\mathcal{R}(x_{i})\in\mathbb{R}^{d}\)</span>，它可以由任何IFDL表示模型<spanclass="math inline">\(\Theta^{r}\)</span>生成。在本文中，我们通过将该批图像xi输入到Cue-Net中，获得解码器D4最后一层的输出作为特征<spanclass="math inline">\(\mathcal{R}(x_{i})\)</span>，而Cue-Net尚未在任何IFDL任务数据上进行训练。<br/>​  第二个特征<spanclass="math inline">\(\mathcal{S}(x_{i})\in\mathbb{R}^{d}\)</span>的获得方式类似，只是特征向量是从学生模型<spanclass="math inline">\(\Theta^{s}\)</span>输出的，该模型在IFDL任务数据上实时训练，旨在有效地表示篡改图像的内容。<br/>​  第三个特征<spanclass="math inline">\(\mathcal{T}_{k}(x_{i})\in\mathbb{R}^{1+1+1}\)</span>，分别由教师模型<spanclass="math inline">\(\Theta_{k}^{t}\)</span>的定位分割预测结果<spanclass="math inline">\(f_{\bf{t}k}^{s}\)</span>、检测分类预测结果<spanclass="math inline">\(f_{\bf{t}k}^{c}\)</span>和篡改边缘预测结果<spanclass="math inline">\(f_{\bf{t}k}^{e}\)</span>组成，旨在指导策略网络执行更优的动作。<span class="math display">\[{\mathcal T}_{k}(x_{i})=C a t(M ax(f_{\mathrm{tk}}^{s}),f_{\mathrm{tk}}^{c},M ax(f_{\mathrm{tk}}^{e})),\]</span>​  其中Max（·）是最大池化操作。<br/>​  因此，状态特征向量<spanclass="math inline">\({\bfF}\left(s_{i}\right)\in{\bf\mathbb{R}}^{2d+3}\)</span>可以表示如下：<span class="math display">\[\mathrm{\bf{F}}(s_{i})=C at({\mathcal{R}}(x_{i}),{\mathcal{S}}(x_{i}),{\mathcalT}_{k}(x_{i})),\]</span> ​  其中，这种连接确保状态特征向量<spanclass="math inline">\(\mathrm{\bf{F}}(s_{i})\)</span>全面反映图像和模型特性，使策略网络能够做出更明智、更有效的决策。</p><p>​  <strong>动作</strong><br/>​  每个教师模型<spanclass="math inline">\(\Theta^{t}\)</span>都与一个策略网络相关联，动作<spanclass="math inline">\(a_{i}\in\{0,1\}\)</span>定义为指示是否选择策略网络将第k个教师模型<spanclass="math inline">\(\Theta_{k}^{t}\)</span>的特殊知识转移到学生模型<spanclass="math inline">\(\Theta^{s}\)</span>。我们使用策略函数<spanclass="math inline">\(\pi_{\theta}(s_{i},a_{i})\)</span>对<spanclass="math inline">\(a_i\)</span>的值进行采样，其中θ是需要学习的参数。在本文中，我们采用以下逻辑函数作为策略函数：<spanclass="math display">\[\begin{array}{l}\pi_{\theta}(s_{i},a_{i})&amp;=P_{\theta}\(a_{i}\mid s_{i})\\&amp;=a_{i}\sigma({\bf W} * {\bfF}(s_{i})+b)+(1-a_{i})(1-\sigma(\mathbf{W}*\mathbf{F}(s_{i})+b))\end{array}\]</span>​  其中<spanclass="math inline">\(\mathbf{W}\in\mathbb{R}^{2d+3}\)</span>，<spanclass="math inline">\(b\in\mathbb{R}^{1}\)</span>为可训练参数。</p><p>​  <strong>奖励</strong><br/>​  合理的奖励可以指导教师模型提炼出一个表现优异的学生模型。对于训练数据<spanclass="math inline">\(\mathcal D=\{\mathcal D_{1},\mathcalD_{2},...,\mathcalD_{M}\}\)</span>，其中M是每个周期的批次数，我们为每个教师模型<spanclass="math inline">\(\Theta_{k}^{t}\)</span>构建一个特征向量<spanclass="math inline">\(s_{i k}\)</span>，并根据策略函数<spanclass="math inline">\(\pi_{\theta}(s_{i k},a_{ik})\)</span>为第i个批次的数据<span class="math inline">\(\mathcalD_{i}\)</span>选择动作<span class="math inline">\(a_{ik}\)</span>，以确定该动作是否被选中。对于所有采样的教师模型，我们将所有教师模型的损失<spanclass="math inline">\(\mathcal{L}_{s o f t}\)</span>与损失<spanclass="math inline">\(\mathcal{L}_{h a rd}\)</span>集成，以更新学生模型参数<spanclass="math inline">\(\Theta^{s}\)</span>。<br/>​  我们提出了三种奖励计算方法。第一种是“硬”损失<spanclass="math inline">\(\mathcal{L}_{h a rd}\)</span>，包括定位分割损失<spanclass="math inline">\(\mathcal{L}_{seg}\)</span>、检测分类损失<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>和篡改边缘检测损失<spanclass="math inline">\(\mathcal{L}_{edg}\)</span>。第二种奖励函数结合了“硬”损失<spanclass="math inline">\(\mathcal{L}_{h a rd}\)</span>以及教师模型的“软”损失<spanclass="math inline">\(\mathcal{L}_{s o ft}\)</span>。最后，为了提高学生模型在IFDL任务上的性能，我们在训练过程中使用学生模型的定位分割F1分数和检测分类准确率作为第三个奖励。总之，我们有<span class="math display">\[\begin{array}{l}r e w a rd_{1}=&amp;-\mathcal{L}_\mathrm{hard}\\r e w a rd_{2}=&amp;-(\mathcal{L}_\mathrm{hard}+\mathcal{L}_\mathrm{soft})\\r e wa rd_{3}=&amp;-\gamma(\mathcal{L}_{\mathrm{hard}}+\mathcal{L}_{\mathrm{soft}})+(1-\gamma)((F1_{se g}+A c c_{c l s})o n D_{i}),\end{array}\]</span> ​  其中，<spanclass="math inline">\(\gamma\)</span>是一个超参数，用于平衡不同的奖励。我们最终选择第三个奖励作为默认选项。值得注意的是，根据强化学习（Williams1992）的方法，在每一步之后不会立即奖励更新，因此我们将更新推迟到训练了10B批次之后，其中B表示批量大小。</p><p>​  <strong>最优化</strong><br/>​  根据策略梯度定理（Sutton等人，1999）和强化算法（Williams，1992），我们计算梯度以更新当前策略如下：<spanclass="math display">\[\theta\leftarrow\theta+\xi\sum_{i}r\sum_{k}\nabla_{\theta\pi_{\theta}(s_{ik},a_{i k})},\]</span> ​  其中r是等式10中定义的奖励函数，ξ是学习率。</p><h2 id="模型的训练过程">3.3模型的训练过程</h2><p>​  算法1概述了ReMTKD框架的训练过程。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512220854959.png"alt="image-20250512220854959" /><figcaption aria-hidden="true">image-20250512220854959</figcaption></figure><p>​  在开始联合训练之前，我们对模型进行预训练。教师模型<spanclass="math inline">\(\Theta_{k}^{t}\)</span>使用针对相应类型篡改数据<spanclass="math inline">\(\mathcal D ^k\)</span>的专门知识进行预训练。然后，在KD知识蒸馏过程中，通过选择所有教师模型的反馈来初始化策略网络<spanclass="math inline">\(\theta_{k}\)</span>。<br/>​  初始化后，执行Re-DTS策略，根据<spanclass="math inline">\(\mathcal{L}\)</span>优化学生模型，如算法2所示。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512221135983.png"alt="image-20250512221135983" /><figcaption aria-hidden="true">image-20250512221135983</figcaption></figure><p>​    在这个过程中，我们首先固定策略网络<spanclass="math inline">\(\theta_{k}\)</span>来训练学生模型<spanclass="math inline">\(\Theta^{s}\)</span>。在预定的迭代次数之后，我们固定<spanclass="math inline">\(\Theta^{s}\)</span>，计算返回内容，并优化教师选择策略网络<spanclass="math inline">\(\theta_{k}\)</span>。重复此过程，直到完成所有训练迭代。</p><h1 id="实验">4实验</h1><h2 id="实验设置">4.1实验设置</h2><p>​  <strong>数据集</strong><br/>​  考虑到可用性和通用性，我们选择了十个具有挑战性的基准数据集来评估我们的方法，涵盖篡改类型：复制移动（Com）、拼接（Spl）、修复（Inp）和多篡改（Multi）。这些数据集的详细信息见附录。<br/>​  1）对于复制移动，我们使用CASIAv2（Dong、Wang和Tan 2013）和TamperedCoco（Liu等2022）进行训练，并使用CASIA v1+（Dong、Wang和Tan2013）、NIST16（Nimble2016）和Coverage（Wen等2016）进行测试。<br/>​  2）对于拼接，我们使用CASIAv2（Dong，Wang，and Tan 2013）和Fantastic-Reality（Kniaz，Knyaz，andRemondino 2019）进行训练，并使用CASIA v1+（Dong，Wang，and Tan2013）、NIST16（Nimble 2016）、Columbia（Hsu and Chang 2006）和DSO-1（DeCarvalho et al. 2013）进行测试。<br/>​  3）对于修复，我们使用GCDresden&amp;Places（Wu和Zhou 2021）进行训练，并使用NIST16（Nimble2016）、AutoSplicing（Jia等2023）和DiverseInp（Wu和Zhou2021）进行测试。<br/>​  4）对于多篡改，我们使用IFC（Challenge2013)、Korus（DeCarvalho等人，2013）和IMD2020（Novozamsky、Mahdian和Saic，2020）进行测试，因为这些数据涉及上述单一篡改类型的复合操作。</p><table><colgroup><col style="width: 6%" /><col style="width: 46%" /><col style="width: 46%" /></colgroup><thead><tr class="header"><th>篡改类型</th><th>训练方法及来源</th><th>测试方法及来源</th></tr></thead><tbody><tr class="odd"><td>复制移动</td><td>CASIA v2 (Dong, Wang, and Tan 2013), <br />Tampered Coco (Liu et al.2022)</td><td>CASIA v1+ (Dong, Wang, and Tan 2013), <br />NIST16 (Nimble 2016),<br />Coverage (Wen et al. 2016)</td></tr><tr class="even"><td>拼接</td><td>CASIA v2 (Dong, Wang, and Tan 2013), <br />Fantastic-Reality (Kniaz,Knyaz, and Remondino 2019)</td><td>CASIA v1+ (Dong, Wang, and Tan 2013), <br />NIST16 (Nimble 2016),<br />Columbia (Hsu and Chang 2006), <br />DSO-1 (De Carvalho et al.2013)</td></tr><tr class="odd"><td>修复</td><td>GC Dresden &amp; Places (Wu and Zhou 2021)</td><td>NIST16 (Nimble 2016), <br />AutoSplicing (Jia et al.2023),<br />DiverseInp (Wu and Zhou 2021)</td></tr><tr class="even"><td>多篡改</td><td>——</td><td>IFC (Challenge 2013), <br />Korus (De Carvalho et al. 2013),<br />IMD2020 (Novozamsky, Mahdian, and Saic 2020)</td></tr></tbody></table><p>​  对于Cue-Net学生模型<spanclass="math inline">\(\Theta^{s}\)</span>和其他SOTA方法，训练是在所有混合的单次篡改数据上进行的。相比之下，在强化多教师知识蒸馏过程中，每个教师模型<spanclass="math inline">\(\Theta^{t}\)</span>是在相应类型的篡改数据上进行训练的。</p><p>​  <strong>实施细节</strong><br/>​  我们使用单个Nvidia Tesla A100 GPU(80GB内存）在PyTorch深度学习框架上进行实验，强化多教师知识蒸馏KD的参数配置如下：<br/>​  1）对于单教师模型预训练，我们将输入图像调整为512×512，并应用AdamW优化器。我们通过学习率设为1×10−4，批量大小设为24，训练轮数设为50来设置训练超参数。为了平衡伪造检测和定位的性能，我们将伪造定位Lseg的权重设为α= 1，λs0设为0.1。检测分类监督Lcls和边缘监督Ledg的权重β设为0.2。<br/>​  2)对于强化多教师知识蒸馏KD，我们使用与单教师模型预训练相同的参数设置，包括输入图像大小、优化器、学习率和批量大小，但在此训练阶段使用更多的训练数据和仅25个epoch。对于奖励3中的γ，我们将其设置为0.2。对于L，我们为整体教师损失Lsoft设置约束因子ω=0.05，该约束因子乘以选定教师的数量，以平衡教师模型知识迁移和学生模型自主学习的过程。<br/>​  3）对于策略网络，学习率设置为3×10−4，并使用Adam优化器通过CosineAnnealingLR进行调整。</p><h2 id="与最新方法的比较">4.2与最新方法的比较</h2><p>​  为了进行公平的比较，我们关注的是在与测试数据集不同的数据集上训练的有可用代码或预训练模型的方法。我们比较针对特定伪造类型和通用伪造检测的方法如下：<br/>​  DoaGan(Islam et al. 2020)、<br/>​  BusterNet (Wu, Abd-Almageed, and Natarajan2018)<br/>​  CMSDSTRD (Chen et al.2020)是为复制移动检测而设计的。<br/>​  MFCN (Salloum, Ren, and Kuo2018)、<br/>​  RRU-Net (Biet al. 2019)<br/>​  CAT-Net (Kwon et al.2022)是为剪接检测而设计的。<br/>​  HP-FCN (Li and Huang 2019),<br/>​  IIDNet (Wu and Zhou 2021) <br/>​  TLTF-LEFF (Li et al.2023)是为修复检测而设计的。<br/>​  H-LSTM (Bappy et al.2019), <br/>​  SPAN(Hu et al. 2020), <br/>​  MVSS-Net (Dong et al.2022), <br/>​  SATL-Net(Zhuo et al. 2022), <br/>​  PSCC-Net (Liu et al.2022), <br/>​  HiFi-Net(Guo et al.2023)<br/>​  IML-Vit (Ma etal.2023)是为通用伪造检测而设计的。</p><h3 id="检测评估">4.2.1检测评估</h3><p>​  表1展示了伪造检测性能。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512222216526.png"alt="image-20250512222216526" /><figcaption aria-hidden="true">image-20250512222216526</figcaption></figure><p>​  我们观察到许多方法表现非常差，其AUC接近0.5，即接近随机猜测，例如Buster-Net、H-LSTM、SPAN和HiFi-Net。得益于提出的Re-MTKD框架，该框架包括Cue-Net和Re-DTS策略，我们的方法在所有篡改类型的数据集上均达到了SOTA性能。特别是在多篡改数据集上，该数据集包含更复杂的篡改类型，所有比较方法的表现都很差，但我们的方法在AUC得分上比第二名高出8.7%。</p><h3 id="定位评估">4.2.2定位评估</h3><p>​  表2展示了伪造定位性能。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512222410647.png"alt="image-20250512222410647" /><figcaption aria-hidden="true">image-20250512222410647</figcaption></figure><p>​  值得注意的是，一些专门针对伪造的检测方法在相应数据上表现出竞争力，例如，Buster-Net在复制移动数据上达到了最佳的AUC分数，CAT-Net在拼接数据上取得了第二好的表现，而IID-Net在修复数据上也获得了具有竞争力的AUC分数。然而，这些方法在应用于其他类型的伪造或多篡改数据时常常表现不佳，导致性能显著下降。有趣的是，很少有通用的伪造检测方法能在特定和多篡改类型中始终表现出强大的效果，这突显了同时学习多个篡改痕迹的挑战。相比之下，我们的方法在所有数据集上都取得了优异的结果，并且平均而言明显优于其他方法，突显了其有效捕捉多种篡改竞赛的共性和特性的能力。</p><h3 id="更多的评估补充资料">4.2.3更多的评估（补充资料）</h3><p>​  <strong>定量结果</strong><br/>​  我们提供了主文中所使用SOTA方法性能的更全面概述。表5展示了使用这些SOTA方法的官方预训练模型获得的IFDL结果。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512230325004.png"alt="image-20250512230325004" /><figcaption aria-hidden="true">image-20250512230325004</figcaption></figure><p>​  可以观察到，特定的伪造检测方法在相应的篡改类型数据上表现出更好的性能。例如，DoaGan(Islam等人，2020年）和Buster-Net(Wu、Abd-Almageed和Natarajan，2018年）在复制移动数据上的定位表现尤为出色，而RRU-Net(Bi等人，2019年）和CAT-Net v1(Kwon等人，2021年）在拼接数据上的定位能力更胜一筹。此外，HP-FCN(Li和Huang，2019年）和TLTF-LEFF(Li等人，2023年）在修复数据上的检测和定位性能分别非常显著。然而，许多通用的伪造检测方法，如SPAN(Hu等人，2020年）、SATL-Net (Zhuo等人，2022年）、PSCC-Net(Liu等人，2022年）和Hi-Fi-Net(Guo等人，2023年），尽管是在大型数据集上训练的，但在多种篡改类型上的表现却较差。<br/>​  此外，我们还纳入了竞争比较方法，如ManTra-Net（Wu、AbdAlmageed和Natarajan2019）和TruFor（Guillaro等人2023），但这些方法未在主文中提及。这是因为这两种方法均未公开发布训练代码，仅提供测试代码。值得注意的是，TruFor是在更大规模的数据集上进行训练的(TruFor使用867k的数据集，而我们的方法使用60k的数据集），并且受到了数据泄露的影响(TruFor使用IMD2020（Novozamsky、Mahdian和Saic2020）进行训练，本文作为多篡改类型测试数据的一部分）。<br/>​  特别是在复制-移动和拼接数据上，TruFor展现了卓越的伪造定位性能。然而，其伪造检测性能欠佳，在修复数据上的表现也显著下降，突显了同时处理多种篡改数据类型的挑战。对于多次篡改的数据，我们的方法仍能与TruFor达到相当的性能，尽管TruFor的测试模型存在上述数据泄露问题。当排除IMD2020数据集时，该数据集存在数据泄露问题，我们的方法在平均AUC分数上比TruFor高出10.2%。总体而言，本文提出的Re-MTKD框架有效提升了模型在各种篡改数据类型上的性能，并实现了最优的平均AUC性能。</p><p>​  <strong>定性结果</strong><br/>​  我们展示了特定和通用伪造检测方法在各种类型篡改数据上的定性评估，包括复制移动、拼接、修复和多次篡改数据，以及真实数据，如图8所示。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512230748573.png"alt="image-20250512230748573" /><figcaption aria-hidden="true">image-20250512230748573</figcaption></figure><p>​  可以看出，特定的伪造检测方法在相应数据上表现更佳，例如Buster-Net（吴、阿卜杜勒-马吉德和纳塔拉詹2018)在复制移动数据上，RRU-Net（毕等人2019)和CAT-Net（权等人2022)在拼接数据上，以及IID-Net（吴和周2021)在修复数据上。与通用的伪造检测方法相比，我们的方法最终在各种类型的篡改数据中实现了更准确的伪造定位结果。此外，我们提出的方法在应用于真实图像时表现出更低的误报率。</p><p>​  <strong>鲁棒性</strong><br/>​  我们进一步评估了在社交媒体洗牌中面对常见图像扰动时定位结果的鲁棒性，即JPEG压缩、高斯模糊、高斯噪声和中值滤波。如图5所示，一些通用的IFDL方法，例如针对JPEG压缩的PSCCNet（刘等，2022)和针对高斯模糊的SATLNet（卓等，2022)，性能显著下降。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512230941967.png"alt="image-20250512230941967" /><figcaption aria-hidden="true">image-20250512230941967</figcaption></figure><p>​  相比之下，我们的方法在整个后处理攻击范围内展示了出色的鲁棒性。</p><p>​  <strong>推理效率</strong><br/>​  如表6所示，我们选择了具有竞争力的SOTA方法在复制移动数据集上进行测试，其中我们的方法在五次运行中平均实现了最快的推理速度和具有竞争力的内存使用。我们的方法展示了卓越的推理效率，推理速度达到38.2毫秒/图像，显著优于其他方法，如MVSS-Net(129.37毫秒/图像）和TruFor (64.87毫秒/图像）。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512231028476.png"alt="image-20250512231028476" /><figcaption aria-hidden="true">image-20250512231028476</figcaption></figure><p>​  最终，它达到了最高的AUC值0.903，证实了我们的方法在推理效率和性能之间达到了最佳平衡，在检测和定位任务中均表现出色。</p><h2 id="消融研究">4.3消融研究</h2><p>​  本小节主要分析Re-MTKD框架关键组件的有效性。表3展示了Re-DTS策略的消融结果，以及分配教师权重的其他策略。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512222551477.png"alt="image-20250512222551477" /><figcaption aria-hidden="true">image-20250512222551477</figcaption></figure><p>​  关于消融实验的更多细节，请参见附录。</p><h3 id="知识蒸馏中教师模型的有效性">4.3.1知识蒸馏中教师模型的有效性</h3><p>​  比较设置#0至设置#3，可以观察到简单的Cue-Net学生模型难以应对广泛的篡改攻击。相比之下，专门针对伪造检测的教师模型能够提高学生模型在相应数据上的性能，例如，在修复数据上，Singe-Inp在伪造检测和伪造定位F1性能方面分别比设置#0提高了14%和19%。<br/>​  比较设置#0和设置#4，U-Ensemble在IFDL任务中提供了一些性能提升，但在某些操作类型如复制移动和多点篡改时表现下降。这表明，跨多个教师平均权重未能充分捕捉不同篡改操作的独特共性，可能不是最有效的策略。</p><h3 id="re-dts策略的有效性">4.3.2Re-DTS策略的有效性</h3><p>​  如最后五个设置所示，我们比较了本文提出的ReDTS中不同奖励和各种教师知识迁移策略Lsoft的效果。<br/>​  （i）奖励：通过比较设置#5、设置#6和设置#9，可以发现增加奖励作为从教师模型到学生模型的知识迁移监督，能够更好地提升学生模型的性能。对于设置#9，本文采用的最终奖励（奖励3），即教师模型Lsoft的“软”损失和“硬”损失Lhard的总奖励，以及学生模型在伪造定位F1和伪造检测Acc上的性能指标，都有效提升了学生的整体表现。<br/>​  （ii）Lsoft：通过比较设置#7、设置#8和设置#9可以看出，教师模型转移的专业知识在伪造检测和定位方面的结合，更高效地提高了学生模型的IFDL性能。我们的方法（设置#9)不仅在多个特定的伪造数据上取得了优异的结果，在更具挑战性的多篡改数据上也达到了同等高效的性能。<br/>​  此外，我们还展示了学习特征的嵌入空间以及不同KD策略的t-SNE（Vander Maaten和Hinton 2008）可视化图，如图3所示。</p><figure><imgsrc="../postimages/Reinforced-Multi-teacher-Knowledge-Distillation-for-Efficient-General-Image-Forgery-Detection-and-Localization/image-20250512222846588.png"alt="image-20250512222846588" /><figcaption aria-hidden="true">image-20250512222846588</figcaption></figure><p>​  我们观察到，在我们提出的Re-MTKD框架中，Cue-Net在使用Re-DTS策略训练时，能够有效区分真实样本和篡改样本的特征分布，优于其他KD策略。这表明模型能够学习不同篡改类型之间的共同特征，从而能够准确地将所有篡改样本分类为篡改样本，无论具体的篡改技术如何。此外，模型还捕捉到了每种篡改类型的特定特征，使得每个类别的篡改样本分布更加集中且明显。</p><h1 id="结论">5结论</h1><p>​  在本文中，我们提出了一种新颖的强化多教师知识蒸馏（Re-MTKD）框架，专为图像伪造检测与定位（IFDL）设计。具体而言，我们开发了一种名为Cue-Net的新网络，该网络采用ConvNeXt-UPerNet结构，并配备了一个边缘感知模块，作为IFDL任务的有效骨干。我们进一步引入了一种强化动态教师选择（Re-DTS）策略，该策略根据不同类型的篡改数据动态选择专门的教师模型，引导学生模型有效学习各种篡改痕迹的共性和特异性。广泛的实验结果表明，与现有的最先进方法相比，我们提出的方法在多个IFDL任务中表现出色。</p><h1 id="评价非作者主观评价">6评价（非作者、主观评价）</h1><p>​  1.总体评价：这篇文章发表于AAAI2025，首先将强化学习的方案应用于图像篡改检测任务确实是一个创新的举动，但是其还有些许不足。<br/>​  2.数据集测试的改动：其没有按照主流方法，以数据集为界，进行测试，而是以篡改方法为界，数据集的篡改有简易有困难，类似于Splicing篡改，Columbia数据集属于比较容易的数据集，其分数一定程度上会拉升在此篡改任务上的分数，在不分享代码的情况下，使用新颖的数据测试方法，无法让其他研究者与其进行比较。</p><p>​  3.格式性错误：第一段应该是图1，写成了图4</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SAFIRE</title>
      <link href="/SAFIRE/"/>
      <url>/SAFIRE/</url>
      
        <content type="html"><![CDATA[<h1 id="safire-segment-any-forged-image-region">SAFIRE: Segment AnyForged Image Region</h1><p><strong>Myung-Joon Kwon</strong>1* <strong>, Wonjun Lee</strong>1*<strong>, Seung-Hun Nam</strong>2 <strong>, Minji Son</strong>1<strong>, Changick Kim</strong>1</p><p>1韩国科学技术院（KAIST）电气工程学院<br/>2NAVER WEBTOONAI，韩国Seongnam</p><h1 id="摘要">摘要</h1><p>​  大多数技术将图像伪造定位问题视为二值分割任务，训练神经网络将原始区域标记为0，伪造区域标记为1。相比之下，我们从更基础的角度出发，根据图像的来源对其进行划分。为此，我们提出了“任意伪造图像区域分割”（SAFIRE），通过点提示解决伪造定位问题。图像上的每个点用于分割包含自身的源区域。这使得我们可以将图像划分为多个源区域，这是首次实现的功能。此外，SAFIRE不是记忆特定的伪造痕迹，而是自然地关注每个源区域内的一致特征。这种方法导致了更稳定和有效的学习，在新任务和传统的二值伪造定位中均表现出色。</p><p>Code: https://github.com/mjkwon2021/SAFIRE</p><h1 id="引言">1. 引言</h1><p>​  在人工智能（AI）时代，图像编辑软件（Fu等人，2023；Yu等人，2023）和复杂的生成模型（Rombach等人，2022；Ho、Jain和Abbeel，2020）的普及使得图像伪造比以往任何时候都更容易被发现，同时也更加难以检测（Lin等人，2024）。图像操作的便捷性对视觉信息完整性至关重要的领域产生了重大影响，包括新闻业中假新闻的传播、执法部门使用伪造证据以及生物医学研究中虚假显微图像的存在（Verdoliva，2020；Sabir等人，2021）。因此，在图像中检测并精确定位伪造内容对于维护数字媒体的信任至关重要。<br/>​  目前，大多数图像取证方法通过二值分割来解决图像伪造定位（IFL）问题（Guillaro等，2023；Kwon等，2022；Liu等，2022；Dong等，2022；Hu等，2020；Wu等，2022；Zhou等，2023a；Ji等，2023a；Sun等，2023）。也就是说，在图像中，未被相机捕捉到的区域标记为0，而被篡改过的区域标记为1，以训练深度神经网络。<br/>​  相反，我们从一个更基本的角度来看待IFL，即根据图像的起源将其划分为不同的区域。在此背景下，我们将这些不同的区域定义为源区域，即独立捕获、AI生成或处理的图像的不同部分（图1）。</p><figure><img src="../postimages/SAFIRE/image-20250506113014879.png"alt="image-20250506113014879" /><figcaption aria-hidden="true">image-20250506113014879</figcaption></figure><p>图1：伪造图像由三个源区域组成。以往的方法仅限于二值预测——分割伪造区域。相比之下，我们的SAFIRE还能够进行多源预测——区分来自同一源图像的区域。</p><p>​  从这个角度来看，我们提出了“任意伪造图像区域分割”（SAFIRE），一种基于点提示的新颖IFL方法，旨在根据图像的原始来源精确地将其分割成区域。SAFIRE采用点提示技术，其中图像上的每个点都会分割出与之共享同一来源的区域（图2）。</p><figure><img src="../postimages/SAFIRE/image-20250506113107272.png"alt="image-20250506113107272" /><figcaption aria-hidden="true">image-20250506113107272</figcaption></figure><p>​  为了实现这一目标，我们利用了“Segment AnythingModel”（SAM）（Kirillov等人，2023年）的点提示功能，并对其进行了几处改进。首先，SAFIRE会分割包含给定点的源区域，而SAM则会分割该点周围的所有有意义的部分。其次，虽然SAM处理的是模糊的真实情况，但SAFIRE有明确的真实情况，即一个源区域内所有点的答案都相同。第三，SAFIRE内部生成并使用点提示，因此无需手动输入来指定点。<br/>​  SAFIRE框架由预训练、训练和推理三个阶段组成。在预训练阶段，基于源区域的对比学习被应用于增强图像编码器的特征提取能力。在训练阶段，模型被训练以分割与给定点提示相对应的源区域。尽管该模型可以使用仅包含二元标签的伪造数据集进行训练，但它能够进行多源预测。在推理阶段，点网格生成多个掩码，然后将这些掩码组合起来，产生最终的源分区结果。<br/>​  SAFIRE是首个能够区分图像被篡改两次或更多次时的每个来源的方法，从而产生三个或更多的来源。区分每个来源比仅仅定位篡改像素能更好地解释被操纵的图像。此外，它还促进了后续分析，例如来源过滤，这涉及从一组候选图像中检索每个源区域的原始图像（Pinto等，2017；Moreira等，2018；Verdoliva，2020）。因此，在现实场景中，当多次操作很常见时，多源分区对图像取证特别有益。<br/>​  此外，新颖的提示机制使SAFIRE能够通过考虑真实区域和篡改区域的可互换性来有效学习，这一特性在IFL中我们称之为标签不可知性。篡改区域通常缺乏共同的痕迹，与真实区域相比，它们只是图像中的不同来源（Huh等人，2018）。因此，试图记忆伪造痕迹会导致混淆，从而导致学习不稳定。相反，SAFIRE使用点作为参考来学习每个源区域的均匀特征，而不是记忆伪造痕迹。这种方法导致了稳定且有效的学习，在传统的二值IFL和新的源分区任务中均取得了高性能。<br/>​  作为第一篇通过多源分割解决IFL问题的论文，我们创建了一个由多源图像组成的SafireMS数据集，以促进该领域的进一步研究。我们计划公开发布。<br/>​  我们的主要贡献可以总结如下：</p><ul><li>我们引入了一种新的IFL任务，该任务将伪造的图像按每个来源进行划分。它有助于理解伪造图像的组成，并使进一步的分析更容易。</li><li>我们提出了一种新的IFL方法SAFIRE，它使用内部的点提示。它是第一个能够进行多源分区的技术，但它可以使用传统的二进制数据集进行训练。</li><li>大量的实验表明，SAFIRE在传统的二进制IFL和新任务中都表现出最佳性能。</li><li>为了便于对新任务的研究，我们构建并发布了一个包含由多个来源组成的图像的伪造数据集。</li></ul><h1 id="相关工作">2. 相关工作</h1><p><strong>图像伪造定位</strong></p><p><strong>Segment Anything Model</strong></p><p><strong>IFL中的SAM</strong></p><p>​  最近，有人尝试在IFL技术中使用SAM。一种方法（Su、Tan和Huang2024）通过向SAM添加SRM滤波器（Zhou等2018）构建了IFL模型。这种方法完全去除了提示编码器，实际上将SAM用作现代分割主干。另一项研究（Karageorgiou、Kordopatis-Zilos和Papadopoulos2024）利用注意力机制融合各种信号，并为此使用预训练且冻结的SAM进行实例分割。</p><p>​  总之，以往的研究主要将SAM用作骨架或仅用于获取分割掩模。这些方法忽视了SAM最重要的特性——其可提示能力，未能充分发挥其潜力。同时，我们开创性地将可提示的分割模型应用于图像的源区域划分。通过使用基于SAM的点提示，我们使每个点都能作为参考，分割包含该点的源区域。此外，受SAM自动掩模生成过程的启发，我们提出了一种推理技术，该技术涉及以网格模式在图像上放置点并汇总结果。这种方法首次实现了多源划分。</p><h1 id="方法">3. 方法</h1><h2 id="概观">3.1 概观</h2><p>​  我们提出的核心方法，即SAFIRE框架，涵盖了IFL的预训练、训练和推理过程。该框架中使用的神经网络称为SAFIRE模型，无需特定结构，可以自由修改。本文中，我们采用了SAM的略微修改结构，在图像编码器中仅添加适配层，以增强模型通过利用低级信号提取取证特征的能力（详见图2及附录）。该模型由图像编码器E（·）、提示编码器F（·）和掩码解码器D（·，·）组成。模型以图像I和点提示P作为输入，输出包含该点的源区域的预测图X和置信度分数s。<br/>​  接下来的章节将深入详细解释SAFIRE框架。首先，为了有效进行源图像分割，通过区域到区域对比学习对图像编码器进行预训练。随后，在主要的训练阶段，模型使用点提示对源区域进行分割进行训练，在最终的推理阶段，以网格的形式将多个点输入到模型中，并将所有结果聚合以获得最终的预测热图。</p><h2 id="预训练区域对区域对比学习">3.2 预训练：区域对区域对比学习</h2><p>​  我们提出区域到区域对比学习，以预训练图像编码器，实现有效的源区域划分（图3）。</p><figure><img src="../postimages/SAFIRE/image-20250506113928257.png"alt="image-20250506113928257" /><figcaption aria-hidden="true">image-20250506113928257</figcaption></figure><p>​  该方法旨在使来自同一源区域的嵌入在特征空间中靠近，而不同源区域的嵌入则相距较远，当一张图像包含两个或多个源时。<br/>​  利用对比学习中InfoNCE损失的经验证明的有效性（Oord，Li和Vinyals2018），我们定义我们的损失函数如下。设<spanclass="math inline">\(I\in\mathbb{R}^{3\times H\timesW}\)</span>是由r个源组成的输入图像，<spanclass="math inline">\(E(\cdot)\)</span>为图像编码器，<spanclass="math inline">\({\mathcal{E}}=E(I)\in\mathbb{R}^{V\times{\frac{H}{K}}\times{\frac{W}{K}}}\)</span>为图像嵌入，下采样比为K。我们稍微滥用一下符号，把<spanclass="math inline">\({\mathcal{E}}\)</span>看作是V维图像嵌入的集合。然后在<spanclass="math inline">\({\mathcal{E}}\)</span>中存在<spanclass="math inline">\(\frac{H}{K}\times{\frac{W}{K}}\)</span>个嵌入<spanclass="math inline">\(q\in\mathbb{R}^{V}\)</span>。我们还令<spanclass="math inline">\(\{\mathcal{E}_{i}\}_{i=1}^{r}\)</span>是<spanclass="math inline">\({\mathcal{E}}\)</span>的划分，其对应于I中的源区域。<br/>​  然后，我们定义区域到区域对比损失<spanclass="math inline">\(\mathcal{L}_{R2R}\)</span>为： <spanclass="math display">\[I n f o N CE(q,p,N)=-\log\left({\frac{\exp\left({\frac{q\cdotp}{\pi}}\right)}{\exp\left({\frac{q\cdot p}{\tau}}\right)+\sum_{n\inN}\exp\left({\frac{q\cdot n}{\tau}}\right)}}\right),\]</span></p><p><spanclass="math display">\[\mathcal{L}_{R2R}=\frac{1}{|\varepsilon|}\sum_{i=1}^{r}\sum_{q\in\varepsilon_{i}}In f o N CE\Big(q,\overline{\varepsilon_{i}\textbackslash\{q\}},\varepsilon\textbackslash\varepsilon_i\Big)\]</span></p><p>​  其中，τ是一个称为温度的超参数，|·|返回元素的数量，<spanclass="math inline">\(\overline{\cdot}\)</span>返回所有元素的平均值。<br/>​  在图像通过图像编码器之前，各种模糊、噪声添加或对比度变化等全局后处理被概率性地应用到图像上。通过这样做，我们期望图像编码器对全局常见变化具有鲁棒性，并更多地关注细微的局部差异。<br/>​  考虑到图像编码器的体积较大，我们确定目前可用的公开伪造数据集在规模和噪声方面都不够充分。因此，我们生成并使用了一个大规模无噪声的数据集SafireMS-Auto。更多内容见附录。</p><h2 id="训练使用点提示进行源区域分割">3.2训练：使用点提示进行源区域分割</h2><p>​  完成图像编码器预训练后，SAFIRE模型将进行主要训练，以根据指定的点提示准确分割源区域（图4）。</p><figure><img src="../postimages/SAFIRE/image-20250506151012101.png"alt="image-20250506151012101" /><figcaption aria-hidden="true">image-20250506151012101</figcaption></figure><p>​  图像编码器和提示编码器均被冻结：图像编码器处于预训练状态，提示编码器则保持原始SAM状态。通过向掩码解码器输入图像嵌入和提示嵌入，训练适配组件和掩码解码器，确保输出与正确的掩码对齐。</p><h3 id="创建点遮罩">3.2.1 创建点遮罩</h3><p>​  在训练过程中，需要将图像级别的真实掩模转换为对应给定点的掩模，我们称之为点掩模。如果存在多源掩模，每个源区域分配不同的标签，则可以通过将包含该点的源区域赋值为1，其他区域赋值为0来简单创建点掩模。然而，目前几乎所有用于IFL任务的数据集都仅以二进制形式存在，将被篡改的部分标记为1，未改变的部分标记为0。<br/>​  我们介绍了一种方法，将这些图像级别的二值掩码转换为点掩码。如果一张处理过的图像仅使用两个来源，标记为0和1的区域各自代表一个单一的源区域。进一步地，我们还考虑了连通组件。包含给定点的连通区域被标记为1，而与该区域相邻的其他连通区域则被标记为0。不相邻的区域被赋予-1的忽略标签，在计算损失时忽略这些标签。这种转换使我们能够仅使用带有二值标签的数据集进行多源分区训练。<br/>​  具体来说，设<spanclass="math inline">\(Y\in\{0,1\}^{H\timesW}\)</span>是图像I的真实掩码，其中包含c个连通分量，<spanclass="math inline">\(R=\{(i,j)\in\mathbb{Z}^{2}:0\leq i\lt H,0\leq j\ltW\}\)</span>是I的整数坐标集，<spanclass="math inline">\(\{R_{i}\}_{i=1}^{c}\)</span>是覆盖Y连通分量的R的划分，<spanclass="math inline">\(P\in R\)</span>是一个点提示，<spanclass="math inline">\(R^P\)</span>是包含P的区域，P属于<spanclass="math inline">\(\{R_{i}\}_{i=1}^{c}\)</span>。然后，点掩模<spanclass="math inline">\(Y_{P}\in\{-1,0,1\}^{H\timesW}\)</span>可以计算为： <spanclass="math display">\[Y_{P}[i,j]=\begin{cases}\begin{aligned}1,\qquad&amp;\mathrm{if}\ (i,j)\in R^{P}\\0,\qquad&amp; \mathrm{if}\ (i,j)\inneighbors(R^{P})\\-1,\qquad&amp;\mathrm{otherwise}\end{aligned}\end{cases}\]</span>​  其中，neighbors（·）返回相邻区域的并集。</p><h3 id="双区域配对点采样">3.2.2 双区域配对点采样</h3><p>​  图像编码器独立于点提示计算图像嵌入。充分利用这一特性，可以通过同时处理单个图像的多个点提示来实现高效训练。此外，为了平衡源区域，根据图像级别的真实标注，始终从标记为0和1的区域中成对采样点。</p><h3 id="区域自适应源分割损失">3.2.3 区域自适应源分割损失</h3><p>​  对于每个点，我们可以定义一个损失函数，该函数最小化预测图与点掩模之间的差异（图4）。这里，并非所有点掩模内的像素都对损失有同等贡献，因为这样做会导致较小区域被忽略。传统的IFL技术通过赋予篡改类别更大的权重来解决大多数图像中篡改区域较小的问题（Kwon等，2022）。然而，在我们的点掩模中，没有区分篡改区域和原始区域；只有多个源区域存在。因此，我们采用一种策略，即无论这些区域中的正确标签是0还是1，都赋予每个点掩模内较小区域更大的权重。这与大多数语义分割任务中使用的特定类别权重不同，后者是在单个图像内计算权重（Wang等，2020）。<br/>​  设I为输入图像，P为点提示，<spanclass="math inline">\((X, s)=D(E(I),F(P))\)</span>为掩码解码器的输出，其中X是预测图，s是置信度分数，<spanclass="math inline">\(Y_P\)</span>为P的真实点掩码。我们仅在有效标签区域<spanclass="math inline">\(R^{Y_{P},\{0,1\}}\)</span>内计算损失，通过<spanclass="math inline">\(R^{A,B}=\left\{(i,j)\in R\;:\;A[i,j]\inB\right\}\)</span>来实现。然后区域自适应源分割损失<spanclass="math inline">\(\mathcal{L}_{AASS}\)</span>定义为： <spanclass="math display">\[\mathcal{L}_{A A SS}=-\operatorname*{\mathbb{E}}_{(i,j)}[w_{1}\cdotY[i,\,j]\cdot\log(\sigma(X[i,\,j]))+\,w_{0}\cdot(1-Y[i,j])\cdot\log(1-\sigma(X[i,j]))]\\w_{1}=\operatorname*{min}\left(\frac{|R^{Y_P,\{0,1\}}|}{|R^{Y_P,\{1\}}|},C_{AA SS}\right){\mathrm{~,~and~}}w_{0}=\operatorname*{min}\left(\frac{|R^{Y_P,\{0,1\}}|}{|R^{Y_P,\{0\}}|},C_{AA S S}\right)\]</span> ​  其中期望是在<spanclass="math inline">\(R^{Y_P,\{0,1\}}\)</span>上计算的，σ（·）是一个S型函数，<spanclass="math inline">\(C_{AASS}\)</span>是一个限制权重的超参数。</p><h3 id="置信度损失">3.2.3 置信度损失</h3><p>​  用于推理时，掩码解码器还预测置信度分数。与SAM不同的是，SAM预测的是框级平均交并比（mIoU）分数，而我们的模型预测像素精度来衡量整体性能，而不是矩形mIoU。置信度分数损失<spanclass="math inline">\(\mathcal{L}_{c o n f}\)</span>定义为： <spanclass="math display">\[\mathcal{L}_{c o nf}=\operatorname*{MSE}_{R^{Y_{P},\{0,1\}}}(a c c\,(b in(X),Y_{P}),s)\]</span>​  其中，bin（·）将输入阈值化为二进制映射，将大于0的值转换为1，小于或等于0的值转换为0，MSE（·，·）返回像素均方误差，acc（·）返回准确度。</p><h3 id="总训练损失">3.2.4 总训练损失</h3><p>​  最后，我们得到总训练损失Ltrain如下： <spanclass="math display">\[\mathcal{L}_{t r a i n}=\mathcal{L}_{A A SS}+\lambda_{c o n f}\cdot L_{c o n f},\]</span>​  其中λconf是一个平衡两种损失的超参数。</p><h2 id="推理多点聚合">3.3 推理：多点聚合</h2><p>​  推理使用多个点提示（图5）。</p><figure><img src="../postimages/SAFIRE/image-20250506154707985.png"alt="image-20250506154707985" /><figcaption aria-hidden="true">image-20250506154707985</figcaption></figure><p>​  除了要推断的图像外，还以网格格式（例如16×16）提供点作为模型的输入。输出掩码被聚合以获得最终预测，这可以是多源地图或二值地图。<br/>​  设I为输入图像，P1，···，PN为提示点。首先，我们计算图像嵌入E= E(I)和所有i的提示点嵌入Fi =F（Pi）。由于图像嵌入提取与提示点无关，因此每张图像只需执行一次。因此，即使使用多个提示点，总计算量也不会大幅增加。<br/>​  之后，图像嵌入和点嵌入通过掩码解码器，从而可以得到对应于每个点的预测。掩码解码器的输出D（·，·）可以表示为：<spanclass="math display">\[(\{X_{1},\cdot\cdot\cdot\,,X_{N}\},\{s_{1},\cdot\cdot\cdot\,,s_{N}\})=D({\mathcal{E}},\{\mathcal{F}_{1},\cdot\cdot\cdot\,,\mathcal{F}_{N}\}),\]</span>​  其中Xi是一个预测图，si是Xi的一个置信度分数。<br/>​  下一步是计算每个预测Xi的代表性特征，这是对应于预测区域的图像嵌入的平均值。我们定义一个函数<spanclass="math inline">\(g:\mathbb{R}^{H\timesW}\rightarrow\mathbb{R}^{V}\)</span>，如下所示： <spanclass="math display">\[g(X)=\frac{1}{\left|\mathcal{R}^{b in(x),\{1\}}\right|}\sum_{(i,j)\in\mathcal{R}^{b in(x),\{1\}}}{\mathcal{E}}[i,j],\]</span> ​  其中，<spanclass="math inline">\({\mathcal{R}}\)</span>是<spanclass="math inline">\({\mathcal{E}}\)</span>的整数坐标集合，<spanclass="math inline">\({\mathcal{X}}\)</span>是X的下采样预测图，以匹配与<spanclass="math inline">\({\mathcal{R}}\)</span>相同的分辨率。这里，<spanclass="math inline">\(\mathcal{R}^{b in(x),\{1\}}\)</span>表示由预测X分割区域在嵌入空间中的坐标集合。对于所有i，其代表性特征可以表示为<spanclass="math inline">\({\mathcal{G}}_{i}=g(X_{i})\)</span>。<br/>​  随后，我们对代表性特征进行聚类。聚类基于这样的假设：SAFIRE模型能够准确提取特征，从而将来自同一源区域的特征聚集在一起。我们将{G1，···，GN}聚类为M个簇C1，···，CM。可以应用任何聚类算法，M可以预先固定或由算法回归确定。对于一般的源区域划分，我们可以允许算法确定合适的M。在已知源数量的情况下，可以使用具有固定簇数的算法。<br/>​  之后，从每个聚类中选择置信度最高的掩模。每个聚类代表输入图像的一个源区域，而置信度最高的掩模对应于该区域的最佳预测。我们收集每个聚类的最大置信度分数的索引：<spanclass="math display">\[j^{*}=\arg\operatorname*{max}_{\mathcal{G}_{i}\inC_{j}}s_{i}.\]</span>​  最后，将这些掩码组合起来以获得最终预测。最简单的方法是采用softmax：<spanclass="math display">\[X^{*}=\operatorname{sofmax}\{X_{1^{*}},\cdot\cdot\cdot,X_{M^{*}}\}.\]</span>​  对于M =2的特殊情况，为了获得二进制预测图，两个预测值的简单平均产生一个有效输出：<spanclass="math display">\[X^{*}=\frac{1}{2}\{\sigma(X_{1^{*}})+(1-\sigma(X_{2^{*}}))\}.\]</span></p><h1 id="二元ifl实验">4. 二元IFL实验</h1><p>​  我们首先从传统的图像中定位伪造区域的任务开始。请注意，SAFIRE可以进行二进制预测和多源预测。</p><h2 id="实验设置">4.1 实验设置</h2><p>​  实施细节。<br/>​  我们的模型先进行预训练，再进行训练。等式(1)中的区域到区域对比学习温度τ设置为0.1。等式(4)中AASS损失的权重上限CAASS设置为10，而等式(6)中的λconf设置为0.1。在推理阶段，M固定为2以获得二进制形式的预测。我们使用16×16个点提示和k均值聚类。</p><p>​  数据集。<br/>​  我们使用一个常见的设置（Guillaro等人，2023）来训练网络，该设置包含了四个数据集(Kniaz,Knyaz, and Remondino 2019; Novozamsky, Mahdian, and Saic 2020; Dong,Wang, and Tan 2013; Kwon etal.2022)，其由真实和虚假图像组成，也被称为CAT-Net（Kwon等人，2022）设置。我们使用五个与训练数据集没有重叠的公开数据集来测试性能：<br/>​  Columbia (Ng, Chang, and Sun 2004),<br/>​   COVERAGE (Wen et al. 2016),<br/>​   CocoGlide (Guillaro et al.2023), <br/>​   RealisticTampering(Korus and Huang 2016), <br/>​   NC16 (Guan et al. 2019)。<br/>​这些数据集包含多种伪造类型，包括拼接、复制移动、移除和使用生成模型添加对象。在测试过程中，图像以原始形式输入，但NC16数据集除外，由于某些比较方法的内存限制，图像被缩小了。</p><p>​  比较方法。<br/>​  根据（Guillaro等，2023）的协议，我们通过选择具有公开可访问代码和预训练模型的最新技术来确保公平比较，这些模型是在测试集的不同数据集上训练的。具体包括<br/>​  ManTra-Net (Wu,AbdAlmageed, and Natarajan 2019), <br/>​   SPAN (Hu etal.2020), <br/>​   AdaCFA (Bammey, Gioi, and Morel 2020), <br/>​   CATNetv2 (Kwon et al. 2022), IF-OSN (Wu et al. 2022),<br/>​   MVSS-Net (Dong etal. 2022), PSCC-Net (Liu et al. 2022),<br/>​   TruFor (Guillaro et al.2023), <br/>​   NCL (Zhou et al.2023a)。<br/>​  此外，我们还使用了在同一数据集上训练的纯SAM（Kirillov等，2023）模型。</p><p>​  度量。<br/>​  我们以与TruFor（Guillaro等人，2023）论文相同的方式评估定位性能。具体而言，性能报告采用置换F1分数（Huh等人，2018；Kwon等人，2022），使用固定0.5阈值(F1fixed）和每张图像的最佳阈值(F1 best）。</p><h2 id="评价结果">4.2 评价结果</h2><p>​  表1展示了二进制IFL性能的比较分析。</p><figure><img src="../postimages/SAFIRE/image-20250506164719887.png"alt="image-20250506164719887" /><figcaption aria-hidden="true">image-20250506164719887</figcaption></figure><p>​  连字符（‘-’)表示该数据集用于训练，因此被排除在外。值得注意的是，SAFIRE在F1固定和F1最佳两个指标上均表现出色，在五个数据集中有四个获得了第一名。此外，所有数据集的平均得分再次证实了SAFIRE的优越性，使其在整体性能上稳居首位。另外，在附录中可以看到，我们的方法在各种全局后处理条件下也优于其他技术，证明了其稳健性。</p><p>​  图6显示了每个模型产生的IFL的定性结果。</p><figure><img src="../postimages/SAFIRE/image-20250506164829126.png"alt="image-20250506164829126" /><figcaption aria-hidden="true">image-20250506164829126</figcaption></figure><p>​  SAFIRE成功识别了其他技术未能检测到的复杂且具有挑战性的操作，并且假阳性检测较少。特别是，与其它技术相比，SAFIRE在复杂的人工智能生成的部分操作中实现了显著更准确的预测。</p><h2 id="消融研究">4.3 消融研究</h2><p>​  为了保证我们研究的完整性，我们对框架的关键组件进行了消融研究：区域到区域对比损失、区域自适应源分割损失中的区域自适应特征、点提示和置信度损失（表2）。</p><figure><img src="../postimages/SAFIRE/image-20250506164928497.png"alt="image-20250506164928497" /><figcaption aria-hidden="true">image-20250506164928497</figcaption></figure><p>​  我们用常规的对应物来代替每一个进行比较。<br/>​  结果表明，在没有任何一个组件的情况下，性能会比完整的SAFIRE框架差，后者集成了所有四个组件。此外，排除所有四个关键特征的基线模型表现出显著较差的结果，这突显了这四个组件在SAFIRE中不可或缺的作用。<br/>​  特别是，我们观察到基于提示的源区域划分优于二值分割。具有相同结构和预训练的模型在使用二值分割时，仅能达到28.0%的固定F1分数。然而，当采用提示进行源区域划分时，模型性能显著提升，达到62.6%。这证明了SAFIRE的提示方法在使网络理解同一源区域特征方面的有效性，从而实现稳定学习和卓越表现。</p><h1 id="多源分区实验">5. 多源分区实验</h1><h1 id="结论">6.结论</h1><p>​  SAFIRE通过将图像划分为多个原始区域，解决了通过二进制分割查看IFL任务的传统方法的问题。通过区域间的对比预训练，我们引导编码器有效地嵌入源分区所需的微妙信号。我们利用基于点提示的分割来训练SAFIRE模型，使其能够准确预测每个点所在的源区域。在推理过程中，我们以网格格式提供点提示，并汇总输出以获得最终预测结果。经过全面评估，SAFIRE成功解决了IFL中的标签无关性问题，并超越了其他最先进方法。它还为图像分区中使用点提示开辟了可能性，并提出了将图像划分为多个源区域的新挑战。这有助于理解伪造图像的结构，促进进一步分析。我们希望我们的研究能为解决AI时代日益复杂的图像伪造问题做出贡献。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Rethinking Image Editing Detection in the Era of Generative AI Revolution</title>
      <link href="/Rethinking-Image-Editing-Detection-in-the-Era-of-Generative-AI-Revolution/"/>
      <url>/Rethinking-Image-Editing-Detection-in-the-Era-of-Generative-AI-Revolution/</url>
      
        <content type="html"><![CDATA[<p>Rethinking Image Editing Detection in the Era of Generative AIRevolution</p><p>Zhihao Sun，Haipeng Fang，Juan Cao，Xinying Zhao，Danding Wang∗</p><p>1中国科学院计算技术研究所、中国科学院大学<br/>2中国科学院计算技术研究所</p><h1 id="摘要">摘要</h1><p>​  图像编辑与处理技术对图像内容的真实性、安全性带来严重威胁，因此图像区域篡改检测研究一直是图像处理领域的重要课题。生成式AI的快速发展显著提升了区域生成编辑方法的可行性和有效性，正逐步取代传统图像编辑工具或算法。然而当前研究仍主要聚焦于传统图像篡改领域，目前缺乏包含大量生成式区域编辑方法处理图像的综合性数据集。<br/>​  我们致力于通过构建GRE数据集来填补这一空白，该数据集是一个大规模生成性区域编辑检测数据集，具有以下优势：<br/>​  1）整合了逻辑化与模拟化的编辑流程，利用多种模态下的大型模型；<br/>​  2）包含具有不同特征的各类编辑方法；<br/>​  3）为相关领域提供最先进方法的全面基准测试与评估；<br/>​  4）从必要性、合理性及多样性等多个维度对GRE数据集进行分析。<br/>​  大量实验和深入分析表明，这个更大更全面的数据集将显著提升生成性编辑检测方法的发展水平。相关数据仓库地址为https://github.com/ICTMCG/GRE。</p><h1 id="引言">1 引言</h1><p>​  尽管图像编辑与处理技术丰富了视觉内容，但同时也对各类媒体中的图像真实性与安全性构成重大威胁。因此，图像区域操控检测研究始终是关键课题。近年来，扩散模型在计算机视觉领域掀起AI生成革命，在可控编辑[29,30,46,47]等任务场景中展现出卓越性能。生成式技术的进步降低了编辑成本并提升了效果，正逐步用生成式编辑方法取代传统工具。然而当前检测研究仍聚焦于传统编辑方式，在新型生成式区域操控检测方面仍存在研究空白。<br/>​  与全图生成技术需要精准控制的高难度操作不同，局部编辑方法展现出更强的灵活性，能够对原始图像中的特定内容进行修改[29,42,48]，从而改变其传达的信息。相较于使用PhotoShop等工具的传统手动处理方式，生成式区域编辑不仅对非专业人士更友好便捷，还能实现高质量的编辑效果。</p><figure><imgsrc="../postimages/Rethinking-Image-Editing-Detection-in-the-Era-of-Generative-AI-Revolution/image-20250726220712108.png"alt="image-20250726220712108" /><figcaption aria-hidden="true">image-20250726220712108</figcaption></figure><p>​  图1(a)展示了多种代表性生成式区域编辑方法的性能表现，直观呈现了区分真实图像与编辑图像的难度。如今我们确实可以断言“眼见未必为实”。[21]因此，生成式区域编辑的检测能力值得我们重点关注。<br/>​  本文构建了一个名为生成区域编辑（GRE，GenerativeRegionalEditing）的新型大规模数据集，专注于检测生成性区域编辑任务。基于该数据集，我们建立了跨领域评估现有检测方法的基准体系，并从必要性、合理性及多样性等多个维度对数据集进行分析。大量实验和深入研究证明，这个规模更大、内容更全面的数据集将显著推动生成编辑检测方法的发展。具体而言，GRE数据集相较于现有相关数据集具有以下显著优势：<br/>​  (1)逻辑与模拟编辑流程。过去，小规模区域编辑数据集通过人工操作确保逻辑连贯性（例如避免天空中出现狗的视觉冲突），而大规模数据集则难以通过简单的自动化编辑流程维持逻辑一致性。为确保编辑过程中的逻辑连贯性、增强编辑语义丰富度、适应数据规模扩展及提升系统可扩展性，我们整合了多种超大模型在不同模态下的应用，构建了一个包含感知、创意和实现三大模块的完整图像编辑流程。<br/>​  (2)多元编辑方法研究。在实际应用场景中，我们无法预知编辑工具或方法的具体形式，因此评估检测模型对不同甚至未知编辑方式的泛化能力至关重要。为此，我们选取了多种代表性编辑方法进行深入探究。这些方法在架构设计上存在差异，包括基于生成对抗网络（GAN）、扩散网络以及黑盒方法等类型，其编辑控制机制也各具特色。<br/>​  (3)全面基准测试。除了区分处理图像与真实图像的二分类任务外，通过解答图像被编辑的具体位置和方式，我们还致力于提升图像篡改检测任务在现实媒体取证场景中的可解释性。我们在数据集中提供了多层级标注，并提出了三项核心任务：1）编辑图像分类，判断图像是否经过编辑；2）编辑方法归因，识别图像中使用的编辑手段；3）编辑区域定位，精确定位图像中的篡改区域。通过评估前沿方法在这三项任务上的表现，实验表明虽然像素级定位任务更具挑战性，但在视觉效果丰富的编辑图像中发现篡改元素仍具有重要价值。<br/>​  (4)深入分析。我们通过大量实验对GRE数据集作为基准需要具备的关键特性进行分析，包括其必要性、合理性、多样性等。通过现有数据集的跨数据集实验，我们验证了GRE数据集在解决新型生成性区域编辑检测研究空白方面的必要性。TCAV分析和用户研究证实，该数据集不存在实体偏见，且人工难以区分编辑操作。交叉编辑方法实验突显了生成式编辑方法多样性的价值。这些多维度验证共同表明，GRE是一个高质量的数据集。</p><h1 id="相关工作">2 相关工作</h1><h2 id="数据集的生成与处理">2.1 数据集的生成与处理</h2><p>​  <strong>图像生成</strong><br/>​  近年来，生成图像检测领域备受关注，催生了DeepArt[38]、IEEE VIPCup[36]、DE-FAKE [41]和CiFAKE[2]等众多基准测试，以及GenImage[50]提供的百万级数据集。然而这些数据集中的生成图像主要适用于图像级生成检测任务，难以完全满足编辑区域定位任务的需求。专门构建用于生成区域编辑检测的数据集不仅成本高昂，其像素级自动化编辑流程也比图像级生成任务更为复杂。</p><p>​  <strong>区域图像编辑</strong><br/>​  检测图像中被篡改或编辑的区域始终是长期存在的技术难题。表1汇总了现有数据集的规模、图像来源及编辑方法，这些数据集已被广泛使用并获得业界认可。</p><figure><imgsrc="../postimages/Rethinking-Image-Editing-Detection-in-the-Era-of-Generative-AI-Revolution/image-20250726221155694.png"alt="image-20250726221155694" /><figcaption aria-hidden="true">image-20250726221155694</figcaption></figure><p>​  其中，Columbia[31]、CASIA [5]、Coverage [39]、NIST16 [7]和IMD20[23]主要包含早期非生成式编辑形式（如简单剪切和复制移动）。唯有DEFACTO[22]收录了相对完整的生成式编辑图像数据集。但DEFACTO的自动化编辑流程仍存在明显痕迹。CocoGlide[8]包含512张基于COCO数据集通过GLIDE扩散模型生成的图像。AutoSplice[11]则利用DALL-E2在文本提示引导下进行自动编辑，并辅以人工校验。然而，这些数据集所采用的生成式编辑方法存在局限性，导致其无法为检测模型提供全面分析或泛化能力。</p><h2 id="生成式区域编辑方法">2.2 生成式区域编辑方法</h2><p>​  <strong>基于扩散的方法</strong><br/>​  扩散模型的出现真正推动了生成式编辑方法在便捷性和有效性方面超越了依赖人工干预的操作序列。StableDiffusion[29]代表了一种先进的文本到图像扩散模型。在推理过程中引入简单的掩码替换操作，可实现精准的区域编辑。ControlNet[48]创新性地引入模块化设计，通过调用预训练的大规模扩散模型来适应不同输入条件。PaintbyExample[42]则采用范例引导式图像编辑技术，突破传统语言引导模式，使编辑过程达到更高精度控制。</p><p>​  <strong>基于GAN的方法</strong><br/>​  然而，我们也必须承认近年来基于生成对抗网络（GAN）的图像编辑方法在性能上取得了显著提升。MAT[15]通过定制化设计了一个以修复为主的transformer模块，其中注意力模块仅从部分有效的标记中提取非局部信息，动态掩码则对此进行有效调控。该方法在应对大规模图像修复挑战时展现出卓越效果。LaMa[33]通过在推理过程中最小化多尺度一致性损失，优化了网络的中间特征图。这种策略巧妙解决了高分辨率图像细节缺失的问题，从而显著提升了视觉效果质量。</p><h1 id="gre结构">3 GRE结构</h1><p>​  现有大多数图像生成数据集仅包含完整生成样本，未考虑图像内部区域编辑的常见场景。以往多数区域编辑数据集仅包含人工操作样本，缺乏生成模型参与，且创作过程缺乏逻辑合理性与语义多样性的考量。相比之下，我们提出的GRE数据集提供了多种生成式区域编辑方法，并定义了三个核心任务（即编辑图像检测、编辑区域定位及编辑方法溯源），共计包含22.8万张图像。我们设计了一套由多模态大型模型辅助的自动化编辑流程，能够执行逻辑一致的编辑操作。如表1所示，我们将GRE与其他公开区域编辑数据集进行对比。从表中列出的各项指标来看，我们的数据集在规模和多样性方面均优于其他同类数据集。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EndToEndClustering</title>
      <link href="/EndToEndClustering/"/>
      <url>/EndToEndClustering/</url>
      
        <content type="html"><![CDATA[<h1 id="twin-contrastive-learning-for-online-clustering">1.TwinContrastive Learning for Online Clustering</h1><p>摘要</p><p>​  本文提出通过在实例和聚类水平上进行双对比学习（TCL）来进行在线聚类。具体来说，我们发现当数据投影到目标簇数维数的特征空间时，其特征矩阵的行和列分别对应于实例和聚类表示。基于观察，对于给定的数据集，提出的TCL首先通过数据增强构造正对和负对。然后，在特征矩阵的行空间和列空间中，分别通过将正对推开，进行实例级和聚类级对比学习。为了减轻内在假阴性对的影响和校正聚类分配，我们采用了一个基于置信度的标准来选择伪标记，以提高实例级和聚类级的对比学习。从而进一步提高了聚类性能。除了双对比学习的优雅理念外，TCL的另一个优点是，它可以独立地预测每个实例的聚类分配，从而毫不费力地拟合在线场景。在6个广泛使用的图像和文本基准上进行的广泛实验证明了TCL的有效性。该代码将在GitHub上发布。</p><p>1介绍</p><p>​  在本研究中，我们基于图1所示的观察结果，提出了一种基于双对比学习（TCL）的端到端在线深度聚类方法。</p><figure><img src="../postimages/EndToEndClustering/image-20250323224746037.png"alt="image-20250323224746037" /><figcaption aria-hidden="true">image-20250323224746037</figcaption></figure><p>​  图1：我们的主要观察结果和基本思想。通过将数据投影到维数为簇数的特征空间中，特征矩阵的第i行和第k列中的元素表示实例i属于聚类k的概率。即，行对应于集群分配概率，这是实例的特殊表示。更有趣的是，如果我们从列视图中看特征矩阵，每一列实际上对应于数据上的集群分布，这可以看作是集群的一种特殊表示。因此，可以分别在行级、列空间和列空间中进行实例级和集群级的表示学习（例如，对比学习）。</p><p>​  简而言之，特征矩阵的行和列分别对应于实例表示和簇表示。在此基础上，TCL对特征矩阵的行空间和列空间进行对比学习，共同学习实例和聚类表示。具体来说，TCL首先通过数据扩充来构造对比对。与大多数现有的使用SimCLR中提出的弱增强的对比学习方法不同（Chenetal.，2020a）不同，我们提供了一种新的混合有效增强策略。对于构建的对，TCL在实例和聚类水平上进行对比学习。实例级对比学习的目的是将不包含的实例拉在一起，同时将类之间的实例分开。聚类级对比学习的目的是区分不同聚类的分布，同时吸引不同增强下的分布。为了减轻内在假阴性对的影响并纠正聚类分配，我们逐步选择可信的预测（即那些聚类分配概率接近一热的预测）来对双对比学习进行微调。这种微调策略是基于以下观察，即具有高可信度的预测更有可能是正确的，因此可以用作伪标签。一旦模型收敛，它就可以以端到端方式独立地对每个实例进行集群分配，以实现集群。本工作的主要贡献总结如下：</p><ul><li>我们揭示了特征矩阵的行和列在本质上对应于实例和集群表示。在此基础上，我们提出了通过在实例和聚类水平上同时进行对比学习来实现聚类的TCL；</li><li>我们提供了一种新的数据增强策略，通过混合弱转换和强转换，它自然适合我们的TCL框架，并在我们的实验中被证明对图像和文本数据都是有效的；</li><li>为了减轻内在假阴性的影响和校正聚类分配，我们采用了一个基于置信度的准则来生成伪标签，以便对实例级和聚类级的对比学习进行微调。实验结果表明，这种微调策略可以进一步提高聚类性能；</li><li>提出的TCL以端到端和在线的方式对数据进行集群，只需要批优化，因此可以处理大规模数据集。此外，TCL可以处理流数据，因为它可以及时地为新的数据进行集群分配，而不访问整个数据集。</li></ul><p>2相关工作</p><p>​  虽然对比学习和深度聚类的结合带来了很好的结果，但大多数现有的工作仍然是分开处理这两个任务的。与现有的研究不同，本研究优雅地将对比学习和深度聚类统一到双对比学习框架中，这可能会给两个社区带来一些见解。值得注意的是，本研究是（Liet al.，2021b）的一个重要扩展，有以下改进：</p><ul><li>在本文中，我们提出了一种基于置信度的增强策略来微调实例级和聚类级的对比学习。具体来说，大多数混杂预测被选择作为伪标签是基于观察到它们更有可能是正确的。在此基础上，我们利用生成的伪标签来缓解实例级对比学习中假阴性对（由类内样本组成）的影响，并在聚类级对比学习中采用交叉熵损失来校正聚类分配。值得注意的是，这种双自我训练范式受益于我们的TCL框架，因为实例特征（来自ICH）的集群分配（来自CCH）可以通过在线方式获得。</li><li>在本文中，我们提出了一种通过混合弱转换和强转换的数据增强策略。虽然这种增强策略看起来很简单，但其有效性与所提出的TCL框架密切相关。先前的研究表明，直接在对比学习框架中引入强增强可能会导致次优表现（WangandQi，2021）。与此结论不同的是，我们表明混合增强策略自然适合所提出的TCL框架（更多细节见表6）。</li><li>为了研究该方法的泛化能力，我们验证了我们的方法在文本聚类中的有效性，尽管在数据增强方面存在差异。实验结果表明，所提出的TCL框架、混合增强策略和基于置信度的增强策略具有优越性。与之前的会议论文相比（Li等人，2021b），该期刊扩展获得了类似的性能提高。</li></ul><p>3方法</p><p>​  提出的TCL的管道如图2所示。</p><figure><img src="../postimages/EndToEndClustering/image-20250323230401168.png"alt="image-20250323230401168" /><figcaption aria-hidden="true">image-20250323230401168</figcaption></figure><p>​  首先，它通过弱增强和强增强来构造数据对。一个共享的主干用于从增强的样本中提取特征。然后，两个独立的MLPs（σ表示ReLU激活，∼表示Softmax操作产生软标签）将特征投射到行和列空间，其中分别进行实例级和聚类级对比学习。最后，基于聚类预测的置信度选择伪标签，以减轻假阴性对的影响，并纠正之前的预测，进一步提高了聚类性能。</p><p>​  该模型由对比对构造（CPC）、实例级对比头（ICH）和集群级对比头（CCH）三个部分组成，通过双对比学习和基于信心的增强进行联合优化。具体来说，在双对比学习阶段，CPC首先通过数据增强来构造对比对，然后将对比对投射到一个潜在的特征空间中。然后，ICH和CCH通过最小化所提出的双对比损失，分别在特征矩阵的行空间和列空间进行实例级和聚类级对比学习。为了缓解对比学习中内在假阴性对的影响，并纠正聚类分配，我们提出了一种基于置信度的增强策略（CB）。详细地，选择了一些自信的预测作为伪标签，利用自监督对比损失和自标记损失对实例级和聚类级对比学习进行微调，进一步提高了聚类性能。<br/>​  一旦模型收敛，CCH就可以对每个实例进行集群分配，以实现在线集群。值得注意的是，尽管双对比学习可以在我们的基本思想中直接在相同的对比头上进行，但我们通过实验发现，将其解耦成两个独立的子空间可以提高聚类性能（详细讨论见第4.6.4节）。<br/>​  在本节中，我们首先介绍了CPC中对比对的构造，然后给出了训练中的双对比损失，最后详细阐述了我们基于信心的增强策略。</p><p>3.1对比对构造</p><p>​  受对比学习的最新发展的启发（Caron等人，2020；Chen等人，2020a），提出的TCL通过数据增强构建对比对。具体来说，对于每个实例xi，CPC随机抽样，并分别应用来自两个增强族t和t0的两组变换t和t0，得到两个相关样本（即数据对），表示为˜x2i−1=t（xi）和x˜2i=t0（xi）。</p><p>​  给定构建的对，使用共享主干f（·）通过h2i−1=f（˜x2i−1）和h2i=f（˜x2i）从增广样本中提取特征h。特定的骨干网用于处理不同类型的数据。在这项工作中，我们分别采用ResNet（Heetal.，2016）和句子变压器（Reimers和Gurevych，2019）作为图像和文本数据的主干。</p><p>3.2双对比学习</p><p>​  在训练阶段，骨干、实例级对比头（ICH）和集群级对比头（CCH）根据以下双对比损失进行联合优化，即：<span class="math display">\[\mathcal{L}_{t r a i n}=\mathcal{L}_{i ns}+\mathcal{L}_{c l u},\]</span> ​  其中，<spanclass="math inline">\(\mathcal{L}_{i ns}\)</span>为ICH上计算的实例级对比损失，<spanclass="math inline">\(\mathcal{L}_{c lu}\)</span>为CCH上计算的簇级对比损失。<br/>​  一般来说，可以添加一个动态权重参数来平衡整个训练过程中的两个损失，但显式地调整权重可能会违反无监督约束。在实践中，我们发现这两个对比损失的简单加法已经很有效了。</p><p>3.2.1实例级对比损失</p><p>​  实例级对比学习的目的是最大化正对的相似性，同时最小化负对的相似性。为了实现聚类，理想情况下，可以将类内实例定义为正，将类间实例定义为负。然而，由于没有给出先前的标签信息，我们基于数据扩充构建实例对作为一种折衷。具体地说，正对由来自同一实例的样本组成，负对组成则相反。<br/>​  形式上，对于大小为N的小批，TCL对每个实例xi执行两种类型的数据增强，从而得到2N增强样本<spanclass="math inline">\(\left\{\tilde{x}_{1},\tilde{x}_{2},\cdot\cdot\cdot,\tilde{x}_{2i-1},\tilde{x}_{2i},\cdot\cdot\cdot,\tilde{x}_{2N}\right\}\)</span>。每个样本˜x2i−1与其他样本形成2N−1对，其中我们选择相应的增广样本<spanclass="math inline">\(\{\tilde{x}_{2i-1},{\tilde{x}}_{2i}\}\)</span>为正，并将其他2N−2对定义为负。<br/>​  由于直接对特征矩阵进行对比学习可能会导致信息丢失（Chenet al.，2020a），我们叠加了一个两层非线性MLP gI（·），通过齐=gI（hi）将特征映射到子空间，其中应用实例级对比学习。成对相似性采用余弦距离进行测量，即，<spanclass="math display">\[s(z_{i},z_{j})=\frac{\mathcal{z}_{i}\mathcal{z}_{j}}{\|z_{i}\|\|z_{j}\|},i,j\in[1,2N].\]</span>​  采用InfoNCE损失（Oord等人，2018）来优化等式定义2的成对相似性，在不丧失一般性的情况下，将给定的增广样本˜xi（假设它与˜xj形成一个正对）的损失定义为<spanclass="math display">\[\ell_{i}=-\log\frac{\exp(s(z_{i},z_{j})/\tau_{I})}{\sum_{k=1}^{2N}\mathrm{~l}_{[k\neqi]}\exp{(s(z_{i},z_{k})/\tau_{I})}},\]</span>​  其中，τI是控制柔软度的实例级温度参数，而1[k=i]是一个计算当为1k/=i时的指标函数。为了识别每个增广样本的正对应物，需要计算所有增广样本的实例提升对比损失，即：<span class="math display">\[\ell_{i ns}={\frac{1}{2N}}\sum_{k=1}^{2N}\ell_{k}.\]</span>3.2.2集群级对比损失</p><p>​  当一个样本被投影到一个维数等于聚类数的子空间时，其特征的第i个元素表示其属于第i个聚类的概率。换句话说，特征向量对应于其聚类分配概率。<br/>​  假设目标簇数为M，类似于实例级对比头，我们使用另一个双层MLPgC（·）通过yi =gC（hi）将特征投影到一维空间中。这里的yi对应于增强样本˜xi的聚类分配概率。形式上，让<spanclass="math inline">\(Y=[y_{1},\cdot\cdot\cdot,y_{2i-1},\cdot\cdot\cdot,y_{2N-1}]\in{\mathcal{R}}^{N\times M}\)</span>是弱增强T下的簇分配概率（和<spanclass="math inline">\(Y^{\prime}=\left[y_{2},\cdot\cdot\cdot,y_{2i},\cdot\cdot\cdot,y_{2N}\right]\)</span>强增强T0下的y2N）。根据图1所示的观察结果，Y和Y0的列对应于小批处理上的聚类分布，可以解释为特殊的聚类表示。我们想指出的是，即使维度大于地面真实的星团数，这一观测结果仍然成立。在这种情况下，考虑了一个更细粒度的集群结构，并在BarlowTwins中验证了其有效性（Zbontar etal.，2021）。<br/>​  为了清晰起见，我们将Y的i-列表示为ˆy2i−1（对Y0的i-列表示为ˆy2i），即在弱（和强）数据增强下的簇i的表示。同一簇在两个增强下的表示形成正簇对{yˆ2i−1，yˆ2i}，i∈[1，M]，而其他对被定义为负。同样，我们使用余弦距离来度量簇ˆyi和簇ˆyj之间的相似性，即<spanclass="math display">\[s(\hat{y}_{i},\hat{y}_{j})=\frac{\hat{y}_{i}^{\top}\hat{y}_{j}}{\|\hat{y}_{i}\|\|\hat{y}_{j}\|},i,j\in[1,2M]\]</span>​  在不丧失一般性的情况下，采用以下损失函数从除其对应的ˆyj外的所有其他2M−2集群中识别集群ˆyi，即：<spanclass="math display">\[\hat{\ell}_{i}=-\log\frac{\exp(s(\hat{y}_{i},\hat{y}_{j})/\tau_{C})}{\sum_{k=1}^{2M}\mathrm{l}_{[k\mpi]}\exp\left(s(\hat{y}_{i},\hat{y}_{k})/\tau_{C}\right)},\]</span>​  其中，τC是控制柔软度的簇级温度参数，而1[k=i]是一个评价为1 iff k =i的指标函数。通过遍历所有的簇，可以计算出簇级的对比损失 <spanclass="math display">\[\mathcal{L}_{c lu}^{\prime}=\frac{1}{2M}\sum_{k=1}^{M}\hat{\ell}_{k}.\]</span>​  由于简单地优化上述聚类级对比损失可能会导致简单的解决方案，即大多数样本被分配到几个聚类中，我们添加了一个聚类熵，以防止模型崩溃，并实现更平衡的聚类（GhasediDizaji等人，2017；Huang等人，2020）。公式上，让<spanclass="math inline">\(P(\hat{y}_{2i-1})\={\frac{1}{N}}\sum_{k=1}^{N}Y_{ki}\)</span>成为弱增强条件下簇i在一个小批内的分配概率，<spanclass="math inline">\(P(\hat{y}_{2i})\={\frac{1}{N}}\sum_{k=1}^{N}Y^{\prime}_{ki}\)</span>成为强增强条件下簇i在一个小批内的分配概率，然后计算出团簇熵<span class="math display">\[{ H}_{c l u}=-\sum_{i=1}^{2M}[{\calP}(\hat{y_{i}})\log{\cal P}(\hat{y_{i}})].\]</span>​  综上所述，簇级对比损失最终定义为 <spanclass="math display">\[\mathcal{L}_{c lu}=\frac{1}{2M}\sum_{k=1}^{2M}\hat{\ell}_{k}-H_{c l u}.\]</span>3.3基于置信度的提升</p><p>​  随着训练的进行，我们注意到该模型倾向于做出更自信的预测（即，聚类分配概率接近于一个热点）。这些相反的预测更有可能是正确的（见图4）。基于这一观察结果，在推进阶段，我们逐步选择最自信的预测作为伪标签，以微调实例级和集群级的对比学习。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LiDiNet</title>
      <link href="/LiDiNet/"/>
      <url>/LiDiNet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Finding Incompatible Blocks for Reliable JPEG Steganalysis</title>
      <link href="/Finding-Incompatible-Blocks-for-Reliable-JPEG-Steganalysis/"/>
      <url>/Finding-Incompatible-Blocks-for-Reliable-JPEG-Steganalysis/</url>
      
        <content type="html"><![CDATA[<p>Finding Incompatible Blocks for Reliable JPEG Steganalysis</p><p>Etienne Levecque , Jan Butora , and Patrick Bas</p><h1 id="摘要">摘要</h1><p>​  本文提出了一个质量因子为100的不兼容JPEG图像的改进概念。它可以检测到在DCT系数中嵌入的隐写方案的存在。我们表明，在JPEG管道中，DCT变换与量化函数的结合可以将像素域中的多个块映射到DCT域中的同一块。然而，并不是每个DCT块都可以获得：我们称这些块不兼容。特别是，当手动修改DCT系数以嵌入消息时，可能会发生不兼容性。我们证明了区分兼容块和不兼容块的问题是一个有解的反问题，并提出了两种不同的方法来解决它。第一个是基于启发式的，如果它存在，就可以快速找到解决方案。第二种是整数线性规划问题，只能在合理的时间内检测到特定DCT变换的不兼容块。我们证明了一个块变得不相容的概率只依赖于修改的数量。最后，利用启发式算法，我们可以根据每幅图像的兼容块的数量得到一个似然比检验来执行步进分析。我们模拟了这个测试的结果，并表明，它仅使用256×256图像的10%的块，在0.001和0.01bpp之间的每个有效负载上都优于深度学习检测器e-SRNet。一个选择通道感知的测试版本甚至更强大，性能优于e-SRNet，而只使用了1%的块。</p><h1 id="i.介绍">I.介绍</h1><p>​  在隐写术中，主要目的是将信息隐藏在无害的媒体中，比如被称为封面媒体的图像或视频。为了确保安全，隐写术信息需要无法从执行步骤分析的人那里检测到，即试图通过分析它们来区分封面和阶梯媒体。这个在隐写师和步骤分析人员之间的猫捉老鼠的游戏非常依赖于许多参数，例如用于隐藏消息的媒体格式、有效负载大小或所使用的嵌入方案。在本文中，我们关注JPEG格式，它具有较好的压缩性能，并且在不同的软硬件实现中广泛应用，仍然是数字图像最常用的压缩方案之一。我们针对一类修改JPEG图像的量化DCT系数的嵌入方案。这代表了各种各样的嵌入方案，如JSTEG[16]，1995年开发的[21]，或流行的学术方案，如JUniward[13]或UERD[11]。<br/>​  在本文中，我们提出了一种利用已知压缩管道的JPEG图像压缩特性的方法，该方法可用于检测所有改变JPEG量化DCT系数的隐写算法的消息。这个属性的主要思想相当简单：给定一个（压缩）管道和一个以JPEG格式编码的图像，我们想知道编码的图像是否是这个管道的输出。</p><figure><imgsrc="../postimages/Finding-Incompatible-Blocks-for-Reliable-JPEG-Steganalysis/image-20250327213437876.png"alt="image-20250327213437876" /><figcaption aria-hidden="true">image-20250327213437876</figcaption></figure><p>图1。2DJPEG压缩的说明，说明不兼容攻击。使用两点DCT算法(3)将每个1×2像素的块（左图）映射到DCT空间（右图）中的一个点。我们可以看到，压缩不是满射的，这意味着DCT空间中的一些块（由孔表示）在像素域中没有任何前因。另一方面，在压缩过程中，一些像素块被映射到同一个DCT块。在具有64个维度的标准JPEG块中，我们观察到相同的特性。该攻击利用了这样一个事实，即在嵌入过程中，会创建一些不兼容的块。</p><p>​  这个问题，如图1所示，可以看作是一个反问题，因为如果我们能找到一个管道的输入图像，作为输出的观测图像，那么我们就可以得出输出图像与管道兼容的结论。相反，如果没有输入生成输出图像，那么它与此管道不兼容。在我们的例子中，这意味着被检查的图像已经被嵌入算法篡改，因此它与压缩方案不兼容。<br/>​  本文研究的不相容性与JPEG压缩管道有关，但不仅是因为它也可以用于任何使用DCT变换和量化DCT系数的量化步长接近或等于1的编码方案。这意味着这种方法可以潜在用于编码方案，如静态图片的HEIC，或用于移动图片的H26x类编码方案。<br/>​  请注意，从隐写分析的角度来看，检测不兼容的图像非常有趣，因为在这种情况下，steganalyst，Eve不必面对潜在的误报。如果她了解压缩管道并且可以在图像中找到至少一个不兼容的块，她确信图像是Stego。因此，能够检测不兼容的图像与执行可靠的隐写分析有关。<br/>​  不兼容性的概念是步骤分析中最早的策略之一，并于2001年在Fridrich等人的开创性论文[10]中引入，当时信息隐藏在解压缩的JPEG图像中。在这种情况下，观察到的图像是以前在JPEG中压缩的像素图像，由于隐写嵌入而对像素值进行了一些潜在的修改。因此，所研究的管道是JPEG解压缩方案。为了重新压缩图像并检测嵌入，他们一边使用像素格式的测试图像之间的差信号，另一侧使用重新压缩解压缩图像之间的差信号。<br/>​  几年后，在Butora和弗里德里奇的论文[7]中引入了反向JPEG兼容性攻击（RJCA），用于将消息嵌入高质量JPEG的DCT域的情况，即等于100接近100的质量因子。对于这种攻击，输出图像是JPEG压缩图像，而管道是JPEG压缩图像。作者使用解压缩图像的舍入误差作为参考信号来检测嵌入，可以通过简单地计算信号的方差（嵌入后增加），或者使用深度神经网络，如e-SRNet[7]。<br/>​  请注意，不兼容性也被用于数字取证，以检测篡改操作。在解切割过程中使用的不同类型的插值之间的不兼容性是由Kirchner和Böhme [15]在2009年提出的。Vázquez-Padín等人[20]提出了一种更一般的方法来检测重采样信号和与插值管道相关的不兼容性，当采样管道——这里是插值核时，使用集合隶属度方法来估计重采样因子。阶梯分析中一个常见的假设称为选择通道感知（SCA），在Denemark等人的论文[9]中首次用于改进现有的检测器。这个假设假设执行步进分析的代理知道在嵌入过程中覆盖元素被修改的概率。在我们的例子中，这些元素是DCT系数，这个假设也可以用来选择更有可能被修改，因此更有可能不兼容的块。<br/>​  阶梯分析中一个常见的假设称为选择通道感知（SCA），在Denemark等人的论文[9]中首次用于改进现有的检测器。这个假设假设执行步进分析的代理知道在嵌入过程中覆盖元素被修改的概率。在我们的例子中，这些元素是DCT系数，这个假设也可以用来选择更有可能被修改，因此更有可能不兼容的块。<br/>​  这一贡献的大纲如图2所示，其中也突出显示了与论文的不同部分的联系。</p><figure><imgsrc="../postimages/Finding-Incompatible-Blocks-for-Reliable-JPEG-Steganalysis/image-20250327214627180.png"alt="image-20250327214627180" /><figcaption aria-hidden="true">image-20250327214627180</figcaption></figure><ul><li>由于不兼容性源于JPEG压缩管道，因此在第二节中介绍了不同类型的压缩机，包括不同的DCT转换实现以及不同的舍入函数。与嵌入方案的潜在使用相关联的压缩机产生的盖或台阶块。</li><li>steganalyst分析了产生的图像DCT块，并试图解决第三节中定义的一个反问题，以在像素域中找到测试块的前因。提出了两种寻找前因的方法。第一个使用一个启发式函数，它被最小化来找到兼容的块。第二个是求解整数线性规划（ILP）问题，该问题可以很容易地为一个特定的JPEG格式，并可以用于检测不兼容的块。从启发式中，可以设计一个定时攻击，它可以看作是[17]中提出的不同优化过程和不同JPEG压缩机的扩展。</li><li>时间攻击最终可以作为一个基于在图像中发现的兼容块或超时块的数量的假设检验。提出了不同的策略来选择块的子集，从而更有效，并与第六节的SOTA深度学习方法进行了评估和比较。</li></ul><p>​  请注意，要进行阶梯分析，Eve唯一需要拥有的材料是，在图2中用红色表示：</p><ul><li>JPEG压缩机的知识可以通过法医分析确定，或根据克尔克霍夫原理假设已知，</li><li>要分析测试图像的样本块，</li><li>另外，嵌入方案的知识可以用来使用SCA方法来选择最佳的候选块。</li></ul><p>​  本文与[17]有一些联系，提出了兼容性的主要特征及其与定时攻击的潜在联系。然而，也有许多不同，其中包括：快速启发式方法和用于提供结果在第四节和第六，似然比测试的设计部分vb，选择选择通道意识测试使用适当的之前，和与SOTA在深度学习阶梯分析部分六。与最初处理RJCA[7]的工作不同，所提出的方法在本质上不是统计学的，而是利用了JPEG格式的硬约束，因此不会出现假阳性。此外，我们还表明，所提出的似然比检验在第VI-B节中优于RJCA中使用的最佳深度学习检测器。<br/>​  我们所有的代码都可以在我们的git存储库中找到。（https://gitlab.cristal.univ-lille.fr/elevecqu/incompatible-jpeg-blocks）</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Image-based_Freeform_Handwriting_Authentication_with_Energy-oriented_Self-Supervised_Learning</title>
      <link href="/Image-based-Freeform-Handwriting-Authentication-with-Energy-oriented-Self-Supervised-Learning/"/>
      <url>/Image-based-Freeform-Handwriting-Authentication-with-Energy-oriented-Self-Supervised-Learning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Image Copy-Move Forgery Detection via Deep PatchMatch and Pairwise Ranking Learning</title>
      <link href="/Image-Copy-Move-Forgery-Detection-via-Deep-PatchMatch-and-Pairwise-Ranking-Learning/"/>
      <url>/Image-Copy-Move-Forgery-Detection-via-Deep-PatchMatch-and-Pairwise-Ranking-Learning/</url>
      
        <content type="html"><![CDATA[<p>Image Copy-Move Forgery Detection via Deep PatchMatch and PairwiseRanking Learning</p><p>Yuanman Li, <em>Senior Member, IEEE,</em> Yingjie He, <em>StudentMember, IEEE,</em> Changsheng Chen, <em>Senior Member, IEEE,</em> LiDong, Bin Li, <em>Senior Member, IEEE,</em> Jiantao Zhou, <em>SeniorMember, IEEE,</em> and Xia Li, <em>Member, IEEE</em></p><h1 id="摘要">摘要</h1><p>​  深度学习算法的最新进展在图像复制-移动伪造检测（CMFD）方面取得了令人印象深刻的进展。然而，这些算法在实际场景中不存在复制区域在训练图像中，或克隆区域是背景的一部分，缺乏通用性。此外，这些算法利用卷积运算来区分源区域和目标区域，当目标区域与背景混合良好时，结果不理想。为了解决这些局限性，本研究提出了一种新的端到端CMFD框架，它集成了传统的和深度学习方法的优势。具体来说，该研究开发了一种深度跨尺度补丁匹配（PM）方法，该方法为CMFD定制，以定位复制移动区域。与现有的深度模型不同，我们的方法利用从高分辨率尺度中提取的特征来寻找源区域和目标区域之间的显式和可靠的点-拓扑匹配。此外，我们提出了一种新的成对秩学习框架来分离源区域和目标区域。通过利用点-点匹配的强先验，该框架可以识别细微的差异，有效地区分源区域和目标区域，即使目标区域与背景很好地融合。我们的框架是完全可微的，可以通过端到训练。全面的实验结果突出了我们的方案在各种复制移动场景中的显著通用性，显著优于现有的方法。</p><h1 id="i.介绍">I.介绍</h1><p>​  数字图像编辑工具的普及使得图像伪造在我们的日常生活中越来越普遍。这些被操纵的图片可以在网络谣言、保险欺诈、假新闻传播、甚至学术欺诈等活动中被恶意利用，给社会带来重大的安全问题。复制-移动伪造是一种普遍的操作形式，涉及到复制和重新定位图像中的对象，以改变其内容。检测这种伪造品具有挑战性，因为伪造区域和未接触区域之间的统计特征相似，包括噪声分布、亮度和光度性属性。<br/>​  图像复制-移动伪造检测（CMFD）是多媒体安全领域的一个重要焦点。近年来，它见证了大量的研究努力，产生了各种被提出的方法。传统的CMFD技术利用手工制作的特性来识别复制-移动通信，包括基于块的方法[1]-[4]和基于关键点的方法[5]-[7]。虽然这些算法通过块或关键点匹配明确连接伪造的痕迹，提供了可信的结果，但它们依赖于为通用视觉任务定制的手工制作的特征，限制了它们对复杂的复制移动伪造和对后处理的敏感性的有效性。此外，这些传统的方法只检测对应关系，而不能区分源区域和目标区域。<br/>​  受深度特征强大的表征能力的推动，近年来，对深度CMFD框架的研究出现了激增。Wu等人[8]引入了具有源/目标分离的开创性的端到端CMFD框架。与传统的方法不同，[8]利用卷积神经网络（CNNs）自适应地从CMFD数据集学习特征，避免了手工设计的需要。这一创新刺激了后续的深度CMFD模型的发展，包括[9]-[11]，所有这些模型都利用了来自[8]的见解。有了这些具有代表性的特征，这些深度CMFD方法表现出更高的现实世界有效性和增强的对后处理的弹性。然而，尽管有这些优点，它们仍然表现出某些固有的限制，如下所述：</p><ul><li>大多数现有的深度复制-移动伪造检测（CMFD）方法通过使用来自深度cnn的高级特征生成注意力地图来识别潜在的复制-移动区域。然而，正如我们将在我们的实验中验证的那样，这些特征经常与训练图像中的物体过度拟合，导致模型的通用性显著下降。因此，在训练数据中缺少复制移动元素的情况下，这些方法的效果较差。此外，当在后台发生复制-移动操作时，它们可能会完全失败。</li><li>与传统的方法相比，深度CMFD技术很难建立复制-移动对应的显式点对点匹配。因此，其检测结果的可解释性和可靠性受到了损害。</li><li>它们只是对整个输入图像进行卷积运算来进行源/目标分离，不能有效地利用源和目标区域之间的强点对点关系。因此，当目标区域与背景良好地融合时，它们往往无法分离源/目标区域。</li></ul><p>​  为了应对上述挑战，我们提出了一个新的CMFD端到端CMFD框架，名为DeepPM和成对排名学习（D2PRL）。D2PRL结合了深度模型和传统技术的优势。首先，D2PRL专注于在高分辨率尺度上捕获源区域和目标区域之间的点对点匹配，增强其跨各种类型的复制移动内容的通用性。其次，D2PRL是完全可区分的，它可以通过端到端训练过程来学习特征。最后，D2PRL可以在强大的点匹配先验知识下有效地区分源/目标区域。我们的工作的主要贡献总结如下：</p><ul><li>我们引入了一个创新的端到端深度CMFD框架，以源/目标分离为特色，利用了传统和现代深度CMFD模型的优势。我们的方法显著优于现有的算法，并展示了对各种复制-移动内容的非常鲁棒的通用性，包括对象和背景。</li><li>我们设计了一种基于为CMFD定制的深度跨尺度PM的细粒度相似度定位方法。此外，还开发了跨尺度匹配和多尺度密集拟合误差估计等技术，以寻求高分辨率尺度下源区域和目标区域之间可靠的点对点匹配。</li><li>通过利用点对点匹配的强先验，我们提出将区分源区域和目标区域的原始问题转换为成对排序问题。这种方法迫使网络发现微妙的线索来区分源区域和目标区域，即使目标区域与背景很好地融合。</li></ul><p>​  本文的其余部分的结构如下：第二节提供了相关工作的简要回顾，而第四节则详细阐述了我们的CMFD框架。第五节介绍了广泛的实验结果和消融研究，第六节总结了本文。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  在本节中，我们将简要回顾相关的工作，包括现有的CMFD方法和PM算法。</p><h2 id="a.图像复制移动伪造检测">A.图像复制移动伪造检测</h2><h2 id="b.-补丁匹配">B. 补丁匹配</h2><h1 id="iii.问题公式">III.问题公式</h1><p>​  给定一个真实的图像，复制-移动伪造涉及到复制图像的一小部分并将其粘贴到同一图像中的不同位置的过程。该技术通常用于隐藏感兴趣的对象或复制图像中的对象。CMFD的目标是识别图像中发生复制移动伪造行为的区域。用于复制的图像的区域称为源区域，被复制区域所覆盖的区域称为目标区域。<br/>​  根据CMFD是否区分源区域和目标区域，可以分为单通道CMFD和三通道CMFD。单通道CMFD可以看作是一个像素级的二值分类问题，其中每个像素被分类为属于背景或目标/源区域。另一方面，三通道CMFD建立在单通道CMFD的基础上，通过进一步区分源区域和目标区域，确定检测到的像素是来自源区域还是目标区域。根据惯例，我们定义了以下相关的面具</p><ul><li><spanclass="math inline">\(M^{gt}\)</span>：地真单通道掩模，其中背景区域用0表示，而源区域和目标区域的像素都被设置为1。</li><li><spanclass="math inline">\(M\)</span>：预测的单通道掩码，其中每个值反映了像素属于目标/源区域的概率。</li><li><spanclass="math inline">\(M^{gt}_c\)</span>：地面真实三通道掩码，其中背景区域用[00 1]（蓝色）表示，源区域用[0 1 0]表示（绿色），目标区域用[1 00]（红色）表示。</li><li><span class="math inline">\(M_c\)</span>：预测的三通道掩码。</li><li><spanclass="math inline">\(M^{gt}_t\)</span>：目标区域的地面真值掩码，其中目标区域中的像素为1，其他像素为0。</li><li><span class="math inline">\(M_t\)</span>：目标区域的预测掩模。</li></ul><h1 id="iv.是我们所提出的算法的框架">IV.是我们所提出的算法的框架。</h1><p>​  在本节中，我们将讨论我们提出的通过深度PM和成对排序学习的CMFD方法。</p><h2 id="a.-d2prl概述">A. D2PRL概述</h2><p>​  如图1所示，我们的方法的框架有两个主要分支：</p><figure><imgsrc="../postimages/Image-Copy-Move-Forgery-Detection-via-Deep-PatchMatch-and-Pairwise-Ranking-Learning/image-20250310173223082.png"alt="image-20250310173223082" /><figcaption aria-hidden="true">image-20250310173223082</figcaption></figure><ul><li><p>通过深度交叉尺度PM进行的密集场匹配(DFM，Dense-Field Matching viaDeep Cross-Scale PM)：</p><p>​  该分支的目的是通过密集场匹配来定位复制移动区域。通过采用设计的深度跨尺度PM算法，我们的方法利用高分辨率尺度的特征，寻求源区域和目标区域之间明确和可靠的点对点匹配。与以往的深度CMFD方法相比，我们的检测框架对各种复制移动内容具有很高的通用性，包括对象、不完整对象和背景。</p></li><li><p>通过成对排名学习而产生的来源/目标歧视(STD，Source/TargetDiscrimination via Pairwise Ranking Learning)：</p><p>​  该分支利用密集场匹配的强先验知识来区分源区域和目标区域。在匹配信息的基础上，将原始识别问题转换成两两排序问题，这使得网络即使目标区域与背景很吻合，网络也能揭示识别源区域和目标区域的细微线索。</p></li></ul><h2id="b.-通过深度交叉尺度pm进行的密集场匹配dfmdense-field-matching-via-deep-cross-scale-pm">B.通过深度交叉尺度PM进行的密集场匹配(DFM，Dense-Field Matching via DeepCross-Scale PM)</h2><p>​  对点关系是图像复制移动伪造检测和定位的关键线索，在传统的CMFD算法中进行可靠决策方面得到了广泛的研究。然而，由于巨大的样本空间，传统方法的匹配过程要么不可微，要么高度复杂，排除了它们在现代深度学习框架中的使用。因此，大多数现有的深度CMFD算法[8]、[10]、[11]基于特征点之间的成对相关性计算注意力图，以呈现复制移动补丁的可能性，只使用来自非常小的特征图的高级特征。正如我们的实验将证明的那样，这可能使它们容易对训练图像中的物体进行过拟合。<br/>​  与以往的工作不同，我们的DFM分支通过使用高分辨率特征图的点对点匹配来识别复制移动区域，有效地结合了深度模型和传统模型的优点。如图1的上图所示，DFM分支由三个方块组成，即1)特征提取、2)交叉尺度匹配和3)误差拟合和预测。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DiRLoc:Disentanglement Representation Learning for Robust Image Forgery Localization</title>
      <link href="/DiRLoc/"/>
      <url>/DiRLoc/</url>
      
        <content type="html"><![CDATA[<p>DiRLoc: Disentanglement Representation Learning for Robust ImageForgery Localization</p><p>Ziqi Sheng, Zuomin Qu, Wei Lu, Member, IEEE, Xiaochun Cao, SeniorMember, IEEE, Jiwu Huang, Fellow, IEEE</p><p>中山大学</p><h1 id="摘要">摘要</h1><p>​  深度学习图像伪造定位方法取得了显著的效果，但当伪造图像被JPEG压缩时，无法保持可比的性能，JPEG是一种广泛用于日常信息传输的格式。对JPEG压缩的鲁棒性已成为图像伪造定位实际应用的瓶颈。为了解决这个问题，针对JPEG压缩导致的性能下降，提出了一种鲁棒的图像伪造定位框架。具体而言，提出了一种先进的渐进式解纠缠策略，该策略结合了粗粒度图像解纠缠来减轻通用JPEG压缩的不利影响，同时利用细粒度元素解纠缠的能力来分离多尺度伪影，从而最大限度地减少内容信息的干扰。此外，决策策略经过精心设计，以强化来自篡改区域的微妙信号，包括伪影融合块推理多尺度伪影和双注意力块，以了解更多与伪造相关的特征。大量的可视化和实验表明，我们的方法在一般的JPEG防伪图像定位中可以实现具有竞争力的性能，特别是在泛化实验的性能方面。</p><h1 id="i.介绍">I.介绍</h1><p>​  近年来，对抗jpeg压缩图像伪造定位的研究越来越多。Kwon等人[10]采用了RGB和DCT域的压缩伪影的法医特征的联合学习。Zhuang等人[11]采用修复策略重建锻造的痕迹。然而，大多数图像伪造定位方法通常将图像伪造定位任务建模为一个分类问题，并对伪造图像的每个像素进行二值分类。这导致人们更倾向于更多地关注内容和语义信息，而不是伪造图像[12]中的人工痕迹。内容偏差的干扰会导致大多数图像伪造定位方法的有限泛化。<br/>​  在本文中，我们提出了一个原始的框架，引入了鲁棒图像伪造定位（DiRLoc）任务的解纠缠表示学习。抗JPEG的图像伪造定位任务存在两个重要的挑战：(i)伪造伪在JPEG压缩后会消失，这增加了JPEG压缩[13]的图像伪造定位的难度。（ii）训练数据集中的内容语义和工件相互耦合，从而使检测器更倾向于将伪造定位任务视为分类任务，导致[14]泛化性较差。为了应对这些挑战，我们提出了一种渐进式解纠缠策略，该策略同时包含了粗粒度图像解纠缠和细粒度元素解纠缠。<br/>​  具体来说，粗粒度图像解纠缠相位将伪造图像与JPEG压缩的轨迹解耦，以消除JPEG压缩对伪造图像的影响。随后，在解耦的伪造图像中，细粒度元素解纠缠阶段将伪元素与内容语义元素解耦，获得与内容无关的伪元素，这对后续的伪造本地化任务有很大的贡献。为了减少内容语义的干扰，我们精心设计了元素独立性损失，以确保内容元素和工件元素保持不同。同时，解耦的伪影元素应该充分代表伪造图像中的被篡改区域，从而设计了图像重建和特征重建损失，以确保伪影元素的完整性。因此，通过设计元件解纠缠阶段的目标损失函数，仔细考虑了解纠缠元件的完整性和独立性。通过解纠缠策略学习相对纯的人工元素，进一步减少了JPEG压缩和内容偏差对伪造线索的影响。总之，这两个阶段是相互关联的，并可以逐步解耦伪造的JPEG图像，以产生语义不可知的伪影。图2直接显示了jpeg处理图像的图像和元素解纠缠的优势。</p><figure><img src="../postimages/DiRLoc/image-20250325170809572.png"alt="image-20250325170809572" /><figcaption aria-hidden="true">image-20250325170809572</figcaption></figure><p>​  当不存在分离策略时，模型会被JPEG跟踪和内容偏差所破坏，使其容易错误定位或忽略被篡改的区域。然后，在加入图像解纠缠模型后，进一步提高了被篡改区域的精度。最后，在采用DiRLoc模型的整体解纠缠策略后，该模型能够得到准确的预测结果。这也充分解释了图像解纠缠相位和元素解纠缠相位的意义。<br/>​  值得注意的是，我们从多层中学习了工件元素，因为较深的特征包含更多的结构信息，而较浅的特征更关注纹理信息。在决策策略中，我们巧妙地设计了伪影融合块（AFB），有效地融合多尺度伪影，并加入双注意块来挖掘伪影特征的通道和位置信息，旨在探索更多与伪造相关的特征。最后，通过决策策略增强了细微的伪造特征，进一步提高了定位性能。通过采用渐进式解纠缠策略，我们确保了DirLoc在一般的抗jpeg图像伪造定位任务和跨数据集泛化实验中优于最先进的方法。例如，在jpeg压缩的事实上的[16]数据集上，DirLoc的F1分数平均优于次优方法IF-OSN[15]5%。同时，DiRLoc在所有跨数据集泛化实验中都取得了良好的性能。这项工作的贡献可以总结如下：</p><ul><li>本文提出了一种创新的针对JPEG压缩进行鲁棒图像伪造定位的框架DiRLoc，该框架包含了一种渐进式解纠缠策略和一种决策策略。</li><li>渐进式解纠缠策略以从粗到细的方式将内容不可知的伪影与伪造的JPEG图像解耦，有效地消除了JPEG压缩的影响和内容语义信息的干扰。</li><li>提出了伪影融合块（AFB）来融合多尺度的伪影，并利用双注意层来学习更多与伪造相关的特征。</li><li>在公共数据集上进行的大量实验表明，DiRLoc的性能优于最先进的方法，特别是在泛化比较的性能方面。</li></ul><h1 id="ii.相关工作">II.相关工作</h1><h2 id="a.图像伪造检测">A.图像伪造检测</h2><h2 id="b.-解纠缠表示法学习">B. 解纠缠表示法学习</h2><p>​  解纠缠表示学习的目的是将复杂的高维耦合信息分解为具有高[30]识别能力的简单特征，使语义无关的篡改伪影容易获得。在现实世界中，获取可靠的工件是一项关键但具有挑战性的任务，而在训练数据集中，伪造操作与特定语义意义之间的过拟合可能会显著阻碍这种努力。各种深度学习方法都致力于通过不同的方法来寻找语义上不相关的工件。由于图像在RGB域中具有丰富的语义特征，一些方法试图从语义无关的域中寻找被篡改的图像伪影，如噪声域或频域[14]。例如，Wang等人[31]促进了高频特征，并将其与多模态补丁嵌入中的RGB特征相结合，以检测和定位图像操作。然而，噪声域或频域中的图像信息可以通过后处理轻松改变。然而，噪声域或频域中的图像信息可以通过后处理轻松改变。<br/>​  为了减少语义信息的干扰，一些方法探索了使用解纠缠表示学习来解耦内容语义。Liang等人[12]提出了一种解纠缠框架来去除人脸伪造检测中的内容信息。Kim等[32]根据相机特征分离身份特征和特征，更加关注身份信息，实现视频人的重新识别。Fu等人[33]引入了一种多层次的特征解纠缠网络，将合成的人脸特征解纠缠为现实特征和伪特征。Liu等人[34]设计了一个对抗性学习框架，将恶搞痕迹分解为模式的分层组合。Li等人[35]提出了一个深度伪检测框架来从深度伪视频中分离伪影。为了确保对语义独立的工件的访问，设计了一种渐进解纠缠策略，包括粗粒度图像解纠缠组件和细粒度元素解纠缠组件。这种新策略消除了伪造图像中内容语义信息的干扰，增强了对JPEG压缩的泛化能力。对渐进式分离策略的详细分析可以在SecIII-B中找到。</p><h1 id="iii.方法论">III.方法论</h1><h2 id="a.-概述">A. 概述</h2><p>​  如上所述，一种典型的图像伪造定位方法在暴露于特定的后处理操作时，可能难以保持其鲁棒性。JPEG压缩是一种广泛应用于日常信息传递和社交媒体传输中的后处理操作。为了满足实际应用的需要，DiRLoc专注于解决抗jpeg的压缩伪造定位问题。<br/>​  我们将源伪造图像表示为<spanclass="math inline">\(I_0\)</span>，<spanclass="math inline">\({I}_{0}\in{\mathcal R}^{H\times W\timesC}\)</span>。JPEG压缩过程被表述为JPEG(·）。伪造的JPEG图像记为<spanclass="math inline">\(I_j\)</span>，其中<spanclass="math inline">\(I_j=JPEG(I_0)\)</span>，<spanclass="math inline">\({I}_{j}\in{\mathcal R}^{H\times W\timesC}\)</span>。<span class="math inline">\(I_0\)</span>和<spanclass="math inline">\(I_j\)</span>的伪造区域像素级标签表示为<spanclass="math inline">\(m\in\{0,1\}^{H\times W\timesC}\)</span>（伪造/真实）。H、W和C分别是图像的高度、宽度和通道。对于<spanclass="math inline">\(I_0\)</span>和<spanclass="math inline">\(I_j\)</span>，通道设置为3，而标签m的通道设置为1。最后，对于任意给定的输入<spanclass="math inline">\(I_j\)</span>，DiRLoc输出像素级预测结果<spanclass="math inline">\(\hatm\)</span>。<br/>​  DiRLoc的总体框架如图3所示。</p><figure><img src="../postimages/DiRLoc/image-20250325195228354.png"alt="image-20250325195228354" /><figcaption aria-hidden="true">image-20250325195228354</figcaption></figure><p>​  原始的框架包含两个重要的组成部分：渐进式解纠缠策略和决策策略。将粗到细的渐进分离策略分为图像分离阶段和元素分离阶段。在图像解纠缠阶段，对于JPEG压缩的篡改图像IJ，我们将其输入到JPEG编码器EJ中，得到I1，确保I1尽可能接近原始图像I0，以抵消JPEG压缩的影响。在图像解纠缠阶段，I0和I1分别使用伪影编码器EA和内容编码器EC来获取相应的伪影元素和内容元素。通过仔细的设计约束，使解耦的工件能够尽可能地独立于内容语义。在决策策略中，通过对解耦伪影的推理和融合，得到最终的像素级定位性能。接下来，我们将在SecIII-B和SecIII-C中分别介绍渐进式解纠缠策略和决策策略。</p><h2 id="b.渐进的分离策略">B.渐进的分离策略</h2><p>​  如图4所示，提出了一种创新的渐进解纠缠策略，以解决针对JPEG压缩的鲁棒图像伪造定位的两个瓶颈。</p><figure><img src="../postimages/DiRLoc/image-20250325195428251.png"alt="image-20250325195428251" /><figcaption aria-hidden="true">image-20250325195428251</figcaption></figure><p>​  一方面，JPEG压缩引入了复杂的退化痕迹，如阻塞轨迹、振铃效应和模糊效应[7]，[8]，这可能会干扰图像伪造伪影。因此，伪造的JPEG图像既包含JPEG痕迹，又包含图像伪造伪影，使得伪造区域的准确定位更加困难。另一方面，泛化性差是图像伪造定位方法中普遍存在的问题。在跨数据集实验中，保持良好的性能更加复杂，因为大多数方法都难以消除特定数据集中的内容语义偏差。<br/>​  为了应对上述挑战，DiRLoc构建了全面的解决方案来解决这些问题。为了解决第一个问题，我们仔细开发了JPEG编码器EJ（·）来执行粗粒度的图像解纠缠和识别JPEG压缩轨迹。JPEG编码器EJ（·）的目标之一是识别并删除JPEG压缩的痕迹。另一个目标是重建伪造伪影A。对于伪造的JPEG图像Ij，图像解纠缠相位可以表示为<spanclass="math inline">\(I_{1}=E_{J}(I_{j})\)</span>。直观地说，解耦图像I1中的伪影a1和内容c1是源图像I0中的伪影a0内容c0的一个子集，其中<spanclass="math inline">\(I_{0}\in\Omega\)</span>和<spanclass="math inline">\(I_{1}\in\tilde{\Omega}\)</span>。为了进一步消除JPEG压缩对伪造图像的负面影响，在细粒度元素解纠缠相位的影响下，应尽可能地减小集合（a1，a0）和（c1，c0）内部的差异。<br/>​  为了提高模型的泛化性，细粒度元素解纠缠阶段通过减少特定数据集的内容干扰来提高模型的泛化性。该阶段主要由两个独立的编码器<spanclass="math inline">\(E_C\)</span>和<spanclass="math inline">\(E_A\)</span>组成，分别用于提取内容和人工元素。该编码器可以消除数据集的内容偏差，从而可以更多地关注伪造伪影。在第III-B1中描述了图像解纠缠阶段的技术细节，在第III-B2中详细阐述了元素解纠缠相位。<br/>​  1)图像解纠缠：<br/>​  图像质量的下降是JPEG压缩过程中的一个众所周知的影响。当源伪造图像I0被JPEG压缩压缩时，I0中包含的伪造伪影被损坏。图像伪造定位任务在JPEG压缩条件下保持鲁棒性能是一个挑战。受许多先前的JPEG跟踪解纠缠工作[36]-[38]的启发，我们提出了一种JPEG编码器EJ来执行粗粒度的图像解纠缠，这可以消除JPEG压缩的干扰。</p><figure><img src="../postimages/DiRLoc/image-20250325200307276.png"alt="image-20250325200307276" /><figcaption aria-hidden="true">image-20250325200307276</figcaption></figure><p>​  如图5所示，JPEG编码器EJ采用了ADN[39]提出的编码器结构，专门用于分离CT图像中的伪影。JPEG编码器EJ的目标是解开JPEG压缩轨迹，并尽量减少解耦图像I1与其源图像I0之间的差异。换句话说，通过消除I1中的JPEG痕迹，I1中的伪造伪影被迫与I0中的伪造伪影保持一致。为此，我们引入一个像素损失来测量解耦图像I1和相应的源图像I0之间的距离。图像解纠缠损失Ldis给出如下：<span class="math display">\[{\mathcal{L}}_{d i s}=\frac{1}{H\timesW}||I_{1}-I_{0}||_{1},\]</span>​  其中，H×W为i0中的像素数。我们使用L1损失而不是L2损失来鼓励更尖锐的输出。通过强制I1尽可能接近I0，可以尽可能多地重建伪造对象，同时消除在I1时JPEG压缩的影响。<br/>​  2)元素解纠缠：<br/>​  我们假设图像的高维潜在表示由内容和伪影元素组成。元素解纠缠网络的主要目的是将复合元素分解为伪影元素和内容元素。然后，将相对纯的伪影元素用于后续的定位任务，而不分散内容语义信息。元素解纠缠阶段主要由两个独立的编码器EC和EA组成，分别用于提取内容和伪影元素，以及一个用于图像重建的解码器D。EA的结构如图6所示。</p><figure><img src="../postimages/DiRLoc/image-20250325200548946.png"alt="image-20250325200548946" /><figcaption aria-hidden="true">image-20250325200548946</figcaption></figure><p>​  编码器EA输出多尺度伪造伪影：高尺度伪影、中型伪影和低尺度伪影[39]。高尺度的伪影包括尖锐的图案和高频结构。中尺度的伪影指的是图像的纹理和边缘。低尺度的伪影包括像素信息，如构造笔画和镜面高光。内容编码器EC与伪影编码器EA具有相同的结构，但参数不共享，只保留最后一层的输出。如图3所示，将一对输入图像（I0、I1）输入编码器EC和EA，输出（a0、c0）和（a1、c1），分别表示对应的伪影和内容元素。</p><figure><img src="../postimages/DiRLoc/image-20250325202646193.png"alt="image-20250325202646193" /><figcaption aria-hidden="true">image-20250325202646193</figcaption></figure><p>​  该过程的计算公式如下： <span class="math display">\[\begin{array}{lc r}{c_{0}=E_{C}(I_{0}),c_{1}=E_{C}(I_{1}),}\\{a_{0}=E_{A}(I_{0}),a_{1}=E_{A}(I_{1}).}\end{array}\]</span>​  图7显示了直观比较两种不同特征的热力图。</p><figure><img src="../postimages/DiRLoc/image-20250325200935961.png"alt="image-20250325200935961" /><figcaption aria-hidden="true">image-20250325200935961</figcaption></figure><p>​  图7中的第三行和第四行分别为伪影元素和内容元素，伪影元素与第二行中显示的groundTruth掩模非常相似。这说明伪影元素更侧重于被篡改的区域，而内容特征更侧重于图像的语义背景。也就是说，元素解纠缠相位可以成功地聚焦于伪造图像的被篡改区域，减少内容偏差信息的影响。因此，DiRLoc比其他伪造图像定位方法具有更大的泛化性，并在SecIV-C中得到了充分的验证。为元素解纠缠相位精心设计了几个关键的损失函数。接下来，我们将详细讨论它们。<br/>​  a)图像重建损失和特征重建损失：<br/>​  解耦的伪影元素必须完全表示伪造图像中的被篡改区域。因此，我们设计了图像和特征重建损失，以确保伪影元素的完整性。对于图像重建，首先将元素加法应用于同一图像编码的内容和伪影元素，得到图像的高维潜在表示特征，即<spanclass="math inline">\(g_{c_{k}}^{a_{k}}=c_{k}+a_{k}\)</span>，其中k =0,1。然后，将<spanclass="math inline">\(g_{c_{k}}^{a_{k}}\)</span>输入到解码器D中，重建相应的原始图像<spanclass="math inline">\(I_{c_{k}}^{a_{k}}\)</span>。该过程的计算公式如下：<spanclass="math display">\[I_{c_{k}}^{a_{k}}=D(g_{c_{k}}^{a_{k}}),\]</span>​  其中，k∈{0,1}。图像重建损失保证了重建图像与原始图像在像素水平上的一致性。因此，将图像重建损失定义为：<span class="math display">\[{\mathcal{L}}_{r ec{1}}=\|I_{c_{0}}^{a_o}-I_{0}\|_{1}+\|I_{c_{1}}^{a_{1}}-I_{1}\|_{1}.\]</span>​  对于特征重建，我们首先将来自不同图像的内容元素和伪影元素交叉组合，得到高维的潜在表示特征，即<spanclass="math inline">\(g_{c_{k}}^{a_{1-k}}=c_{k}+a_{1-k}\)</span>，其中k∈{0,1}。此外，将<spanclass="math inline">\(g_{c_{k}}^{a_{1-k}}\)</span>输入到解码器D中，以重建图像<spanclass="math inline">\(I_{c_{k}}^{a_{1-k}}\)</span>。该过程的计算公式如下：<spanclass="math display">\[I_{c_{k}}^{a_{1-k}}=D(g_{c_{k}}^{a_{1-k}}),\]</span>​  其中，k∈{0,1}。重建图像的编码元素应与原始元素一致，元素重建损失定义如下：<span class="math display">\[\mathcal{L}_{r ec2}=\sum_{k=0}^{1}(||E_{C}(I_{c_{k}}^{a_{k}})-c_{k}||_{1}+||E_{A}(I_{c_{k}}^{a_{k}})-a_{k}||_{1}+||E_{C}(I_{c_{k}}^{a_{1-k}})-c_{k}||_{1}+||E_{A}(I_{c_{k}}^{a_{1-k}})-a_{1-k}||_{1}).\]</span>​  b)元素独立性损失：<br/>​  如上所述，伪影元素和内容元素包含在两个独占域中。伪影元素和内容元素可以被视为不同的类，并且类间的距离预计将显著大于工件元素类的类内距离。换句话说，我们期望伪影元素在内部更加聚合，并且离内容元素更远。具体来说，我们将类内伪影元素视为正对，将类间元素视为负对。然后，采用对比学习策略，进一步消除了伪影元素和内容元素可能存在的重叠。受[40]的启发，我们使用内容元素和伪影元素的Gram矩阵作为一个清晰的表示：<span class="math display">\[G_{f}=(f_{i}^{T}f_{i})_{n\timesn}=\begin{bmatrix}f_{1}^{T}f_{1} &amp; \cdots &amp;f_{1}^{T}f_{n}\\\vdots &amp; \ddots &amp; \vdots\\f_{n}^{T}f_{1} &amp;\cdots &amp; f_{n}^{T}f_{n}\end{bmatrix}\]</span>​  其中，f表示需要计算Gram矩阵的特征，fi表示特征f的列向量，n表示特征f的行数。我们采用余弦距离来测量元素距离，其中较近的元素呈现更大的分数。最后，我们采用InfoNCE[41]来构建伪影和内容元素之间的元素独立性损失： <spanclass="math display">\[\mathcal{L}_{i n d}=-l og[\frac{\mathcal{E}(d(G_{a_{0}},G_{a_{1}}))}{\mathcal{E}(d(G_{a_{0}},G_{a_{1}}))+\sum_{i=0}^{1}\mathcal{E}(d(G_{a_{i}},G_{c_{1-i}}))},\]</span>​  其中，Gai和Gci分别表示ai和ci的gram矩阵的平坦向量，E表示exp（·），d（·）表示余弦相似度。<br/>​  综上所述，伪影解纠缠网络是通过解耦内容偏差信息和学习多尺度伪造伪影来消除伪造图像中内容信息的负面影响。这些多尺度伪影对后续的图像伪造定位任务至关重要。</p><h2 id="c.-决策策略">C. 决策策略</h2><p>​  基于学习到的伪造伪影，提出了一种增强jpeg压缩图像中伪造特征的决策策略。该决策策略的体系结构如图3的底部所示。</p><figure><img src="../postimages/DiRLoc/image-20250325202709618.png"alt="image-20250325202709618" /><figcaption aria-hidden="true">image-20250325202709618</figcaption></figure><p>​  该模块以解耦的图像I1和多尺度伪影作为输入，旨在尽可能准确地预测相应的伪造区域掩模<spanclass="math inline">\(\hat m\)</span>。<br/>​  我们使用4个ResBlocks[42]作为骨干，从I1中提取基本特征f1、f2、f3和f4，以增强伪造操作造成的伪造特征。此外，我们还引入了伪影融合块（AFB）来融合多尺度伪影，以提高对JPEG压缩的定位精度。伪影融合块的结构如图8所示。</p><figure><img src="../postimages/DiRLoc/image-20250325202837093.png"alt="image-20250325202837093" /><figcaption aria-hidden="true">image-20250325202837093</figcaption></figure><p>​  具体地说，在渐进解纠缠策略中，解耦的图像I1被解耦为三个不同尺度的伪影。然后将这些伪影分别输入到AFB中，以产生混合良好的伪影特征a。随后，将基本特征f4和伪影特征a连接并输入双注意块，得到伪造图像的最终预测掩模<spanclass="math inline">\(\hat m\)</span>，表示如下： <spanclass="math display">\[\hat{m}=S i g m o i d(\mathrm{D}(r)),\]</span>​  其中<span class="math inline">\(r=c o n c at(f_{4},a)\)</span>，Sigmoid为激活函数，D（·）表示双注意块。<br/>​  受[43]方法的启发，提出了双注意块来进一步增强脆弱的伪造特征，其中两种注意机制并行工作。</p><figure><img src="../postimages/DiRLoc/image-20250325203156062.png"alt="image-20250325203156062" /><figcaption aria-hidden="true">image-20250325203156062</figcaption></figure><p>​  如图9所示，双注意块包括两个主要部分：通道注意部分和位置注意部分。通道注意部分集中于通道内特征的相关性。然后，位置注意部分通过所有位置的特征的加权和有选择地更新每个位置的特征。将这两个注意部分的输出汇总并转换为大小为W×H×1的特征图。<br/>​  伪造像素的数量远少于被篡改图像中真实像素的数量。我们使用了Dice损失函数，该函数对从极不平衡的数据[44]中学习很有效：<span class="math display">\[\mathcal{L}_{m a sk}=1-\frac{2\times\sum_{i=1}^{M}\sum_{i=1}^{M}\hat{m}_{i j}m_{ij}}{\sum_{i=1}^{M}\sum_{i=1}^{H}\hat{m}_{ij}^{2}+\sum_{i=1}^{M}\sum_{i=1}^{H}m_{i j}^{2}},\]</span>​  其中，W和H为伪造图像的空间分辨率。<spanclass="math inline">\(m_{ij}\in\{0,1\}\)</span>是一个二进制标签，表示（i，j）像素是否被篡改，而<spanclass="math inline">\(\hat m_{i j}\)</span>是预测的掩码<spanclass="math inline">\(\hatm\)</span>的像素值。在本文中，0表示为篡改，1表示为真实。</p><h2 id="d.-总损失">D. 总损失</h2><p>​  本文使用了五种损失的组合，表示用等式(1)，等式(4)，等式(6)，等式(8)和等式(10)。总损失记为：<span class="math display">\[L o s s=\lambda_{1}{\cal L}_{d is}+\lambda_{2}{\cal L}_{i n d}+\lambda_{3}{\cal L}_{r e c1}\]</span>​  其中，λ1、λ2、λ3、λ4和λ5均为超参数。</p><h1 id="iv.实验">IV.实验</h1><p>​  在本节中，我们进行了广泛的实验来评估所提方法的性能。首先，我们在第IV-A中介绍了训练/测试数据集和实验细节。然后，在IV-B，定量和定性实验旨在彻底研究DiRLoc与其他最先进的方法相比的性能。在IV-C，交叉数据集实验评估DiRLoc在Sec的泛化。最后，在IV-D，我们测量了在所提出的模型中涉及的每个组件和损失函数的影响。</p><h2 id="a.实验装置">A.实验装置</h2><p>​  1)数据集：<br/>​  我们在实验中使用的数据集汇总在表二中。</p><figure><img src="../postimages/DiRLoc/image-20250325203955500.png"alt="image-20250325203955500" /><figcaption aria-hidden="true">image-20250325203955500</figcaption></figure><p>​  为了与最先进的方法进行合理的比较，我们选择了5个公共数据集：DEFACTO[16]、CASIAv2[45]、NIST16 [46]、IMD2020 [47]和CASIAv1[45]。在训练集中，伪造图像中被篡改区域的比例越大，对伪造伪影的学习效果就越好。因此，我们对DEFACTO[16]、NIST16[46]、IMD2020 [47]和CASIAv1[45]采用了伪造率选择策略，只剩下伪造率为[0.2,0.8]的图像。DEFACTO[16]是一个具有挑战性的数据集，它包含了所有四种篡改技术（复制-移动、拼接、删除和修改绘画）。CASIAv1[45]和CASIAv2[45]不包含地面真实掩码。我们通过对伪造图像和真实图像之间的阈值差异来计算相应的GT掩模。NIST16[46]主要包含几何伪造图像。IMD2020[47]包含真实的操纵图像。对于这些不同大小的数据集，我们不使用调整大小操作来统一这些数据集的大小，因为直接减少高分辨率的图像会对图像细节造成重大损害，甚至对伪造痕迹的损失。因此，在预处理阶段，训练和测试数据集中的所有图像都被裁剪到256×256×3。此外，训练和测试数据集是不相交的集。<br/>​  2)基线网络：<br/>​  我们选择了几种先进的方法来与DiRLoc进行比较。这些高级方法可以分为两种类型：一种用于伪造的JPEG图像任务，另一种用于常见的伪造图像任务。第一种类型包括IF-OSN[15]、CAN-DAS [22]、ReLoc [11]和CAT-Net[10]。他们在训练阶段引入JPEG图像来解决JPEG压缩问题。IF-OSN[15]研究了在线社交传输对图像伪造定位的整体影响，我们只比较了其对抗JPEG压缩的能力，而该压缩是在线社交传输的重要组成部分。Rao等人[22]提出的比较方法的主要部分是压缩近似网络和域自适应策略（CAN-DAS[22]）。我们在上述公共数据集上使用授权提供的代码来训练CAN-DAS[22]。对于ReLoc [11]和CAT-Net[10]，我们使用它们的开放源代码。第二种类型，包括MVSS-Net [1]和ManTra-Net[21]，只在训练阶段使用初始伪造图像。为了进行公平的比较，我们在训练阶段直接使用伪造的JPEG图像而不是原始图像来重新训练MVSS-Net[1]和ManTra-Net[21]。<br/>​  3)训练设置：<br/>​  该方法DiRLoc采用PyTorch框架实现，所有实验均在NVIDIAGeForce RTX 3090上进行。使用随机QFs（QF =50、60、70、80、90、100）通过MATLABAPI功能从初始伪造图像I0将伪造的JPEG图像Ij压缩。我们选择QF的范围从50到100，因为来自真实社交媒体的图片通常都在这个范围内，并且这个设置被其他比较方法[22]使用。整个框架使用Adam优化器进行端到端训练，将小批量大小设置为10。我们采用F1分数和MCC作为性能指标。F1得分是精确度和查全率的调和平均值，范围为0~1,1表示完美的精确度和查全率。MCC在处理不平衡样本时特别有用，因为正样本和负样本的数量不相等。初始学习速率设置为10−5，每5个周期减少10%，在训练阶段，权重衰减固定为10−5。等式中的超参数λ1、λ2、λ3、λ4和λ5（11）分别设置为1、0.2、0.01、1和1。</p><h2 id="b.-与最先进的方法进行比较">B. 与最先进的方法进行比较</h2><p>​  为了全面评估我们的方法的优越性，我们比较了DiRLoc与最先进的方法：IF-OSN [15]，CAN-DAS [22]，ReLoc [11]，CAT-Net [10]，MVSS-Net[1]，和ManTra-Net [21]。比较实验在四个数据集上进行：DEFACTO[16]、CASIAv2[45]、NIST16 [46]和IMD2020[47]。我们分别对JPEG压缩图像和未压缩图像进行了充分的实验。<br/>​  如表三所示，在具有固定阈值（0.5）的f1分数（F1）和在JPEG压缩图像中的Matthews相关系数（MCC）上，我们将我们的方法与各种最先进的方法进行了比较。</p><figure><img src="../postimages/DiRLoc/image-20250325211241749.png"alt="image-20250325211241749" /><figcaption aria-hidden="true">image-20250325211241749</figcaption></figure><p>​  具体来说，为了彻底评价所提方法的鲁棒性，这些方法在QF =50、55、60、65、70、80、90上进行了测试。每个数据集的最后一行显示了所有QF因素的平均性能。DiRLoc在所有数据集的不同QF因子下达到了前两个性能，特别是在数据集CASIAv2[45]上，它达到了平均F1 = 0.577，比第二优的方法CAT-Net[10]高出18%。在其他三个数据集上，即事实上的[16]、NIST16 [46]和IMD2020[47]，我们的方法的平均F1比第二优的方法高出5.2%、8.3%和0.7%。我们还在广泛的QFs上评估了我们的模型，以证明其对JPEG压缩的强鲁棒性。DiRLoc仍然优于其他基线，即使是在极端情况下，如QF= 50，在那里它的F1分数在事实上的[16]数据集中比第二优的方法IF-OSN[15]高出5.4%。IF-OSN[15]在事实上的[16]数据集上表现良好，但在其他三个数据集上的性能有所下降。我们假设，这可能是因为IF-OSN[15]通过比较压缩数据和未压缩数据之间的差异来建模JPEG压缩过程，在事实上的[16]数据集中有更多的训练样本，从而更容易学习差异。由于渐进式解纠缠策略，DiRLoc在较少的训练样本上取得了良好的性能。为了进一步比较泛化结果，我们用QF= 55和QF = 65设置了两个测试用例，它们没有出现在训练集中。CAT-Net[10]在CASIAv2 [45]、NIST16 [46]和IMD2020[47]数据集上取得了第二好的结果。然而，在QF = 55和QF =65的情况下，事实上的[16]数据集上的CAT-Net [10]的F1和MCC都低于QF = 50和QF=60，而DiRLoc受到非常小的干扰。<br/>​  虽然我们的问题设置集中在伪造的JPEG图像上，但DiRLoc在非JPEG压缩数据集上也获得了优越的性能。表四展示了DirLoc在非jpeg压缩图像上的定位性能。</p><figure><img src="../postimages/DiRLoc/image-20250325211447272.png"alt="image-20250325211447272" /><figcaption aria-hidden="true">image-20250325211447272</figcaption></figure><p>​  JPEG压缩可以被视为对数字图像的强干扰，从而导致阻塞伪影、振铃效应等。这些复杂的压缩轨迹与图像篡改伪影重叠。我们专门设计的DiRLoc经过训练，提取了更多的内在和基本特征，这些特征与篡改痕迹高度相关，从而减轻了复杂JPEG痕迹的干扰。然而，CAT-Net和MVSS-Net的性能在非jpeg压缩图像上表现出明显的下降。这主要是由于在训练数据集中缺少非jpeg压缩图像。因此，这些方法并没有学习到这些非jpeg压缩图像的特征，并且无法处理它们。该方法即使在非jpeg压缩数据集上进行训练，仍能获得良好的结果，很好地反映了其泛化性。<br/>​  此外，我们观察到，在NIST16[46]和IMD2020[47]数据集上，有几种方法在90到50的QFs范围内表现出稳定的性能。导致这种现象有两个原因。首先，在训练数据集和测试数据集中的JPEG压缩范围是一致的。在训练阶段，我们采用了一种数据增强策略，将来自集合{50、60、70、80、90、100}的不同质量因子（QF）值随机应用到训练数据集中的图像上。而在测试阶段，QF的范围与训练集保持一致。这意味着该模型已经被训练成对各种压缩伪影具有鲁棒性，从而在对使用不同QF值压缩的图像进行评估时获得相对稳定的性能。其次，我们假设IMD2020[47]和NIST16 [46]数据集比事实上的[16]和CASIAv2[45]数据集包含更少的测试图像。因此，在这些较小的测试集上，不同压缩级别之间的性能差异可能不那么明显，这可能导致测试模型获得更稳定的结果。<br/>​  最后，我们将定性定位结果与图10中所有测试数据集上的预测结果进行了比较，说明了我们可以更精确地定位伪造区域的情况。</p><figure><img src="../postimages/DiRLoc/image-20250327113623902.png"alt="image-20250327113623902" /><figcaption aria-hidden="true">image-20250327113623902</figcaption></figure><p>​  DiRLoc的预测结果显示在最后一列中。与其他最先进的方法相比，DiRLoc的预测结果与实际结果最接近，这也表明了该方法的优势。这些伪造的图像具有复杂的篡改边界、纹理、光和影效果等。这些锻造的区域与周围的场景融合在一起，即使是人眼也无法区分。大多数被比较的方法都失败了，但我们的方法仍然可以找到正确的区域。</p><h2 id="c.-泛化研究">C. 泛化研究</h2><h2 id="d.-消融研究">D. 消融研究</h2><h1 id="v.结论">V.结论</h1><p>​  本文从一个全新的角度研究了针对JPEG压缩的伪造图像定位问题。所提出的框架DiRLoc利用渐进式解纠缠策略和决策策略获得了一个稳定、准确的预测。渐进式解纠缠策略包括粗粒度图像解纠缠阶段和细粒度元素解纠缠阶段，分别专注于去除JPEG跟踪和学习内容不可知的伪影。最后，该决策策略对这些多尺度伪影进行了推理，以实现像素级预测。定量和定性实验结果表明，该方法DiRLoc在一般的抗jpeg压缩图像伪造定位任务中优于最先进的方法，特别是在泛化性能方面。本研究的主要局限性是，它主要针对的是JPEG压缩的常见图像处理方法，而没有解决在现实应用中遇到的更广泛的图像操作，如在社交网络中发现的那些。在未来，我们计划将这项工作扩展到处理更复杂和多样化的场景中的图像伪造定位任务，包括那些在社交媒体平台中普遍存在的任务。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JPEG压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IMDL-BenCo:A Comprehensive Benchmark and Codebase for Image Manipulation Detection &amp; Localization</title>
      <link href="/IMDL-BenCo/"/>
      <url>/IMDL-BenCo/</url>
      
        <content type="html"><![CDATA[<h1id="imdl-benco-a-comprehensive-benchmark-and-codebase-for-image-manipulation-detection-localization">IMDL-BenCo:A Comprehensive Benchmark and Codebase for Image Manipulation Detection&amp; Localization</h1><p>Xiaochen Ma1†, Xuekang Zhu1†, Lei Su1†, Bo Du1†, Zhuohang Jiang1†,Bingkui Tong1†,<br/>Zeyu Lei1,2†, Xinyu Yang1†, Chi-Man Pun2, JianchengLv1,3, Jizhe Zhou1,3∗</p><p>1中国四川大学计算机科学学院<br/>2澳门大学计算机与信息科学系<br/>3中国教育部机器学习与工业智能工程研究中心</p><p>(† 平等的贡献。∗ 通讯作者： Jizhe Zhou (jzzhou@scu.edu.cn))</p><h1 id="摘要">摘要</h1><p>​  在图像操作检测与定位（IMDL）领域，尚未建立一个全面的基准。没有这样一个基准，导致模型评价不足和具有误导性，严重破坏了这一领域的发展。然而，由于开源基线模型的缺乏以及不一致的培训和评估协议，使得在IMDL模型之间进行严格的实验和忠实的比较具有挑战性。为了解决这些挑战，我们引入了IMDL-BenCo，第一个全面的IMDL基准测试和模块化代码库。<br/>​  IMDL-BenCo:<strong>i)</strong>将IMDL框架分解为标准化的、可重用的组件，并修改模型构建管道，提高了编码效率和定制的灵活性；<strong>ii)</strong>全面实现或整合最先进的模型的培训代码，以建立一个全面的IMDL基准；<strong>iii)</strong>基于已建立的基准测试和代码库进行深入分析，为IMDL模型体系结构、数据集特征和评估标准提供了新的见解。具体来说，IMDL-BenCo包括通用的处理算法，8个最先进的IMDL模型（其中1个从头开始复制），2套标准的训练和评估协议，15个gpu加速评估指标，以及3种鲁棒性评估。这个基准和代码库代表了在校准IMDL领域的当前进展和鼓舞未来突破方面的重大突破。代码可从以下网址获得：https://github.com/scu-zjz/IMDLBenCo。</p><h1 id="介绍">1介绍</h1><p>​  “实验是科学真理的最终仲裁者。”-理查德·费曼。<br/>​  授权的图像操作或生成模型驱动图像操作检测和定位（IMDL）任务到信息取证和安全[24,36]的前沿。虽然该任务在文献中偶尔被称为“伪造检测”[15,13]或“篡改检测”[32,31]，但现在的共识倾向于将术语IMDL[13]作为这个研究领域最合适的描述符。IMDL内的“操作”范围会产生与原始内容[38]的语义差异。它不涉及纯生成的图像（例如，从纯文本生成的图像），也不涉及应用图像处理技术，引入噪声或其他非语义变化，而不改变图像[5]的基本意义。<br/>​  术语“检测和定位”表示了一个IMDL模型的双重责任：同时进行图像级和像素级的评估。这包括在图像级别上的二值分类，识别输入图像是被操纵的还是真实的，以及在像素级别上的分割任务，通过掩模描绘精确的操纵区域。简而言之，IMDL模型应识别语义上显著的图像改变，并提供两个结果：一个类标签和一个操作掩码。<br/>​  尽管深度神经网络在IMDL领域[10,45,11]中取得了快速的成功，但现有的模型存在不一致的训练和评估协议，由附录A.1中的表支持。这些不一致导致了不相容和不公平的比较，产生了不充分和具有误导性的实验结果。因此，建立一个统一和全面的基准是IMDL领域最受关注的问题。然而，构建这个基准测试远不仅仅是直接的协议统一或简单的模型再训练。首先，大多数最先进的（SoTA）作品的培训代码是不公开的，而且一些SoTA作品的源代码是完全未发布的[27]。其次，IMDL模型通常包含不同的低级特性[43,4,13]和复杂的损失函数，需要高度定制的模型架构和解耦的管道设计来实现有效的复制。现有的框架，如OpenMMLab2和Detectron23，严重依赖于注册表机制和紧密耦合的管道。这种冲突导致了在现有框架下复制IMDL模型时严重的效率问题，并导致了具有极高编码负载和低可伸缩性的单片模型体系结构。因此，一个全面的IMDL基准测试尚未建立起来。<br/>​  为了解决这个问题，我们引入了IMDL-BenCo，第一个全面的IMDL基准测试和模块化代码库。<br/>​  IMDL-BenCo：i)具有一个包含四个组件的模块化代码库：数据加载器、模型库、训练脚本和评估器；模型库包含可定制的模型体系结构，包括SoTA模型和主干模型。损失设计也独立于模型库，而其他组件则由接口进行标准化，并且具有高度的可重用性；这种方法减轻了模型定制和编码效率之间的冲突；ii)完全实现或整合8种SoTAIMDL模型的代码（见表1）和建立了一个具有2套标准的训练和评估方案的综合基准，15gpu加速评估指标，和3种鲁棒性评估；iii)基于建立基准和代码库进行深入分析，为IMDL模型架构、数据集特征和评估标准提供新的见解。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306170018904.png"alt="image-20250306170018904" /><figcaption aria-hidden="true">image-20250306170018904</figcaption></figure><p>​  这个基准测试和代码库代表了在校准IMDL领域的当前进展方面的一个重大飞跃，并可以激发未来的突破。</p><h1 id="相关工程">2相关工程</h1><p>​  <strong>IMDL模型架构</strong><br/>​  IMDL的关键是识别通过篡改创建的伪影。伪影被认为是表现在低级特征空间上的。因此，几乎所有现有的IMDL模型都采用了“主干网络+低级特征提取器”的范式。例如，SPAN[18]和ManTra-Net [43]使用VGG [34]作为其模型的主干，并结合SRM[49]和BayarConv [1]滤波器，以获得图像的低级特征。MVSS-Net[4]结合了提取边缘信息的Sobel [35]操作符和在其ResNet- 50[16]主干上的BayarConv来提取图像噪声。关于每个模型的详细信息见表1。各种低级特征提取器会导致各种损失函数和极其定制的模型架构。因此，在现有框架内复制IMDL模型是低效的。各种损失函数和训练架构之间的高耦合也使得扩展不同的模型训练框架变得极其困难。训练框架之间的差异进一步增加了模型再现的难度。这种紧密耦合也严重影响了算法的创新和快速迭代。</p><p>​  <strong>不一致的训练和评估方案</strong><br/>​  除了模型复制困难，到目前为止，存在多个显著不同的训练和评估IMDL模型的协议。MVSSNet、CAT-Net和TruFor在ImageNet[6]数据集上进行了预训练。SPAN [18]、PSCC-Net [25]、CAT-Net [22]和TruFor[22]使用合成数据集进行训练。此外，TruFor还使用了大量来自流行的照片分享网站Flickr和DPReview的原始图片来训练其Noiseprint++提取器。MVSS-Net[4]和IML-ViT [27]在CASIAv2 [8]数据集上进行训练。另一方面，NCL[47]没有使用预训练，而是在NIST16[12]数据集上进行了训练。这些模型的详细训练和评估方案见附录A.1。考虑到IMDL基准数据集都是小尺寸（几百到几千张图片）[47]，训练数据集的实质性差异使它不可避免地使用大型训练集或预训练模型将表现异常好在其他评估集，大大挑战模型性能评估的公平性。此外，如表1所示，大多数模型的代码并没有完全开源。他们的研究结果很难校准，并且可能对新的IMDL研究人员有高度的误导性。</p><p>​  <strong>现有的IMDL总结和基准</strong><br/>​  尽管IMDL总结[28,46]已经注意到IMDL研究中协议的不一致性和模型复制的困难，但很少有努力致力于解决这个问题。现有的调查往往依赖于具有独特的训练策略和数据集的独立设计的模型和数据集，导致报告结果的偏差。此外，据我们所知，目前还没有全面的基准来确保IMDL模型的公平和一致的评估。这种缺乏统一的基准测试导致了误导性的、不忠实的模型评估，并破坏了IMDL领域的总体进展。</p><h1 id="我们的代码库">3我们的代码库</h1><p>​  本节介绍了我们使用PyTorch4实现的模块化的、面向研究的和用户友好的代码库。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306170503847.png"alt="image-20250306170503847" /><figcaption aria-hidden="true">image-20250306170503847</figcaption></figure><p>​  如图1所示，它包括四个关键组件：数据加载器、模型库、训练脚本和评估器。我们的代码库在为IMDL任务提供标准化的工作流和为用户提供广泛的定制选项以满足他们的特定需求之间取得了平衡。</p><h2 id="数据加载程序">3.1数据加载程序</h2><p>​  数据加载器主要处理数据集的排列、增强和转换过程。</p><p>​  <strong>数据集管理</strong><br/>​  我们为每个数据集提供转换脚本，以将它们重新排列成一组JSON文件。后续的培训和评估可以根据这些JSON文件进行。</p><p>​  <strong>增强和转换</strong><br/>​  由于需要专家注释和大量的手工工作，IMDL数据集通常非常小，这使得它难以满足越来越大的模型的需求。因此，数据增强是必要的。此外，确保输入模式和图像形状满足现有模型的要求是至关重要的。我们的数据加载器设计了以下转换序列：<br/>​  1)IMDL特定的变换：受MVSSNet[4]的启发，我们实现了朴素的初始绘制和朴素的复制-移动转换，这可以在不需要额外数据集的情况下有效地提高性能。<br/>​  2)通用变换：这包括典型的视觉转换，如翻转、旋转和随机亮度调整，使用Albumentations[3]库实现。<br/>​  3)后处理变换：有些模型需要除RGB模式以外的其他信息。例如，CAT-Net[22]需要JPEG格式特有的特定元数据，这些元数据可以通过回调函数的增强图像从RGB域进一步获得。<br/>​  4)形状变换：这包括零填充[27]，裁剪和调整大小，以确保均匀的输入形状。<br/>​  此外，评估者可以自动适应不同的塑造策略来完成度量计算。</p><h2 id="模型库">3.2模型库</h2><p>​  该模型库目前由8个SoTA模型、6个基于开箱即用的视觉骨干模型的骨干模型和5个常用于IMDL任务的特征提取模块组成。需要强调的是，我们的目标是使用相同的训练脚本（标准化）对所有模型进行训练，同时也适用于所有SoTAIMDL模型（定制）。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306170503847.png"alt="image-20250306170503847" /><figcaption aria-hidden="true">image-20250306170503847</figcaption></figure><p>​  如图1所示，我们将损失函数的计算积分到模型的正向函数中。通过一个统一的接口，我们传递所需的信息，如图像和掩码，并输出预测结果、反向传播的损失，以及任何需要可视化的损失、中间特征和图像。因此，对于新的IMDL方法，用户只需要将带有损失设计的模型脚本添加到模型库中，这样它就可以与IMDL-BenCo中的所有其他组件无缝集成。通过使用这个框架有效地再现当前的SoTA模型，我们证明了我们已经成功地平衡了标准化和定制之间的冲突。</p><p>​  <strong>1)SoTA模型</strong><br/>​  如表1所示，我们如实地复制了8个主流的IMDLSoTA模型，并遵循了原始工作的设置。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306170018904.png"alt="image-20250306170018904" /><figcaption aria-hidden="true">image-20250306170018904</figcaption></figure><p>​  只要有可能，我们就使用公开可用的代码，只进行必要的接口修改。对于缺乏公开可用代码的模型，我们根据它们各自的论文中描述的设置来实现它们。每个模型的实现细节列在附录A.3中。</p><p>​  <strong>2)骨干模型：</strong><br/>​  作为分类和分割任务，主流IMDL算法广泛使用现有的视觉骨干，这些骨干的性能也会影响已实现算法的性能。因此，我们将广泛使用的视觉骨干，包括ResNet[16]、U-Net [30]、ViT[9]、Swin-Transformer[26]和SegFormer[44]作为IMDL-BenCo的骨干网络。</p><p>​  3)特征提取器模块：<br/>​  目前，一些标准的特征提取器被广泛应用于IMDL任务中。我们已经实现了5个主流功能提取器作为<em>nn.module</em>，包括离散余弦变换（DCT），快速傅里叶变换（FFT）[2]，Sobeloperator[35]，BayarConv[1]，SRM过滤器[49]，允许无缝集成与骨干模型与注册表机制管理大规模的实验或直接导入方便在后续研究中使用。</p><h2 id="训练脚本">3.3训练脚本</h2><p>​  训练脚本是使用IMDL-BenCo的入口点，它还集成了其他组件来执行特定的功能。它可以有效地自动化任务，如模型训练、度量评估、可视化、GradCAM分析[33]，以及基于配置文件（如JSON、命令行或YAML）的复杂性计算。为了避免在其他框架中看到的训练管道的高耦合（例如，开放MM实验室通常需要修改Python包函数来定制特性），我们提供了一个代码生成器，允许用户创建高度定制的训练脚本，同时仍然利用IMDL-BenCo的高效组件来提高开发效率。</p><h2 id="评估器">3.4评估器</h2><p>​  评估指标对于评估IMDL模型的性能至关重要的。然而，现有的方法面临两个关键问题：1)指标往往不明确，如optimal-F1[4]、permute-F1[22,13]、micro-F1和macro-F15，这些评分被匿名用作F1评分；2)大多数开源代码在CPU上计算指标，导致处理速度慢。<br/>​  为了解决这些问题，我们在PyTorch中开发了gpu加速评估器，并作为标准指标集成。每个评估者计算一个特定的度量，包括图像级F1评分（检测）、AUC精度（曲线下面积）；以及像素级F1评分（定位）、AUC、准确性和IOU（与Union的交集）。所有的算法都会自动适应数据加载器中的形状转换，从而提供了额外的便利。我们还明确地实现了导出的算法，如inverse-F1和permute-F1来评估它们的高估倾向，如第5.3节所示。这强调了在未来的工作中，精确和透明的度量选择的重要性，以确保公平和一致的比较。<br/>​  我们用来自CASIAv2数据集的12,554张图像和4张NVIDIA4090gpu进行了实验，并用nn.Identity作为模型测试了我们的评估者的时间效率。其计算时间可以忽略不计。结果如表2所示，我们的算法显著减少了度量评估时间，为大规模IMDL任务提供了一个更快、更可靠的工具。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306194814851.png"alt="image-20250306194814851" /><figcaption aria-hidden="true">image-20250306194814851</figcaption></figure><h1 id="我们的基准">4我们的基准</h1><h2 id="基准设置">4.1基准设置</h2><p>​  <strong>数据集</strong><br/>​  我们的基准测试包括8个在IMDL领域常用的公开数据集：CASIA[8]，FantasticReality[21]，IMD2020[29]，NIST16[12]，Columbia[17]，COVERAGE[42]，tamperedCOCO[22]，tampered RAISE[22]。每个数据集的详细信息见附录A.2。</p><p>​  <strong>评价指标</strong><br/>​  由于篡改区域的大小通常比真实区域小，因此像素级f1分数被广泛用作评估模型性能的合适指标。我们在两个协议中使用0.5的固定阈值来评估每个模型的像素级F1分数。我们还使用像素级的AUC和IOU指标来评估所有的模型。对于具有检测头的模型，我们另外报告了图像级F1分数。最后，我们给出了在高斯模糊、高斯噪声和JPEG压缩条件下的像素级F1分数的鲁棒性测试结果。</p><p>​  <strong>硬件配置</strong><br/>​  实验在3台不同的服务器上进行，分别使用两个AMDEPYC 7542cpu和128G RAM，分别包含4个×NVIDIA A40 GPUs、6个×NVIDIA 3090GPUs和4个×NVIDIA 4090 GPUs。</p><p>​  <strong>模型和超参数</strong><br/>​  我们的基准测试在IMDL字段中选择了8个SoTA方法作为实现方法的初始批。模型是：Mantra-Net[43]，MVSS-net[4]，CAT-Net[22]，ObjectFormer[40]，PSCC-Net[25]，NCL-IML[47]，Trufor[13]和IML-ViT[27]。我们的小修改、设置和超参数的细节可以在附录A.3中找到。</p><h2 id="基准协议">4.2基准协议</h2><p>​  现有公共数据集的规模和质量的不平衡导致了IMDL方法的训练和评估协议的不一致。这使得很难在现有方法之间实现公平和方便的比较。为此，我们选择了两种合理且广泛使用的协议：<br/>​  <strong>MVSS协议</strong>，由MVSS-Net[4]提出，其中，模型只在CASIAv2数据集上进行训练，然后直接在其他数据集上进行测试，而不进行微调。CASIAv2数据集大小中等，但质量较高，这使得该协议成为模型泛化能力的一个很好的度量。<br/>​  <strong>CAT协议</strong>，由CAT-Net[22]提出，其中，模型在一个由CASIAv2、FantasticReality、IMD2020、tampered COCO和tamperedRAISE组成的混合数据集上进行训练。<br/>​  对于每个epoch，随机抽取固定数量的图像进行训练，然后直接在其他数据集上进行模型测试，而不进行微调。这个混合数据集包括各种类型的篡改和大量的篡改图像，使模型能够有效地演示其学习能力。<br/>​  具体来说，在MVSS协议中，我们在训练没有检测头的方法时不使用真实的图像。而在cat协议中，我们并没有做出这种区分。此外，我们还使用了在每篇论文中指定的推荐的图像缩放尺寸。我们还对所有方法应用了一致的数据增强技术，如翻转、模糊、压缩以及使用OpenCV6实现的简单复制-移动和修改操作。</p><h2 id="我们的基准结果">4.3我们的基准结果</h2><p>​  对于MVSS协议，我们报告了COVERAGE、Columbia、NIST16、CASIAv1和IMD2020数据集的F1得分。对于CAT协议，我们排除了IMD2020，因为它被包含在训练过程中。像素级F1得分和图像级F1得分分别见表3和表4。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306200707168.png"alt="image-20250306200707168" /><figcaption aria-hidden="true">image-20250306200707168</figcaption></figure><figure><img src="../postimages/IMDL-BenCo/image-20250306200727454.png"alt="image-20250306200727454" /><figcaption aria-hidden="true">image-20250306200727454</figcaption></figure><p>​  像素级的AUC和IoU见附录A.4。我们复制的模型的性能与原始论文中报告的结果之间存在一些差异。此外，一些模型在相同的协议下表现出有限的性能。我们还在图2中描述了MVSS协议下的鲁棒性测试。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306200828584.png"alt="image-20250306200828584" /><figcaption aria-hidden="true">image-20250306200828584</figcaption></figure><p>​  我们观察到，一些模型存在显著的鲁棒性变化，证明了统一框架的必要性和保真度。</p><h1 id="实验分析">5实验分析</h1><p>​  我们的代码库和基准测试统一了IMDL模型的测试和训练协议。尽管有这种统一，但与其他检测或分割任务相比，IMDL任务也保留了多个独特和关键的特征，特别是依赖于“主干网络+低级特征提取器”范式、具有随机分割的基准数据集和各种评估指标。因此，我们进一步调查和深入分析了4个广泛关注但不太深入的问题，包括：<br/>​  1)低级特征提取器是IMDL中必须使用的吗？<br/>​  2)哪种主干架构最适合IMDL任务？<br/>​  3)随机分割训练和测试数据集是否会影响模型的性能？<br/>​  4)哪些指标主要代表了模型的实际行为？<br/>​  通过大量的实验，我们是第一个用证据事实来回答上述问题的人，并为模型设计、数据集清理和IMDL领域的度量提供了新的关键见解。</p><h2 id="低级特征提取器和骨架">5.1低级特征提取器和骨架</h2><p>​  如表1所示，目前流行的IMDL方法严重依赖于特征提取器来检测操作痕迹。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306170018904.png"alt="image-20250306170018904" /><figcaption aria-hidden="true">image-20250306170018904</figcaption></figure><p>​  然而，很少有文章专门分析不同的提取器的优点。在本节中，我们将模型动物园中实现的主干模型（见第3.2节）与不同的特征提取器模块相结合，以探索每个特征提取器的性能及其与主干的兼容性。各组合模型的复杂性如表5所示。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306201212499.png"alt="image-20250306201212499" /><figcaption aria-hidden="true">image-20250306201212499</figcaption></figure><p>​  所有的组合模型都在CASIAv2数据集上进行了200个epoch的训练，图像大小为512×512。详细的实验设置请见附录A.7.1。然后在四个不同的数据集上对它们进行评估——casiav1、Columbia、NIST16、Coverage和IMD2020——以评估它们的泛化能力。表6报告了四个数据集的每个设置的平均F1得分。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306201322070.png"alt="image-20250306201322070" /><figcaption aria-hidden="true">image-20250306201322070</figcaption></figure><p>​  实验表明，特定的特征提取器，如BayarConv和Sobel，会对模型的性能产生负面影响。相比之下，ResNet模型在配备DCT、FFT和SRM特征提取器时，显示出更好的性能。ViT模型，当配备特征提取器时，往往不拟合，可能需要更多的轮次来收敛。一种更好的特征融合方法可以消除目前ViT模型存在的问题。对于SwinTransformer，添加特征提取器可能会导致过拟合，而分段形成器的性能通常会下降。详细的讨论和实验结果详见附录A.7.2。</p><p>​  <strong>特征提取器的必要性</strong><br/>​  简而言之，BayarConv和Sobel不适合执行IMDL任务。适当的低级特征提取器，如DCT、FFT和SRM，可以提高ResNet模型的性能。然而，所有的特征提取器都可能阻碍ViT及其变体的收敛，导致SwinTransformer的过拟合，并导致传感器的整体性能下降。因此，低级别的特征提取器在IMDL中并不是必需的。</p><p>​  <strong>骨干适用</strong><br/>​  如表6所示，SwinTransformer和Segformer在IMDL任务上表现出了稳健的性能，优于ResNet和ViT。U-Net体系结构并不太适合此任务。</p><h2 id="数据集偏差和清理方法">5.2数据集偏差和清理方法</h2><p>​  <strong>数据集偏差</strong><br/>​  通过我们的基准测试，我们发现将我们的协议下的模型性能与他们在论文中进行微调后的模型性能进行比较，在每个模型上，在NIST16数据集上观察到显著的性能下降。然而，在其他基准数据集上并不会出现如此巨大的下降。经过彻底的分析，我们发现NIST16数据集包含了“非常相似”的篡改模式。然后，当这些非常相似的图像被随机分割成训练集和测试集时，模型可以通过记忆非常相似的训练样本来有效地定位操作区域。我们将这种关键的数据集偏差称为“标签泄漏”。图3说明了NIST16数据集中的一个标签泄漏实例。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306201828142.png"alt="image-20250306201828142" /><figcaption aria-hidden="true">image-20250306201828142</figcaption></figure><p>​  <strong>数据集清理</strong><br/>​  为了提高NIST16进行评估的可靠性，我们引入了一个新的数据集，NIST16-C7，它是通过应用基于结构相似度（SSIM）[41]的过滤方法创建的。这一过程有助于消除过度相似的图像，减少数据集偏差，并防止由标签泄漏造成的性能高估。关于我们的分析和清洗程序的更多细节，请见附录A.7.3。</p><p>​  我们对NIST16-C进行了广泛的基准测试，测试结果如表7所示。</p><figure><img src="../postimages/IMDL-BenCo/image-20250306202011792.png"alt="image-20250306202011792" /><figcaption aria-hidden="true">image-20250306202011792</figcaption></figure><p>​  这表明我们已经消除了NIST16数据集中的冗余数据，解决了标签泄漏问题。因此，模型现在可以专注于学习篡改的底层特性。</p><h2 id="评价指标选择">5.3评价指标选择</h2><p>​  <strong>有争议的F1分数</strong><br/>​  f1度规具有不同的计算方程的多种变化，如invert-F1和permute-F1[20,22,13]。Invert-F1是通过计算反向预测结果与原始掩码之间的f1得到的值。permute-F1是原始F1和invert-F1之间的最大值。公式为Permute-F1(G,P) =max(F1(G, P), Invert-F1(G,P))，其中G是地面真相，P是预测的掩模。如图4所示，当掩模的白色面积较大，且模型的预测与掩模存在显著偏差时，Invert-F1得分远高于f1得分。这个指标影响了评估的公平性。</p><p>​  此外，在通过sklearn库计算f1分数时，使用“macro”、“micro”和“weighted”等参数是不合适的，因为它人为地夸大了我们的f1指标，这是不合理的。我们在附录A.7.4中进一步分析了这些具有误导性的F1指标。将这些F1分数匿名混合使用将会导致重大的公平性问题。总之，我们认为使用f1分数和“二进制”参数进行计算是更加科学和严格。我们希望未来的研究能够统一采用这个标准。此外，我们还在附录A.7.5中讨论了AUC被高估的当前问题。</p><h1 id="结论">6结论</h1><p>​  总之，IMDL-BenCo标志着图像处理检测和定位领域的一个重大进展。通过提供一个全面的基准测试和一个模块化的代码库，IMDL-BenCo提高了编码效率和定制的灵活性，从而促进了严格的实验和IMDL模型的公平比较。此外，IMDL-BenCo通过为模型开发和评估提供一个统一的和可扩展的框架来激发未来的突破。我们预计，IMDL-BenCo将成为研究人员和从业者的重要资源，推动IMDL技术在各个领域的能力和应用，包括信息取证和安全。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>边界引导</title>
      <link href="/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/"/>
      <url>/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="一contextrast">一、Contextrast</h1><p>​  边界感知负值（BANE，boundary-aware negative）抽样</p><figure><imgsrc="../postimages/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/image-20250303215048258.png"alt="image-20250303215048258" /><figcaption aria-hidden="true">image-20250303215048258</figcaption></figure><p>​  (e)视觉描述我们的边界感知负值（BANE）采样（具有红色和红色边界的三角形）。我们的抽样优先选择在边缘（红色三角形）上的错误预测的特征，而不是在区域内（带有红色边框的三角形）作为负样本。</p><h1id="二attentive-and-contrastive-image-manipulation-localization-with-boundary-guidance">二、Attentiveand Contrastive Image Manipulation Localization With BoundaryGuidance</h1><p>​  边界感知注意力学习</p><figure><imgsrc="../postimages/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure><figure><imgsrc="../postimages/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/image-20250303220543188.png"alt="image-20250303220543188" /><figcaption aria-hidden="true">image-20250303220543188</figcaption></figure><p>​  在实践中，所有四个尺度都生成了操作掩模，而仅对两个尺度的中间边界掩模进行预测。对于具有最深特征的预测掩模，我们应用二进制交叉熵（BCE）损失和IoU损失进行监督。此外，我们采用加权二值交叉熵损失[63]和加权IoU损失[63]来监督预测的掩模和边界。边界的groundtruth值是通过从膨胀图像中减去二值地面真实掩模的侵蚀而得到的。具体来说，我们应用核大小为5×5、步幅为1的最大池化操作来进行图像扩张和侵蚀。</p><h1id="三iml-vit-benchmarking-image-manipulation-localization-by-vision-transformer">三、IML-ViT:Benchmarking Image Manipulation Localization by Vision Transformer</h1><figure><imgsrc="../postimages/%E8%BE%B9%E7%95%8C%E5%BC%95%E5%AF%BC/image-20250306104609719.png"alt="image-20250306104609719" /><figcaption aria-hidden="true">image-20250306104609719</figcaption></figure><p>​  边缘监督损失</p><p>​  为了解释伪影通常在被篡改区域的边缘更为普遍，而被操纵区域和真实区域之间的差异最为明显的事实，我们开发了一种策略，更强调被操纵区域的边界区域。具体来说，我们使用膨胀（⊕）和侵蚀（⊖）[32]等数学形态学运算从原始掩模图像M中生成一个二值边缘掩模M⋆，然后取结果的绝对值。我们用来生成边缘掩码的公式是：</p><p><span class="math inline">\({M}^{\star}=\,|(M\odotD(k))-\,(M\leftrightarrow B(k))|\)</span></p><p>​  其中，<span class="math inline">\(B(x)\)</span>生成一个<spanclass="math inline">\((2x+1)\times(2x+1)\)</span>交叉矩阵，其中只有第x列和第x行的值为1，而矩阵的其余部分包含0s。选择整数值x近似等于边界掩模中白色区域的宽度。</p><p>​  综合损失：为了计算损失函数，我们首先将地面真掩模M和边缘掩模<spanclass="math inline">\({M}^{\star}\)</span>垫到H×W的大小，并将它们分别称为<spanclass="math inline">\(M_p\)</span>和<spanclass="math inline">\(M_p^{\star}\)</span>。然后，我们使用以下公式计算最终的损失：</p><p><span class="math inline">\(\mathcal{L}=\mathcal{L}s eg(P,M_{p})+\lambda\cdot\mathcal{L}e d ge(P*M_{p}^{\star},M_{p}*M_{p}^{\star})\)</span></p><p>​  其中，∗表示点向乘积，它掩盖了原始图像。Lseg和Ledge都是二进制交叉熵损失函数，而λ是一个超参数，它控制着分割和边缘检测损失之间的平衡。默认情况下，我们搜索最优的λ=20来指导模型关注于边缘区域，这是图5所支持的。我们为λ选择一个较大的值也有两个原因：(1)强调边界区域，(2)为了平衡由零填充引入的大量零。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DBSCAN</title>
      <link href="/DBSCAN/"/>
      <url>/DBSCAN/</url>
      
        <content type="html"><![CDATA[<p>来自维基百科，一个免费的百科全书</p><h1 id="一介绍">一、介绍</h1><p>​  基于密度的噪声应用空间聚类（DBSCAN，<strong>Density-based spatialclustering of applications withnoise</strong>）是由马丁·埃斯特、汉斯-彼得·克里格尔、桑德和徐小伟于1996年提出的一种数据聚类算法。[1]这是一种基于密度的聚类非参数算法：给定某些空间中的一组点，它将紧密排列的点（有许多相邻的点）聚在一起，并标记为低密度区域的异常点（那些最近的邻居太远）的异常点。DBSCAN是最常用和最被引用的聚类算法之一。[2]<br/>​  2014年，在领先的数据挖掘会议ACMSIGKDD上，该算法获得了时间测试奖（授予在理论和实践上受到大量关注的算法）。[3]截至2020年7月，后续论文“DBSCAN重访，重访：为什么你应该和如何（仍然）使用DBSCAN”[4]出现在著名的ACM数据库系统交易（TODS）期刊的8篇下载次数最多的文章列表中。<br/>​  另一个后续文章HDBSCAN*最初由RicardoJ. G. Campello, David Moulavi和JörgSander于2013年出版，[6]在2015年与亚瑟·齐梅克进行了扩展。[7]它修改了一些原始的决策，如边界点，并产生了一个层次化的结果，而不是一个平坦的结果。</p><h1 id="二历史">二、历史</h1><p>​  1972年，Robert F.Ling在计算机期刊上的“k-簇的理论和构建”[8]中发表了一个密切相关的算法，估计运行时复杂度为O（n³）。[8]DBSCAN的最坏情况是O（n²），而面向数据库的DBSCAN的范围查询公式允许索引加速。这些算法在处理边界点上略有不同。</p><h1 id="三准备工作">三、准备工作</h1><p>​  考虑某个空间中的一组点。设ε是一个参数，指定一个邻域相对于某个点的半径。为了进行DBSCAN聚类，将这些点划分为核心点、（直接-）可达点和异常值，如下：</p><ul><li>如果至少有minPts点在它的距离ε范围内（包括p），则点p是一个核心点。</li><li>如果点q与核心点p在距离ε内，则点q可以直接从p到达。点只说可以从核心点直接到达。</li><li>如果有一个路径p1，...，pn，且p1 = p和pn =q，则一个点q可以从p到达，其中每个pi+1可以直接从pi到达。注意，这意味着初始点和路径上的所有点必须是核心点，可能的q除外。</li><li>所有不能从任何其他点可到达的点都是异常值或噪声点。</li></ul><p>​  现在，如果p是一个核心点，那么它与从它可以到达的所有点（核心或非核心）一起形成一个集群。每个集群中至少包含一个核心点；非核心点可以是集群的一部分，但它们形成了集群的“边缘”，因为它们不能被用来到达更多的点。</p><figure><img src="../postimages/DBSCAN/image-20250228215919795.png"alt="image-20250228215919795" /><figcaption aria-hidden="true">image-20250228215919795</figcaption></figure><p>​  在这个图中，minPts =4。点A和其他红点是核心点，因为在ε半径内，这些点周围的区域中包含了至少4个点（包括点本身）。因为它们都是相互访问的，所以它们形成了一个单独的集群。点B和点C不是核心点，但可以从A到达（通过其他核心点），因此也属于集群。N点是一个既不是核心点，也不是直接可到达的噪声点。<br/>​  可达性不是一种对称的关系：根据定义，只有核心点才能到达非核心点。相反的，所以一个非核心点可能可以到达，但什么也不能到达。因此，需要进一步的连通性概念来正式定义DBSCAN发现的集群的范围。两点p和q是密度连接的，如果有一个点o使得p和q都是从o可到达的。密度-连通性是对称的。<br/>​  然后一个集群满足两个属性：</p><ol type="1"><li>簇内的所有点都是相互密度连接的。<br/>2.如果一个点是从集群的某个点可以到达密度的，那么它也是集群的一部分。</li></ol><h1 id="四算法">四、算法</h1><h2 id="原始基于查询的算法">1.原始基于查询的算法</h2><p>​  DBSCAN需要两个参数：ε（eps）和形成密集区域[a]（minPts）所需的最小点数。它从一个未被访问过的任意起点开始。检索这个点的ε邻域，如果它包含足够多的点，就启动一个集群。否则，该点将被标记为噪声。请注意，这个点以后可能会在另一个不同点的足够大小的ε环境中找到，因此会成为集群的一部分。<br/>​  如果一个点被发现是一个集群的密集部分，它的ε邻域也是该集群的一部分。因此，在ε邻域内发现的所有点都被添加起来，当它们自己的ε邻域也很密集时。这个过程一直持续到密度连接的集群完全找到。然后，一个新的未访问的点被检索和处理，导致发现进一步的集群或噪声。<br/>​  DBSCAN可以用于任何距离函数[1][4]（以及相似度函数或其他谓词）。[9]因此，距离函数（dist）可以看作是一个附加的参数。该算法可以用伪代码表示如下：[4]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DBSCAN(DB, distFunc, eps, minPts) &#123;</span><br><span class="line">    C := 0                                                  /* 集群计数器 */</span><br><span class="line">    for each point P in database DB &#123;</span><br><span class="line">        if label(P) ≠ undefined then continue               /* 在内循环中预处理 */</span><br><span class="line">        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* 找领域 */</span><br><span class="line">        if |N| &lt; minPts then &#123;                              /* 密度检查 */</span><br><span class="line">            label(P) := Noise                               /* 标记为噪音 */</span><br><span class="line">            continue</span><br><span class="line">        &#125;</span><br><span class="line">        C := C + 1                                          /* 标记为下一个聚类 */</span><br><span class="line">        label(P) := C                                       /* 标记为原点 */</span><br><span class="line">        SeedSet S := N \ &#123;P&#125;                                /* 要扩展的邻域 */</span><br><span class="line">        for each point Q in S &#123;                             /* 处理每个种子点Q */</span><br><span class="line">            if label(Q) = Noise then label(Q) := C          /* 将噪声更改为边界点 */</span><br><span class="line">            if label(Q) ≠ undefined then continue           /* 预处理 (e.g., 边界点) */</span><br><span class="line">            label(Q) := C                                   /* 标记为邻域 */</span><br><span class="line">            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* 找领域 */</span><br><span class="line">            if |N| ≥ minPts then &#123;                          /* 密度检查 (如果Q是一个核心点) */</span><br><span class="line">                S := S ∪ N                                  /* 向种子集添加新的邻域 */</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​  其中RangeQuery可以使用数据库索引来实现更好的性能，或使用缓慢的线性扫描：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RangeQuery(DB, distFunc, Q, eps) &#123;</span><br><span class="line">    Neighbors N := empty list</span><br><span class="line">    for each point P in database DB &#123;                      /* 扫描数据库中的所有点 */</span><br><span class="line">        if distFunc(Q, P) ≤ eps then &#123;                     /* 计算距离并检查距离 */</span><br><span class="line">            N := N ∪ &#123;P&#125;                                   /* 添加到结果 */</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return N</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="抽象算法">2.抽象算法</h2><p>​  DBSCAN算法可以抽象为以下步骤：[4]</p><ol type="1"><li><p>找到每个点的ε（eps）邻域中的点，并识别出超过minPts邻域的核心点。</p></li><li><p>在邻居图上找到核心点的连通分量，忽略所有的非核心点。</p></li><li><p>如果集群是一个ε（eps）邻居，则将每个非核心点分配给附近的集群，否则将其分配给噪声。</p></li></ol><p>​  一个简单的实现需要在步骤1中存储社区，因此需要大量的内存。最初的DBSCAN算法不需要一次执行一个点执行这些步骤。</p><h2 id="优化标准">3.优化标准</h2><p>​  DBSCAN优化以下损失功能：[10]对于任何可能的集群<spanclass="math inline">\(C=\{C_{1},\ldots,C_{l}\}\)</span>的所有的集群<spanclass="math inline">\(\cal{C}\)</span>，它减少集群的数量条件下，每一对点集群是密度可达的，这对应于最初的两个属性“最大性”和“连接”的集群：[1]<span class="math display">\[{\displaystyle \min _{C\subset {\mathcal{C}},~d_{db}(p,q)\leq \varepsilon ~\forall p,q\in C_{i}~\forall C_{i}\inC}|C|}\]</span> ​  其中<span class="math inline">\({\displaystyled_{db}(p,q)}\)</span>给出最小的<spanclass="math inline">\(\varepsilon\)</span>，这样两点p和q是密度连接的。</p><h2 id="复杂度">4.复杂度</h2><p>​  DBSCAN访问数据库的每个点，可能会访问多次（例如，作为不同集群的候选点）。然而，出于实际考虑，时间复杂度主要由区域查询调用的数量决定。DBSCAN执行一个这样的查询，如果使用一个索引结构，执行一个邻居查询O(<em>n</em>log <em>n</em>)，一个整体平均运行时复杂度的O(<em>n</em> log<em>n</em>)（如果参数ε选择在一个有意义的方式，例如，平均只有O(<em>n</em>log<em>n</em>)点返回）。如果不使用加速索引结构，或对退化数据（例如，在距离小于ε范围内的所有点），最坏情况下的运行时复杂度仍然是O（n²）。距离矩阵的上三角形<spanclass="math inline">\({\displaystyle \textstyle {\binom {n}{2}}} - n =(n²-n)/2\)</span>可以物化以避免距离重新计算，但这需要O（n²）内存，而基于非矩阵的DBSCAN实现只需要O(n)内存。</p><h2 id="优势">5.优势</h2><ol type="1"><li>DBSCAN不需要预先指定数据中的集群数量，而不是k-means。<br/>2.DBSCAN可以找到任意形状的团簇。它甚至可以找到一个完全被一个不同的集群包围（但没有连接到）的集群。由于MinPts参数，所谓的单链效应（不同的簇通过一条细点线连接）被减少。<br/>3.DBSCAN有噪声的概念，并且对异常值具有鲁棒性。<br/>4.DBSCAN只需要两个参数，并且对数据库中的点的排序大多不敏感。（然而，如果点的顺序发生了改变，那么位于两个不同集群边缘的点可能会交换集群成员关系，并且集群分配仅在同构之前是唯一的。）<br/>5.DBSCAN是为使用可以加速区域查询的数据库而设计的，例如使用R*树。<br/>6.如果数据理解，minPts可以由领域专家设置参数mints和ε。</li></ol><h2 id="劣势">6.劣势</h2><ol type="1"><li>DBSCAN并不是完全确定性的：可以从多个集群中可访问的边界点可以是任何一个集群的一部分，这取决于数据被处理的顺序。对于大多数数据集和域，这种情况并不经常出现，对聚类结果的影响也很小：[4]无论是核心点还是噪声点，DBSCAN都是确定性的。DBSCAN*[6][7]是一种将边界点视为噪声的变化，这种方式实现了完全确定的结果以及密度连接分量的更一致的统计解释。<br/>2.DBSCAN的质量取决于函数区域查询（P，ε）中使用的距离度量。最常用的距离度量是欧氏距离。特别是对于高维数据，由于所谓的“维数诅咒”，这个度量几乎变得无用，使得很难为ε找到合适的值。然而，这种效应也存在于任何其他基于欧氏距离的算法中。<br/>3.DBSCAN不能很好地聚类密度差异很大的数据集，因为不能适当地选择minPts-ε组合。[11]<br/>4.如果数据和规模没有被很好地理解，那么选择一个有意义的距离阈值ε可能是困难的。</li></ol><p>请参阅下面的扩展部分，以了解处理这些问题的算法修改。</p><h2 id="参数估计">7.参数估计</h2><p>​  每个数据挖掘任务都存在参数问题。每个参数都会以特定的方式影响算法。对于DBSCAN，需要参数ε和minpt。这些参数必须由用户指定。理想情况下，ε的值是由要解决的问题（例如物理距离）给出的，然后minPts是期望的最小簇大小。[a]</p><ul><li>MinPts：根据经验，最小的MinPts可以从数据集中的维数D中得到，如MinPts≥D+ 1。minPts =1的低值没有意义，因为每个点都是一个核心点。使用minPts≤2，结果将与单链接度量的层次聚类相同，树状图切割高度为ε。因此，<em>minPts</em>必须选择至少为3。然而，对于有噪声的数据集，更大的值通常更好，并将产生更显著的集群。根据经验法则，可以使用minPts=2·dim，[9]，但对于非常大的数据、有噪声的数据或包含许多重复的数据，可能需要选择更大的值。[4]</li><li>ε：ε的值可以用一个<ahref="https://en.wikipedia.org/wiki/Nearest_neighbor_graph">k-distancegraph</a>来选择，绘制到 <em>k</em> = minPts-1个最近邻的距离，按从最大值到最小值的顺序排列。[4]ε的良好值是这个图中显示一个“肘部”的地方：[1][9][4]如果选择ε太小，大部分数据将不会聚集；而如果ε值过高，集群将合并，大多数对象将在同一个集群中。一般来说，小的ε值更可取，[4]和作为经验法则，只有一小部分点应该在这个距离内彼此。或者，一个<a href="https://en.wikipedia.org/wiki/OPTICS_algorithm">OPTICS</a>图可以用来选择ε，[4]，但然后OPTICS算法本身也可以用来聚类数据。</li><li>距离函数：距离函数的选择与ε的选择紧密耦合，对结果有重大影响。一般来说，在选择参数ε之前，需要首先为数据集确定一个合理的相似性度量。对这个参数没有估计，但需要为数据集适当地选择距离函数。例如，在地理数据上，<ahref="https://en.wikipedia.org/wiki/Great-circle_distance">great-circledistance</a> 通常是一个很好的选择。</li></ul><p>​   <ahref="https://en.wikipedia.org/wiki/OPTICS_algorithm">OPTICS</a>可以看作是DBSCAN的泛化，它用最影响性能的最大值代替ε参数。然后，MinPts基本上成为了要找到的最小集群大小。虽然该算法比DBSCAN更容易参数化，但结果使用起来有点困难，因为它通常会产生分层聚类，而不是DBSCAN产生的简单数据分区。<br/>​  最近，DBSCAN的一位原始作者重新回顾了DBSCAN和OPTICS，并发表了一个改进版本的分层DBSCAN（hdbscan*），[6][7]，它不再有边界点的概念。相反，只有核心点构成了集群。</p><h2 id="与光谱聚类的关系">8.与光谱聚类的关系</h2><p>​  DBSCAN的光谱实现与在确定连通图组件的平凡情况下的光谱聚类有关——没有边切割的最优聚类。[12]然而，它可以是计算密集型的，高达<span class="math inline">\(\displaystyle O(n^{3})\)</span>。此外，还必须选择特征向量的数量来计算。由于性能原因，原始的DBSCAN算法仍然优于其光谱实现。</p><h2 id="延伸">9.延伸</h2><p>​  广义DBSCAN（GDBSCAN）[9][13]是由同一作者对任意“邻域”和“密集”谓词的推广。ε和minPts参数将从原始算法中删除，并移动到断言中。例如，在多边形数据上，“邻域”可以是任何相交的多边形，而密度谓词使用多边形区域，而不仅仅是对象计数。对DBSCAN算法提出了各种扩展，包括并行化方法、参数估计和对不确定数据的支持。利用光学算法将其基本思想扩展到层次聚类中。DBSCAN也被用作子空间聚类算法的一部分，如PreDeCon和SUBCLU。HDBSCAN*[6][7]是DBSCAN的一个层次版本，它也比光学更快，它可以从层次中提取一个由最突出的集群组成的平面分区。[14]</p><h1 id="五源自">五、源自</h1><p><a href="https://en.wikipedia.org/wiki/DBSCAN#Disadvantages">DBSCAN -Wikipedia</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Image as Set of Points</title>
      <link href="/Image-as-Set-of-Points/"/>
      <url>/Image-as-Set-of-Points/</url>
      
        <content type="html"><![CDATA[<h1 id="image-as-set-of-points">Image as Set of Points</h1><p><strong>Xu Ma</strong>1∗ <strong>, Yuqian Zhou</strong>2∗ <strong>,Huan Wang</strong>1 <strong>, Can Qin</strong>1 <strong>, BinSun</strong>1 <strong>, Chang Liu</strong>1 <strong>, YunFu</strong>1<br/>1东北大学<br/>2Adobe Inc.</p><h1 id="摘要">摘要</h1><p>​  什么是图像，如何提取潜在的特征？<br/>​  <strong>卷积网络</strong>(ConvNets)将图像视为矩形的有组织的像素，并通过局部区域的卷积操作提取特征；<strong>VisionTransformers</strong>(ViTs)视觉变压器（ViTs）将图像视为一系列补丁，并通过注意机制在全局范围内提取特征。在这项工作中，我们引入了一个直接而有前途的视觉表示范式，它称为<strong>ContextClusters</strong>。上下文聚类（CoCs，Contextclusters）将图像视为一组无组织的点，并通过简化的聚类算法提取特征。具体地说，每个点都包括原始特征（如颜色）和位置信息（如坐标），并采用简化的聚类算法对深度特征进行分层分组和提取。我们的CoCs是无卷积和无注意的，并且只依赖于聚类算法来进行空间交互。由于简单的设计，我们通过聚类过程的可视化展示了CoCs赋予了令人满意的可解释性。我们的CoCs旨在为图像和视觉表示提供一个新的视角，这可能在不同的领域享有广泛的应用，并表现出深刻的见解。即使我们没有针对SOTA的性能，COCs仍然在几个基准测试上可以取得比ConvNets或ViTs相当甚至更好的结果。<br/>​  代码可获得：https://github.com/ma-xu/Context-Cluster。</p><h1 id="介绍">1介绍</h1><p>​  我们提取特征的方式在很大程度上取决于我们如何解释图像。作为一种基本范式，卷积神经网络（ConvNets）近年来主导了计算机视觉领域，并显著提高了各种视觉任务的性能(Heet al., 2016;Xie et al., 2021; Ge et al.,2021)。从方法论上讲，卷积网络将图片概念化为矩形形式的排列像素的集合，并以滑动窗口的方式使用卷积提取局部特征。受益于一些重要的归纳偏差，如局部性和翻译等价性，卷积网络被证明是高效和有效的。最近，VisionTransformers（ViTs）显著挑战了卷积神经网络在视觉领域的霸权。Transformers(Vaswani et al.,2017)从语言处理中得出，将图像视为一系列补丁，并采用全局范围的自关注操作来自适应地融合补丁中的信息。通过所得到的模型（即ViTs），卷积神经网络中固有的归纳偏差被抛弃，并获得了令人满意的结果(Touvronet al.,2021)。<br/>​  最近的研究表明，视觉社区有了巨大的进步，这些进步主要建立在卷积或注意力的基础上(<em>e.g.</em>, ConvNeXt (Liu et al., 2022), MAE (He et al., 2022), andCLIP (Radford et al.,2021))。同时，一些尝试将卷积和注意力结合在一起，如CMT (Guo et al.,2022a)和CoAtNet (Dai et al.,2021)。这些方法在网格中扫描图像（卷积），同时探索序列的相互关系（注意力），在不牺牲全局接收（注意力）的情况下享受局部先验（卷积）。虽然它们继承了两者的优点并实现了更好的实证性能，但见解和知识仍然局限于卷积神经网络和ViTs。我们强调，除了卷积和关注之外，一些特征提取器也值得研究，而不是被引诱到追求增量改进的陷阱中。虽然卷积和注意力被认为具有显著的好处，并对视野产生巨大的影响，但它们并不是唯一的选择。基于MLP的架构(Touvronet al., 2022; Tolstikhin et al.,2021)已经证明，纯基于MLP的设计也可以实现类似的性能。<br/>​  在这项工作中，我们回顾了基本视觉表示的经典算法，聚类方法(Bishop &amp; Nasrabadi,2006)。整体地说，我们将图像视为一组数据点，并将所有点分组到集群中。在每个集群中，我们将这些点聚合到一个中心中，然后自适应地将这些中心点分配到所有的点上。我们称之为设计上下文集群。图1说明了这个过程。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250224222659358.png"alt="image-20250224222659358" /><figcaption aria-hidden="true">image-20250224222659358</figcaption></figure><p>​  具体来说，我们将每个像素视为一个具有颜色和位置信息的5维数据点。在某种意义上，我们将图像转换为一组点云，并利用点云分析的方法(Qi et al., 2017b; Ma etal.,2022)进行图像视觉表示学习。这连接了图像和点云的表示，显示了很强的泛化，并为多模式的简单融合打开了可能性。利用一组点，我们引入了一种简化的聚类方法来将这些点进行聚类。聚类处理与超级像素像素SuperPixel(Ren &amp;Malik,2003)有相似的想法，其中相似的像素被分组，但它们在本质上是不同的。据我们所知，我们是第一个引入一般视觉表示的聚类方法并使其工作的人。相反，超级像素及以后版本主要用于图像预处理（Jampanani等人，2018）或语义分割等特定任务（Yang等人，2020；Yu等人，2022b）。<br/>​  我们基于上下文集群实例化我们的深度网络，并将生成的模型命名为上下文集群（CoCs）。我们的新设计与ConvNets或ViTs有本质上的不同，但我们也从它们那里继承了一些积极的哲学，包括来自ConvNets的层次表示（Liu等人，2022），以及来自ViTs的元代表（Yu等人，2022c）框架。CoCs具有明显的优势。首先，通过将图像作为一组点来考虑，CoCs对点云、RGBD图像等不同的数据域表现出了很强的泛化能力。其次，上下文聚类处理为CoCs提供了令人满意的可解释性。通过可视化每一层的聚类，我们可以明确地理解每一层的学习。尽管我们的方法没有针对SOTA的性能，但在几个基准测试上，它仍然取得了比ConvNets或ViTs相当甚至更好的性能。我们希望我们的上下文聚类将为视觉社区带来新的突破。</p><h1 id="相关工作">2相关工作</h1><h2 id="图像处理中的聚类">2.1图像处理中的聚类</h2><p>​  虽然图像处理中的聚类方法（Castleman，1996）在深度学习时代已经失宠，但它们从未从计算机视觉中消失。一个历史悠久的作品是SuperPixel(Ren &amp; Malik,2003)，它通过将一组具有共同特征的像素分组，将图像分割成区域。由于所期望的稀疏性和简单表示，SuperPixel已成为图像预处理的常见做法。在整个图像上简单地应用超级像素穷尽集群（例如，通过K-means算法）像素，使得计算成本沉重。为此，SLIC（Achantaetal.，2012）限制了局部区域的聚类操作，并均匀地初始化K-means中心，以更好更快地收敛。近年来，聚类方法的兴趣激增，并与深度网络紧密相关(Li&amp;Chen，2015年；Jampani等，2018年；秦等，2018年；Yang等，2020年）。为了创建深度网络的超像素，SSN（Jampanietal.，2018）提出了一种可微的SLIC方法，该方法是端到端可训练，具有良好的运行时间。最近，人们尝试将聚类方法应用于网络的特定视觉任务，如分割（Yuet al.，2022b；Xu et al.，2022）和细粒度识别（Huang &amp;Li，2020）。例如，CMT-DeepLab（Yu etal.，2022a）将分割任务中的对象查询解释为集群中心，并将分组的像素分配给每个集群的分割。然而，据我们所知，目前还没有通过聚类进行一般视觉表示的工作。我们的目的是弥补这个空缺，并在数值和视觉上证明其可行性。</p><h2 id="convnets-vits">2.2ConvNets &amp; ViTs</h2><p>​  自深度学习时代以来，ConvNets就一直主导着视觉社区（西蒙尼扬和齐瑟曼，2015年；He等人，2016年）。最近，ViTs（多索维茨基等人，2020年）将视觉社区引入了纯基于注意力的变压器（Vaswani等人，2017年），并在各种视觉任务上设置了新的SOTA性能。一个常见而合理的猜想是，这些令人满意的成就被归功于自我注意机制。然而，这个直观的猜想很快就受到了挑战。大量的实验也表明，ResNet（He等人，2016年）可以通过适当的训练配方和最小的修改，达到同等甚至更好的性能（怀特曼等人，2021年；Liu等人，2022年）。我们强调，虽然卷积和注意力可能具有独特的优点（即网络具有归纳偏差（Liuet al.，2022），而vit擅长泛化（Yuan etal.，2021b）），但它们没有显示出显著的性能差距。与卷积和注意不同，在这项工作中，我们从根本上提出了一个新的范式的视觉表示使用聚类算法。通过定量和定性分析，我们表明，我们的方法可以作为一个新的一般主干，并具有令人满意的可解释性。</p><h2 id="最近的进展">2.3最近的进展</h2><p>​  在ConvNets和ViTs的框架内，视觉任务的表现得到了广泛的努力（Liu等，2021b；丁等，2022b；Wu等，2021）。为了同时利用卷积和注意力，一些工作学习在混合模式下混合这两种设计，如CoAtNet（Dai等人，2021）和移动前者（Chen等人，2022b）。我们还注意到，一些最近的进展探索了更多的视觉表示方法，超出了卷积和注意力。类MLP模型（托尔斯蒂金等人，2021年；Touvron等人，2022年；侯等人，2022年；Chen等人，2022a)直接考虑空间交互作用的MLP层。此外，一些工作采用了转移（Lian等人，2021；Huang等人，2021）或汇集（Yuetal.，2022c）来进行本地交流。与我们将图像视为无序数据集的工作类似，VisionGNN（ViG）（Han etal.，2022）为视觉任务提取图级特征。不同的是，我们直接应用传统图像处理的聚类方法，表现出良好的泛化能力和可解释性。</p><h1 id="方法">3方法</h1><p>​  上下文聚类放弃了时尚的卷积或注意力，而考虑经典算法聚类，为视觉学习的表示。在本节中，我们首先描述上下文集群管道。然后详细解释了所提出的特征提取的上下文聚类操作（如图二所示）。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250224224804579.png"alt="image-20250224224804579" /><figcaption aria-hidden="true">image-20250224224804579</figcaption></figure><p>​  之后，我们建立了上下文集群架构。最后，一些公开的讨论可能会帮助个人理解我们的工作，并根据我们的上下文集群探索更多的方向。</p><h2 id="上下文集群管道">3.1上下文集群管道</h2><h3 id="从图像到点集">3.1.1从图像到点集</h3><p>​  给定一个输入图像<spanclass="math inline">\({\textbf{I}}\in{\mathbb{R}}^{3\times w\timesh}\)</span>，我们首先用每个像素<spanclass="math inline">\(\operatorname{I}_{i,j}\)</span>的二维坐标增强图像，其中每个像素的坐标表示为<spanclass="math inline">\([{\frac{i}{w}}-0.5,{\frac{j}{h}}-0.5]\)</span>。进一步研究位置增强技术以潜在地提高性能是可行的。这种设计是考虑到其简单性和实用性。然后将增强的图像转换为一个点的集合（即像素）<spanclass="math inline">\(\mathbf{P}\in\mathbb{R}^{5 \timesn}\)</span>，其中，<span class="math inline">\(n=w\timesh\)</span>是点的数量，每个点同时包含特征（颜色）和位置（坐标）信息；因此，点集可以是无序和杂乱无章的。<br/>​  我们通过提供一个新的图像视角，一组点来获得优秀的泛化能力。一组数据点可以被认为是一种通用的数据表示，因为大多数领域中的数据可以作为特征和位置信息（或两者中的任何一种）的组合给出。这激励我们将一个图像概念化为一组点。</p><h3 id="使用图像设置点的特征提取">3.1.2使用图像设置点的特征提取</h3><p>​  根据ConvNets方法（He等人，2016；Liu等人，2022年），我们使用上下文聚类块（参考见图2，解释见图3.2）分层提取深度特征。图3显示了我们的上下文集群架构。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250224225440089.png"alt="image-20250224225440089" /><figcaption aria-hidden="true">image-20250224225440089</figcaption></figure><p>​  给定一组点<span class="math inline">\(\mathbf{P}\in\mathbb{R}^{5\timesn}\)</span>，我们首先降低点数以提高计算效率，然后应用一系列的上下文聚类块来提取特征。为了减少点数，我们在空间中均匀地选择一些锚点，并将最近的k个点通过线性投影进行连接和融合。请注意，如果所有点按顺序排列，并且k正确设置（即4和9），这种缩减可以通过卷积操作来实现，就像ViT（Dosovitskiy等人，2020）。为了明确前面所述的中心和锚点，我们强烈建议读者查看附录B。</p><h3 id="任务特定的程序">3.1.3任务特定的程序</h3><p>​  对于分类，我们对最后一个块输出的所有点进行平均，并使用FC层进行分类。对于下游的检测和分割等密集预测任务，我们需要在每个阶段后按位置重新排列输出点，以满足大多数检测和分割头的需求（如Mask-RCNN（Heetal.，2017））。也就是说，上下文集群在分类方面提供了显著的灵活性，但仅限于在密集预测任务的需求和我们的模型配置之间的折衷。我们期望创新的检测和分割头（如DETR（Carionet al.，2020））能够与我们的方法无缝集成。</p><h2 id="上下文集群操作">3.2上下文集群操作</h2><p>​  在本小节中，我们将介绍我们工作中的关键贡献——上下文集群操作。整体上，我们首先将特征点分组到聚类中；然后，将每个集群中的特征点进行聚合，然后发回，如图1所示。</p><h3 id="上下文聚类">3.2.1上下文聚类</h3><p>​  给定一组特征点<spanclass="math inline">\(\mathbf{P}\in\mathbb{R}^{n \timesd}\)</span>，我们根据相似性将所有的点分成几个组，每个点被单独分配给一个簇。我们首先将<spanclass="math inline">\(\mathbf{P}\)</span>线性投影到<spanclass="math inline">\(\mathbf{P}_s\)</span>来进行相似性计算。根据传统的超像素方法SLIC（Achantaetal.，2012），我们均匀地提出了空间中的c个中心，并通过平均其k个最近点来计算中心特征。然后我们计算了在<spanclass="math inline">\(\mathbf{P}_s\)</span>和所得到中心点集之间的成对余弦相似度矩阵<spanclass="math inline">\(\mathbf{S}\in\mathbb{R}^{c \timesn}\)</span>。由于每个点同时包含特征信息和位置信息，因此在计算相似性时，我们隐式地突出了这些点的距离（局部性）和特征相似性。之后，我们将每个点分配到最相似的中心，从而得到c个集群。值得注意的是，每个集群可能有不同数量的点。在极端情况下，一些集群可能有零点，在这种情况下，它们是冗余的。</p><h3 id="特征聚合">3.2.2特征聚合</h3><p>​  我们根据与中心点的相似性动态地聚合集群中的所有点。假设一个簇包含m个点（P中的一个子集），m个点与中心的相似度为<spanclass="math inline">\(s\in\mathbb{R}^{m}\)</span>（S中的一个子集），我们将这些点映射到一个值空间，得到<spanclass="math inline">\(P_{\nu}\in\mathbb{R}^{m\timesd^{\prime}}\)</span>，其中<spanclass="math inline">\(d^{\prime}\)</span>是值维数。我们还提出了一个在价值空间中的中心vc，如聚类中心方案。聚合特征<spanclass="math inline">\(g\in\mathbb{R}^{d}\)</span>由： <spanclass="math display">\[g={\frac{1}{C}}\left(\nu_{c}+\sum_{i=1}^{m}\mathrm\,(\alphas_{i}+\beta)*\nu_{i}\right),\qquad{\mathrm{s.t.}},\;\;C=1+\sum_{i=1}^{m}\mathrm\,(\alphas_{i}+\beta)\,.\]</span>​  在这里，α和β是可学习的标量来缩放和移动相似度，而sig（·）是一个s型函数来重新缩放相似度到（0,1）。<spanclass="math inline">\(\nu_{i}\)</span>表示<spanclass="math inline">\(P_{\nu}\)</span>中的第i个点。根据经验，这种策略将比直接应用原始相似性获得更好的结果，因为没有涉及负值。Softmax不被考虑，因为这些点之间并不相互矛盾。我们在等式1中加入了价值中心vc为数值稳定性<spanclass="math inline">\(^1\)</span>以及进一步强调的局部性。为了控制大小，聚合特征归一化为<spanclass="math inline">\(C\)</span>。<br/>​  （1如果没有涉及到vc，也没有任何点被同时分组到集群中，那么C将为零，网络就不能被优化。在我们的研究中，这个难题经常发生。添加一个像1e−5这样的小值并没有帮助，并且会导致梯度消失的问题。）</p><h3 id="特征处理">3.2.3特征处理</h3><p>​  然后，聚合的特征g根据相似性自适应地分配到集群中的每个点。通过这样做，这些点可以相互通信，并共享来自集群中所有点的特征，如图1所示。对于每个点pi，我们更新它<spanclass="math display">\[p_{i}^{\prime}=p_{i}+\mathrm{FC}\left(\mathrm{sig}\left(\alphaS_{i}+\beta\right)*g\right).\]</span>​  在这里，我们遵循相同的过程来处理相似性，并应用一个全连接（FC）层来匹配特征维度（从值空间维度<spanclass="math inline">\(d^{\prime}\)</span>到原始维度d）。</p><h3 id="多头计算">3.2.3多头计算</h3><p>​  我们承认自我注意机制中的多头设计（Vaswani etal.，2017），并使用它来增强我们的上下文集群。为了简单起见，我们考虑h头，并将值空间Pv和相似度空间Ps的维数设为<spanclass="math inline">\(d^{\prime}\)</span>。多头操作的输出由一个FC层连接和融合。正如我们通过经验证明的那样，多头体系结构也有助于上下文集群的令人满意的改进。</p><h2 id="架构初始化">3.3架构初始化</h2><p>​  虽然上下文集群从根本上不同于卷积和关注，但来自ConvNets和vit的设计理念，如层次表示和元变压器架构（Yuetal.，2022c），仍然适用于上下文集群。为了与其他网络对齐，并使我们的方法与大多数检测和分割算法兼容，我们在每个阶段逐步减少点数16、4、4和4倍。在第一阶段，我们考虑了选定锚点的16个最近邻，在其余阶段，我们选择了它们的9个最近邻。<br/>​  一个潜在的问题是计算效率。假设我们有n个d维点和c个聚类，计算特征相似度的时间复杂度为O（ncd），当输入图像分辨率较高时，这是不可接受的（例如，224×224）。为了解决这个问题，我们引入了区域划分，将点分割成几个局部区域，如SwinTransformer（Liuetal.，2021b），并计算局部相似度。因此，当局部区域的数量设置为r时，我们显著地将时间复杂度降低了r的因子，从O（ncd）到<spanclass="math inline">\(O\left(r{\frac{n}{r}}{\frac{c}{r}}d\right)\)</span>。详细配置见附录A。请注意，如果我们将点集分割到几个局部区域，我们将限制上下文集群的接受域，并且局部区域之间没有可用的通信。</p><h2 id="讨论">3.4讨论</h2><h3id="集群的固定中心还是动态中心">3.4.1集群的固定中心还是动态中心？</h3><p>​  传统的聚类算法和超像素技术都是迭代地更新中心直到收敛。然而，当集群被用作每个构建块中的关键组件时，这将导致过高的计算成本。推理时间将呈指数级增长。在上下文集群中，我们将固定中心视为推理效率的一种替代方案，这可以被认为是准确性和速度之间的一种折衷方案。</p><h3 id="是重叠的还是不重叠的集群">3.4.2是重叠的还是不重叠的集群？</h3><p>​  我们只将点分配到一个特定的中心，这不同于以前的点云分析设计理念。我们有意地坚持传统的聚类方法（非重叠聚类），因为我们想证明简单和传统的算法可以作为一个通用的主干。尽管重叠聚类可能会产生更高的性能，但重叠聚类对我们的方法并不是必需的，而且可能会导致额外的计算负担。</p><h1 id="实验">4实验</h1><p>​  我们验证了ImageNet-1K (Deng et al., 2009), ScanObjectNN (Uy et al.,2019), MS COCO (Lin et al., 2014), 和ADE20k (Zhou et al.,2017)数据集，用于图像分类、点云分类、目标检测、实例分割和语义分割任务。<br/>​  即使我们没有追求像ConvNeXt(Liu et al., 2022) 和DaViT (Ding et al.,2022a)那样的最先进的性能，上下文集群仍然在所有任务上都显示出有希望的结果。详细的研究证明了我们的上下文集群的可解释性和泛化能力。</p><h2 id="在imagenet-1k上的图像分类">4.1在ImageNet-1K上的图像分类</h2><h2 id="聚类可视化">4.2聚类可视化</h2><h2id="扫描对象网络上的三维点云分类">4.3扫描对象网络上的三维点云分类</h2><h2id="ms-coco上的对象检测和实例分割">4.4MS-COCO上的对象检测和实例分割</h2><h2 id="ade20k上的语义分割">4.5ADE20K上的语义分割</h2><h1 id="结论">5结论</h1><p>​  我们引入了上下文集群，一种新的特征提取范式的视觉表示。受点云分析和超像素算法的启发，我们将图像视为一组无组织的点，并采用简化的聚类方法来提取特征。在图像解释和特征提取操作方面，上下文集群与ConvNets和vit有根本上的区别，并且在我们的架构中不涉及卷积或关注。我们展示了我们的上下文集群可以在多个任务和域上获得与ConvNet和ViT基线相当甚至更好的结果，而不是追求SOTA的性能。最值得注意的是，我们的方法显示出了很好的可解释性和泛化特性。我们希望我们的上下文集群除了具有卷积和注意力外，还可以作为一种新的视觉表示方法。<br/>​  正如结尾的附录3所讨论的，我们的视觉表示的新视角和设计也带来了新的挑战，主要是在准确性和速度之间的妥协。更好的策略值得探索。脱离当前的检测和分割框架，将我们的上下文聚类哲学应用于其他任务，也是一个值得追求的方向。</p><h1 id="代码结构">6代码结构</h1><h2 id="总体架构">6.1.总体架构</h2><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226101653920.png"alt="image-20250226101653920" /><figcaption aria-hidden="true">image-20250226101653920</figcaption></figure><p>​  其中forward_embeddings的作用是将位置信息嵌入，然后进行降维，forward_tokens的作用是聚类，然后基于聚类结果进行特征融合。</p><h2 id="forward_embeddings">6.2.forward_embeddings</h2><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226101849604.png"alt="image-20250226101849604" /><figcaption aria-hidden="true">image-20250226101849604</figcaption></figure><p>​  RGB三维加上横向竖向的两维，总共五维，作为输入，然后使用卷积，降低分辨率，即降低要聚类的点：由1024*1024个点降为256*256个点。</p><h2 id="forward_tokens">6.3.forward_tokens</h2><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226110412217.png"alt="image-20250226110412217" /><figcaption aria-hidden="true">image-20250226110412217</figcaption></figure><p>​  forward_tokens的作用是聚类，然后基于聚类结果进行特征融合，具体而言，其在不同的尺度上进行聚类块的操作，以保证聚类引导结果。其使用PointRecuder降低分辨率。</p><h3 id="clusterblock">6.3.1.ClusterBlock</h3><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226102347529.png"alt="image-20250226102347529" /><figcaption aria-hidden="true">image-20250226102347529</figcaption></figure><p>​  这里只展示第一个stage下的ClusterBlock模块，具体而言，首先使用Cluster模块进行聚类，与可学习的权重相乘再与原来的相加，随后使用MLP层，与可学习的权重相乘再与原来的相加。</p><h3id="clusterblock中的cluster模块">6.3.2.ClusterBlock中的Cluster模块</h3><p>​  聚类模块是这篇论文的重点，其是将一个空间的聚类结果指导另一个空间的计算，其大致如下：</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250325162851687.png"alt="image-20250325162851687" /><figcaption aria-hidden="true">image-20250325162851687</figcaption></figure><p>​  我们首先将这些点映射到一个点空间<spanclass="math inline">\(\mathbf{P}_s\)</span>和一个值空间<spanclass="math inline">\(\mathbf{P}_{\nu}\)</span>，其中<spanclass="math inline">\(\mathbf{P}_s\)</span>来进行相似性计算。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250303112600303.png"alt="image-20250303112600303" /><figcaption aria-hidden="true">image-20250303112600303</figcaption></figure><p>​  然后使用了区域划分，将点分割成64个局部区域，通过池化操作，得到初始化的中心。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250303112629918.png"alt="image-20250303112629918" /><figcaption aria-hidden="true">image-20250303112629918</figcaption></figure><p>​  然后计算局部相似度。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250303112818784.png"alt="image-20250303112818784" /><figcaption aria-hidden="true">image-20250303112818784</figcaption></figure><p>​  其中创建掩膜并应用是指如下，这样每个点就被分到一个类下。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># we use mask to sololy assign each point to one center</span><br><span class="line">sim_max, sim_max_idx = sim.max(dim=1, keepdim=True)</span><br><span class="line">mask = torch.zeros_like(sim)  # binary #[B,M,N]</span><br><span class="line">mask.scatter_(1, sim_max_idx, 1.)</span><br><span class="line">sim = sim * mask</span><br></pre></td></tr></table></figure><p>​  然后是将特征聚合，按照公式融合特征。</p><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226105153915.png"alt="image-20250226105153915" /><figcaption aria-hidden="true">image-20250226105153915</figcaption></figure><p>​  最后得到输出结果。</p><h3 id="clusterblock中的mlp模块">6.3.3.ClusterBlock中的MLP模块</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Mlp(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implementation of MLP with nn.Linear (would be slightly faster in both training and inference).</span><br><span class="line">    Input: tensor with shape [B, C, H, W]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_features, hidden_features=None,</span><br><span class="line">                 out_features=None, act_layer=nn.GELU, drop=0.):</span><br><span class="line">        super().__init__()</span><br><span class="line">        out_features = out_features or in_features</span><br><span class="line">        hidden_features = hidden_features or in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    def _init_weights(self, m):</span><br><span class="line">        if isinstance(m, nn.Linear):</span><br><span class="line">            trunc_normal_(m.weight, std=.02)</span><br><span class="line">            if m.bias is not None:</span><br><span class="line">                nn.init.constant_(m.bias, 0)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.fc1(x.permute(0, 2, 3, 1))</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x).permute(0, 3, 1, 2)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>​  MLP就是线性层、激活函数和dropout层构成。</p><h3 id="pointrecuder层">6.3.4.PointRecuder层</h3><p>​  PointRecuder是由卷积层构成，kernel_size=(3, 3), stride=(2, 2),padding=(1, 1)。</p><h3 id="各个stage下的设置">6.3.5.各个stage下的设置</h3><figure><imgsrc="../postimages/Image-as-Set-of-Points/image-20250226105756280.png"alt="image-20250226105756280" /><figcaption aria-hidden="true">image-20250226105756280</figcaption></figure><h1 id="基于此论文的改进论文">7基于此论文的改进论文</h1><h2 id="clusterfomer">7.1 ClusterFomer</h2><p>​  发表于A类会议,NeurIPS,2023，其解析为<ahref="clusterformer">clusterformer</a>，该模型包含两个创新设计：①循环交叉注意力聚类，重新定义了TransFORMER中的交叉注意力机制，通过递归更新聚类中心，促进强大的表示学习；②特征调度，利用更新后的聚类中心，通过基于相似性的度量重新分配图像特征，形成一个透明的处理流程。</p><p>​  代码层面，其改进了ClusterBlock中的Cluster模块，</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>可微深度聚类</title>
      <link href="/%E5%8F%AF%E5%BE%AE%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/"/>
      <url>/%E5%8F%AF%E5%BE%AE%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="dvlo">DVLO</h1><p>​  DVLO: Deep Visual-LiDAR Odometry with Local-to-Global FeatureFusion and Bi-Directional Structure Alignment</p><h2 id="local-fuser-module">Local Fuser Module</h2><p>​  受ContextClusters[38]的启发，它提出了一种通用的基于聚类的视觉主干将图像视为一组点，我们对其进行了扩展，并提出了一种新的基于聚类的特征融合模块（LocalFuser），没有任何CNN或transformer。该模块可以局部合并图像中更细粒度的二维纹理和每个聚类内的点的几何特征，如图3所示。</p><figure><imgsrc="../postimages/%E5%8F%AF%E5%BE%AE%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/image-20250224215352161.png"alt="image-20250224215352161" /><figcaption aria-hidden="true">image-20250224215352161</figcaption></figure><p>​  我们的基于聚类的方法也保持了高效率，其中总推理时间仅是基于注意力的方法的一半，如表7所示。</p><figure><imgsrc="../postimages/%E5%8F%AF%E5%BE%AE%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/image-20250224215442877.png"alt="image-20250224215442877" /><figcaption aria-hidden="true">image-20250224215442877</figcaption></figure><h3 id="从图像到伪点">从图像到伪点</h3><p>​  给定图像特征<span class="math inline">\(F_{I}\ \in\\mathbb{R}^{H_{I}\times W_{I}\timesC}\)</span>，我们首先将它们重塑为伪点<span class="math inline">\(F_{pp}~\in~\mathbb{R}^{M\times C}\)</span>的集合，其中<spanclass="math inline">\(M\,=\,H_{I}\,\times\,W_{I}\)</span>是伪点的个数。在这种情况下，图像具有与LiDAR点相同的数据结构，这有利于局部像素到点对点的对应关系的建立和进一步的基于聚类的特征聚合。</p><h3 id="伪点聚类">伪点聚类</h3><p>​  我们首先将激光雷达点投影到图像平面上，得到它们在图像坐标系中相应的二维坐标x‘和y’作为聚类中心。中心特征<spanclass="math inline">\(F_{c}\in\mathbb{R}^{N\timesC}\)</span>是通过基于x‘，y’的FI上的双线性插值计算出来的。然后，根据中心特征Fc和伪点特征Fpp之间的成对余弦相似性，将所有伪点划分为多个簇。在这里，我们将每个伪点分配到最相似的中心，从而得到N个簇。为了提高效率，遵循SwinTransformer[37]，我们在计算相似度时使用区域划分。</p><h3 id="本地特性聚合">本地特性聚合</h3><p>​  在[38]之后，我们根据与集群中心的相似性，动态地聚合同一集群内的所有伪点特征。给定在第i个簇中心周围包含k个伪点的簇，其局部融合特征<spanclass="math inline">\(F_{L}^{i}\in\mathbb{R}^{1\timesC}\)</span>的计算方法为： <spanclass="math display">\[F_{L}^{i}=\frac{1}{X}\left(F_{c}^{i}+\sum_{j=1}^{k}si g m o i d\left(\alpha s_{i j}+\beta\right)\cdot F_{pp}^{j}\right),\]</span></p><p><span class="math display">\[X=1+\sum_{j=1}^{k}s i g m o id\left(\alpha s_{i j}+\beta\right),\]</span></p><p>​  其中<span class="math inline">\(F_{pp}^{j}\)</span>是第j个伪点的特征。<span class="math inline">\(s_{ij}\)</span>是第j个伪点与第i个簇中心之间的相似性得分。α和β是可学习的标量来缩放和移动相似度。sigmoid（·）是一个sigmoid函数，用来重新缩放相似性到（0,1）。X是标准化因子。由于我们将激光雷达点投影到图像平面上作为簇中心，以及每个中心的聚合特征，局部融合特征<spanclass="math inline">\(F_{L}\,\in\,\mathbb{R}^{N\timesC}\)</span>与原始激光雷达点具有相同的维数。因此，我们也可以将局部融合特征FL作为HP×WP×C大小的伪图像，作为全局Fuser模块的输入。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SparseViT:Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization through Spare-Coding Transformer</title>
      <link href="/SparseViT/"/>
      <url>/SparseViT/</url>
      
        <content type="html"><![CDATA[<p>Can We Get Rid of Handcrafted Feature Extractors?<br/>SparseViT:Nonsemantics-Centered, Parameter-Efficient Image ManipulationLocalization through Spare-Coding Transformer</p><p><spanclass="math inline">\(\begin{array}{c}&lt;br/&gt;{\mathrm{Lei~Su^{1,2},~Xiaochen~Ma^{3},~Xuekang~Zhu^{1,2},~Chaoqun~Niu^{1,2,4},~Zeyu~Lei^{1,2,*}}}\\&lt;br/&gt;{\mathrm{^1~中国四川大学计算机科学学院}}\\&lt;br/&gt;{\mathrm{^2~中国教育部机器学习与产业诚信工程研究中心}}\\&lt;br/&gt;{\mathrm{^3~穆罕默德·本·扎耶德人工智能大学}}\\&lt;br/&gt;{\mathrm{^4~澳门大学计算机与信息科学系}}&lt;br/&gt;\end{array}&lt;br/&gt;\)</span></p><h1 id="摘要">摘要</h1><p>​  非语义特征或语义不可知特征与图像上下文无关，但对图像篡改敏感，被认为是图像篡改定位（IML）的证据。由于手工标签是不可能的，现有的工作依赖于手工制作的方法来提取非语义特征。手工制作的非语义特征危及了IML模型在看不见或复杂场景中的泛化能力。因此，对于IML，重要的是：<strong>如何自适应地提取非语义特征？</strong>非语义特征与上下文无关，且对篡改敏感。也就是说，在图像中，除非发生篡改，否则它们在各个补丁之间是一致的。这样，图像块之间的稀疏和离散交互足以提取非语义特征。然而，图像语义在不同块上差异很大，需要图像块之间密集和连续的交互来学习语义表示。因此，在本文中，我们提出了一个稀疏视觉Transformer（SparseViT），它将ViT中的密集、全局的自注意重新定义为稀疏、离散的方式。这种稀疏的自注意打破了图像的语义，迫使稀疏vit自适应地提取图像的非语义特征。此外，与现有的IML模型相比，稀疏自注意机制大大减少了模型的大小（最大值为80%），实现了惊人的参数效率和计算减少。大量的实验表明，在没有任何手工特征提取器的情况下，SparseViT在跨基准数据集的泛化和效率方面都具有优势。</p><p>代码-https://github.com/scu-zjz/SparseViT</p><h1 id="引言">1引言</h1><p>​  随着图像编辑工具和图像生成技术的快速发展，图像处理已经变得非常方便。为了解决这一趋势，研究人员开发了图像篡改定位（IML）技术，以识别图像中特定的篡改区域。由于篡改后不可避免地留在图像上的伪影（篡改痕迹），这些伪影可以分为语义和非语义（语义无关）特征。语义不可知的特征是指突出低级伪影信息的特征，这些信息独立于图像的语义内容。这些特征显示了图像的篡改和未篡改区域之间的分布的显著差异。(Guillaro et al. 2023)现有的主干网络(Simonyan and Zisserman 2014) (Wanget al. 2020) (Dosovitskiy etal.2020)，主要用于语义相关任务设计，可以有效地提取篡改图像的语义特征。在提取非语义特征方面，大多数现有的方法依赖于手工制作的特征提取器(Zhou et al. 2018) (Bayar and Stamm 2018) (Cozzolino and Verdoliva2019)。如表1所示，几乎所有现有的IML模型都遵循了“语义分割骨干网络”和“手工制作的非语义特征提取”的设计。</p><figure><img src="../postimages/SparseViT/image-20250219164532886.png"alt="image-20250219164532886" /><figcaption aria-hidden="true">image-20250219164532886</figcaption></figure><p>​  然而，该方法需要针对不同的非语义特征进行自定义提取策略，但在提取这些特征时缺乏适应性。因此，该方法在提高模型适应未知场景的能力方面受到了限制。与传统的手工提取非语义特征的方法不同，我们提出了一种自适应机制来提取被篡改图像中的非语义特征。我们认识到，图像的语义特征表现出很强的连续性和显著的上下文相关性（Wanget al.2018），这意味着局部语义特征在表示图像的全局语义方面往往不足。因此，局部区域之间紧密而连续的交互是构建全局语义特征的必要条件。相比之下，图像的非语义特征，如频率和噪声，对篡改高度敏感，并在图像的不同区域表现出更大的独立性。这一特性允许我们使用稀疏编码来建立非语义特征的全局交互，利用它们的敏感性来检测篡改。<br/>​  在此基础上，我们介绍了一种新的稀疏视觉变换稀疏视觉Transformer。SparseViT采用稀疏自注意机制，重新设计了ViT中密集的全局自注意，以更好地适应非语义特征的统计特性。通过稀疏处理，自我注意机制选择性地抑制了语义信息的表达，专注于捕捉与图像篡改相关的非语义特征。使用分层策略，SparseViT在不同层次上应用不同程度的稀疏度来精细地提取非语义特征。我们还设计了一个多尺度融合模块（LFF）作为解码器，集成了不同稀疏度层次提取的特征图，丰富了模型对跨多尺度非语义内容的理解，增强了其鲁棒性。这种设计使SparseViT能够专注于学习对篡改敏感的非语义特征，而忽略语义特征，允许从图像中自适应地提取非语义特征。<br/>​  据我们所知，目前还没有明确设计用于自适应提取非语义特征的模型。SparseViT可以被认为是自适应非语义特征提取的开创性工作。我们所有的实验都是在相同的评价方案下进行的。所有模型都在CAT-Net（Kwonet al.2021）数据集上进行训练，并在多个基准数据集上进行测试。我们提出的方法在几个基准数据集上展示了出色的图像处理定位能力，与其他模型相比，我们的模型获得了最好的平均性能。综上所述，我们的贡献如下：</p><ul><li>我们发现，图像中的语义特征需要连续的局部交互来构造全局语义，而非语义特征由于其局部独立性，可以通过稀疏编码实现全局交互。</li><li>基于语义特征和非语义特征的不同行为，我们提出了一种使用稀疏自注意机制来自适应地从图像中提取非语义特征。</li><li>为了解决传统的多尺度融合方法的非可学习性问题，我们引入了一种可学习的多尺度监督机制。</li><li>我们提出的SparseViT在不依赖特征提取器的情况下保持了参数效率，并在四个公共数据集上实现了最先进的（SoTA）性能和优秀的模型泛化能力。</li></ul><h1 id="相关工作">2相关工作</h1><h2 id="伪影特征提取">2.1伪影特征提取</h2><p>​  早期的图像篡改定位方法主要依赖于手工制作的卷积内核从图像中提取非语义特征。例如，BayarConv（Bayar和Stamm2018）设计了一个带有高通滤波器结构的卷积核来捕获图像中的噪声模式。RGB-N（Zhouet al.2018）引入了SRM滤波器来捕获噪声分布的差异，从而表示非语义特征。随着深度学习在各种计算机视觉和图像处理任务中的成功，许多最近的技术也采用了深度学习来解决图像处理定位问题（Zhouet al.2018）。然而，由于现有的用于语义相关任务的网络在表示非语义特征方面的局限性，目前几乎所有的篡改定位方法都依赖于语义分割骨干网络与手工制作的非语义特征提取相结合。<br/>​  例如，ManTra-Net(Wu, AbdAlmageed, and Natarajan 2019)和SPAN (Hu et al.2020)都将BayarConv和SRM作为其模型的第一层。ObjectFormer (Wang et al.2022)基于Transformer架构，另外采用手工制作的DCT模块来提取高频特征，从而更好地捕获图像中的非语义特征。TruFor(Guillaro et al. 2023)使用手工制作的Noiseprint (Cozzolino and Verdoliva2019)特征提取器，并通过对比学习，利用这些提取的特征来增强其篡改检测和定位能力。NCL(Zhouet al.2023)利用基于sobel的（Dong等人2022a）非语义特征提取器来增强其识别非语义特征的能力。各模型从操纵图像中提取非语义特征的方法如表1所示。例如，ManTra-Net（吴、阿卜杜拉·阿尔马吉德和纳塔拉扬，2019）和SPAN（Hu等人，2020）都将BayarConv和SRM作为其模型的第一层。前者（Wanget al.2022）基于变压器架构，另外采用手工制作的DCT模块来提取高频特征，从而更好地捕获图像中的非语义特征。TruFor（Guillaro等人，2023年）使用手工制作的噪声打印（科佐利诺和维多利瓦，2019年）特征提取器，并通过对比学习，利用这些提取的特征来增强其篡改检测和定位能力。NCL（Zhou等人2023）利用基于sobel的（Dong等人2022a）非语义特征提取器来增强其识别非语义特征的能力。各模型从操纵图像中提取非语义特征的方法如表1所示。</p><h2id="视觉transformer中的稀疏自注意力">2.2视觉Transformer中的稀疏自注意力</h2><p>​  该Transformer最初被提出用于处理自然语言处理（NLP）任务，并首次应用于序列数据。本文（Dosovitskiy等人，2020年）介绍了一种新的视觉Transformer（ViT）模型，为将Transformer应用于视觉领域提供了新的见解。<br/>​  自从Transformer引入视觉领域以来，对稀疏关注的研究从未停止。SwinTransformer（Liuet al.2021b）在一个分层结构中使用移动的窗口来聚集注意力。SparseTransformer（Childet al.2019）通过限制注意权值中的非零元素的数量来降低计算复杂度。ResMLP（Touvronet al. 2022）将局部连接纳入注意机制，而（Liu et al.2021a）则利用MLPs的非线性特性来取代传统的注意计算。ViViT（Arnab等人.2021）和CSWinTransformer（Dong等人.2022b）通过分解Transformer内的多头自注意，降低了计算成本，并提高了模型处理长序列的能力。ViViT将注意力分解为时间和空间计算，而CSWinTransformer将多头自注意力分成两个平行组，一个组处理水平条纹，另一个组处理垂直条纹，形成一个十字形窗口。焦点自我注意（Yanget al.2021）通过结合细粒度的局部和粗粒度的全局交互，使注意模式稀疏化。在IML领域，目前还没有提出利用稀疏注意自适应地从篡改图像中提取语义不可知信息的方法。我们的工作是在IML领域的开创性工作。</p><h1 id="方法">3.方法</h1><p>​  当前数据集中的篡改实例通常专注于移动、删除或复制整个对象等篡改。这使得现有的模型（Pun、Yuan和Bi2015）能够通过仅依赖语义特征来相对较好地识别被操纵的区域。然而，这种对语义特征的过度依赖忽略了非语义特征的重要性，限制了模型在不熟悉或复杂的篡改场景中的泛化能力。我们观察到，图像的语义信息表现出很强的连续性和上下文依赖性（Wanget al.2018），这需要全球注意机制来加强局部和全球区域之间的相互作用（Vaswani2017）。相比之下，非语义信息倾向于在局部特征和全局特征之间保持一致，并在图像的不同区域之间表现出更大的独立性（乌利亚诺夫、维达尔迪和兰皮茨基2018）。通过利用这种区别，我们可以设计一种机制，减少对语义信息的依赖，同时增强对非语义信息的捕获。<br/>​  为此，我们提出将全局注意机制分解为“稀疏注意”形式。稀疏注意，当表示图像的语义信息时，可以防止模型对其进行过拟合，允许模型更多地关注图像中的非语义信息。如图1所示，我们改进了Uniformer(Li et al.2023)中传统的注意计算，用稀疏自注意代替全局自注意，其稀疏性呈指数衰减。</p><figure><img src="../postimages/SparseViT/image-20250221114324242.png"alt="image-20250221114324242" /><figcaption aria-hidden="true">image-20250221114324242</figcaption></figure><h2 id="稀疏的自注意力">3.1稀疏的自注意力</h2><p>​  传统的深度模型侧重于检测语义对象，旨在拟合这些语义对象。因此，传统的自注意采用全局交互模式，即图像中的每个块与所有其块一起参与token-totoken的注意力计算 (Liu et al.2021b) (Yuan et al.2021)。然而，在图像篡改定位领域，这种全局交互引入了许多不相关的键值对。此外，该模型过分强调语义信息，意味着在全局交互过程中，它考虑了图像中所有斑块的特征，如颜色和形状，从而全面理解图像的整体内容。由于该模型主要关注全局交互过程中图像的整体语义结构，因此往往忽略了篡改后出现的非语义信息的局部不一致性。<br/>​  为了解决这个问题，我们建议使用稀疏注意力来代替原来的全局注意力。我们引入了一个新的架构超参数，称为“稀疏率”，缩写为“S”。给定一个输入特征图<spanclass="math inline">\(X\ \in\ \mathbb{R}^{H\times W\timesC}\)</span>，我们不关注整个<span class="math inline">\(H\timesW\)</span>特征图，而是将特征划分为形状为（<spanclass="math inline">\(S\timesS,\frac{W}{S}\times\frac{W}{S},C\)</span>）的张量块。这意味着该特征映射被分解为大小为<spanclass="math inline">\({\frac{H}{S}}\times{\frac{H}{S}}\)</span>的<spanclass="math inline">\(S\timesS\)</span>非重叠张量块，并分别在这些张量块内进行自注意力计算。</p><figure><img src="../postimages/SparseViT/image-20250221115130228.png"alt="image-20250221115130228" /><figcaption aria-hidden="true">image-20250221115130228</figcaption></figure><p>​  如图2所示，只有标记有相同颜色的张量块才能执行自注意力计算。该设计抑制了稀疏语义信息在注意块中的表达，使模型专注于提取非语义特征。此外，特征图中张量块的稀疏化消除了在篡改定位中涉及大量无关键值对的注意计算，从而减少了FLOPs。</p><h2 id="多尺度特征">3.2多尺度特征</h2><p>​  在图像篡改定位任务中，引入具有不同稀疏率的多尺度监督是至关重要的。稀疏度较小的特征映射具有丰富的语义信息，有助于模型理解图像的全局上下文和结构。相反，稀疏性较大率的特征图包含更多的非语义信息，有助于模型捕获图像细节和局部特征。多尺度监督的引入允许模型通过不同程度的抑制语义特征来自适应地提取各种非语义特征，从而提高其在不同视觉场景中的泛化能力。</p><figure><img src="../postimages/SparseViT/image-20250221114324242.png"alt="image-20250221114324242" /><figcaption aria-hidden="true">image-20250221114324242</figcaption></figure><p>​  如图1所示，我们在阶段3和阶段4的不同块中引入了不同的稀疏率。3和4阶段各块稀疏率计算方法如下：<span class="math display">\[S3_{S}^{b_{i}}=2^{(3-\frac{i}{5})},\quadi=0\dots\cdot19\]</span></p><p><spanclass="math display">\[S4_{S}^{b_{i}}=2^{(1-\frac{i}{4})},\;\;\;\;i=0\dots6\]</span></p><p>​  在这里，上标<spanclass="math inline">\(b_i\)</span>表示一个阶段内的不同层，其中每一层从0开始编号，下标S表示稀疏性。我们使用阶段3和阶段4中最后一个块在不同稀疏率下的输出作为我们的多尺度特征映射。此外，由于全局关注的稀疏性，我们可以很容易地获得多尺度信息。该方法不仅在不增加计算负担的情况下显著提高了模型的准确性和性能，而且使模型的效率和鲁棒性。</p><h2 id="轻质和有效的预测头lff">3.3轻质和有效的预测头LFF</h2><p>​  Layer scale (Touvron et al.2021)是Transformers中使用的一种技术，其中多层自注意和前馈网络通常堆叠，每一层引入一个可学习的缩放参数<spanclass="math inline">\(\gamma\)</span>。这个缩放参数可以学习不同的值，从而能够在整个网络中更有效地进行信息传输。目前，特征融合方法通常通过加法或连接等简单的操作来实现（Linet al.2017），它只提供固定的特征映射的线性聚合，而不考虑这种组合对特定对象是否最优。对于模型的最终预测，我们的目标是设计一个简单而有效的预测头。受变压器架构中的层尺度机制的启发，我们为每个特征图引入了一个可学习的参数来控制尺度比例，允许更自适应的特征融合。<br/>​  所提出的LFF（可学习特征融合，LearnableFeature Fusion）预测头由五个主要部分组成，如图3所示。</p><figure><img src="../postimages/SparseViT/image-20250221115930665.png"alt="image-20250221115930665" /><figcaption aria-hidden="true">image-20250221115930665</figcaption></figure><p>​  首先，使用LFF层将特征映射F1到F4的通道统一到512维。特征图F5和F6被上采样到原始大小的十六分之一。然后，将每个特征映射乘以其相应的<spanclass="math inline">\(\gamma\)</span>缩放参数，其初始化为一个较小的类似于1e-6的值。然后，使用另一个LFF层对所有缩放后的特征图进行求和，并将求和结果的信道维数降为1。最后对结果进行上采样，以上采样H×W×1掩模作为最终预测结果。LFF过程可以形式化如下：<span class="math display">\[F_{i}=\mathrm{Linear}(C_{i},C)(F_{i}),\quadi=1\dots4\]</span></p><p><span class="math display">\[F_{i}=\mathrm{U\!{p}s a m p le}\left({\frac{H}{16}}\times{\frac{W}{16}}\right)(F_{i}),\quadi=5,6\]</span></p><p><span class="math display">\[M_{p}=\mathrm{A dd}\left(F_{i}\times\gamma\right),\quad i=1\dots6\]</span></p><p><spanclass="math display">\[M_{p}=\mathrm{Linear}(C,1)(M_{p})\]</span></p><p><span class="math display">\[M_{p}=\mathrm{Upsample}\left(H\timesW\right)\left(M_{p}\right)\]</span></p><p>​  通过设置特征图的权值参数，该模型可以动态地调整每个特征图对融合结果的贡献，从而提高了特征融合的灵活性。通过这种简单的设计，该模型可以更好地平衡和集成多尺度特征，突出重要的特征，同时抑制不相关或冗余的特征。</p><h1 id="结果">4.结果</h1><h2 id="实验设置">4.1实验设置</h2><p>​  为了确保公平比较与现有的最先进的图像操作定位方法，我们训练模型CAT-Net(Kwon et al.2021)介绍的数据集，然后测试CASIAv1 (Dong, Wang, and Tan2013), NIST16 (Guan et al. 2019), COVERAGE (Wen et al. 2016), Columbia(Hsu and Chang 2006), 和DEF-12k (Mahfoudi et al.2019)数据集。与之前的大多数工作类似，我们使用像素级F1分数和AUC（曲线下面积）来衡量模型的性能。除非另有说明，我们使用默认阈值0.5报告结果。有关实验设置和DEF-12k数据集的详细信息，请参见附录A。</p><h2 id="消融研究">4.2消融研究</h2><p>​  为了更好地评估每个组件的性能影响，我们采用了一种增量的方法，逐步添加组件，并将它们与包含所有组件的完整模型进行比较。这种方法允许我们彻底地测量和优化我们所提出的模型的架构。我们研究了使用稀疏注意和全局注意对模型参数和浮点运算（FLOPs，floating-pointoperations）的影响。此外，我们还比较了手工设计的特征提取器和稀疏注意机制在提取非语义特征方面的能力。为了探究LFF预测头的影响，我们将其性能与LFF（Xieetal.2021）在稀疏注意引入下的MLP预测头进行了比较。这种比较不仅帮助我们评估了预测头部设计的有效性，而且还揭示了不同头部对整体模型性能的具体影响。此外，我们还比较了传统的单尺度监督与我们提出的多尺度监督方法，以研究多尺度监督的优势及其对模型性能的贡献。所有这些评估的结果都是基于对CAT-Net提出的数据集进行的训练，并在CASIAv1,NIST16, COVERAGE,Columbia和DEF-12k上进行了测试。实验结果见表2和表3。</p><h2id="稀疏注意可以有效地捕获非语义信息">4.3稀疏注意可以有效地捕获非语义信息。</h2><p>​  <img src="../postimages/SparseViT/image-20250221154647153.png"alt="image-20250221154647153" /></p><p>​  在表2中，我们比较了5个数据集的稀疏注意和全局注意的性能。此外，我们还报告了人工提取的非语义特征和稀疏注意在这5个数据集上的性能。结果一致证实了稀疏注意机制在从操纵图像中提取非语义特征方面的显著优势。我们观察到，某些手工制作的特征提取方法并没有显著提高数据集上的模型性能，在某些情况下，甚至还导致了性能下降。这提出了人工非语义特征提取有效性的问题，值得进一步研究。然而，很明显，稀疏注意机制显著提高了所有数据集的模型性能，在5个不同的数据集上实现了全面的增强。<br/>​  此外，稀疏注意的设计也显示了其在减少计算负担方面的优势。与全局注意相比，稀疏注意使模型的浮点操作减少了约15%，这在大规模图像处理任务中尤其有价值。综上所述，稀疏注意通过精确提取被操纵图像中的非语义信息，提高了模型对细微伪影的敏感性，从而显著提高了模型的泛化能力。</p><figure><img src="../postimages/SparseViT/image-20250221154928506.png"alt="image-20250221154928506" /><figcaption aria-hidden="true">image-20250221154928506</figcaption></figure><p>​  如图4所示，我们定性地证明了在稀疏化之后，该模型成功地抑制了需要密集编码和长期上下文依赖的语义特征，同时能够提取不需要密集编码的非语义特征。在附录C中，我们对稀疏注意力和手工制作的特征提取器进行了定性分析。</p><h2 id="lff的影响">4.4LFF的影响</h2><p>​  在表3中，我们报告了单尺度特征、LFF和MLP（Xie et al.2021）预测头在数据集上的性能。</p><figure><img src="../postimages/SparseViT/image-20250221170051189.png"alt="image-20250221170051189" /><figcaption aria-hidden="true">image-20250221170051189</figcaption></figure><p>​  实验结果表明，无论是使用单尺度或多尺度特征，还是采用不同的特征融合策略，CASIAv1数据集上的F1得分均具有较高的一致性。我们将这种现象归因于CASIAv1和CASIAv2来自同一个数据集，因此在CASIAv1数据集上的性能不足以反映模型的泛化能力（Maet al.2023）。进一步的分析显示，与仅使用单尺度特征相比，LFF预测头和MLP预测头在5个数据集上的平均F1得分上均有显著改善。这表明，有效的特征融合策略可以显著提高模型的图像操作检测性能。具体来说，与MLP预测头相比，LFF在平均F1方面也取得了改进，验证了可学习的特征融合在性能方面优于简单的特征添加。<br/>​  LFF的优点在于它能够自适应地学习不同特征映射之间的最优融合权值，而不仅仅是添加它们。这种学习机制允许LFF更精确地处理多尺度特征，从而更好地捕获图像中的操作痕迹。此外，多尺度特征的使用已被证明是有益的，因为它提供了不同级别的语义和非语义信息，帮助模型在各种操作条件下做出更准确的预测。</p><h2 id="最先进的比较">4.5最先进的比较</h2><p>​  为了确保评估的公平性，我们只考虑了代码在网上公开的模型。我们遵循与CAT-Net相同的协议，重新训练这些模型，并在公共数据集上测试它们。在本研究中，我们考虑了多种方法，并最终包括四种依赖于手工提取操纵图像的非语义特征的方法：ManTraNet、MVSS、CATNetv2和TruFor。此外，我们还包括了一种不使用手工特征提取的方法：PSCC-Net（Liu et al.2022）。表1对这些方法进行了简要的总结，以供参考。我们的目标是提供一个全面和公平的比较，以获得更深入的了解在图像操作定位的不同方法的性能和潜力。</p><h3 id="定位结果">4.5.1 定位结果</h3><p>​  在表4中，我们展示了各种方法在像素级定位中的性能。</p><figure><img src="../postimages/SparseViT/image-20250324114735831.png"alt="image-20250324114735831" /><figcaption aria-hidden="true">image-20250324114735831</figcaption></figure><p>​  我们的方法以其优越的平均f1分数而突出，在所有数据集上排名最好。对这些结果的详细分析表明，我们的模型优于基于手工非语义特征提取的传统方法和不依赖于手工特征的模型。我们的模型之所以在于它在特征学习和表现方面的创新。通过深入探索操纵图像的内在结构，我们的模型可以准确地捕捉到操作留下的细微痕迹。即使面对复杂多样的操作技术，它也能保持较高的检测精度。</p><h3 id="检测结果">4.5.2 检测结果</h3><p>​  我们选择了在Pixel-F1指标方面表现最好的权重参数来评估模型的AUC性能。通过分析表4中的数据，我们观察到我们的SparseViT模型在几乎所有测试数据集上都获得了最好的性能，并显示出最高的平均AUC值。这一结果表明，SparseViT模型在广泛的性能评估点上都优于现有的基线。</p><h3 id="模型大小比较">4.5.3 模型大小比较</h3><p>​  与目前最好的Trufor相比，SparseViT不仅在相同的训练数据大小（512×512像素）下获得了更好的F1和AUC性能，而且将模型大小减少了80%以上。此外，即使与使用更小的训练数据（256×256像素）的ManTraNet相比，SparseViT在减少计算负荷方面显示出显著的优势。具体数据见表5。</p><figure><img src="../postimages/SparseViT/image-20250324114856904.png"alt="image-20250324114856904" /><figcaption aria-hidden="true">image-20250324114856904</figcaption></figure><h3 id="鲁棒性分析">4.5.4 鲁棒性分析</h3><p>​  根据参考文献（Wu，AbdAlmageed和Natarajan，2019）和（Hu等人，2020），我们评估了模型对CASIAv1数据集上图像操作定位的三种常见攻击方法的鲁棒性，即JPEG压缩、高斯模糊和高斯噪声。结果如图6所示。</p><figure><img src="../postimages/SparseViT/image-20250324114957340.png"alt="image-20250324114957340" /><figcaption aria-hidden="true">image-20250324114957340</figcaption></figure><p>​  观察表明，SparseViT在抵抗这些干扰方面优于现有的最先进的模型，显示出优越的鲁棒性。</p><p>​  总的来说，与在一个公平的跨数据集评估协议下测试的现有模型相比，我们的模型达到了最先进的性能。图5定性地说明了我们的模型的一个关键优势：无论是否涉及对象级操作，我们的模型有效地利用了独立于图像语义内容的非语义特征来准确识别被操纵的区域，从而避免了与语义相关的误报。</p><h1 id="结论">5.结论</h1><p>​  依靠手工制作的方法来增强模型提取非语义特征的能力，往往会限制其在不熟悉的场景中的泛化潜力。为了超越手工方法，我们建议使用一种稀疏的自注意机制来学习非语义特征。稀疏自注意引导模型更多地关注操作敏感的非语义特征，同时抑制语义信息的表达。我们的自适应方法不仅参数高效，而且比以前的手工方法更有效，大量的实验表明SparseViT实现了SoTA性能和泛化能力。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>公式识别方法</title>
      <link href="/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/"/>
      <url>/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="一本地公式识别">一、本地公式识别</h1><h2 id="pix2tex">1.pix2tex</h2><p><ahref="https://github.com/lukas-blecher/LaTeX-OCR">lukas-blecher/LaTeX-OCR:pix2tex: Using a ViT to convert images of equations into LaTeX code.(github.com)</a></p><p>以下是PC安装的流程</p><p>首先安装pix2tex：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install &quot;pix2tex[gui]&quot;</span><br></pre></td></tr></table></figure><p>然后再安装了pix2tex的环境下的终端下输入：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">latexocr</span><br></pre></td></tr></table></figure><p>最后编写一个’run_latexocr.bat‘文件，内容为：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@echo off</span><br><span class="line">:: 打开 Anaconda Prompt (miniconda3)，激活环境并运行 latexocr</span><br><span class="line"></span><br><span class="line">:: 设置 Anaconda 安装目录</span><br><span class="line">set ANACONDA_PATH=C:\Users\zhaozw\miniconda3</span><br><span class="line"></span><br><span class="line">:: 打开 Anaconda Prompt 并激活环境</span><br><span class="line">%windir%\System32\cmd.exe /K &quot;%ANACONDA_PATH%\Scripts\activate.bat %ANACONDA_PATH% &amp;&amp; conda activate pythonStudy &amp;&amp; latexocr&quot;</span><br></pre></td></tr></table></figure><p>之后每次使用，都可以之间点击’run_latexocr.bat‘文件来打开公式识别的界面</p><h2 id="mixtex">2.MixTeX</h2><p><ahref="https://github.com/RQLuo/MixTeX-Latex-OCR/releases/tag/MixTeX-v3.2.4">ReleaseMixTeX-v3.2.4 · RQLuo/MixTeX-Latex-OCR (github.com)</a></p><p>下载<ahref="https://github.com/RQLuo/MixTeX-Latex-OCR/releases/download/MixTeX-v3.2.4/MixTeX.zip">MixTeX.zip</a></p><p>解压缩之后，直接点击exe文件，出现</p><figure><imgsrc="../postimages/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/image-20250219112042353.png"alt="image-20250219112042353" /><figcaption aria-hidden="true">image-20250219112042353</figcaption></figure><p>然后就可以自动识别截图，然后将识别结果粘贴到剪切板了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Contextrast:Contextual Contrastive Learning for Semantic Segmentation</title>
      <link href="/Contextrast/"/>
      <url>/Contextrast/</url>
      
        <content type="html"><![CDATA[<h1id="contextrast-contextual-contrastive-learning-for-semantic-segmentation">Contextrast:Contextual Contrastive Learning for Semantic Segmentation</h1><figure><img src="../postimages/Contextrast/image-20250217221621583.png"alt="image-20250217221621583" /><figcaption aria-hidden="true">image-20250217221621583</figcaption></figure><h1 id="摘要">摘要</h1><p>​  尽管在语义分割方面有了很大的改进，但由于缺乏局部/全局上下文以及它们之间的关系，挑战仍然存在。在本文中，我们提出了上下文分析法，一种基于对比学习的语义分割方法，它允许捕获局部/全局上下文并理解它们之间的关系。我们提出的方法包括两部分：a）上下文对比学习（CCL）和b）边界感知负值（BANE，boundary-awarenegative）抽样。上下文对比学习从多尺度特征聚合和特征间/内部关系中获得局部/全局上下文信息，从而获得更好的识别能力。同时，BANE采样选择沿错误预测区域边界的嵌入特征，作为对比学习的硬负样本，利用细粒度细节解决沿边界区域的分割问题。我们证明了我们的上下文结构大大提高了语义分割网络的性能，在不同的公共数据集上优于最先进的对比学习方法，如Cityscapes、CamVid、PASCALC、COCO-Stuff和ADE20K，而不增加推理过程中的计算成本。</p><h1 id="介绍">1.介绍</h1><p>​  语义分割是一种广泛应用的基本技术，包括自动驾驶、医学成像和机器人[12,13,18,32,37,40]。最近的实证研究在语义分割方面取得了显著的进展，显著地得益于广泛的数据集[1,2,9,10,29,55]的可用性。为了提高分割性能，研究人员提出了更大的深度神经网络（DNN）架构(4-8、11、14、16、19、21、24、25、27、34、35、39、42-44、46-49、51-54、56]和新的损失函数[36,38,50]。<br/>​  尽管取得了这些成就，但语义分割有时会产生不准确的分割，如图1(b)所示。</p><figure><img src="../postimages/Contextrast/image-20250217222120567.png"alt="image-20250217222120567" /><figcaption aria-hidden="true">image-20250217222120567</figcaption></figure><blockquote><p>图1.(a)Ground truth，(b)HRNet[35]的输出，和(c)我们的输出。(d)我们的上下文对比学习（CCL，contextualcontrastivelearning）概述：最后一层的代表性锚，来自较高的嵌入空间级别，被聚合到较低层的代表性锚，以封装局部和全局上下文。通过这样做，在第i层<spanclass="math inline">\(\mathbf{a}_{i}^{n}\)</span>上的第n类锚被更新为<spanclass="math inline">\(\mathbf{\hata}_{i}^{n}\)</span>（在右边，它的位置被移动），增强了每个类锚之间的区别性。(e)视觉描述我们的边界感知负值（BANE）采样（具有红色和红色边界的三角形）。我们的抽样优先选择在边缘（红色三角形）上的错误预测的特征，而不是在区域内（带有红色边框的三角形）作为负样本。每个形状都代表一个从各自的类派生出来的嵌入向量（最好用颜色来看）。</p></blockquote><p>​  它可以通过增加网络[25,27,34,42,43]的复杂性来解决；然而，这些方法需要更多的内存，并可能会降低推理速度。因此，有必要在不增加任何神经网络模块的情况下提高性能。<br/>​  由于这些原因，另一种值得注意的方法，对比学习，已经成为一个有价值的解决方案[17,41]，因为对比学习的目的是使网络在训练过程中更好地理解语义上下文。这是通过在训练阶段附加细化模块并将它们在推理上分离来实现的，以便网络模型在不增加架构复杂性的情况下保持推理速度。<br/>​  然而，以往的研究[17,41]忽略了多尺度特征的重要性，包括全局和局部背景。为了缓解这一问题，Pissas等人[31]提出了一种从多个编码器层中提取多尺度嵌入特征的方法。然而，由于多尺度和跨尺度的对比学习是独立考虑的，因此该方法并不能一致地理解不同尺度特征之间的关系。因此，它很难理解本地和全球环境之间的关系。<br/>​  为了解决上述问题，我们提出了一个有监督的对比学习框架，其中结合了两种新的语义分割方法，称为上下文统计法。首先，提出了上下文对比学习（CCL），从代表局部和全局上下文的多个编码器层中获取嵌入式特征。基于嵌入的特征，我们在每一层中定义具有代表性的锚点，它们作为每个类的语义质心。最后一层的锚比下层的锚代表更多的全局上下文。最后一层的锚点用于更新每层的锚点（图1(d)中的绿色箭头）。因此，较下层的锚点可以同时具有全局和局部上下文。因此，它使用更新的代表性锚点，即共享相同的全局上下文，一致地理解本地和全局上下文之间的关系。其次，受关注边界区域的[50]和[38]的启发，提出了边界感知负样本（BANE）采样，以沿着错误预测区域的边界对负样本进行采样（图1(e)）。它利用了采样更难的负样本和捕获细粒度细节的优点，因此所提出的方法在[20,41]训练过程中获得了更多的信息梯度。我们总结了最先进的方法和我们的方法的几个关键特性，如表1所示。</p><figure><img src="../postimages/Contextrast/image-20250217222530183.png"alt="image-20250217222530183" /><figcaption aria-hidden="true">image-20250217222530183</figcaption></figure><p>​  综上所述，本文的贡献如下：</p><ul><li>我们的Contextrast使分割模型能够从多尺度特征中捕获全局/局部上下文信息，并通过具有代表性的锚点一致地理解它们之间的关系。</li><li>我们的BANE采样能够获取信息负样本的对比学习和细粒度细节。它指导模型在训练过程中逐步关注混淆区域。</li><li>为了证明我们的上下文在语义分割中的适用性，我们验证了在各种强大的CNN模型[6,35,49]和公共数据集上的语义分割的先进性能：Cityscapes[9]，CamVid[1]，PASCAL-C [29]，COCO-Stuff [2]和ADE20K[55]，它们是在不同领域获得的。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="语义分割">2.1.语义分割</h2><p>​  语义分割是计算机视觉中的一项基本任务，它需要对图像进行像素级的对象分类。近年来，深度学习的显著进步推动了语义分割领域达到了前所未有的准确性和效率水平。首先，全连接网络（FCNs）[28]通过引入端到端密集特征学习，在语义分割方面取得了重大进展。然而，由于局部接受域狭窄，FCNs的空间和背景信息有限。<br/>​  因此，以下研究人员专注于在语义分割中捕获更好的空间和上下文信息。空间空间金字塔池（ASPP）[6]捕获不同范围的上下文信息。HRNet[35]在整个网络中维护高分辨率的表示，确保了细粒度细节的保存。为了进一步改进，引入了OCRNet[49]架构，该架构集成了对象-上下文表示，允许网络考虑场景中对象之间的关系。由于这些先进的方法利用个体图像中的上下文信息来学习辨别能力，因此全局特征辨别能力存在局限性。</p><h2 id="语义分割的对比学习">2.2.语义分割的对比学习</h2><p>​  对比学习是一种特征学习准则，其目的是最小化类内特征之间的距离，同时最大化类间特征之间的距离。语义分割[17,22,31,41]对比学习的最新进展显示了令人印象深刻的性能。<br/>​  Hu等人[17]和Wang等人[41]提出了一种完全监督设置下的语义分割方法，该方法探索全局像素关系，从多个图像中提取特征，全局正则分割嵌入空间。Wang等人[41]引入了记忆库和分割感知的负采样方法，存储大量数据来训练独特的表示，并在训练过程中获得更多的梯度贡献。Hu等人[17]引入了由正样本生成的类加权区域中心，作为对比学习的锚点。然而，仅关注正样本进行加权可能会降低模型的识别能力。此外，[17,41]除了忽略了最后一层的特征外，还忽略了多个尺度的特征，因此它们只捕获有限的局部/全局上下文以及局部和全局上下文之间的关系。<br/>​  最后，Pissas等人[31]提出了一种利用多尺度特征进行监督对比学习的方法。也就是说，研究者将对比学习应用于多尺度和跨尺度的特征。通过这样做，[31]方法从多尺度特征中获取全局/局部上下文信息，并从跨尺度特征中获取局部和全局上下文之间的关系。然而，由于多尺度和跨尺度的对比学习是分开操作的，因此它不能一致地掌握跨特征尺度的关系。在某些情况下，特征在多尺度和跨尺度对比学习中可以有不同的排列。例如，在跨尺度对比学习中，通过多尺度对比学习改变的特征可能会发生不同的变化。</p><h1id="contextrast使用bane抽样的上下文对比学习">3.Contextrast：使用BANE抽样的上下文对比学习</h1><h2 id="总体框架">3.1.总体框架</h2><p>​  如图2所示，我们提出了一个包含两种新的语义分割方法的监督对比学习框架。</p><figure><img src="../postimages/Contextrast/image-20250818215507816.png"alt="image-20250818215507816" /><figcaption aria-hidden="true">image-20250818215507816</figcaption></figure><blockquote><p>图2.Contextrast框架整体架构。该框架采用由语义丰富的代表性锚点向量集AI更新的代表性锚点，整合局部与全局上下文及其关联关系。BAE采样算法通过在预测误差区域边界处采样样本，获取更具信息量的负样本并捕捉对比学习所需的细粒度特征细节。IBatch表示批量图像数据，Yˆ为模型预测结果，Fi是第i层编码器的特征图，Vi是编码函数π（·）生成的第i组嵌入特征向量，Ai代表第i个嵌入特征向量的代表性锚点。更新后的代表性锚点Aˆ_i由低层级与高层级锚点叠加生成，权重参数wh和wl用于优化代表性锚点。LPA是提出的像素-锚点损失函数，LCE为交叉熵损失函数。各语义类别的特征以不同形状和颜色呈现（最佳效果需彩色查看）。</p></blockquote><p>​  首先，我们提出了一个代表性锚点的概念，它是多尺度感知的显著特征，通过利用层次设计隐式地表示类，如章节3.2所述。其次，我们故意对被错误预测为负样本的区域内边界对应的特征进行采样，如章节3.3所述。</p><h2 id="上下文对比学习ccl">3.2.上下文对比学习（CCL）</h2><p>​  我们假设编码器由<spanclass="math inline">\(I\)</span>层组成。然后，我们首先将第i个编码器层对应的代表性锚点表示为<spanclass="math inline">\(A_i\)</span>，其中i∈{1，···，I}。<spanclass="math inline">\(A_i\)</span>由N个类级别代表性锚组成。第n类的每个锚用<spanclass="math inline">\(\mathbf{a}_{i}^{n}\in\mathbb{R}^{d}.\)</span>表示，定义为批图像中ground-truth语义类嵌入特征向量的平均值如下： <spanclass="math display">\[\mathbf{a}_i^n=\frac{\sum_{\mathbf{v}\in\mathbf{V}_i}\mathbf{v}\mathbb{1}[g(\mathbf{v})=n]}{\sum_{\mathbf{v}\in\mathbf{V}_i}\mathbb{1}[g(\mathbf{v})=n]},n=1,2,...,N,\]</span>​  其中<spanclass="math inline">\(\mathbf{V}_i\)</span>是基于第i个编码器层<spanclass="math inline">\({\textbf{f}}\in{\textbf{F}}_{i}\)</span>特征映射特征的嵌入特征向量集，如图2所示，即<spanclass="math inline">\(\mathbf{v}=\pi(\mathbf{f})\)</span>；<spanclass="math inline">\(g(\cdot)\)</span>表示一个函数，返回每个嵌入特征向量的ground-truth语义标签；<spanclass="math inline">\(\mathbb{1}[\cdot]\)</span>是艾弗森括号，如果条件满足则输出一个，则为零。通过使用等式(1)，<spanclass="math inline">\(A_i\)</span>表示为<spanclass="math inline">\(\mathbf{A}_{i}=\{\mathbf{a}_{i}^{1},\mathbf{a}_{i}^{2},...,\mathbf{a}_{i}^{N}\}\)</span>。为了方便起见，我们以矩阵形式来表示<spanclass="math inline">\(A_i\)</span>，即<spanclass="math inline">\(\mathbf{A}_{i}=[\mathbf{a}_{i}^{1}\,\mathbf{a}_{i}^{2}\,\dots\,\mathbf{a}_{i}^{N}]\in\mathbb{R}^{d\timesN}\)</span>。<br/>​  然后，用最后一层<spanclass="math inline">\(A_I\)</span>的代表性锚点更新低级锚点<spanclass="math inline">\(A_i\)</span>，以封装高级和低级上下文，从而考虑多尺度。因此，更新后的代表性锚点<spanclass="math inline">\(\hat A_i\)</span>被定义为<spanclass="math inline">\(\hat{\bf A}_{i}=w_{l}{\bf A}_{i}+w_{h}{\bfA}_{I}=\{\hat{\bf a}_{i}^{1},\hat{\bf a}_{i}^{2},\ldots,\hat{\bfa}_{i}^{N}\}\)</span>，其中，wl和wh是锚点更新的权重超参数（见图2）。通过更新低级锚，<spanclass="math inline">\(\hatA_i\)</span>可以作为捕获不同尺度的关系的标准。当<spanclass="math inline">\(i=I\)</span>时，<spanclass="math inline">\(A_I\)</span>被定义为<spanclass="math inline">\(\hat A_I=A_I\)</span>。 <spanclass="math display">\[L_{\mathrm{NCE}}=\frac{-1}{|{\bfV}_{+}|}\sum_{\bf v_+\in\bf V_+}\log\frac{\exp({\bf v}\cdot{\bfv}_+/\tau)}{\exp({\bf v}\cdot\bf v_+/\tau)+\sum_{\bf v_-}\exp({\bfv}\cdot\bf v_-/\tau)},\]</span> ​  接下来，我们在等式(2)中将<spanclass="math inline">\(\hat A_i\)</span>加入InfoNCE[15,30]损失，其称为像素锚点（PA，pixel-anchor）损失，如下： <spanclass="math display">\[L_{\mathrm{PA}}=\sum_{i=1}^{I}\lambda_{i}\biggl[{\frac{1}{N}}\sum_{\hat{\bf{a}}_{i}^{n}\in\hat{\bf{A}}_{i}}\frac{-1}{|{\bfV}_{+}|}\sum_{\bf v_+\in\bf V_+}L_a\biggr]\]</span></p><p><spanclass="math display">\[L_a=\log\frac{\exp(\hat{\mathbf{a}}_i^n\cdot\mathbf{v}_+/\tau)}{\exp(\hat{\mathbf{a}}_i^n\cdot\mathbf{v}_+/\tau)+\sum_{\mathbf{v}_-\in\mathbf{V}_-}\exp(\hat{\mathbf{a}}_i^n\cdot\mathbf{v}_-/\tau)}\]</span></p><p>​  其中，v+/−分别表示正样本和负样本，λi表示第i个编码器层像素锚对比损失的权重超参数。而等式(2)中的锚点是个体特征的集，等式(4)中的锚点是代表性锚点的集合<spanclass="math inline">\(\hatA_i\)</span>。像素锚损失的目的是通过最小化类内特征与其对应的代表性锚之间的距离来优化嵌入特征，同时最大限度地提高类间特征与其对应的代表性锚之间的分离。因此，该网络以具有代表性的锚点为标准，从多尺度特征及其连接中获取全局背景和复杂的细节。<br/>​  此外，像素-锚定损失与传统的像素级交叉熵损失LCE[35]相结合，提供了一种提高分割性能的补充方法。这种协同作用是有目的的：像素级交叉熵损失的目的是预测每个样本的正确标签，而像素级锚定损失的目的是通过考虑不同样本之间的关系来学习良好的数据表示。<br/>​  因此，整个框架的主要目标是优化以下损失：<span class="math display">\[L=L_{\mathrm{CE}}+\alphaL_{\mathrm{PA}},\]</span> ​  其中，α表示像素锚定损失的权重。</p><h2 id="边界感知负值bane采样">3.3.边界感知负值（BANE）采样</h2><p>​  在增强损失函数的同时，我们还提出了一种有效的负值抽样方法，即考虑预测误差的边界，以提高等式(4)中v−的质量。为此，我们采用了一种简单而有效的从<spanclass="math inline">\(\hatY\)</span>提取中边界的方法。该方法主要包括三个步骤，如图3所示：</p><figure><img src="../postimages/Contextrast/image-20250217230323141.png"alt="image-20250217230323141" /><figcaption aria-hidden="true">image-20250217230323141</figcaption></figure><p>​  1)将预测输出分解为类级二值误差图，2)基于类级误差图进行距离变换，3)选择负样本。<br/>​  为了提取负样本，每个像素（u，v）的类二进制误差映射Bni定义为： <span class="math display">\[{\bf B}_{\i}^{n}(u,v)=\mathbb{1}[(\hat{y}_{i}\neq{n})\wedge(g(\hat{y}_{i})={n})],\]</span>​  其中，<spanclass="math inline">\(\hat{y}_{i}\)</span>表示从最后一层的预测类降采样的第i层的预测类。g（·）表示与3.2节中相同的标记函数。对于错误预测的像素，<spanclass="math inline">\({\bfB}_{i}^{n}\)</span>的值为1，即为负样本，否则为零，如图3(a)所示。<br/>​  接下来，通过距离变换[23]将<spanclass="math inline">\({\bf B}_{i}^{n}\)</span>的转换为类向距离映射<spanclass="math inline">\({\bf D}_{i}^{n}\)</span>的。<spanclass="math inline">\({\bfD}_{i}^{n}\)</span>的像素值是像素（u，v）与边缘像素<spanclass="math inline">\({\bfE}_{i}^{n}\)</span>之间的最小距离，来自对应的类误差映射，定义如下：<span class="math display">\[{\bfD}_{i}^{n}(u,v)=\operatorname*{min}_{(x,y)\in{\bfE}_{i}^{n}}\sqrt{(u-x)^{2}+(v-y)^{2}}.\]</span>​  这意味着在错误预测的区域内，<span class="math inline">\({\bfB}_{i}^{n}\)</span>的像素值为1（图3(a)中的白色区域），值越低表示像素在边界上的概率越高。<br/>​  最后，在<spanclass="math inline">\({\bfB}_{i}^{n}\)</span>中值为1的区域中，我们选择对应于<spanclass="math inline">\({\bfD}_{i}^{n}\)</span>中最小距离较低的前百分之K的嵌入向量作为等式(4)中第n个代表锚点的负样本。通过将这些向量纳入到等式(3)中，Contextrast允许这些向量接近真实类别的锚点，而远离在训练过程中被错误预测类别的特征。因此，这些具有边界感知能力的负样本有助于网络更好地学习分割类之间的空间间关系。</p><h1 id="实验">4.实验</h1><h2 id="实验设置">4.1. 实验设置</h2><p>​  <strong>数据集</strong><br/>​  我们使用五个公共数据集进行实验：Cityscapes[9]、ADE20K[55]、PASCAL-C [29]、COCO-Stuff [2]和CamVid[1]数据集。为了进行公平的比较，我们遵循数据集现有的训练和验证设置（细节在补充材料中解释）。其中，由于城市景观数据集通过使用测试数据另外提供了公共基准测试，因此我们使用后缀测试来区分验证集和测试集，即Cityscapes测试。</p><p>​  <strong>训练设置。</strong><br/>​  为了证明我们提出的方法的有效性，我们采用了三种网络：a) DeepLabV3 [6]，b) HRNet [35]，和c) OCRNet[49]。DeepLabV3中使用了D-ResNet-101主干网络。HRNet和OCRNet网络采用HRNetV2-W48主干网络。我们使用相同的超参数，并在ImageNet[10]上使用预先训练的权值初始化网络，而其余的层被随机初始化。我们使用颜色抖动、水平翻转和随机缩放来增强数据。采用随机梯度下降（SGD）作为CNN骨干的优化器，动量为0.9。此外，采用多项式退火策略[6]来调度学习速率，它乘以<spanclass="math inline">\(\left(1-\frac{Iteration }{Total\iterations}\right)^{0.9}\)</span>。<spanclass="math inline">\(\lambda_{4\rightarrow1}\)</span>被设置为1.0、0.7、0.4和0.1。α被设置为0.1。在Cityscapes数据集上，我们为40K次迭代设置了8的批处理大小，并从1024×2048裁剪到512×1024。该模型在CamVid数据集上训练，批大小为16，进行6K迭代。在ADE20K上，模型被训练的作物大小为512×512，批处理大小为12，80K迭代。在COCO-Stuff和PASCAL-C上，模型以512×512和16的批处理大小进行训练。请注意，我们不使用任何额外的训练数据。</p><p>​  <strong>测试设置。</strong><br/>​  我们遵循一般设置[6,35,49]，对CamVid、COCO-Stuff、ADE20K和PASCAL-C数据集在多个尺度上平均分割结果。缩放因子从0.75设置到2.0，间隔为0.25。我们采用单尺度的城市景观评价来遵循多/跨尺度对比学习[31]的实验设置。</p><p>​  <strong>评价指标。</strong><br/>​  在实验中，我们定量分析了a)语义分割结果和b)特征特征的性能。<br/>​  为了评价语义分割的性能，使用类并联合（mIoU）[17,41]的平均值作为评价度量。对于城市景观测试，一个实例级的交叉过联合度量（iIoU）[9]也被用来评估单个实例如何被很好地分割。这是因为mIoU可能会偏向于覆盖街景中较大图像区域的对象实例。iIoU的定义如下：<spanclass="math display">\[\mathrm{i}\mathrm{Io}\mathrm{U}=\frac{\mathrm{i}\mathrm{TP}}{\mathrm{(iTP+FP+iFN)}}\]</span>​  其中，iTP、FP、iFN分别表示真阳性、假阳性和假阴性像素的数量。请注意，iTP和iFN是根据每个类的平均实例大小与相应的地面真实实例大小的比值，用加权像素贡献来计算的。接下来，特征级分析，我们采用以下三个指标：类内对齐(A)评估类内特征紧密聚集，类间一致性(U)评估多远的质来自不同类的特征分离嵌入空间，和类间社区均匀性（Ul）测量l的分离，这表明如何清楚l最近的质心之间的决策边界的区别。更多的细节可以在[26]中找到(见第4.3节)。</p><h2 id="语义分割性能">4.2.语义分割性能</h2><h2 id="特征级别的深入分析">4.3.特征级别的深入分析</h2><h2 id="消融研究">4.4.消融研究</h2><h2 id="结论">4.5.结论</h2><p>​  在本文中，我们提出了一种新的边界感知对比学习的语义分割，称为Contextrast。通过利用多尺度的上下文对比学习，我们使网络能够捕获局部/全局上下文信息，并始终理解它们之间的关系。特别是，我们证明了我们的BANE抽样通过在对比学习阶段提供更困难的BANE负样本，显著增加了mIoU。因此，与在公共数据集上的其他对比学习方法相比，我们的方法取得了很好的结果。</p><h1 id="补充材料">5.补充材料</h1><h2id="详细说明实验设置和实验数据集">5.1.详细说明实验设置和实验数据集</h2><p>​  实验设置的详细情况见表1。这些数据集的细节描述如下。</p><ul><li>Cityscapes[3]包括来自德国50个城市的图像，包括农村和城市环境。它包含5000张具有2975张训练图像的图像，500张验证图像和1525张具有19个语义类的测试图像。</li><li>ADE20K[10]有描绘各种场景的图像，包括室内和室外环境。与专注于自动驾驶等特定领域的数据集不同，ADE20K包含了不同的场景，如卧室、办公室、公园等。它包括20,210个训练图像和2,000个验证图像，其中包含150个语义类。</li><li>PASCAL-C[5]包含4998个训练图像和5105个测试图像，包含59个语义类。它还包括室内和室外的环境。</li><li>COCO-Stuff[2]拥有9000张火车和1000张测试图像。它提供了80个对象类和91个东西类。</li><li>CamVid[1]包含367个训练图像、101个验证图像和233个包含11个语义类的测试图像。</li></ul><h2 id="特征级别分析指标的详细说明">5.2.特征级别分析指标的详细说明</h2><p>​  我们采用了来自[4]的评价指标，表示为对齐、均匀性和邻域均匀性。类内对齐，记为A，表示类内特征的收敛程度，定义如下：<spanclass="math display">\[\mathrm{A}={\frac{1}{N}}\sum_{i=1}^{N}{\frac{1}{|V_{i}|^{2}}}\sum_{v_{j},v_{k}\inV_{i}}||v_{j}-v_{k}||_{2},\]</span>​  其中，N、i、Vi分别表示语义类的数量、第i个语义类和第i个语义类的特征集。通过这样做，等式(1)表示类内特征在到达分割头之前聚集的紧密程度。类内特征的有效聚类意味着提高了识别能力。<br/>​  类间均匀性，记为U，表示类间特征中心在特征空间中的分离程度，定义如下：<spanclass="math display">\[\mathrm{U}=\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{j=1,j\neqi}^{N}||\mu_{i}-\mu_{j}||_{2},\]</span>​  其中，N和μi分别表示语义类的数量和第i个语义类的中心。<br/>​  最后，邻域均匀性记为Ul，测量类间特征的l个最近中心的分离。邻域一致性的定义如下：<span class="math display">\[U_{l}=\frac{1}{Nl}\sum_{i=1}^{N}\operatorname*{min}_{j_{1},\cdots,j_{l}}\left(\sum_{j=1,j\not=i}^{l}||\mu_{i}-\mu_{j}||_{2}\right).\]</span>​  均匀性和邻域均匀性都暗示了该模型定义类间特征之间的决策边界。因此，对齐A、均匀性U和邻域均匀性Ul代表了模型区分类内和类间特征的能力。</p><h2 id="损失函数的梯度">5.3.损失函数的梯度</h2><p>​  在本节中，我们证明对比学习中的硬负样本在训练过程中带来更多的梯度贡献。提出的损失函数如下：<span class="math display">\[\begin{align*}L_i = \frac{1}{N}\sum_{\hat{a}_i^n \in \hat{A}_i} \left| \frac{1}{V_+} \right| \sum_{v_+\in V_+} L_{\hat{a}},\end{align*}\]</span></p><p><span class="math display">\[\begin{align*}L_{\mathbf{a}} = -\log\frac{\exp \left( \hat{\mathbf{a}}_i^n \cdot \mathbf{v}_+ /{\tau}\right)}{\exp \left( \hat{\mathbf{a}}_i^n \cdot \mathbf{v}_+ /{\tau}\right) + \sum_{\mathbf{v}_- \in \mathbf{V}_-} \exp \left(\hat{\mathbf{a}}_i^n \cdot \mathbf{v}_- /{\tau}\right)}\end{align*}\]</span></p><p>​  然后，得到了<span class="math inline">\(L_{i}\)</span>对锚点<spanclass="math inline">\(\hat{\bf{a}}_{i}^{n}\)</span>的导数如下： <spanclass="math display">\[\begin{align*}\frac{\partial L_i}{\partial\dot{a}_i^n} = \frac{-1}{\tau N | V_+|} \sum_{\dot{a}_i^n \in \dot{A}_i}\sum_{v_+ \in V_+} \left((1-p_+) \cdot v_+ - \sum_{v_- \in V_-} p_- v_-\right),\end{align*}\]</span> ​  其中，<spanclass="math inline">\(p_{+/-}=\frac{\exp{\hat{\bf{a}}_{i}^{n}\cdotv_{+/-}/\tau}}{\sum_{v\in V}\hat{\bf{a}}_{i}^{n}\cdotv/\tau}\)</span>表示锚点与样本之间的匹配概率。因此，一旦我们通过BANE抽样对较硬的负样本进行采样，锚点<spanclass="math inline">\({\bf{a}}_{i}^{n}\)</span>和负样本v−之间的点积接近于1。因此，负p−的匹配概率增加了。因此，当负样本较硬时，损失函数的梯度增大。</p><h2 id="其他定量结果">5.4.其他定量结果</h2><h2 id="其他定性结果">5.5.其他定性结果</h2><h2 id="特征级分析的定性比较">5.6.特征级分析的定性比较</h2><h2 id="额外的消融研究">5.7.额外的消融研究</h2><h2 id="限制性分析">5.8.限制性分析</h2><p>​  本文提出了在层次结构中利用代表性锚点的上下文变换法。因此，它允许在每个层中共享高级特性的全局上下文。它在公共数据集上主要取得了最先进的性能，但COCO-Stuff[2]和PASCAL-C [5]的改进没有CamVid [1]、Cityscapes[3]和ADE20K[10]那么大。我们认为，在最后一层有广义的代表性锚点是有限制的，因为一些数据集在场景中有这么多不同的类；上下文法对每个类只有有限的特性，训练批处理大小有限。在未来，我们计划进一步研究如何在不增加训练批规模的情况下，在许多数据集中推广代表性锚点。</p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Clustering for Unsupervised Learning of Visual Features</title>
      <link href="/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/"/>
      <url>/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/</url>
      
        <content type="html"><![CDATA[<p>Deep Clustering for Unsupervised Learning of Visual Features</p><p>Xiaohang Zhan∗ 1 , Jiahao Xie∗2 , Ziwei Liu1 , Yew Soon Ong2,3, ChenChange Loy2 1CUHK - SenseTime Joint Lab, The Chinese University of HongKong 2Nanyang Technological University 3AI3, A*STAR, Singapore</p><p>1 {zx017, zwliu}<span class="citation"data-cites="ie.cuhk.edu.hk">@ie.cuhk.edu.hk</span> 2 {jiahao003,asysong, ccloy}<span class="citation"data-cites="ntu.edu.sg">@ntu.edu.sg</span></p><h1 id="摘要">摘要</h1><p>​  联合聚类和特征学习方法在无监督表示学习中表现出了显著的性能。然而，在特征聚类和网络参数更新之间交替的训练计划会导致视觉表示的学习不稳定。为了克服这一挑战，我们提出了在线深度聚类（ODC），它同时执行聚类和网络更新，而不是交替执行。我们的关键观点是，聚类质心应该稳定地进化，以保持分类器的稳定更新。具体来说，我们设计并维护了两个动态内存模块，即用于存储样本标签和特征的样本内存，以及用于质心进化的质心内存。我们将突然的全局聚类分解为稳定的内存更新和批量标签重新分配。该过程被集成到网络更新迭代中。通过这种方式，标签和网络并肩发展，而不是交替发展。大量实验表明，ODC稳定了训练过程，有效地提高了性能。</p><h1 id="介绍">1.介绍</h1><p>​  无监督表示学习[1,2,3,4,5,6,7,8,9]的目的是学习可转移的图像或视频表示，而无需手动注释。其中，基于聚类的表示学习方法[10,11,12,13,14]成为这一领域的一个很有前途的方向。与基于恢复的方法[2,3,4,8]不同，基于聚类的方法只需要很少的领域知识[13]，同时获得了令人鼓舞的性能。与仅捕获图像内不变性的对比性表示学习[15,16,17]相比，基于聚类的方法能够探索图像间的相似性。与传统的通常在固定特征[18,19]上执行的聚类不同，这些工作共同优化聚类和特征学习。<br/>​  虽然早期工作[11,12]的评估大多是在小数据集上进行的，但Caron等人提出的DeepClustering[13]（DC）是首次尝试扩大基于聚类的表示学习。DC在深度特征聚类和CNN参数更新之间交替进行。特别是，在每个历元开始时，它对整个数据集执行离线聚类算法，以获得伪标签作为下一个历元的监督。离线聚类不可避免地会在不同的时代排列所分配的标签，即即使某些聚类没有变化，聚类后的索引也会随机排列。因此，分类器中的参数不能从上一个历元继承，它们必须在每个历元之前进行随机初始化。该机制引入了训练的不稳定性，并使表象暴露在表象腐败的高风险之中。如图1(a)所示，DC中的网络更新在每个时期都被特征提取和聚类中断。这与传统的监督分类不同，后者使用固定的标签，其迭代由网络的前向和向后传播组成。</p><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217152428929.png"alt="image-20250217152428929" /><figcaption aria-hidden="true">image-20250217152428929</figcaption></figure><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217152524157.png"alt="image-20250217152524157" /><figcaption aria-hidden="true">image-20250217152524157</figcaption></figure><p>​  在这项工作中，我们寻求设计一个联合聚类和高稳定性的特征学习范式。为了减少DC和监督学习之间的训练机制的差异，我们将聚类过程分解为小批量的标签更新，并将该更新过程集成到网络更新的迭代中。基于这种直觉，我们提出了在线深度聚类（ODC，OnlineDeepClustering）的联合聚类和特征学习。具体来说，ODC迭代包括正向和向后传播、标签重新分配和质心更新。对于标签更新，ODC在前向传播中重用了这些特征，从而避免了额外的特征提取。为了方便在线标签重新分配和质心更新，我们设计并维护了两个动态内存模块，即样本存储以存储样本的标签和特征，质心内存用于质心进化。通过这种方式，ODC以一种类似于监督分类的不间断方式进行训练，而不需要手动注释。在训练过程中，标签和网络参数是肩并肩的，而不是交替的。由于标签在每次迭代中都不断即时更新，CNN中的分类器也更加稳定，从而产生更稳定的损失曲线，如图1(b)所示。</p><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217153059962.png"alt="image-20250217153059962" /><figcaption aria-hidden="true">image-20250217153059962</figcaption></figure><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217153117089.png"alt="image-20250217153117089" /><figcaption aria-hidden="true">image-20250217153117089</figcaption></figure><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217153130638.png"alt="image-20250217153130638" /><figcaption aria-hidden="true">image-20250217153130638</figcaption></figure><p>​  虽然ODC单独在各种基准测试上实现了引人注目的无监督表示学习性能，但它可以自然地用于对使用其他无监督学习方法训练过的模型进行微调。大量的实验表明，ODC的稳定性帮助它作为一个无监督的微调工具，执行优于DC。我们总结出了我们的贡献如下：</p><ol type="1"><li>我们提出了ODC，以一种无监督的方式学习图像表示与高稳定性。<br/>2.ODC也作为一个统一的无监督微调方案，进一步改进了以前的自监督表示学习方法。<br/>3.在不同的基准上观察到良好的性能，表明联合聚类和特征学习的巨大潜力。</li></ol><h1 id="相关工作">2.相关工作</h1><h1 id="方法论">3.方法论</h1><p>​  在下面的小节中，我们首先在3.1节讨论在秒中提出的ODC与传统的DC[13]之间的区别。然后，在3.2节，我们推荐一些有用的策略在使用ODC时保持稳定的集群大小。最后，我们解释如何使用ODC进行无监督微调(在3.3节)和ODC的实现细节(在3.4节)。</p><h2 id="在线深度聚类">3.1.在线深度聚类</h2><p>​  我们首先讨论了DC[13]的基本思想，然后详细介绍了所提出的ODC。为了学习表示方法，DC在离线特征聚类和使用伪标签的网络反向传播之间交替进行。离线聚类过程需要对整个训练集进行深度特征提取，然后采用全局聚类算法，如K-Means聚类。全局聚类使伪标签的排列幅度较大，要求网络在随后的时代快速适应新的标签。</p><h3 id="框架概述">3.1.1.框架概述</h3><p>​  与DC不同，ODC不需要额外的特征提取过程。此外，标签也会随着网络参数的更新而平稳地演化。这是由于新引入的采样和质心记忆而实现的。</p><figure><imgsrc="../postimages/Deep-Clustering-for-Unsupervised-Learning-of-Visual-Features/image-20250217154520065.png"alt="image-20250217154520065" /><figcaption aria-hidden="true">image-20250217154520065</figcaption></figure><p>​  如图2所示，样本存储器存储了整个数据集的特征和伪标签，而质心存储器存储了类质心的特征，即一个类中所有样本的平均特征。这里的“类”表示在训练过程中不断发展的临时集群。在ODC的不间断迭代期间，标签和网络参数同时更新。引入了特定的技术，包括损失重新加权和处理小集群，以避免ODC陷入琐碎的解决方案。</p><h3 id="odc迭代">3.1.2.ODC迭代</h3><p>​  假设我们有一个随机初始化的网络<spanclass="math inline">\(f_{\theta}\left(*\right)\)</span>和一个线性分类器<spanclass="math inline">\(g_{w}\left(*\right)\)</span>，其目标是训练主干参数θ来产生高度判别的表示。为了准备ODC，样本和质心记忆通过一个全局聚类过程进行初始化，例如，K-Means。接下来，我们可以迭代地执行不间断的ODC。<br/>​  一个ODC迭代包含四个步骤。首先，给定一批输入图像{x}，网络将图像映射到紧凑的特征向量<spanclass="math inline">\(F=f_{\theta}(x)\)</span>中。其次，我们从样本内存中读取这批处理的伪标签。利用伪标签，我们用随机梯度下降法更新网络，以解决以下问题：<spanclass="math display">\[\operatorname*{min}_{\theta,w}\frac{1}{B}\sum_{n=1}^{B}l\left(g_{w}\left(f_{\theta}\left(x_{n}\right)\right),y_{n}\right),\]</span>​  其中yn是样本内存中的当前伪标签，B表示每个小批的大小。第三，重用L2归一化后的<spanclass="math inline">\(f_{\theta}(x)\)</span>来更新样本内存： <spanclass="math display">\[F_{m}\left(x\right)\leftarrowm\frac{f_{\theta}\left(x\right)}{\left|\left|f_{\theta}\left(x\right)\right|\right|_{2}}+\left(1-m\right)F_{m}\left(x\right),\]</span>​  其中Fm(x)为样本内存中x在存储器中的特征，m∈(0,1]为动量系数。同时，通过找到最近的质心，为每个涉及的样本分配一个新的标签：<spanclass="math display">\[\operatorname*{min}_{y\in\left\{1,..,C\right\}}\|F_{m}\left(x\right)-C_{y}\|_{2}^{2}\]</span>​  其中，Cy表示y类的质心特征。最后，记录所涉及的中心，包括新成员加入的中心和旧成员离开的中心。通过平均属于相应质心的所有样本的特征，每k次迭代都更新。</p><h2 id="处理odc中的集群分布">3.2.处理ODC中的集群分布</h2><h3 id="损失重新加权">3.2.1.损失重新加权</h3><p>​  为了避免训练崩溃成几个巨大的簇，DC在每个纪元之前采用均匀采样。然而，对于ODC，集群上的样本数量在每次迭代中都会发生变化。使用统一采样需要在每次迭代中重新采样整个数据集，这一过程被认为是冗余的和昂贵的。我们提出了另一种方法，即根据每个类中的样本数量来重新加权损失。为了验证它们的等价性，我们实现了一个具有损失重新加权的直流模型，并通过经验发现，当权重遵循<spanclass="math inline">\(w_{c}\ \propto\{\frac{1}{\sqrt{N_{c}}}}\)</span>时，性能保持不变，其中Nc表示第c类中的样本数量。因此，我们对ODC采用相同的损失重加权公式。随着损失重加权，小簇中的样本对反向传播的贡献更大，从而推动决策边界进一步接受更多的潜在样本。</p><h3 id="处理小集群">3.2.2.处理小集群</h3><p>​  损失重新加权有助于防止巨大的簇的形成。然而，我们仍然面临着有一些小集群崩溃成空集群的风险。为了克服这个问题，我们建议在非常小的集群崩溃之前预先处理和消除它们。将正常簇表示大小大于阈值的Cn，小簇表示大小不大于的C，对于∈，我们首先将c中的样本分配给Cn中最近的质心，使c为空。接下来，我们用k-均值将最大的簇cmax∈Cn分成两个子簇，并随机选择其中一个子簇作为新的c。我们重复这个过程，直到所有的集群都属于Cn。尽管这个过程突然改变了一些集群，但这只影响参与这一过程的一小部分样品。</p><h3 id="尺寸减少">3.2.3.尺寸减少</h3><p>​  一些主干网络将图像映射到高维向量，例如，AlexNet产生4096维特征，ResNet-50产生2048维特征，导致后续聚类中较高的空间和时间复杂性。DC对整个数据集的特征进行了主成分分析，以减少维度。然而，对于ODC，不同样本的特征有不同的时间戳，导致样本之间的统计数据不兼容。因此，PCA已不再适用。在每次迭代中执行PCA的成本也很高。因此，我们添加了一个非线性的头层，以将高维特征减少到256维。它是在ODC迭代期间联合调优的。为下游任务删除头部层。</p><h2 id="odc用于无监督的微调">3.3.ODC用于无监督的微调</h2><p>​  与倾向于捕获图像内语义的自监督学习方法相比，基于聚类的方法更多地关注图像间的信息。因此，DC和ODC自然地是对以前的自我监督学习方法的补充。由于DC和ODC并不局限于一个专门设计的目标，如旋转角度或颜色预测，它们很容易作为一种无监督的微调方案来提高现有的自监督方法的性能。在本文中，我们研究了DC和ODC作为一个从不同的自监督学习方法中初始化的微调过程的有效性。</p><h2 id="实施细节">3.4.实施细节</h2><p>​  <strong>数据预处理。</strong><br/>​  我们使用ImageNet，它包含128万张没有标签的图像进行训练。图像首先被随机裁剪，使其分辨率为224x224，其增强功能包括随机翻转和旋转（±2◦）。DC对图像采用Sobel滤波器，避免利用颜色作为快捷方式。这样的预处理步骤要求下游任务也包括Sobel层，这可能会限制其应用程序。我们发现，强烈的颜色抖动显示出与Sobel过滤器相同的效果，而它允许正常的RGB图像作为输入。具体来说，我们采用PyTorch风格的颜色抖动变换，包括亮度因子（0.6、1.4）、对比度系数（0.6、1.4）、饱和度因子（0、2）和色调因子（−0.5、0.5）。此外，我们将图像随机转换为灰度，概率为0.2。应用于训练样本的随机颜色抖动和灰度随机化了以颜色测量的相似度。这阻碍了网络利用颜色中的琐碎信息。</p><p>​  <strong>ODC的训练。</strong><br/>​  我们使用ResNet-50作为骨干。考虑到大多数早期的作品都使用了AlexNet，我们也在AlexNet上进行了实验以进行比较。在[13]之后，我们使用不需要本地响应规范化的AlexNet架构，并添加批处理规范化层。AlexNet和ResNet-50的ODC模型是从零开始训练的。批处理大小是512个，分配给8个gpu。400个时代，AlexNet的学习率一直为0.01，ResNet-50为0.03,80个时代的学习率衰减0.1。在DC之后，集群的数量设置为10000，是ImageNet注释的类数的10倍。动量系数m设为0.5。识别小集群的阈值设置为20。改变这个阈值不会显著影响结果，前提是它不超过集群中样本的平均数量。质心存储器每10次迭代更新一次。质心更新频率构成了学习效率和学习效率之间的权衡。在我们的实验中，我们观察到，只要频率被限制在一个合理的范围内，ODC的性能就对它不敏感。</p><h1 id="实验">4.实验</h1><h1 id="结论">5.结论</h1><p>​  我们提出了一种有效的联合聚类和特征学习范式的无监督表示学习。该方法采用在线深度聚类（ODC），通过对特征聚类进行分解并将过程整合到网络更新迭代中，实现了有效稳定的深度神经网络无监督训练。ODC单独作为一种无监督的表示学习方案，表现得令人信服。它还可以用于微调和大大改进以前的自监督学习方法。</p><h1 id="代码">6.代码</h1><p>​  其代码整合在https://github.com/open-mmlab/mmselfsup/tree/main/configs/selfsup/odc。</p><h1 id="配置代码">6.1.配置代码</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_base_ = [</span><br><span class="line">    &#x27;../_base_/models/odc.py&#x27;,</span><br><span class="line">    &#x27;../_base_/datasets/imagenet_odc.py&#x27;,</span><br><span class="line">    &#x27;../_base_/schedules/sgd_steplr-200e_in1k.py&#x27;,</span><br><span class="line">    &#x27;../_base_/default_runtime.py&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># model settings</span><br><span class="line">model = dict(</span><br><span class="line">    head=dict(num_classes=&#123;&#123;_base_.num_classes&#125;&#125;),</span><br><span class="line">    memory_bank=dict(num_classes=&#123;&#123;_base_.num_classes&#125;&#125;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># optimizer</span><br><span class="line">optimizer = dict(type=&#x27;SGD&#x27;, lr=0.06, weight_decay=1e-5, momentum=0.9)</span><br><span class="line">optim_wrapper = dict(</span><br><span class="line">    type=&#x27;OptimWrapper&#x27;,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    paramwise_cfg=dict(custom_keys=&#123;&#x27;head&#x27;: dict(momentum=0.)&#125;))</span><br><span class="line"></span><br><span class="line"># learning rate scheduler</span><br><span class="line">param_scheduler = [</span><br><span class="line">    dict(type=&#x27;MultiStepLR&#x27;, by_epoch=True, milestones=[400], gamma=0.4)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># runtime settings</span><br><span class="line">train_cfg = dict(max_epochs=440)</span><br><span class="line"># the max_keep_ckpts controls the max number of ckpt file in your work_dirs</span><br><span class="line"># if it is 3, when CheckpointHook (in mmcv) saves the 4th ckpt</span><br><span class="line"># it will remove the oldest one to keep the number of total ckpts as 3</span><br><span class="line">default_hooks = dict(</span><br><span class="line">    checkpoint=dict(type=&#x27;CheckpointHook&#x27;, interval=10, max_keep_ckpts=3))</span><br></pre></td></tr></table></figure><h2 id="模型配置">6.2.模型配置</h2><h3 id="总览">6.2.1.总览</h3><p>odc.py如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># model settings</span><br><span class="line">model = dict(</span><br><span class="line">    type=&#x27;ODC&#x27;,</span><br><span class="line">    data_preprocessor=dict(</span><br><span class="line">        mean=(123.675, 116.28, 103.53),</span><br><span class="line">        std=(58.395, 57.12, 57.375),</span><br><span class="line">        bgr_to_rgb=True),</span><br><span class="line">    backbone=dict(</span><br><span class="line">        type=&#x27;ResNet&#x27;,</span><br><span class="line">        depth=50,</span><br><span class="line">        in_channels=3,</span><br><span class="line">        out_indices=[4],  # 0: conv-1, x: stage-x</span><br><span class="line">        norm_cfg=dict(type=&#x27;SyncBN&#x27;)),</span><br><span class="line">    neck=dict(</span><br><span class="line">        type=&#x27;ODCNeck&#x27;,</span><br><span class="line">        in_channels=2048,</span><br><span class="line">        hid_channels=512,</span><br><span class="line">        out_channels=256,</span><br><span class="line">        with_avg_pool=True),</span><br><span class="line">    head=dict(</span><br><span class="line">        type=&#x27;ClsHead&#x27;,</span><br><span class="line">        loss=dict(type=&#x27;mmcls.CrossEntropyLoss&#x27;),</span><br><span class="line">        with_avg_pool=False,</span><br><span class="line">        in_channels=256,</span><br><span class="line">        num_classes=10000),</span><br><span class="line">    memory_bank=dict(</span><br><span class="line">        type=&#x27;ODCMemory&#x27;,</span><br><span class="line">        length=1281167,</span><br><span class="line">        feat_dim=256,</span><br><span class="line">        momentum=0.5,</span><br><span class="line">        num_classes=10000,</span><br><span class="line">        min_cluster=20,</span><br><span class="line">        debug=False))</span><br></pre></td></tr></table></figure><h3 id="backbone">6.2.2.backbone</h3><p>​  模型的backbone为ResNet，其代码在https://github.com/open-mmlab/mmselfsup/blob/db861f30487c9a0b0cb3e35e176d6058887660bc/mmselfsup/models/backbones/resnet.py。</p><p>​  模型的输入通道为3，输出通道为2048。</p><h3 id="neck网络">6.2.2.neck网络</h3><p>​  模型的backbone为odc_neck，其代码在https://github.com/open-mmlab/mmselfsup/blob/db861f30487c9a0b0cb3e35e176d6058887660bc/mmselfsup/models/necks/odc_neck.py</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@MODELS.register_module()</span><br><span class="line">class ODCNeck(BaseModule):</span><br><span class="line">    &quot;&quot;&quot;The non-linear neck of ODC: fc-bn-relu-dropout-fc-relu.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        in_channels (int): Number of input channels.</span><br><span class="line">        hid_channels (int): Number of hidden channels.</span><br><span class="line">        out_channels (int): Number of output channels.</span><br><span class="line">        with_avg_pool (bool): Whether to apply the global</span><br><span class="line">            average pooling after backbone. Defaults to True.</span><br><span class="line">        norm_cfg (dict): Dictionary to construct and config norm layer.</span><br><span class="line">            Defaults to dict(type=&#x27;SyncBN&#x27;).</span><br><span class="line">        init_cfg (dict or list[dict], optional): Initialization config dict.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channels: int,</span><br><span class="line">        hid_channels: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        with_avg_pool: bool = True,</span><br><span class="line">        norm_cfg: dict = dict(type=&#x27;SyncBN&#x27;),</span><br><span class="line">        init_cfg: Optional[Union[dict, List[dict]]] = [</span><br><span class="line">            dict(type=&#x27;Constant&#x27;, val=1, layer=[&#x27;_BatchNorm&#x27;, &#x27;GroupNorm&#x27;])</span><br><span class="line">        ]</span><br><span class="line">    ) -&gt; None:</span><br><span class="line">        super().__init__(init_cfg)</span><br><span class="line">        self.with_avg_pool = with_avg_pool</span><br><span class="line">        if with_avg_pool:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))</span><br><span class="line">        self.fc0 = nn.Linear(in_channels, hid_channels)</span><br><span class="line">        self.bn0 = build_norm_layer(</span><br><span class="line">            dict(**norm_cfg, momentum=0.001, affine=False), hid_channels)[1]</span><br><span class="line">        self.fc1 = nn.Linear(hid_channels, out_channels)</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.dropout = nn.Dropout()</span><br><span class="line"></span><br><span class="line">    def forward(self, x: List[torch.Tensor]) -&gt; List[torch.Tensor]:</span><br><span class="line">        &quot;&quot;&quot;Forward function.</span><br><span class="line"></span><br><span class="line">        Args:</span><br><span class="line">            x (List[torch.Tensor]): The feature map of backbone.</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">            List[torch.Tensor]: The output features.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        assert len(x) == 1</span><br><span class="line">        x = x[0]</span><br><span class="line">        if self.with_avg_pool:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(0), -1)</span><br><span class="line">        x = self.fc0(x)</span><br><span class="line">        x = self.bn0(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        return [x]</span><br></pre></td></tr></table></figure><h3 id="head网络">6.3.3.head网络</h3><p>​  使用的是ClsHead，CrossEntropyLoss损失</p><h3 id="memory_bank网络">6.3.4.memory_bank网络</h3><h2 id="运行时配置文件">6.3.运行时配置文件</h2><p>default_runtime.py如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">default_scope = &#x27;mmselfsup&#x27;</span><br><span class="line"></span><br><span class="line">default_hooks = dict(</span><br><span class="line">    runtime_info=dict(type=&#x27;RuntimeInfoHook&#x27;),</span><br><span class="line">    timer=dict(type=&#x27;IterTimerHook&#x27;),</span><br><span class="line">    logger=dict(type=&#x27;LoggerHook&#x27;, interval=50),</span><br><span class="line">    param_scheduler=dict(type=&#x27;ParamSchedulerHook&#x27;),</span><br><span class="line">    checkpoint=dict(type=&#x27;CheckpointHook&#x27;, interval=10),</span><br><span class="line">    sampler_seed=dict(type=&#x27;DistSamplerSeedHook&#x27;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">env_cfg = dict(</span><br><span class="line">    cudnn_benchmark=False,</span><br><span class="line">    mp_cfg=dict(mp_start_method=&#x27;fork&#x27;, opencv_num_threads=0),</span><br><span class="line">    dist_cfg=dict(backend=&#x27;nccl&#x27;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">log_processor = dict(</span><br><span class="line">    window_size=10,</span><br><span class="line">    custom_cfg=[dict(data_src=&#x27;&#x27;, method=&#x27;mean&#x27;, window_size=&#x27;global&#x27;)])</span><br><span class="line"></span><br><span class="line">vis_backends = [dict(type=&#x27;LocalVisBackend&#x27;)]</span><br><span class="line">visualizer = dict(</span><br><span class="line">    type=&#x27;SelfSupVisualizer&#x27;, vis_backends=vis_backends, name=&#x27;visualizer&#x27;)</span><br><span class="line"># custom_hooks = [dict(type=&#x27;SelfSupVisualizationHook&#x27;, interval=1)]</span><br><span class="line"></span><br><span class="line">log_level = &#x27;INFO&#x27;</span><br><span class="line">load_from = None</span><br><span class="line">resume = False</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 在线深度聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DivClust:Controlling Diversity in Deep Clustering</title>
      <link href="/DivClust/"/>
      <url>/DivClust/</url>
      
        <content type="html"><![CDATA[<p>DivClust: Controlling Diversity in Deep Clustering</p><p>Ioannis Maniadis Metaxas*, Georgios Tzimiropoulos, Ioannis Patras</p><p>Queen Mary University of London</p><p>Mile End road, E1 4NS London, UK</p><p><em>{</em>i.maniadismetaxas, g.tzimiropoulos,i.patras<em>}</em>@qmul.ac.uk</p><h1 id="摘要">摘要</h1><p>​  聚类一直是机器学习领域的一个主要研究课题，深度学习最近在这个领域取得了显著的成功。然而，现有的深度聚类方法所没有解决的聚类的一个方面是，它可以有效地为给定的数据集生成多个、不同的分区。这一点尤其重要，因为一组不同的基聚类对于共识聚类是必要的，这已经被发现比依赖于单一聚类能产生更好、更鲁棒的结果。为了解决这一差距，我们提出了DivClust，一种多样性控制损失，可以纳入现有的深度聚类框架，以产生具有所需多样性程度的多个聚类。我们用多个数据集和深度聚类框架进行了实验，结果表明：a)我们的方法有效地控制多样性跨框架和数据集与非常小的额外计算成本，b)由DivClust学习到的聚类集包括解决方案明显优于单聚类基线，和c)使用现成的共识聚类算法，DivClust产生共识聚类解决方案始终优于单聚类基线，有效地提高基础深度聚类框架的性能。代码在https://github.com/ManiadisG/DivClust。</p><h1 id="介绍">1.介绍</h1><p>​  视觉数据量的指数级增长，随着计算能力的进步和强大的深度神经网络体系结构的发展，已经恢复了人们对使用视觉数据的无监督学习的兴趣。特别是深度聚集，这是一个近年来取得重大进展的领域。现有的工作集中于生成一个单一的聚类，这是根据该聚类与相关数据集的地面真实标签的匹配程度来评估的。然而，在深度聚类的背景下，共识或集成聚类仍有待充分研究，尽管事实上已经发现它比单一聚类结果[3,17,45,73]能持续提高性能。<br/>​  共识聚类由两个阶段组成，具体生成一组基聚类，然后应用共识算法对它们进行聚合。为了在每个设置中产生更好的结果，确定集合应该拥有的属性一直是一个开放的问题[18]。然而，研究发现，集合内的聚类多样性是一个重要的、理想的因素[14,20,24,34,51]，以及个体聚类质量，多样性应该调节[15,22,51]。此外，一些研究表明，控制集合中的多样性对于研究其影响和确定其在每个设置[22,51]中的最佳水平具有重要意义。<br/>​  产生不同聚类的典型方法是通过使用不同的初始化/超参数或数据[3,17]的子集对数据进行多次聚类来促进多样性。然而，这种方法并不能保证或控制多样性的程度，而且计算成本很高，特别是在深度聚类的情况下，它将需要训练多个模型。已经提出了一些方法，通过在聚类过程中包含与多样性相关的目标来发现不同的聚类，但这些方法只应用于预先计算的特征聚类，不能简单地纳入深度学习框架。其他方法通过创建和聚类不同的特征子空间来解决不同的聚类，包括一些应用于深度聚类[48,61]的方法。然而，这些方法并不能控制集群间的多样性。相反，它们通过它们所创建的子空间的属性来间接地影响它。此外，通常，现有的方法一直侧重于产生正交的聚类或基于相对简单的视觉数据的独立属性（如颜色/形状）来识别聚类。因此，它们面向最大化聚类间多样性，这不适用于共识聚类[15,22,51]。<br/>​  为了解决这一差距，即利用深度聚类框架和所期望的多样性程度有效地生成多个聚类，我们提出了DivClust。我们的方法可以直接合并到现有的深度聚类框架中，以学习其多样性被明确控制的多个聚类。具体来说，该方法使用一个主干进行特征提取，然后是多个投影头，每个投影头产生聚类分配以进行相应的聚类。给定一个用户定义的多样性目标，在这项工作中，用聚类之间的平均NMI来表示，DivClust将聚类间的相似度限制在一个适当的、动态估计的阈值以下。这是通过一种新的损失组件来实现的，该组件基于模型产生的软聚类分配来估计聚类间的相似性，并惩罚超过阈值的值。重要的是，DivClust引入了最小的计算成本，并且不需要对基本的深度聚类框架进行超参数调优，这使得它的使用简单且计算效率高。<br/>​  在三种深度聚类方法（IIC[37]，PICA[32]，CC[44]）（CIFAR10、CIFAR100、10）上的实验表明，DivClust可以在不降低聚类质量的情况下有效控制聚类间的多样性。此外，我们还证明，通过使用现成的共识聚类算法，DivClust学习到的不同基聚类产生了优于基框架的共识聚类解决方案，以最小的计算成本有效地改进了它们。值得注意的是，尽管共识聚类对集成的属性具有敏感性，但我们的方法在不同的多样性水平上都是稳健的，在大多数情况下表现优于基线，通常利润率很大。我们的工作为提高深度聚类框架的性能提供了一种简单的方法，同时也为研究深度聚类集成[51]中多样性的影响提供了一种新的工具。<br/>​  总之，DivClust：a)可以以即插即用的方式纳入现有的深度聚类框架，计算成本非常小，b)可以显式和有效地控制聚类多样性以满足用户定义的目标，c)学习聚类，通过共识聚类提高深度聚类框架的性能。</p><h1 id="相关工作">2.相关工作</h1><h2 id="深度聚类">2.1.深度聚类</h2><p>​  深度聚类是指在学习数据特征的同时对数据进行聚类的方法。它们通常分为两类，即在聚类和特征学习之间交替训练，以及同时学习。</p><p>​  <strong>交替学习：</strong><br/>​  采用这种方法的方法通常采用两步训练制度，定期重复进行（例如，每时代或每步）。首先，基于模型提取的表示法（例如，通过特征聚类）生成样本伪标签。其次，利用这些伪标签来改进学习到的表示，通常是通过训练特征提取模型作为一个分类器。这些方法包括DEC[64]、DAC [8]，DCCM [63]，DDC [7]，JULE [66]、SCAN [57]、ProPos[33]和SPICE [49]，以及DSC-N [36]，IDFD [65]和MIX‘EM[58]，它们提出了训练模型的方法，这些模型的表示在聚类时产生更好的结果。在这一领域的其他工作还有DeepCluster[5]、SeLa[1]、PCL [42]和HCSC [21]，尽管他们的主要重点是表示学习。</p><p>​  <strong>同时学习：</strong><br/>​  这些方法共同学习特征和集群分配。它们包括ADC[23]、IIC[37]和PICA[32]，它们使用损失函数端到端训练聚类模型，在集群分配上强制执行期望的属性；ConCURL[11,52]，它建立在BYOL[19]的基础上，其损失最大化了来自转换嵌入的聚类的一致性；DCCS[71]，它利用了集群过程中的一个对抗性的组件；以及GatCluster[50]，它提出了一种结合四个自学习任务的注意机制。最后，SCL[31]、CC[44]、GCC[72]、TCC[53]和MiCE[56]等方法利用了对比学习。</p><p>​  虽然一些深度聚类方法[1,37,52]使用了多重聚类，但大多数都没有探讨聚类间多样性的流行程度和影响，也没有人提出控制它的方法。据我们所知，我们的工作是第一个解决这两个问题的工作。</p><h2 id="多样化聚类">2.2.多样化聚类</h2><p>​  生成多个不同聚类的最直接方法是多次对数据进行聚类。增加多样性的典型方法包括改变聚类算法或其超参数，使用不同的初始化，以及对样本或特征的子集进行聚类[3]。然而，这种方法a）计算成本很高，因为它需要多次对数据进行聚类，b）不可靠，因为一些增加多样性的方法可能会降低聚类的质量（例如使用数据的子集），c）无效，因为无法保证达到所需的多样性程度。<br/>​  为了解决这个问题，已经提出了几种方法来创建多个、不同的聚类[25]。我们确定了两种促进聚类间多样性的主要方法：a)明确地，通过优化适当的目标，以及b)隐式地，通过优化去相关/正交特征子空间，当聚类时，会导致不同的聚类。第一类方法包括COALA[2]、Meta聚类[6]、Dec-kmeans [35]、MNMF [67]、MSC [26]、ADFT[10]和MultiCC [60]。子空间聚类方法包括MISC [59]、ISAAC [69]、NRkmeans[47]、RAOSC [70]和ENRC [48]。明显的是，OSC [9]、MVMC [68]、DMSMF[46]、DMClusts [62]、DiMSC [4]和DiMVMC[61]也在多视图数据的背景下探索了不同的聚类。<br/>​  据我们所知，除了DiMVMC和ENRC之外，现有的方法都不能与深度学习兼容，它们需要一个学习的特征空间，而且大多数方法相对于样本的数量具有二次复杂度。这限制了它们在现实生活中的高维数据上的使用，其中深度聚类会产生更好的结果[56,57]。对于DiMVMC和ENRC，它们依赖于基于自动编码器的架构，并将其适应最近的深度聚类框架，其性能要好得多，这并不简单。更重要的是，它们利用子空间聚类，继承了其在控制多样性方面的局限性。具体来说：a)没有提出方法来推断不同的子空间必须为了导致特定程度的集群多样性，和b)子空间聚类方法继承聚类方法聚类算法的随机性应用于子空间（对于DiMVMC和ENRC的K-means），这进一步限制了他们对结果的控制。</p><h2 id="共识聚类">2.3.共识聚类</h2><p>​  聚类算法的性能随数据及其属性、算法本身及其超参数的不同而变化。这使得找到可靠的聚类解决方案特别困难。共识，或集成，聚类已经成为解决这个问题的一个解决方案，特别是通过组合多个，不同的聚类的结果，而不是依赖于一个单一的解决方案。这比单聚类方法[3,17,18,45]产生更好、更稳健的结果。共识聚类的过程分为两个阶段：a)生成多个、不同的基聚类，b)使用共识算法对这些聚类进行聚合。</p><p>​  <strong>生成不同的聚类：</strong><br/>​  共识算法所使用的聚类集的性质是获得良好性能的关键因素。[24,40,51]的多项研究发现，单个基集群的质量和多样性都是至关重要的，事实上，具有中等程度多样性的聚类集合会导致更好的结果[15,20,22]。集成生成的典型方法包括使用不同的聚类算法[13]，使用不同的初始化相同的聚类算法或不同的超参数（例如集群的数量）[16,22,41]，集群与不同的子集特征[54]，使用随机预测多样化特征空间[14]，和集群与不同数据集的子集[12,13]。然而，识别最优超参数的具体方法，如多样性的程度、集合中聚类的数量以及生成集合的方法，仍然难以捉摸。</p><p>​  <strong>共识算法：</strong><br/>​  共识算法的目标是聚合多个、不同的聚类，以产生一个单一的、健壮的解决方案。解决这一问题的各种方法已经被提出，如使用矩阵分解[43]，聚类[75]之间的距离最小化，利用多视图[55]，图学习[28,73,74]和矩阵协关联[29,38]。我们注意到，虽然改进共识算法总体上提高了共识聚类的鲁棒性，但集成生成的阶段及其与共识算法的聚合在很大程度上是独立的。</p><p>​  <strong>共识聚类和深度学习：</strong><br/>​  尽管共识聚类比单聚类方法有既定的优势，但共识聚类尚未在深度聚类的背景下被探索。一个可能的原因是生成多个、不同的基聚类的计算成本，这将需要训练多个模型。据我们所知，在深度聚类设置中应用共识聚类的唯一工作是DeepCluE[27]。值得注意的是，值得注意的是，DeepCluE使用的基聚类并不是都是由模型学习的。相反，训练单聚类模型，用U-SPEC[30]从模型的多层聚类特征生成集成。我们的工作解决了这一差距，通过提出一种方法来训练一个单一的深度聚类模型，以生成具有受控制的多样性和最小的计算开销的多个聚类。</p><h1 id="方法">3.方法</h1><h2 id="概述">3.1概述：</h2><p>​  我们的方法由两个组件组成：a)一种新的损失函数，可以纳入深度聚类框架中通过应用阈值来控制聚类间的多样性，和b)一种根据用户定义的度量动态估计该阈值以使模型学习到的聚类足够多样化的方法。</p><figure><img src="../postimages/DivClust/image-20250217101153060.png"alt="image-20250217101153060" /><figcaption aria-hidden="true">image-20250217101153060</figcaption></figure><figure><img src="../postimages/DivClust/image-20250217101837461.png"alt="image-20250217101837461" /><figcaption aria-hidden="true">image-20250217101837461</figcaption></figure><p>​  更具体地说，我们假设一个学习K聚类（通常是一个主干编码器，后面跟着K个投影头）、一个深度聚类框架及其损失函数<spanclass="math inline">\(L_{main}\)</span>的深度聚类模型，和一个由用户设置的多样性目标<spanclass="math inline">\(D^T\)</span>集，表示为聚类间相似性的上界1（即最大可接受的相似度）。为了控制学习聚类的聚类间相似性<spanclass="math inline">\(D^R\)</span>，使<spanclass="math inline">\(D^{T}\leqD^{R}\)</span>，我们提出了一个互补损失<span class="math inline">\(L_{d iv}\)</span>。具体来说，给定一对簇<span class="math inline">\(A.B\inK\)</span>的软集群分配，我们定义集群间相似矩阵<spanclass="math inline">\(S_{A B}\in\mathbb{R}^{C_{A}\timesC_{B}}\)</span>，<span class="math inline">\(C_A\)</span>和<spanclass="math inline">\(C_B\)</span>是每个集群中的集群数，和<spanclass="math inline">\(S_{AB}(i,j)\in[0,1]\)</span>测量集群之间的相似性<spanclass="math inline">\(i\in C_{A}\)</span>和<spanclass="math inline">\(j\in C_{B}\)</span>。由此可见，降低<spanclass="math inline">\(S_{AB}\)</span>的值可以减少了A簇和B簇之间的相似性，从而增加了它们的多样性。因此，<spanclass="math inline">\(L_{d i v}\)</span>利用<spanclass="math inline">\(S_{AB}\)</span>将聚类间的相似度限制在相似度上界d下。d值在训练过程中动态调整，<spanclass="math inline">\(D^{R}\gt D^{T}\)</span>时降低，<spanclass="math inline">\(D^{R}\leqD^{T}\)</span>时增加，从而收紧和放松损失函数，从而在整体和整个训练过程中，聚类间相似性<spanclass="math inline">\(D^{R}\)</span>保持在或低于期望<spanclass="math inline">\(D^{T}\)</span>的水平。</p><h2 id="定义聚类相似矩阵s_a-b">3.2定义聚类相似矩阵<spanclass="math inline">\(S_{A B}\)</span>：</h2><p>​  我们的方法假设一个标准的深度聚类结构，由一个编码器f，然后是K个投影头h1，...，hK，每个产生一个聚类K的赋值。具体来说，设X是N个未标记样本的集合。编码器将每个样本x∈X映射到一个表示f(x)，每个投影头hk将f (x)映射到Ck簇，这样<spanclass="math inline">\(p_{k}(x)=h_{k}(f(x))\in\mathbb{R}^{C_{k}\times1}\)</span>表示一个概率分配向量，其将样本x∈X映射到聚类k中的Ck簇。在不丢失一般性的情况下，我们假设<spanclass="math inline">\(C=C_{k}~\forallk~\in~K\)</span>。每个聚类都可以用聚类分配矩阵<spanclass="math inline">\(P_{k}(X)=[p_{k}(x_{1}),p_{k}(x_{2}),...,p_{k}(x_{N})]\in\mathbb{R}^{C\timesN}\)</span>表示。列<spanclass="math inline">\(p_k(n)\)</span>，即第n个样本的概率分配向量，对样本xn被分配给不同簇的程度进行编码。行向量<spanclass="math inline">\(q_{k}(i)\in\mathbb{R}^{N}\)</span>显示了哪些样本被温和地分配给聚类i∈c。我们将<spanclass="math inline">\(q_{k}(i)\)</span>称为聚类隶属度向量。</p><p>​  为了量化聚类A和B之间的相似性，我们定义了聚类间相似性矩阵<spanclass="math inline">\(S_{A B}\in\mathbb{R}^{C\timesC}\)</span>。我们将每个元素<span class="math inline">\(S_{AB}(i,j)\)</span>定义为簇<span class="math inline">\(i\inA\)</span>的簇成员向量<spanclass="math inline">\(q_{A}(i)\)</span>和簇<spanclass="math inline">\(j\in B\)</span>的簇成员矢量<spanclass="math inline">\(q_{B}(j)\)</span>之间的余弦相似度： <spanclass="math display">\[S_{A B}(i,j)=\frac{q_{A}(i)\cdotq_{B}(j)}{||q_{A}(i)||_{2}||q_{B}(j)||_{2}}\]</span>​  这个度量表示了数据集中的样本被分配到类似于集群i和j的程度。具体来说，如果<spanclass="math inline">\(q_{A}(i)\perp q_{B}(j)\)</span>，<spanclass="math inline">\(S_{A B}(i,j)=0\)</span>，如果<spanclass="math inline">\(q_{A}(i)= q_{B}(j)\)</span>，<spanclass="math inline">\(S_{AB}(i,j)=1\)</span>。因此，它是簇i和簇j相似性的可微度量。</p><h2 id="定义损失函数">3.3定义损失函数：</h2><p>​  基于聚类间相似度矩阵<span class="math inline">\(S_{AB}\)</span>，我们定义了DivClust的损失，以温和地强制聚类A与聚类B的聚集聚类相似度不大于相似度上限d。聚合相似度<spanclass="math inline">\({S}_{A B}^{a g gr}\)</span>定义为聚类A的聚类与其最相似的聚类B的聚类的平均相似性(Eq(2))。利用这个度量，我们提出了<span class="math inline">\(L_{d iv}\)</span> (Eq(3))，该损失通过强迫<span class="math inline">\(S_{AB}^{a g g r}\ltd,\mathrm{for}\;d\in[0,1]\)</span>来调节集群A和集群B之间的多样性。从等式3可以明显看出<spanclass="math inline">\(S_{A B}^{a g g r}\lt d\Rightarrow L_{d iv}(A,B)=0\)</span>，在这种情况下，多样性要求得到满足，损失没有影响。相反，<spanclass="math inline">\(S_{A B}^{a g g r}\geq d\Rightarrow L_{d iv}(A,B)\gt 0\)</span>，在这种情况下，损失要求聚类间相似性降低。 <spanclass="math display">\[S_{A B}^{a g gr}=\frac{1}{C}\sum_{i=1}^{C}\operatorname*{max}_{j}(S_{AB}(i,j))\]</span></p><p><span class="math display">\[L_{d i v}(A,B)=[S_{A B}^{a g gr}-d]_{+}\]</span></p><p>​  在定义了两个聚类之间的多样性损失<span class="math inline">\(L_{d iv}\)</span>后，我们将其扩展到多个聚类K，并将其与基础深度聚类框架的目标相结合。对于聚类k∈K，我们用<spanclass="math inline">\(L_{m a in}(k)\)</span>表示该聚类的基本深度聚类框架的损失，用<spanclass="math inline">\(L_{d iv}(k,k^{\prime})\)</span>表示聚类k和聚类k’之间的多样性控制损失。我们给出了等式(4)中每个聚类k的联合损失<spanclass="math inline">\(L_{joint}(k)\)</span>，其中<spanclass="math inline">\(L_{m a i n}(k)\)</span>依赖于簇分配矩阵Pk，而<spanclass="math inline">\(L_{d iv}(k,k^{\prime})\)</span>依赖于Pk和Pk’。因此，该模型的训练损失<spanclass="math inline">\(L_{total}\)</span>，见等式(5)，是所有集群中<spanclass="math inline">\(L_{joint}\)</span>的平均值。 <spanclass="math display">\[L_{j o i n t}(k)=L_{m a in}(k)+\frac{1}{K-1}\sum_{k^{\prime}=1,k^{\prime}\neq k}^{K}L_{d iv}(k,k^{\prime})\]</span></p><p><span class="math display">\[L_{t o t al}={\frac{1}{K}}\sum_{k=1}^{K}L_{j o i n t}(k)\]</span></p><p>​  因此，损失总量是每个聚类<span class="math inline">\(k\inK\)</span>的基深度聚类框架的损失<span class="math inline">\(L_{m a in}\)</span>和用于控制聚类间多样性的损失<span class="math inline">\(L_{div}\)</span>的组合。所提出的损失公式适用于任何通过模型产生聚类分配的深度聚类框架（而不是使用MIX‘EM[58]的框架），它涵盖了第二节中概述的大多数深度聚类框架。</p><h2 id="动态上界d">3.4动态上界d：</h2><p>​  提出的损失<span class="math inline">\(L_{d iv}\)</span>通过根据相似度上界d限制<spanclass="math inline">\(S_{AB}\)</span>值来控制聚类间多样性。然而，<spanclass="math inline">\(S_{AB}\)</span>的值是基于软簇分配的余弦相似度来计算的。这意味着成对的聚类分配向量<spanclass="math inline">\(i,j\)</span>将有不同的相似度值<spanclass="math inline">\(S_{AB}(i,j)\)</span>，这取决于它们的锐度，即使它们以相应的硬分配指向相同的聚类。由此可见，<spanclass="math inline">\(S_{AB}\)</span>和d的影响依赖于集群分配的置信度，并在整个训练过程中和实验之间有所不同（因为集群的数量和模型容量等因素会影响集群分配的置信度）。因此，d是用户定义多样性目标的一个模糊和不直观的度量标准。<br/>​  为了解决这个问题，并为定义多样性目标提供一种直观的可靠的方法，我们建议在训练过程中动态确定阈值d的值。具体地说，设D是用户选择的聚类相似度度量。在这项工作中，我们使用平均的归一化互信息（NMI），一个很好的估计聚类间相似度的度量。<spanclass="math display">\[D={\frac{1}{(K-1)(K/2)}}\sum_{k=1}^{K-1}\sum_{k^{\prime}=k+1}^{K}N\!MI(P_{k}^{h},P_{k^{\prime}}^{h})\]</span> ​  其中，<spanclass="math inline">\(P_{k}^{h}\in\mathbb{Z}^{N}\)</span>是聚类<spanclass="math inline">\(k\in K\)</span>中N个样本的硬聚类分配向量，<spanclass="math inline">\(N MI(P_{k}^{h},P_{k^{\prime}}^{h})\)</span>表示<spanclass="math inline">\(k\)</span>和<spanclass="math inline">\(k^{\prime}\)</span>之间的NMI。<spanclass="math inline">\(D\in[0,1]\)</span>，其值越高，表示聚类程度越多。<br/>​  假设用户定义的相似性目标<spanclass="math inline">\(D^T\)</span>，表示为度量D的值，我们用<spanclass="math inline">\(D^R\)</span>表示模型学习到的聚类的测量聚类间相似性，用相同的度量表示。DivClust的目标是控制聚类间的多样性，这转化为学习聚类，使<spanclass="math inline">\(D^{R}\leqD^{T}\)</span>。因此，在训练过程中必须使用适当的阈值d。在假设<spanclass="math inline">\(D^R\)</span>相对于d单调递减的情况下，我们提出了以下d的更新规则：<span class="math display">\[d_{s+1} = \begin{cases}m ax(d_{s}(1-m),0),~~~\mathrm{if}~D^{R}\gt D^{T}\\m in(d_{s}(1+m),1),~~~\mathrm{if}~D^{R}\le D^{T} ,\\\end{cases}\]</span>​  其中，<span class="math inline">\(d_s\)</span>和<spanclass="math inline">\(d_{s+1}\)</span>是当前和下一步步骤的阈值d的值，而<spanclass="math inline">\(m\in(0,1)\)</span>调节了更新步骤的大小。根据这个更新规则，当测量的聚类间相似度<spanclass="math inline">\(D^R\)</span>需要减少时，我们会减少d，否则就会增加d。为了计算效率，我们不是在每个训练步骤中计算整个数据集的<spanclass="math inline">\(D^R\)</span>，而是在M=10000集群分配的内存库中每20次迭代一次——后者以FIFO方式更新。我们在所有实验中将超参数m设置为m= 0.01。</p><h1 id="实验">4.实验</h1><h1 id="讨论">5.讨论</h1><h2 id="多样性控制和共识聚类性能">5.1.多样性控制和共识聚类性能</h2><h2 id="复杂性和计算成本">5.2.复杂性和计算成本</h2><h1 id="结论">6.结论</h1><p>​  我们引入了DivClust，这是一种可以整合到现有的深度聚类框架中的方法，可以在控制聚类间多样性的同时学习多个聚类。据我们所知，这是第一种可以基于用户定义的目标明确控制间聚类多样性的方法，并且与学习特性和端到端聚类聚类的深度聚类框架兼容。我们的实验，使用多个数据集和深度聚类框架进行，证实了DivClust在控制聚类间多样性及其适应性方面的有效性，因为它可以兼容各种框架，而不需要修改和/或超参数调优。此外，结果表明，DivClust学习了高质量的聚类，在共识聚类的背景下，与单一聚类基线和替代集成聚类方法相比，这导致了提高的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 共识聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learnable Subspace Clustering</title>
      <link href="/Learnable-Subspace-Clustering/"/>
      <url>/Learnable-Subspace-Clustering/</url>
      
        <content type="html"><![CDATA[<p>Learnable Subspace Clustering</p><h1 id="摘要">摘要</h1><p>​  本文研究了具有数百万个数据点的大规模子空间聚类（<spanclass="math inline">\(LS^2C\)</span>）问题。许多流行的子空间聚类方法都不能直接处理<spanclass="math inline">\(LS^2C\)</span>问题，尽管它们被认为是针对小规模数据点的最新方法。一个简单的原因是，这些方法经常选择所有的数据点作为一个大型的字典来构建巨大的编码模型，这导致了较高的时间和空间复杂度。在本文中，我们开发了一个可学习的子空间聚类范式来有效地解决LS2C问题。关键的概念是学习一个参数函数来将高维子空间划分为它们的底层低维子空间，而不是需要计算要求的经典编码模型。此外，我们提出了一个统一的、鲁棒的、预测编码机（RPCM，robustpredictive codingmachine）来学习参数函数，它可以通过交替最小化算法来求解。此外，我们还给出了参数函数的有界收缩分析。</p><h1 id="介绍">1.介绍</h1><p>​  高维大数据集越来越多无处不在，在计算机视觉和机器学习任务中越来越可用和流行。例如，[1]和YouTube上分别有数百万帧的图像和数十亿帧的视频。不幸的是，大数据的高维性通常会导致算法的高时间和空间复杂性，而大数据中的复杂错误（如噪声、异常值和损坏）严重损害了它们的性能[3]。然而，高维大数据通常是由低维子结构[4]组成的。因此，在大数据中寻找低维结构是降低时间和空间复杂度、减少复杂误差的影响、提高学习和分割任务性能的一种可能的解决方案。<br/>​  子空间聚类[5]是最常见的鲁棒地恢复高维数据低维表示的方法之一，因为它已经提供了理论保证[6]，[7]成功应用于众多研究领域，如图像分割[8]，[9]，人体运动分割[10]，图像处理[11]，[12]，顺序数据聚类[13]，医学成像[14]和生物信息学[15]。特别是光谱式方法（如稀疏子空间聚类（SSC）[16]、低秩表示（LRR）[17]、[18]和最小二乘回归（LSR）[19]）近年来由于在小规模数据集（如霍普金斯155[20]和扩展YaleB[21]）上的良好性能好而受到越来越多的关注。<br/>​  然而，经典的范式限制了子空间聚类方法来处理高维大数据（例如，数以百万计的图像[23]和视频[2]）。一个简单的原因是，SE属性导致了一个大的自我表达字典和巨大的简约编码模型。它们导致了计算瓶颈，因为它们通常通过迭代许多优化算子来解决，如收缩阈值算子[24]和奇异值分解（SVD）[25]。因此，它很自然地产生了一个具有数百万个数据点的具有挑战性的子空间聚类问题。<br/>​  为了解决这个问题，我们开发了一个可学习的子空间聚类（LeaSC）范式（见图1），以快速和稳健地从高维大数据中恢复低维结构。</p><figure><imgsrc="../postimages/Learnable-Subspace-Clustering/image-20250216110434525.png"alt="image-20250216110434525" /><figcaption aria-hidden="true">image-20250216110434525</figcaption></figure><p>​  首先，给定一组大的高维数据点，通常来自高维子空间[16]，[17]，我们发现一些代表或样本，可以有效地描述绘制数据点[26]有效地减少大自我表达的字典的大小。其次，利用代表来学习从高维子空间到低维子空间的参数函数（例如，神经网络）。第三，其余的数据点通过学习到的参数函数而不是简约的编码模型快速映射到低维编码中。最后，我们采用基于地标的光谱聚类（LbSC）[27]对代码进行聚类。<br/>​  在已开发的LeaSC范式中，我们提出了一种新的模型，称为预测编码机（PCM）来学习参数函数。通过找到代表作为一个小的自我表达的字典，PCM的基本过程是优化理想回归，稀疏，低秩，或弹性网络代码通过使用一个平方的弗罗比尼乌斯范数，1规范，核范数或弹性网正则化，并共同训练参数函数近似理想的代码。此外，由于在现实应用[16]，[17]中由于测量/处理噪声经常被噪声（如高斯噪声、稀疏异常项和缺失项）破坏，我们通过考虑各种噪声污染的数据，将PCM扩展到鲁棒PCM（RPCM）。由于参数函数的非线性，RPCM是非凸的。我们提出了一种拟凸优化方法来解决它们，该方法替代了交替方向的乘子法（ADMM）[28]，[29]和梯度下降法（GD）算法[30]，[31]。通过选择适当数量的代表，给出了保证参数函数能够正确地将高维数据点压缩到代表的理想码的理论条件。这进一步验证了在代表上训练的PCM和RPCM可以有效地执行具有数百万个数据点的子空间聚类。总的来说，我们的主要贡献如下。</p><ul><li>我们开发了一个有效的范式，称为LeaSC，以一种高效的方式处理具有数百万个数据点的子空间聚类问题。我们的目标是学习参数函数，将高维子空间划分为它们的底层低维子空间，而不是经典编码（CCod）模型的昂贵代价。这个参数函数导致了LeaSC的线性复杂性。</li><li>我们提出了一个PCM模型及其鲁棒扩展RPCM来学习LeaSC范式中的参数函数。由于RPCM是非凸的，我们提出了一种拟凸的优化方法，通过交替的ADMM和GD来求解它。</li><li>我们提供了由PCM模型学习到的参数函数的有界收缩分析。结果表明，与原始子空间聚类模型相比，参数函数能很好地计算高维数据的收缩低维表示。</li></ul><p>​  本文是对我们之前几个会议作品[32]-[35]的重要扩展。与会议版本相比，有四个主要的区别。首先，我们开发了一个统一的LeaSC范式来集群数百万个数据点。其次，我们提出了一个具有弗罗比尼乌斯范数[32]、核范数[33]、1-范数或弹性网正则化的平方的一般RPCM。第三，我们给出了一个保证学习到的参数函数收缩的理论条件。第四，我们在百万尺度的数据集（即MNIST8M[23]和YouTube8M[2]）上添加了更多的实验，以证明LeaSC的有效性。<br/>​  本文的其余部分组织如下。我们在第二节中回顾了相关的子空间聚类工作。我们统一的LeaSC范式和RPCM在第三节中开发。我们在第四节中提出了一种针对RPCM的拟凸优化方法。我们在第五节中提供了一个收缩子空间恢复理论，第六节显示了实验结果。最后，在第七节中得出了结论。</p><h1 id="相关工作">2.相关工作</h1><p>​  在本节中，我们主要回顾了CCod模型、大尺度光谱聚类（LsSPC）方法和可伸缩的编码方法，因为它们在小规模数据集上显示出了良好的性能。此外，我们还讨论了直接编码模型来应用我们的LeaSC范式。</p><h2 id="a.-ccod模型">A. CCod模型</h2><p>​  一般来说，CCod模型基于SE属性[16]，采用不同的编码模型来计算相似矩阵，并将（大规模）光谱聚类[22]，[36]-[38]应用于该相似矩阵。例如，SSC[16]、LRR [17]和LSR[19]分别使用1、核范数和2正则化的编码模型构建相似度矩阵。根据SE属性，这些编码模型及其变体[18]，[39]-[41]通常选择所有的数据点作为字典。当面对大数据集（例如，数百万张图像）时，字典中的自动（或基础）的数量非常大。在实践中，这个大型字典将导致用于谱聚类的高维（百万）相似矩阵，并进一步导致如果资源有限，编码模型无法在一台机器上运行。此外，由于采用迭代优化方式[42]、[43]进行求解，因此它们仍然存在较高的时间复杂度。因此，简约的编码模型需要高的计算时间和大量的内存来构建相似矩阵。此外，分布式LRR（DLRR）[44]和分散SSC（DSSC）[45]将一个大数据集划分为许多小数据集，并使用LRR或稀疏编码来计算LRR或稀疏表示。然而，它们仍然需要大量的计算机资源。幸运的是，我们探索了LeaSC范式来学习一个参数函数，以快速计算相似度矩阵的表示。</p><h2 id="b.-lsspc">B. LsSPC</h2><p>​  一般的谱聚类方法[22]需要计算由相似矩阵组成的归一化拉普拉斯矩阵的特征向量，并应用K-均值对特征向量进行聚类。不幸的是，计算特征向量是非常昂贵的[27]。因此，它导致了处理大规模数据集的难题。因此，将光谱聚类方法扩展到LsSpC中，通过以下两种方式对大数据进行聚类。首先，利用¨值方法逼近特征向量[46]，以降低计算代价，在许多子系统[47]中特征值计算是并行的。其次，选取少量样本作为地标点，构建K-means[48]、样本外[49]和随机选择[27]的稀疏相似矩阵。它们将导致较差的聚类结果，因为一个不可分割的相似度矩阵通常是由具有复杂结构的原始数据构建的。通过比较，我们可以通过在已开发的LeaSC范式中使用我们的RPCM模型来学习鲁棒表示。</p><h2 id="c.-可扩展编码方法">C. 可扩展编码方法</h2><p>​  在以下两种策略中，还提出了可扩展的编码方法来进行大数据的聚类。首先，可伸缩的SSC[50]，[51]是用一个小的子字典来解决一系列的子问题，以减少计算时间。其次，采样-聚类分类（S-C-C）[42]，[43]从大数据集中选择一个小的子数据集，使用SSC、LRR和LSR进行子空间聚类，并学习一个简单的线性分类器或基于协作表示的分类器（CRC）[52]，获得最终的聚类结果。不幸的是，可扩展SSC仍然需要更多的计算时间，例如，它花费1000秒处理10万个数据样本[50]，S-C-C将导致较差的聚类结果，因为简单的分类器在复杂样本[53]中不容易识别。通过比较，我们可以快速计算鲁棒表示来构造相似度矩阵，并执行LsSpC（如LbSC[27]）。</p><h2 id="d.-直接编码模型">D. 直接编码模型</h2><p>​  在我们的LeaSC范式中，我们提出了RPCM模型来学习参数函数，以直接编码数据点的表示，以降低计算复杂度。实际上，有一些直接编码模型，如自动编码器（AEs）[54]、稀疏自动编码器（SAEs）[55]、[56]、去噪自动编码器（DAEs）[57]、预测稀疏分解（PSD）[34]、[58]、鲁棒主成分分析编码器（RPCAec）[59]和潜在LRR（latLRR）[60]。AE和SAE都不是健壮的模型，需要干净的输入数据。虽然DAE和PSD可以学习健壮的代码，但它仍然需要干净的输入数据。不幸的是，真实的数据通常会有大量的噪声[16]，[17]。为了处理噪声，RPCAec[59]和latLRR [60]可以通过分离噪声快速计算出鲁棒的低秩码。然而，RPCAec[59]只处理一个单个低秩子空间，而latLRR[60]只学习一个线性预测器，这很难捕获复杂的数据结构[31]，[53]。相比之下，我们的RPCM不需要明确的数据，也不需要一个非线性预测器来计算鲁棒表示。更重要的是，我们还将把这些编码模型应用到LeaSC范式中来处理LS2C。</p><h1 id="leasc">3. LEASC</h1><p>​  在本节中，我们开发了一个有效的LeaSC范式来聚集大量的多子空间数据点。具体地说，我们首先介绍了一个用于子空间聚类的机器学习问题。为了解决这一问题，我们提出了一个统一的PCM方法，即从高维空间到低维空间学习一个参数函数。然后利用参数函数快速恢复数据点的子空间表示。最后，我们对子空间表示进行LbSC[27]，以获得最终的聚类结果。</p><p>​  标记法：<spanclass="math inline">\(\mathcal{X}\)</span>记为高维空间。随机变量、普通向量、矩阵和矩阵块分别用大写、小写、大写和黑板粗体写，例如，X、x、<spanclass="math inline">\(\mathbf{X}\)</span>和<spanclass="math inline">\(\mathbb{X}=\binom{\mathbf X}{\mathbfY}\)</span>。期望（离散情况，p为概率质量）用<spanclass="math inline">\(\operatorname{E}_{p(X)}({\mathcal{L}}(X))\ \ =\\sum_{\mathbf{x}}p(X\ \ =\\mathbf{x}){\mathcal{L}}({\mathbf{x}})\)</span>表示。给定数组<spanclass="math inline">\(\mathbf{A}=\,[a_{i\,j}\,]\,\in\,\mathbb{R}^{d\timesn}\)</span>，我们的核范数用<spanclass="math inline">\(\|\mathbf{A}\|_{*}=\textstyle\sum_{i}\sigma_{i}(\mathbf{A})\)</span>（A的奇异值之和）表示，<spanclass="math inline">\(\ell_{2,1}\)</span>用<spanclass="math inline">\(||\mathbf{A}||_{2,1}=\sum_{j}||\mathbf{a}_{:j}||_{2}\)</span>表示，F范数用<spanclass="math inline">\(||\mathbf{A}||_{\mathrm}=(\sum_{ij}(a_{i j})^{2})^{1/2}\)</span>表示和f范数的平方用<spanclass="math inline">\(||\mathbf{A}||_{\mathrm{F}}^{2}=\sum_{i j}(a_{ij})^{2}\)</span>表示。一个包含m个元素的集合的n-组合数用<spanclass="math inline">\(\binom{m}{n}\)</span>表示。</p><h2 id="a.问题陈述">A.问题陈述</h2><p>​  在本节中，我们描述了一个用于子空间聚类的显式机器学习问题。具体地说，我们考虑一个d维高维空间<spanclass="math inline">\(\mathcal{X}\)</span>，它是s≥1个未知低维<spanclass="math inline">\(d_{i}=\dim(\mathcal{X}_{i})(0\lt d_{i}\ltd,i=1,\ldots,s)\)</span>的线性或仿射子空间<spanclass="math inline">\(\{\mathcal{X}_{i}\}_{i=1}^{s}\)</span>的未知并集。形式上，我们假设<spanclass="math inline">\(X\in\mathcal{X}\)</span>是一个根据未知分布p(X)的随机变量，它可以分解为 <spanclass="math display">\[X=\mathbf{B}Z=[\mathbf{B}_{1}\quad\cdots\quad\mathbf{B}_{k}]{\left[\begin{array}{l}{Z_{1}}\\{\cdots}\\ {Z_{k}}\end{array}\right]}\]</span> ​  其中，<spanclass="math inline">\(\mathbf{B}_{i}\in\mathbb{R}^{d\timesd_{i}}\)</span>是一个线性跨越第i个数据子空间的未知子空间基，<spanclass="math inline">\(Z_i\)</span>是一个<spanclass="math inline">\(d_i\)</span>维低维子空间表示变量。我们的子空间聚类问题的机器学习目标是寻找一个参数函数<spanclass="math inline">\(f(\bullet;\theta)\)</span>，它从X预测Z，以将空间X划分为子空间<spanclass="math inline">\(\{\mathcal{X}_{i}\}_{i=1}^{s}\)</span>。<br/>​  为了实现这一目标，根据基础分布p(X)，将f的预期风险定义为 <spanclass="math display">\[\operatorname*{min}_{\theta}\E_{p(X)}[\mathcal{L}(Z,f(X;\theta))]\quad\mathrm{s.t.}\X=\mathrm{B}Z\]</span> ​  其中，<spanclass="math inline">\(\mathcal{L}(Z,f(X;\theta))\)</span>是Z和<spanclass="math inline">\(f(X;\theta)\)</span>之间的损失函数，例如，平方误差损失函数<spanclass="math inline">\((Z-f(X;\theta))^2\)</span>。由于p(X)未知，预期风险不能在(2)中直接测量。但是我们可以考虑一个由N个数据点<spanclass="math inline">\(\mathbf{X}=[\mathbf{x}_{i}]_{i=1}^{n}\in\mathbb{R}^{d\timesn}\)</span>构成，且假设为p(x)分布的经验分布<spanclass="math inline">\(p^0(X)\)</span>。相应的在p0(X)上的经验风险1平均值可以描述为<span class="math display">\[\operatorname*{min}_{\theta}\E_{p^0(X)}[\mathcal{L}(Z,f(X;\theta))]\quad\mathrm{s.t.}\X=\mathrm{B}Z\]</span> ​  在学习参数函数<spanclass="math inline">\(f(\bullet;\theta)\)</span>后，将空间X快速映射到低维表示空间，很容易划分为子空间<spanclass="math inline">\(\{\mathcal{X}_{i}\}_{i=1}^{s}\)</span>。我们将在第三节-b节中提出一个稳健和有效的模型，以更好地计算经验风险。<br/>​  备注1：我们的机器学习问题不同于传统的子空间聚类问题[5]。后者通常考虑通过使用昂贵的稀疏/低秩优化从空间X中得到的给定（训练）数据点X的分割。前者学习一个参数函数，将空间X划分为子空间<spanclass="math inline">\(\{\mathcal{X}_{i}\}_{i=1}^{s}\)</span>。它的过程是从空间X中提取一个数据集，或从给定的X中选择一个小的数据集，并训练参数函数。这个过程有一个明显的好处，即参数函数可以快速地将新的（大的）数据点投影到低维表示中，并分组到它们的子空间中。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</title>
      <link href="/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/"/>
      <url>/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<p>A Lightweight Clustering Framework for Unsupervised SemanticSegmentation</p><p>Yau Shing Jonathan Cheung Xi Chen Lihe Yang Hengshuang Zhao<br/>TheUniversity of Hong Kong</p><h1 id="摘要">摘要</h1><p>​  无监督语义分割的目的是将图像中的每个像素分类为一个相应的类，而不使用带注释的数据。由于获取标记数据集的代价昂贵，这是一个被广泛研究的领域。虽然该领域的工作已经证明了模型精度的逐渐提高，但大多数需要神经网络训练。这使得分割同样昂贵，特别是在处理大规模数据集时。因此，我们提出了一个用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉Transformer的注意特征表现出很强的前景-背景可微性。因此，聚类可以有效地分离前景和背景图像斑块。在我们的框架中，我们首先跨数据级、类别级和图像级执行多级聚类，并在整个过程中保持一致性。然后，对提取的二进制补丁级伪模进行上采样、细化，最后进行标记。此外，我们提供了自我监督视觉变压器特征的全面分析，并提供了DINO和DINOv2之间的详细比较，以证明我们的主张。我们的框架在无监督语义分割方面显示出了巨大的前景，并在快速的VOC和MSCOCO数据集上取得了最先进的结果。</p><h1 id="介绍">1.介绍</h1><p>​  一方面，模型训练仍然需要大量的计算资源，特别是在大型数据集上操作时。另一方面，需要对不同的数据集进行广泛的超参数调优，从而降低了该方法的泛化能力。<br/>​  我们提出了一个用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉Transformer的注意力特征表现出很强的前景-背景可微性。因此，聚类可以用于分离前景和背景图像斑块。我们首先在三个层次上执行聚类：数据集级，通过对同一数据集中的特征进行分组；类别级，通过在同一超类中聚集特征；以及图像级，通过对单个图像中的特征进行分组。数据集级和类别级的结果都提供了高质量的预测。图像级掩模，尽管是最粗糙的，但便于容易的前景-背景检测，并帮助去除噪声区域。因此，我们保持了多级聚类的一致性来提取最终的二进制补丁级伪掩码。图1显示了数据集级、类别级和图像级掩码的可视化结果，以及最终提取的伪掩码。</p><figure><imgsrc="../postimages/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/image-20250215220520078.png"alt="image-20250215220520078" /><figcaption aria-hidden="true">image-20250215220520078</figcaption></figure><p>​  图1.通过简单的注意特征聚类，我们可以得到准确的伪掩模预测。在同一数据集、超类别和图像中，分别通过聚类特征来提取数据集级、类别级和图像级掩模。不同层次的面具都有自己的优缺点。在简单的背景下，所有的掩膜都能提供准确的预测。然而，在复杂的背景下，分割变得更具挑战性。数据集级和类别级掩模都提供了对对象结构的精确估计，而图像级掩模虽然最粗糙，但可以用于1)在数据集级和类别级掩模中识别前景类，2)去除噪声。通过确保多级聚类的一致性，我们可以获得高质量的补丁级二进制伪掩码。</p><p>​  然后，对补丁级伪掩模进行上采样到像素级并进行后处理。为了分配类标签，将根据对象区域对图像进行裁剪，并将相应的CLS标记聚类为各自的类。利用简单的聚类方法，我们的无网络方法不仅计算成本低，而且取得了高质量的分割结果。<br/>​  我们的主要贡献总结如下：</p><ul><li>我们提出了一个用于无监督语义分割的轻量级聚类框架。</li><li>我们确保了跨数据集级、类别级和图像级的多级聚类一致性，以获得高质量的伪掩模。</li><li>我们的方法在帕斯卡VOC和MS COCO数据集上都取得了最先进的性能。</li><li>我们提供了对自监督视觉Transformer特性的全面分析，并详细进行了DINO和DINOv2之间的详细比较。</li></ul><h1 id="相关工作">2.相关工作</h1><h1 id="方法">3.方法</h1><p>​  在本节中，我们将提供轻量级集群框架的详细信息。图2说明了我们的方法的总体结构。</p><figure><imgsrc="../postimages/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/image-20250215220915193.png"alt="image-20250215220915193" /><figcaption aria-hidden="true">image-20250215220915193</figcaption></figure><p>​  图2我们的轻型聚类框架的说明。我们首先利用自监督的Transformer来提取图像的补丁特征。然后，我们在图像级、类别级和数据集级执行聚类。进一步保证了多级聚类的一致性，并提取了二进制补丁级伪掩码。然后对掩模进行上采样和改进。最后，对象区域被裁剪并聚集到它们各自的类中。</p><h2 id="前期准备工作">3.1.前期准备工作</h2><p>​  <strong>查询键值（QKV）注意特征。</strong><br/>​  Transformers[8,41]中的QKV注意特征是用于自我注意的嵌入式表示。它们使模型能够在计算注意力分数的同时，查询、比较和注意输入序列的不同部分。我们发现，自监督视觉Transformer[3,29]的注意特征具有较强的前景-背景可微性。因此，我们可以通过简单的聚类来提取准确的伪掩模预测。在我们的框架中，我们采用余弦距离聚类，因为它易于实现，并且适合处理大量的样本。</p><p>​  <strong>分类（CLS）令牌。</strong><br/>​  VisionTransformer[8]中的分类标记作为输入序列中的第一个标记被添加，并提供了整个图像的汇总表示。由于实例较少，我们利用光谱聚类在整个框架中对CLS标记进行聚类，并且可以有效地捕获非线性关系。</p><h2 id="余弦距离聚类">3.2.余弦距离聚类</h2><p>​  我们采用具有余弦距离的k-均值聚类作为距离度量。余弦相似度有效地捕获向量之间的相似度，而不偏向于那些较大的向量。注意特征A和注意特征B之间的余弦距离可以表示为<span class="math display">\[1-{\frac{A\cdot B}{\|A\|\ \|B\|}}\]</span>​  值得注意的是，具有余弦距离的k均值可以很容易地实现传统的k均值。这是因为当向量被归一化时，余弦距离与欧氏距离成正比。<spanclass="math display">\[\begin{array}{ll}||X_{1}-X_{2}||_{2}^{2}&amp;=X_{1}^{T}X_{1}+X_{2}^{T}X_{2}-2X_{1}^{T}X_{2}\\&amp;=2-2X_{1}^{T}X_{2}\\ &amp;=2(1-X_{1}^{T}X_{2})\end{array}\]</span>​  首先对注意特征进行l2归一化，然后用k-means聚类对特征向量进行余弦距离聚类。</p><h2 id="多级聚类的一致性">3.3.多级聚类的一致性</h2><p>​  注意特征的聚类是我们的框架的主要焦点。我们发现，聚集在不同层次上的特征具有各自的优缺点，因此有不同的用途。分别为图像级、类别级和数据集级聚类设置2、3和4个集群，可以得到所有数据集的最佳结果。</p><p>​  <strong>图像级别。</strong><br/>​  图像级特征是指单个图像的注意特征。它们被分为两组：一组是前景，另一组是背景。虽然在图像级别上产生的二进制掩模是最粗糙的，但它们可以方便地识别前景和背景区域。这有助于从分别在类别级和数据集级聚类中使用的三个和四个集群中选择前景聚类；更多的细节在第3.4节中提供。此外，图像级别的掩模提供了一个目标区域的粗略近似。这允许我们通过排除图像级别的掩模中不存在的区域来消除最终伪掩模中的随机噪声。</p><p>​  <strong>类别级别。</strong><br/>​  类别级特征是指在同一个超类内的所有样本的注意特征。首先，提取所有图像的CLS标记，并将它们聚集到各自的超类中。例如，PASCALVOC包含四个超类：人、动物、车辆和室内。由于同一超类中的对象倾向于共享相似的背景，我们将每个超类中的类别级特征聚为三个簇：一个用于前景，其余两个用于背景。通过正确地选择前景簇，我们可以在类别级获得一个精确的伪掩模。</p><p>​  <strong>数据集级别。</strong><br/>​  数据集级特征指的是数据集中所有样本的注意特征。由于大多数数据集包含不同的背景，我们发现将特征划分为四个集群可以产生最好的结果。一个集群对应于前景，而其余三个集群包含背景图像补丁。通过准确地识别前景聚类，我们可以在数据集级获得高质量的伪掩码预测。</p><p>​  为了确保多级聚类的一致性，我们保留了数据集级和类别级掩模之间的共同前景区域，同时删除了图像级前景掩模中不存在的噪声区域。这可以被表述为：<span class="math display">\[(D a t a s e t\cap C a t e g o r y)-(1-I ma g e)\]</span></p><h2 id="前景集群的选择">3.4.前景集群的选择</h2><p>​  图像级掩模用于在数据集级和类别级聚类结果中识别单个前景聚类。这是因为它们只包含两组，并且很容易区分前景和背景。数据集级或类别级聚类中的前景聚类是基于投票系统来确定的。首先提取所有四个预测角的高置信度图像级掩码为前景或背景。然后，执行掩模翻转检查，以确保掩模覆盖前景而不是背景。最后，我们在数据集级或类别级掩码中选择与二进制前景掩码共享最常见像素的集群索引。此步骤在整个数据集/类别中重复进行，接收到最多投票的集群索引被确定为前景集群。文中给出了算法1中的伪代码。</p><figure><imgsrc="../postimages/A-Lightweight-Clustering-Framework-for-Unsupervised-Semantic-Segmentation/image-20250215223445589.png"alt="image-20250215223445589" /><figcaption aria-hidden="true">image-20250215223445589</figcaption></figure><h2 id="掩码细化">3.5.掩码细化</h2><p>​  通过上述聚类过程得到的伪掩模处于补丁级，分辨率较低。因此，掩模首先被上采样到像素级。然后去除小的分量，最后应用条件随机场（CRF，ConditionalRandom Field）[18]对二进制掩模进行细化。</p><h2 id="类分配">3.6.类分配</h2><p>​  将发现的对象区域裁剪并输入自监督ViT，提取相应的CLS令牌。然后将这些标记被聚集成各自的类。</p><h1 id="实验">4.实验</h1><h1 id="消融研究">5.消融研究</h1><h1 id="限制">6.限制</h1><p>​  虽然我们的框架在PASCAL VOC和MSCOCO数据集上实现了很高的分割性能，但需要注意的是，这两个基准都将背景视为一个类。需要进一步的探索来将背景区域聚集到它们各自的类中。</p><h1 id="结论">7.结论</h1><p>​  在这项工作中，我们引入了一个轻量级的聚类框架的无监督语义分割。我们发现，自监督视觉Transformer的注意特征具有很强的前景-背景可微性。因此，聚类可以有效地分离前景和背景图像斑块。通过在同一数据集、超类和图像内的聚类特征，分别提取数据集级、类别级和图像级掩模。每个层次的结果都表现出独特的特征，通过确保多层次聚类的一致性，我们可以提取出准确的二进制补丁级伪掩码。然后对掩模进行上采样、细化，最后根据对象区域的CLS标记的聚类进行标记。我们的方法在帕斯卡VOC和MSCOCO数据集上都取得了最先进的结果。此外，我们还提供了对自监督ViT特征的全面分析，并详细进行了DINO和DINOv2之间的详细比较。我们希望这项工作将鼓励在无监督学习中的研究人员不仅要优先考虑模型的准确性，而且还要考虑所花费的资源和训练时间。通过这样做，我们可以使分割更加经济有效和高效。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 多级聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CASIA1数据集</title>
      <link href="/CASIA1%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/CASIA1%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 11%" /><col style="width: 11%" /><col style="width: 14%" /><col style="width: 27%" /><col style="width: 17%" /><col style="width: 16%" /></colgroup><thead><tr class="header"><th>Dataset name</th><th>Image Format</th><th>Post-processing</th><th>Forgery types</th><th>Real/Forged Images</th><th>Train/Test Images</th></tr></thead><tbody><tr class="odd"><td>CASIA v1.0</td><td></td><td></td><td>Splicing, copy move, removal</td><td>800/921</td><td></td></tr></tbody></table><p>Xinru Chen, Chengbo Dong, Jiaqi Ji, Juan Cao, and Xirong Li. Imagemanipulation detection by multi-view multi-scale supervision. InProceedings of the IEEE/CVF International Conference on Computer Vision,pages 14185–14193, 2021.</p><p>0.5的阈值</p><p><strong>定位性能</strong></p><p>DiffForensics</p><figure><img src="../postimages/DiffForensics/image-20250213161138796.png"alt="image-20250213161138796" /><figcaption aria-hidden="true">image-20250213161138796</figcaption></figure><p>Attentive and Contrastive Image Manipulation Localization WithBoundary Guidance</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910143404035.png"alt="image-20240910143404035" /><figcaption aria-hidden="true">image-20240910143404035</figcaption></figure><p>AdaIFL:Adaptive Image Forgery Localization via a Dynamic andImportance-aware Transformer Network</p><figure><imgsrc="../postimages/CASIA1%E6%95%B0%E6%8D%AE%E9%9B%86/image-20241120112901191.png"alt="image-20241120112901191" /><figcaption aria-hidden="true">image-20241120112901191</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>评价指标</title>
      <link href="/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<p>在图像篡改检测（Image Forgery Detection）中，常用的评价指标包括<strong>IoU（Intersection over Union）</strong>、<strong>F1分数</strong> 和 <strong>AUC（Area UnderCurve）</strong>。每个指标的意义如下：</p><h3 id="iouintersection-over-union">1. IoU（Intersection overUnion）</h3><ul><li><p><strong>定义</strong>：IoU是评估两个区域（通常是预测结果和真实标签）重叠程度的指标。具体来说，IoU是预测区域与真实区域交集的面积与它们并集的面积之比。</p></li><li><p><strong>公式</strong>： <spanclass="math display">\[    \text{IoU} =\frac{\text{交集面积}}{\text{并集面积}} = \frac{|A \cap B|}{|A \cupB|}    \]</span> 其中 A 是预测区域，B 是真实标签区域。</p></li><li><p><strong>意义</strong>：IoU衡量了预测的篡改区域与实际篡改区域的重叠程度。在篡改检测任务中，IoU值越高，说明预测结果越接近真实值，检测效果越好。</p></li></ul><h3 id="f1-分数">2. F1 分数</h3><ul><li><p><strong>定义</strong>：F1分数是精确度（Precision）和召回率（Recall）的调和平均值，常用来评估模型的分类性能，尤其是在类别不平衡的情况下。</p></li><li><p>公式： <span class="math display">\[    \text{F1} = 2 \cdot\frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} +\text{Recall}}    \]</span> 其中：</p><ul><li><p><strong>精确度（Precision）</strong>：在所有被预测为篡改的像素中，实际篡改的比例：<span class="math display">\[        \text{Precision} =\frac{\text{TP}}{\text{TP} + \text{FP}}        \]</span></p></li><li><p><strong>召回率（Recall）</strong>：在所有实际篡改的像素中，成功检测为篡改的比例：<span class="math display">\[        \text{Recall} =\frac{\text{TP}}{\text{TP} + \text{FN}}        \]</span> <br/> -<strong>TP</strong>、<strong>FP</strong> 和 <strong>FN</strong>分别代表真阳性、假阳性和假阴性。<br/><br /></p></li></ul></li><li><p><strong>意义</strong>：F1分数综合考虑了精确度和召回率，能够有效衡量模型在图像篡改检测中的整体性能，尤其是在类别不平衡（例如篡改区域较小）时，F1分数提供了更为可靠的评估。</p></li></ul><h3 id="f1分数和iou分数的关系">3. F1分数和IoU分数的关系</h3><p>确实，F1分数和IoU在一些情况下会有不同的表现。它们虽然都衡量模型的预测质量，但它们侧重点不同，因此可以在某些情况下出现F1高而IoU低的情况，或者相反。</p><h4 id="f1高但iou低的情况">3.1 F1高但IoU低的情况</h4><ul><li><strong>情境</strong>：如果模型预测的篡改区域覆盖了真实篡改区域的大部分，但预测的区域稍微多了一些（假阳性较多），则会出现F1较高但IoU较低的情况。<br/>-<strong>示例</strong>：假设真实篡改区域是一个小的矩形区域，而模型预测了一个稍大一些的矩形区域，虽然大部分预测区域与真实篡改区域重合，但额外的部分导致了较高的假阳性。<br/><br/> - <strong>真实标签（GroundTruth）</strong>：真实篡改区域是一个小矩形区域，例如大小为 (100 )像素。<br/> -<strong>预测结果（Prediction）</strong>：模型预测了一个稍大的矩形区域，大小为(120 ) 像素。<br/> <br/> 这种情况下：<br/> -<strong>IoU</strong>：由于预测区域稍大，预测区域和真实区域的交集相对较小（例如(100 ) 区域），并且并集较大，因此IoU较低。<br/> - <strong>F1分数</strong>：尽管IoU较低，模型的<strong>精确度</strong>可能仍然较高，因为大部分预测区域与真实篡改区域是重叠的，且召回率也可以维持在较高的水平。因此，F1分数可能较高。</li></ul><h4 id="iou高但f1低的情况">3.2. IoU高但F1低的情况</h4><ul><li><strong>情境</strong>：如果模型只预测了非常精确的小区域，但这个区域漏掉了真实篡改区域的其他部分，或者没有足够的召回（漏检），则IoU可能较高，但F1较低。<br/>-<strong>示例</strong>：假设真实篡改区域比预测区域稍大，模型仅预测了其中的一小部分。<br/><br/> - <strong>真实标签（GroundTruth）</strong>：真实篡改区域是一个较大的区域，大小为 (200 )像素。<br/> -<strong>预测结果（Prediction）</strong>：模型只预测了一个小的矩形区域，大小为(50 ) 像素，尽管这部分区域与真实篡改区域有较高的重叠。<br/> <br/>这种情况下：<br/> -<strong>IoU</strong>：因为预测区域和真实区域有较大重叠，IoU可能较高（例如预测区域和真实区域的交集占大部分并集）。<br/> - <strong>F1分数</strong>：尽管 IoU较高，但<strong>召回率</strong>较低，因为模型未能检测到所有的篡改区域。<strong>精确度</strong>虽然较高（没有太多假阳性），但由于召回率很低，F1分数可能较低。</li></ul><h4 id="总结">总结</h4><ul><li><strong>F1高而IoU低</strong>：这种情况通常表示模型多预测了一些区域，也就是说模型预测的篡改区域覆盖了真实篡改区域的大部分，但同时也出现了较多的假阳性（即模型预测为篡改的区域实际上并没有篡改）。虽然这些预测区域大部分与真实篡改区域重合，因此召回率较高，F1分数较高，但由于多出了额外的假阳性区域，导致IoU较低。</li><li><strong>IoU高而F1低</strong>：这种情况通常表示模型漏掉了一些真实的篡改区域，预测的篡改区域较小，但与真实的篡改区域有很高的重叠，因此IoU较高。但是，由于漏检了很多篡改区域，导致召回率下降，F1分数较低。</li></ul><p>总结起来，F1和IoU的差异主要反映了模型在预测时是<strong>多检了很多</strong>（F1高、IoU低）还是<strong>漏检了很多</strong>（IoU高、F1低）。</p><h3 id="aucarea-under-curve">4. AUC（Area Under Curve）</h3><ul><li><strong>定义</strong>：AUC 是指 ROC（Receiver OperatingCharacteristic）曲线下的面积，表示模型在所有可能的阈值下的分类能力。AUC的值越大，表示模型的分类能力越强。</li><li>公式：AUC是通过计算不同阈值下的真阳性率（TPR）和假阳性率（FPR）得到的：<br/> -<strong>TPR（True Positive Rate）</strong>：召回率，即 <spanclass="math inline">\(\frac{\text{TP}}{\text{TP} +\text{FN}}\)</span>。<br/> - <strong>FPR（False PositiveRate）</strong>：假阳性率，即 <spanclass="math inline">\(\frac{\text{FP}}{\text{FP} +\text{TN}}\)</span>。</li><li><strong>意义</strong>：AUC 值越接近1，说明模型在各种阈值下都能较好地区分篡改区域与非篡改区域，模型的性能越好。AUC是评估二分类问题中模型区分能力的一个重要指标，尤其是在处理不平衡数据集时，AUC可以更加全面地反映模型的表现。</li></ul><h3 id="总结-1">总结</h3><ul><li><strong>IoU</strong>：衡量预测区域与真实区域的重叠度，越高说明检测结果越准确。</li><li><strong>F1分数</strong>：综合评估模型的精确度和召回率，尤其在数据不平衡时能提供更可靠的评估。</li><li><strong>AUC</strong>：评估模型在不同决策阈值下的分类性能，越接近 1表示模型性能越好。</li></ul><p>这三种评价指标在图像篡改检测中各自有不同的侧重点，通常会结合使用，全面评价模型的表现。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DDPM</title>
      <link href="/DDPM/"/>
      <url>/DDPM/</url>
      
        <content type="html"><![CDATA[<p>Denoising Diffusion Probabilistic Models</p><p>Jonathan Ho<br/>UC Berkeley<br/>jonathanho@berkeley.edu</p><p>Ajay Jain<br/>UC Berkeley<br/>ajayj@berkeley.edu</p><p>Pieter Abbeel<br/>UC Berkeley<br/>pabbeel@cs.berkeley.edu</p><h1 id="摘要">摘要</h1><p>​  我们使用扩散概率模型提出了高质量的图像合成结果，这是一类潜在变量模型的考虑来自非平衡热力学。我们最好的结果是通过训练加权变分界设计根据新颖的连接扩散概率模型和去噪分数匹配朗之万动力学，和我们的模型自然承认一个渐进的有损减压方案，可以解释为自回归解码的泛化。在无条件的CIFAR10数据集上，我们获得了9.46分和最先进的FID得分为3.17分。在256x256LSUN上，我们获得了类似于进展性gan的样品质量。我们的实现可以在https://github.com/hojonathanho/diffusion上找到。</p><figure><img src="../postimages/DDPM/denoising-diffusion.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>实验记录</title>
      <link href="/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"/>
      <url>/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="指标">1.指标</h1><p>使用如下指标</p><ul><li>PixelF1(threshold=0.5, mode="double")</li><li>PixelIOU(threshold=0.5, mode="double")</li><li>PixelAUC(threshold=0.5, mode="double")</li></ul><p>图片大小统一1024</p><h1 id="数据集">2.数据集</h1><h2 id="columbia">2.1 Columbia</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Trufor：</span><br><span class="line"></span><br><span class="line">&#123;&quot;test_pixel-level F1&quot;: 0.9372479109609413, &quot;test_pixel-level IOU&quot;: 0.9063901597401924, &quot;test_pixel-level AUC&quot;: 0.9900337934494019, &quot;epoch&quot;: 1&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><table><colgroup><col style="width: 10%" /><col style="width: 30%" /><col style="width: 30%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th></th><th>F1</th><th>IOU</th><th>AUC</th></tr></thead><tbody><tr class="odd"><td>Trufor</td><td>0.9372479109609413</td><td>0.9063901597401924</td><td>0.9900337934494019</td></tr><tr class="even"><td></td><td></td><td></td><td></td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DiffForensics:Leveraging Diffusion Prior to Image Forgery Detection and Localization</title>
      <link href="/DiffForensics/"/>
      <url>/DiffForensics/</url>
      
        <content type="html"><![CDATA[<p>DiffForensics: Leveraging Diffusion Prior to Image Forgery Detectionand Localization</p><p>Zeqin Yu <span class="math inline">\(^{∗,1}\)</span> Jiangqun Ni<spanclass="math inline">\(^{\dagger,2,3}\)</span> Yuzhen Lin<spanclass="math inline">\(^{∗,4}\)</span> Haoyi Deng<spanclass="math inline">\(^4\)</span> Bin Li<spanclass="math inline">\(^{\dagger,4}\)</span></p><p>1中山大学计算机科学与工程学院<br/>2中山大学网络科技学院<br/>3彭成实验室新网络系<br/>4广东深圳大学智能信息处理重点实验室</p><h1 id="摘要">摘要</h1><p>​  由于篡改图像可能会导致对视觉内容的误解，解决图像伪造检测和定位（IFDL）问题已经引起了公众的严重关注。在这项工作中，我们提出了一个简单的假设，即有效的法医方法应该关注图像的介观性质。在此基础上，提出了一种新的基于IFDL任务扩散模型的两阶段自监督框架，即扩散取证。扩散取证从自监督去噪扩散范式开始，通过冻结预先训练的编码器（例如ADE-20K）来继承一般图像特征的宏观特征，同时鼓励解码器学习图像的微观特征表示，强制整个模型聚焦介观表示。将预训练的模型作为先验，利用定制的边缘提示增强模块（ECEM，EdgeCue EnhancementModule）对IFDL任务进行进一步微调，该模块逐步突出被操纵区域内的边界特征，从而以更好的精度细化篡改区域定位。在几个具有挑战性的数据集上的大量实验表明，与其他先进的方法相比，提出的方法的有效性。所提出的差异取证可以显著提高模型的精确篡改检测和精确篡改定位的能力，同时提高其泛化和鲁棒性。</p><h1 id="介绍">1.介绍</h1><p>​  随着GAN[24,25]和扩散模型[2,36]等图像编辑工具的快速发展，处理图像已经变得越来越轻松。用户可以很容易地伪造那些不存在或不可能实现的动态图像。在政治、经济和个人隐私方面，这些伪造的图片所带来的风险是显而易见的。因此，识别图像伪造的对策已成为社会保障领域的一个紧迫课题。为了推动图像取证的前沿，在本研究中，我们研究了图像伪造检测和定位（IFDL）任务，特别是改变图像语义的部分修改。一般来说，IFDL任务在图像级（检测）和像素级（定位）上都涉及二进制分类（真实和伪造）。到目前为止，最先进的[8,16,17,22,30,31,42,45,46]通常建立在基于深度学习的语义分割元框架上，由编码器和解码器两个组件组成。编码器提取图像特征，然后由解码器进行处理，以预测分类结果和伪造掩码。尽管在该领域取得了相当大的进展，但目前的SOTA检测器的性能还不足以进行野外部署，这主要是由于它们在泛化、鲁棒性和检测性能方面的不足。<br/>​  受MesoNet[1]的启发，我们提出通过关注图像的介观特性来解决IFDL问题。事实上，基于伪影（例如，图像噪声）的微观分析不能应用于社交媒体洗钱的背景下，因为后处理将不可避免地削弱法医痕迹。同样地，在更高的语义层面（即宏观）上，人眼很难区分伪造的图像。这就是为什么我们建议采用一种中间的方法。<br/>​  为了实现这一目标，我们提出了一种新的两阶段自监督的IFDL任务的扩散取证方法。训练过程从自监督去噪扩散预训练阶段开始，然后是IFDL的多任务微调阶段。在第一阶段，我们冻结了经过分割任务（如ADE20K）[44]预训练的编码器，以保留提取宏观语义特征的能力，同时鼓励解码器使用自监督去噪扩散范式学习与伪造图像相关的微观特征。通过对上述分别关注宏观和微观特征的编码器和编码器的解码器进行集成，得到了能够学习介观特征表示的模型。在第二阶段，我们然后对预先训练好的模型（包括编码器和解码器）进行微调，并在第二阶段对伪造图像进行监督。我们提出了一个边缘提示增强模块（ECEM），并将其集成到多个尺度的解码器中，旨在突出从粗到细的篡改区域的痕迹。大量的实验表明，我们的方法在几个公共数据集上的泛化和鲁棒性性能方面优于几个最先进的竞争对手。<br/>​  本文的主要贡献总结如下：</p><ul><li>我们提出了一个结合宏观特征和微观特征的IFDL任务的两阶段学习框架，该框架包括自监督去噪扩散的训练前阶段和多任务微调阶段。据我们所知，这是第一个探索IFDL任务的去噪扩散范式的工作。</li><li>我们提出了一种新的边缘提示增强模块，该模块集成在多个尺度的解码器中，以增强被篡改的边缘痕迹从粗到细。</li><li>大量的实验结果表明，我们提出的方法在几个最近出现的数据集上，包括人工操作的图像和人工智能生成的图像上，取得了更好的性能。</li></ul><h1 id="相关工作">2.相关工作</h1><p>​  <strong>去噪扩散概率模型。</strong><br/>​  去噪扩散概率模型（DDPM，denoisingdiffusion probabilitymodel）主要由两个阶段[19]组成，即逐步增加随机噪声的扩散过程，以及从噪声中学习重构所需数据样本的反向过程。除了广泛应用于生成模型[11]，如图像生成[13,33,35,38]、图像生成[10,36]和图像编辑[2,9]，其潜在的表示学习能力也应用于其他计算机视觉任务，如图像分割[4,6]和异常检测[41,43]。通过执行噪声估计和重建过程，去噪扩散范式可以有效地学习图像的微观噪声模式。同时，噪声分析是解决IFDL任务的有力解决方案之一。因此，为IFDL任务引入去噪扩散范式是有意义的。</p><p>​  <strong>图像伪造品的检测和定位。</strong><br/>​  大多数现有的方法都是进行像素级分类来识别伪造区域，[8,16,17,30,31,45,46]使用ImageNet预先训练的权重作为其在篡改检测任务中的特征提取编码器的基础。这些方法试图通过探索宏观特征来提高被篡改图像的检测性能。然而，在处理看不见的篡改图像或未知攻击时，它们在通用性和鲁棒性方面退化。最近的方法[5,7,18,21,22,28,40,42]旨在通过自监督学习发现更有效的篡改微特征，以提高IFDL的性能。mannet[42]和SPAN[22]设计了一个自监督学习任务来学习鲁棒的图像处理轨迹。CAT-Net[28]对JPEG图像进行双压缩检测，获得具有微观特征权值的编码器和宏观特征权值的并行组合，形成双流网络，从而提高对JPEG图像的拼接检测性能。CA-IFL[40]和Bi等[5]分别提出了基于小波表示学习策略和设计JPEG压缩操作链跟踪预训练获得微观特征权重能够学习JPEG压缩跟踪，用于提高对JPEG压缩的定位性能。Chen等人[7]和Hu等人[21]通过掩膜重建真实或被篡改的人脸，RealForensics[18]比较了不同模式之间的密集联系。对于这些方法，[7,18,21]将寻求学习具有更好的表示能力的微观特征，并在面对跨数据集测试时提高泛化性能。</p><figure><img src="../postimages/DiffForensics/image-20250115210800672.png"alt="image-20250115210800672" /><figcaption aria-hidden="true">image-20250115210800672</figcaption></figure><p>​  然而，从表1中可以看出，在随机初始化解码器权值的同时，在编码器中保留宏特征或微特征权值的训练策略，但是不能在IFDL任务中充分利用这两种特征。<br/>​  在本文中，我们提出了一种新的编解码器模型的训练方案。对于编码器，我们利用语义分割任务中预先训练的权值，并冻结它们来提取全面的宏观特征。对于解码器，我们引入了一个基于DDPM的范式来捕获复杂的微观特征。结合上述过程，使模型更加关注图像的介观特性。这样的集中有利于后续的微调阶段，使模型能够更精确地用于IFDL任务。</p><h1 id="提出的方法">3.提出的方法</h1><p>​  在本节中，我们首先介绍了DiffForensics的概述，如图2所示。</p><figure><img src="../postimages/DiffForensics/image-20250115211126117.png"alt="image-20250115211126117" /><figcaption aria-hidden="true">image-20250115211126117</figcaption></figure><p>​  在架构方面，我们的方法包括一个编码器<spanclass="math inline">\(E_{\phi}\)</span>和一个解码器<spanclass="math inline">\(D_{\theta}\)</span>，它们分别由两组权重<spanclass="math inline">\(\phi\)</span>和<spanclass="math inline">\(\theta\)</span>参数化。我们提出的框架的训练过程包括两个阶段：自监督去噪扩散预训练和多任务微调。后续的小节将提供每个阶段的细节。</p><h2 id="自监督去噪扩散预训练">3.1.自监督去噪扩散预训练</h2><p>​  <strong>流程线。</strong><br/>​  在这一阶段，我们的目标是使模型聚焦于图像的介观性质，这可以进一步有效地微调为IFDL任务。<br/>​  对于编码器，我们利用来自SegFormer[44]的transformer编码器块，并应用来自语义分割任务（如ADE20K）的预先训练的权重<spanclass="math inline">\(\phi^*\)</span>。我们冻结了权值，以保留提取宏观语义特征的能力。对于解码器，我们使用了Unet[37]中常用的解码器块。考虑到DDPM[19]由两个相反的过程，添加噪声和反向去噪，它可以有效地学习图像的微观噪声表示。在此基础上，我们提出了一种基于去噪扩散的范式作为自我监督的代理任务来优化<spanclass="math inline">\(\theta\)</span>，而不利用伪造监督。整体训练过程如图2左侧所示，详见算法1。</p><figure><img src="../postimages/DiffForensics/image-20250213202146868.png"alt="image-20250213202146868" /><figcaption aria-hidden="true">image-20250213202146868</figcaption></figure><figure><img src="../postimages/DiffForensics/image-20250213202202765.png"alt="image-20250213202202765" /><figcaption aria-hidden="true">image-20250213202202765</figcaption></figure><p>​  具体来说，给定一个图像<spanclass="math inline">\(x_{0}\in\mathbb{R}^{3\times h\timesw}\)</span>，以及时间步长t，我们通过扩散过程<spanclass="math inline">\(q(x_{t}|x_{t-1})\)</span>添加噪声<spanclass="math inline">\(\epsilon\)</span>来破坏<spanclass="math inline">\(x_0\)</span>，并执行逆过程<spanclass="math inline">\(P(\phi^{*},\theta)(x_{t-1}|x_{t})\)</span>，将噪声估计为<spanclass="math inline">\(\epsilon_{(\phi^{*},\theta)}(x_{t}|x_{0})=D_{\theta}(E_{\phi}(x_{0}),t)\)</span>，然后进行去噪。通过这种方式，我们训练整个自动编码器模型<spanclass="math inline">\(E_{\phi}^{\star}\circL_{\theta}\)</span>（即冻结编码器和可训练解码器），使重构误差目标函数最小化如下：<span class="math display">\[\ell_{s}=\mathbb{E}_{t\in[1,T],x_{0}\simq(x_{0}),\epsilon\simS(\nu,N,\gamma)}[||\epsilon-\epsilon(\phi^{*},\theta)]|^{2}]\]</span>​  通过结合上述宏观和微观表示，我们引导整个自动编码器<spanclass="math inline">\(E_{\phi}^{\star}\circL_{\theta}\)</span>集中研究图像的介观特征。</p><p>​  <strong>单形噪声</strong>。<br/>​  与普通的DDPM[19]不同，我们通过在扩散过程中加入单形噪声[43]而不是高斯噪声来破坏<spanclass="math inline">\(x_0\)</span>。</p><figure><img src="../postimages/DiffForensics/image-20250115215528669.png"alt="image-20250115215528669" /><figcaption aria-hidden="true">image-20250115215528669</figcaption></figure><p>​  如图3所示，这种噪声对标准高斯扰动的潜在好处是直观的：图像的破坏更有结构化（例如，被篡改区域的边缘），去噪过程将能够“修复”它们，从而促进对这种结构化异常的学习。对于单形噪声<spanclass="math inline">\(\epsilon\simS(\nu,N,\gamma)\)</span>的超参数，我们设置了起始频率<spanclass="math inline">\(\nu=2^{-6}\)</span>，跨度N = 6和衰减γ = 0.8。</p><h2 id="多任务微调">3.2.多任务微调</h2><p>​  <strong>流程线。</strong><br/>​  经过预训练后，我们在IFDL监督下（即伪造标签和掩码）对预训练的自动编码器（编码器和解码器）进行微调。根据我们的消融研究，多任务学习可以帮助学习更好的代表性特征和良好的性能。因此，我们在解码器的后一个部分中添加多任务头（即检测和定位头），如图2右侧所示。</p><figure><img src="../postimages/DiffForensics/image-20250213151946406.png"alt="image-20250213151946406" /><figcaption aria-hidden="true">image-20250213151946406</figcaption></figure><p>​  <strong>边缘提示增强模块ECEM。</strong><br/>​  为了进一步挖掘被篡改区域的细微痕迹，我们引入了一个边缘提示增强模块，以增强在水平和垂直方向上的三个尺度解码器块的输出特征上的边缘线索，如图4所示。</p><figure><img src="../postimages/DiffForensics/image-20250115215807419.png"alt="image-20250115215807419" /><figcaption aria-hidden="true">image-20250115215807419</figcaption></figure><p>​  具体来说，设<spanclass="math inline">\({\{dk\}}^3_{k=1}\)</span>是每个解码器块的输出特征映射。注意，<spanclass="math inline">\(d_k\in{\bf\mathbb{R}}^{b\times c\times h\timesw}\)</span>是一个四维特征向量，我们只在<spanclass="math inline">\(\mathbf{d}_{k}\)</span>的最后二维（即高度和宽度）中进行以下过程。首先，我们计算<spanclass="math inline">\(\mathbf{d}_{k}\)</span>中相邻行之间的差值，然后取绝对值来保持一致的梯度方向。这个绝对差异被重新分配到当前行，增强了行方向上的边缘提示特征映射。随后，我们将相同的过程应用于增强特征的列，其中计算相邻列之间的差值，并取其绝对值，以确保梯度方向的一致性。这样，我们就得到了<spanclass="math inline">\(\mathbf{d}_{k}\)</span>的边缘增强特征，记为<spanclass="math inline">\(\mathbf{g}_{k}\)</span>。上述流程可表述为： <spanclass="math display">\[\mathbf{g}_{k}=|\mathbf{V}*|\mathbf{H}*\mathbf{d}_{k}||\]</span>​  其中，∗为卷积运算，|·|为abs运算。H = [1，−1]和V =[1，−1]⊤分别是水平方向和垂直方向上的边缘增强算符。<br/>​  之后，我们计算区别<spanclass="math inline">\(\mathbf{d}_{k}\)</span>和<spanclass="math inline">\(\mathbf{g}_{k}\)</span>和采用3×3卷积减少维度，最后使用sigmoid函数规范化线索特征映射0-1，最后样本相同大小的输入图像获得我们的边缘预测概率地图<spanclass="math inline">\(f_k^e\)</span>，可以标记为： <spanclass="math display">\[f_{k}^{e}=U\left(\sigma\left(F_{c o v}\left({\bfd}_{k}-{\bf g}_{k}\right)\right)\right).\]</span>​  其中Fcov是一个3×3卷积运算，σ是sigmoid归一化，U是一个上采样运算，得到的每个解码器的边缘预测概率映射<spanclass="math inline">\(f_k^e\)</span>和边缘标签<spanclass="math inline">\(y^e\)</span>用于损失迭代。我们在<spanclass="math inline">\(\mathbf{d}_{k}\)</span>的所有三个尺度上都使用了上述的边缘提示增强模块ECEM。</p><p>​  <strong>损失函数</strong><br/>​  该方法有三种监督类型，即局部分割监督<spanclass="math inline">\(\mathcal{L}_{seg}\)</span>、检测分类监督<spanclass="math inline">\(\mathcal{L}_{clf}\)</span>和边缘线索监督<spanclass="math inline">\(\mathcal{L}_{edg}\)</span>。<br/>​  对于像素级定位分割监督，我们使用加权<spanclass="math inline">\(\ell_{w b c e}\)</span>和<spanclass="math inline">\(\ell_{dice}\)</span>[32]的组合。 <spanclass="math display">\[\mathcal{L}_{s eg}\left(x\right)=\lambda_{0}^{s}\ell_{w b ce}+\left(1-\lambda_{0}^{s}\right)\ell_{d i c e}.\]</span> ​  其中，<spanclass="math inline">\(\lambda_{0}^{s}\)</span>为分割平衡权值，加权分割<spanclass="math inline">\(\ell_{w b c e}\)</span>和<spanclass="math inline">\(\ell_{dice}\)</span>分别为： <spanclass="math display">\[\ell_{wbce} =-\frac{1}{N}\sum_{i,j}\left(\lambda_{1}^{s}\cdot y_{i,j}^{s}\cdot\logf^{s}(x_{i,j}\right)+\lambda_{2}^{s}\cdot(1-y_{i,j}^{s})\cdot\log{(1-f^{s}(x_{i,j}))}).\]</span></p><p><span class="math display">\[\ell_{dice}=1-\frac{2\sum_{i,j}f^s(x_{i,j})\cdoty_{i,j}^s}{\sum_{i,j}(f^s(x_{i,j}))^2+\sum_{i,j}(y_{i,j}^s)^2}.\]</span></p><p>​  其中<spanclass="math inline">\(y_{i,j}^{s}\in\{0,1\}\)</span>是像素级边界标签，代表<spanclass="math inline">\(\{i.j\}\)</span>处的像素是否被篡改。<spanclass="math inline">\(\lambda_{1}^{s}\)</span>和<spanclass="math inline">\(\lambda_{2}^{s}\)</span>分别用来平衡篡改像素和真实像素的权重，这鼓励网络更关注那些困难像素样本。<br/>​  对于边缘监督，我们使用同样的dice损失作为上面的分割监督，但是这里，为了逐步标准化从粗粒度到细粒度的篡改位置边缘，我们设计的多尺度监督权重，即概率图<spanclass="math inline">\(\{f_{k}^{e}\}_{k=1}^{3}\)</span>，旨在给予细粒度的边缘监督更大的权重，在标准化粗粒度边缘监督的同时，使<spanclass="math inline">\(f_{k}^{e}\)</span>能够更好地细化一阶段细粒度边缘监督<spanclass="math inline">\(f_{k}^{e-1}\)</span>。 <spanclass="math display">\[\mathcal{L}_{e dg}\left(x\right)=\sum_{k=1}^{3}\frac{1}{2^{k-1}}\ell_{d i ce}\left(f_{k}^{e},y^{e}\right).\]</span>​  对于图像级的检测和分类监督，为了缓解图像级数据的正负样本的不平衡，我们使用了加权<spanclass="math inline">\(\ell_{wbce}\)</span>。 <spanclass="math display">\[\mathcal{L}_{c l f}(x)=-(\lambda_{0}^{c}\cdot\y^{c}\cdot\logf^{c}(x)+\lambda_{1}^{c}\cdot(1-y^{c})\cdot\mathrm{log}(1-f^{c}(x))).\]</span>​  其中，<spanclass="math inline">\(y^{c}\)</span>为图像级二值标签，<spanclass="math inline">\(f^{c}(x)\)</span>为分类预测结果。由于图像水平上的正负样本的数量容易测量，我们自动将篡改权重设为<spanclass="math inline">\(\lambda_{0}^{c}~=\lfloor \frac{10*N u m_{F}}{N um_{F+R}}\rfloor / 10\)</span>，并设置真实权重设为<spanclass="math inline">\(\lambda_{1}^{c}~=\lfloor \frac{10*N u m_{R}}{N um_{F+R}}\rfloor / 10\)</span>，<span class="math inline">\(N um_{F}\)</span>和<span class="math inline">\(N um_{R}\)</span>分别表示伪造图像和真实图像的数量。<br/>​  最后，我们将总损失<spanclass="math inline">\(\mathcal{L}\)</span>定义为上述三个损失的加权组合，公式为：<span class="math display">\[\mathcal{L}=\alpha\cdot(\mathcal{L}_{s eg}+\mathcal{L}_{e d g})+\beta\cdot\mathcal{L}_{c l f}.\]</span>​  其中<span class="math inline">\(\alpha,\beta\in[0,1]\)</span>。</p><h1 id="实验">4.实验</h1><h2 id="实验设置">4.1.实验设置</h2><p>​  <strong>数据集。</strong><br/>​  考虑到可用性和通用性，我们选择了一些具有挑战性的基准数据集来评估我们的方法，其中CASIAv2.0[14], Fantasitic Reality [26], CASIAv1+ [8], Columbia [20], NIST16[15],IMD2020 [34], DSO-1 [12] 和Korus [27]，这些数据集使用传统的图像编辑工具篡改，而AutoSplicing[23]和OpenForensics[29]使用深度生成模型（DGMs，deepgenerativemodels）篡改。这些数据集的详细信息见附录，不同阶段的配置细节如下：<br/>​  (1)去噪扩散预训练：我们将CASIAv2.0[14] 和 Fantasitic-Reality[26]的所有数据（伪造和真实）混合进行自我监督预训练，在此阶段不使用伪造监督。<br/>​  (2)多任务微调：我们还利用了CASIAv2.0[14] 和 Fantasitic-Reality [26]数据集及其伪造监督。请注意，我们只对Fantasitic-Reality[26]数据集使用伪造图像，以平衡伪造的数量和真实像素的整体。<br/>​  (3)评估：为了验证泛化性能，我们在其他图像编辑伪造数据集上评估了我们的方法，即CASIAv1+ [8], Columbia [20], NIST16 [15], IMD2020 [34], DSO-1 [12] 和Korus[27]数据集。我们还使用了两个由近期高级深度生成模型DGMs建立的数据集，即AutoSplicing[23]和OpenForensics[29]。</p><p>​  <strong>实施细节。</strong><br/>​  我们使用4个NVIDIATeslaA100GPUs（80GB内存）在PyTorch深度学习框架上进行实验。我们为这两个阶段执行以下参数配置：<br/>​  (1)去噪扩散预训练：在训练前阶段，我们将输入图像调整到512×512，并应用了AdamW优化器。我们将训练超参数设置为10−4，扩散步长T设置为1000，批大小设置为16，epoch设置为100。<br/>​  (2)多任务微调：在微调阶段，我们还将输入图像调整到512×512，并应用了AdamW优化器。我们将学习率的训练超参数设置为10−4，批大小为32，epoch为50，固定时间嵌入为t=5（细节可在消融研究中看到）。为了平衡伪造检测和定位的性能，我们将篡改定位<spanclass="math inline">\(\mathcal{L}_{s e g}\)</span>和边缘监督<spanclass="math inline">\(\mathcal{L}_{e d g}\)</span>的权重设置为α =0.8，其中<span class="math inline">\(\mathcal{L}_{s eg}\)</span>中的λ0、λ1和λ2分别为0.1、2和0.5。篡改检测的监督<spanclass="math inline">\(\mathcal{L}_{c lf}\)</span>的权重β设置为0.1，<spanclass="math inline">\(\lambda_{0}^{c}\)</span>和<spanclass="math inline">\(\lambda_{1}^{c}\)</span>分别为0.7和0.3。</p><p>​  <strong>评估指标。</strong><br/>​  对于伪造定位，我们报告了像素级F1和AUC（接收机工作特征曲线的曲线下面积）。对于伪造检测，除了图像级ACC和AUC外，我们还进一步报告了EER（等错误率）来评估误报和遗漏检测性能。对于伪造检测和本地化，默认阈值均为0.5，除非另有指定。</p><h2 id="与最先进的方法的比较">4.2.与最先进的方法的比较</h2><p>​  为了进行公平的比较，我们关注具有可用代码或预训练模型的方法，如下。</p><p>​  <strong>(1)可提供预先训练过的模型：</strong><br/>​  为了避免偏差，我们只包括在不同于测试数据集的数据集上训练的方法。ManTra-Net[42]在100万个私有数据集上进行了预训练。MVSS-Net[8]在CASIA2数据集上进行了预训练。对于这些方法，我们直接使用它们的预先训练过的模型来进行评估。</p><p>​  <strong>(2)可用代码：</strong> <br/>​  H-LSTM [3]，HP-FCN[30]，GSRNet [45]、SPAN [22]，SATL-Net [46]、CAT-Net [28]、PSCCNet[31]和HiFi-Net[17]。对于这些方法，我们使用与我们相同的实验设置来重新训练它们，并使用最优的超参数配置。</p><p>​  <strong>定位性能评估。</strong><br/>​  表2显示了伪造的定位性能。</p><figure><img src="../postimages/DiffForensics/image-20250213161138796.png"alt="image-20250213161138796" /><figcaption aria-hidden="true">image-20250213161138796</figcaption></figure><p>​  我们观察到，我们的方法在所有数据集上都取得了优越的性能。值得一提的是，专门为DGM伪造检测和定位而设计的HiFi-Net在DGM伪造数据集上取得了最好的F1分。总的来说，我们提出的方法达到了最佳的平均性能，这证明了其有效性。</p><p>​  <strong>检测性能评价。</strong><br/>​  在[8,31]之后，我们使用具有真实图像和篡改图像的数据集进行了图像级分类的评估。表3显示了伪造检测的性能。</p><figure><img src="../postimages/DiffForensics/image-20250213161821208.png"alt="image-20250213161821208" /><figcaption aria-hidden="true">image-20250213161821208</figcaption></figure><p>​  我们观察到，我们的方法在所有数据集上也取得了优越的性能。总的来说，该方法获得了最佳的平均AUC、EER和第二好的ACC，这也证明了其有效性。需要注意的是，对于具有极不平衡的数据集，如IMD2020[34]（真实：414，篡改：2010），与阈值相关的度量不能评估整体性能。虽然我们的方法在阈值为0.5时没有显示出更好的ACC评分，但它在AUC评分方面取得了更好的整体性能，在EER方面取得了更好的平衡错误率。</p><p>​  <strong>鲁棒性。</strong><br/>​  我们进一步评估了在社交媒体洗钱中面对常见的图像扰动时，即JPEG压缩和高斯噪声的鲁棒性。我们报告了F1和AUC评分的平均值作为指标。</p><figure><img src="../postimages/DiffForensics/image-20250213162035230.png"alt="image-20250213162035230" /><figcaption aria-hidden="true">image-20250213162035230</figcaption></figure><p>​  可以看出，该方法在伪造定位和伪造检测任务中都表现出更好的鲁棒性性能。特别是在伪造定位方面，通过对宏特征和微特征的双重支持，取得了显著的性能优势。</p><h2 id="消融研究">4.3.消融研究</h2><p>​  本节分析了在提出的两阶段训练阶段的几个关键组成部分的有效性。</p><p>​  <strong>自监督去噪扩散预训练。</strong><br/>​  在这一部分中，我们分析了扩散噪声和模型权重对去噪扩散预训练的影响。如表4所示，我们验证了在不同权重组合下的扩散噪声选择的性能。</p><figure><img src="../postimages/DiffForensics/image-20250213162224283.png"alt="image-20250213162224283" /><figcaption aria-hidden="true">image-20250213162224283</figcaption></figure><p>​  首先，第1行不执行DDPM预训练的基线网络，第2行和第3行使用高斯噪声进行DDPM预训练，第4行和第5行使用单形噪声进行DDPM预训练。比较第2行和第3行，比较第5行，可以看出单纯形噪声预训练在人工篡改和综合篡改数据集上都取得了更好的效果，说明单纯形噪声对微篡改的影响更大。对痕迹的感知学习更为明显。加载的权重也是本文的重点。通过比较第1、3、5行，我们可以看出本文提出的编码器宏特征提取与解码器微观特征提取相结合的策略可以有效地提高IFDL任务的性能。通过比较第2行和第4行与其他三行，可以看到编码器的DDPM训练可能会导致原始宏观特征的灾难性遗忘。<br/>​  此外，我们在图7中展示了使用t-SNE[39]可视化的学习特征的嵌入空间。</p><figure><img src="../postimages/DiffForensics/image-20250213162550146.png"alt="image-20250213162550146" /><figcaption aria-hidden="true">image-20250213162550146</figcaption></figure><p>​  我们可以看到，在最终的方案中，噪声选择和编码解码器权重选择的组合可以有效地区分真实样本和被篡改样本的特征分布。综合结果表明，本文提出的训练方法将宏观特征与监督权值和单纯形噪声DDPM预训练得到的微观特征相结合，获得了最佳的IFDL性能。</p><p>​  <strong>多任务微调。</strong><br/>​  在此，我们分析了损失函数和时间嵌入<spanclass="math inline">\(t_f\)</span>的影响。</p><p>​  <strong>(1)损失函数的组合：</strong><br/>​  对于<spanclass="math inline">\(\mathcal{L}_{s e g}\)</span>和<spanclass="math inline">\(\mathcal{L}_{c l f}\)</span>，<spanclass="math inline">\(\ell_{s1}\)</span>和<spanclass="math inline">\(\ell_{c1}\)</span>代表加权的<spanclass="math inline">\(\ell_{bce}\)</span>，<spanclass="math inline">\(\ell_{s2}\)</span>和<spanclass="math inline">\(\ell_{c2}\)</span>代表未加权的<spanclass="math inline">\(\ell_{bce}\)</span>。<br/>​  对于<spanclass="math inline">\(\mathcal{L}_{s eg}\)</span>中的每个参数<br/>​    (i) <spanclass="math inline">\(\ell_{e1}\)</span>：将具有ECEM的边缘监督添加到最后的解码器输出中，其权重为1。<br/>​    (ii)<spanclass="math inline">\(\ell_{e2}\)</span>：使用ECEM对所有解码器输出添加边缘监控，但权重均为1。<br/>​    (iii)<spanclass="math inline">\(\ell_{e3}\)</span>：本文提出的ECEM多尺度加权边缘监督为粗粒度边缘监督设置了较小的权重，为细粒度边缘监督设置了较大的权重。</p><figure><img src="../postimages/DiffForensics/image-20250213163518180.png"alt="image-20250213163518180" /><figcaption aria-hidden="true">image-20250213163518180</figcaption></figure><p>​  通过比较表5的第一行和最后一行，可以看出，多权值、多尺度边缘提示增强了监督损失，不仅大大提高了篡改定位任务，而且提高了篡改检测任务的性能。通过对第二、第三、最后行的比较，本文针对不同粒度的尺度边缘设计了不同的加权策略，可以更好地增强不同尺度篡改区域的痕迹。最后，通过比较第4行、第5行和最后一行，分别对<spanclass="math inline">\(\mathcal{L}_{s e g}\)</span>和<spanclass="math inline">\(\mathcal{L}_{c lf}\)</span>进行加权，可以在IFDL中实现一定的性能提高。<br/>​  我们还在图6中描述了一些定性的结果。</p><figure><img src="../postimages/DiffForensics/image-20250213163846478.png"alt="image-20250213163846478" /><figcaption aria-hidden="true">image-20250213163846478</figcaption></figure><p>​  从左到右，可以观察到，在多尺度边缘提示增强模块的监督下，被篡改区域的位置和轮廓更精确地定位。同时，该方法还能有效地降低真实图像的误报风险。</p><p>​  <strong>(2)固定时间嵌入时间<spanclass="math inline">\(t_f\)</span>：</strong><br/>​  我们使用T∈[0,1000]对扩散预训练进行去噪，并在多任务微调过程中采用固定时间步长<spanclass="math inline">\(t_f\)</span>进行训练和测试。为了优化<spanclass="math inline">\(t_f\)</span>以获得更好的特征表示，我们在t∈[0,1000]处进行了网格搜索，结果汇总如表6所示。</p><figure><img src="../postimages/DiffForensics/image-20250213164039645.png"alt="image-20250213164039645" /><figcaption aria-hidden="true">image-20250213164039645</figcaption></figure><p>​  我们观察到，较小的t有利于学习篡改痕迹，因此，我们使用<spanclass="math inline">\(t_f=5\)</span>作为时间嵌入参数。</p><h1 id="结论">5.结论</h1><p>​  在本研究中，我们提出了一种新的两阶段自监督结构的方法，用于图像伪造检测和定位任务。在第一个去噪扩散预训练阶段，对对分割任务进行预训练的编码器进行冻结，而解码器采用自监督去噪扩散范式进行训练。它旨在鼓励模型集中于图像的介观性质。经过预训练后，我们使用监督多任务框架对预训练的模型进行微调，并在解码器中引入边缘提示增强模块，以增强篡改痕迹从粗到细。大量的实验结果表明，我们提出的方法在检测和定位性能方面，在几个新兴的数据集（包括人工操作和人工智能生成的图像）上，比目前最先进的竞争对手取得了更好的性能。</p><p>​  <strong>致谢</strong><br/>​  国家自然科学基金资助项目(项目No.U23B2022、U22A2030、U22B2047)、广东省基础应用基础研究重大项目（GrandNo.2023B030300001010）、广东省学生科技创新培养专项项目（pdjh2022b0444）。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> UNet </tag>
            
            <tag> 两阶段 </tag>
            
            <tag> 扩散模型去噪预训练 </tag>
            
            <tag> 边缘增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SUMI-IFL:An Information-Theoretic Framework for Image Forgery Localization with Sufficiency and Minimality Constraints</title>
      <link href="/SUMI-IFL/"/>
      <url>/SUMI-IFL/</url>
      
        <content type="html"><![CDATA[<p>SUMI-IFL: An Information-Theoretic Framework for Image ForgeryLocalization with Sufficiency and Minimality Constraints</p><p>Ziqi Sheng<span class="math inline">\(^1\)</span>, Wei Lu1<spanclass="math inline">\(^*\)</span>, Xiangyang Luo<spanclass="math inline">\(^{2*}\)</span>, Jiantao Zhou<spanclass="math inline">\(^3\)</span>, Xiaochun Cao<spanclass="math inline">\(^4\)</span></p><p>1中山大学广东省信息安全技术重点实验室教育部信息技术重点实验室计算机科学与工程学院<br/>2数学工程和高级计算的国家重点实验室<br/>3澳门大学计算机与信息科学系智能城市物联网国家重点实验室<br/>4中山大学深圳校区网络科技学院</p><h1 id="摘要">摘要</h1><p>​  图像伪造定位（IFL）是防止被篡改的图像误用和保护社会安全的关键技术。然而，由于图像篡改技术的快速发展，提取更全面、更准确的伪造线索仍然是一个紧迫的挑战。为了解决这些挑战，我们引入了一个新的信息理论的IFL框架，名为SUMI-IFL，它对伪造特征表示施加了足够的视图和最小化视图约束。首先，在互信息的理论分析基础上，在特征提取网络上施加充分视图约束，确保潜在伪造特征包含全面的伪造线索。考虑到仅从单一方面获得的伪造线索可能是不完整的，我们通过从多个角度整合多个单独的伪造特征来构建潜在的伪造特征。其次，基于信息瓶颈，对特征推理网络施加最小视图约束，以实现精确、简洁的伪造特征表示，以对抗任务相关特征的干扰。大量的实验表明，SUMI-IFL的性能优于现有的最先进的方法，不仅在数据集中的比较上，而且在跨数据集的比较上。</p><h1 id="引言">1. 引言</h1><p>​  虽然潜在的伪造特性能够捕获足够的伪造痕迹，但不可避免地会引入一些与任务无关的信息。信息瓶颈（IB）理论为理解信息处理中压缩和准确性之间的最佳权衡提供了理论基础（Tishby，Pereira和Bialek2000）。基于IB理论，我们推导了最小视图约束，以确保最终特征简洁，在保留与任务相关的信息时最小化与任务相关的信息。特别地，我们将离散的地面真值掩模映射到一个连续的伪造特征空间，以指导伪造特征消除与任务无关的信息。利用充分视图约束和最小视图约束的好处，SUMI-IFL在内数据集和跨数据集实验上获得了与其他先进技术相比具有竞争力的性能。综上所述，我们的贡献如下：</p><ul><li>我们提出了一种创新的信息理论IFL框架，名为SUMI-IFL，它将充分性视图和最小性视图约束应用于伪造特征表示，确保框架学习全面的伪造线索，并对抗任务不相关特征的干扰，并得到严格的理论分析的支持。</li><li>在特征提取网络中应用充分视图约束，保证潜在的伪造特征包含全面的伪造线索，并由几个单独的伪造特征构造。</li><li>在特征推理网络中应用最小视图约束，通过减少与任务无关的信息，获得简洁的伪造特征，从而帮助模型抵抗与任务无关的特征的干扰。</li></ul><h1 id="相关工作">2.相关工作</h1><h1 id="方法">3.方法</h1><h2 id="概括">3.1概括</h2><p>​  如图2所示，在一个标准的IFL任务中，我们将<spanclass="math inline">\(h_{\theta}=(r\circe)\)</span>表示为一个参数为<spanclass="math inline">\(\theta\)</span>的深度神经网络。</p><figure><img src="../postimages/SUMI-IFL/image-20250115165309912.png"alt="image-20250115165309912" /><figcaption aria-hidden="true">image-20250115165309912</figcaption></figure><p>​  这里，<span class="math inline">\(e:\mathbb{R}^{dx}\to\mathbb{R}^{d f}\)</span>将输入图像X映射到潜在伪造特征<spanclass="math inline">\(\mathcal F\)</span>，<spanclass="math inline">\(r:\mathbb{R}^{d f}\to\mathbb{R}^{dz}\)</span>进一步将潜在特征F映射到最终预测特征Z，因此e是一个特征提取网络，<spanclass="math inline">\(\mathcal F =e(X)\)</span>，r是一个特征推理网络，<spanclass="math inline">\(r({\mathcal F}) =r(e(X))=Z\)</span>。此外，一组来自不同骨干的个体特征记为<spanclass="math inline">\(\mathcal F =e(X)=\{f_1,f_2,\dots,f_n\}\)</span>，n表示特征提取网络中的骨干数量。每个特征都有自己的特征图大小和通道尺寸，记为<spanclass="math inline">\(f_{i}\in\mathbb{R}^{C\times H\timesW}\)</span>，其中C、H和W分别表示通道数、特征高度和宽度，<spanclass="math inline">\(i=1\dotsn\)</span>。<br/>​  对特征提取网络e应用了充分视图约束，以保证特征表示的全面性。具体地说，我们通过最大化<spanclass="math inline">\(\mathcalF\)</span>和地真标签之间的互信息来确保潜在伪造特征<spanclass="math inline">\(\mathcalF\)</span>的全面性。此外，我们从不同的角度揭示了独立的伪造特征<spanclass="math inline">\(f_i\)</span>，以确保被篡改图像中隐藏的任何伪造痕迹不会被遗漏。<br/>​  同时，对特征推理网络r应用了最小视角约束，保证了简洁的伪造特征Z在保留任务相关信息的同时丢弃与任务无关的信息。我们从信息瓶颈理论中推导出该约束，得到了该约束的形式表示。</p><h2 id="充分视图约束">3.2充分视图约束</h2><p>​  通过最大化<span class="math inline">\(\mathcalF\)</span>和地面真标签之间的互信息，构造了充分视图约束<spanclass="math inline">\(\mathcal L_{SU}\)</span>。在本节中，我们将提供<span class="math inline">\(\mathcalL _{SU}\)</span>的关键推导和特征提取网络的详细结构。</p><p>...</p><h2 id="最小视图约束">3.3最小视图约束</h2><p>​  从信息瓶颈理论推导出最小视图约束<spanclass="math inline">\(\mathcal L_{MI}\)</span>，以确保简洁的伪造特征在保留与任务相关的信息的同时有效地丢弃与任务无关的信息。在本节中，我们提供了最小视图约束<spanclass="math inline">\(\mathcal L_{MI}\)</span>的关键推导和推理网络的详细结构。</p><p>...</p><h2 id="总体目标">3.4总体目标</h2><p>​  总损失函数$L <spanclass="math inline">\(包括四个部分：局部损失\)</span>L <em>{loc}<spanclass="math inline">\(、充分视图约束\)</span>L </em>{SU}<spanclass="math inline">\(、最小视图约束\)</span>L <em>{MI}<spanclass="math inline">\(和辅助掩模损失\)</span>L </em>{auc}<spanclass="math inline">\(：\)</span><span class="math inline">\(\mathcal{L}=\mathcal {L}_{l o c}+\lambda_{1}\times\mathcal {L}_{SU}+\lambda_{2}\times\mathcal {L}_{M I}+\lambda_{3}\times \mathcal {L}_{au x}\)</span>$ ​  其中，λ1 = 0.1、λ2 = 1和λ3 = 0.1。</p><h1 id="实验">4实验</h1><h2 id="设置">4.1设置</h2><p>​  <strong>数据集</strong><br/>​  表1给出了在我们的方法中使用的训练和测试数据集。</p><figure><img src="../postimages/SUMI-IFL/image-20250115170527818.png"alt="image-20250115170527818" /><figcaption aria-hidden="true">image-20250115170527818</figcaption></figure><p>​  我们首先对四个公共数据集的训练部分进行预训练：DEFACTO-12 (Mahfoudiet al. 2019)(real/tampered), SSRGFD (Yin et al. 2023)(real/tampered),CASIAv2 (Dong, Wang, and Tan 2013) (real/tampered), and Spliced COCO(Kwon et al. 2022b) created by CAT-Net (Kwon et al. 2022b) based on theCOCO 2017 dataset (Lin et al.2014)。然后，我们在上述数据集的测试部分上测试我们的模型，除了拼接COCO。<br/>​  为了进一步评估SUMIIFL的泛化能力，我们还比较了另外两个数据集上的定位性能：CIMD(Zhang, Li, and Chang 2024) (real/tampered) and NIST16 (Guan et al.2019)(real/tampered)。所有伪造的图像都被裁剪成256个×256个补丁。为了评估所提出的SUMI-IFL的定位性能，我们遵循之前的方法（Raoet al. 2022），我们采用f1评分和曲线下面积（AUC）作为评价度量。</p><p>​  <strong>实施细节</strong><br/>​  所提出的SUMI-IFL通过PyTorch实现，所有实验都在NVIDIAGTX GeForce A100GPU平台上进行。采用AdamW优化器进行12批处理100次训练，余弦退火调度器设置的初始学习速率为5e-4，权重衰减为0.005。</p><h2 id="与最先进的方法进行比较">4.2与最先进的方法进行比较</h2><p>​  我们在三种设置下将SUMI-IFL与其他最先进的方法进行了比较：1)域内数据集比较：在复合伪造数据集上进行训练和在综合测试数据集上进行评估。2)域外数据集比较：直接将预先训练好的模型应用于一个看不见的数据集来评估泛化。3)鲁棒性评价：对测试数据集应用JPEG压缩和高斯模糊来评价鲁棒性。我们用七种最先进的方法来评估其性能：MMFusion(Triaridis and Mezaris 2024), EITL-Net (Guo, Zhu, and Cao 2024),HiFi-IFDL(Guo et al. 2023) , WSCL (Zhai et al. 2023), IF-OSN (Wu et al.2022), MVSS-Net (Dong et al. 2022), PSCC-Net (Liu et al. 2022)。</p><p>​  <strong>域内数据集比较</strong><br/>​  表2报告了在F1评分和AUC评分方面的最优和次优定位。</p><figure><img src="../postimages/SUMI-IFL/image-20250115170908787.png"alt="image-20250115170908787" /><figcaption aria-hidden="true">image-20250115170908787</figcaption></figure><p>​  我们可以观察到，SUMI-IFL在事实上、SSRGFD和CASIAv2数据集上取得了最高的性能。特别是，SUMI-IFL在立体伪造数据集SSRGFD上获得了0.7995的F1分，并且优于次优方法15.7%。这证实了最小性视图约束可以帮助框架捕获准确的伪造痕迹，即使在SSRGFD数据集中存在的重构伪影的干扰。在其他两个数据集中，SUMI-IFL在F1分数和AUC分数方面也都优于其他方法，证明了它获得优越的伪造特征表示的能力。</p><p>​  <strong>域外数据集比较</strong><br/>​  为了进一步证明SUMI-IFL的通用化性，我们使用了两个与训练数据集分布完全不同的测试数据集。表3报告了跨数据集的F1评分和AUC评分方面的性能，SUMI-IFL在测试数据集中始终排名前两名。</p><figure><img src="../postimages/SUMI-IFL/image-20250115171003458.png"alt="image-20250115171003458" /><figcaption aria-hidden="true">image-20250115171003458</figcaption></figure><p>​  CIMD是一个新发布的数据集，具有相对较小的篡改区域，这对IFL方法是一个挑战。在这个数据集中，所有IFL方法的定位性能都下降了。然而，SUMI-IFL可以学习全面的伪造线索，并优于其他IFL方法。在NIST16数据集中，被篡改图像的比例更高。虽然所有方法都表现出较强的性能，但SUMI-IFL的F1分数比第二优方法高出9.7%。跨数据集比较的性能证明了SUMI-IFL的良好泛化。</p><h2 id="鲁棒性评价">4.3鲁棒性评价</h2><p>​  我们对事实上的-12数据集的原始图像应用不同的图像失真方法，并评估我们的SUMI-IFL的鲁棒性。失真类型包括1)具有固定质量因子的JPEG压缩和2)具有固定核大小的高斯模糊。我们将预训练模型的操作定位性能（AUC分数）与扭曲数据集上的其他方法进行了比较，结果如图5所示。</p><figure><img src="../postimages/SUMI-IFL/image-20250115171136130.png"alt="image-20250115171136130" /><figcaption aria-hidden="true">image-20250115171136130</figcaption></figure><p>​  如图5(a)所示，在JPEG压缩条件下，SUMI-IFL的性能下降幅度低于其他基线，表明该方法具有良好的JPEG鲁棒性。如图5(b)所示，SUMI-IFL也能抵抗高斯模糊，表明该方法对低质量图像具有较强的鲁棒性。</p><h2 id="消融研究">4.4消融研究</h2><p>​  在本节中，我们研究了去除充分视图约束LSU、最小视图约束LMI和辅助掩模损失Laux的影响。我们在前面提到的复合数据集上训练模型，并在事实上的-12和SSRGFD数据集的测试部分上对它们进行测试。</p><figure><img src="../postimages/SUMI-IFL/image-20250115171246376.png"alt="image-20250115171246376" /><figcaption aria-hidden="true">image-20250115171246376</figcaption></figure><p>​  没有任何一种损失都会导致模型性能的显著下降。在定量上，LSU和LMI对我们的方法有主要贡献，导致F1比实际的-12和SSRGFD分别增加了9.8%和5.1%。没有Laux，F1得分在事实上和SSRGFD上分别下降了2.5%和9.8%。这一经验证据表明，合并所提出的损失可以提取更全面和更少的与任务无关的伪造特征，促进后续的定位性能。</p><h2 id="可视化结果">4.5可视化结果</h2><p>​  如图6所示，我们提供了各种方法的预测伪造掩模。</p><figure><img src="../postimages/SUMI-IFL/image-20250115171336481.png"alt="image-20250115171336481" /><figcaption aria-hidden="true">image-20250115171336481</figcaption></figure><p>​  可以观察到，有些方法错误地将某些图像对象识别为篡改区域，例如在第一列的第三行，MMFusion错误地将图像的右下区域识别为篡改区域。可视化结果的比较表明，SUMI-IFL不仅可以更准确地定位被篡改的区域，而且可以产生更清晰的区域。这是由于充分视图约束LSU和最小视图约束LMI，使模型能够在获得全面的任务相关特征表示的同时，有效地抵抗了任务不相关特征的干扰。</p><h1 id="结论">5. 结论</h1><p>​  在本文中，我们提出了一个新的信息理论的IFL框架，SUMI-IFL，它利用充分性视图约束和最小性视图约束来约束伪造特征的表示。一方面，将充分性视图约束应用于特征提取网络中，保证了潜在的伪造特征能够捕获全面的任务相关信息。特征提取网络由三个注意骨干组成，从不同的角度发现伪造线索。另一方面，在特征推理网络中采用了最小视图约束，保证了简洁的伪造特征以消除多余的信息，从而帮助模型抵抗冗余特征的干扰。基于互信息最大化理论和信息理论瓶颈理论，我们分别对这两种约束条件进行了详细的推导。在几个基准测试中获得的大量实验结果证明了SUMI-IFL的优越性能，表明这两个关键约束有助于更全面和准确的特征表示。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EditGuard:Versatile Image Watermarking for Tamper Localization and Copyright Protection</title>
      <link href="/EditGuard/"/>
      <url>/EditGuard/</url>
      
        <content type="html"><![CDATA[<p>EditGuard: Versatile Image Watermarking for Tamper Localization andCopyright Protection</p><p>Xuanyu Zhang<span class="math inline">\(^{1,2}\)</span>, RunyiLi<span class="math inline">\(^1\)</span> , Jiwen Yu1 , Youmin Xu<spanclass="math inline">\(^1\)</span> , Weiqi Li<spanclass="math inline">\(^1\)</span> , Jian Zhang<spanclass="math inline">\(^{1,2}\)</span></p><p>1 School of Electronic and Computer Engineering, Peking University<br/>2 Peking University Shenzhen Graduate School-Rabbitpre AIGC JointResearch Laboratory</p><h1 id="摘要">摘要</h1><p>​  在人工智能生成内容（AIGC）的时代，恶意篡改对版权的完整性和信息安全构成了迫在眉睫的威胁。目前的深图像水印，虽然被广泛接受用于保护视觉内容，但只能保护版权和确保可追溯性。它们未能将日益现实的图像篡改本地化，这可能导致信任危机、侵犯隐私和法律纠纷。为了解决这一挑战，我们提出了一个创新的主动取证框架<strong>EditGuard</strong>，以统一版权保护和篡改不可知的定位，特别是对于基于AIGC的编辑方法。它可以提供一个细致的嵌入难以察觉的水印和精确解码篡改区域和版权信息。利用我们观察到的图像图像隐写的脆弱性和局部性，EditGuard的实现可以转换为一个统一的图像比特隐写问题，从而完全将训练过程与篡改类型解耦。大量的实验验证，我们的编辑guard平衡了篡改定位精度、版权恢复精度和各种基于AIGC的篡改方法的普遍性，特别是对于肉眼难以检测的图像伪造。</p><h1 id="介绍">1.介绍</h1><p>​  为了明确我们的任务范围，我们再次强调了双重取证任务的定义，如图1所示：</p><figure><img src="../postimages/EditGuard/image-20250113224604349.png"alt="image-20250113224604349" /><figcaption aria-hidden="true">image-20250113224604349</figcaption></figure><p>​  (1)版权保护：“这个图像属于谁？”我们渴望准确地检索一幅图像的原始版权，甚至遭受各种篡改和退化。<br/>​  (2)篡改定位：“这张照片是在哪里被操纵的？”我们的目标是精确定位被篡改的区域，不受特定篡改类型的限制。</p><p>​  据我们所知，没有现有的方法同时完成这两项任务，同时保持高精度和广泛推广的平衡。<br/>​  为了满足这一迫切的需求，我们提出了一种新的主动取证框架，称为EditGuard，以保护版权和为基于AIGC的编辑方法本地化篡改区域。具体来说，灵感从我们观察到的图像到图像（I2I）的脆弱性和隐写术固有的鲁棒性，我们可以转换实现的联合图像比特隐写术问题，允许训练编辑完全解耦的篡改类型，从而赋予其特殊的泛化性和以零镜头的方式定位篡改。简而言之，我们的贡献如下：</p><p>​  (1)我们首次尝试设计一个深度通用的主动取证框架EditGuard，用于通用篡改定位和版权保护。它将双隐形水印嵌入到原始图像中，并准确地解码被篡改的区域和版权信息。<br/>​  (2)我们观察了I2I隐写技术的脆弱性和局部性，创新地将该双取证任务的解决方案转换为训练一个统一的象位隐写网络（IBSN），并利用IBSN的核心组件构建EditGuard。<br/>​  (3)我们引入了一个基于提示的后验估计模块，以提高该框架的定位精度和退化鲁棒性。<br/>​  (4)我们的方法的有效性已经在我们构建的数据集和经典的基准测试上得到了验证。与其他竞争方法相比，我们的方法在定位精度、泛化能力和版权精度方面具有显著的优点，而不需要任何标记数据或对特定的篡改类型需要额外的训练。</p><h1 id="相关工作">2.相关工作</h1><h2 id="篡改定位">2.1.篡改定位</h2><h2 id="图像水印">2.2.图像水印</h2><h1 id="editguard总体框架">3.EditGuard总体框架</h1><h2 id="动机">3.1.动机</h2><p>​  <strong>现有方法的挑战：</strong><br/>​  (1)如何装备现有的水印方法，仅是为了版权保护，具有定位篡改的能力是编辑保护的关键。我们将通过第3.2节中的框架设计来解决它。<br/>​  (2)以往大多数篡改定位方法在网络训练中不可避免地引入特定的篡改数据，但在未知篡改类型中往往会引起泛化问题，这将在第3.3节中解决。</p><figure><img src="../postimages/EditGuard/image-20250113225639573.png"alt="image-20250113225639573" /><figcaption aria-hidden="true">image-20250113225639573</figcaption></figure><p>​  <strong>我们的观察：</strong><br/>​  幸运的是，我们观察到图像到图像（I2I）的隐写术表现出明显的脆弱性和定位性，具有解决这些问题的巨大潜力。具体地说，I2I隐写[3,28,42,46,64,69]的目标是将秘密图像<spanclass="math inline">\(\mathbf{X}_{sec}\)</span>隐藏到封面图像<spanclass="math inline">\(\mathbf{X}_{cov}\)</span>中，以生成容器图像<spanclass="math inline">\(\mathbf{X}_{con}\)</span>，并以最小失真准则从接收图像<spanclass="math inline">\(\mathbf{X}_{con}^{\prime}\)</span>中显示<spanclass="math inline">\(\mathbf{\hat X}_{sec}\)</span>和<spanclass="math inline">\(\mathbf{\hat X}_{cov}\)</span>。我们发现，当<spanclass="math inline">\(\mathbf{X}_{con}^{\prime}\)</span>比<spanclass="math inline">\(\mathbf{X}_{con}\)</span>发生显著变化时，<spanclass="math inline">\(\mathbf{\hatX}_{sec}\)</span>也会被损坏并产生伪影（图2的第一行），这被称为脆弱性。此外，我们注意到<spanclass="math inline">\(\mathbf{\hatX}_{sec}\)</span>中的伪影几乎是像素级对应的<spanclass="math inline">\(\mathbf{X}_{con}^{\prime}\)</span>相对于<spanclass="math inline">\(\mathbf{X}_{con}\)</span>的变化，这被称为定位性。为了证明这一定位性，我们在<spanclass="math inline">\(\mathbf{\hatX}_{sec}\)</span>选择了5个7×7点集，并计算了它们关于<spanclass="math inline">\(\mathbf{X}_{con}^{\prime}\)</span>的属性图。如图2的第二行所示，<spanclass="math inline">\(\mathbf{\hat X}_{sec}\)</span>只在<spanclass="math inline">\(\mathbf{X}_{con}^{\prime}\)</span>的相应位置及其附近表现出强烈的响应，几乎与其他像素无关。这些特性促使我们将<spanclass="math inline">\(\mathbf{X}_{sec}\)</span>视为一种特殊的定位水印，并将其嵌入到现有的水印框架中。</p><h2 id="框架设计和取证过程">3.2.框架设计和取证过程</h2><p>​  实现统一篡改定位和版权保护，EditGuard设想嵌入二维定位水印和一维版权可追溯水印以难以察觉的方式到原始图像，这允许解码结束获得图像的版权和二进制掩码反映篡改区域。然而，设计这样一个框架需要解决两种类型的水印的兼容性问题。</p><p>​  <strong>(1)局部vs全局：</strong>定位水印需要隐藏在原始图像对应的像素位置上，而版权水印需要与空间位置无关，并冗余地嵌入到全局区域中。<br/>​  <strong>(2)半脆弱vs鲁棒：</strong>定位水印的期望属性是半脆弱的，这意味着它在网络传输过程中对篡改很脆弱，但对一些常见的退化（如高斯噪声、JPEG压缩和泊松噪声）具有鲁棒性。然而，版权应该几乎无损，无论篡改还是退化。</p><p>​  为了解决这两个关键冲突，EditGuard采用了“顺序编码和并行解码”结构，其中包括双水印编码器、篡改定位器和版权提取器。</p><p>​  如图3所示，双水印编码器将把用户提供的预定义的定位水印和全局版权水印<spanclass="math inline">\(\mathbf w_{cop}\)</span>依次添加到原始图像<spanclass="math inline">\(\mathbf I_{oir}\)</span>中，形成容器图像<spanclass="math inline">\(\mathbfI_{con}\)</span>。我们的实验已经证明并行编码不能有效地添加到图像（在补充材料<strong>(S.M.)</strong>）。相比之下，顺序嵌入通过隐藏这两个水印可以有效地防止交叉干扰。此外，我们对接收（篡改）图像<spanclass="math inline">\(\mathbf I_{rec}\)</span>从<spanclass="math inline">\(\mathbfI_{con}\)</span>转换为的网络传输过程建模为： <spanclass="math display">\[{\mathbf I}_{r e c}=\mathcal D({\mathbf I}_{c on}\odot(\bf1-\bf M)+\mathcal T({\bf I}_{c o n}\odot{\bf M})\]</span>​  式中，<span class="math inline">\(\mathcal T (\cdot)\)</span>、<spanclass="math inline">\(\mathcal D (\cdot)\)</span>和<spanclass="math inline">\(\bfM\)</span>分别表示篡改功能、降级操作和回火掩模。此外，并行解码过程使我们能够在不同的鲁棒性水平下灵活地训练每个分支，并通过篡改定位器获得预测掩码<spanclass="math inline">\(\hatM\)</span>，通过版权提取器获得可追溯性水印<spanclass="math inline">\(\hat{\mathbfw}_{cop}\)</span>。我们可以将EditGuard的双重取证过程分为以下场景。</p><p>​  <strong>场景1：</strong>如果<span class="math inline">\(\hat{\bfw}_{c o p}\not\approx\bf {w}_{c o p}\)</span>，那么置信度低的<spanclass="math inline">\(\mathbfI_{rec}\)</span>要么没有在我们的EditGuard中注册，要么经历了极其严重的全局篡改，使其成为不可靠的证据。<br/>​  <strong>场景2：</strong>如果<spanclass="math inline">\(\hat{\bf w}_{c o p}\approx\bf {w}_{c op}\)</span>且<span class="math inline">\(\hat M \not\approx0\)</span>，那么置信度低的<span class="math inline">\(\mathbfI_{rec}\)</span>经历了篡改，取消将其作为有效证据的资格。用户可以根据<spanclass="math inline">\(\hatM\)</span>推断篡改者的意图，并决定是否启用图像的其他部分。<br/>​  <strong>场景3：</strong>如果<spanclass="math inline">\(\hat{\bf w}_{c o p}\approx\bf {w}_{c op}\)</span>且<span class="math inline">\(\hat M \approx0\)</span>，那么<span class="math inline">\(\mathbfI_{rec}\)</span>在EditGuard的盾牌下保持不被篡改且值得信赖。</p><h2 id="将双重取证转化为隐写术">3.3.将双重取证转化为隐写术</h2><p>​  为了实现通用且篡改不可知的定位性能，我们利用了我们观察到的I2I隐写术的局部性和脆弱性。</p><figure><img src="../postimages/EditGuard/image-20250113232601473.png"alt="image-20250113232601473" /><figcaption aria-hidden="true">image-20250113232601473</figcaption></figure><p>​  如第3.1节所述，通过图像的隐写和揭示，可以有效地实现图3中的定位水印和篡改定位性能。同时，结合当前位对图像隐写技术的鲁棒性，通过位加密和恢复，实现了图3中的版权水印和提取器。因此，我们可以将双取证框架EditGuard的实现转换为一个统一的图像比特隐写术网络。<br/>​  我们的训练目标只是一种自我恢复机制，这意味着它只需要确保隐写术网络的输入和输出在不同的鲁棒性水平下保持高保真度，而不需要引入任何标记数据或篡改样本。<br/>​  在推理过程中，它可以通过简单的零镜头比较来自然地定位篡改，并准确地提取版权。</p><h1 id="联合图像比特隐写术网络ibsn">4.联合图像比特隐写术网络IBSN</h1><h2 id="网络架构">4.1.网络架构</h2><figure><img src="../postimages/EditGuard/image-20250113232835654.png"alt="image-20250113232835654" /><figcaption aria-hidden="true">image-20250113232835654</figcaption></figure><p>​  如图4所示，所提出的IBSN（Image-bit SteganographyNetwork）包括图像隐写模块（IHM，image hidingmodule）、比特加密模块（BEM，bit encryptionmodule）、比特恢复模块（BRM，bit recoverymodule）和图像显示模块（IRM，image revealingmodule）。首先，图像隐写模块IHM的目标是将一个定位水印<spanclass="math inline">\({\bf W}_{l o c}\in\mathbb{R}^{H\timesW\times3}\)</span>隐藏到原始图像<span class="math inline">\({\bfI}_{ori}\in\mathbb{R}^{H\timesW\times3}\)</span>中，从而得到一个中间输出<spanclass="math inline">\({\bf I}_{med}\in\mathbb{R}^{H\timesW\times3}\)</span>。随后，将<span class="math inline">\({\bfI}_{med}\)</span>输入到比特加密模块BEM进行特征细化，同时将版权水印<spanclass="math inline">\(\mathbfw_{cop}\in\{0,1\}^{L}\)</span>输入到比特加密模块BEM，形成最终的容器图像<spanclass="math inline">\({\bf I}_{con}\in\mathbb{R}^{H\timesW\times3}\)</span>。在网络传输后，比特恢复模块BRM将从接收到的容器图像<spanclass="math inline">\({\bf I}_{rec}\)</span>中如实地重建版权水印<spanclass="math inline">\(\hat{\bf w}_{c o p}\)</span>。同时，<spanclass="math inline">\({\bfI}_{rec}\)</span>通过基于提示的后验估计对缺失的信息<spanclass="math inline">\(\hatZ\)</span>进行预测，并将其作为可逆块的初始化，产生<spanclass="math inline">\({\bf \hat I}_{ori}\)</span>和半脆性水印<spanclass="math inline">\(\hat{\bf W}_{l o c}\)</span>。</p><h2id="图像隐写模块ihm和图像显示模块irm中的可逆块">4.2.图像隐写模块IHM和图像显示模块IRM中的可逆块</h2><p>​  考虑到基于流的模型具有精确恢复多媒体信息的固有能力，我们利用堆叠的可逆块来构造图像隐藏和揭示模块。原始图像<spanclass="math inline">\({\bf I}_{o r i}\in\mathbb{R}^{H\timesW\times3}\)</span>和定位水印<span class="math inline">\({\bf W}_{l oc}\in\mathbb{R}^{H\timesW\times3}\)</span>将经过离散小波变换（DWT），得到频率解耦的图像特征。然后，我们使用增强的加性仿射耦合层来投影原始图像及其相应的定位水印分支。这些转换参数将相互生成。增强的仿射耦合层由五层密集卷积块[46]和轻量级特征交互模块（LFIM）[4]组成。LFIM可以增强转换的非线性，并以低计算成本捕获长期依赖性。更多细节见S.M.。最后，通过反DWT将显示的特征转换到图像域。</p><h2 id="基于提示的后验估计">4.3.基于提示的后验估计</h2><p>​  为了提高图像隐藏和揭示模块的保真度和鲁棒性，我们引入了一个基于退化提示的后验估计模块（PPEM，posteriori estimation module）。由于编码网络倾向于将<spanclass="math inline">\([{\bf I}_{o r i};{\bf W}_{l oc}]\in\mathbb{R}^{H\times W\times3}\)</span>压缩到容器图像<spanclass="math inline">\({\bf I}_{c o n}\in\mathbb{R}^{H\timesW\times3}\)</span>中，以前的方法[42,64]通常在解码端使用随机高斯初始化或全零映射来补偿丢失的高频信道。然而，我们的观察表明，被丢弃的信息隐藏在容器图像的边缘和纹理中。因此，部署专用网络被证明是用来预测消失的定位水印信息的后验均值<spanclass="math inline">\(\mathbf{\hat Z}=\mathbb{E}[Z|{\bfI}_{rec}]\)</span>的一种更有效的策略。</p><figure><img src="../postimages/EditGuard/image-20250114105632150.png"alt="image-20250114105632150" /><figcaption aria-hidden="true">image-20250114105632150</figcaption></figure><p>​  具体来说，如图5所示，我们堆叠M个残差<spanclass="math inline">\(\mathrm{Res}(\cdot)\)</span>[19]和M个通道级变压器块<spanclass="math inline">\(\mathrm{Trans}(\cdot)\)</span>[70]，以提取局部和非局部特征<spanclass="math inline">\(\mathbf{F}_c\)</span>。 <spanclass="math display">\[\mathbf{F}_{c}=\mathrm{Trans}(\mathrm{Res}(\mathrm{D}W\mathrm{T}(\mathbf{I}_{re c})))+\mathrm{Res}(\mathrm{D}W\mathrm{T}(\mathbf{I}_{r ec})).\]</span>​  考虑到容器图像在网络传输过程中容易发生各种退化，我们将N个可学习的嵌入张量预先定义为退化提示<spanclass="math inline">\(\mathbf{P} =[\mathbf{P}_{1},\mathbf{P}_{2},\cdot\cdot\cdot,\mathbf{P}_{N}]\)</span>，其中N表示退化类型的数量，设置为3。这些学习到的提示<spanclass="math inline">\(\mathbf{P}\)</span>可以自适应地学习不同范围的退化表示，并与从<spanclass="math inline">\({\bf I}_{r ec}\)</span>中提取的内在特征集成，使所提出的IBSN能够使用一组参数处理多种类型的退化。为了更好地促进输入特征<spanclass="math inline">\(\mathbf{F}_{c}\)</span>和退化提示<spanclass="math inline">\(\mathbf{P}\)</span>之间的交互作用，将特征<spanclass="math inline">\(\mathbf{F}_{c}\)</span>传递到全局平均池化（GAP）层、1×1卷积和softmax算子，以产生一组动态权值系数。每个退化提示符<spanclass="math inline">\(\mathbf{P}_{i}\)</span>使用这些动态系数<spanclass="math inline">\(\mathbf{W}_{p\circledasti}\)</span>进行组合，然后通过上采样算子<spanclass="math inline">\(\uparrow\)</span>和3×3卷积进行积分，以获得增强的表示<spanclass="math inline">\(\mathbf{P}_{c}\)</span>。 <spanclass="math display">\[\mathbf{P}_{c}=\mathrm{Conv_{3x3}}\left(\left(\sum_{i-1}^{N}\mathbf{w}_{p\circledast  i}\mathrm{P}_{i}\right)_{\uparrow}\right),\]</span>​  其中<span class="math inline">\({\bf w}_{p}=\mathrm{Softmax}\left({\bfC o n v}_{1\times1}({\bf C}{\bf A P}\left({\bfF}_{c}{\bf\rangle}\right)\right)\)</span>。<br/>​  最后，我们利用3×3卷积来融合基于提取的特征<spanclass="math inline">\(\mathbf{F}_{c}\)</span>的学习退化表示，以丰富退化特定的上下文，获得<spanclass="math inline">\(\mathbf{\hat Z}\)</span>。此过程可表述为： <spanclass="math display">\[\hat{\mathbf{Z}}=\operatorname{Conv}_{3\times3}\left([\mathbf{P}_{c};\mathbf{F}_{c}]\right)\in\mathbb{R}^{\frac{H}{2}\times\frac{W}{2}\times12}.\]</span></p><h2 id="比特加密和恢复模块">4.4.比特加密和恢复模块</h2><figure><img src="../postimages/EditGuard/image-20250113232835654.png"alt="image-20250113232835654" /><figcaption aria-hidden="true">image-20250113232835654</figcaption></figure><p>​  如图4所示，为了将版权水印的<span class="math inline">\({\bfw}_{cop}\)</span>编码到<span class="math inline">\({\bfI}_{med}\)</span>中，我们首先通过堆叠的MLPs扩<spanclass="math inline">\({\bfw}_{cop}\in\{0,1\}^L\)</span>，并将其重塑为几个L×L消息特征图。同时，将<spanclass="math inline">\({\bfI}_{med}\)</span>馈入u型特征增强网络，提取每个下采样和上采样层的特征。最后，通过融合机制[21,62]，将消息特征进行升级，并与多层次图像特征进行集成，实现双定时信息的调制。在解码端，<spanclass="math inline">\({\bfI}_{rec}\)</span>被馈入一个u形的子网络，并被降采样到L×L的大小。然后通过MLP提取恢复的版权水印<spanclass="math inline">\({\bf \hat w}_{cop}\)</span>。更多细节见S.M.。</p><h2 id="通过ibsn构建editguard">4.5.通过IBSN构建EditGuard</h2><p>​  为了稳定所提出的IBSN的优化，我们提出了一种双级优化策略。给定任意原始图像图像和水印<spanclass="math inline">\({\bf w}_{cop}\)</span>，首先通过<spanclass="math inline">\(\ell_2\)</span>损失训练比特加密和恢复模块。 <spanclass="math display">\[\ell_{c o p}=\|\mathbf{I}_{c o n}-\mathbf{I}_{m ed}\|_{2}^{2}+\lambda\,\|\hat{\mathbf{w}}_{c o p}-\mathbf{w}_{c op}\|_{2}^{2}\]</span> ​  其中，<spanclass="math inline">\(\lambda\)</span>被设置为10。此外，我们冻结了比特加密模块BEM和比特恢复模块BRM的权值，并联合训练了图像隐写模块IHM和图像显示模块IRM。给定一个随机的原始图像<spanclass="math inline">\({\bf I}_{ori}\)</span>、定位水印<spanclass="math inline">\({\bf W}_{loc}\)</span>和版权水印<spanclass="math inline">\({\bf w}_{cop}\)</span>，其损失函数为： <spanclass="math display">\[\ell_{l o c}=\|{\bf  I}_{o r i}-\bf I_{o ri}\|_{1}+\alpha\|\bf I_{c o n}-I_{o r i}\|_{2}^{2}+\beta\|\bf\hat{W}_{lo c}-W_{l o c}\|_{1}\]</span> ​  其中，<spanclass="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>分别设置为100和1。在训练期间，我们只引入降级，没有暴露到任何篡改。在获得预先训练的IBSN后，我们可以通过IBSN的组件构建所提出的EditGuard。如图4所示，EditGuard的双水印编码器由图像隐写模块IHM和图像显示模块BEM组成，分别对应于图3中的定位水印和版权水印。版权提取器严格对应于比特恢复模块BRM。篡改定位器包括图像显示模块IRM和掩模提取器（ME）。注意，我们需要预先定义一个定位水印<spanclass="math inline">\({\bfW}_{loc}\)</span>，它在编码端和解码端之间共享。<spanclass="math inline">\({\bfW}_{loc}\)</span>的选择对于我们的方法来说是非常普遍的。它可以是任何自然图像，甚至是纯色图像。最后，通过比较预定义的水印<spanclass="math inline">\({\bf W}_{loc}\)</span>与解码的<spanclass="math inline">\(\hat{\bfW}_{loc}\)</span>，我们可以得到一个二进制掩码<spanclass="math inline">\({\bf M}\in\mathbb{R}^{H\times W}\)</span>： <spanclass="math display">\[\hat{\mathrm{M}}[i,j]=\theta_{\tau}(\mathrm{max}(|\hat{\mathrm{W}}_{lo c}[i,j,:]-\mathrm{W}_{l o c}[i,j,:]))\]</span></p><h1 id="实验">5.实验</h1><h2 id="实施细节">5.1.实施细节</h2><p>​  我们通过COCO[38]的训练集来训练我们的EditGuard，而没有任何被篡改的数据。因此，对于篡改定位，我们的方法实际上是零次学习的。Adam[30]用于用<span class="math inline">\(\beta_1=0.9\)</span>和<spanclass="math inline">\(\beta_2=0.5\)</span>训练250K次迭代。学习速率初始化为1×10−4，每30K迭代减少一半，批处理大小设置为4。我们在原始图像中嵌入一个64位的版权水印和一个简单的定位水印，如纯蓝色图像（[R，G，B]=[0,0,255]）。随后，跟随[8,17,39]，使用F1-score、AUC、IoU和比特精度来评估定位和版权保护性能。由于之前的任何方法都不能同时实现这种双重取证，所以我们对篡改定位和图像水印方法进行了单独的比较。</p><h2 id="与定位方法的比较">5.2.与定位方法的比较</h2><p>​  为了与篡改定位方法进行公平的比较，我们对四个经典基准[9,16,20,58]进行了广泛的评估，如表一所示。</p><figure><img src="../postimages/EditGuard/image-20250115155756601.png"alt="image-20250115155756601" /><figcaption aria-hidden="true">image-20250115155756601</figcaption></figure><p>​  由于EditGuard是一种主动的方法，我们首先将水印嵌入到真实的图像中，然后将被篡改的区域粘贴到容器图像中。值得注意的是，即使是现有方法专门从事的篡改类型，EditGuard的定位精度在四个数据集上始终优于SOTA方法[17]，F1分数分别提升为0.102、0.116、0.441和0.065，这验证我们主动定位机制的优越性。</p><figure><img src="../postimages/EditGuard/image-20250115160442433.png"alt="image-20250115160442433" /><figcaption aria-hidden="true">image-20250115160442433</figcaption></figure><p>​  如图6所示，我们的EditGuard可以精确定位像素级的篡改区域，但其他方法只能产生一个粗略的轮廓或仅在某些情况下有效。同时，我们的比特精度保持在99.8%以上，而其他所有方法都无法实现有效的版权保护。</p><h2 id="与水印方法的比较">5.3.与水印方法的比较</h2><h2 id="扩展到基于aigc的编辑方法">5.4.扩展到基于AIGC的编辑方法</h2><h2 id="鲁棒性分析">5.5.鲁棒性分析</h2><h2 id="消融研究">5.6.消融研究</h2><h1 id="结论">6.结论</h1><p>​  我们首次尝试设计一种深多功能水印机构EditGuard。它通过嵌入难以察觉的定位和版权水印，解码准确的版权信息和篡改区域，提高图像的可信度，使其成为艺术创作和法律法医分析的可靠工具。在未来，我们将重点提高EditGuard的鲁棒性，不仅努力提供像素级的定位结果，而且努力提供语义级的结果。此外，我们计划进一步扩展EditGuard到更广泛的模式和应用程序，包括视频、音频和3D场景。我们在信息真实性方面的努力不仅服务于AIGC行业，也服务于我们对数字世界的信任，确保每个像素都能说出真相，每个人的权利都得到保障。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>UGEE-Net:Uncertainty-guided and edge-enhanced network for image splicing localization</title>
      <link href="/UGEE-Net/"/>
      <url>/UGEE-Net/</url>
      
        <content type="html"><![CDATA[<p>UGEE-Net: Uncertainty-guided and edge-enhanced network for imagesplicing localization</p><p>Qixian Hao a,1 , Ruyong Ren a,1 , Shaozhang Niu a,b,* , Kai Wang a ,Maosen Wang b , Jiwei Zhang a</p><p>1北京邮电大学，计算机学院，智能电信软件与多媒体的北京重点实验室，Beijing100876, China<br/>2东南数字经济发展研究所，衢州324000</p><h1 id="摘要">摘要<br/></h1><p>​  图像拼接是一种普遍的图像篡改方法，它严重破坏了图像的真实性。现有的图像拼接定位（ISL）方法在处理难以察觉的篡改和多个篡改区域时，面临着有限的精度和性能不佳等挑战。我们为ISL引入了一个不确定性引导和边缘增强网络（UGEE-Net）来解决这些问题。UGEE-Net包括两个核心任务：不确定性引导和边缘增强。我们使用贝叶斯学习来建模被篡改区域的不确定性地图，将模型的焦点指向具有挑战性的像素。同时，我们采用频域辅助边缘增强策略，将局部特征注入全局轮廓信息和细粒度局部细节。这些机制并行工作，协同提高性能。此外，我们还引入了一种跨层融合和传播机制，有效地利用上下文信息进行跨层特征集成，并利用通道级相关性进行跨层特征传播，逐步增强了定位特征的细节。实验结果证实了UGEE-Net在检测精度、鲁棒性和泛化能力方面的优势。此外，为了满足图像取证中对高质量数据集日益增长的需求，我们提出了HTSI12K数据集，该数据集包括12000张拼接图像，具有难以察觉的篡改痕迹和不同的类别，使其适合于现实世界的辅助模型训练。</p><h1 id="介绍">1.介绍</h1><p>​  虽然主流的ISL方法已经取得了实质性的进展，但持续存在的挑战阻碍了进一步的发展。当无缝地集成到图像背景中时，这些方法难以区分被操纵的元素和真实的元素，特别是在有多个被篡改区域的复杂场景中，可能会导致潜在的问题，如错误的判断，如图1所示。</p><figure><img src="../postimages/UGEE-Net/image-20250112222447920.png"alt="image-20250112222447920" /><figcaption aria-hidden="true">image-20250112222447920</figcaption></figure><p>​  我们将这一现象归因于两个主要因素：一是当前主流方法对篡改痕迹的过度依赖；图像均匀化技术可以有效地消除RGB图像及其噪声视图中被篡改区域和未被篡改区域之间的属性差异和局部异常。例如，在噪声辅助方法中，在噪声分布中引入没有额外的干扰轨迹的噪声视图可能会产生不利的干扰，误导模型做出错误的判断。其次，单任务设计往往会导致过度自信，导致过度分割或分割不足。在传统的边缘辅助方法中，如当后处理方法（去噪、滤波、局部平滑）巧妙地隐藏边缘轨迹时，标准的边缘提取方法无法捕获完整的边缘信息，从而采用单一策略确定模型的次优定位结果。</p><p>​  为了解决这些挑战，我们引入了多任务引导的概念，并提出了一种基于不确定性引导和边缘增强的新型ISL网络。首先，不确定性引导涉及概率估计，为模型提供了一个被篡改区域的置信度量。在篡改不那么明显的情况下，不确定性指导表明了模型对这些区域的不确定性，提高了对微妙操作的敏感性。<br/>​  具体来说，在特征选择方面，UGEE-Net侧重于空间域中高级特征的融合和交互，平衡全局语义和局部细节，避免了高分辨率特征带来的隔离效应。在不确定性推断中，UGEE-Net将特征转换为拉普拉斯分布形式，为每个像素提供一个概率分布，从而生成一个不确定性图。与传统方法中常用的高斯分布相比，拉普拉斯分布更适合于描述像素的后验分布，特别是在处理边缘和不确定性问题时。</p><p>​  其次，边缘引导强调图像内的边缘信息，突出显示被篡改区域的边缘和结构。为了解决常见的后处理操作和难以察觉的篡改痕迹，与其他边缘辅助方法不同，我们考虑在频域特征中提取边缘信息。|​  具体来说，在频域特征学习方面，我们采用八度卷积自动将中高级特征转换为低频和高频，捕获频域内难以察觉的篡改痕迹。与传统的频域辅助方法不同，我们充分利用了高频和低频信息，避免了信息丢失，这是一个具有灵活性的在线可学习过程。在边缘信息提取方面，我们结合扩张卷积，在多个尺度上提取被篡改区域的边缘映射。该边缘图为后续的定位特征提供了局部细节和全局轮廓信息，从而得到更全面和精细的定位结果。</p><p>​  此外，考虑到不确定性引导和边缘引导作为补充机制，我们的设计旨在利用它们各自的优势。鉴于对不确定引导和边缘增强特征的独特关注，我们提出了一种不确定-边缘融合机制来有效增强不确定引导和边缘增强特征，并整合两者有价值的线索。我们重点探索三个方面：位置对齐，通道重新校准，和特征之间的相互作用。</p><p>​  最后，为了有效地融合不同层次的特征，提高深度学习模型的学习性能，我们提出了跨层次的特征融合和传播机制。跨层次的特征融合是指通过引入多尺度的通道注意机制来融合不同层次的特征，以提高模型的感知能力。跨层次特征传播通过信道级相关机制，选择性地增强了先前定位特征与相邻级特征之间的交互作用，进一步增强了特征表示能力。UGEE-Net提供了一个ISL的解决方案的关键挑战，特别是在处理难以察觉的篡改痕迹和复杂场景时。</p><p>​  此外，由于缺乏专门针对图像拼接而定制的高质量数据集，因此提出了一个挑战，因为现有的拼接图像数据集往往具有明显的篡改痕迹。在这样的数据集上训练的模型可能无法达到其最佳的性能水平。为了解决这个问题，我们引入了一个新的数据集，名为“12,000个带有隐藏痕迹的拼接图像”（HTSI12K）。首先，利用深度学习技术授权的智能图像编辑软件生成HTSI12K图像，导致篡改图像的篡改痕迹，检测更加复杂和具有挑战性。其次，我们保留了原始图像的纹理细节和边缘信息。这种保存增加了像素级定位的难度，并给ISL带来了新的挑战。最后，HTSI12K包括各种类别。这种多样性使其适用于图像取证、目标检测、语义分割、医学疾病检测等各个领域。</p><p>​  本文的主要贡献可以总结如下：</p><ol type="1"><li>我们介绍了UGEE-Net，一种为ISL设计的新网络。在UGEE-Net中，我们结合了两个关键的机制：不确定性引导和边缘增强。这些机制协同工作，显著提高了检测性能。为了进一步增强UGEE-Net的能力，我们实现了两个关键机制：跨级融合和传播，以及不确定性-边缘融合。这些机制有助于有效地提取和放大嵌入在多层次和双引导特征中的有价值的信息。<br/>2.我们介绍了HTSI12K数据集，这是一个高质量的拼接图像数据集，可以作为推进该领域研究的基础资源。由12,000张不同类别的图像组成，每个被篡改的图像都是精心制作的，具有难以察觉的篡改痕迹。建议的数据集可以在https://github.com/QixianHao/-HTSI12K-dataset上公开获得。<br/>3.实验结果表明，UGEE-Net在定位性能、鲁棒性和泛化能力方面比主流方法具有显著的优势。</li></ol><p>​  本文的其余部分组织如下：第2节提供了相关工作的详细描述。第3节详细阐述了拟议的UGEE-Net。第4节详细介绍了所提出的HTSI12K数据集。第5节报告并分析了实验结果。第6节总结了本研究。</p><h1 id="相关工作">2.相关工作</h1><h2 id="图像剪接定位">2.1.图像剪接定位</h2><h2 id="贝叶斯神经网络">2.2.贝叶斯神经网络</h2><h2id="金字塔视觉转换器版本2pvtv2">2.3.金字塔视觉转换器版本2（PVTv2）</h2><p>​  PVTv2（Wang etal.，2022b）集成了金字塔结构和视觉变形器的关键概念。利用金字塔结构，PVTv2可以提取不同层次和尺度的特征。同时，通过引入视觉转换器的自注意机制，该模型可以动态地聚焦于图像内的不同区域。PVTv2是对PVTv1的改进，结合了三种设计增强来提高性能：线性复杂度注意层、重叠的图像补丁嵌入和卷积前馈网络。通过这些改进，PVTv2将PVTv1的计算复杂度降低到线性水平，从而在图像分类、目标检测和语义分割等基本视觉任务方面取得了重大进展。</p><h1 id="提出的框架">3.提出的框架</h1><h2 id="整体架构">3.1.整体架构</h2><p>​  与Fan等人（2021年）对狩猎的描述类似，我们将UGEE-Net的每个级别的操作分为三个阶段：提取阶段、增强阶段和决策阶段。UGEE-Net的整体架构如图2所示。</p><figure><img src="../postimages/UGEE-Net/image-20250112223051836.png"alt="image-20250112223051836" /><figcaption aria-hidden="true">image-20250112223051836</figcaption></figure><p>​  具体来说，在提取阶段，我们设计了不确定性学习（UL）模块和边缘感知（EA）模块，分别提取被篡改区域的不确定性图和边缘信息。在增强阶段，我们首先引入了跨层融合和传播（CLFP）模块，它促进了特征在相邻层之间的融合和传播。随后，我们设计了具有相同结构的边缘增强（EE）模块和不确定性引导（UG）模块，并行运行，以增强多尺度融合特征中的不确定性区域和边缘区域。这些模块利用扩展的卷积和多分支结构来扩展接受域和利用上下文，提高了检测性能。我们在决策阶段引入了不确定性-边缘融合（UEF）模块。UEF的目标是增强有用的部分和对齐不确定性引导和边缘增强特征中的空间位置，将来自这两个来源的有价值的线索整合到不确定性-边缘融合特征中。最后，采用多层次的监督策略来生成最终的定位结果。我们将在下面提供每个关键组件的详细描述。</p><h2 id="提取阶段">3.2.提取阶段</h2><h3 id="不确定性学习ul">(1)不确定性学习（UL）</h3><p>​  在图像拼接中，伪造者经常操纵拼接边界，使其看起来更平滑、更自然。因此，被篡改的图像经常被模糊，以消除明显的篡改痕迹。这种模糊操作导致被篡改区域周围的边缘模糊，并引入了相关像素的高不确定性。然而，现有的方法并没有明确地解释这种不确定性，这可能导致ISL中的次优甚至误导性的表示学习。不确定性指导可以帮助模型分析拼接区域的统计特征和概率分布，从而揭示伪造的痕迹。如图3所示，我们在本文中提出了一个基于BNN（Maddox等人，2019年）的UL模块。</p><figure><img src="../postimages/UGEE-Net/image-20250112223222277.png"alt="image-20250112223222277" /><figcaption aria-hidden="true">image-20250112223222277</figcaption></figure><p>​  UL模块结合了一个dropout层作为正则化技术来估计预测结果的方差。UL模块将输出分布参数化为拉普拉斯分布，其中方差表示不确定性。方差越大，不确定性越高，方差越小，不确定性越低。通过从这个分布中抽样，我们可以生成一个不确定性图。给定一个大小为W×H×3的输入图像I，PVTv2可以生成不同级别的特征<spanclass="math inline">\(f_i(i=1,...,4)\)</span>。<br/>​  高级特性通常包含更抽象的表示，它们捕获输入数据的全局模式和结构。然而，仅仅依赖于最高级别的特性可能会导致网络丢失在早期阶段学习到的细粒度特征信息，从而影响其处理本地特性的性能。此外，最高层次特征的抽象性质可能会使网络难以捕获图像中的特定细节。忽略早期阶段的特性可能会导致对局部特性缺乏理解。最后，虽然最高级别的特征对于理解图像的整体上下文很有用，但对于某些任务来说，它们可能过于抽象，并且无法提供足够的局部信息。如果网络过度依赖于这些高级特性，那么它可能无法充分利用ISL任务所需的细粒度信息。低级别的特征具有更高的分辨率和丰富的纹理细节，这使它们更适合生成细粒度的定位结果。然而，具有高分辨率特性的风险在于，它们可能会将被篡改的区域与真实的区域隔离开来，这可能会限制后续特征表示的有效性。此外，更高的分辨率意味着更高的计算成本。因此，为了平衡有效性和资源消耗，我们选择在不确定性推断过程中不涉及低水平的特征f1。我们采用中层特征f2和f3来辅助高级特征的不确定性学习。具体来说，我们首先通过随机冻结一些神经元来引入随机性，通过dropout层引入随机性。接下来，我们对高分辨率特征f2和f3进行降采样，以确保它们的尺寸与低分辨率特征f4的尺寸相匹配，从而避免了高分辨率特征的隔离所造成的问题。最后，我们将低采样的高分辨率特征f2和f3与低分辨率特征f4进行聚合，以帮助平衡细粒度的局部特征和全局上下文信息。<spanclass="math display">\[f^{\scriptscriptstyle{agg}}=Drop(f_{4})\oplusDown(Drop(f_{3}\bigr)\bigr)\oplus Down(Dropp(f_{2})).\]</span>​  其中，<spanclass="math inline">\(Drop(\cdot)\)</span>为dropout层，<spanclass="math inline">\(Down(\cdot)\)</span>为降采样操作，<spanclass="math inline">\(\oplus\)</span>为广播加法操作。然后，我们采用频率通道注意机制（FCA）（Qinet al.，2021），在确保信息不丢失的同时，增强特征<spanclass="math inline">\(f_4^{drop}\)</span>下降。FCA支持通道之间的特性交互。传统的信道注意机制代表了使用标量的信道的重要性，但这种表示会导致信息丢失。FCA通过进行特征分解，将信道注意机制的压缩过程推广到频域，从而实现了信道之间的特征交互。<span class="math display">\[f^{u l}=f_{4}^{d r o p}\otimes F C A(f^{a gg})\oplus f_{4}^{d r o p}\]</span>​  其中，FCA（⋅）表示FCA，⊗表示乘法。<br/>​  然后，我们使用均值卷积运算和方差卷积运算得到与<spanclass="math inline">\(W^u×H^u×1\)</span>相同大小的均值映射μ和方差映射σ。此过程可以描述如下：<span class="math display">\[\begin{array}{c}{\mu=M e a n\bigl(f^{ul}\bigr),}\\ {\sigma=V a r i a n c e\bigl(f^{ul}\bigr).}\end{array}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 边缘增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>End-to-end Differentiable Clustering with Associative Memories</title>
      <link href="/End-to-end-Differentiable-Clustering-with-Associative-Memories/"/>
      <url>/End-to-end-Differentiable-Clustering-with-Associative-Memories/</url>
      
        <content type="html"><![CDATA[<p>End-to-end Differentiable Clustering with Associative Memories</p><p>Bishwajit Saha 1 Dmitry Krotov 2 Mohammed J. Zaki 1 Parikshit Ram 23</p><h1 id="摘要">摘要</h1><p>​  聚类是一种广泛使用的无监督学习技术，涉及到一个密集的离散优化问题。联想记忆模型或AMs是一种可微的神经网络，定义了一个递归动力系统，它已与各种深度学习架构集成。我们揭示了AM(AssociativeMemory)动力学和聚类中所需的固有离散分配之间的新联系，提出了一种新的无约束的连续松弛问题，使端到端可微聚类，称为ClAM(clusteringwithAM)。利用AMs的模式完成能力，我们进一步开发了一种新的自监督聚类损失。我们对不同数据集的评估表明，ClAM受益于自监督，并显著改进了传统的Lloyd‘sk-means算法和最近的连续聚类松弛（剪影系数高达60%）。</p><h1 id="引言">1. 引言</h1><p>​  最近，传统的联想记忆或AM模型(Hopfield, 1982;1984)被重新制定，以显著提高其记忆存储容量，并与现代深度学习技术集成(Krotov&amp; Hopfield, 2016; Ramsauer et al., 2020; Krotov &amp; Hopfield,2021; Krotov, 2021)。这些新模型被称为密集联想记忆（Dense AssociativeMemories），是完全可微的系统，能够在突触权重中存储大量的多维向量，称为模式或“记忆”。它们可以使用反向传播算法在端到端完全可微的设置中进行训练，通常带有自监督损失。<br/>​  我们认为，这种以端到端可微的方式学习AM的突触权值的能力，以及每个数据点恰好指向一个记忆的离散分配（关联），使AM唯一适合于可微聚类的任务。我们在图1中给出了一个简单的结果，其中标准的基于原型的聚类方案（离散的和可微的）无法找到正确的聚类，但我们提出的基于AM的方案是成功的。具体来说，我们做出了以下贡献：</p><ul><li>我们开发一个灵活的数学框架集群AM或ClAM，这是一个新颖的连续无约束放松的离散优化问题，允许集群端到端可微的方式，同时保持整个训练的离散集群分配，与线性时间集群分配。</li><li>我们利用AMs的模式完成能力来开发一个可区分的自监督损失，以提高聚类质量。</li><li>我们的经验证明，ClAM能够持续提高k-means高达60%，可以同时与基于光谱的聚类方法和与基于凝聚的聚类方法竞争，并产生深刻的解释。</li></ul><h1 id="相关工作">2.相关工作</h1><p>​  <strong>离散聚类算法</strong><br/>​  聚类是一个固有的硬组合优化问题。基于原型的聚类公式，如k-means聚类是np困难的，即使是对于k=2（Dasgupta，2008）。然而，广泛使用的Lloyd’s算法或Voronoi迭代方法（Lloyd，1982）可以相当有效地找到局部最优解，这可以通过多次重启和仔细的迭代来改进(Vassilvitskii&amp; Arthur,2006)。它是一种直观的离散算法，在A（根据当前原型诱导的Voronoi分区（Aurenhammer，1991）为集群的分配点）和B（基于当前集群分配更新原型点）之间交替使用。然而，该方案的离散性质使其难以逃脱任何局部最小值。我们认为，基于SGD的原型聚类能够通过逃避局部最小值来提高集群的质量，同时也继承了SGD的计算效率。光谱聚类公式（Donath&amp;霍夫曼，1973；VonLuxburg，2007）利用点的成对相似性或亲和性的特征谱（用图拉普拉斯式表示）将数据划分为“连接组件”，但没有明确地提取每个聚类的原型。这允许光谱聚类以Voronoi分区不允许的方式对数据进行分区。然而，它天真地需要二次时间（点数）来生成拉普拉斯矩阵，也需要三次时间来进行谱分解。层次聚类（Johnson，1967）基于先前建立的聚类找到连续的聚类，以自上而下的方式将它们划分，或以自下而上的方式进行组合。这些离散分层算法天真地在点的数量，尽管一些形式的自下而上的凝聚方案可以显示在二次（Sibson，1973）甚至次二次时间（3月等，2010）利用双树算法（Ram等人，2009；Curtin等人，2013；2015）。</p><p>​  <strong>密度聚类方案</strong><br/>​  如流行的DBSCAN（酯等，1996）和SNN（阿吉拉尔等，2001），不认为集群作为离散优化问题，但发现的问题模式数据分布，允许任意形状的集群鲁棒性噪声和异常值（桑德etal.，1998）。然而，多维密度估计是一项具有挑战性的任务，特别是使用非参数方法（西尔弗曼，1986；Ram&amp;Gray，2011），导致使用参数模型，如高斯混合模型（雷诺兹，2009）（可以从期望最大化的数据中估计（Dempsteret al.，1977））。</p><p>​  <strong>可微深度聚类</strong><br/>​  可微性在深度聚类中至关重要，我们希望同时学习点的潜在表示，并在该潜在空间中执行聚类(Renet al., 2022;Zhou et al., 2022)。现有的可微聚类公式，如范数和（Panahi etal.，2017）或非负矩阵分解（Kim &amp;Park，2008），本质上是解决约束优化，不直接适合端到端可微深度学习管道。因此，各种方案利用了某种形式的软聚类公式，如模糊k-均值（Bezdeketal.，1984）。Xie等人（2016）提出了一种新的概率k-means公式，启发于t-SNE（Minton，2008），对预先训练的自编码器（AE）的表示进行可微聚类（DEC），Guoet al. (2017a)（IDEC）显示了从联合学习表征和聚类中获得的收益。对于图像深度聚类中的表示学习，Song等人（2013）、Xie等人（2016）和Yang等人（2017）使用了堆叠自动编码器，Guo等人（2017b）（DCEC）使用卷积自动编码器（CAE）进行了进一步的改进。Chazan等人（2019年）使用了多个AEs，每个聚类一个，但（软）聚类分配是使用模糊k-means进行的。Cai等人（2022）在DEC放松中增加了一个“焦点损失”，以惩罚非离散任务，同时也增强了表征学习。深度子空间聚类（Peng等人，2016；Ji等人，2017；Zhang等人，2019）放宽了组合光谱聚类问题（同样采用部分聚类分配），并利用“self-expressive”层以可微的方式同时学习必要的拉普拉斯矩阵（相似度矩阵）和聚类。因此，深度聚类通常使用一些概率部分分配的点给多个簇，偏离了原始k-means的离散性质。此外，这些方案通常利用通过Lloyd（1982）进行的预先训练过的AEs和/或k-means来初始化网络参数。我们提出的ClAM保持了聚类的离散分配性质，并且在没有任何特殊播种的情况下工作良好。</p><p>​  <strong>联想记忆（AM）</strong><br/>​  这是一个神经网络，它可以存储一组多维向量-记忆-作为一个循环动态系统的定点吸引子状态。它被设计为将初始状态（呈现给系统）与固定点（内存）上的最终状态关联起来，从而定义吸引力的不相交盆地或数据域的分区，因此可以表示聚类。该网络在数学上被形式化为经典的Hopfield网络（霍普菲尔德，1982），但已知其内存容量有限，只能在d维数据域中存储≈0.14d随机内存（Amit等人，1985；McEliece等人，1987）。对于相关数据，容量甚至更小，并且通常会导致一个不动点将整个数据集吸引到单个盆地中。对于相关数据，容量甚至更小，并且通常会导致一个不动点将整个数据集吸引到单个盆地中。这种行为对于集群来说是有问题的，因为集群的数量——稳定的固定点的数量——应该与数据维数解耦。Krotov&amp;Hopfield（2016）提出了现代霍普菲尔德网络或密集AM，通过在动态系统中引入快速增长的非线性激活函数，允许更密集的内存排列，以及超线性(d)内存容量。对于某些激活函数，致密AMs具有幂律甚至指数容量（克罗托夫&amp;霍普菲尔德，2016；德米西吉尔等人，2017；拉姆索尔等人，2020年；卢西贝罗和梅扎德，2023年）。Ramsauer等人（2020）证明了变压器的注意机制（Vaswani等人，2017）是具有软max激活的密集AMs的一种特殊极限情况。密集的AMs也被用于描述整个变压器块（Hoover等人，2023年），以及集成在复杂的基于能量的神经结构中（Hoover等人，2022年）。对这些结果的回顾见Krotov（2023）。密集的AMs最近也被研究与缝线连接（Burns&amp; Fukai，2023），并应用于异质结合环境（Liang etal.，2022）。在我们的工作中，我们将证明密集算法的循环动态和大内存容量使它们唯一适合聚类。</p><h1 id="集群中的联想记忆">3.集群中的联想记忆</h1><p>​  在本节中，我们提出了(i)基于AMs的ClAM的数学框架，（ii）激发了其对聚类的适用性，以及（iii）提出了一种学习记忆的方法，以进行良好的聚类。我们将所有这些放在我们的新的基于原型的聚类算法ClAM中。</p><h2 id="联想记忆数学框架">3.1.联想记忆：数学框架</h2><p>​  在d维欧几里得空间中，考虑M内存<spanclass="math inline">\(\boldsymbol{\rho}_\mu\in\mathbb{R}^d,\mu\in[M]\triangleq\{1,\ldots,M\}\)</span>（我们稍后将讨论记忆是如何学习的）。这个数学框架的关键方面是<strong>能量函数</strong>和<strong>attractordynamics</strong>(Krotov &amp; Hopfield, 2021; Millidge et al.,2022)。一个适合聚类的能量应该是一个点（或一个粒子）<spanclass="math inline">\(v\in\mathbb{R}^d\)</span>的连续函数。此外，能量应该有M个局部最小值，对应于每个内存。最后，当一个粒子逐渐接近一个记忆时，它的能量应该主要由单个记忆决定，而剩余的M−1记忆的贡献应该很小。一个满足这些要求的能量函数为：<spanclass="math display">\[E(\boldsymbol{v})=-\frac{1}{\beta}\log\left(\sum_{\mu\in[M]}\exp(-\beta\|\boldsymbol{\rho}_{\mu}-\boldsymbol{v}\|^{2})\right)\]</span>​  用一个标量的<span class="math inline">\(\beta &gt;0\)</span>被解释为一个逆的“温度”。随着<spanclass="math inline">\(\beta\)</span>的增长，方程1中的exp（·）确保只有前导项仍然显著，而其余的M−1项被抑制。因此，整个能量函数将用最近记忆周围的抛物线来描述。该空间将被划分为每个记忆周围的M个吸引盆地，每个记忆周围的能量函数的形状将主要由最近的记忆来定义，并对其他记忆进行了小的修正。<br/>​  attractordynamics通过dv/dt控制v在时间内空间中的移动，同时确保能量减少。这就是dE(v)/dt &lt;0，它确保了一个粒子收敛到一个局部最小值——动力学的一个不动点。attractordynamics可以通过能量景观上的梯度下降来描述： <spanclass="math display">\[\tau\frac{d\boldsymbol{v}}{dt}=-\frac{1}{2}\nabla_{\boldsymbol{v}}E=\sum_{\mu\in[M]}(\rho_{\mu}-\boldsymbol{v})\boldsymbol{\sigma}(-\beta\|\rho_{\mu}-\boldsymbol{v}\|^{2})\]</span>​  其中<span class="math inline">\(\tau &gt;0\)</span>是一个特征<strong>时间常数</strong>，描述了粒子在能量景观上移动的速度，<spanclass="math inline">\(\sigma(\cdot)\)</span>是对记忆缩放距离的softmax函数，定义为<spanclass="math inline">\(\sigma(z_{\mu})=\exp(z_{\mu})\Big/\sum_{m=1}^{M}\exp(z_{m})\)</span>的任何<spanclass="math inline">\(\mu\in[M]\)</span>。这一点如图2a所示。这就保证会减少能量 <spanclass="math display">\[\frac{dE(\boldsymbol{v})}{dt}\stackrel{(\mathbf{a})}{=}\nabla_{\boldsymbol{v}}E(\boldsymbol{v})\cdot\frac{d\boldsymbol{v}}{dt}\stackrel{(\mathbf{b})}{=}-2\tau\Big\|\frac{d\boldsymbol{v}}{dt}\Big\|^2\stackrel{(\mathbf{c})}{\leq}0\]</span>​  其中(a)为链式规则，(b)遵循方程2，(c)中的等式表示局部平稳性。在一个离散的时间步长t+1下，对于点v从状态<spanclass="math inline">\(v_t\)</span>到<spanclass="math inline">\(v^{t+1}=v^t+\delta^{t+1}\)</span>的有效更新<spanclass="math inline">\(\delta^{t+1}\)</span>是通过有限差分： <spanclass="math display">\[\delta^{t+1}=\frac{dt}{\tau}\sum_{\mu\in[M]}(\rho_\mu-v^t)\sigma(-\beta\|\rho_\mu-v^t\|^2)\]</span>​  给定一个点S的数据集，对于每个点v∈S，我们可以用一些噪声破坏它，从而产生一个扭曲的点v˜。这可以作为AM网络v0←v˜的初始状态。AM动态是由可学习的权重ρµ，µ=1，…，M，它也对应于动态（记忆）的固定点。根据方程4，T递归的网络随时间演化，其中选择T以确保充分收敛到一个固定点（见图2b中的例子）。将最终状态vT与未损坏的v进行比较来定义损失函数<span class="math display">\[\mathcal{L}=\sum_{\boldsymbol{v}\inS}\|\boldsymbol{v}-\boldsymbol{v}^{T}\|^{2}\\\mathrm{~where~}\boldsymbol{v}^{0}\leftarrow\tilde{\boldsymbol{v}}\]</span>​  相对于AM参数ρµ，它的反向传播最小。在集群的背景下，AM模型自然诱导一个分区的数据空间不重叠盆地的吸引力，隐式定义了一个硬集群分配定点在每个盆地，并通过完全连续的和可微的动态，允许学习与标准深度学习框架，我们接下来讨论。</p><h2 id="am作为一个可微离散arg-min求解器">3.2.AM作为一个可微离散argmin求解器</h2><p>​  考虑具有数据集<spanclass="math inline">\(S\subset\mathbb{R}^{d}\)</span>的原始<em>k</em>-means目标，其中我们学习k个原型<spanclass="math inline">\(R\triangleq\{\rho_{\mu},\mu\in[k]\}\)</span>与<spanclass="math inline">\([k]\triangleq\{1,\cdot\cdot,k\}\)</span>通过解决以下问题：<span class="math display">\[\min_{R}\sum_{\boldsymbol{x}\inS}\|\boldsymbol{x}-\boldsymbol{\rho}_{\mu_{\boldsymbol{x}}^{\star}}\|^{2}\\\text{s.t.}\mu_{\boldsymbol{x}}^{\star}=\arg\min_{\mu\in[k]}\|\boldsymbol{x}-\boldsymbol{\rho}_{\mu}\|^{2}\]</span>​  <spanclass="math inline">\(\mu^⋆_x\)</span>（对于每个x）的离散选择使方程6成为一个不能通过（随机）直接解决梯度下降的组合优化问题。这个问题的一个常见的连续松弛方法如下：<span class="math display">\[\operatorname*{min}_{R}\sum_{x\inS}\sum_{\mu\in[k]}w_{\mu}(x)\left\|x-\rho_{\mu}\right\|^{2}\]</span>​  其中（通常）<span class="math inline">\(w_{\mu}(x)\ \in\[0,1]\)</span>和<spanclass="math inline">\(\sum{}_{\mu\in[k]}\,w_{\mu}(x)=1,\forall x\inS\)</span>。因此，这些权重<spanclass="math inline">\(\{w_{\mu}(x),\mu\in[k]\}\)</span>定义k原型的概率，并设计在最接近的原型<spanclass="math inline">\(\rho_{\mu_{x}^*}\)</span>，并在剩余的原型上尽可能小。例如，具有到原型的距离的softmax函数<spanclass="math inline">\(\sigma(\cdot)\)</span>已被用作<spanclass="math inline">\(w_{\mu}(x)=\sigma(-\gamma\|\alpha-\rho_{\mu}\|^{2})\)</span>，其中，<spanclass="math inline">\(\gamma &gt; 0\)</span>是一个超参数，并作为<spanclass="math inline">\(\gamma\to\infty,\)</span>，<spanclass="math inline">\(\sum_{\mu\in[k]}\left.w_{\mu}(x)\|x-\rho_{\mu}|\right|^{2}\to\left\|x-\rho_{\mu_x^*}\right\|^{2}\)</span>。其他加权函数也开发出具有类似的特性（Xieetal.，2016），本质上利用了到学习原型的距离的加权和，从而在本质上与在原始问题中的离散分配<spanclass="math inline">\(\mu^⋆_x\)</span>不同（方程6）。另一个微妙的点是，在所有原型上<strong>训练时</strong>对任何x∈S的<strong>软加权分配</strong>与<strong>推理时</strong>对最近原型的<strong>硬聚类分配</strong>不匹配，这导致了训练和推理之间的不一致。<br/>​  我们提出了一个新的替代连续松弛方法来解决离散k均值问题，利用AM动力学（3.1），在k均值目标中保持离散分配。给定原型（内存）<spanclass="math inline">\(R=\{\rho_{\mu},\mu\in[k]\}\)</span>，动力学（方程2）和更新（方程4）确保任何例子（粒子）x∈Rd将收敛到一个原型<spanclass="math inline">\(\rho_{\hat{\boldsymbol{\mu}}_x}\)</span>对应的一个盆地，如果我们设置<spanclass="math inline">\(x^{0}\leftarrow{x}\)</span>和对T时间步长的x0应用公式4中的更新，那么<spanclass="math inline">\({x}^{T}\approx\rho_{\hat{\mu}_{x}}\)</span>。此外，对于适当的β，<spanclass="math inline">\(\hat{\mu}_{x}\)</span>匹配k-means目标中的离散分配<spanclass="math inline">\({\mu}_{x}^*\)</span>（公式6）。<br/>​  这意味着，对于适当设置β和T，<spanclass="math inline">\({x}^{T}\approx\rho_{\mu_{x}^*}\)</span>，允许我们取代所需的每个例子损失<spanclass="math inline">\(\|x-\rho_{\mu_{z}^*}\|^{2}\)</span>目标<spanclass="math inline">\(\|x-x^T\|^{2}\)</span>，允许我们重写k意味着（方程6）以下连续优化问题：<span class="math display">\[\min_R\sum_{x\in S}\left\|x-x_R^T\right\|^2\\\mathrm{~where~}x^0\leftarrow x\]</span> ​  其中，<spanclass="math inline">\(x^T_R\)</span>是通过T递归步骤对任何x∈S进行更新得到的，下标“R”突出了对原型R的依赖性。<br/>​  通过上面的讨论，方程8中的目标对原始k-means目标（方程6）具有期望的离散分配，因为<spanclass="math inline">\(x^T_R\)</span>恰好收敛到其中一个原型。与现有的“加权和”方法相比，这是k-均值目标的一个显著不同的放松。这种松弛的紧密性依赖于β（逆温度）和T（递归深度）的选择——我们将它们视为超参数，并以一种依赖于数据的方式来选择它们。对于任何给定的β和T，我们可以通过通过时间的反向传播，用SGD（和变量）最小化方程8。</p><h2 id="理解由ams引起的分区">3.3.理解由AMs引起的分区</h2><h2 id="clam聚类-与ams和自我监督">3.4.ClAM：聚类 与AMs和自我监督</h2><p>​  接下来，我们希望利用上述AMs（3.1）的强大模式完成能力。我们使用标准掩膜自我监督学习——我们应用（随机）掩膜<spanclass="math inline">\(m\in\{0,1\}^ d\)</span>到点<spanclass="math inline">\(x\inS\subset\mathbb{R}^{\bar{d}}\)</span>和利用上e递归完成部分模式<spanclass="math inline">\(x^0 = m\odotx\)</span>和ground-truth之间的失真掩盖值和完成模式我们的损失如下: <spanclass="math display">\[\mathcal{L}=\sum_{x\inS}\mathbb{E}_{m\sim\mathcal{M}}\|\bar{m}\odot(x-x_{R}^{T})\|^{2},x^{0}\leftarrow m\odot x\]</span> ​  其中<spanclass="math inline">\(x^T_R\)</span>由<spanclass="math inline">\(x^0\)</span>演变而来，方程4的T递归。原型R是通过SGD最小化自监督模式完成损失（方程9）来学习的。精确的学习算法在算法1的TrainClAM子程序中给出，并如图5所示。</p><figure><imgsrc="../postimages/End-to-end-Differentiable-Clustering-with-Associative-Memories/image-20241007095524047.png"alt="image-20241007095524047" /><figcaption aria-hidden="true">image-20241007095524047</figcaption></figure><p>图5：算法1。对于x∈S，我们首先应用一个掩码（紫色的）m到x来得到AM递归的初始迭代x0。对于T递归，我们有一个完整的版本xT R。原型R用自监督损失L上的梯度∇RL进行更新（公式9）。</p><p>​  在随机播种原型（第2行）之后，我们对数据（第3行）（数据批B中的每个x），我们应用随机掩码（第8行），使用当前原型（第9-12行）完成AM递归的模式，累积公式9（第13行），并使用批梯度（第14行）更新原型。</p><figure><imgsrc="../postimages/End-to-end-Differentiable-Clustering-with-Associative-Memories/image-20241006230200990.png"alt="image-20241006230200990" /><figcaption aria-hidden="true">image-20241006230200990</figcaption></figure><p>​  <strong>命题3.1</strong>：算法1中的TrainClAM子例程占用<spanclass="math inline">\(O(dkTN|S|)\)</span>时间，其中<spanclass="math inline">\(|S|\)</span>是S的基数，收敛到一个<spanclass="math inline">\(O(N^{-1/2})\)</span>-平稳点。<br/>​  <spanclass="math inline">\(N^{-1/2}\)</span>项来自于光滑非凸问题的标准SGD的收敛速度（内斯特罗夫，2003），可以通过动量改进到<spanclass="math inline">\(N^{-3/2}\)</span>（Fang等人，2018），考虑到我们的提出的端到端可微性，这是很简单的。算法1的InferClAM子程序使用学习到的原型R分配集群，并一次通过S。这在聚类结束时调用一次。<br/>​  <strong>命题3.2</strong>：算法1中的InferClAM子例程需要占用<spanclass="math inline">\(O(dkTN|S|)\)</span>时间，其中<spanclass="math inline">\(|S|\)</span>是S的基数。<br/>​  从命题3.1和3.2中，我们可以看到，ClAM训练和推理在数据集S中的点<spanclass="math inline">\(|S|\)</span>数、维数d和簇数k上是线性的。对于数据集S上的N个时期，ClAM训练取<spanclass="math inline">\(O(dkTN|S|)\)</span>，其中T为AM递归深度，而k-means训练（采用Lloyd算法）取<spanclass="math inline">\(O(dkN|S|)\)</span>。因此，k-means和ClAM都与点数成线性关系，尽管ClAM运行时有一个乘法系数T，其中T通常小于10。所以k-means直觉上比ClAM快。需要注意的是，k-means和ClAM明显高于谱聚类和凝聚聚类，凝聚聚类的样本数量通常在二次和三次之间。注意，对于相同数量的时代，我们可以为基于SGD的ClAM建立收敛保证（这可以通过使用基于动量的SGD来改进），而k-means没有类似的收敛保证。</p><p>（未完）</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 可微 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Use K-means for Big Data Clustering?</title>
      <link href="/How-to-Use-K-means-for-Big-Data-Clustering/"/>
      <url>/How-to-Use-K-means-for-Big-Data-Clustering/</url>
      
        <content type="html"><![CDATA[<p>How to Use K-means for Big Data Clustering?</p><p>Rustam Mussabayev<span class="math inline">\(^a\)</span> , NenadMladenovic<span class="math inline">\(^a\)</span> , Bassem Jarboui<spanclass="math inline">\(^c\)</span> , Ravil Mussabayev <spanclass="math inline">\(^{b,*}\)</span></p><p>a信息与计算技术研究所，普希金出版社，信息与计算技术研究所，信息过程分析与建模实验室<br/>b华盛顿大学数学系，Padelford Hall C-138, Seattle, 98195-4350, WA,USA<br/>c 高等技术学院工业管理系，St # 19, Abu Dhabi, 25026, UAE</p><h1 id="摘要">摘要</h1><p>​  K-means在数据挖掘中起着至关重要的作用，是欧氏最小平方聚类模型（MSSC，MinimumSum-of-Squares Clustering）下最简单、应用最广泛的算法。然而，当应用于大量数据时，它的性能会急剧下降。因此，通过使用尽可能少的以下计算资源将k均值扩展到大数据，改进k均值是至关重要的：数据、时间和算法成分。我们提出了一种新的并行方案，使用K-means和K-means++算法进行大数据聚类，该算法满足“真实大数据”算法的特性，并在解决方案质量和运行时方面优于经典和最近最先进的MSSC方法。新方法通过分解MSSC问题自然地实现了全局搜索，而不使用额外的元启发式。</p><h1 id="介绍">1.介绍</h1><p>​  聚类分析的目标是通过相似性将实体集合组织成子集，称为集群。聚类分析方法已被证明是一种强大的数据挖掘工具。这些方法解决了模式的无监督分类问题，在不同的领域有大量的应用，如模式识别[1]、模式分类[2]、图像检索和识别[3]、多模态学习[4]、数据挖掘和知识发现[5]、网络分析[6]、文档聚类和数据压缩[7]。聚类分析的应用假设在被分析的数据[8]中存在一个聚类结构。<br/>​  最小平方和聚类（MSSC，MinimumSum-of-Squares Clustering）是聚类分析中的一个基本问题，是研究最广泛的[9]之一。给定一组m个数据点<spanclass="math inline">\(X=\{x_{1},\ldots,x_{m}\}\)</span>在欧几里得空间<spanclass="math inline">\(\mathbb{R}^{n}\)</span>n，它解决的问题找到k个集群中心（centroids）<spanclass="math inline">\(C=(c_{1},\dots,c_{l})\in\mathbb{R}^{n\timesk}\)</span>最小化的平方距离<spanclass="math inline">\(x_i\)</span>到其最近的集群中心<spanclass="math inline">\(c_j\)</span>： <spanclass="math display">\[\operatorname*{min}_{c}f(C,X)=\sum_{i=1}^{m}\|x_{i}-c_{j}\|^{2}\]</span>​  其中，<spanclass="math inline">\(\|\cdot\|\)</span>代表欧几里得规范。式(1)是目标函数，称为距离的平方和。对于一般的k和m，MSSC被认为是一个np困难问题[9]。<br/>​  MSSC问题的突出困难在于目标函数(1)，它有许多局部极小化器。MSSC可以表示为一个全局优化问题，其目标是通过正确地将数据集划分为给定数量的簇[10]来最小化(1)。期望全局最小化器能为给定的数据集[11]提供更好的聚类结构。因此，许多可替代的局部[12]和全局[13]搜索算法被提出，以提供更好的解决方案。<br/>​  虽然大数据和文献中经常使用大数据和大数据的概念，但这些概念并没有明确和普遍接受的定义。实际上，在实践中，用平均计算机上的经典方法对可用数据量进行分析，会导致以下情况之一：处理没有差异；有一些技术困难，但处理仍然是可能的；由于计算资源短缺或不可接受的时间成本，处理不可能。根据这三种情况，可以对所有数据集进行以下明确的分类：小、大和巨大。因此，巨大数据是一个体积如此庞大的数据集，其处理会造成重大的技术困难，或者在使用传统方法和平均计算资源时是不可能的。<br/>​  在本文中，我们只考虑具有许多向量表示和相对较少的特征（向量分量）的大数据类型。这种选择主要是由于在MSSC问题中使用了欧氏距离，而这并不是测量高维向量[14]之间距离的最佳度量。<br/>​  MSSC问题已经在理论上得到了很好的研究。提出了大量的近似[16]算法和一些精确的[17]算法。然而，大多数这些算法对所需的计算成本与数据集的大小有线性或超线性的依赖关系。因此，大多数现有的方法只适用于相对较小的数据集。MSSC[9]的np硬度和实际数据集的大小解释了为什么大多数MSSC算法都是启发式的，旨在在合理的计算时间[11]内产生一个近似解。目前，这一领域的许多研究都是为了开发有效的方法，在大数据集[18]的实际实际条件下解决np硬MSSC问题的[9]，其中大多数经典方法都显示出缺乏效率。<br/>​  算法成分是一组典型的高级操作，它们构成了解决相关问题的各种搜索算法的共同组件。在构建高级启发式算法时，重要的是要保持它们的简单性，使用尽可能少的算法成分来提供可能的最好的结果[19]。通常，所开发的启发式算法的效率和未来的流行程度直接取决于它的简单性，因为这会导致计算成本和时间成本的减少。启发式算法的简单性对于大数据处理尤为重要。<br/>​  本文的其余部分的组织方式如下。第2节将回顾聚类分析领域中最关键的发展；第3节将介绍所提出的算法；第4节将对所提出的算法进行简要分析；第5节和第6节将分析实验结果，得出结论，并概述未来的研究方向。</p><h1 id="相关工作">2.相关工作</h1><p>​  现有的MSSC聚类算法可以分为两大类：传统类和替代类。传统的算法包括被广泛研究的算法，由于其简单性和有效性，在文献中得到了广泛的认可。为了解决MSSC问题，K-means算法与基于它的其他算法一起被广泛接受：Forgy [20]、K-means++[21]、multi-startK-means[22]等。Ward[23]方法也可以归类为传统方法，其性能在所得解的质量方面与最近先进的启发式方法相当；然而，由于需要计算平方距离矩阵，它只适用于中小型的数据集。此外，文献中还积极提出了更复杂的替代算法，其主要目的是提高解决MSSC问题的质量和速度。以下方法是最先进和最新的替代算法之一：MDEClust[13], HG-means [11], LMBM-Clust [18], I-k-means-+ [24], BWKM [16], BDCSM[25], Coresets[26]。绝大多数的替代算法本质上是复杂的和混合的，也就是说，它们通常基于各种元思想，或者是其他几种更简单的算法的组合。通常，复杂的替代算法成为关于所获得的解的质量的最先进的算法，但可能在时间上损失到k均值高达几个数量级。与此同时，有一个深刻的信念，即在质量上获胜对聚类效度的影响很小；然而，这种观点在[11]中存在着积极的争议。更复杂的混合方法的广泛使用在某种程度上导致了质量更好的聚类解决方案；另一方面，这一趋势的不利影响包括同时增加了时间的复杂性和潜在用户理解的困难。替代算法的复杂性，没有其提高的速度的支持，由于潜在用户的代码实现不同，以及隐藏的未探索的特性，导致潜在用户的不信任，这可能在未来导致意想不到的问题。这些问题解释了为什么大多数替代算法在文献和潜在用户中没有得到广泛的接受。因此，一个重要的任务是开发新的更简单的算法，可以在所获得的解的质量和运行时间之间取得正确的平衡。目前，这种平衡只能通过使用Kmeans++种子初始化的K-means算法来实现。<br/>​  K-means算法（Lloyds算法[27]）是最简单的，同时也是最有效的聚类算法之一，因为它在相当短的时间内提供了一个相对高质量的解决方案。它可以被认为是MSSC问题[15]的一种基本求解方法。此外，K-means可以成功地用于加速其他聚类模型，如基于密度的聚类[29]和光谱聚类[30]。<br/>​  K-means是一种迭代改进算法，包括分配和更新两个步骤。该算法的输入数据是k个中心<spanclass="math inline">\(C=(c_{1},\dots,c_{k})\)</span>的初始集合和最大迭代次数。在分配步骤中，每个点<spanclass="math inline">\(x_i\)</span>根据欧几里得距离的平方被分配到最近的中心。接下来，在更新步骤中，通过找到结果分区的每个单元格<spanclass="math inline">\(X_j\)</span>中的点的均值来确定新的中心：<spanclass="math inline">\(c_{j}={\frac{1}{|X_{j}|}}\sum_{x_{i}\inX_{j}}x_{i}\)</span>。重复这两个步骤，直到对集群的点分配没有变化或达到最大迭代次数为止。<br/>​  初始中心集可以通过初始化方法来确定，如K-means++[21]，该方法从原始数据点采样一个初始解，其概率与从候选点到已经采样的中心集[31]之间的平方距离成正比。一种更简单但效率较低的初始化方法是Forgy[20]，它也从原始数据点中均匀随机地采样初始解。|​  虽然K-means++比Forgy慢，但K-means++种子化确保K-means执行的迭代次数要少得多以达到最佳状态。此外，通常K-means++在达到的最优值处比朴素随机初始化提供的目标函数值要小得多。Makarychev等人在[32]中表明，由K-means++得到的解的预期成本最多为最优解成本的5倍（logk+2）。实际上，Kmeans++是初始化Kmeans算法的最新方法。<br/>​  许多可替代的局部[12]和全局[13]搜索算法已经被提出，以提供更好的解决方案。然而，没有一种替代算法比K-means在文献中得到如此广泛的认可和传播。替代算法的不受欢迎可以解释为过度的计算量、高的时间成本以及它们对最终聚类有效性[11]的影响不显著。<br/>​  Kmeans和Kmeans++的结合是文献中解决聚类问题最标准的方法。然而，使用K-means和简单初始化方法的经典方案有两个显著的缺点。首先，最终的聚类结果在很大程度上取决于初始的质心。其次，所获得的解决方案的质量可能比可能的最大值要差得多，特别是随着数据维度和集群数量的增加。经典的K-means算法可以通过适当地选择初始簇中心或将其嵌入到一些全局优化算法中作为局部搜索方法[22]来进行改进。<br/>​  多启动初始化方法克服基于K-means经典方法的缺点。在其最直接的实现中，Kmeans在整个数据集上运行了几次，并最终选择产生最佳质量的结果。在我们提出的方案中，K-means++仅用于初始化第一次K-means运行。所有后续的启动都使用前面的启动中的最佳解决方案初始化，每次运行时只聚集一个随机样本，而不是整个数据集。这两种简化方法都显著地降低了多启动方法的计算复杂度。使用随机样本而不是整个数据集，会在中间解中引入适当数量的可变性，这是寻找全局最优解的必要组成部分。<br/>​  通常，K-means作为高级启发式算法中的局部搜索例程，遵循自定义多启动局部搜索[11,13,24,31]的原则。该算法也不例外。通过将K-means算法嵌入到高级启发式[13]中作为局部搜索例程，可以赋予K-means缺失的全局优化特性。这种方法将消除仅收敛到局部最优解的不必要的性质，这是朴素的k-means算法[11]的主要缺点。根据定制的多启动方法，采用更高级的逻辑初始化单独的局部搜索，关注之前启动的结果，对当前搜索进行更最优的初始化。各种元启发式或元思想通常被构建在多启动过程中，以获得最优的初始化。一些例子有遗传搜索[11,13]、变量邻域搜索（VNS，variableneighborhood search）[12,33]、贪婪随机自适应搜索过程（GRASP，GreedyRandomized Adaptive SearchProcedure）[31]、模拟退火[34]、tabu搜索[35]等。因此，一个主题任务是建立这样的算法，允许在大数据条件下组织全局搜索过程，而不使用任何已知的元启发式或元思想。因此，在所提出的算法中，我们只使用K-means局部搜索的自然性质来组织MSSC问题的最优解的全局搜索。在我们的算法中，多启动方法不仅在使用更高级的初始化逻辑的层次上进行定制，而且在选择数据进行聚类的层次上进行定制。<br/>​  替代算法在质量上优于标准算法。LMBM-Clust算法可以被认为是所有替代算法中最先进的算法。目前，LMBM-Clust提供了最好的质量比和聚类速度。这一特性使得LMBM-Clust适合于大数据的聚类。虽然BDCSM算法[25]在技术上适用于大数据，因为它不需要一个完整的数据集，我们的实验[36]表明，与其他算法相比，BDCSM在聚类质量方面没有显著的优势。绝大多数其他经典和替代算法不适用于大数据，因为它们需要一些（Coresets）或许多（数百或数千）完整通过整个大数据。因此，例如，HG-means花费超过6个小时来集群由13,500个对象和5000个特征组成的k= 25[11]。LMBM-Clust算法在1小时内处理相同的数据集。同时，我们的算法在38秒内完成了该数据集的聚类，并在质量方面提供了最好的结果。如果我们放宽质量要求，我们的算法可以在一个数量级的时间内完成这个数据集的聚类，而只略微损失结果的质量。<br/>​  将聚类算法应用于大数据的必要属性之一是它对数据集[11]大小的可伸缩性。例如，聚类算法可以通过不使用数据集中的所有可用对象而产生结果来实现可伸缩性。可伸缩性也可以通过将数据集分解（或分区）分解为其后续并行处理[36]的部分来实现。假设一个聚类算法可以相对独立地处理每个所获得的数据部分。在这种情况下，它可以在一个分布式系统上实现，其中每个部分都在该分布式计算系统的一个单独的节点上进行处理。该算法可以结合上述所有方法进行缩放。<br/>​  我们的文献综述显示，先进的替代算法在大多数情况下并不适用于大数据。介绍替代算法的文章将这些算法应用于最多由几十万个对象组成的数据集，它们的聚类需要几个小时。即使文献提供了有条件地适用于大数据的替代算法，如coreset或BDCSM，这些替代算法也只能在聚类质量方面接近标准方法，但不能超过它们。同时，相对于标准方法的质量损失可高达20%的[26]。对于大多数标准算法和替代算法，大数据是一个需要解决的问题，而不是用来增强聚类结果的优势。因此，一个关键的重点是开发相对简单的聚类算法，该算法不仅可以对大数据处理有效，而且可以利用大数据作为提高聚类结果的优势。<br/>​  因此，计算机科学界迫切需要开发具有以下有用特性组合的新的“真正的大数据”聚类算法：</p><ul><li>与其他现有的最先进的算法相比，在算法的简单性、聚类结果的质量和收敛速度之间达到最佳平衡；</li><li>在大数据条件下全局搜索最优解，而不使用任何已知的全局优化元启发式；</li><li>可伸缩性，用以下特性表示：<br/> -能够配置在结果的质量和达到它的速度之间的权衡；<br/> -在小、大、巨大数据聚类方面的效果相同；<br/> -能够随着数据集中处理对象数量的增加来提高聚类质量；<br/> -从数据集中所有可用对象的不完全使用中获得可接受质量的结果的可能性；<br/>- 数据集具有并行处理和分布式处理的能力，可分为若干部分；<br/> -新输入数据部分的流处理能力。</li></ul><p>​  结合一种新开发的算法的上述有价值的特性，将使其在文献中获得广泛的认可，并在模式识别的从业者的需求。在本文中，我们试图创建一个新的简单的具有上述所有属性的算法。</p><h1 id="提出的算法">3.提出的算法</h1><h2 id="mssc问题的分解">3.1.MSSC问题的分解</h2><p>​  大多数MSSC算法的性能在应用于大量数据时将受到影响，因为它们的迭代性质和较差的迭代时间局部性。利用MSSC问题分解，我们建立了一个新的启发式，以更快更准确的大数据聚类。这种启发式方法可以使用任何合适的MSSC算法作为本地搜索过程。为了为创建新的高效大数据MSSC算法奠定基础，我们引入了MSSC问题分解的新概念。<br/>​  分解是一种常见的问题解决技术。它将初始问题分解为一组子问题，这些子问题的总体差异不超过初始问题的难度。通过聚合从子问题中得到的解决方案，就可以为整个原始问题创建一个解决方案。<br/>​  MSSC问题的分解是一个由X的q个子集<spanclass="math inline">\(X_i\)</span>组成的集合<spanclass="math inline">\(\mathbb{D}\)</span>，每个子集都有相同的基数，是X的一个简单随机样本：<span class="math display">\[\mathbb{D}=\{X_{i},\ i\in\mathbb{N}_{q}:\X_{i}\subset X,\ |X_{i}|=S\}\]</span> ​  对于<spanclass="math inline">\(i\in\mathbb{N}_{q}\)</span>，在解决了Xi上定义的子问题，可以并行执行后，得到的局部解以某种方式聚合，形成定义在X上的原始问题解。这种方法是一个更一般的分治范式的实例。换句话说，解决一个优化任务有两个主要阶段：分解和聚合。将大数据集分解为相对较小的样本，并使用多个处理器进行并行处理是提高可伸缩性[36]的实用工具。</p><h2 id="big-means算法">3.2.Big-means算法</h2><p>​  该算法的思想非常简单：在每次迭代中，从给定的数据集中抽取一个新的均匀随机样本，并使用k-means进行聚类。使用K-means++算法对第一个样本进行初始化聚类。每个后续的样本都使用在之前的迭代中通过目标函数(1)对样本的最小化方法进行初始化。在中间迭代中，只有退化集群使用K-means++重新初始化。继续迭代，直到满足“停止条件”。对CPU时间的限制或待处理样本的最大数量可以用作“停止条件”。该算法的结果是在整个迭代过程中达到最佳目标函数值(1)的质心集。最终，所有的数据点都可以通过它们接近生成的质心而分布到集群中。</p><hr /><p>算法1：Big Data 聚类</p><hr /><p>1 function BigMeans (X, k, s);<br/> 输入：特征向量<spanclass="math inline">\(X=\{x_{1},\ldots,x_{m}\}\)</span>；<br/>所需的集群数量为k；<br/> 样本量s； <br/> 输出：质心<spanclass="math inline">\(C=\{c_{1},\ldots,c_{k}\}\)</span>；<br/>点到集群的分配<spanclass="math inline">\(A=\{a_{1},\ldots,a_{m}\}\)</span>；<br/>目标函数的值<span class="math inline">\(f(C,X)\)</span>。<br/>2 <spanclass="math inline">\(C\leftarrow\{\emptyset_{1},\dots,\emptyset_{k}\}\)</span>；//所有k个质心都未初始化（即∼退化）<br/>3<span class="math inline">\(f_{o p t}\leftarrow\infty\)</span>；<br/>4<strong>while</strong>不满足停止条件<strong>do</strong><br/>5 | <spanclass="math inline">\(P\leftarrow\)</span>从X中得到的s向量的均匀随机样本；<br/>6| <span class="math inline">\(C^{\prime}\leftarrow C\)</span>；<br/>7 |使用K-means++重新初始化<spanclass="math inline">\(C^{\prime}\)</span>中的所有退化质心；<br/>8 |<span class="math inline">\(C^{\prime\prime}\leftarrowK-means(P,C^{\prime})\)</span>//局部搜索<br/>9 | <strong>if</strong><span class="math inline">\(f_{o p t}\gtf\left(C^{\prime\prime},P\right)\)</span><strong>then</strong><br/>10|<span class="math inline">\(C\leftarrowC^{\prime\prime}\)</span>；//取代了现有的解决方案<br/>11| <spanclass="math inline">\(f_{o p t}\,\leftarrowf(C^{\prime\prime},P)\)</span>；//及其目标<br/>12|<strong>end</strong><br/>13 <strong>end</strong><br/>14 <spanclass="math inline">\(A\leftarrow\)</span>将每个点xi∈X分配到其最近的质心cj∈C<br/>15<strong>return</strong> C, A, f(C, X)</p><hr /><p>​  算法1显示了所提出的大均值启发式算法的一般版本。“Big-means”的源代码可以在https://github.com/R-Mussabayev/bigmeans/上找到。在每次迭代中，从数据集中均匀地随机地抽取一个新的样本。聚类过程由当前最好的质心集初始化，称为现有解。通常，选择的样本大小要比原始数据集的大小小得多。聚类过程由当前最好的质心集初始化，称为现有解。通常，选择的样本大小要比原始数据集的大小小得多。第一个现有解的所有簇中心都是退化的，即未初始化的。对每个后续数据样本的处理首先根据初始化启发式（K-means++）重新初始化现有解C中的退化簇，中间变量C存储此结果。然后，该算法利用局部搜索启发式（K-means）和C作为初始化方法，对样本P进行局部搜索。对样本P进行聚类后，如果得到的质心C导致的目标函数f（C，P）的值小于现有解C的目标函数f（C，P）的值，则声明它们为新的现有解。因此，这次更新将按照“保持最佳状态”的原则进行。在最后一步，每个点xi∈X被分配给其最近的质心cj∈C。<br/>​  我们使用了以下基本启发式的组合：K-means作为局部搜索，K-means++作为初始化例程。然而，大方法可以接受其他合适的快速和准确的MSSC启发式的组合。使用K-means作为局部搜索启发式是合理的，因为K-means是所有MSSC算法中最简单和最快的局部搜索启发式之一。K-means算法具有以下有益的特性：使用更好的初始质心（就目标函数(1)最小化而言）可以加快算法的收敛速度。因此，更好的每个后续样本的初始质心会导致更高的局部收敛速度和加速全局搜索。</p><h2 id="退化解的初始化">3.3.退化解的初始化</h2><p>​  各种方法可以初始化第一个示例[22]，包括一个随机初始化。此外，可以使用不同的策略来处理退化集群，用[37]代替K-means++。在提出的启发式中，使用K-means++获得初始质心并重新初始化退化簇。我们的算法检查当前的局部解C是否存在退化簇，并在必要时重新初始化它们。退化集群是空集群的等价项。通常，在K-means集群过程中，当其所有对象被重新分配到其他集群之间时，一个最初的非空集群就会退化。当使用Kmeans时，集群退化的影响似乎是它的缺点。一些退化簇意味着我们使用更少的簇来最小化目标函数。也就是说，我们能够更密集地将对象打包到更少的集群中。通过使用K-means++添加缺失的簇，我们几乎可以保证得到进一步的改进。</p><h2 id="数据点到集群的最终分布">3.4.数据点到集群的最终分布</h2><p>​  所得到的质心C可以被认为是MSSC问题的一个自给自足的解决方案，因为它们唯一地定义了跨集群的数据点的分布。该算法的最后一步是简单地在所获得的质心之间重新分配所有的数据点。假设K-means从初始质心c开始聚集整个数据集，而不是最后一步。由于K-means对整个数据集的计算成本很高，这种聚类对于实际的大数据可能不可行。然而，如果发生了这种情况，我们应该期望在最小化目标函数(1)方面，结果集群的质量会有更显著的改进。使用我们的算法的最后阶段可能没有一些实际应用。例如，有时，仅将有限的对象样本分配给它们最近的质心就足够了。如果我们省略了算法的最后一步，我们可以考虑将大均值作为在大数据条件下初始化k均值的一种新方法。</p><h2 id="取样">3.5.取样</h2><p>​  在Big-means中，有一种获取数据样本的方法是至关重要的。我们已经将统一抽样纳入了大均值法中。这种抽样不计算任何联合概率分布。它可以用O(1)的总复杂度来完成，假设样本大小是一个预定义的常数数。这种抽样方法是计算上最快的，从简单的角度来看是最优的。此外，在对数据集X结构的温和假设下，[38]的作者为应用于MSSC聚类的简单均匀抽样提供了可靠的概率保证。此外，也不需要在数据集中随机排列特征向量，因为均匀采样会自动打破存在的任何顺序。<br/>​  在Big-means的框架下，采样过程自然起着振动过程的作用。振动过程在每次迭代中修改现有的解，得到一个邻域解。这是大手段的下一个新奇事物。我们通过绘制一个小的、均匀的随机样本，从而在每次迭代中获得原始数据集的稀疏表示。如果我们在Rn中将整个数据集表示为点云，那么稀疏样本将在一定程度上近似该云的空间形式。因此，减少样本量增加了在样本上的聚类结果的可变性。样本量越小，对现有解的扰动越强；反之亦然，样本量越大，扰动就越弱。因此，通过微调样本量，人们应该在变异性的程度和原始数据形式的近似程度之间找到适当的权衡。对样本量的最优选择将导致算法所期望的最优行为。不同样本的聚类结果之间的可变性增加对全局搜索有积极的影响，因为每个后续样本都提供了一些变化的聚类配置，这在搜索过程中检查最优性。</p><h2 id="并行处理">3.6.并行处理</h2><p>​  由于MSSC问题是np困难的，并应用于大数据，因此开发有效利用现代高性能计算（HPC）技术的MSSC算法是至关重要的。HPC使用超级计算机和计算机集群来解决高级计算问题。由于HPC网络集群和网格使用多台处理器和计算机，因此可伸缩性是HPC算法的一个基本属性。可伸缩性的实现主要是由于使用许多可用的计算节点、计算机或处理器并行处理所分析的数据。该算法1的固有性质有助于其有效的并行化。该算法的并行化可以通过几种不同的方式来实现：</p><ol type="1"><li>单独的数据样本被依次处理，但聚类过程在K均值和K均值++的实现级别上是并行化的。也就是说，大均值的主循环都是保持顺序的，而Kmeans和Kmeans++的所有内部循环都是并行化的；<br/>2.工作人员在每个可用的处理器上并行开始，每个工作人员使用K-means和K-means++的顺序版本独立于其他工作人员集群其数据样本。工人在每次迭代时只使用他们以前的最佳质心进行初始化。这种并行化模式被称为竞争，因为所有的工人都是独立的，并且相互竞争；<br/>3.工作人员在每个可用的处理器上并行开始，每个工人都被分配了一个单独的数据样本。在第一次迭代中，每个工作人员都独立地执行初始化集群过程。在所有后续的迭代中，每个工作者使用在之前的迭代中获得的所有工作者中的最佳质心集来初始化一个新的随机数据样本。这种并行化模式被称为集体模式，因为工人们共享关于最佳解决方案的信息。</li></ol><p>​  在实验中，我们使用了一个CPU数量相对较少的硬件配置。在此配置中，初步实验表明，第一种并行化方法比其他两种方法更有效，因为其主循环之前迭代的最佳结果由于它们的执行顺序而更有效地转移到后续迭代中。因此，在第5节中使用竞争算法的实验采用了第一种并行化方法。尽管第二种和第三种方法在工作人员之间重用最佳结果的效率方面不如第一种方法，但它们对大量处理器具有更好的可伸缩性。当使用巨大的数据集时，也可能会出现关于如何在计算系统的并行节点之间分配处理过的数据的问题。在我们的Bigmeans实现中，所有工作人员都可以访问公共数据集，每个工作人员为自己形成了一个聚类的随机数据样本。然而，其他方案也可以在节点之间部分地分配数据集。对各种大均值并行化和数据分布方案的有效性进行更详细的研究是未来研究的一个课题。</p><h1 id="对该算法的分析">4.对该算法的分析</h1><h2 id="主要特性">4.1.主要特性</h2><h2 id="时间复杂度">4.2.时间复杂度</h2><h1 id="实验分析">5.实验分析</h1><h1 id="结论及未来的研究">6.结论及未来的研究</h1>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> K-means </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PiCIE:Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering</title>
      <link href="/PiCIE/"/>
      <url>/PiCIE/</url>
      
        <content type="html"><![CDATA[<p>PiCIE: Unsupervised Semantic Segmentation using Invariance andEquivariance in Clustering</p><h1 id="摘要">摘要</h1><p>​  本文提出了一种新的无注释的语义分割框架。现成的聚类方法仅限于策划的、单标签的和以对象为中心的图像，而现实世界的数据主要是未策划的、多标签的和以场景为中心的。我们将聚类从图像扩展到像素，并为每个图像中的不同实例分配单独的聚类成员关系。然而，仅仅依赖于像素级的特征相似度并不能学习高级语义概念，以及对低级视觉线索的过度拟合。<br/>​  我们提出了一种将几何一致性作为归纳偏差的方法来学习光度和几何变化的不变性和等方差。有了我们新颖的学习目标，我们的框架可以学习高级语义概念。我们的方法，PiCIE（Pixel-levelfeature Clustering using Invariance andEquivariance），是第一种能够在没有任何超参数调优或任务特定的预处理的情况下同时分割事物和东西类别的方法。我们的方法在COCO[31]和城市景观[8]上大大优于现有的基线，+17.5的Acc和+4.5mIoU。我们证明了PiCIE为标准的监督训练提供了更好的初始化。该代码可在https://github.com/janghyuncho/PiCIE上找到。</p><h1 id="介绍">1.介绍</h1><p>​  从一组未标记的图像中进行无监督学习已经获得了广泛的欢迎，但仍然主要局限于单类的、以对象为中心的图像。请考虑到图1（上图）中所示的图像。给定这些和其他未标记图像的集合，机器能从每张图像中发现“草”、“天空”、“房子”和“树”的概念吗？更进一步说，它能否识别出每个概念在每个图像中出现的位置，并将其分割出来？<br/>​  一个能够进行这种无监督语义分割的系统可以自动发现具有精确边界的对象类别，从而消除了收集和标记COCO等数据集的大量成本。它甚至可能发现注释者可能事先不知道的对象、材料和纹理。这对于分析新的领域特别有用：例如，在卫星图像中发现新的视觉结构。该系统发现和分割未知物体的能力也可能被证明对试图在野外操纵这些物体的机器人很有用。<br/>​  然而，虽然无监督语义分割可能有用，但它也具有挑战性。这是因为它结合了类发现的问题和穷尽像素标记的挑战。自监督学习和无监督学习的最新进展表明，识别系统肯定可以发现图像水平的类。然而，图像级标记更容易，因为网络可以简单地依赖于一些独特的、稳定的特征，而丢弃图像的其余部分。例如，一个识别系统可以通过将图1中的所有四张图像组合在一起，只需检测每个图像中的屋顶瓦片的存在，而忽略图像中的其他一切。相比之下，在分割图像时，没有任何像素可以被忽略；无论它是一个独特的对象（东西）还是一个背景实体（东西），每个像素都必须被识别和准确地表征，尽管有潜在的较大的类内变化。因此，很少有之前的工作试图解决发现语义分割的问题，结果仅限于极其粗糙的东西分割。<br/>​  在本文中，我们朝着一个实际有用的无监督语义分割系统迈出了一步：我们提出了一种方法，能够以比现有技术更细的粒度分割所有像素，无论是东西还是东西。我们的方法是基于一个直接的目标，它只编码了两个常识性的约束。首先，具有相似外观的像素（即，它们在一个学习到的特征空间中聚集在一起）应该被类似地标记，反之亦然。其次，像素标签应该与颜色空间变换不变，与几何变换等变。我们的结果表明，单独使用这两个目标，我们可以训练一个基于ConvNet的语义分割系统的端到端，没有任何标签。<br/>​  我们发现，尽管它很简单，但我们的方法在这个任务上远远优于之前的工作，比现有技术的准确性提高了一倍多（图1，底部）。与之前的工作相比，我们的基于聚类的损失函数（上面的第一个目标）导致了一个更简单、更容易的学习问题，而之前的工作则试图学习参数像素分类器。但不变性和等方差目标是关键。它们允许卷积网络将跨尺度、姿态和颜色变化的像素连接在一起，而这是之前的系统无法做到的。这种对不变性的鲁棒性也允许我们的方法有效地分割对象。我们通过一项消融研究证明了这些直觉是正确的，我们发现其中每一个都有助于显著改善性能。<br/>​  总之，我们的研究结果表明，卷积网络不仅可以学习发现图像级的概念，而且还可以在没有任何监督的情况下对图像进行语义解析。这为真正的大规模发现打开了大门，在那里，这样一个经过训练的网络可以自动地从一个未标记的、未管理的数据集中呈现出新类别的对象、材料或纹理。</p><h1 id="相关工作">2.相关工作</h1><p>​  <strong>学习聚类。</strong>利用深度神经网络来学习聚类友好的嵌入空间，[4,5,58,57,51,14,54]已经得到了广泛的研究。DEC[51]和IDEC[14]训练嵌入函数通过训练自动编码器（AE）[49]与重建损失。深度聚类及其变体[4,5,57]使用k-means[38]对整个数据集的特征向量进行了明确的聚类，以便为每个数据点分配伪标签，然后训练一个编码器网络。所有这些方法都有一个共同的理念，即聚类损失的迭代优化改进了特征空间，以考虑高级视觉相似性。<br/>​  除了从表示学习的角度来看，最近有一些工作通过聚类数据点[51,18,17,23,48,55]来处理没有标签的分类问题。IIC[23]，SeLa[55]和其他工作的[48,34,60,16]最大化了从单个图像的两个版本的软集群分配之间的互信息。最大化互信息可以防止网络陷入退化解，但有效地加强了集群上的均匀分布。因此，无监督聚类预计只适用于良好平衡的数据集，如MNIST[28]和CIFAR [26]。最近在ImageNet等大规模数据集如ImageNet[9]上进行了测试，仍然假设了一组平衡的单类、以对象为中心的图像。由于这些方法不显式地对数据进行聚类，因此它们被称为隐式聚类方法，这与显式聚类[51,14,4,5,57,52,56,42,29]相反。</p><p>​  <strong>没有标签的分割。</strong></p><p>​  <strong>等效学习。</strong></p><p>#3.PiCIE</p><p>​  我们得到了一个未经管理的、未经标记的图像数据集 D.在这个数据集上，我们想要发现一组视觉类C并学习一个语义分割函数fθ。当提供来自D的一个看不见的图像时，fθ应该能够从类集合中为每个像素分配一个标签C。<br/>​  我们将无监督图像分割的任务表示为像素级聚类，其中每个像素都被分配给一个聚类。聚类通常需要一个好的特征空间，但没有预先存在这样的特征表示。因此，我们提出了一种与聚类联合学习特征表示的方法。PiCIE（Pixel-levelfeature Clustering using Invariance andEquivariance）的整体架构，即使用不变性和等方差的像素级特征聚类，如图2所示。我们将在下面描述我们的方法。</p><figure><img src="../postimages/PiCIE/image-20250108110707600.png"alt="image-20250108110707600" /><figcaption aria-hidden="true">image-20250108110707600</figcaption></figure><h2 id="一种聚类基线方法">3.1.一种聚类基线方法</h2><p>​  我们从之前的工作开始，学习端到端神经网络，将未标记图像聚类到图像级类[4,5,51,14,53]。本文所解决的关键问题是，将图像聚类为类需要强特征表示，而对于训练强特征表示，则需要类标签。为了解决这个鸡和蛋问题，最简单的解决方案是深度集群[4]确定的：交替使用当前的特征表示进行聚类，并使用聚类标签作为伪标签来训练特征表示。对于无监督的语义分割任务，人们也可以遵循类似的策略。唯一的区别是，我们需要使用一个嵌入函数fθ来生成一个特征映射，为每个像素生成一个特征向量。分类器还必须对单个像素进行操作。然后，我们可以交替聚类像素特征向量来得到像素伪标签，并使用这些伪标签来训练像素特征表示。<br/>​  具体地说，假设我们有一组未标记的图像<spanclass="math inline">\(x_{i},i=1,\cdot\cdot\cdot,n\)</span>。假设我们的嵌入用fθ表示，产生了一个特征张量fθ(x)。这将为图像x中的每个像素p产生一个特征表示。用fθ(x)[p]表示这个像素级的特征表示。用gw（·）表示一个处理这些像素特征向量的分类器。然后，我们的基线方法在两个步骤之间交替进行：</p><ol type="1"><li>使用当前的嵌入和k-means来聚类数据集中的像素。</li></ol><p><spanclass="math display">\[\operatorname*{min}_{N,\mu}\sum_{i,p}||f_{\theta}(x_{i})[p]-\mu_{y_{ip}}||^{2}\]</span></p><p>​  其中，<span class="math inline">\(y_{ip}\)</span>为第<spanclass="math inline">\(i\)</span>幅图像中第p个像素的聚类标签，<spanclass="math inline">\(\mu_k\)</span>为第k个聚类质心。（我们使用小批量k-means[39]）。</p><ol start="2" type="1"><li>使用聚类标签使用标准交叉熵损失训练像素分类器。</li></ol><p><span class="math display">\[\operatorname*{min}_{\theta,{\bfw}}\sum_{i,p}\mathcal{L}_{C E}(g_{\bf w}(f_{\theta}(x_{i})[p]),y_{ip})\]</span></p><p><span class="math display">\[\mathcal{L}_{CE}(g_{\mathrm{w}}(f_{\theta}(x_{i})[p]),y_{ip})=-1\mathrm{og}\,\frac{e^{s_{y_{ip}}}}{\sum_{k}e^{s_{k}}}\]</span></p><p>​  其中sk是分类器<spanclass="math inline">\(g_{\mathrm{w}}(f_{\theta}(x_{i})[p])\)</span>输出的第k类分数。</p><h2 id="基于非参数原型的分类器">3.2.基于非参数原型的分类器</h2><p>​  上面的深度聚类启发框架使用了一个单独的、已学习的分类器。然而，在不断变化的伪标签的无监督设置中，联合训练分类器是具有挑战性的。一个训练不足的分类器可以将噪声梯度输入到特征提取器中，导致下一轮训练的噪声聚类。<br/>​  因此，我们建议完全抛弃参数化像素分类器gw。相反，我们根据像素到kmeans估计的质心（“原型”[41]）的距离来标记像素。这就导致了以下目标的改变。<spanclass="math display">\[\operatorname*{min}_{\theta}\sum_{i,p}\mathcal{L}_{cl u s t}(f_{\theta}(x_{i})[p],y_{i p},\mu)\]</span></p><p><span class="math display">\[\mathcal{L}_{c l u st}(f_{\theta}(x_{i})[p],y_{ip},\mu)=-log\left(\frac{e^{-d(f_{\theta}(x_{i})[p],\mu_{y_{ip})}}}{\sum_{l}e^{-d(f_{\theta}(x_{i})[p],\mu_{l})}}\right)\]</span></p><p>​  其中，<spanclass="math inline">\(d(\cdot,\cdot)\)</span>为余弦距离。</p><h2 id="不变性和等效性">3.3.不变性和等效性</h2><p>​  将上述特征表示与聚类联合学习肯定会产生在特征空间中紧凑的集群，但没有理由说这些集群必须是语义的。为了得到像素的语义分组，我们需要引入一个额外的归纳偏差。如果我们没有标签，这种归纳偏差又会是什么呢？<br/>​  我们引入的归纳偏差是光度变换的不变性和几何变换的等方差：如果像素颜色轻微抖动，标记不应该改变，当图像被几何扭曲时，标记应该是类似的扭曲。具体地说，如果Y是图像x的输出语义标记，如果P和G分别是光度变换和几何变换，则变换后的图像G（P(x)）的输出语义标记应该是G（Y）。<br/>​  在联合聚类和学习框架中实现这个约束是很棘手的，因为每个图像都没有ground-truth标签。伪ground-truth标记本身来自于聚类，而聚类本身是由特征映射产生的，因此它本身对输入转换很敏感。因此，在这种情况下，不变性/等方差意味着两件事：第一，我们应该产生相同的簇，而不考虑转换，第二，预测的像素标签应该显示所期望的等或不等方差。</p><h3 id="对光度变换的不变性">3.3.1对光度变换的不变性</h3><p>​  我们首先讨论不变性的问题。对于数据集中的每一个图像<spanclass="math inline">\(x_i\)</span>，我们随机抽取两个光度变换，<spanclass="math inline">\(P_i^{(1)}\)</span>和<spanclass="math inline">\(P_i^{(2)}\)</span>。这将为每个图像xi中的每个像素p产生两个特征向量：<span class="math display">\[z_{ip}^{(1)}=f_{\theta}(P_{i}^{(1)}(x_{i}))[p]\]</span></p><p><span class="math display">\[z_{ip}^{(2)}=f_{\theta}(P_{i}^{(2)}(x_{i}))[p]\]</span></p><p>​  然后，我们分别在两个“视图”中进行聚类，得到两组伪标签和质心： <spanclass="math display">\[\mathbf{y}^{(1)},\mu^{(1)}={\mathrm{arg\,min}}\sum_{i,p}\left|\left|z_{ip}^{(1)}-\mu_{y i p}\right|\right|^{2}\]</span></p><p><spanclass="math display">\[\mathbf{y}^{(2)},\mu^{(2)}={\mathrm{arg\,min}}\sum_{i,p}\left|\left|z_{ip}^{(2)}-\mu_{y i p}\right|\right|^{2}\]</span></p><p>​  给定这两组质心和这两组伪标签，我们使用了两组损失函数：</p><ol type="1"><li>与前面一样，我们希望特征向量坚持聚类标签。现在我们有了两个视图，我们希望在每个视图中都是如此：</li></ol><p><span class="math display">\[\mathcal{L}_{w i t h in}=\sum_{i,p}\mathcal{L}_{c l u s t}(z_{i p}^{(1)},y_{ip}^{(1)},\mu^{(1)})+\mathcal{L}_{c l u s t}(z_{i p}^{(2)},y_{ip}^{(2)},\mu^{(2)})\]</span></p><ol start="2" type="1"><li>因为我们假设聚类应该对光度转换是不变的，所以我们也希望来自一个视图的特征向量来匹配另一个视图的聚类标签和质心：</li></ol><p><span class="math display">\[\mathcal{L}_{c r o ss}=\sum_{i,p}\mathcal{L}_{c l u s t}(z_{i p}^{(1)},y_{ip}^{(2)},\mu^{(2)})+\mathcal{L}_{c l u s t}(z_{i p}^{(2)},y_{ip}^{(1)},\mu^{(1)})\]</span></p><p>​  这个多视图框架和交叉视图损失实现了两件事。首先，通过强制来自一个转换的特征向量坚持由另一个转换产生的标签，它鼓励网络学习特征表示，这将被相同地标记，而不考虑任何光度变换。其次，通过强制相同的特征表示与两种不同的聚类解决方案保持一致，它鼓励了对这两种解决方案本身进行匹配，从而确保通过聚类发现的概念集对光度变换是不变的。</p><h3 id="与几何变换的等效方差">3.3.2 与几何变换的等效方差</h3><p>​  房子和放大的房子应该贴上类似的标签，但可能产生截然不同的特征。更准确地说，放大内部的分割应该是原始分割的放大版本。这是几何变换（如随机作物）的等方差的概念，我们在接下来添加它。<br/>​  为了学习关于几何变换的等方差，我们对每幅图像采样一个几何变换（具体来说，随机裁剪和水平翻转）Gi。然后，在上述框架中，一个视图使用转换后的图像的特征向量，而另一个视图使用原始图像的转换后的特征向量：<span class="math display">\[z_{ip}^{(1)}=f_{\theta}(G_{i}(P_{i}^{(1)}(x_{i})))[p]\]</span></p><p><span class="math display">\[z_{ip}^{(2)}=G_{i}(f_{\theta}(P_{i}^{(2)}(x_{i})))[p]\]</span></p><p>​  其他的步骤也是完全相同的。这两个视图分别聚类，最终的训练目标是视图内目标和交叉视图目标的组合：<span class="math display">\[\mathcal{L}_{t o t a l}=\mathcal{L}_{w i th i n}+\mathcal{L}_{c r o s s}\]</span></p><h1 id="实验">4.实验</h1><h2 id="训练详细信息">4.1.训练详细信息</h2><h2 id="基线">4.2.基线</h2><p>​  我们描述了我们比较PiCIE与： IIC[23]和改进深度聚类[4]方法。它们是最先进的隐式和显式的基于聚类的学习方法。</p><p>​  <strong>IIC。</strong>IIC[23]是一种隐式聚类方法，其中网络直接预测每个像素级特征向量的（软）聚类分配。主要目标是最大化一个像素和相邻像素(s)的预测之间的相互信息。对于控制实验，我们使用与PiCIE相同的ResNet-18的FPN，以及ResNet-18的前两个残差块（IIC-res12），类似于原始的浅层VGG-like[40]模型（细节见补充）。在原始论文[23]之后，我们使用了K =45的辅助过聚类损失。</p><p>​  <strong>改进深度聚类。</strong>深度聚类是一种显式的聚类方法，其中网络对给定图像的特征向量进行聚类，并使用聚类分配作为标签来训练网络。为了适应我们的问题设置，我们修改了原始的深度集群，以在最终池化层之前集群像素级的特征向量。这允许网络为每个像素分配一个标签。然而，由于图像的大小爆炸了要聚类的特征向量的数量，我们应用小批k-means[39]首先计算聚类质心，分配标签，并训练网络。</p><h2 id="数据集">4.3.数据集</h2><h2 id="结果">4.4.结果</h2><h2 id="消融研究">4.5.消融研究</h2><h2 id="分析">4.6.分析</h2><h1 id="结论">5.结论</h1><p>​  本文介绍了一种基于聚类的无监督语义分割框架。我们的主要贡献是将几何一致性作为归纳偏差，以学习光度和几何变化的不变性和等方差。我们的新交叉视图丢失是简单但非常有效的学习高级视觉概念必要的分割事物的类别。我们的方法是第一个无监督的语义分割，它既适用于东西和事物类别，又没有严格的超参数调整或特定于任务的预处理。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey on Deep Clustering:From the Prior Perspective</title>
      <link href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/"/>
      <url>/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/</url>
      
        <content type="html"><![CDATA[<p>A Survey on Deep Clustering: From the Prior Perspective</p><p>四川大学计算机科学学院，成都，中国四川</p><h1 id="摘要">摘要</h1><p>​  由于神经网络具有强大的特征提取能力，深度聚类在分析高维和复杂的真实世界数据方面取得了巨大的成功。深度聚类方法的性能受到网络结构和学习目标等各种因素的影响。然而，正如本调查中所指出的，深度聚类的本质是对先验知识的整合和利用，这在很大程度上被现有的工作忽略了。从开创性基于数据结构假设的深度聚类方法到最近基于数据增强不变性的对比聚类方法，深度聚类的发展本质上对应于先验知识的演化。在本调查中，我们通过将深度聚类方法分为六种先验知识类型，提供了一个全面的回顾。我们发现，总的来说，先前的创新遵循两个趋势，即，i)从采矿到建设，以及ii)从内部到外部。此外，我们在五个广泛使用的数据集上提供了一个基准，并分析了具有不同先验的方法的性能。通过提供一个新的先验知识视角，我们希望这次调查能够提供一些新的见解，并启发未来在深度聚类社区的研究。</p><h1 id="介绍">1 介绍</h1><h1 id="问题定义">2 问题定义</h1><p>​  在本节中，我们将介绍深度聚类的管道，包括符号和问题定义。除非特别通知，否则在本文中，我们使用粗体大写和小写分别表示矩阵和向量。表1总结了常用的符号。</p><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107222628947.png"alt="image-20250107222628947" /><figcaption aria-hidden="true">image-20250107222628947</figcaption></figure><p>​  深度聚类问题的正式定义如下：给定一组属于C类的实例<spanclass="math inline">\(\mathcal{D}=\{\bfx_{i}\}_{i=1}^{N}\in\mathcal{X}\)</span>，深度聚类的目的是学习鉴别特征，并根据实例的语义将其分为C聚类。具体来说，深度聚类方法首先学习一个深度神经网络<spanclass="math inline">\(f:{\mathcal{X}}\rightarrow{\mathcal{Z}}\)</span>用于特征提取<spanclass="math inline">\(\mathbf{z}_{i}=f(\mathbf{x}_{i})\)</span>。给定潜在空间的实例特征，可以得到聚类结果。最直接的方法是应用经典算法，如K-means（MacQueen等，1967）和DBSCAN（Ester等，1996a）。另一种解决方案是训练一个额外的集群头<spanclass="math inline">\(h\:\mathcal{Z}\to\mathbb{R}^{C}\)</span>生成满足<spanclass="math inline">\(\textstyle{\sum_{i=0}^{K}\mathbf{p}_{ij}}\;=\;1\)</span>的生成软集群分配<spanclass="math inline">\(\mathbf{p}_i=\operatorname{sotrmax}(h(\mathbf{z}_{i}))\)</span>。第<spanclass="math inline">\(i\)</span>个实例的硬簇分配可以通过argmax操作来计算，即： <spanclass="math display">\[\tilde{y}_{i}=\arg\operatorname*{max}_{j}\ {\bfp}_{i j},1\leq j\leq C\]</span>​  聚类分配提供了数据底层的固有语义结构，可以用于各种下游分析。</p><h1 id="深度聚类的先验条件">3 深度聚类的先验条件</h1><p>​  在本节中，我们将从先验知识的角度来回顾现有的深度聚类方法。先验情况如图1所示，方法分类总结在表2中。</p><hr /><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107223309527.png"alt="image-20250107223309527" /><figcaption aria-hidden="true">image-20250107223309527</figcaption></figure><p>图1.深度聚类的6类先验知识。(a)结构先验：数据结构可以反映实例之间的语义关系。(b)分布先验：来自不同集群的实例遵循不同的数据分布。(c)增强不变性：由相同实例增强的样本具有相似的特征。(d)邻域一致性：相邻的样本具有一致的聚类分配。(e)伪标签：具有高可信度的聚类分配很可能是正确的。(f)外部知识：在开放世界的数据和模型中存在大量有利于聚类的知识。</p><hr /><p>表2 从先验知识的角度总结了深度聚类方法。</p><table><colgroup><col style="width: 14%" /><col style="width: 50%" /><col style="width: 20%" /><col style="width: 14%" /></colgroup><thead><tr class="header"><th>先验知识</th><th></th><th>方法</th><th>主要贡献</th></tr></thead><tbody><tr class="odd"><td>结构先验</td><td>固有的数据结构反映了语义关系</td><td>ABDC (2013)</td><td>以EM的方式优化特征和聚类分配</td></tr><tr class="even"><td></td><td></td><td>DEN (2014)，SpectralNet (2018)</td><td>将光谱聚类从浅层扩展到深层</td></tr><tr class="odd"><td></td><td></td><td>PARTY (2016)</td><td>引入了从子空间学习到深度聚类的稀疏性先验</td></tr><tr class="even"><td></td><td></td><td>JULE (2016)</td><td>将聚集层从浅层扩展到深层</td></tr><tr class="odd"><td></td><td></td><td>DCC (2018)</td><td>提出了关系匹配来实现非参数深度聚类</td></tr><tr class="even"><td>分布先验</td><td>不同语义的实例遵循不同的数据分布</td><td>VaDE (2016)</td><td>利用高斯混合模型学习不同的簇分布</td></tr><tr class="odd"><td></td><td></td><td>ClusterGAN (2019)，DCGAN (2015)</td><td>使用GAN隐式地学习集群分布</td></tr><tr class="even"><td>增强不变性</td><td>实例特征对数据增强不变</td><td>IMSAT (2017)</td><td>提出了成对增广样本之间的不变性</td></tr><tr class="odd"><td></td><td></td><td>IIC (2019)，Completer (2021)</td><td>提出了关于增强不变性的互信息框架</td></tr><tr class="even"><td></td><td>集群分配对数据增强是不变的</td><td>PICA (2020)</td><td>探索增强样本的聚类分配之间的不变性</td></tr><tr class="odd"><td></td><td></td><td>CC (2021)，DRC (2020)</td><td>同时探索在实例级别和集群级别上的增强不变性</td></tr><tr class="even"><td></td><td></td><td>TCC (2021)</td><td>利用集群语义和实例组合的统一表示</td></tr><tr class="odd"><td>邻域一致性</td><td>相邻的实例具有相似的语义</td><td>SCAN (2020)</td><td>在相邻实例之间强加一致的集群分配</td></tr><tr class="even"><td></td><td></td><td>NNM (2021)</td><td>在邻居之间执行集群级的对比学习</td></tr><tr class="odd"><td></td><td></td><td>GCC (2021)</td><td>在邻居之间执行实例级和集群级的对比学习</td></tr><tr class="even"><td>伪标签</td><td>具有高置信度的聚类分配是可靠的</td><td>DEC (2016)</td><td>通过锐化构造目标集群分布</td></tr><tr class="odd"><td></td><td></td><td>DeepCluster (2018)</td><td>使用K-means生成伪标签</td></tr><tr class="even"><td></td><td></td><td>SCAN (2020)</td><td>选择高置信度的预测，用强增强样本来微调模型</td></tr><tr class="odd"><td></td><td></td><td>SPICE (2022)</td><td>利用原型选择伪标签，采用半监督学习的方法对模型进行微调</td></tr><tr class="even"><td></td><td></td><td>TCL (2022)</td><td>在对比学习中使用伪标签来减少假负对</td></tr><tr class="odd"><td></td><td></td><td>ProPos (2022)</td><td>使用K-means中的伪标签来增加集群的紧凑性</td></tr><tr class="even"><td>外部知识</td><td>在开放的世界中存在着丰富的集群有利知识</td><td>SIC (2023)</td><td>从预先训练过的视觉语言模型的文本空间中生成图像伪标签</td></tr><tr class="odd"><td></td><td></td><td>TAC (2023b)</td><td>构建更有区别的文本，并进行跨模态蒸馏来改进聚类</td></tr></tbody></table><hr /><h2 id="结构先验">3.1 结构先验</h2><h2 id="分配先验">3.2 分配先验</h2><h2 id="增强不变性">3.3 增强不变性</h2><h2 id="邻域一致性">3.4 邻域一致性</h2><h2 id="伪标签">3.5 伪标签</h2><h2 id="外部知识">3.6 外部知识</h2><h1 id="实验">4 实验</h1><p>​  在本节中，我们将介绍对深度聚类的评估。简单地说，我们首先介绍评估指标和通用基准。然后对现有的深度聚类方法的结果进行了分析。</p><h2 id="评价指标">4.1 评价指标</h2><p>​  对于聚类评估，通常使用三个度量标准来度量预测的聚类分配<spanclass="math inline">\(\tilde{y}\)</span>如何与 ground-truth标签<spanclass="math inline">\(y\)</span>相匹配，包括准确性（ACC）、标准化互信息（NMI）和调整兰德系数（ARI）。指标值越高，对应的聚类性能越好。这三个指标的定义如下：</p><ul><li>ACC（Amig´o et al, 2009）表示聚类预测的正确率：</li></ul><p><spanclass="math display">\[\mathrm{ACC}={\frac{1}{N}}\sum_{i=1}^{N}\mathrm{1}\{y_{i}=\tilde{y}_{i}\},\]</span></p><ul><li>NMI（McDaid et al，2011）量化了预测标签<spanclass="math inline">\(\tilde{Y}\)</span>和 ground-truth标签<spanclass="math inline">\(Y\)</span>之间的互信息：</li></ul><p><span class="math display">\[\mathrm{NMI}={\frac{I(\bar{\bf Y};{\bfY})}{\frac{1}{2}[H(\bar{\bf Y})+H({\bf Y})]}},\]</span></p><p>​  其中H (Y)表示Y的熵，<span class="math inline">\(I(\bar{\bf Y};{\bfY})\)</span>表示<span class="math inline">\(\tilde{Y}\)</span>和<spanclass="math inline">\(Y\)</span>之间的互信息。</p><ul><li>ARI（Hubert and Arabie，1985）是兰德系数(RI，randindex)的归一化，它计算同一集群和不同集群中的实例对的数量：</li></ul><p><spanclass="math display">\[\mathrm{RI}={\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{C}_{N}^{2}}},\]</span></p><p>​  其中，TP和TN为真正对和真负对的个数，<spanclass="math inline">\(C^2_N\)</span>为可能的实例对的个数。ARI是通过添加以下规范化来计算出来的：<spanclass="math display">\[\mathrm{ARI}={\frac{\mathrm{RI}-\mathbb{E}(\mathrm{RI})}{\operatorname*{max}(\mathrm{RI})-\mathbb{R}(\mathrm{RI})}},\]</span></p><h2 id="数据集">4.2 数据集</h2><p>​  在早期阶段，深度聚类方法在相对较小的低维数据集上进行评估（例如COIL-20（Nene等，1996），YaleB（Georghiades等，2001））。近年来，随着深度聚类方法的快速发展，在更复杂和更具有挑战性的数据集上评估聚类性能变得越来越流行。有五个被广泛使用的基准数据集：</p><ul><li>CIFAR-10 (Krizhevsky et al, 2009)由来自10个不同类别的6万张彩色图像组成，包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。</li><li>CIFAR-100 (Krizhevsky et al, 2009)包含100个类，分为20个超类。每个图像都带有一个“细”类标签和一个“粗”超类标签。</li><li>STL-10 (Coates et al, 2011)包含来自10个对象类的13000张标记图像。此外，它还提供了10万张未标记图像用于自监督学习，以提高聚类性能。</li><li>ImageNet-10 (Chang et al, 2017)是ImageNet数据集的一个子集（Deng等人，2009）。它包含10个类，每个类都有1300张高分辨率图像。</li><li>ImageNet-Dog (Chang et al, 2017)是ImageNet的另一个子集。它由属于15个犬种的图像组成，适用于细粒度的聚类任务。</li></ul><p>​  除此之外，最近的一些工作采用了两个更具挑战性的大规模数据集，Tiny-ImageNet(Le and Yang, 2015)和ImageNet-1K (Deng et al,2009)，来评估其有效性和效率。表3总结了对这些数据集的简要描述。</p><h2 id="性能比较">4.3 性能比较</h2><p>​  在5个广泛使用的数据集上的聚类性能如表4所示。</p><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107225754511.png"alt="image-20250107225754511" /><figcaption aria-hidden="true">image-20250107225754511</figcaption></figure><p>​  由于深度神经网络的特征提取能力，早期基于结构和分布先验的深度聚类方法获得了比经典的K-means方法更好的性能。然后，一系列的对比聚类方法通过数据增强引入额外的先验，显著提高了性能。在此之后，更先进的方法通过进一步考虑邻域一致性（GCC与CC相比），并使用伪标签（SCAN与SCAN∗相比）来提高性能。值得注意的是，不同先验的性能收益是独立的。例如，ProPos通过分别利用增强不变性和伪标记先验，显著优于DEC和CC。最近，基于外部知识的方法取得了最先进的性能，证明了这是一种新的深度聚类范式的广阔前景。此外，当类别数量不断增长（从CIFAR-10到CIFAR-100）或语义变得更加复杂（从CIFAR-10到ImageNet-Dogs）时，聚类就变得更具挑战性。这些结果表明，更具挑战性的数据集，如完整的ImageNet-1K，有望在未来的工作中进行基准测试。</p><h1 id="在vicinagearth中的应用">5 在Vicinagearth中的应用</h1><p>​  在本节中，我们将探讨在Vicinagearth领域内深度聚类的一些典型应用，这是一个由“Vicinage”和“Earth”融合而成的术语。Vicinagearth代表了从海平面以下1000米（阳光停止穿透的深度）到海拔10000米（商用飞机的典型巡航高度）的关键空间高度。这个区域非常重要，因为它包括人类活动的核心区域，包括居住和生产地区。近年来，深度聚类已成为邻近地球内不可或缺的分析工具，有助于揭示邻近空间内数据的复杂模式和结构。深度聚类在该区域的多种应用包括异常检测、环境监测、社区检测、人员再识别等。<br/>​  <strong>异常检测，</strong>也被称为异常值检测（Comaniciu和Meer，2002）或新奇检测（Esteretal.，1996b），试图识别异常实例或模式。在Vicinagearth的背景下，深度聚类被证明是为了分析从不同来源获得的传感器数据，如水下监测系统、空中传感器或地面传感器（Chatterjee和Ahmed，2022）。通过对传感器数据的模式和典型行为的分析，系统能够检测异常，这些异常可能是安全威胁或不规则活动的信号。<br/>​  <strong>环境监测</strong>包括分析从环境传感器收集的数据（Xia和Vlajic，2007），如监测空气质量、水条件和地质因素。其主要目标是确保生态系统的健康（Wuetal，2016），并发现潜在的环境威胁，如污染事件或自然灾害。深度聚类技术在对相似的环境模式进行分组方面起着至关重要的作用，有助于对异常情况的识别。这一应用程序有助于实时环境监测（Kumaretal，2012），提高了及时应对环境挑战的能力。<br/>​  <strong>社区检测</strong>（Fortunato，2010；Jin等人，2021年）涉及到评估节点组是如何聚集或分割的，以及它们在网络中加强或分裂的趋势。在Vicinagearth的背景下，这项技术被用于识别密切相互作用或共享相似生态位的物种群（默多克和耶格尔，2011）。深度聚类在复杂生态网络的分析中起着关键作用（Montoyaetal，2006），有助于更深入地了解生态群落及其动态。<br/>​  <strong>人的再识别</strong>（Wu等人，2019；Ye等人，2021）是一项关键的任务，涉及到识别和匹配不同摄像机视图中的个体（Yangetal，2022a）。这项技术在公共安全和执法行动中发挥着重要作用，因为它有助于监测人口密集的地区，以将潜在的威胁或主题列入观察名单。深度聚类算法的集成显著提高了人的再识别系统的可伸缩性和效率（Yanetal，2023）。深度聚类有效地管理大量和动态变化的人群带来的复杂性。此外，深度聚类技术的适应性扩大了其应用范围，包括监测自然栖息地和跟踪在不同的和不受控制的环境中的野生动物。</p><h1 id="未来的挑战">6 未来的挑战</h1><p>​  虽然现有的工作取得了显著的性能，但一些实际的挑战和新出现的要求尚未得到充分解决。在本节中，我们将深入探讨现代深度集群的一些未来方向。</p><h2 id="细粒度聚类">6.1 细粒度聚类</h2><p>​  细粒度聚类的目的是识别数据中细微和复杂的变化，这在像识别生物亚种这样的研究中特别有利（Lietal，2023c，d）。主要的挑战是，细粒度的类表现出高度的相似性，其区别通常在于颜色、标记、形状或其他微妙的特征。在这种情况下，传统的粗粒度集群先验经常被证明是不够的。例如，在增强不变性之前的颜色和形状增强会变得无效。最近，C3-GAN（Kim和Ha，2021）在对抗性训练中使用对比学习来生成逼真的图像，能够细致入微地捕获细粒度的细节，并确保集群之间的可分离性。</p><h2 id="非参数聚类">6.2 非参数聚类</h2><p>​  许多集群方法通常需要预定义的和固定数量的集群。然而，真实世界的数据集经常对未知的集群数量的挑战，反映了更接近现实的情况。只有少数作品(Chen,2015; Shah and Koltun, 2018; Zhao et al, 2019;Wang et al, 2021)一直致力于解决这个问题。这些方法通常依赖于计算全局相似度，并引入巨大的计算成本，特别是在大规模数据集中。因此，有效地确定簇数C的最优值仍然是一个开放的挑战，通常涉及到人类先验的合并。在现有的工作中，DeepDPM引入了狄利克雷过程高斯混合模型（DPGMM，DirichletProcess Gaussian MixtureModels）（Antoniak，1974），利用狄利克雷过程作为混合组件的先验分布。DeepDPM通过MetropolisHastings框架(Hastings,1970)指导下的拆分和合并操作动态调整集群C的数量。</p><h2 id="公平聚类">6.3 公平聚类</h2><p>​  用不同的获取方法从不同的来源收集真实世界的数据集，可以增强机器学习模型的泛化性。然而，这些数据集经常表现出固有的偏见，特别是在敏感的属性上，如性别、种族和民族。这些偏差将导致个人和少数群体之间的差异，导致聚类划分偏离数据的潜在目标特征。在公正和公平的分析至关重要的应用程序中，如就业、医疗保健和教育，追求公平尤其相关。为了解决这一挑战，公平聚类试图减轻这些偏见的影响，因为每个样本的偏见属性。<br/>​  为了解决这一艰巨的任务，Chierichettietal首先引入了一种称为球流分解的数据预处理方法。最近的研究进展通过对抗性训练（Li等人，2020年）和互信息最大化（Zeng等人，2023年）在大规模数据上解决了这一问题。值得注意的是，Zeng等人设计了一种新的度量方法，从信息论的角度来评估聚类质量和公平性。尽管有这些发展，但仍有改进的空间，建立更好的评价指标是本研究的一个持续领域。</p><h2 id="多视图聚类">6.4 多视图聚类</h2><p>​  多视图数据（Xu etal，2013；Liu等人，2019b）在现实世界中很常见，即信息从各种传感器捕获或从多个角度观察。这些数据本身就很丰富，提供了多样化而又一致的信息。例如，RGB视图将提供颜色细节，而深度视图将显示空间信息，这表示视图的互补方面。同时，存在一种视图一致性级别，因为同一对象在不同的视图之间具有共同的属性。为了处理多视图数据，提出了多视图聚类（Denget al，2015；Liu etal，2019a）来利用互补和一致的特征。其目标是整合来自所有视图的信息，以产生一个统一的和深刻的聚类结果。<br/>​  近年来，几种深度学习方法（Andrew等，2013；Wang等，2016；赵等，2016；Peng等人，2019）旨在应对这一挑战。二进制多视图聚类Zhang等人（2018）同时细化了二进制聚类结构和离散数据表示，确保了内聚聚类。为了追求视图的一致性，Lin等人（2021,2022）最大化了跨视图的互信息，从而对齐了共同的属性。SURE（Yangetal，2022b）旨在通过利用鲁棒对比损失来加强视图之间共享特征的一致性。最近，Li等人（2023a）执行了约束对比损失，以在集群水平上保持视图互补。这些创新的方法表明了在多视图分析领域取得的重大进展，在那里，聚类在加强对多视图数据的协同利用方面继续发挥关键作用。</p><h1 id="结论">7 结论</h1><p>​  深度聚类或无监督学习的关键是寻求有效的监督来指导表示学习。与传统的网络结构或数据类型的分类法不同，本调查从先验知识的角度提供了一个全面的回顾。随着聚类技术的发展，出现了一种明显的趋势，从探索数据本身内部的先验转向诸如自然语言指导等外部知识。探索ChatGPT或GPT-4V（ision）等外部预训练模型可能成为一个很有前途的途径。这项调查可能提供了一些有价值的见解，并激发了对深度聚类的进一步探索和进步。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> K-means </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation</title>
      <link href="/Fuzzy-clustering-method-for-image-segmentation/"/>
      <url>/Fuzzy-clustering-method-for-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Pixel and region level information fusion in membership regularizedfuzzy clustering for image segmentation</p><p>发表于期刊Information Fusion 2023<br/><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241229221827412.png"alt="image-20241229221827412" /></p><h2 id="摘要">摘要：</h2><p>​  隶属度正则化模糊聚类方法应用了一个重要的先验，即相邻的数据点根据亲和度/相似度矩阵应该具有相似的隶属度。因此，它们在许多数据挖掘任务中都取得了良好的性能。然而，这些聚类方法在正则化过程中并没有充分利用图像空间信息。它们在图像分割问题上的性能仍然不理想。在本文中，我们首先关注于建立一个新的亲和矩阵来存储和呈现图像空间信息，以帮助成员正则化模糊聚类方法获得良好的分割结果。为此，通过融合像素级和区域级信息来计算亲和度值，以表示图像中两点之间的微妙关系。此外，为了减少图像噪声的影响，我们在算法的迭代中使用了固定的聚类中心，因此，隶属度值的更新仅以融合信息的先验为指导。在合成图像数据集和真实图像数据集上的实验结果表明，该方法比现有的聚类方法具有更好的分割效果。</p><h2 id="介绍">1.介绍</h2><p>​  图像分割是图像分析中的关键环节，已广泛应用于目标识别[1]、医学诊断支持系统[2]、工业过程[3]等计算机视觉任务[4,5]。它是一种将图像分割成几个具有独特属性的特定区域的技术，可以看作是一个像素聚类过程。许多关于图像分割的方法已经被提出，如基于超像素的方法[6]、基于神经网络[7,8]的监督方法、聚类[9,10]等。虽然这些技术在一定程度上对图像分割取得了良好的效果，但对于不同的图像都不具有足够的鲁棒性和效率。例如，基于超像素的方法对复杂图像不能很好地保留细节，基于神经网络的监督方法需要大量的训练样本和标记图像，而且分割结果具有粗糙的轮廓。本文主要研究了基于模糊聚类的无监督分割方法。<br/>​  在实践中，聚类由于其有效性，是图像分割中最常用和最重要的方法之一。在图像分割的聚类算法方面也有许多进展。提出了基于K-means聚类算法和一种快进量子优化算法的FFQOAK[11]方法。这是一种硬划分方法，只承认像素属于单个簇/段。此外，Pritpal等人[12]提出了一种基于模糊集理论和相关概念的模糊d-均值融合聚类算法。模糊集的固有模糊模型具有四个固定程度的成员关系，如真、假、真模糊和假模糊。在各种聚类方法中，模糊聚类在处理图像分割[13]的模糊特性中的有效性和鲁棒性得到了广泛的研究和应用。模糊c-均值（FCM）是一种基于模糊集理论的经典聚类算法，最初由Dunn[14]提出，后来由Bezdek[15]进行改进。模糊集理论不同于模糊集理论。每个数据的隶属度始终属于范围[0,1]。此外，模糊聚类是一种软分区方法，它允许数据点在所有聚类中都具有成员资格，在处理图像[16]中的边缘信息时比硬分区方法效果更好。但传统的模糊聚类方法在处理有噪声或其他伪影的图像时往往效果较差，因此如何在复杂图像上获得准确的分割结果仍然是一项具有挑战性的任务[17]。<br/>​  为了克服FCM的缺点，对距离测度的重新设计进行了重视和探索，因此，提出了一些核诱导的FCM变量[18,19]，这种核诱导的距离测度被视为非线性数据挖掘和图像分割的一个强大的形式工具。为了解决普通FCM对模糊指数值变化敏感的问题，宫本等人开发了最大熵FCM[20]，而且，两个[21,22]都引入了相对熵来改进其目标函数。近年来，为了解决不准确的分类信息或图像分割受离群值的影响，一些算法基于模糊聚类和信息融合[23–25]提出，如Musaet al.[26]试图提出一个基于模糊聚类集成聚合器的基础集群和实现成功的性能、速度和鲁棒性。<br/>​  事实上，在模糊聚类过程中涉及到一些利用空间信息的技术，并对处理有噪声的图像分割任务[27]有积极的影响。例如，Guo等人[28]开发了一种基于噪声检测（NDFCM）的自适应图像分割FCM，其中采用两种图像滤波方法来抑制噪声，并使用一个参数，通过测量每个邻域的灰度水平的方差来保持图像细节。为了实现鲁棒的分割结果与低执行时间，Lei等[29]开发了一个快速、鲁棒的模糊c均值聚类算法（FRFCM）基于平方社区空间水平信息，采用形态重建和会员滤波平滑图像详细保存，降低计算成本。Zhang等[30]在考虑空间邻域信息的基础上，考虑了测量值与理论值之间的偏差，提出了偏差-稀疏模糊c-均值（DSFCM_N）来提高图像分割的聚类性能。毕竟，研究人员正专注于使用越来越复杂的基于空间信息的技术来解决复杂的图像分割问题，但他们忽略了图像中不同层次的空间信息的使用，如区域级[31]的空间信息。<br/>​  近年来，在多视图数据集[32]和真实世界数据库[33]上，提出了许多针对不同类型数据挖掘任务的成员正则化模糊聚类方法。RSSFCA[34]涉及高斯度量下的正则化，将其集成到模糊聚类算法的目标函数中，以获得稀疏的隶属度，以减少噪声特征的比例。Chen等人[35]提出了一种基于空间信息构造的图的正则化FCM。[36]等人提出了一种成员变异正则化方法来修改FCM，用于具有噪声和不完整数据的图像分割。这些方法中的正则化是试图充分利用最终反对函数中对数据的先验知识来指导聚类处理[37,38]。然而，这些最近提出的隶属度正则化模糊聚类方法[39,40]大多用于数据挖掘，而不将空间信息作为先验知识，这对图像分割问题很有价值。更广泛地说，成员正则化模糊聚类的亲和性不适用于图像分割任务，存储在亲和矩阵中的先验知识不能充分反映图像像素点之间的相似性。换句话说，这些隶属度正则化模糊聚类方法需要改进，采用一种新的适当的亲和/相似矩阵获取图像分割方法来考虑数据的空间结构，这是图像数据[37,41]的主要特征。<br/>​  一般来说，亲和矩阵是一种基本的统计技术，用于度量一组数据点中的相似性或关系。它存储并呈现了关于数据内在结构的先验知识。该矩阵可以用一些半监督学习方法[42,43]得到，也可以用一些光谱聚类方法[44,45]构造。它在数据挖掘问题上工作得很好。例如，模糊聚类获得的会员可以被视为一个新的表示原始数据，然后数据点彼此应该拥有类似的会员，并基于这个角度，郭etal. [46]探索成员亲和力套索项（MAL）改进模糊聚类方法数据挖掘，命名MALFCMFCM的增强版本。实际上，对于图像分割问题，图像中两个像素点的亲和度值应该同时反映像素级和区域级上的相似性。这意味着在同一集群中分配的附近的两个点同时具有空间一致性和相似的灰度值。因此，在此基础上，本文以MALFCM为例，提出了一种新的隶属度正则化模糊聚类亲和矩阵，以提高其在图像分割上的性能。<br/>​  简而言之，虽然人们为提高图像分割结果付出了大量的努力，但现有的算法主要存在以下限制和挑战。</p><p>（1）传统的模糊聚类方法只采用单一空间信息来提高其对图像分割的性能。基于聚类的图像信息是利用不同层次图像分割模型的关键。<br/>（2）在这些所讨论的模型中，由于隶属度正则化方法的亲和矩阵中存储的先验知识，不能充分反映图像像素点之间的相似性。</p><p>​  在本研究中，为了解决上述挑战，我们提出了一种基于信息融合技术的模糊聚类方法。在该方法中，提出了一种新的亲和矩阵来存储和呈现关于图像的先验知识。此外，为了抑制噪声和其他异常值的影响，我们使用了一个同时考虑像素的空间信息和灰度信息的加权距离。具体来说，在该方法中，像素之间的亲和度受到两层空间约束的影响：像素级近邻域和区域级空间上下文，这是通过简单线性迭代聚类（SLIC）[6]、均值位移[47]等超像素方法获得的。与基于超像素和其他基于模糊的方法相比，一个像素的标记会受到上述两种信息的影响。通过融合这两个层次的信息，提出基于信息融合的方法将缩小同一区域像素之间的差异，减少像素的相似性属于不同地区，即使其中一些被噪声和其他异常值，同时保持图像细节。此外，预计成员关系的更新仅受融合信息的指导，因此我们在合理的迭代处理中使用固定的聚类中心进行初始化。基于此融合信息，该方法利用更多的空间数据先验知识，指导聚类过程，找到更准确和合理的分割结果。</p><p>​  我们的主要贡献可以总结如下：</p><ul><li>新的亲和矩阵：提出了一种亲和矩阵<spanclass="math inline">\(A^{SR}\)</span>，并通过融合像素和区域级信息进行计算，改进了图像分割的隶属度正则化模糊聚类方法。这种更精确的先验知识，存储在亲和矩阵中，反映了图像中两点之间的微妙关系。</li><li>基于信息融合的新模型：在新的亲和矩阵的基础上，利用加权距离融合了像素的空间信息和灰度信息。因此，提出了一种改进的成员正则化模糊聚类方法，并命名为<spanclass="math inline">\(A^{SR}MF\)</span>。</li><li>使用固定聚类中心的策略：仅凭简单的直觉，在亲和矩阵中存储的融合信息的指导下，可以得到合理的分割结果。因此，我们采用了使用固定的聚类中心，利用新的亲和矩阵通过正则化项目更新隶属度值的策略，即先验知识将在分割任务中发挥更大的作用。</li></ul><p>​  本文的其余部分组织如下。在第二节中，我们将描述与我们的研究相关的工作的概述。在第3节中，所提出的方法在这里显示。实验结果报告见第4节。最后，我们在第5节中作出结论。</p><h2 id="前期准备工作">2.前期准备工作</h2><p>​  在本节中，我们将简要回顾一下与我们的研究相关的工作，包括传统的图像分割的模糊聚类、超像素技术和一些隶属度正则化模糊聚类方法。</p><h3 id="图像分割的模糊聚类方法">2.1.图像分割的模糊聚类方法</h3><p>​  对于图像分割，模糊聚类使用模糊隶属关系将数据/像素点分配给每个段/区域。我们以经典的FCM模型为例来说明它。给定一个图像<spanclass="math inline">\(X=\{X_{1},X_{2},\dots,X_{N}\}\)</span>，FCM通过迭代最小化如下定义的函数，将𝑁个像素点划分为𝐶个簇/段：<spanclass="math display">\[J_{\mathrm{FCM}}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}\vert\vert X_{i}-P_{j}\vert\vert^{2}\]</span> ​  限制： <spanclass="math display">\[\sum_{j=1}^{C}u_{i j}=1,i=1,\ldots,N\]</span>​  其中<spanclass="math inline">\(m&gt;1\)</span>是控制会员模糊性的模糊因子，<spanclass="math inline">\(0\,\leq\,u_{ij}\,\leq\,1\)</span>，𝑃𝑗表示𝑗th集群的原型/中心值，𝑢𝑖𝑗是𝑖th数据点对𝑗th集群的会员值，FCM采用拉格朗日乘数法更新会员<spanclass="math inline">\(U(u_{i j})\)</span>和集群中心<spanclass="math inline">\(P(P_{i})\)</span>，如下 <spanclass="math display">\[u_{i j}={\frac{(\lVertX_{i}-P_{j}\rVert)^{-2/(m-1)}}{\sum_{j=1}^{C}(\lVertX_{i}-P_{j}\rVert)^{-2/(m-1)}}}\]</span></p><p><span class="math display">\[P_{j}=\frac{\sum_{i=1}^{N}u_{ij}^{m}X_{i}}{\sum_{i=1}^{N}u_{i j}^{m}}    \]</span></p><p>​  当函数收敛或达到最大迭代次数时，此更新过程就会结束。<br/>​  然而，正如我们从FCM的目标函数中可以看到的，它没有考虑任何关于数据的空间关系的信息，这使得传统的FCM对噪声和其他干扰很敏感。将空间信息纳入目标函数是缓解FCM[48,49]弱点的最常用的尝试之一。Ahmed等人[50]提出了具有空间约束的FCM_S，以便允许一个像素的标记受到其邻域标签的影响。目标函数：<spanclass="math display">\[J_{\mathrm{FCM}_{-}S}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}(\|X_{i}-P_{j}\|^{2}+\frac{\alpha}{N_{R}}\sum_{k\inN_{i}}\|X_{k}-P_{j}\|^{2})\]</span> ​  其中，<spanclass="math inline">\(𝑁_𝑖\)</span>代表落入<spanclass="math inline">\(X_i\)</span>周围窗口的邻居集，<spanclass="math inline">\(N_R\)</span>为其基数，<spanclass="math inline">\(\alpha\)</span>控制空间项的影响。按照同样的思路，在[51]中提出了FCM_S的两个低复杂度变量，FCM_S1和FCM_S2，他们分别采用均值滤波图像和中值滤波图像来获取空间邻域信息。因此，简化的目标函数可以改写为<spanclass="math display">\[J_{\mathrm{FCM}_{S}S1/S2}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}(||X_{i}-P_{j}||^{2}+\alpha\|\bar{X}_{i}-P_{j}||^{2})\]</span>​  其中<span class="math inline">\(\bar{X}_{i}\)</span>为<spanclass="math inline">\(X_i\)</span>附近指定窗口内邻居的平均值或中值。遵循这一趋势，研究人员专注于使用越来越复杂的基于空间的技术来解决复杂的图像分割问题，但他们忽略了图像中不同层次的空间信息的使用，如基于超像素技术的区域层次。</p><h3 id="基于超像素的图像分割方法">2.2.基于超像素的图像分割方法</h3><p>​  超像素算法根据像素的位置关系、颜色、纹理等特征将像素划分为有意义的区域。它们在捕获图像冗余方面具有很大的优势，大大降低了后续图像处理任务的复杂性，并广泛应用于图像分割应用。生成超像素的算法有很多，如平均位移算法[47]、多尺度形态梯度重建（MMGR）[52]和简单线性迭代聚类（SLIC）[6]。<br/>​  平均位移是一种基于密度估计的方法，假设不同簇的像素符合不同的概率分布，迭代计算并更新像素的平均位移向量，生成超像素区域。<br/>​  MMGR采用不同的结构元素获得多个重建图像，然后将这些重建的梯度图像融合，获得超像素图像。<br/>​  SLIC采用k-means，利用颜色和空间信息对CIELAB局部颜色空间中的像素进行聚类。在SLIC中的距离测度被表示为<span class="math display">\[d_{l ab}=\sqrt{(l_{j}-l_{i})^{2}+(a_{j}-a_{i})^{2}+(b_{j}-b_{i})^{2}}\]</span></p><p><spanclass="math display">\[d_{s}={\sqrt{(x_{j}-x_{i})^{2}+(y_{j}-y_{i})^{2}}}\]</span></p><p><span class="math display">\[d(i,c)=\sqrt{(d_{l ab})^{2}+(\frac{d_{s}}{S})^{2}m_{s}^{2}}\]</span></p><p>​  其中<spanclass="math inline">\(d_{lab}\)</span>代表两个像素之间的颜色空间差异，<spanclass="math inline">\([l,a,b]^T\)</span>是CIELAB颜色空间中像素颜色的表示，<spanclass="math inline">\(𝑑_𝑠\)</span>是两个像素之间的空间距离，<spanclass="math inline">\([x,y]^T\)</span>指像素的空间位置，<spanclass="math inline">\(d(i,c)\)</span>是一个加权距离，用于测量第<spanclass="math inline">\(i\)</span>个像素和第<spanclass="math inline">\(c\)</span>个聚类中心之间的距离，<spanclass="math inline">\(m_s\)</span>是用于平衡<spanclass="math inline">\(d(i,c)\)</span>中颜色和空间信息的参数，<spanclass="math inline">\(\mathbf{S}\times\mathbf{S}\)</span>为所需超像素的大小，其中<spanclass="math inline">\(S={\sqrt{N/C_{s}}}\)</span>，𝑁为图像像素的数量，<spanclass="math inline">\(𝐶_𝑠\)</span>为超像素的数量。<br/>​  在Liu的论文[27]中，提出了一种基于均值移算法自适应局部信息的模糊聚类方法。目标函数被定义为<span class="math display">\[J_{L iu^{\prime}s}=\sum_{i=1}^{C}\sum_{j=1}^{N}u_{i j}D_{ij}^{L}+\lambda\sum_{i=1}^{C}\sum_{j=1}^{N}u_{i j}\log(\frac{u_{ij}}{\pi_{i j}})\]</span> ​  其中，𝜆是控制簇模糊性的超参数，<spanclass="math inline">\(𝐷_{𝑖𝑗}^𝐿\)</span>是结合像素级和区域级差异的距离函数，如下所示<span class="math display">\[D_{i j}^{L}={\frac{d_{ij}^{L}+d_{R_{i},j}^{L}}{2}}\]</span> ​  这里，<spanclass="math inline">\(𝑑_{𝑖𝑗}^𝐿\)</span>是𝑗th像素与𝑖th聚类中心之间的像素级距离，<spanclass="math inline">\(d_{R_{i},j}^{L}\)</span>是由均值位移算法得到的区域<spanclass="math inline">\(𝑅_𝑗\)</span>值与𝑖th聚类中心之间的区域级距离，<spanclass="math inline">\(𝑅_𝑗\)</span>是𝑗th像素所属的区域。另外，<spanclass="math inline">\(\pi_{i j}\)</span>是先验概率函数。<br/>​  在SFFCM[52]中，提出了另一种基于超像素的模糊c均值聚类，在多尺度形态梯度重建（MMGR）得到的超像素图像上实现直方图参数的FCM。因此，SFFCM利用区域级空间信息，在真实图像分割问题上具有良好的性能。</p><h3 id="隶属度正则化模糊聚类方法">2.3.隶属度正则化模糊聚类方法</h3><p>​  隶属度正则化模糊聚类是试图充分利用最终反对函数中对数据的先验知识来改进聚类处理。先验知识也以不同的形式呈现出来。Pedrycz[53]认为，模糊聚类获得的隶属关系是原始数据的新表示，换句话说，相邻的点应该具有相似的隶属关系。基于这一观点，并受到网络套索的启发，Guo等人[46]探索了一个新的成员亲和套索术语，并将该术语添加到FCM的目标函数中，名为MALFCM。MALFCM对应的目标函数定义如下<spanclass="math display">\[J_{\mathrm{MAL}}=\sum_{i=1}^{C}\sum_{j=1}^{N}u_{ij}^{m}||X_{j}-P_{i}||^{2}+\frac{\lambda}{2}\sum_{j=1}^{N}\sum_{k=1}^{N}\omega_{jk}|u_{i j}-u_{i k}|\]</span>​  其中，等式的第二项（12）是隶属度亲和度套索正则化，𝜆是一个折衷参数，<spanclass="math inline">\(\omega_{j k}\)</span>是点<spanclass="math inline">\(X_{j}\)</span>和点<spanclass="math inline">\(X_{k}\)</span>之间的亲和度。采用乘子交替方向法（ADMM），使上述函数最小化。<br/>​  引入隶属度正则化，隶属度亲和lasso，是为了确保附近数据的隶属度接近，所以MALFCM可以通过从原始数据中建立良好的亲和矩阵得到良好的分类结果。显然，反映数据点之间关系的矩阵在隶属度正则化聚类方法中具有重要意义。然而，在这些聚类方法中构建的亲和矩阵是用于数据挖掘的，并没有充分利用具有空间结构的数据。它们在图像分割问题上的性能仍然不理想。本文在不同层次图像信息融合的基础上，提出了一种新的隶属度正则化模糊聚类亲和矩阵，并给出了一种新的图像分割模型，并给出了一种求解的算法。</p><h1 id="提出的方法mathbfasrmathbfmf">3.提出的方法：<spanclass="math inline">\(\mathbf{A}^{SR}\mathbf{MF}\)</span></h1><p>​  在本研究中，基于上述分析，我们旨在利用像素级邻域和区域级上下文的融合信息来生成亲和矩阵，如图1所示。</p><figure><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241230161302607.png"alt="image-20241230161302607" /><figcaption aria-hidden="true">image-20241230161302607</figcaption></figure><p>​  以MALFCM为例，解释了所提出的亲和力<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>在图像分割任务上，我们还引入了一种新的MALFCM距离测量，考虑了像素的空间和灰度信息。此外，我们采用了在迭代过程中使用固定的簇中心的策略，即使用新的亲和矩阵只通过ADMM更新隶属度值。我们将修改后的方法命名为ASRMF。ASRMF的主要框架如图2所示。</p><figure><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241230161435927.png"alt="image-20241230161435927" /><figcaption aria-hidden="true">image-20241230161435927</figcaption></figure><h2id="基于像素和区域级信息的融合构建亲和矩阵">3.1.基于像素和区域级信息的融合构建亲和矩阵</h2><h2 id="隶属度正则化的模糊聚类模型">3.2.隶属度正则化的模糊聚类模型</h2><h2 id="参数估计">3.3.参数估计</h2><hr /><p>算法1 <spanclass="math inline">\(\mathbf{A}^{SR}\mathbf{MF}\)</span></p><hr /><p><strong>输入：</strong>图像<spanclass="math inline">\(\mathbf{X}(x_i,i=1,2,\cdot\cdot\cdot,N)\)</span>和聚类中心<spanclass="math inline">\(\mathbf{V}\)</span>；正则化参数𝜑、终止准则𝜀和最大迭代次数𝑡𝑚𝑎𝑥；<br/><strong>输出：</strong>成员关系矩阵𝑈。<br/>1.通过SLIC算法获得超像素图像，并计算不同区域的平均值；<br/>2.初始化辅助变量𝑐和𝑞，并设置迭代数𝑡=0；<br/>3.通过输入的簇中心得到隶属度划分矩阵𝑈；<br/>4.通过等式13、等式14、等式15推导出亲和矩阵<spanclass="math inline">\(\mathbf A^{S R}(A_{j k}^{SR})\)</span>;<br/>5.通过等式17计算每个像素对每个簇<spanclass="math inline">\(\mathbf D(D_{ij})\)</span>的归属;<br/>6.<strong>repeat</strong><br/>7 Let t = t +1;<br/>8 通过等式26更新成员资格矩阵<span class="math inline">\(\mathbf𝑈\)</span>;<br/>9 通过等式30更新辅助变量𝑐;<br/>10通过等式32更新辅助变量q;<br/>11.直到<span class="math inline">\(t\gtt_{m a x}\)</span>或者<spanclass="math inline">\(\|U^{t+1}-U^{t}\|\leq\varepsilon\)</span>;</p><hr /><h1 id="实验结果">4.实验结果</h1><h2 id="参数设置">4.1.参数设置</h2><h2 id="评价指标">4.2.评价指标</h2><h2 id="使用固定的集群中心的合理性">4.3.使用固定的集群中心的合理性</h2><h2 id="对合成图像的分析结果">4.4.对合成图像的分析结果</h2><h2 id="对医学图像的检测结果">4.5.对医学图像的检测结果</h2><h2 id="对真实图像的结果">4.6.对真实图像的结果</h2><h1 id="运行时间分析">5.运行时间分析</h1><h1 id="结论及未来的工作">6.结论及未来的工作</h1><p>​  本文首先创新地融合了像素级邻域信息和区域级上下文信息两种图像信息，构建了隶属度正则化聚类方法的亲和矩阵<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>，以提高其在图像分割上的性能。然后，在聚类过程中采用固定聚类中心的策略；分割结果仅依赖于亲和矩阵中存储的先验知识，可以减少异常值或噪声像素对隶属度计算的负面影响。以隶属度亲和套索为例，提出了基于信息融合的ASRMF方法，它利用新的亲和矩阵<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>来指导隶属度值的更新。实验结果表明，该方法ASRMF能够在不同类型的图像分割中取得良好的效果。它的性能优于其他最先进的算法。<br/>​  本文只考虑单通道图像，而多通道图像也是一个重要的研究趋势。因此，在我们的未来工作中，我们将研究多通道和单通道融合范式[65]，以创建更好的亲和矩阵。然而，如何自动设置ASRMF的相关参数以及聚类的数量仍是一个有待进一步研究的问题。此外，如何在其他聚类方法中使用该亲和矩阵，如光谱聚类[66]，也是我们需要研究和改进的一个问题。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 模糊聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection</title>
      <link href="/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/"/>
      <url>/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/</url>
      
        <content type="html"><![CDATA[<p>Forgery-aware Adaptive Learning with Vision Transformer forGeneralized Face Forgery Detection</p><p>(发表于TCSVT，IEEE Transactions on Circuits and Systems for VideoTechnology， 1区B刊)</p><h1 id="摘要">摘要</h1><p>​  随着生成模型的快速发展，目前人脸伪造检测面临的挑战是如何有效地检测来自不同未知领域的真实操纵人脸。虽然以往的研究表明，经过预训练的基于视觉变换器（ViT）的模型在对深度伪数据集进行完全微调后，可以获得一些有希望的结果，但其泛化性能仍不令人满意。为此，我们提出了一种在自适应学习范式下的伪造感知自适应视觉Transformer网络（FAViT），其中预先训练的ViT网络中的参数保持不变，而设计的自适应模块进行优化以捕获伪造特征。<br/>​  具体来说，设计了一个全局自适应模块来模拟输入令牌之间的长期交互，利用自注意机制来挖掘全局伪造线索。为了进一步探索必要的局部伪造线索，提出了一种局部自适应模块，通过增强局部上下文关联来暴露局部不一致性。此外，我们还引入了一个细粒度的自适应学习模块，该模块强调通过细粒度对的关系学习对真实面孔的共同紧凑表示，驱动这些提出的自适应模块感知细粒度的伪造感知信息。<br/>​  大量的实验表明，我们的FA-ViT在交叉数据集评估中取得了最先进的结果，并增强了对看不见扰动的鲁棒性。特别是在FA-ViT的跨数据集评估中，Celeb-DF和DFDC数据集的AUC得分分别达到93.83%和78.32%。该代码和训练过的模型已经在：https://github.com/LoveSiameseCat/FAViT 上发布了。</p><p>索引术语-人脸伪造检测，视觉转换器，自适应学习，泛化性能。</p><h1 id="i.介绍">I.介绍</h1><p>​  随着深度学习技术的快速发展，人工智能生成内容（AIGC）技术在多种多媒体任务中取得了重大进展。然而，这一进步对人眼辨别这些数字内容提出了巨大的挑战。特别是，攻击者可以很容易地为各种恶意目的生成伪造的面部内容（又名Deepfakes），对社会构成金融欺诈、政治冲突和冒充的紧迫威胁。<br/>​  以往的大多数工作都使用卷积神经网络（CNN）来构建检测器，其中初始[1]或效率网络[2]由于其出色的深度伪检测性能而被广泛应用作基本主干。为了提高其普遍性，一些作品探索了隐藏在被操纵面孔中的常见的伪造线索，如噪声信息[3]-[5]、混合伪影[6]-[8]、频率特征[9]-[11]等。然而，cnn中有限的接受域限制了它们全面学习更广义的特征[12]的能力。作为回应，一些方法寻求使用视觉变压器（ViT）进行人脸伪造检测[13]-[16]。由于自注意机制，这些基于vit的方法可以模拟不同输入标记之间的长程关系。然而，ViT难以捕获局部特征细节，这在深度伪造检测[17]中尤为重要。为了解决这一限制，之前的工作将CNN本地先验纳入到ViT架构[18]，[19]中。<br/>​  与从头开始训练的模型[20]相比，预先训练的模型在下游任务中表现出更好的收敛性和泛化性，并且预先训练的ViT1在取证任务[21]中已被证明是有效的。因此，在以前的工作[13]，[17]-[19]中，通常使用公共可访问的预先训练的权重来初始化基于vit的检测器，然后在深度假数据集上更新这些参数。然而，最近的工作[22]，[23]指出，基于vit的模型对特定的下游任务进行完全微调会破坏预先训练的特征，并可能过度拟合特定的数据模式[21]，可能会阻碍它们在开放集环境中的泛化能力。另一方面，如图1所示，被篡改人脸上的伪造伪影会导致全局或局部的不一致，这表明需要从多个角度对关键表示进行建模。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226110804711.png"alt="image-20241226110804711" /><figcaption aria-hidden="true">image-20241226110804711</figcaption></figure><p>​  基于这些观察结果，我们提出了用于广义人脸伪造检测的伪造感知自适应视觉Transformer网络（FA-ViT），其中预先训练的权值是固定的，只有设计的自适应模块被优化，以从全局和局部的角度捕获丰富的伪造感知信息。<br/>​  FA-ViT的概述如图2所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226110937373.png"alt="image-20241226110937373" /><figcaption aria-hidden="true">image-20241226110937373</figcaption></figure><p>​  它主要由两个模块组成：全局自适应模块（GAM，Global AdaptiveModule）和局部自适应模块（LAM， Local AdaptiveModule），以用于捕获全局和局部伪造感知信息。具体来说，GAM设计来为接收ViT令牌，并与自注意层的原始查询、键和值输出进行交互。<br/>​  具体来说，GAM被设计为接收ViT令牌，并与自注意层的原始查询、键和值输出进行交互。考虑到全局伪影跨越了操纵面部的大区域，GAM利用自我注意机制来增强其从这些伪影中学习全局取证线索的能力。<br/>​  另一方面，LAM采用二次编码[24]来增强每个ViT标记与其相邻空间信息之间的局部关联，指导模型强调局部不一致性之间的异常情况。因此，全局和局部伪造感知信息共同适应固定的预训练特征，形成用于在各种场景中检测深度伪造的广义法医表示。<br/>​  此外，常用的交叉熵损失强调类别水平的差异，但难以捕获细粒度信息，揭示操纵和真实面孔之间的细微差别[10]，[25]，[26]的差异。为了解决这一问题，我们设计了细粒度自适应学习（FAL）。如图2所示，将具有相似视觉语义内容但属于不同类别的细粒度对作为输入对进行分组。FAL利用来自最后一个完全连接（FC）层的权重作为真实面孔的代理原型，并通过圆损失[27]来规范原型和每个细粒度对之间的关系。在FAL的指导下，将所提出的自适应模块挖掘更细粒度的伪造感知信息，在特征空间中压缩真实面，从而进一步提高模型的通用性。<br/>​  我们的主要贡献总结如下：</p><ul><li>我们观察到，当对深度假检测的任务进行完全微调时，基于vit的模型难以推广到看不见的数据集。为了解决这一问题，我们提出了一种新型的伪造感知自适应视觉Transformer网络（FAViT），用于自适应学习范式下的广义人脸伪造检测。</li><li>我们提出了全局自适应模块（GAM）和局部自适应模块（LAM），它们有效地使全局和局部伪造感知信息适应于预先训练的ViT特征。此外，我们设计了一种新的线粒度自适应学习（FAL）来指导这些自适应模型在自适应学习过程中捕获细粒度信息。</li><li>我们在多个数据集和传感器上进行了广泛的实验，结果表明，我们提出的FA-ViT在各种评估中优于最先进的方法。</li></ul><h1 id="ii.-相关工作">II. 相关工作</h1><h3 id="a.面部伪造检测">A.面部伪造检测</h3><h3 id="b.模型适应">B.模型适应</h3><h3 id="c.-细粒度的信息学习">C. 细粒度的信息学习</h3><h1 id="iii.-提出的方法">III. 提出的方法</h1><p>​  在本节中，我们首先在第III-A节中概述了所提出的FA-ViT。三。然后，我们在章节III-B和III-C中描述了全局自适应模块（GAM）和局部自适应模块（LAM）。最后，在第III-D节中提出了细粒度自适应学习（FAL）。</p><h2 id="a.-概述">A. 概述</h2><p>​  我们提出的FA-ViT的框架如图2所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226145840044.png"alt="image-20241226145840044" /><figcaption aria-hidden="true">image-20241226145840044</figcaption></figure><p>​  它采用预先训练好的ViT为基本骨干，由12个ViT块组成。每个块由一个自我注意（SA）层和一个多层感知器（MLP）层组成。在训练过程中，这些块内的参数保持冻结。<br/>​  在FA-ViT中，输入的<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{H\timesW\times3}\)</span>被划分为L个图像补丁，并进一步处理成D维标记，记为<spanclass="math inline">\({\bf X}_{v i t}\ \in\ \mathbb{R}^{L\timesD}\)</span>。在每个ViT块中，我们在SA层中插入一个GAM。当<spanclass="math inline">\({\bf X}_{v it}\)</span>通过SA层时，GAM借助自我注意机制捕获全局伪造感知信息。另一方面，我们从<spanclass="math inline">\({\bfX}\)</span>中提取多尺度空间特征，其中每个尺度特征<spanclass="math inline">\({\bf X}_{c nn}\)</span>都是从一个由三个卷积层组成的CNN块中获得的。LAM为每个ViT标记聚合了来自<spanclass="math inline">\({\bf X}_{c nn}\)</span>的空间伪造感知信息，这有助于以自适应学习的方式捕获丰富的局部细节。除了常用的交叉熵（CE）损失外，我们还引入了FAL来指导GAMs和LAMs来捕获更细粒度的伪造感知信息。</p><h2 id="b.-全局自适应模块gam">B. 全局自适应模块GAM</h2><p>​  自注意层是ViT中的一个关键组件，它使每个输入令牌能够聚合来自所有其他令牌的信息。我们首先简要介绍了自注意层的计算过程。<br/>​  表示<spanclass="math inline">\(\mathbf{X}_{v i t}^{i n}\in\mathbb{R}^{L\timesD}\)</span>是ViT块的输入，首先通过三个可学习矩阵<spanclass="math inline">\(W_{Q}\in\mathbb{R}^{D\times D}\)</span>，<spanclass="math inline">\(W_{K}\in\mathbb{R}^{D\times D}\)</span>和<spanclass="math inline">\(W_{V}\in\mathbb{R}^{D\timesD}\)</span>，预测查询令牌<span class="math inline">\({\bfQ}\in\mathbb{R}^{L\times D}\)</span>，关键令牌<spanclass="math inline">\({\bf K}\in\mathbb{R}^{L\timesD}\)</span>和值令牌<span class="math inline">\({\bfV}\in\mathbb{R}^{L\times D}\)</span>： <span class="math display">\[{\bfQ}={\bf X}_{v i t}^{i n}W_{Q},\ \ {\bf K}={\bf X}_{v i t}^{i n}W_{K},\ \{\bf V}={\bf X}_{v i t}^{i n}W_{V}.\]</span>​  然后将自注意层的计算表示为： <span class="math display">\[{\bf X}_{v it}^{o u t}=\mathrm{Attention}({\bf Q},{\bf K},{\bfV})=\mathrm{softmax}(\mathbf{QK}^{\mathsf{T}}/{\sqrt{D}})\mathbf{V},..\]</span>​  其中，<span class="math inline">\({\bf X}_{v i t}^{o ut}\)</span>是输出。<br/>​  所提出的GAM建立在自注意层之上，利用自注意机制挖掘全局信息。先前的工作[18]，[61]，[62]已经证明了标记嵌入（即块表示）表现出比与遥远标记的邻近标记更强的相关性。因此，采用具有瓶颈结构的卷积层在GAM中建模这种局部空间关系，在全局自适应学习过程中引入与标记相关的局部先验。我们提出的GAM的细节如图3所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226152740847.png"alt="image-20241226152740847" /><figcaption aria-hidden="true">image-20241226152740847</figcaption></figure><p>​  具体来说，首先将<span class="math inline">\(\mathbf{X}_{v i t}^{in}\)</span>根据其原始的空间位置重塑为<spanclass="math inline">\(\sqrt{L}\times\sqrt{L}\timesD\)</span>的形状。然后应用1×1卷积层来降维，然后使用3×3卷积层来捕获令牌级依赖。最后，GAM通过三种不同的1×1卷积生成Q、K和V的全局自适应信息。此过程的表述如下：<span class="math display">\[\triangle{\bf Q},\triangle{\bfK},\triangle{\bfV}=\mathrm{Conv}_{1\times1}^{Q,K,V}(\mathrm{Conv}_{3\times3}(\mathrm{Conv}_{1\times1}({\bfX}_{w i t}^{i n}))),\]</span></p><p><span class="math display">\[{\bf X}_{v i t}^{o ut}=\mathrm{Atention}(\bf Q+\triangle Q,{\bf K}+\triangle\bf K,{\bfV}+\triangle{\bf V}),\]</span></p><p>​  其中<span class="math inline">\(\triangle{\bf Q},\triangle{\bfK},\triangle{\bfV}\)</span>是原始Q、K和V的自适应信息。由于原始信息和自适应信息融合在一起，并随后通过自我注意操作进行处理，这确保了GAM与所有令牌交互，从而从全局的角度捕获伪造感知特征。值得注意的是，<spanclass="math inline">\(\mathrm{Conv}_{1\times1}^{Q,K,V}\)</span>中的参数被初始化为零，这有助于全局伪造感知知识的稳定学习。</p><h2 id="c.-局部自适应模块lam">C. 局部自适应模块LAM</h2><p>​  纯ViT处理通过堆叠的线性层的输入，这很难捕获对检测被操纵的面孔至关重要的局部细节。如图1所示，被操纵面中的伪影往往会引入局部不一致，说明每个查询令牌必须强调其与周围位置的关系，以检测局部伪造线索。以前的方法使用交叉注意模块[14]、[15]、[63]或加法操作[17]、[64]将局部空间信息注入到类似vit的架构中，但经常忽略了每个查询令牌的上下文重要性。为了缓解这一问题，我们提出的LAM旨在强调自适应学习过程中的上下文信息，从而有效地从局部不一致中捕获关键的局部伪造线索。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226160540754.png"alt="image-20241226160540754" /><figcaption aria-hidden="true">image-20241226160540754</figcaption></figure><p>​  图4提供了我们提出的LAM的细节。空间特征<spanclass="math inline">\({\bf X}_{c n n}\)</span>首先通过MLP层投射到<spanclass="math inline">\(\hat{\mathbf{X}}_{c n n}\)</span>上，其中<spanclass="math inline">\(\hat{\mathbf{X}}_{c n n}\)</span>和<spanclass="math inline">\({\bf X}_{v i t}\)</span>具有相同的形状。对于<spanclass="math inline">\({\bf X}_{v i t}\)</span>中的第a个令牌<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>，LAM通过同时考虑其全局注意<spanclass="math inline">\(\mathrm{A}_{g l o b al}^{a}\)</span>和局部注意<span class="math inline">\(\mathrm{A}_{l o c al}^{a}\)</span>，计算其在<span class="math inline">\(\hat{\mathbf{X}}_{cn n}\)</span>不同部分的注意得分： <spanclass="math display">\[\mathbf{A}_{f i n al}^{a}=(1-\varphi(\sigma))\mathbf{A}_{g l o b al}^{a}+\varphi(\sigma)\mathbf{A}_{l o c a l}^{a},\]</span> ​  其中<spanclass="math inline">\(\mathbf{A}_{f i n a l}^{a}\)</span>是<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>的最终注意图。<spanclass="math inline">\(\varphi(\cdot)\)</span>表示s型函数，<spanclass="math inline">\(\sigma\)</span>是一个初始化值为零的可学习参数。<spanclass="math inline">\(\mathrm{A}_{g l o b al}^{a}\)</span>的计算类似于交叉注意，其表示如下： <spanclass="math display">\[{\bf A}_{g l o b a l}^{a}=s o f t m a x(({\bfX}_{t o k e n}^{a}W_{Q})({\bf\hat{X}_{c n n}}W_{K})^{T}).\]</span>​  利用二次编码[24]，引入<span class="math inline">\(\mathrm{A}_{l o c al}^{a}\)</span>算法来强调<span class="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>周围空间信息的局部性： <span class="math display">\[{\bfA}_{l o c a l}^{a}=s o f t m a x(\Phi\Psi^{T}),\]</span> ​  其中，<spanclass="math inline">\(\Phi \in\mathbb{R}^{\sqrt{L}\times{\sqrt{L}}\times3}\)</span>表示局部强度，<spanclass="math inline">\(\Psi\in \mathbb{R}^{1\times3}\)</span>是决定<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>注意方向的方向向量。注意，为了简化，我们在softmax操作之前省略了平坦操作。假设<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>的空间位置为<spanclass="math inline">\((i_a,j_a)\)</span>，同样，在<spanclass="math inline">\(\hat{\mathbf{X}}_{c nn}\)</span>中第b个标记的空间位置为<spanclass="math inline">\((i_b,j_b)\)</span>。<spanclass="math inline">\(\Phi\)</span>在位置<spanclass="math inline">\((a,b)\)</span>上的相对局域性强度<spanclass="math inline">\(\phi_{a,b}\in\mathbb{R}^{1\times3}\)</span>表示为：<spanclass="math display">\[\phi_{a,b}=(\|(i_{b}-i_{a},j_{b}-j_{a})\|_{2},i_{b}-i_{a},j_{b}-j_{a})\,.\]</span>​  另一方面，方向向量<span class="math inline">\(\Psi\)</span>表示为：<spanclass="math display">\[\Psi=(-1,2\psi_{1},2\psi_{2}),(\psi_{1},\psi_{2})\in\{-1,0,1\}\,.\]</span>​  在实践中，我们在不同的头部为<spanclass="math inline">\(\psi_{1}\)</span>和<spanclass="math inline">\(\psi_{2}\)</span>分配不同的值，以探索不同方向的局部信息，如图5所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226163304337.png"alt="image-20241226163304337" /><figcaption aria-hidden="true">image-20241226163304337</figcaption></figure><p>​  我们为每个查询标记收集最终的注意映射，以形成<spanclass="math inline">\(\mathbf{X}_{v i t}\)</span>的<spanclass="math inline">\(\mathbf{A}_{f\,i n al}\)</span>，并使用它将局部空间信息注入到ViT特征中。此过程的表述如下：<span class="math display">\[\hat{\bf X}_{v i t}={\bf X}_{v it}+\beta{\bf A}_{f i n a l}\hat{\bf X}_{c n n}W_{V},\]</span>​  其中β是一个初始化的可学习缩放因子，<spanclass="math inline">\(\hat{\bf X}_{v it}\)</span>被传递到下一个vit块。根据经验，将多尺度空间信息分别注入第一、第四和第七个ViT块。</p><h2 id="d.-细粒度的自适应学习fal">D. 细粒度的自适应学习FAL</h2><p>​  细粒度信息对于提高泛化性能[17]、[47]、[48]非常重要。因此，我们引入了FAL来促进细粒度伪造感知信息的自适应学习。我们首先将最后一个FC层中与真实人脸对应的权重向量设置为真实人脸的代理原型，因为之前的工作[65]已经证明，分类器中的权重收敛到每个类的中心方向。在每个细粒度对中，FAL拉近了原型和真实人脸之间的相似性，同时通过修改后的circle损失将篡改过的人脸从原型中推开[27]：<span class="math display">\[L_{F AL}=\log\big[1+\sum\exp(\eta(\gamma_{n}(s_{n}-m_{n})-\gamma_{p}(s_{p}-m_{p})))\big],\]</span></p><p><span class="math display">\[s_{p}=\mathrm{CosSim}(\mathrm{F}_{r e al},\mathrm{F}_{p r o}),\ s_{n}=\mathrm{CosSim}(\mathrm{F}_{f a ke},\mathrm{F}_{p r o})\]</span></p><p><span class="math display">\[\gamma_{p}=m ax(1+m-s_{p},0),\;\;\gamma_{n}=m a x(m+s_{n},0),\]</span></p><p><span class="math display">\[m_{p}=1-m,\ m_{n}=m,\]</span></p><p>​  其中CosSim表示余弦相似度，而<spanclass="math inline">\(F_{pro}\)</span>是真实面孔的原型。η是比例因子。M是控制细粒度对中的裕度和加权因子的超参数。<spanclass="math inline">\(F_{real}\)</span>和<spanclass="math inline">\(F_{fake}\)</span>是来自最后一个ViT块的细粒度对的编码特性。<br/>​  在优化过程中，通过<spanclass="math inline">\(\gamma_{n}(s_{n}-m_{n})-\gamma_{p}(s_{p}-m_{p})=0.\)</span>，实现了决策边界。通过使用方程式13和14，将决策边界转换为：<span class="math display">\[s_{n}^{2}+(1-s_{p})^{2}=2m^{2}\]</span>​  方程15表明FAL鼓励细粒度对向以sp = 1和sn = 0为中心、半径为<spanclass="math inline">\({\sqrt{2}}m\)</span>的圆收敛，如图6所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226171306453.png"alt="image-20241226171306453" /><figcaption aria-hidden="true">image-20241226171306453</figcaption></figure><p>​  这种优化使每个细粒度对能够提供更灵活的梯度来指导细粒度信息的学习。例如，在A点，当远远大于<spanclass="math inline">\(grad_r\)</span>时，该模型强调捕获细粒度的鉴别信息，以推开被篡改的人脸。相比之下，在B点，当<spanclass="math inline">\(grad_f\)</span>比<spanclass="math inline">\(grad_r\)</span>小得多时，该模型侧重于学习细粒度的一致性，以拉近真实的人脸。另一方面，FAL忽略了非细粒度对的梯度，如图图6中C点的例子。在这种情况下，差异涉及到背景或非必要的操作区域，这可能会导致模型过拟合到平凡的特征。<br/>​  显然，FAL通过探索细微差异区域的细粒度信息，在特征空间中紧凑真实的人脸特征。它没有明确地惩罚被操纵面孔的距离，因为我们希望在不同的伪造技术中保留操纵痕迹的多样性。与单中心损失（SCL，SingleCenterLoss）[10]使用不同类别之间的平均距离来压缩类内方差不同，FAL专注于学习每个细粒度对中的细粒度鉴别信息和一致信息，使挖掘关键信息的过程更加精确。</p><h2 id="e.-总损失">E. 总损失</h2><p>​  我们提出的FA-ViT是端到端训练的，并由预测结果<spanclass="math inline">\(\haty\)</span>和地面真实标签y之间的交叉熵损失进行监督： <spanclass="math display">\[L_{ce}=-y\log\hat{y}-(1-y)\log\left(1-\hat{y}\right),\]</span>​  其中，标签y为0是为真实的面孔，否则y为1。总体目标函数由两个组成部分组成：<span class="math display">\[L_{t o t a l}=L_{c e}+\lambda L_{F AL},\]</span>​  其中，λ是一个加权参数。在第一个训练阶段，我们将λ设置为0，允许模型专注于学习分类信息。随后，我们将λ调整为1，从而促进了细粒度信息的学习。</p><h1 id="iv.-实验">IV. 实验</h1><h2 id="a.实验设置">A.实验设置</h2><h3 id="数据集">1)数据集：</h3><p>​  我们采用了8个广泛使用的公共数据集来评估我们的模型。<br/>​  1)FaceForensics++（FF++）[66]是一个广泛使用的数据集，由四种类型的人脸操作技术组成：DeepFakes(DF) [75], Face2Face (F2F)[76], FaceSwap (FS) [77], and NeuralTextures(NT)[78]。<br/>​  2)Celeb-DF-v2（CDF）[68]是一个高质量的深度假数据集，专门针对名人的人脸。<br/>​  3)WildDeepfake（WDF）[69]从互联网上收集深度伪造视频，其中包括各种场景和伪造方法。在WDF上的评价结果反映了检测器在现实世界场景中的性能。<br/>​  4)DeepfakeDetectionChallenge（DFDC）[71]提供了一个具有挑战性的数据集，包含来自不同场景的各种深度假视频，使用了不同的深度伪造、基于GAN和传统的人脸交换方法。<br/>​  5)DeepfakeDetection ChallengePreview（DFDC-P）[70]提供了一个DFDC数据集的预览版本，并合并了两种面部修改算法。<br/>​  6)DeepFakeDetection（DFD）[72]是另一个全面的深度假数据集。这个数据集包括超过3000个被操纵的视频，包括28个演员和不同的场景。<br/>​  7)DeeperForenics-1.0（DFR）[67]是通过使用FF++的真实视频和创新的端到端面部交换框架。它也可以作为一个流行的数据集来衡量morel的鲁棒性。<br/>​  8)FFIW-10K (FFIW) [73]是一个最近的大规模数据集，它专注于多人的场景。对于DFD数据集，我们使用所有的视频来进行评估。对于其他的数据集，我们按照官方的策略来分割相应的数据集。关于一个全面的概述，请参见表一。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226212602830.png"alt="image-20241226212602830" /><figcaption aria-hidden="true">image-20241226212602830</figcaption></figure><h3 id="实施细节">2)实施细节：</h3><p>​  我们使用MTCNN[79]裁剪面区域，并将其调整为224×224。我们从每个视频中只采样20帧来构建训练数据。在测试过程中，我们从每个视频中抽取50帧的样本进行评估。我们的方法是在一个NVIDIAGTX 3090上使用PyTorch库[80]实现的。我们采用在ImageNet-21K[81]上预训练的ViT-Base模型作为FA-ViT的主要骨干。为了进行优化，我们使用了Adam[82]优化器，其初始学习速率为3×10−5，权重衰减为1×10−5，批量大小为32。学习速率每5个时代衰减0.5个。FAL中m和η的超参数将在第2节中进行讨论。IV-F.CNN的详细结构如表二所示，参数使用随机初始化方法进行初始化。</p><h3 id="评价指标">3)评价指标：</h3><p>​  我们遵循[17]中的评价策略，以准确性（ACC）和受试者工作特征曲线下面积（AUC）作为我们的评价指标。为了进行公平的比较，我们对同一视频中的预测进行平均，得到视频级的预测，并给出了其他工作的结果。</p><h2 id="b.-数据集内评估">B. 数据集内评估</h2><p>​  我们在广泛使用的FF++数据集上进行了数据集内实验，包括高质量（C23）和低质量（C40）数据集。所有的模型都在同一个数据集上进行训练和测试，其中的性能显示了模型在伪造的人脸中捕获操作痕迹的能力。表3说明了数据集内的结果，其中我们分别加粗和下划线显示了最佳和第二优的分数。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226213115559.png"alt="image-20241226213115559" /><figcaption aria-hidden="true">image-20241226213115559</figcaption></figure><p>​  一般来说，在C40压缩数据中删除了许多操作痕迹，这使得它们很难被检测到。在这种具有挑战性的环境下，可以观察到，我们提出的FAViT比大多数以前的艺术有相当大的优势。例如，我们的方法在AUC方面比最近基于vit的模型F2Trans多出了2.53%。当对C23数据进行评估时，所有的SOTA方法都能够实现非常高的检测acc。FA-ViT仍然取得了97.86%的ACC评分和99.60%的AUC评分，证明了其对C23数据的有效性。此外，与ViT基线相比，FAViT在C40设置下将ACC从90.00%提高到92.17%，在C23设置下将96.00%提高到97.86%。<br/>​  虽然FA-ViT在数据集内评估中获得了令人满意的检测精度，但其性能仍低于CFM[39]和RECCE[37]。这一结果可能是由于CFM需要像素级标签来进行监督，而侦察采用了像素级重建任务，使得这些方法对数据集内模式更加敏感。我们将研究其他设计的可能性，以进一步提高数据集内设置的性能。</p><h2 id="c.-跨数据集评估">C. 跨数据集评估</h2><p>​  跨数据集评估是一个非常具有挑战性的设置，因为这些数据集中的场景和字符是复杂的，并且与训练数据不同。为了全面评估在不同的不可见数据集上的泛化性能，我们在7个基准数据集上进行了广泛的实验。具体来说，所有的模型都在FF++（C23）上进行训练，并在不可预见的数据集上进行评估：CDF、WDF、DFDC-P、DFDC、DFD、DFR和FFIW。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20250109162720276.png"alt="image-20250109162720276" /><figcaption aria-hidden="true">image-20250109162720276</figcaption></figure><p>​  表四说明了在AUC方面的交叉数据集比较结果。与其他基于ViT的CDF方法相比，包括UIA-ViT、F2Trans、TALL-ViT和ViT，我们提出的FA-ViT方法分别优于4.98%、6.37%、7.25%和10.05%。值得注意的是，CFM获得了最好的数据集内性能，但其泛化性能不如我们提出的FA-ViT。此外，与最近的方法LSDA和DSRL相比，我们的FA-ViT在AUC方面分别提高了4.88%和4.75%，突出了其检测看不见的深度假面孔的最先进的泛化能力。总的来说，FA-ViT在所有7个数据集上都表现出了出色的性能，这些发现验证了自适应学习范式对预训练的ViT的有效性。</p><h3 id="d.-交叉篡改评估">D. 交叉篡改评估</h3><p>​  人脸伪造检测器在新操作方法上的泛化性能在现实应用中非常重要。我们遵循[17]，[25]中提出的协议来评估交叉操作性能，其中模型在不同的篡改类型的样本上进行训练，并在未知的篡改方法上进行测试。结果见表五。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20250109162912984.png"alt="image-20250109162912984" /><figcaption aria-hidden="true">image-20250109162912984</figcaption></figure><p>​  我们提出的方法在交叉操作评估方面取得了优越的性能，分别比GID-DF（C23）的3.22%和GID-DF（C40）的3.42%。值得注意的是，我们观察到完全精细的ViT并不能很好地推广到F2F，而我们提出的FA-ViT在GIDF2F（C23）和GID-F2F（C40）上都取得了实质性的改进，这表明自适应学习策略极大地提高了模型的泛化性能。</p><h3 id="e.-对真实世界扰动的鲁棒性">E. 对真实世界扰动的鲁棒性</h3><p>​  通过在线社交网络传输深度假视频不可避免地会引入各种失真[85]，如视频压缩、噪声等。为了研究对不可见扰动的鲁棒性性能，我们遵循了DFR[67]中提出的协议。具体来说，具体来说，不同的扰动应用于DFR的测试集，包括单水平随机类型畸变（std/sing）、随机水平随机类型畸变（std/rand）、三种随机水平随机类型畸变的混合（std/mix混合3）和四种随机水平随机类型畸变的混合（std/mix4）。我们在DFR的原始训练集上训练FA-ViT和ViT。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20250109163518716.png"alt="image-20250109163518716" /><figcaption aria-hidden="true">image-20250109163518716</figcaption></figure><p>​  表六说明了在ACC方面的比较结果。我们可以观察到，当对std/mix3和std/mix4设置中的原始输入应用多种类型的扰动时，其他方法会显著降低。例如，ViT在std/rand设置中达到了96.77%，但在std/mix4设置中其性能下降到了87.06%。相比之下，我们提出的FA-ViT在所有失真设置中都获得了最好的性能，在复杂的场景中其性能逐渐下降。主要原因可能是我们在自适应学习过程中保留了预先训练过的ViT特征，因为这些特征在各种扰动[86]下表现出了鲁棒性能。</p><h3 id="f.-消融研究">F. 消融研究</h3><p>​  为了分析我们提出的FA-ViT中不同成分的影响，我们通过在FF++（C23）上训练所有变体来进行消融实验。我们给出了FF++的数据集内结果和CDF和WDF数据集的跨数据集结果。为了确保公平的比较，所有的实验都使用相同的随机种子进行。</p><p>1)对不同模型自适应的影响</p><p>2)GAM的消融：</p><p>3)LAM的影响</p><p>4)对提出的FAL进行的实验</p><p>5)不同参数对FAL的影响</p><p>6)零初始化的影响</p><p>7)不同的预训练初始化的影响</p><p>8)不同的预训练初始化的影响</p><h2 id="g.-显著性地图可视化">G. 显著性地图可视化</h2><p>​  为了更好地阐明我们的FA-ViT的有效性，我们使用GradCAM++[92]将模型对深度假脸的注意力可视化，如图10所示。可以观察到，FA-ViT为不同的深度假脸生成了可区分的显著性图，并捕获了方法特有的伪影，如FS的前额区域和F2F的口腔区域。在跨数据集场景中，ViT难以在复杂环境中检测深度造假，如CDF中的大姿态人脸或DFDC中具有挑战性的照明条件。相比之下，FA-ViT在不同的未知数据集上持续地跟踪被操纵的区域，从而从决策的角度验证其有效性。</p><h1 id="v.-结论">V. 结论</h1><p>​  在本文中，我们从模型自适应的角度提出了一种新的伪造感知自适应自适应视觉变压器（FA-ViT），这在以往的研究中被忽略了。具体来说，在训练过程中设计了全局自适应模块（GAM）和局部自适应模块（LAM），将全局和局部伪造感知信息用于广义表示学习时，保留了预训练后的ViT的表达性。此外，我们还引入了细粒度自适应学习（FAL），以促进细粒度伪造感知信息的自适应学习。总之，我们提出的框架为人脸伪造检测的挑战提供了一个广义和稳健的解决方案。我们相信，我们提出的方法可以为研究界提供有价值的见解，并进一步推进人脸伪造检测系统的发展。</p>]]></content>
      
      
      <categories>
          
          <category> deepfake </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning</title>
      <link href="/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/"/>
      <url>/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/</url>
      
        <content type="html"><![CDATA[<p>Deep Adaptive Fuzzy Clustering for Evolutionary UnsupervisedRepresentation Learning</p><h1 id="摘要">摘要</h1><p>​  在模式识别和计算机视觉中，大型和复杂数据集的聚类分配是一项关键但具有挑战性的任务。在本研究中，我们探索了在深度神经网络框架中使用模糊聚类的可能性。因此，我们提出了一种新的具有迭代优化的进化无监督学习表示模型。它实现了深度自适应模糊聚类（DAFC）策略，从只给定的未标记数据样本中学习卷积神经网络分类器。DAFC由深度特征质量验证模型和模糊聚类模型组成，其中实现了深度特征表示学习损失函数和具有加权自适应熵的嵌入式模糊聚类。我们将模糊聚类联合到深度重建模型中，其中利用模糊隶属度来表示一个清晰的深度聚类分配结构，并联合优化深度表示学习和聚类。同时，该联合模型通过检查从估计的瓶颈空间重采样的数据是否具有一致的聚类特性来评估当前的聚类性能，从而逐步改进深度聚类模型。在不同数据集上的实验表明，与其他先进的深度聚类方法相比，该方法在重建和聚类质量方面都有更好的性能，大量实验的深入分析证明了这一点。</p><h1 id="i.介绍">I.介绍</h1><p>​  受深度特征学习框架、深度信念网络（DBN）[18]和稀疏自编码器层次结构[19]、[20]的成功启发，它们试图从输入数据中提取特征，如反进化网络，贪婪地以无监督的方式从图像向上构建层。在这些研究中，每一层都由具有聚类模型的编码器和解码器组成。此外，用于深度聚类的卷积神经网络（CNN）架构，其中变分自编码器（VAE）是用于深度生成学习的非常强大的无监督框架。在深度聚类中广泛使用的cnn是堆叠自动编码器（SAEs）[21]和结构化自动编码器（StructAE），它们结合了基于图的聚类来实现精确的深度表示。它们都采用了多阶段管道，用无监督学习方法对统一的深度网络模型进行预训练，并对大规模图像实现了传统的聚类方法作为后处理。然而，它们在聚类分配中进行深度聚类，且高度依赖于预先训练好的相似度矩阵，因此在大数据集上的聚类性能不够好。<br/>​  此外，Yang等人的[11]使用循环框架迭代地学习深度特征表示和聚类分配。该框架在聚类模型中的连续操作被表示为循环过程中的步骤，并堆叠在通过cnn输出的表示方式之上。他们的模型在小的复杂数据集上表现出了很好的性能，但可能对多凸网竞争所需的大量复杂图像具有挑战性。深度嵌入式聚类（DEC）[22]是一种著名的深度网络结构，它可以同时实现特征表示和聚类模型来训练复杂的数据集。该方法通过采用高度机密的样本作为监督，使每个聚类中样本的分布更密集、更清晰地分组。然而，在使用随机梯度下降（SGD）训练大而复杂的数据集时，并不能保证将样本拉到正确的聚类附近，这也不能保证快速收敛。<br/>​  虽然卷积网方法的联合聚类和特征学习在无监督学习中表现出了显著的性能，但在特征聚类和网络参数更新之间交替的训练计划导致特征表示[23]的学习不稳定。此外，它们也没有联合优化深度表示学习和聚类。具有自进化聚类[24]和大规模多目标决策聚类[25]的深度对流模型由于能够处理大规模数据集和复杂的表示而受到越来越多的关注。由于计算复杂度高，深度卷网络结构的应用通常需要一个具有强大计算能力的平台。一些聚类方法已经用深度共流网络进行了研究，但深度聚类的关键成分仍不清楚。例如，如何有效地将实例为巨大的复杂数据分组到集群中，并提供定义面向集群的损失函数的有效信息？如何在精度和效率之间实现良好的权衡，并提高深度卷网的聚类性能？哪些类型的神经网络结构适合用于聚类的特征表示学习？<br/>​  在本研究中，我们的目标是开发新的进化表示学习解决方案的深度无监督聚类问题。综上所述，本研究的主要贡献如下。</p><ol type="1"><li>我们提出了DAFC来自动分组图像，得到的迭代优化问题可以通过小批量RMSprop和反向传播而不是SGD有效地解决，可以学习一个更聚类友好的瓶颈空间。<br/>2.我们仔细地制定了一个目标，以包含对高聚类性能至关重要的三个关键方面：有效的潜在表示空间、相似度度量和深度聚类的加权自适应熵，它们可以集成到深度学习架构中<br/>3.与单独优化这些目标的情况相比，这种深度进化无监督表示学习对网络损失和具有模糊聚类的重建损失提供了优越的性能。<br/>4.加权自适应熵的深度聚类，我们计算模糊会员和最优权重作为全局变量，可以共同优化深度表示学习，有效地组实例到集群为巨大的复杂数据和实现一个好的权衡精度和效率，以及进一步提高深度双convnet集群的性能。</li></ol><p>​  本研究的其余部分组织如下。我们首先回顾了在第二节中包含一些有效的深度聚类框架的相关工作。我们在第三节提出的深度自适应模糊聚类（DAFC）。在第四节中，我们讨论了最先进的深度聚类方法的结果和分布，并将它们与我们的方法进行了比较。本研究的结论和今后的工作见第五节。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  VAE的网络由编码器和解码器组成，其中潜在特征层后面是潜在特征层，如图1所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165218699.png"alt="image-20241223165218699" /><figcaption aria-hidden="true">image-20241223165218699</figcaption></figure><p>​  编码器将输入的样本映射到潜在的特征层。潜在特征层从编码器模型中学习输入数据的潜在特征，并将高维特征映射到低维子空间，然后利用聚类模型对所映射的数据进行划分。VAE的解码器恢复并重建特征，使特征数据能够重构为原始数据。期望VAE的输入和输出可以以一致的或无损的方式进行重构，且潜在特征向量的维数远小于输入样本的维数。使用学习到的潜在层来聚类分配和其他任务将更有效。该方法有利于学习更多重要的特征，而忽略了一些冗余的特征。但是，如果目标的大小与训练图像的背景有较大的差异，训练网络很容易忽略学习过程[34]中忽略目标特征。因此，当训练诸如ImageNet数据集等庞大而复杂的数据集时，VAE的精度并不高，甚至不能对其进行分区。<br/>​  虽然该方案利用SAE和VAE将输入数据映射到一个具有代表性的特征空间，然后进行聚类分析，但特征表示空间和聚类方法是两个独立的过程，其目标函数没有联合优化。在第三节中，我们将基于ConvNets结构建立更深层次的EF和重建模型，并结合模糊聚类模型。</p><h1 id="iii.方法">III.方法</h1><p>​  在本节中，我们将描述我们的深度进化无监督表示学习，并表明通过深度聚类框架可以获得有用的通用的巨大和复杂的特征。</p><h2 id="a.网络架构">A.网络架构</h2><p>​  基于统计学习和深度cnn的现代深度网络计算机视觉方法需要良好的图像特征化。在DAFC方法中，我们建立了一个更深入的FE模型和Rec模型，并通过潜在的表示瓶颈层将它们连接起来。瓶颈空间的有效潜在表示是深度进化无监督表示学习的一个关键方面，它可以更好地提取网络的每一层之间的特征。在联合聚类的瓶颈空间中构建了一个局部结构，以实现更好的聚类分配。与最先进的深度聚类相比，我们很容易假设这种优势是由于瓶颈空间可以通过最小化重构和聚类损失来保留输入数据的局部结构。我们设计了一种有效的基于凸面的模糊聚类模型分类器，以广泛地利用瓶颈空间，其中损失函数是重构损失和模糊聚类损失的和。<br/>​  DAFC的网络体系结构的目标是将多层数据点划分为完全没有任何标签的集群，并联合优化深度表示学习，进一步提高了对异常值的鲁棒性。对于同一样本，不同类型的层（深和浅）之间的相互信息应该最大化。为了提高深度聚类的准确性，我们增加了模型中网络的特征训练层的数量。受ResNet[35]和DenseNet学习[36]的启发，我们试图控制网络优化器，以便对SGD、Adam和RMSprop进行比较。在网络的梯度下降问题上，我们采用了一个较慢的学习速率来处理脊间的跳跃问题，这有助于获得更好的聚类性能。<br/>​  所提出的网络模块的关键因素是，在我们的深度ConvNets模型中，我们使用了具有步幅的卷积层和池化层，而不是接在池化层后的卷积层。它不同于VAE的模块。在这项工作中设计的模块导致了更高的转换能力。我们对卷积层、ReLU层、批归一化、池化和退出层进行多个组合，如图2所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165700880.png"alt="image-20241223165700880" /><figcaption aria-hidden="true">image-20241223165700880</figcaption></figure><p>​  此外，深度ConvNets模型通常通过池层缩小特征映射的大小。所设计模型的每个模块通过前馈方式将每一层连接到每一层，模块1为22层前馈神经网络，具有前2层，模块2为8层。该模型采用多个全连通层作为瓶颈空间的潜在表示。我们还可以根据输入数据的要求，灵活地深化网络模块的建设。当在足够复杂的数据上进行训练时，该方法在标准的竞争分类基准上不断取得最佳性能。<br/>​  在我们的网络模型中，我们使用多个全连接层，加深全连接层的数量，以提高模型的非线性表达式和特征学习能力。为了防止深化模型的学习能力过好，导致过拟合，我们添加了dropout层，然后是全连接层。dropout层只允许为每个反向传播调整部分网络参数。如图3所示，我们根据网络模块建立了DAFC的深度卷积神经网络模型。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223170209837.png"alt="image-20241223170209837" /><figcaption aria-hidden="true">image-20241223170209837</figcaption></figure><p>​  在这些模块中，每个块都有不同的功能层。此外，模块1和模块2分别对应于所设计的网络模型的每个部分。<br/>​  在该网络体系结构中，模块1被设计为FE模型，模块2被设计为重构模型，如图2所示。FE模型提供了一个从输入到潜在表示瓶颈空间的自底向上的映射，而重构（Rec）模型将提取的特征映射回输入空间，希望能给出一个接近原始数据的重构。FE和重构模型，并通过卷积操作的潜在表示瓶颈层将它们连接起来。在联合模糊聚类的瓶颈空间中构建了一个局部结构，以实现更好的聚类分配。在Rec模型中，我们设计了一种有效的基于深度凸网的分类器，具有加权自适应熵模糊聚类模型，以广泛利用瓶颈空间。此外，我们还研究了一种联合策略，其中损失函数是最小化重建损失和模糊聚类损失的和，其中FE模型<spanclass="math inline">\(F = f_w (x)\)</span>和Rec模型<spanclass="math inline">\(G\equivg_{\theta}(F)\)</span>的参数。<br/>​  为此，我们将FE模型的重构损失和模糊聚类损失同时结合为目标函数。深度卷积网络的FE模型保留了复杂数据生成分布的局部结构，避免了特征空间的破坏。然后，随着学习的进行，设计的深度卷积网络可以通过迭代训练来测量更准确的相似性，并且会逐渐选择更多的聚类任务来找到更精细的组。模糊聚类模型的目的是在大量和不同的数据点之间对相似或相同的模式进行分组。在这个深度聚类模型中，我们使用隶属度使聚类结果更具区分性，µ是数据点x属于第j个聚类的分配概率。</p><h2 id="b.深度聚类策略">B.深度聚类策略</h2><p>​  在我们的网络体系结构中，FE和Rec模型重叠了复杂多层次的所有特征，其中重叠的域，这是整个体系结构的层次组成。通过将l层的特征映射f(x)作为层l + 1的输入，可以很容易地堆叠形成层次，其中输入数据x到瓶颈空间Z(x)的初始非线性映射，如图4所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223171332756.png"alt="image-20241223171332756" /><figcaption aria-hidden="true">image-20241223171332756</figcaption></figure><p>​  映射F和G通过FE模型和Rec模型实现，它们通过优化的瓶颈空间与卷积层和多个全连接层进行连接。改进了瓶颈空间表示，使具有属于同一集群的高概率的映射点对将被拉得更近在一起。因此，设计瓶颈空间将FE模型和Rec模型连接起来，这对联合模糊聚类更有意义。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223170642966.png"alt="image-20241223170642966" /><figcaption aria-hidden="true">image-20241223170642966</figcaption></figure><p>​  f是训练过程中模糊隶属度为<spanclass="math inline">\(\mu\)</span>的瓶颈空间中输入数据的特征，如图5中的相似性结构所示，z为瓶颈空间表示。它们也代表了在网络超参数下由卷积网分类器进行的特征划分的结果。通过优化重构的损失函数，共同学习ConvNets分类器的参数θ和映射的参数w，公式如下：</p><p><spanclass="math display">\[{L}_{\mathrm{Rec}}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),z_{i}\right]\]</span></p><p>​  首先，我们通过数据增强得到一批原始数据<spanclass="math inline">\(\{x\}\)</span>加上转换后的数据<spanclass="math inline">\(\{x^{\prime}\}\)</span>，然后得到输入数据：<spanclass="math inline">\(x_i = x + x^{\prime}\)</span>。<spanclass="math inline">\(x_i\)</span>表示第i张图像，M为图像数。在输入数据中，每个图像<spanclass="math inline">\(x_i\)</span>都与<spanclass="math inline">\(\{0,1\}^k\)</span>中的一个标签<spanclass="math inline">\(y_i\)</span>相关联。该网络将输入数据映射到紧凑的特征映射<spanclass="math inline">\(F=f_w(x_i)\)</span>中，并对FE模型的输出进行聚类，然后使用后续的聚类分配作为伪标签来优化(1)，并作为反馈信息反向传播到Rec模型中。下一步通过使用Rec模型的网络生成输入数据的特征标签，其中相似度矩阵从ConvNets的样本内存中读取该批数据的伪标签。图5是所提出的使用模糊聚类模型的深度进化无监督表示学习的示意图。我们在定义1中定义了相似矩阵，并对于潜在瓶颈空间表示<spanclass="math inline">\(z_{i}=f_{\mu}(y_{i},x_{i})\)</span>。利用相似度矩阵，利用最优优化器对深度网络进行更新，结合模糊聚类模型如下：</p><p><spanclass="math display">\[{L}_{\mathrm{Rec}}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\]</span>​  其中<spanclass="math inline">\(f_{\mu}(\cdot)\)</span>为相似度矩阵每一列设置的模糊隶属度，这种表示应该更准确。基于(2)，我们进一步得到<spanclass="math display">\[f_{\mu}\left(y_{i},x_{i}\right)=C_{i,j}\left(x_{i}\right)\cdoty_{i}\]</span></p><h3 id="定义1相似矩阵">定义1：相似矩阵。</h3><p>​  我们假设C是生成特征标签的邻接矩阵，<spanclass="math inline">\(a_i\)</span>是C的第i列。标签的节点i与节点j的相似性为<spanclass="math display">\[C_{i,j}(\cdot)=\frac{a_{i}^{T}a_{j}}{\|a_{i}\|}\=\frac{a_{i}^{T}a_{j}}{\sqrt{d_{i}}\sqrt{d_{j}}}\]</span>​  我们将FE模型训练的特征转移到瓶颈空间，在目标中加入模糊聚类，并随着聚类损失进行优化。将模糊聚类模型的输出结果作为伪标签反向传播到ConvNets中，并对<spanclass="math inline">\(L_{Rec}\)</span>进行优化，并对网络参数进行优化。在经过逐层贪婪训练后，通过反向传输将FE模型和Rec模型的所有层连接起来，构建深度联合训练模型。然后对联合模型进行微调，设置一个高阈值和两个权衡参数，以最小化重构信息的损失。Rec模型保留了伪标签生成分布的局部结构，避免了瓶颈空间的破坏。该深度聚类模型可以迭代地学习输入数据的特征，并对其进行划分。<br/>​  此外，我们计算特征之间的相似性，并选择高置信度的生成的标签特征，通过联合框架反馈FE和Rec模型的训练。在Rec模型中，我们设置了一个高阈值来确定是否应该将一些图像伪标签合并到特征映射中。如果两个标签之间的相似性大于高阈值，并且我们将这种类型的标签分组属于同一个聚类。反馈的反向传播FE和Rec旨在学习基于卷积神经网络的深度映射函数g，该函数由θ参数化。通过模糊聚类模型最小化Rec的损失函数来更新联合框架的参数<span class="math display">\[L_{F_{-}\mathrm{clu}}=\ell\[||X_{i}-g_{\theta}(f_{\mu}(x_{j}))||_{2}^{2}]+H(W)\\=\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\left\|X_{i}-g_{\theta}\bigl(f_{\mu}\bigl(x_{j}\bigr)\bigr)\right\|_{2}^{2}+H(W)\]</span>​  式中，M为数据集中的数据样本数，j为第j个簇，k为簇分配的数量。<br/>​  在模糊聚类的损失函数中，我们考虑了一个庞大而复杂的数据集的权值在瓶颈空间中被划分为聚类，它也代表了该数据特征映射在形成聚类中的贡献概率。我们进一步改进了加权自适应损失函数的模糊聚类，增加加权熵，刺激更多的特征有助于聚类识别。这样，DAFC模型即使使用加权自适应熵，也可以直接进行深度端到端训练，这种学习到的层次表示被证明对深度聚类任务是有效的。</p><h3id="定义2深度聚类的加权自适应熵44">定义2：深度聚类的加权自适应熵[44]。</h3><p>​  假设聚类的权值信息为H (w)，并同时设置最优模糊隶属度和<spanclass="math inline">\(w_{i j}\)</span>的最优权值。H (w)的计算方法如下：<spanclass="math display">\[H(w)=\lambda_{1}\Biggl(1-\sum_{j=1}^{k}\mu_{ij}\Biggr)+\lambda_{2}\sum_{i=1}^{M}\sum_{j=1}^{k}w_{i j}\log w_{ij}\]</span> ​  其中，<spanclass="math inline">\(\lambda_{1}\)</span>是一个权衡参数，控制分配给各种类型异常值的模糊鲁棒性，<spanclass="math inline">\(\lambda_{2}\)</span>也是一个权衡参数，控制模糊隶属度的簇分布。DAFC的目标函数是<span class="math display">\[\mathrm{min}\, L=L_{\mathrm{Rec}}+L_{F_{-}\mathrm{clu}}\]</span> ​  可改写如下： <spanclass="math display">\[\begin{array}{ll}\operatorname*{min}L&amp;=&amp;\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\cdot\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\\&amp;&amp;+\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\cdot\ell\,\left\|X_{i}\,-\,g_{\theta}\big(f_{\mu}\big(x_{j}\big)\big)\right\|^{2}\\&amp;&amp;+H(W)\\&amp;{\mathrm{s.t.}}&amp;~\mu_{ij}\in\{0,1\}\\&amp;&amp;\sum_{j=1}^{k}\mu_{i j}=1,\quadi=1,\dots,M\end{array}\]</span> ​  考虑一组M图像<spanclass="math inline">\(\{x_{1},\cdot\cdot\cdot,x_{M}\}^{k}\)</span>聚类到瓶颈空间的k个聚类，每个图像<spanclass="math inline">\(x_i\)</span>与<spanclass="math inline">\(\{0,1\}^k\)</span>中的标签<spanclass="math inline">\(y_i\)</span>相关联。我们还使用模糊隶属度µ表示图像到k个可能的预定义簇之一的概率。输入数据x的瓶颈空间Z(x)离质心<spanclass="math inline">\(c_{j}=g_{\theta}(f_{\mu}(x_{j}))\)</span>越近，属于聚类j的x的模糊隶属度就越高。µ是模糊隶属度，m是拉格朗日乘子。然后从这个目标函数中推导出µ如下：<span class="math display">\[{\frac{\partialL}{\partial\mu}}=0\Rightarrow\mu=\left({\frac{\lambda_{1}}{m\cdot\eta}}\right)^{\frac{1}{m-1}}\]</span>​  其中<spanclass="math inline">\(\eta=\ell[\|X_{i}-g_{\theta}(f_{\mu}(x_{j}))\|_{2}^{2}]\)</span>。<br/>​  现在我们考虑函数L的导数与在新的步骤中的权重。给定度量µ和cj是固定的，L为(8)中的最优权重w最小化<span class="math display">\[w_{ij}=\exp\left[2\sum_{j=1}^{k}g_{\theta}{\big(}f_{\mu}(x_{j}){\big)}-1\right]\]</span>​  自适应熵的加入允许聚类用一个更快的迭代算法来解决分割问题。<br/>​  总之，我们引入了一个用于进化无监督表示学习的DAFC。在算法1中描述了DAFC的伪代码。在每次迭代过程中，深度模糊聚类通过固定的深度FE模型和Rec模型交替选择相似和不同的样本组，并根据所选的样本组对其进行训练。具体步骤见算法1。</p><hr /><p>算法1：深度自适应模糊聚类（DAFC）</p><hr /><p><strong>输入：</strong>数据集<spanclass="math inline">\(X=\{x_{i}\}_{i=1}^{k}\)</span>，平衡系数<spanclass="math inline">\(\lambda_{1}\)</span>、<spanclass="math inline">\(\lambda_{2}\)</span>，高阈值<spanclass="math inline">\(\epsilon_r\)</span><br/><strong>输出：</strong>深度聚类结果R和Test_Err<br/>1随机初始化θ；<br/>2 用pre_train FE模型和Rec模型初始化C、G和F；<br/>3<span class="math inline">\(\{A c c,A R I,N M I\}=R\)</span>；<br/>4重复；<br/>5 <strong>for</strong> <em>epoch</em> ∈ 0<em>,</em> 1<em>, .. . ,</em> <em>MaxEpochs</em> <strong>do</strong><br/>6 |生成深度ConvNets表示G；<br/>7 | <strong>for</strong> <em>iter</em> ∈0<em>,</em> 1<em>, . . . ,</em> <em>MaxIter</em><strong>do</strong><br/>8 | | float Max <spanclass="math inline">\(\mu_{i j}\)</span> = 1；<br/>9 | | <spanclass="math inline">\(f_{\mu}(x)\neq n u l l\)</span>；<br/>10| |计算相似度矩阵<spanclass="math inline">\(C_{i,j}(\cdot)=\frac{a_{i}^{T}a_{j}}{\|a_{i}\|}\=\frac{a_{i}^{T}a_{j}}{\sqrt{d_{i}}\sqrt{d_{j}}}\)</span>；<br/> | |模糊隶属度=<spanclass="math inline">\(f_{\mu}\left(y_{i},x_{i}\right)=C_{i,j}\left(x_{i}\right)\cdoty_{i}\)</span><br/>11| | 计算<spanclass="math inline">\(L_{Rec}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\)</span><br/>| | 计算聚类的权值信息<spanclass="math inline">\(H(w)=\lambda_{1}\Biggl(1-\sum_{j=1}^{k}\mu_{ij}\Biggr)+\lambda_{2}\sum_{i=1}^{M}\sum_{j=1}^{k}w_{i j}\log w_{ij}\)</span><br/> | | 计算<spanclass="math inline">\(L_{F_{-}\mathrm{clu}}=\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\left\|X_{i}-g_{\theta}\bigl(f_{\mu}\bigl(x_{j}\bigr)\bigr)\right\|_{2}^{2}+H(W)\)</span><br/>12|| 通过最小化(7)加上L Rec和L F_clu；<br/>13| | 通过最小化来更新<spanclass="math inline">\(\mu_{i j}\)</span>(9)；<br/>14| |<strong>while</strong> <em>not end of R</em> <strong>do</strong><br/>15|| | 计算设计的convnet的精度（1 - test_error）；<br/>16| |根据（10）的要求更新<span class="math inline">\(w_{ij}\)</span>；<br/>17| | 在DAFC中向FE和Rec进行反向传播；<br/>18| |直到<span class="math inline">\(\{A c c,A R I,N MI\}=MaxR\)</span>；<br/>19| | <strong>end</strong>;<br/>20|<strong>end</strong>;<br/>21 return *R**,* <em>Test</em>_<em>Err</em>;</p><hr /><h1 id="iv.实验">IV.实验</h1><h2 id="a.实验设置">A.实验设置</h2><p>​  然后，我们与IV-d中最新的深度聚类模型和传统的聚类方法进行了全面的比较。这些实验都是在cudnnv7和RTX 2080 TiGPU（NVIDIA）中实现的，以测量不同深度聚类方法的性能和效率。我们考虑相同的批处理大小和相同的时代数量，以及以毫秒为单位度量推理时间。当对街景房子号（SVHN）和STL-10数据集进行训练时，我们需要注意计算所需的内存量，如果训练要在所需的时间范围内完成，也需要仔细平衡内核大小和过滤器大小。对于所有的卷积层，我们设置了不同的通道数（64、128、256和512）和滤波器大小（2×2和3×3）、步幅=1和填充=1。对于池化层，我们在最大池中设置内核大小=2和步幅=2，在平均池中设置内核大小=4和步幅=4。每一层都经过了2000次迭代的预训练，退出率为50%。我们在几个基准测试上研究了这些算法。据我们所知，我们为每个轮次设置了最大的batch_size。<br/>​  本文采用了三种优化器作为优化算法。我们已经验证了当使用Adam和RMSprop优化器时的性能。与Adam和SGD相比，我们发现Adam患有通常的平衬里问题，因此给出了一个糟糕的解决方案。SGD被发现需要数千次迭代来收敛，并且也给出了一个很差的解决方案。RMSprop为在选定的复杂数据集上训练深度聚类框架提供了更精确的解决方案。此外，我们还设置了学习率（lr）=1×10−4。为了公平地比较和清楚地说明DAFC的有效性，每种比较方法的训练期数和学习率都保持在相同的数量。我们实现了我们的方法和比较方法，并给出平均结果，以防止随机情况。<br/>​  在相同的实验环境和设置下，我们还使用最优参数训练了深度自适应聚类（DAC）和深度综合相关挖掘（DCCM）。自适应参数λ被初始化为0，并在DAC中使用具有学习率lr= 0.009的RMSprop。DCCM采用带有lr = 1e-4的RMSprop优化器，设置α = 5和β =0.1，采用3个1×1卷积层的网络进行互信息估计鉴别器。</p><h2 id="b.评价指标">B.评价指标</h2><h2 id="c.-数据集">C. 数据集</h2><h2 id="d.-baselines">D. Baselines</h2>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 模糊聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fuzzy_Machine_Learning</title>
      <link href="/Fuzzy-Machine-Learning/"/>
      <url>/Fuzzy-Machine-Learning/</url>
      
        <content type="html"><![CDATA[<p>Fuzzy Machine Learning: A Comprehensive Framework and SystematicReview</p><h1 id="摘要">摘要</h1><p>​  机器学习的力量来自各种学科，包括计算机科学、认知科学和统计学。虽然机器学习在理论和实践上都取得了很大的进步，但其方法在处理复杂情况和高度不确定的环境时存在一定的局限性。数据的不足、不精确的观察和模糊的信息/关系都会混淆传统的机器学习系统。为了解决这些问题，研究人员从不同的方面整合了机器学习和模糊技术，包括模糊集、模糊系统、模糊逻辑、模糊测度、模糊关系等。本文从模糊机器学习，从理论到应用方法，总体目标是概述模糊机器学习领域的最新进展。为此，将所讨论的概念和框架分为五类：1）模糊经典机器学习；2）模糊迁移学习；3）模糊数据流学习；4）模糊强化学习；和5）模糊推荐系统。所提供的文献将使研究人员深入了解模糊机器学习研究的进展及其应用。</p><h1 id="i.介绍">I.介绍</h1><p>​  在动态的技术领域，机器学习已经深刻地改变了各个领域。它通过解码复杂的数据模式，推动人工智能的进步，并影响我们如何参与信息和理解计算系统的能力，以此来引领创新。然而，在大多数现有的机器学习方法中，具有不确定性的场景的准确性会受到影响，例如唯一可用的观测是不精确的或数据是有噪声或不完整的。此外，许多真实世界的数据集包含不确定的关系，传统的机器学习方法通常发现很难识别或处理这些结构。为了解决这些问题，研究人员已经使用模糊技术集成到机器学习中，称为模糊机器学习（FML）[1]作为一种解决方案，因为模糊技术能够成功地处理不确定性。FML系统融合机器学习算法与模糊技术，如模糊集[2]，模糊系统[3]，模糊聚类[4]，模糊关系[5]，模糊措施[6]，模糊匹配[7]，模糊优化[8]等等，建立新的模型更健壮的许多和各种类型的不确定性在现实世界的问题。<br/>​  FML在复杂和动态（不确定）的环境中是一个宝贵的盟友，展示了提高其功效的实质性优势。与传统的机器学习方法不同，通常基于模糊集[9]和模糊理论[10]的模糊技术擅长捕捉和导航动态场景中固有的细微不确定性。它们建模不确定性的内在能力使其能够优雅地适应动态环境中不断变化的模式。在传统模型可能会动摇或难以跟上步伐的情况下，模糊技术作为鲁棒的问题解决者出现，提供了真实数据[11]中固有模糊性的更准确的表示。</p><ol type="1"><li>模糊集[2]可以用来表示模糊或模糊的概念和数据，例如在语言变量、噪声或不完整数据和区间值数据中常见的数据。模糊集增强了算法在不确定和复杂情况下做出决策的能力，这在现实世界条件可能不可预测的应用中特别有用，如机器人或自动驾驶汽车。<br/>2.基于模糊规则的系统[3]可以提供一个透明和可解释的预测框架。基于模糊规则的系统使用语言规则来表示知识，因此，可以用来对系统所做出的决策进行解释。这在医疗诊断等应用程序中很有用。<br/>3.模糊聚类[4]是一种著名的聚类方法，它可以通过识别传统聚类方法不易识别的数据模式来改进机器学习算法。模糊集群不仅允许重叠的集群，而且还可以确定地处理可能不属于任何特定集群的数据点。这在图像识别等应用程序中很有用。<br/>4.模糊关系[5]可以提供变量或数据点之间关系的更灵活和微妙的表示。它们还可以捕获非线性关系，以使更准确和更有表现力的机器学习模型成为可能。此外，模糊关系在处理多模态数据或从多个来源组装的数据时是有用的，因为研究人员可以定义不同模态之间的模糊关系，从而得到一个更全面和准确的模型。</li></ol><p>​  在过去的十年里，在高质量的期刊和会议论文集上，已经有超过50万篇包含“模糊”和“机器学习”字样的文章。然而，这些文章都没有提供关于最近关于FML的文献的全面综述。以前在该地区进行的几次调查只对FML的某些子领域提供了有价值的见解。例如，Baraldi和Blonda[12]提供了模糊聚类的模式识别算法，而Skrjanc等人。[13]总结了基于进化模糊规则和神经模糊网络（NFNs）的模型，用于聚类、回归、识别和分类问题。此外，Zheng等人[14]回顾了最近在将深度学习模型与模糊系统融合方面的工作。此外，近十年来，在FML中出现了新的子领域，如模糊迁移学习和模糊数据流学习。提供一份调查报告来概述这些新的子领域是很重要的。由于这些原因，有必要进行一个新的、更全面的、更最新的FML调查。本文主要针对利用模糊技术提高机器学习方法性能的研究人员，特别是在涉及复杂或不确定因素的情况下。<br/>​  本研究中纳入的研究分为以下三个步骤。<br/>​  步骤1)确定并确定要搜索的适当的发布数据库集。我们搜索了科学直接数据库、ACM数字图书馆数据库、IEEEXplore数据库和SpringerLink数据库等著名的数据库。这些研究提供了关于机器学习和FML的研究论文的全面参考书目。<br/>​  步骤2)文章的初步筛选：第一次搜索是基于关键词。这些文章：a)提出了FML领域的新理论、算法或方法；或b)报告了一个围绕FML算法构建的应用程序。<br/>​  步骤3）对陈述结果进行过滤：将步骤2选择的文章分为五组，分为分为部分：a）模糊经典机器学习；b）模糊迁移学习；c）模糊数据流学习；d）模糊强化学习（RL）；e）模糊推荐系统。此时，我们对文章进行了最后的筛选（见图1）。如果一项研究证明足够，则保留：a)新颖性，即在过去十年内发表；b)影响，即它发表在高质量的期刊/会议上或高被引用。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223114336678.png"alt="image-20241223114336678" /><figcaption aria-hidden="true">image-20241223114336678</figcaption></figure><p>​  本文的主要贡献如下。</p><ol type="1"><li>全面总结了FML领域的发展和成果。这一领域的工作主要分为五大类进行讨论。<br/>2.本文分析了现实场景中传统机器学习方法的缺点，然后解释了FML如何被用于解决这些问题。所提供的见解旨在帮助研究人员了解FML研究及其应用的发展背景。<br/>3.它提供了一个对最先进的（SOTA）FML模型的批判性的讨论，并概述了未来研究的方向。</li></ol><h1 id="ii.fml的基本概念">II.FML的基本概念</h1><p>​  在本节中，我们将简要介绍一些相关的数学概念，来说明如何将模糊逻辑集成到迁移学习、数据流学习、RL和推荐系统中。这些概念应该可以帮助研究人员更好地理解以下章节中介绍的文章。</p><h2 id="a.模糊迁移学习">A.模糊迁移学习</h2><p>​  迁移学习[15]试图通过利用来自另一个领域（源）的知识来训练在一个领域（目标）中表现良好的模型，该领域与前一个领域具有不同的分布或学习任务。本节介绍了两个具有代表性的模糊迁移学习框架：1)基于模糊规则的[16]和2)基于模糊等价的[17]。</p><h3id="基于模糊规则的迁移学习框架16">1)基于模糊规则的迁移学习框架[16]：</h3><p>​  设<spanclass="math inline">\(\mathcal{S}=\{S^{1},S^{2},\cdot\cdot\cdot,\mathcal{S}^{N}\}\)</span>表示一组源域，其中<spanclass="math inline">\({\cal S}^{n}=\{({\bfx}_{i}^{\mathrm{S}_{n}},{y}_{i}^{\mathrm{S}_{n}})|{\bfx}_{i}^{\mathrm{S}_{n}}\in\chi^{n},{y}_{i}^{\mathrm{s}_{n}}\in\mathcal{Y}\}_{i=1}^{m_{n}}\)</span>，<spanclass="math inline">\(n\in [N]\)</span>和<spanclass="math inline">\(({\bfx}_{i}^{\mathrm{S}_{n}},{y}_{i}^{\mathrm{S}_{n}})\)</span>是n源域的第一对输入输出数据对。这里，<spanclass="math inline">\(\chi^{n}\subset\mathbb{R}^{p}\)</span>表示每个源域的特征空间，<spanclass="math inline">\(\mathcal{Y}\)</span>是一个响应空间（<spanclass="math inline">\(\mathcal{Y} =\{1,2，...，K\}\)</span>给定一个分类任务，<spanclass="math inline">\(\mathcal{Y}\subset\mathbb{R}\)</span>给定一个回归任务）。<spanclass="math inline">\(\mathcal{T}=\{\bf x_{i}^{T}|{\bfx}_{i}^{T}\in\chi^{T}\}_{i=1}^{m_{t}}\)</span>是未标记的目标域（对于无监督的场景），其中<spanclass="math inline">\(\chi^{T}\subset\mathbb{R}^{p}\)</span>是目标域的特征空间。在同质情况下，<spanclass="math inline">\(X^1\)</span>，...，<spanclass="math inline">\(X^N\)</span>和<spanclass="math inline">\(X^T\)</span>具有相同数量的特征，而在异构情况下，它们包含不同数量的特征。<br/>​  我们将<spanclass="math inline">\(\mathcal{R}=\{\mathcal{R}^{1},\mathcal{R}^{2},\dots,\mathcal{R}^{N}\}\)</span>表示为S构造的模糊规则空间，其中<spanclass="math inline">\(\mathcal{R}=\{r(v_{l}^{S_{n}},a_{l}^{S_{n}})\}_{l=1}^{l_{n}},n\in[N]\)</span>是<spanclass="math inline">\(\mathcal{S}^n\)</span>的第n个规则集。这里，规则<spanclass="math inline">\(r(v_{l}^{S_{n}},a_{l}^{S_{n}})\)</span>表示为<spanclass="math display">\[\begin{array}{l}{\mathrm{if~x}_{i}^{S_{n}\mathrm{~is~}A_{l}({\bfx}_{i}^{S_{n}},v_{l}^{S_{n}}),}}\\{\mathrm{then~}y_{i}^{S_{n}\mathrm{~is~}P_{l}({\bfx}_{i}^{S_{n}},a_{l}^{S_{n}}),}}\\ {l=1,2,\ldots..\cdotl_{n}.}\end{array}\]</span> ​  其中<spanclass="math inline">\(\mathcal{R}^T\)</span>表示得到的目标域<spanclass="math inline">\(\mathcal{T}\)</span>的模糊规则。<br/>​  最后，将<spanclass="math inline">\(\Phi=\{\Phi^{1},\Phi^{2},\dots,\Phi^{N}\}\)</span>表示为<spanclass="math inline">\(\mathcal{R}\)</span>（例如，线性组合）的结果，其中<spanclass="math inline">\(\Phi^{n}(\mathcal{R}^{n},\mu_{n}),n\in[N]\)</span>是<spanclass="math inline">\(\mathcal{R}^n\)</span>的第n个结果。因此，基于模糊规则的迁移学习的目的是利用<spanclass="math inline">\(\mathbf{D}=\{\mathcal{S},\mathcal{R},\Phi\}\)</span>的知识拟合目标域的数据，即得到<spanclass="math inline">\(\mathcal{R}^T\)</span>和<spanclass="math inline">\(\mathcal{R}^T\)</span>的结果。</p><h3id="基于模糊等价的迁移学习框架17">2)基于模糊等价的迁移学习框架[17]：</h3><p>​  与基于模糊规则的迁移学习不同，该框架应用源域和目标域特征之间的模糊等价关系来代替模糊规则。设<spanclass="math inline">\(\mathcal{U}=\{\mathcal{U}^{1},\mathcal{U}^{2},\dots,\mathcal{U}^{N}\}\)</span>表示<spanclass="math inline">\(\mathcal{S}\)</span>中特征的隶属函数空间，其中<spanclass="math inline">\(\mathcal{U}^{n}=\{\mu_{1}^{S_{n}},\mu_{2}^{S_{n}},\ldots,\mu_{m_n}^{S_{n}}\}\)</span>，<spanclass="math display">\[n\in[N]\]</span>，<spanclass="math inline">\(\mu_{i}^{S_{n}},i\in[m_{n}]\)</span>为<spanclass="math inline">\(\mathbf{x}_{i}^{S_{n}}\)</span>的隶属函数。</p><p><spanclass="math inline">\(\mathbf{R}_{S}^{M}=\{\mathbf{R}_{1}^{M},\mathbf{R}_{2}^{M},\dots,\mathbf{R}_{N}^{M}\}\)</span>表示为<spanclass="math inline">\(\mathcal{S}\)</span>上的模糊等价关系空间，其中，<spanclass="math inline">\(\mathbf{R}_{n}^{M},n\in[N]\)</span>，为<spanclass="math inline">\(S^n\)</span>上的模糊等价关系。这里，<spanclass="math inline">\(\mathbf{R}_{n}^{M}\)</span>是一个<spanclass="math inline">\(m_n\times m_n\)</span>矩阵（详见[17]和[18]） <spanclass="math display">\[({\bf R}_{n}^{M})_{i j}={\bf R}_{S_{n}}({\bfx}_{i}^{S_{n}},{\bfx}_{j}^{S_{n}};\mu_{i}^{S_{n}},\mu_{j}^{S_{n}}),\quadi,j\in[m_{n}]\]</span> ​  其中，<spanclass="math inline">\(R_{S_n}\)</span>是<spanclass="math inline">\(S_n\)</span>上的一个模糊等价关系算子。<br/>​  因此，基于模糊等价的迁移学习框架旨在利用从<spanclass="math inline">\(\mathbf{D}=\{\mathcal{S},\mathcal{U},\mathbf{R}_{S}^{M}\}\)</span>中获得的知识来拟合目标域内的数据。</p><h2 id="b.模糊数据流学习">B.模糊数据流学习</h2><p>​  数据流学习[19]，[20]，也被称为流挖掘，指的是一组技术和算法，旨在处理和分析以流的方式持续到达的数据。然而，在现实世界的场景中，数据的统计属性可能会随着时间的推移而变化，这使得以前精确的模型和算法会随着时间的推移而失效。这种现象被称为概念漂移[21]，[22]，[23]。下面是概念漂移的正式定义。</p><p>​  定义1（概念漂移[23]）：考虑一个时间段[0，t]和一组样本，记为<spanclass="math inline">\(S_{0,t} = \{d_0，...，d_t\}\)</span>，其中<spanclass="math inline">\(d_i=(X_i，y_i)\)</span>是一个观察结果（或一个数据实例）。<spanclass="math inline">\(X_i\)</span>是特征向量，<spanclass="math inline">\(y_i\)</span>是标签，<spanclass="math inline">\(S_{0,t}\)</span>遵循一定的分布<spanclass="math inline">\(\mathbb{F}_{0,t}(X,y)\)</span>。当<spanclass="math inline">\(\mathbb{R}_{0,t}(X,y)\neq\mathbb{R}_{t+1,\infty}(X,y)\)</span>时，在t+1时刻发生概念漂移，记为<spanclass="math inline">\(\existst:\mathbb{P}_{t}(X,y)\neq\mathbb{P}_{t+1}(X,y)\)</span>。<br/>​  因此，当概念漂移发生在t+ 1时，我们的目标是调整预测<spanclass="math inline">\(H_{t}=\arg\operatorname*{min}_{h\in\calH}\ell(h,X,y|(X,y)\in\mathbb{P}_{t}(X,y))\)</span>以适应新的分布Pt+1（X，y）。接下来，我们简要介绍了一个基于模糊聚类的漂移学习结构[24]，来展示如何将模糊逻辑集成到数据流学习中。</p><h2 id="c.-模糊强化学习">C. 模糊强化学习</h2><p>​  RL[27]是在学习者（称为代理）主动与环境交互以实现特定目标的场景下进行计划和学习的研究。代理的目标是制定积累奖励的最佳策略。它通过从它接收到的反馈中学习来做到这一点。RL已经成功地应用于各种现实世界的问题，如机器人控制[28]、游戏玩[29]和自动驾驶[30]。在本节中，我们将提供关于如何将模糊逻辑集成到RL中的信息。<br/>​  首先，模糊集可以用来表示RL中的状态、行动或奖励空间的不确定性。例如，模糊奖励信号[31]表示代理所接收到的奖励的不确定性或不精确性。此外，使用模糊逻辑将输入映射到控制动作的模糊控制器[32]可以集成到RL系统中，以处理不确定的或定性的控制决策。接下来，我们给出了一个模糊控制器的一般数学表达式。设X1，...，Xn为模糊控制器的输入变量，Y为表示控制动作的输出变量。与每个变量相关的模糊集表示为A1，...，An表示输入，B表示输出。设μAi（xi）表示输入Xi的模糊集Ai的隶属度函数，设μB(y)表示输出Y的模糊集B的隶属度函数。然后，定义从输入到输出的映射的通用模糊规则可以表示为<spanclass="math display">\[{\mathrm{if}}\,X_{i}\,{\mathrm{i}}\,{\mathrm{k}}\,A_{1j}\,{\mathrm{~and}}\cdot\cdot\cdot\mathrm{and}\,X_{n}\,{\mathrm{is}}\,A_{nj}\,,{\mathrm{then}}\,Y\,{\mathrm{is}}\,B_{j}\]</span>​  应用模糊规则后，进行去模糊化，得到一个清晰的输出值。<br/>​  此外，模糊推理系统可以用于RL[33]中的决策，例如基于代表不确定状态或奖励的模糊输入信号来确定下一步要采取的行动。例如，模糊q学习[34]扩展了q学习，通过结合模糊逻辑来处理不确定和不精确的状态-动作对。采用模糊规则和隶属度函数对q值进行了更新。</p><h2 id="d.-模糊重组系统">D. 模糊重组系统</h2><h1 id="iii.模糊经典机器学习">III.模糊经典机器学习</h1><p>​  经典的机器学习算法，如决策树、支持向量机（SVMs）和神经网络，在理论上和实践角度都取得了显著的成就。许多文章涉及到结合模糊技术与经典的机器学习算法，以克服不同类型的不确定性问题，如不完整的信息和不精确的观察。在本节中，我们总结了这些工作，并将这些技术分为两类：1)基于非深度学习的方法和2)基于深度学习的方法。</p><h2 id="a.-基于非深度学习的方法">A. 基于非深度学习的方法</h2><h2 id="b.-基于深度学习的方法">B. 基于深度学习的方法</h2><p>​  表一提供了与使用fnn的深度学习相关的SOTA文献的摘要。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223163949938.png"alt="image-20241223163949938" /><figcaption aria-hidden="true">image-20241223163949938</figcaption></figure><h1 id="iv.-模糊迁移学习">IV. 模糊迁移学习</h1><p>​  值得注意的是，大多数当前的迁移学习[158]方法在处理具有不确定性的真实情况时都有局限性，例如当只有少数标记实例可用时。为了克服这些问题，许多研究者已经转向了模糊集和模糊逻辑。<br/>​  现有的关于迁移学习的研究可以根据正在转移的知识类型进行分类。这些知识类别包括实例[159]、特征表示[160]、模型参数[161]和关系知识[162]。另外，根据所解决的问题设置，研究可以分为四类：多任务学习[163]、领域适应[164]、[165]、跨领域适应[166]和异构学习[167]。基于所使用的模糊技术，我们将我们最近的工作（2015-2023年）的总结分为三个领域。这些都是模糊集、模糊系统和模糊关系。表二总结了模糊迁移学习领域的最新成果。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223163902862.png"alt="image-20241223163902862" /><figcaption aria-hidden="true">image-20241223163902862</figcaption></figure><h2 id="a.基于模糊集的迁移学习">A.基于模糊集的迁移学习</h2><h2 id="b.基于模糊系统的迁移学习">B.基于模糊系统的迁移学习</h2><h2 id="c.-基于模糊关系的迁移学习">C. 基于模糊关系的迁移学习</h2><h1 id="v.-模糊数据流学习">V. 模糊数据流学习</h1><h1 id="vi.-模糊强化学习">VI. 模糊强化学习</h1><h1 id="vii.-模糊推荐系统">VII. 模糊推荐系统</h1><h1 id="viii.-未来研究方向">VIII. 未来研究方向</h1><h2 id="a.模糊经典机器学习">A.模糊经典机器学习</h2><h2 id="b.模糊转移学习">B.模糊转移学习</h2><h2 id="c.模糊数据流学习">C.模糊数据流学习</h2><h2 id="d.模糊强化学习">D.模糊强化学习</h2><h2 id="e.模糊重组系统">E.模糊重组系统</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A survey on deep learning-based image forgery detection</title>
      <link href="/A-survey-on-deep-learning-based-image-forgery-detection/"/>
      <url>/A-survey-on-deep-learning-based-image-forgery-detection/</url>
      
        <content type="html"><![CDATA[<p>A survey on deep learning-based image forgery detection</p><p>Fatemeh Zare Mehrjardi <span class="math inline">\(^a\)</span> , AliMohammad Latif <span class="math inline">\(^{*,a}\)</span> , MohsenSardari Zarchi <span class="math inline">\(^b\)</span> , RaziehSheikhpour $^c $ <br/>a Computer Engineering Department, YazdUniversity, Yazd, Iran <br/>b Computer Engineering Department, MeybodUniversity, Meybod, Yazd, Iran <br/>c Department of ComputerEngineering, Faculty of Engineering, Ardakan University, PO Box 184,Ardakan, Iran</p><h1 id="摘要">摘要</h1><p>​  图像被称为人类之间的交流工具之一。随着相机和手机等数字设备的发展和普及，在任何地方拍照都变得容易。图像被用于许多医学、法医学和司法应用中。有时图像被用作证据，所以数字图像的真实性和可靠性越来越重要。有些人通过添加或删除图像的部分来操作图像，这使图像无效。因此，图像伪造的检测和定位是很重要的。图像编辑工具的发展使这个问题成为计算机视觉领域的一个重要问题。近年来，许多不同的算法来检测图像和像素级的伪造。这些算法主要分为传统方法和深度学习方法。深度学习方法是人工智能科学的重要分支之一。该方法由于具有自动识别和预测过程，以及对几何变换和后处理操作的鲁棒性，已成为大多数计算机视觉问题中最流行的方法之一。本文综合研究了图像伪造类型、基准数据集、伪造检测中的评价指标、传统伪造检测方法、发现传统方法的缺点和局限性、深度学习方法的伪造检测方法以及该方法的性能。根据深度学习方法的扩展及其在大多数计算机视觉问题上的成功表现，我们在本研究中的主要重点是基于深度学习方法的伪造检测。本调查有助于研究人员获得伪造检测领域的深层背景。</p><h1 id="介绍">1.介绍</h1><p>​  数字图像可以通过各种技术来伪造。现有的数字图像伪造检测技术大致可分为两大类：主动/非盲法和被动/盲法[6–8]。这些类别及其子类别如图1所示。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218144724434.png"alt="image-20241218144724434" /><figcaption aria-hidden="true">image-20241218144724434</figcaption></figure><ul><li>主动方法：在主动方法中，对图像进行预处理，并将信息嵌入到原始图像中。一些主动方法的例子包括数字水印和数字签名。这种方法需要特殊的软件、硬件和原始图像来输入信息或从图像中提取信息。如果没有这些要求，这种方法将是不可能的和无效的。此外，在这种方法中，如果先决条件是[9]的，伪造检测就很容易了。</li><li>被动方法：在被动方法中，即通常被称为盲法，不需要对图像进行预处理。被动方法检测图像是否伪造，并通过分析图像的内容和结构来发现不一致性。这种方法比主动方法更受青睐，因为不需要任何先验信息[9]。被动方法的一些例子包括复制移动伪造、图像拼接、图像润饰和物体去除。</li></ul><h2 id="被动处理方法的类型">1.1.被动处理方法的类型</h2><p>​  本调查主要关注下面所描述的被动方法类型。</p><ul><li><p>​  移动伪造是最简单、最常见的图像处理方法之一。这种类型的伪造包括复制图像的一个或多个部分，并将其粘贴到同一图像的其他位置。该方法的目的主要是隐藏一些重要的信息或在图像中插入一些错误的信息。由于被复制的部分属于同一图像，因此在结构和纹理上与整个图像是兼容的。</p><p>​  除了复制和粘贴之外，还可以对图像执行几何变换，如旋转、缩放和后处理操作，如模糊、改变亮度、压缩和添加噪声。这些转换和操作导致被操纵的区域不容易被人类的[10,11]识别。图2显示了一个复制移动伪造图像的例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218144927743.png"alt="image-20241218144927743" /><figcaption aria-hidden="true">image-20241218144927743</figcaption></figure></li><li><p>​  图像拼接是一种将两个或多个图像组合成一个图像的方法。在生成的图像中，在拼接的位置有边缘和模糊区域。使用图像编辑工具，这些区域可以与图像合并，使人类视觉不会检测到伪造的[6,12]。图3为一个图像拼接的例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145015606.png"alt="image-20241218145015606" /><figcaption aria-hidden="true">image-20241218145015606</figcaption></figure></li><li><p>​  图像修饰是其他图像伪造方法中危害最小的伪造方法。该方法增强或减少了图像[13]上的像素特征。此外，它也是一种流行的照片编辑应用程序和杂志[2]的方法。这种方法通过增加或减少像素颜色等某些特征，使图像更具吸引力。图4显示了图像修饰的一个例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145056515.png"alt="image-20241218145056515" /><figcaption aria-hidden="true">image-20241218145056515</figcaption></figure></li><li><p>​  对象删除被称为破坏性伪造，因为它可能会改变图像的语义内容。[14]有两类对象删除技术：复制-移动和图像插入绘画。复制移动通过从主图像或另一个图像复制一个区域并粘贴到被删除对象的区域来删除所需的对象。复制移动由于其简单性，被广泛用于对象删除。</p><p>​  图像中的绘制最初是为了恢复受损的信息和去除旧照片中的划痕。此方法通过用周围的像素填充对象的位置来删除对象。图像内画可以同时保持纹理和结构的一致性。图像插入图例如图5所示。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145218284.png"alt="image-20241218145218284" /><figcaption aria-hidden="true">image-20241218145218284</figcaption></figure></li></ul><p>​  伪造检测是近十年来研究人员最有趣的课题之一。因此，许多研究主要分为两类：传统学习方法和深度学习方法。在[2,16]中可以找到关于这两种方法的一些优秀的综述。尽管在伪造检测领域有适当的综述文章，但我们的综述文章的目的是对两种伪造检测方法进行综合检查，以比较和发现它们的优缺点和挑战。另一方面，根据深度学习方法表现良好在大多数问题如果硬件条件和适当的数据集，我们文章的另一个重点是检查伪造检测深度学习从两个新的和不同的方面：使用不同的深度学习方法和不同的深度学习架构。<br/>​  本文的其余部分分为以下几个部分：第2节给出了不同的深度学习架构和用于伪造检测的各种评估指标的简要背景。第3节解释了基准数据集在复制-移动、拼接和不绘制伪造检测方面的细节。第4节对传统的伪造检测方法及其细节进行了解释和比较。第5节对具有不同策略和深度学习架构的基于深度学习的伪造检测进行了全面的回顾。最后，在第6节中给出了所有伪造检测方法的结论、其缺点和优点、存在的挑战和未来的建议。</p><h1 id="背景">2.背景</h1><p>​  目前，大多数伪造检测方法都是基于机器学习和深度学习的方法。本节概述了机器学习算法、深度学习架构和根据回顾的研究得出的评估指标。</p><h2 id="机器学习">2.1.机器学习</h2><ul><li>支持向量机（SVM）</li><li>决策树（DT）</li><li>K-最近的邻居（KNN）</li></ul><h2 id="深度学习dl">2.2.深度学习（DL）</h2><h3 id="cnn">2.2.1. CNN</h3><ul><li>卷积层</li><li>池化层</li><li>全连接层</li></ul><h3 id="rnn">2.2.2. RNN</h3><p>​  递归神经网络，也称为重复神经网络，用于序列数据处理。这些网络有一个反馈层，在其中网络输出与下一个输入一起返回到网络。递归神经网络的内部记忆回忆起它之前的输入，并使用这个记忆来处理一系列的输入。递归神经网络具有短期的内部记忆，这与长期依赖性的梯度消失问题有关。因此，不同类型的递归神经网络被引入于长期依赖问题。长短期记忆（LSTM）网络是一种著名的递归神经网络，它试图模拟一个组件和它的前身[23,24]之间的长依赖关系。</p><ul><li><p>LSTM是一种旨在解决消失梯度问题的RNN网络之一。它由四个组件组成，分别称为单元格、输入门、输出门和忘记门。该单元格是一个随时间的推移而存储和更新信息的地方。单元格内的值由三个门[24]进行更新。图7说明了LSTM的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145635177.png"alt="image-20241218145635177" /><figcaption aria-hidden="true">image-20241218145635177</figcaption></figure><p>LSTM网络的基本概念是单元及其相关门的状态；单元是一条高速公路，沿着信息链。这个单元格可以被认为是网络内存。盖茨以更新的状态保存信息。他们决定哪些信息进入单元格，并学习在网络训练[25]期间应该存储或忘记哪些信息。</p></li></ul><h3id="区域卷积神经网络r-cnnregional-convolutional-neural-network">2.2.3.区域卷积神经网络（R-CNN，Regional convolutional neural network）</h3><p>​  图像中的物体识别和定位是计算机视觉中最基本和最具挑战性的概念之一。Girshick等人[26]提出了一种名为R-CNN的目标识别和分割系统。该系统使用多层卷积计算区分特征，对图像区域进行分类，并在识别的对象周围划定边界。图8显示了R-CNN网络的步骤。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218150308547.png"alt="image-20241218150308547" /><figcaption aria-hidden="true">image-20241218150308547</figcaption></figure><p>​  首先，在R-CNN网络中，使用选择性搜索算法识别出2000个区域。调整所有区域的大小，进入预先训练好的卷积神经网络，然后从区域中提取特征。接下来，使用这些特征向量和支持向量机分类器来识别包含对象的区域。最后，通过边界盒回归[26]来确定对象的边界。</p><h3 id="fast-r-cnn">2.2.4. Fast R-CNN</h3><p>​  R-CNN网络对图像的2000个区域运行，使得速度非常慢。Girshick[27]，改进了R-CNN算法，使神经网络应用于整个图像一次。这个网络被称为快速R-CNN。图9显示了FastR-CNN网络的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218150158867.png"alt="image-20241218150158867" /><figcaption aria-hidden="true">image-20241218150158867</figcaption></figure><p>​  如图9所示，首先将整个图像和一组目标建议给给FastR-CNN网络作为输入。该网络对整个图像进行处理，并生成一个连续的特征图。接下来，使用感兴趣区域（ROI)池化层从每个对象提议的特征映射中提取一个固定长度的特征向量。每个特征向量进入完全连接的层序列。在网络的最后，有两个并行层：一个softmax层，指定每个对象的类，和一个线性回归层，输出四个值作为每个检测对象[28]的限制边界。</p><h3 id="faster-r-cnn">2.2.5. Faster R-CNN</h3><p>​  Ren等人在2015年发布了FastR-CNN的改进版本[29]。该网络与RastR-CNN网络的主要区别是，RastR-CNN网络使用选择性搜索算法来创建感兴趣的区域，而更快的R-CNN网络使用区域建议网络（RPN）层来实现这个目的。特征映射给给RPN层，该层输出建议对象及其分数。<br/>​  在RPN层中，一个滑块窗口将应用于特征映射。该窗口包含具有k个不同形状和大小的锚点的方框。锚点被放置在整个图像中。最后，该层将所提出的对象输出为具有不同大小和形状的盒子，并将它们发送到ROI层。ROI层将所提议的盒子转换为相同的大小，并从每个锚点中提取一个特征映射。最后，将特征映射给出到一个全连接的层，每个对象的类别由一个softmax层来确定。图10显示了更快的R-CNN网络的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218151029473.png"alt="image-20241218151029473" /><figcaption aria-hidden="true">image-20241218151029473</figcaption></figure><h3 id="mask-r-cnn">2.2.6. Mask R-CNN</h3><p>​  2017年，Kaim等[30]人引入了一种名为MaskR-CCNN的新架构，这是一种更快的R-CNN的改进架构。在这个体系结构中，除了指定对象及其类的边界外，对象的像素还与每个边界中的其他像素分开。换句话说，在每个绑定中都会创建一个二进制掩码。这是通过在FasterR-CNN网络中添加一个新的分支来实现的，这样每个被ROI层检测到的边界都被给出给卷积层。然后，二值掩码指定对象像素的值为1，其他对象像素的值为零。图11显示了MaskR-CNN网络的结构。</p><h3 id="you-only-look-once-yolo">2.2.7. You only look once (YOLO)</h3><h3 id="单镜头探测器ssdsingle-shot-detector">2.2.8.单镜头探测器（SSD，Single shot detector）</h3><h3 id="自编码器">2.2.9. 自编码器</h3><h3 id="生成对抗网络gan">2.2.10.生成对抗网络（GAN）</h3><h2 id="评价指标">2.3.评价指标</h2><p>​  评估指标是用于描述模型或算法在基准数据集上的性能的工具。伪造检测方法的评价基于图像伪造检测和像素伪造检测[35,36]两个层次。在第一种方法中，整个图像的标签应分为两类：伪造和健康，而在第二种方法中，像素的标签应分为这两类。<br/>​  当研究人员使用没有地面真实图像的数据集时，计算图像级别的评估指标。由于缺乏地面真实图像，因此无法在像素级进行评估。使用混淆矩阵计算了两个级别的评估度量。混淆矩阵由四个组成部分组成。这些组件基于像素级别的定义如下：</p><ul><li>TP（真阳性）：伪造像素正确检测为伪造像素。</li><li>FP（假阳性）：检测到错误的真实像素为伪造像素。</li><li>FN（假阴性）：伪造的像素被错误地遗漏为伪造的像素。</li><li>TN（真阴性）：正确检测到的真实像素为真实像素。</li></ul><p>​  注：对于图像级别，这些组件是图像。在下面，一些基于像素级别的标准评估指标，如准确率（Acc）、召回率(R)、精度(P)、F1评分、假阳性率（FPR）和使用混淆矩阵的组成部分定义IoU[9,35]。</p><h1 id="基准数据集">3.基准数据集</h1><h2 id="micc">3.1. MICC</h2><h2 id="comofod">3.2. CoMoFod</h2><h3 id="casia">3.3. CASIA</h3><h2 id="coverage">3.4. COVERAGE</h2><h2 id="inpainting-datasets">3.5. Inpainting datasets</h2><h1 id="传统的伪造检测方法">4.传统的伪造检测方法</h1><p>​  传统的复制移动伪造检测（CMFD）方法主要可以分为两类：基于块的方法和基于关键点的方法。这些方法包括伪造检测的三个连续步骤：特征提取、特征匹配和伪造定位[9,58]。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218151606505.png"alt="image-20241218151606505" /><figcaption aria-hidden="true">image-20241218151606505</figcaption></figure><p>​  图16显示了两种传统的CMFD方法[59]的步骤。对每个步骤的描述如下所示。</p><ol type="1"><li><p>预处理步骤：一些伪造检测算法的第一步是预处理。在这个步骤中，执行一些操作，如将彩色图像转换为灰度、调整图像大小、降噪和转换为其他颜色空间等，即执行[59]。</p></li><li><p>两种伪造检测方法：</p><p>​  基于块的方法：在基于块的方法中，将图像分为重叠或不重叠的矩形或圆形块。在特征提取步骤中，采用不同的方法从离散余弦变换（DCT）系数、主成分分析（PCA）、奇异值分解（SVD）、定向梯度直方图（HOG）、Hu矩、局部二值模式（LBP）、泽尼克矩、极复指数变换（PCET）[60–63]等所有块中提取特征向量。</p><p>​  在特征匹配步骤中，采用排序、相关和欧氏距离的方法来适应相似的特征向量。在伪造定位步骤中，计算了匹配对之间的几何变换。这个计算有助于消除任何不匹配的对。随机样本共识（RANSAC）被广泛应用于仿射同调性的准确估计。它导致了最小的误差，并过滤了一些不匹配的对[9,64]。基于块的方法的缺点是计算复杂度高，对一些几何变换[65]的性能较差（图17）。</p><p>​  基于关键点的方法：在基于关键点的方法中，整个图像不需要被分割成块。该方法在特征提取步骤中使用角和边，每个属性由一个描述符表示。关键点的提取使用了各种算法，如尺度不变特征变换（SIFT）[67]，加速鲁棒特征（SURF）[68]，以及不分割图像的加速分段测试（FAST）[69]中的特征。</p><p>​  在特征匹配步骤中，采用了聚类、欧氏距离和最近邻域等不同的方法。如果关键点匹配，则将检测到伪造文件。在伪造定位步骤中，类似于基于块的方法中的此步骤，计算匹配对之间的几何变换，并消除任何不匹配对。与基于块的方法相比，该方法的计算量较少，且对几何变换具有鲁棒性。然而，在均匀区域的伪造检测中，识别相似的图像为伪造图像，以及根据关键点是该方法[6,70,71]存在的问题（图18）。</p></li><li><pre><code>后处理：在基于块和关键点的方法中，伪造的位置可能不能被准确地本地化。为此，在后处理步骤中，使用了诸如侵蚀和分层等形态学操作来寻找伪造的[71]的确切位置。</code></pre></li></ol><p>​  传统的伪造检测方法也有一些局限性。这些方法包括三个步骤，每个步骤都是单独完成的，并且有许多参数，必须手动调整这些参数。这些方法大多是在具有高性能的特定数据集上进行的调优，但它们不适用于其他数据集[9]。<br/>​  下面，我们回顾了一些基于块、基于关键点和混合方法的研究。这些审查的结果将在下一小节中给出。图19概述了传统的伪造检测方法和对每种方法进行的研究。</p><h2id="基于块的伪造检测的一些研究概述">4.1.基于块的伪造检测的一些研究概述</h2><h2id="基于关键点的伪造检测的研究概述">4.2.基于关键点的伪造检测的研究概述</h2><table><colgroup><col style="width: 4%" /><col style="width: 24%" /><col style="width: 21%" /><col style="width: 24%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>Year</th><th>Summary</th><th>Dataset</th><th>Performance</th><th>Metrics</th></tr></thead><tbody><tr class="odd"><td>2020 [89]</td><td>预处理：将RGB图像转换为灰度图像，并对灰度图像应用DWT算法。<br />特征提取：在2级DWT上的SURF+很活跃。<br />特征匹配：2个NN+DBSCAN。<br />后处理：RANSAC。</td><td>CoMoFod, <br />MICC-F220, <br />MICCF2000</td><td>优点：该方法对几何变换和后处理操作具有鲁棒性。它可以检测到多个伪造物。该方法采用DBSCAN聚类来减少搜索空间，减少错误匹配，并降低计算成本。<br />缺点：该方法的性能较差，在锻造区域中缩放、平滑、亮度变化过多。</td><td>R=91.24%, <br />P=95.98%, <br />FPR=9.82%, <br />TPR=96.68%</td></tr><tr class="even"><td>2020 [81]</td><td>特征提取：SIFT。<br />特征匹配：反转G2NN。<br />后处理：使用两种类似的措施，如HAC和j-链接，消除不匹配。</td><td>GRIP, FAU</td><td>优点：该方法使用聚类算法降低了时间复杂度。它在简单伪造、几何变换伪造和小规模后处理伪造等方面都具有鲁棒性。<br />缺点：该方法不稳定，对大规模伪造区影响较差。</td><td>R=99.67%, <br />P=99.79%, <br />F1=99.72%</td></tr><tr class="odd"><td>2020 [91]</td><td>特征提取：SURF+旋转局部二进制模式（RLBP）。<br />特征匹配：G2NN+欧氏距离，层次聚类。<br />后处理：RANSAC</td><td>COVERAGE</td><td>优点：该方法对几何变换、模糊和JPEG压缩具有鲁棒性。</td><td>ACC=70.5%</td></tr><tr class="even"><td>2021 [83]</td><td>预处理：将对比限自适应直方图均衡（CLAHE）算法应用于RGB图像，提高平滑区域的特征检测。<br />特征提取：SIFT。<br />特征匹配：FANN，DBSCAN集群，<br />后处理：使用RANSAC和GORE删除异常值</td><td>MICC-F220</td><td>优点：它对几何变换、模糊、压缩和添加噪声具有鲁棒性。它可以处理具有最少的错误匹配的多个复制移动伪造。</td><td>TPR=100%, <br />FPR=3.63%, <br />F1=97.56%</td></tr><tr class="odd"><td>2022 [94]</td><td>特征提取：分别使用SIFT从原始图像和缩放图像中提取关键点并进行合并。<br />特征匹配：通过比较从关键点中获得的筛选描述符和作为第二个关键点匹配的处理关键点集来检测类似的关键点。<br />后处理：采用双自适应滤波法进行去除，采用凸包查找法进行伪造定位</td><td>CoMoFod, <br />MICC-F220, <br />CASIA, <br />COVERAGE</td><td>优点：该方法使用第二个关键点匹配来匹配更多的SIFT关键点，并检测单个和多个CMFD。双自适应过滤可以更能自适应地去除错误的关键点匹配，并更精确地定位伪造区域。</td><td>R=94.5%, <br />P=86.7%, <br />F1=90.4%</td></tr></tbody></table><h2id="基于块和基于关键点的伪造检测方法的研究综述">4.3.基于块和基于关键点的伪造检测方法的研究综述</h2><p>表6给出了混合伪造检测研究的总结</p><table style="width:100%;"><colgroup><col style="width: 5%" /><col style="width: 33%" /><col style="width: 5%" /><col style="width: 33%" /><col style="width: 22%" /></colgroup><thead><tr class="header"><th>Year</th><th>Summary</th><th>Dataset</th><th>Performance</th><th>Metrics</th></tr></thead><tbody><tr class="odd"><td>2020 [97]</td><td>预处理：将图像分割成正方形的方块。<br />特征提取：SIFT+冲浪。<br />特征匹配：具有倒数空间距离的凝聚式层次聚类方法。<br />后处理：RANSAC。</td><td>MICC-F220</td><td>优点：该方法对几何变换具有良好的鲁棒性。结合SIFT和SURF算法，对平滑图像和小伪造区域具有良好的性能</td><td>R=92.5%, <br />FPR=8.9%, <br />F1=91.7%</td></tr></tbody></table><h1 id="具有深度学习的伪造检测">5.具有深度学习的伪造检测</h1><p>​  近年来，深度学习方法已经在计算机视觉中被考虑，如伪造检测。深度学习方法可以自动从数据中提取层次特征。该方法学习了丰富的语义表示，避免了手工特征开发。深度学习方法的最大问题是训练过程[104]需要大量的数据。随后，我们提出了一些解决这个问题的解决方案。</p><h2 id="深度学习中的训练方法">5.1.深度学习中的训练方法</h2><p>​  通过对深度学习网络进行伪造检测的研究，我们发现有两种流行的方法：1)端到端网络和2)预训练网络。在第一种方法中，从输入层到最终层的所有层参数都与一个大数据集一起进行训练。在第二种方法中，由于在大多数问题中缺乏合适的数据集，因此在新问题中使用具有大数据集的预训练网络作为起点或特征提取部分。图20提供了这两种方法及其子分支的概述。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218152341125.png"alt="image-20241218152341125" /><figcaption aria-hidden="true">image-20241218152341125</figcaption></figure><p>​  接下来，将描述这些子分支。</p><h3 id="从头开始训练">5.1.1.从头开始训练</h3><p>​  开始这种方法需要大量的数据和时间来训练网络。它在控制架构和参数方面很重要，并将创建更有效的网络。由于在一些计算机视觉研究中没有大型数据集，使用这种方法几乎是不切实际和效率低效的。Ansari等人[105]使用较小的VGGNet和MobileNetV2.0来检测图像级别的复制移动伪造。他们使用MICC-F2000和CASIAV2.0数据集和增强技术来训练网络。在另一个例子[106]中，VGG16网络被用于检测拼接伪造。使用了CASIAv2.0数据集和训练网络的图像补丁。</p><h3 id="迁移学习">5.1.2.迁移学习</h3><p>​  第二种方法是迁移学习。该方法以某一任务的预先训练模型为起点，在新任务[104]中使用少量的训练样本对模型的参数进行轻微的再训练。这些预先训练好的模型如AlexNet、VGGNet、谷歌lenet、ResNet都是用ImageNet等大型训练数据集进行训练的，具有良好的泛化能力。迁移学习的优点是节省了训练时间，而且不需要大量的数据。深度学习体系结构试图检测初始层中的边和角，中间层中的形状，以及最终层[107]中的任务的具体特征。在迁移学习中，使用预训练网络的初始层和中间层来处理新问题，负责识别边缘和形状，并根据期望的问题和数据集进行调整。<br/>​  利用迁移学习方法检测了复制-移动和拼接方法中的伪造图像。例如，在[108]中，在CASIAv2.0数据集上使用了三个预先训练的模型，VGG16、VGG19和ResNet152。评价结果表明，初始层的特征对伪造图像的检测是有用的。在另一个例子中，迁移学习方法也被用于检测插入绘制方法[109]中的伪造像素。为此目的，首先，对图像应用了一个高通滤波器来突出显示伪造区域。接下来，将图像提供给预先训练过的CNN和ResNet网络的四个初始层。最后，放置一个上采样层，并显示一个包含伪造像素的二进制图像。</p><h3id="使用预先训练好的模型作为特征提取器">5.1.3.使用预先训练好的模型作为特征提取器</h3><p>​  预先训练好的模型可以作为特征提取器，从新的样本中提取出有意义的特征。可以在预先训练的模型上添加新的分类器对特征图进行分类。或者，这些特征图可以作为特征向量，机器学习算法可以用来对它们进行分类。在这种方法中，不需要对整个模型进行重新训练。基本层已经包含了对于对不同任务进行分类通常有用的特性。该方法可用复制移动和拼接方法检测图像[110]和像素[111]级的伪造。<br/>​  预先训练过的网络，如AlexNet[112]、VGGNet [113]、ResNet [114,115]和盗梦空间v3.0[116]已经被用来获取特征向量。然后，可以使用不同的机器学习算法，如SVM、KNN、决策树、朴素贝叶斯和浅层网络进行分类。在某些情况下，使用决策融合技术[114]来对这些特征向量进行分类。一些研究人员使用基于块的方法和深度学习方法[110]相结合，或将人工特征与从深度学习[115]中获得的特征相结合来检测伪造。<br/>​  例如，在[117]中，使用不同的预训练模型如VGG16、MobileNet和概念v3作为特征提取器，研究不同的机器学习算法如KNN、决策树、朴素贝叶斯和随机森林作为分类器，用于复制移动伪造检测。在另一个例子[114]中，使用从预先训练好的ResNet中提取的特征向量和使用KNN、SVM和朴素贝叶斯分类器的决策融合方法进行拼接伪造检测。在另一个例子[115]中，使用DWT和LBP算法结合手工特征，并从ResNet模型中提取特征向量。然后利用浅层网络对这些组合特征进行分类，并检测拼接图像。接下来，表7总结了使用这种方法进行的几个伪造检测研究。</p><h2id="使用不同的深度学习架构进行伪造检测">5.2.使用不同的深度学习架构进行伪造检测</h2><p>​  在本节中，我们全面回顾了用于伪造检测中的各种深度学习架构。图21显示了用于伪造检测的五种著名架构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218154514179.png"alt="image-20241218154514179" /><figcaption aria-hidden="true">image-20241218154514179</figcaption></figure><p>​  在下面，我们对在每个架构中进行的一些研究进行了回顾，这些调查的结果将在下面的小节中给出。</p><h3id="利用卷积神经网络进行伪造检测">5.2.1.利用卷积神经网络进行伪造检测</h3><h3id="利用目标检测网络进行伪造检测">5.2.2.利用目标检测网络进行伪造检测</h3><h3id="基于自动编码器网络的伪造检测">5.2.3.基于自动编码器网络的伪造检测</h3><h3id="基于生成式对抗网络的伪造检测">5.2.4.基于生成式对抗网络的伪造检测</h3><h3id="基于递归神经网络的伪造检测">5.2.5.基于递归神经网络的伪造检测</h3><h2 id="讨论">5.3.讨论</h2><p>​  对伪造检测的研究分为图像伪造检测和像素伪造检测两个层次。第3节中描述的所有数据集都用于图像伪造检测研究，并且只使用具有地面真实值的数据集，如CoMoFod、MICC-F600、GRIP、NIST16等。由于几何变换和后处理操作的多样性，CoMoFod是一个用于在可用的复制移动伪造数据集中评估复制移动伪造检测方法的合适数据集。<br/>​  所有的深度学习架构都用于图像伪造检测级别。然而，具有迁移学习技术的卷积神经网络几乎总是用于图像伪造检测。在像素伪造检测的研究中，经常采用对象检测网络和自动编码器网络。这些网络使用具有地面真实值的数据集来训练和预测二值掩模。由于需要大量的数据进行训练，缺乏合适的伪造数据集，以及健康图像和伪造图像的不平衡，生成对抗网络通常被用作单类分类方法。这些网络通常只用健康的数据进行训练，将伪造的图像或补丁识别为不一致的图像。递归神经网络通常与其他网络一起使用来检测像素或补丁之间的不一致性。根据上述主题，我们回顾了2019-2023年的图像伪造检测研究，并将其分为三组。图33-35显示了这三组人。</p><h1 id="结论">6.结论</h1><p>​  本文在整个图像和像素水平上，使用基于块、基于关键点和深度学习的方法，研究了不同的伪造方法、伪造数据集和伪造检测方法。研究表明，基于块的方法的计算时间和复杂度较高，且在一些几何变换和后处理中表现较差。基于关键点的方法对几何变换具有更强的鲁棒性。由于缺乏足够的关键点，该方法对小伪造区域的检测效果较差。基于块和基于关键点的方法在预处理、特征提取、特征匹配、发现相似区域和后处理等方面都有不同的步骤。这些步骤的参数必须单独调整，以检测伪造。与前两种方法不同的是，深度学习在不同架构的训练过程中自动执行伪造检测过程。伪造检测的深度学习最重要的问题是缺乏合适的数据集和长时间的训练。为了解决这个问题，研究人员在大数据集上使用了预先训练过的网络和迁移学习技术。<br/>​  本研究回顾了各种伪造检测方法，并帮助研究人员熟悉新的想法和挑战。伪造检测是一个非常具有挑战性的问题，它仍然是一个开放的研究课题。在平滑和小区域中的伪造检测、使用几何变换和各种后处理操作的伪造检测，构建合适的伪造数据集，多重伪造检测，使用传统和深度学习方法的组合进行伪造检测，以及在各种伪造数据集上推广伪造检测方法，都是伪造检测领域仍需研究的案例。例如，根据研究，已经提出了许多方法来分别检测所有的伪造类型。每种方法都有其参数和设置，并已在特定的伪造品和数据集上进行了评估。因此，当伪造或数据集类型改变时，所提出的方法失去了有效性。有一些方法可以同时检测所有类型的伪造品。在未来的工作中，结合深度学习方法、基于块的方法、基于关键点的方法和边缘检测等不一致检测算法，可以同时识别所有类型的伪造。<br/>​  另一方面，没有一个合适的数据集，其中包含足够的图像，包括各种伪造、几何变换和后处理操作。因此，每种方法都关注于一个特定的伪造和数据集。在未来的工作中，可以使用生成对抗网络来生成合适的数据集，包括所有类型的伪造、几何变换、后处理操作和用于像素伪造检测的地面真实掩模。<br/>​  检测非常小的伪造区域和利用几何变换检测伪造是该领域的其他问题。在未来的工作中，可以使用不同的图像增强算法，如调整图像的直方图，结合几种关键点提取算法来增加关键点。为了解决过度几何变换的伪造检测问题，可以结合使用几种不同的深度学习架构、目标检测网络或更抗几何变换的胶囊网架构。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPL</title>
      <link href="/SPL/"/>
      <url>/SPL/</url>
      
        <content type="html"><![CDATA[<p>多任务网络上的分布式安全聚类与估计</p><h1 id="摘要">摘要</h1><p>​  近年来，分布式安全估计问题得到了广泛的研究。然而，在多任务网络中，恶意信息和任务之间的不确定关系对现有的相关算法提出了挑战。为了获得可靠的估计，提出了分布式安全聚类和估计（DSCE，DistributedSecure Clustering andEstimation）算法。该算法包括一个具有自适应组合系数的扩散滤波器，以及一个维数检测器。通过估计节点间的最大任务间间隙，完成了恶意信息的自适应区分过程。此外，提出了安全聚类方案，通过根据检测结果动态采用防御策略来减轻攻击者的影响。仿真结果证明了该算法对攻击的鲁棒性。</p><h1 id="i.介绍">I.介绍</h1><p>​  分布式参数估计问题近年来得到了广泛的讨论。目标是基于分布在网络中的节点集合的测量值和回归向量来近似最优估计。这些节点与相邻节点进行协同估计，以提高结果的准确性。此外，还确定了三种主要的协作方法：增量方法[1]、共识方法[2]和扩散方法[3]。其中，扩散方法被认为是最突出的。<br/>​  当考虑到网络攻击的存在时，来自节点的数据通常是伪造的，从而导致估计过程的中断。因此，没有适当防御机制的算法通常会导致性能下降。为了解决这一挑战，我们提出了各种安全估计算法，如：[4]-[9]所示。在[4]中，采用了一种基于声誉的检测器来识别恶意传感器。在[5]、[6]中，Hua等人分别设计了一种基于kl-发散的检测机制和交叉验证方法来区分被攻击的节点。在[9]中，建立了一个参考子系统来检测恶意信息，并提供可靠的替换信息。此外，一些研究也关注于多任务场景下的问题。在[10]中，Wang等人提出了一种基于贝叶斯的估计器来在多任务对抗网络中获得可靠的估计。在[11]中，实现了局部-离群值-因子（LOF，Local-Outlier-Factor）来测量和定位多任务环境下的恶意估计。<br/>​  上述研究为解决单任务和多任务环境下的安全估计问题做出了贡献。然而，很少有研究考虑先前聚类信息不可用的多任务环境，这在实际应用中是一个更普遍和具有挑战性的场景。<br/>​  值得注意的是，在多任务网络中，集群信息的不确定性导致节点之间的协作不协调，导致相关算法的性能下降。此外，任务间邻居之间未知的内在差异混淆了检测过程，从而增加了区分被攻击节点的难度。<br/>​  本文主要考虑了FDI攻击和部分FDI攻击的存在性，并提出了分布式安全聚类和估计（DSCE）算法。在该算法中，通过具有自适应组合系数的扩散滤波器（[12]中的AC-dLMS滤波器），得到了一个锚定估计，作为可靠的参考。通过对锚定估计的维数检测过程，识别出可靠的数据。此外，还设计了一种自适应更新节点阈值的方法，基于评估任务间间隙造成的最大差异。随后，采用了安全聚类方案。通过根据检测结果选择不同的防御策略，在聚类过程中拒绝恶意信息。最后，将来自安全簇内邻居的中间估计结合在一起以获得安全估计。仿真结果表明，该算法在抑制FDI攻击和部分FDI攻击方面是可行的。</p><h1 id="ii.准备工作">II.准备工作</h1><p>​  本节分别简要介绍了在无攻击环境下的多任务扩散LMS（multi-dLMS）算法和攻击模型。</p><h2 id="a.-多任务环境下的扩散lms算法">A. 多任务环境下的扩散LMS算法</h2><p>​  本文简要考虑了一个具有N个节点的多任务模型，并将其划分为Q个簇。<br/>​  在每个时刻t，每个节点接收到<spanclass="math inline">\(\{d_{k,t},\mathbf{u}_{k,t}\}\)</span>，其中<spanclass="math inline">\(d_{k,t}\)</span>表示标量测量值，<spanclass="math inline">\(\mathbf{u}_{k,t}\)</span>表示一个L维的输入回归向量。特别是它们之间的线性关系如下：<spanclass="math display">\[d_{k,t}=\mathbf{u}_{k,t}^{T}\mathbf{w}_{k}^{o}+v_{k,t},\]</span>​  其中<spanclass="math inline">\(\mathbf{w}_{k}^{o}\)</span>是节点k的未知l维最优估计，<spanclass="math inline">\(v_{k,t}\)</span>是噪声项。<br/>​  不同集群之间的关系表示如下：<span class="math display">\[\begin{array}{ll}\mathbf{w}_{k}^{o} =\mathbf{w}_{C(q)}^{o}, &amp; \text{if } k \in \cal C(q);\\\mathbf{w}_{C(p)}^{o} = \mathbf{w}_{C(q)}^{o}, &amp; \text{if } \calC(p) \text{ and } \cal C(q) \text{ are connected;}\end{array}\]</span>​  其中，<spanclass="math inline">\(\mathbf{w}_{C(q)}^{o}\)</span>为第p个簇中节点的索引集。<br/>​  主要目标是通过与邻居通信来近似最近的最优估计。本部分主要讨论了采用适应后再组合（ATC，adapt-then-combine）策略的多任务dLMS算法。这些方程式说明如下：<span class="math display">\[\varphi_{k,t}={\bfw}_{k,t-1}+\mu_{k}\sum_{\ell\in{\cal N_{k}}\cap{\calC}_{k}}[\cal{c}_{\ell,t}(d_{\ell,t}-{\bf u}_{\ell,t}^{T}{\bfw}_{k,t-1})]\]</span></p><p><span class="math display">\[{\bf w}_{k,t}=\sum_{\ell\in{\calN_{k}}\cap{\cal C}_{k}}a_{\ell,k}\varphi_{k,t},\]</span></p><p>​  其中<span class="math inline">\(\varphi_{k,t}\)</span>和<spanclass="math inline">\(\mathbf{w}_{k\cdott}\)</span>分别为节点k在时间t时的中间估计和瞬时估计，<spanclass="math inline">\(\mu_k\)</span>为步长，<spanclass="math inline">\({\cal N}_{k}\)</span>为节点k的邻居集合，<spanclass="math inline">\({\calC}_{k}\)</span>为节点k所属的聚类集合。非负系数<spanclass="math inline">\(a_{\ell,k}\)</span>、<spanclass="math inline">\(c_{\ell,k}\)</span>分别为左随机矩阵A和右随机矩阵C的<spanclass="math inline">\((\ell,k)\)</span>级别实体，即： <spanclass="math display">\[\mathbf{A}^{T}\mathbf{1}_{N}=\mathbf{1}_{N},a_{\ell,k}=0\;\mathrm{if}\;\ell\not\in\mathcal{N}_{k},\]</span></p><p><spanclass="math display">\[\mathrm{C1}_{N}\,=\,{\bf1}_{N},c_{\ell.k}\,=\,0\mathrm{~if~}\ell\not\in\mathcal{N}_{k},\]</span></p><p>​  其中，<spanclass="math inline">\({\bf1}_{N}\)</span>表示带有单位项的列向量。</p><h2 id="b.攻击模型">B.攻击模型</h2><p>​  本文考虑了FDI攻击和部分FDI攻击，具体如下所示：</p><h3 id="fdi攻击">1) FDI攻击</h3><p>​  对于FDI攻击，注入恶意信息篡改每个节点的测量值<spanclass="math inline">\(d_{k,t}\)</span>，如下： <spanclass="math display">\[\tilde{d}_{k,t}\,=\,d_{k,t}\,+\,d_{k,t}^{a tt},\]</span> ​  其中<spanclass="math inline">\(\tilde{d}_{k,t}\)</span>是受损的测量值，<spanclass="math inline">\(d_{k,t}^{a t t}\)</span>是一个标量，满足： <spanclass="math display">\[d_{k,t}^{a tt}=\mathbf{u}_{k,t}^{T}\mathbf{p}_{k}\]</span> ​  其中<spanclass="math inline">\(\mathbf{p}_{k}\)</span>是一个被攻击节点的l维非零向量，而是一个普通节点的零向量。</p><h3 id="部分fdi攻击">2)部分FDI攻击</h3><p>​  部分FDI攻击是FDI攻击的一种特例。由于网络内的攻击强度和范围降低，对攻击者更隐蔽和高效。<br/>​  类似地，部分FDI攻击者向被攻击的节点发送满足(7)和(8)的恶意信息。然而，只有部分L维的估计是证伪的，这可以显示为：<spanclass="math inline">\(0\lt \|\mathbf{q}_{k}\|_{0}\ltL\)</span>。<br/>​  此外，在我们所考虑的对抗性环境中，假设在[4]-[6]，[9]-[11]中所采用的假设如下。<br/>​  假设1：被攻击的节点数小于<spanclass="math inline">\([{\frac{|N_{k}|}{2}}]\)</span>。</p><h1 id="iii.分布式安全聚类与估计算法">III.分布式安全聚类与估计算法</h1><p>​  在本部分中，提出了DSCE算法来在存在攻击的情况下产生安全的聚类和估计结果。从系统学的角度来看，采用了AC-dLMS滤波器。从实现的角度来看，主要包括四个主要步骤：自适应、检测、聚类和组合。在下面的几个部分中，我们将简要地描述这些过程。</p><h2 id="a.自适应">A.自适应</h2><p>​  在这一步中，基于<spanclass="math inline">\(\{d_{k,t},\mathbf{u}_{k,t}\}\)</span>，每个节点调整中间估计值<spanclass="math inline">\(\varphi_{k,t}\)</span>如下： <spanclass="math display">\[\varphi_{k,t}=\psi_{k,t-1}+\mu_{k}\sum_{\ell\in{N_{k}}}c_{\ell,k}{\mathbf{u}}_{\ell,i}(d_{\ell,i0}-{\mathbf{u}}_{\ell,i}^{T}\psi_{k,t-1})\]</span>​  其中，<spanclass="math inline">\(\psi_{k,t-1}\)</span>表示节点k在t-1时刻的另一个中间估计值。<br/>​  组合系数<spanclass="math inline">\(a_{\ell,k}\)</span>、<spanclass="math inline">\(c_{\ell,k}\)</span>可以根据任务间的相似性进行调整。因此，具有异常估计的恶意节点可以被未受攻击的节点自适应地隔离。此外，利用邻居之间的协作可以获得更准确的估计。因此，与非合作滤波器相比，AC-dLMS滤波器更有效。</p><h2 id="b.检测">B.检测</h2><p>​  在这一步中，获得排除恶意信息的锚点估计<spanclass="math inline">\({\tilde{\mathbf{w}}}_{k,i}\)</span>以供参考。具体建立了节点k的第m维的排序序列如下：<spanclass="math display">\[\boldsymbol{\Phi}_{k,t}^{(m)}=\{\varphi_{\ell_{1},t}^{(m)},...,\varphi_{\ell_{s},t}^{(m)},...,\varphi_{\ell_{n_k},t}^{(m)}\}\]</span>​  其中<spanclass="math inline">\(\varphi_{\ell_{1},t}^{(m)}&lt;\varphi_{\ell_{s},t}^{(m)}&lt;\varphi_{\ell_{n_k},t}^{(m)}\)</span>和<spanclass="math inline">\(\ell_{1},\ell_{s},\ell_{n_{k}}\in{\calN}_{k}\)</span>。<br/>​  根据假设1，攻击者倾向于使数据偏离之前的值。因此，当节点k及其相邻节点按估计值进行排序时，被攻击的节点更有可能位于被排序序列的左侧或右侧。因此，中心中的值通常比其他值更可靠，可以被认为是锚定估计的元素。<br/>​  因此，可以建立锚定估计中节点k的第m维估计：<spanclass="math display">\[\bar{\mathbf{w}}_{k,t}^{(m)}=\boldsymbol{\Phi}_{\lceil\frac{|\mathcal{N}_k|}{2}\rceil,i}^{(m)}.\]</span>​  由于得到了每个节点的可靠估计，因此设计了一个基于阈值测试过程的维数攻击检测器来定位可靠的信息。<br/>​  具体来说，对于节点k的第m个维数，如果中间估计<spanclass="math inline">\(\varphi_{k,t}\)</span>和锚估计<spanclass="math inline">\(\mathbf{\barw}_{k,t}\)</span>之间的距离超过阈值，可以推断维数受到攻击，反之亦然。判断过程表现如下：<spanclass="math display">\[\|\bar{\mathbf{w}}_{k,t}^{(m)}-\varphi_{k,t}^{(m)}\|^2\overset{\mathbb{H}_1}{\underset{\mathbb{H}_2}{\lessgtr}}b\cdot\theta_{k,t},\]</span></p><p><span class="math display">\[\theta_{k,t}=\begin{cases}\epsilon_k,&amp; \mathrm{if}\epsilon_k&gt;\theta_{k,t-1}; \\\theta_{k,t-1}, &amp;\mathrm{otherwise}, &amp; \end{cases}\]</span></p><p><span class="math display">\[\epsilon_{k}=\|m a x\{\bar{\bfw}_{k,t}^{(m)}-\bar{\bfw}_{\ell,i}^{(m)}|\ell\in\mathcal{N}_{k}\|^{2}\]</span></p><p>​  其中，<spanclass="math inline">\(\theta_{k,t}\)</span>为自适应阈值，视为由任务间差异引起的节点之间的最大距离，更新公式在（13）和（14）中。此外，b是一个预定义的常数，用于调整判断条件的松弛或紧性，H1和H2分别是节点k的第m维估计是安全的和被攻击的假设。<br/>​  此外，以下变量用于记录检测结果：<span class="math display">\[T_{m,k}=\begin{cases}1, &amp;\mathrm{if}~\mathbb{H}_1\text{is achieved}; \\0, &amp;\mathrm{if}~\mathbb{H}_2\text{is achieved}; &amp;\end{cases}\]</span></p><p><span class="math display">\[s_{k}={\left\{\begin{array}{ll}{1,}&amp;{\mathrm{if~}\|\mathbf{T}_{k}\Vert_{1}^{1}=L;}\\{0,}&amp;{\mathrm{otherwise;}}\end{array}\right.}\]</span></p><p>​  其中<span class="math inline">\({\bfT}_{k}=\{T_{1,k},T_{2,k},\ldots,T_{L,k}\}^{T}\)</span>。</p><h2 id="c.-聚类">C. 聚类</h2><p>​  在这一步中，通过以下自适应聚类方案更新系数<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\(c_{\ell,k}\)</span>的值，完成了AC-dLMS滤波器的聚类过程：<spanclass="math display">\[a_{\ell,k}={\frac{\|\varphi_{k,\ell}+\mu_{k}{\bfq}_{k,\ell}-\varphi_{\ell,\imath}\|^{-2}}{\sum_{j\in{\calN}_{k}}\|\varphi_{k,\ t}+\mu_{k}{\bf q}_{k,\t}-\varphi_{j,\imath}\|^{-2}}}\]</span></p><p><span class="math display">\[c_{k,\ell}=a_{\ell,k}\]</span></p><p>​  其中<span class="math inline">\({\bf q}_{k,t}={\bfu}_{k,t}(d_{k,t}-{\bfu}_{k,t}^{T}\varphi_{k,t})\)</span>。<br/>​  因此，对于节点k，估计值相似性较高的邻居赋值越大，在每个信息融合过程中占越大的影响。<br/>​  同样，在该算法中，也可以通过调节系数<spanclass="math inline">\(\tilde{a}_{\ell,k}\)</span>来消除恶意信息的影响。因此，提出了安全聚类方案。在该方案中，根据检测结果，动态地采用了防御策略。</p><h3 id="情形1">1)情形1</h3><p>​  当<spanclass="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}=L\)</span>时，节点k状态为安全。此外，恶意节点被<spanclass="math inline">\(a_{\ell,k}\)</span>排除，因此，没有必要采取其他措施，我们设置：<span class="math display">\[\check{a}_{\ell,k}=a_{\ell,k}\]</span></p><h3 id="情形2">2)情形2</h3><p>​  当<spanclass="math inline">\(0&lt;\|\mathbf{T}_{k}\|_{1}^{1}&lt;L\)</span>时，表示节点k受到部分攻击。因为可以利用来自未受攻击的维数的信息来判断节点之间的相似性。因此，提出了自适应部分聚类（APC，AdaptivePartial Clustering）算法来恢复聚类信息。<br/>​  首先，利用<spanclass="math inline">\(\mathbf{T}_{k}\)</span>对Hadamard积的尺寸级异常信息进行过滤，生成部分型安全信息如下：<spanclass="math display">\[\dot{\varphi}_{\ell.i}=\varphi_{\ell.i}\odot\mathbf{T}_{k},\ell\in{\mathcal{N}}_{k},\]</span></p><p><spanclass="math display">\[{\dot{\mathbf{q}}}_{k,t}=\mathbf{u}_{k,t}(d_{k,t}-\mathbf{u}_{k,t}^{T}{\dot{\varphi}}_{k,t})\]</span></p><p>​  然后，生成<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>如下： <spanclass="math display">\[\check{a}_{\ell,k}=\frac{s_{\ell}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{\ell,i}\|^{-2}}{\sum_{j\inN_{k}}s_{j}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{j,i}\|^{-2}}\]</span> ​  其中，<spanclass="math inline">\(s_j\)</span>用于过滤异常节点。<br/>​  在得到<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>后，其安全的簇内邻居通过组合步骤可以恢复被攻击节点的错误估计。</p><h3 id="情形3">3)情形3</h3><p>​  当<span class="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}\ =\0\)</span>时，节点k被认为受到FDI攻击者的影响。在这种情况下，将无法恢复聚类信息。此外，也没有可用的资料来替代受攻击的估计数。因此，将去掉节点k，以减轻其对网络的影响。</p><h2 id="d.-组合">D. 组合</h2><p>​  当前一个步骤完成后，计算出组合系数<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>。因此，在这一步中，每个节点组合了该步骤中来自其邻居的中间估计。<br/>​  对于AC-dLMS滤波器，估计值<spanclass="math inline">\(\psi_{k,i}\)</span>的计算方法如下： <spanclass="math display">\[\psi_{k,t}=\sum_{\ell\in{\calN}_{k}}a_{\ell,k}\varphi_{k,t}\]</span> ​  可靠的瞬时估计<spanclass="math inline">\(\mathbf{w}_{k,t}\)</span>生成为： <spanclass="math display">\[\mathbf{w}_{k,t}=\sum_{\ell\in {\calN}_{k}}{\check{a}}_{\ell,k}\varphi_{k,t}\]</span>​  综上所述，在算法1中总结了该算法的更新过程。</p><hr /><p>算法1：DSCE算法</p><hr /><ol type="1"><li>初始化：Let <spanclass="math inline">\(\mathrm{w}_{k,0}=\psi_{k,0}=\varphi_{k,0}=0\)</span>。<br/>2.for t=1 to T do<br/>3. ​ for k=1 to N do<br/>4. ​公式（9）中的自适应中间估计 <spanclass="math inline">\(\varphi_{k,t}=\psi_{k,t-1}+\mu_{k}\sum_{\ell\in{N_{k}}}c_{\ell,k}{\mathbf{u}}_{\ell,i}(d_{\ell,i0}-{\mathbf{u}}_{\ell,i}^{T}\psi_{k,t-1})\)</span><br/>5.​获得公式（10）、公式（11）的锚定估计 <spanclass="math inline">\(\bar{\mathbf{w}}_{k,t}^{(m)}=\boldsymbol{\Phi}_{\lceil\frac{|\mathcal{N}_k|}{2}\rceil,i}^{(m)}\)</span><spanclass="math inline">\(\boldsymbol{\Phi}_{k,t}^{(m)}=\{\varphi_{\ell_{1},t}^{(m)},...,\varphi_{\ell_{s},t}^{(m)},...,\varphi_{\ell_{n_k},t}^{(m)}\}\)</span><br/>6.​检测信任维度并获取<span class="math inline">\({\bf T}_{k}\)</span> <spanclass="math inline">\(T_{m,k}=\begin{cases}1, &amp;\mathrm{if}~\mathbb{H}_1\text{is achieved}; \\0, &amp;\mathrm{if}~\mathbb{H}_2\text{is achieved}; &amp;\end{cases}\)</span><br/>7. ​ 执行自适应聚类方案，得到<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\(c_{\ell,k}\)</span> <spanclass="math inline">\(a_{\ell,k}={\frac{\|\varphi_{k,\ell}+\mu_{k}{\bfq}_{k,\ell}-\varphi_{\ell,\imath}\|^{-2}}{\sum_{j\in{\calN}_{k}}\|\varphi_{k,\ t}+\mu_{k}{\bf q}_{k,\t}-\varphi_{j,\imath}\|^{-2}}}\)</span> 、<spanclass="math inline">\(c_{k,\ell}=a_{\ell,k}\)</span><br/>8. ​ 如果<spanclass="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}=L\)</span><br/>9. ​设置<spanclass="math inline">\(\check{a}_{\ell,k}=a_{\ell,k}\)</span><br/>10. ​如果<spanclass="math inline">\(0&lt;\|\mathbf{T}_{k}\|_{1}^{1}&lt;L\)</span><br/>11.​设置<spanclass="math inline">\(\check{a}_{\ell,k}=\frac{s_{\ell}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{\ell,i}\|^{-2}}{\sum_{j\inN_{k}}s_{j}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{j,i}\|^{-2}}\)</span><br/>12. ​否则删除节点k<br/>13. ​ 结合中间估计，分别得到<spanclass="math inline">\(\psi_{k,i}\)</span>和<spanclass="math inline">\(\mathbf{w}_{k,t}\)</span>。 <spanclass="math inline">\(\psi_{k,t}=\sum_{\ell\in{\calN}_{k}}a_{\ell,k}\varphi_{k,t}\)</span> <spanclass="math inline">\(\mathbf{w}_{k,t}=\sum_{\ell\in {\calN}_{k}}{\check{a}}_{\ell,k}\varphi_{k,t}\)</span></li></ol><hr /><h1 id="iv.模拟">IV.模拟</h1><p>​  在本节中，我们考虑了一个包含5个不同集群的21个节点的多任务网络，其拓扑结构如图1(a)所示。</p><figure><img src="../postimages/SPL/image-20241216092147022.png"alt="image-20241216092147022" /><figcaption aria-hidden="true">image-20241216092147022</figcaption></figure><p>​  具体而言，<spanclass="math inline">\(\mathcal{C}_{1}=\{1,2,3,4\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{2}=\{5,6,7,8,9\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{3}=\{10,11,12,13,14\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{4}=\{15,16,17\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{5}=\{18,19,20,21\}\)</span>。输入信号<spanclass="math inline">\(\mathbf{u}_{k,t}\)</span>和噪声项<spanclass="math inline">\({v}_{k,t}\)</span>均被视为零均值高斯随机变量，其方差为<spanclass="math inline">\(\sigma_{u,k}^{2}\)</span>和<spanclass="math inline">\(\sigma_{v,k}^{2}\)</span>，如图1 (b)所示。<spanclass="math inline">\(\mathbf{w}_{k}^{o}\)</span>的尺寸设置为：<spanclass="math inline">\(L=5\)</span>。<br/>​  为了进行比较，本文还说明了[13]中的多dLMS算法、nc-LMS算法、[12]中的AC-dLMS算法以及[9]中的S-dLMS算法的曲线。此外，在图例中用星号标记的项目表示已经给出了聚类信息。</p><h2 id="a.示例1">A.示例1</h2><p>​  在第一个例子中，在网络中只考虑了部分的FDI攻击。具体来说，被攻击节点的集合为<spanclass="math inline">\(\{2,13,19\}\)</span>。相应估计的前两个维度是受到部分FDI攻击。</p><figure><img src="../postimages/SPL/image-20241216103214212.png"alt="image-20241216103214212" /><figcaption aria-hidden="true">image-20241216103214212</figcaption></figure><p>​  图2(a)给出了不同算法的瞬态MSD曲线。在存在攻击的情况下，DSCE算法表现出良好的性能，几乎达到了与无攻击的S-dLMS*和多dLMS算法相同的性能。计算结果表明，该算法已经完成了安全的聚类和估计任务。特别是，即使没有任务间关系的先验信息，该算法也获得了与与聚类信息相关的算法几乎相同的精度。<br/>​  从图2(b)中的稳态MSD曲线可以看出，被攻击节点的估计值被其任务对应的安全值所取代。<br/>​  此外，与nc-LMS算法相比，AC-dLMS算法的未攻击节点具有较低的稳态MSD值，说明AC-dLMS滤波器在获得更准确的锚定估计方面具有优越性。</p><h2 id="b.示例2">B.示例2</h2><p>​  为了验证该算法在更复杂的攻击环境下的鲁棒性，在示例2中考虑了这两种类型的攻击。具体来说，节点10和21受到FDI攻击，节点1和5受到部分FDI攻击，相应的前三个维度被篡改。</p><figure><img src="../postimages/SPL/image-20241216102940720.png"alt="image-20241216102940720" /><figcaption aria-hidden="true">image-20241216102940720</figcaption></figure><p>​  不同算法的瞬态MSD曲线如图3(a)。所示得到了与示例1相似的结果，证明了其对攻击的鲁棒性。<br/>​  从图3(b)可以看出，在该算法中，受FDI攻击的节点被删除，而受部分FDI攻击的节点被其安全的簇内邻居恢复。结果表明，该检测器在区分攻击方面的可靠性和安全聚类方案在消除攻击者影响方面的有效性。<br/>​  综上所述，这两个实例都证明了该算法在攻击下是有效的。</p><h1 id="v.结论">V.结论</h1><p>​  为了获得存在网络攻击情况下的安全估计，本文引入了DSCE算法。该算法采用了一种具有自适应协同系数的扩散滤波器。伴随着具有自适应阈值的维级攻击检测器，可以更准确地定位恶意信息。此外，还提出了安全聚类方案来消除恶意信息的影响。最后，通过融合邻居的任务内安全估计，得到了可靠的估计。仿真结果表明，该算法在抵御FDI攻击和部分FDI攻击方面是有效的。</p><h1 id="评审">评审：</h1><h2 id="对比论文一">对比论文一</h2><p>Secure Distributed Estimation Over Wireless Sensor Networks UnderAttacks</p><p>初步和问题制定</p><p>无攻击的扩散LMS</p><p>受攻击下的安全分布式估计</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Fuzzy K-Means With Adaptive Loss and Entropy Regularization</title>
      <link href="/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/"/>
      <url>/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/</url>
      
        <content type="html"><![CDATA[<p>Deep Fuzzy K-Means With Adaptive Loss and Entropy Regularization</p><p>（发表于IEEE Transactions on Fuzzy Systems 2019）</p><h1 id="摘要">摘要</h1><p>​  基于神经网络的聚类方法由于特征提取更有效，通常比传统方法具有更好的性能。大多数现有的深度聚类技术要么先利用图信息从原始数据中提取关键的深度结构，要么简单地利用随机梯度下降（SGD）。然而，他们经常遭受关于降维和聚类的学习步骤的分离。为了解决这些问题，提出了一种具有自适应损失函数和熵正则化特性的深度模糊k-means（DFKM）。DFKM同时进行深度特征提取和模糊聚类，生成更合适的非线性特征映射。此外，DFKM还结合了FKM，从而利用模糊信息来表示深度簇的清晰结构。为了进一步提高模型的鲁棒性，采用自适应权值对目标应用鲁棒损失函数。此外，采用熵正则化的亲和性来提供每个赋值的置信度，相应的隶属度和质心矩阵通过立体解而不是SGD来更新。大量的实验表明，在三个聚类指标下，DFKM比目前最先进的模糊聚类技术具有更好的性能。</p><p>索引项-自动编码器（AE）、深度神经网络、图像分割、鲁棒模糊k均值（FKM）、无监督嵌入式聚类。</p><h1 id="介绍">1.介绍</h1><p>​  为了解决这些问题，提出了一种具有自适应损失函数和熵正则化的深度模糊k-均值（DFKM），该模型将模糊聚类合并到AE中，提取更合适的深度特征。利用自适应损失函数[38]来增强对异常值的鲁棒性。图1显示了DFKM的框架。本文的主要贡献总结如下。</p><ul><li>聚类嵌入训练神经网络，使声发射能够将数据映射到更合适的深度特征空间。换句话说，DFKM同时进行深度特征提取和聚类。</li><li>利用鲁棒损失函数来增强该模型对具有自适应权值的异常值的不敏感性。为了解决这一问题，提出了一种有效的算法，并进一步保证了其收敛到局部最小值。</li><li>对亲和矩阵引入熵正则化，为每个分配提供置信度。</li><li>在我们的模型中不需要类似的基于图的信息，亲和矩阵和质心矩阵是通过紧密形式的解而不是SGD来更新的。因此，它可以在大数据上有效地执行。</li></ul><p>​  符号：在本文中，所有大写粗体字母表示矩阵，而所有小写粗体字母表示向量。对于矩阵M，<spanclass="math inline">\(m^i\)</span>表示第i个行向量，<spanclass="math inline">\(m_i\)</span>表示第i个列向量，<spanclass="math inline">\(m_{ij}\)</span>是它的<spanclass="math inline">\((i,j)\)</span>个元素。此外，MT是矩阵M的转置，<spanclass="math inline">\({\textbf{1}}=[1,1,\cdot\cdot\cdot,1]^{T}\)</span>，0表示零矩阵。M&gt; 0表示每个元素都为正。<spanclass="math inline">\(\|\mathbf{m}\|_{1}\)</span>和<spanclass="math inline">\(\|\mathbf{m}\|_{2}\)</span>分别表示<spanclass="math inline">\(\ell_{1}\)</span>范数和<spanclass="math inline">\(\ell_{2}\)</span>范数。<spanclass="math inline">\(\nabla_{\mathbf{x}}f(\mathbf{x})=[{\frac{\partialf}{\partial x_{1}}},{\frac{\partial f}{\partialx_{3}}},\cdot\cdot,{\frac{\partial f}{\partialx_{n}}}]^{T}\)</span>，其中f (x)是一个标量输出函数，而<spanclass="math inline">\(\nabla_{\mathbf{x}}g(\mathbf{x})=[{\frac{\partialg_{1}}{\partial x}},{\frac{\partial g_{2}}{\partialx}},\cdot\cdot,{\frac{\partial g_{n}}{\partial x}}]^{T}\)</span>，其中g(x)是一个向量输出函数。</p><h1 id="相关工作">2.相关工作</h1><h2 id="a.模糊聚类">A.模糊聚类</h2><h2 id="b.深度聚类">B.深度聚类</h2><h1 id="方法">3.方法</h1><p>​  由于传统的核k-means（KKM）和基于谱的聚类方法对大数据难以处理，而KM和非负矩阵分解等有效技术过于简单，应用于非线性数据，因此提出了处理大数据集和非线性分布数据的DFKM。在本节中，我们首先介绍了自适应损失函数和熵正则化的FKM。然后，详细介绍了DFKM的研究细节。</p><h2 id="a.-自适应损耗函数">A. 自适应损耗函数</h2><p>​  <span class="math inline">\(\ell_{2，1}\)</span>范数： <spanclass="math display">\[\|\mathbf{M}\|_{2,1}=\sum_{i}\|\mathbf{n^{i}}\|_{2}\]</span>​  Frobenius范数： <spanclass="math display">\[\|\mathbf{M}\|_{F}^{2}=\sum_{i}\|\mathbf{m}^{i}\|_{2}^{2}\]</span>​  为了利用它们的优点，将一个鲁棒损失函数即自适应损失函数定义为[38]，[40]如下：<span class="math display">\[||{\bfM}||_{\sigma}=\sum_{i}{\frac{(1+\sigma)||{\bf m}^{i}||_{2}^{2}}{||{\bfm}^{i}||_{2}+\sigma}}\]</span>​  其中，σ是一个权衡参数，它控制对各种类型异常值的鲁棒性。不同σ下向量的自适应损失函数说明如图2所示。</p><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215846737.png"alt="image-20241204215846737" /><figcaption aria-hidden="true">image-20241204215846737</figcaption></figure><p>​  从图2中可知，自适应损失函数在2,1-范数和平方弗罗比尼乌斯范数之间进行插值，它继承了平方弗罗比尼乌斯范数的平滑性。<spanclass="math inline">\(\|\mathbf{M}\|_{\sigma}\)</span>的性质总结如下。</p><ul><li><spanclass="math inline">\(\|\mathbf{M}\|_{\sigma}\)</span>是二倍微分、凸和非负的，因此它适合作为一个损失函数。</li><li>如果<span class="math inline">\(\forall i\)</span>，<spanclass="math inline">\(\|\mathbf{m}_{i}||\ll\sigma\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}{\frac{1+\sigma}{\sigma}}\end{array}}||\mathbf{M}||^{2}_{F}\)</span>。</li><li>如果<span class="math inline">\(\forall i\)</span>，<spanclass="math inline">\(\|\mathbf{m}_{i}||\gg\sigma\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}{(1+\sigma)}\end{array}}||\mathbf{M}||_{2,1}\)</span>。</li><li>如果<span class="math inline">\(\sigma\to0\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}\end{array}}||\mathbf{M}||_{2,1}\)</span>。</li><li>如果<span class="math inline">\(\sigma\to\infty\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}\end{array}}||\mathbf{M}||^{2}_{F}\)</span>。</li></ul><h2 id="b.-具有加权自适应损失函数的fkm">B.具有加权自适应损失函数的FKM</h2><p>​  对于数据数为N的任意数据矩阵<spanclass="math inline">\(\mathbf{X}=\left[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{N}\right]\)</span>，将具有熵正则化的FKM的目标函数定义为[26]，[39]<span class="math display">\[\operatorname*{min}_{c_j,u_{ij}}\sum_{i=1}^{N}\sum_{j=1}^{k}u_{i j}\vert\vert{\bf x}_{i}-{\bfc}_{j}\vert\vert_{2}^{2}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\\\mathrm{s.t.}\,\,\sum_{j=1}^{k}u_{i j}=1,0\lt u_{i j}\lt 1\]</span>​  其中，<span class="math inline">\(\gamma\)</span>是控制<spanclass="math inline">\(u_{ij}\)</span>分布的权衡参数。当<spanclass="math inline">\(\gamma\rightarrow\infty,\,u_{ij}\rightarrow\,{\textstyle\frac{1}{N}}\)</span>时。<br/>​  提出了一种新的具有自适应损失函数的FKM代价函数，如下[41]：<span class="math display">\[\sum_{i=1}^{N}\sum_{j=1}^{k}u_{ij}{\frac{(1+\sigma)||{\bf x}_{i}-{\bf c}_{j}||_{2}^{2}}{||{\bfx}_{i}-{\bf c}_{j}||_{2}+\sigma}}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\]</span></p><p>​  可以重写为</p><p><span class="math display">\[\operatorname*{min}_{c_j,u_{ij}}\sum_{i=1}^{N}\sum_{j=1}^{k}u_{i j}\|{\bf x}_{i}-{\bfc}_{j}\|_{\hat{\sigma}}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\\\mathrm{s.t.}\,\,\sum_{j=1}^{k}u_{i j}=1,0\lt u_{i j}\lt1\]</span></p><p>​  其中，<spanclass="math inline">\(\|\mathbf{M}\|_{\hat\sigma}\)</span>相当于任何向量<spanclass="math inline">\(\mathbf{m}\in\mathbb{R}^{d}\)</span>的<spanclass="math inline">\(\|\mathbf{M^T}\|_{\sigma}\)</span>。<br/>​  在下一节中，我们开发了一个有效的算法来解决上面问题。</p><h2 id="c.-dfkm的代价函数">C. DFKM的代价函数</h2>$$<span class="math display">\[\begin{array}{ll}J_1=||{\bfH}^{(M)}-\mathrm{X}||_{F}^{2}\\J_2=\sum_{i=1}^{N}\sum_{j=1}^{k}u_{ij}||\mathbf{h}_{i}^{({\frac{M}{2}})}-\mathbf{c}_{j}||_{\hat{\sigma}}+\gammau_{i j}\log u_{ij}\\J_3=\sum_{m=1}^{M}||\mathbf{W}^{(m)}||_{F}^{2}+||\mathbf{b}^{(m)}||_{2}^{2}\end{array}\]</span><p>{c}$$</p><p>因此，通过将熵正则化和自适应损失嵌入，提出了DFKM模型 <spanclass="math display">\[\begin{aligned}&amp;\underset{\operatorname{W}^{(m)}, \operatorname{D}^{(m)},\mathbf{C}}{\text{minimize}}&amp; &amp; J_1 + \lambda_{1}J_2 +\lambda_{2}J_3 \\&amp; \text{subject to}&amp; &amp; \sum_{j=1}^{k}u_{ij}=1, \quad 0 &lt; u_{i j} &lt; 1, \quad \forall i\end{aligned}\]</span>其中，λ1和λ2是权衡参数。<spanclass="math inline">\(\mathbf{c}_{j}\in\mathbb{R}^{d^{\prime}}\)</span>是低维特征空间中的第j个簇质心，具有<spanclass="math inline">\(d^{\prime}=d^{(\frac{M}{2})}\)</span>。</p><p>J1、J2和J3被设计为不同的目的。J1保证了重构误差的最小值。j2是问题（10）中提出的具有自适应损失函数的FKM的代价函数。因此，λ1是重建和FKM之间的权衡参数。请注意，如果我们将λ1设置为一个较大的值，即较小的J2，那么由于重建不良，该模型的性能将不会太理想。J3是一种正则化方法，用于避免与正则化参数λ2过拟合的不良事件。术语J3也能够防止声发射生成一个平凡的映射。</p><p>因此，DFKM是将原始数据投影到一个非线性的低维特征空间上，并通过非线性映射特征同时学习一个软聚类隶属度矩阵。</p><h1 id="优化算法">4.优化算法</h1><p>​  在本节中，我们首先开发了一个有效的算法来求解自适应（10）中损失函数的FKM，该算法保证收敛到局部最小值。然后，提出了一种求解（14）中DFKM损失函数的算法。</p><h2 id="a.-加权自适应损失函数的优化">A. 加权自适应损失函数的优化</h2><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215225997.png"alt="image-20241204215225997" /><figcaption aria-hidden="true">image-20241204215225997</figcaption></figure><h2 id="b.-dfkm的优化">B. DFKM的优化</h2><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215259535.png"alt="image-20241204215259535" /><figcaption aria-hidden="true">image-20241204215259535</figcaption></figure><h1 id="实验">5.实验</h1><h1 id="结论">6.结论</h1><p>​  在本文中，我们提出了一种基于神经网络的聚类方法DFKM，该方法采用了熵正则化和具有自适应权值的鲁棒损失函数。通过结合AE和鲁棒FKM，DFKM将原始数据映射到一个更合适的空间，从而获得更好的性能。换句话说，DFKM同时执行聚类和特征提取，而不是将它们分成两个单独的步骤。此外，隶属度矩阵和质心矩阵通过近似解而不是SGD进行更新，使相应的优化算法快速收敛。大量的实验表明，在三个聚类指标下，我们的模型比目前最先进的模糊聚类算法获得了更好的性能。此外，DFKM还获得了更好的图像分割结果，验证了该模型的优越性。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust deep fuzzy K-means clustering for image data</title>
      <link href="/Robust-deep-fuzzy-k-means-clustering-for-image-data/"/>
      <url>/Robust-deep-fuzzy-k-means-clustering-for-image-data/</url>
      
        <content type="html"><![CDATA[<p>Robust deep fuzzy K-means clustering for image data</p><p>Xiaoling Wu , Yu-Feng Yu<span class="math inline">\(^{a,∗}\)</span> ,Long Chen<span class="math inline">\(^ b\)</span> , Weiping Ding<spanclass="math inline">\(^ c\)</span> , Yingxu Wang<spanclass="math inline">\(^ d\)</span> <br/>a 广州大学统计系, 中国广州<br/>b 澳门大学计算机与信息科学系, 中国澳门<br/>c南通大学信息科学与技术学院, 中国南通<br/>d济南大学网络智能计算山东省重点实验室, 中国济南</p><p><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0031320324002553"><imgsrc="https://img.shields.io/badge/PR-2024-yellow" alt="PR" /></a></p><h1 id="摘要">摘要</h1><p>​  图像聚类是计算机视觉中的一项艰巨任务，具有重要的应用价值。这项任务的关键是图像特征的质量。目前，大多数的聚类方法都面临着这一挑战。也就是说，特征学习和聚类的过程是独立运行的。为了解决这个问题，一些研究人员已经致力于一起进行特征学习和深度聚类。然而，所获得的特征缺乏成功处理高维数据的可鉴别性。为了解决这一问题，我们提出了一种新的鲁棒深度模糊𝐾-means聚类（RD-FKC，robustdeep fuzzy 𝐾-meansclustering）模型，该模型有效地将图像样本投影到一个具有代表性的嵌入空间中，并将隶属度精确地学习到一个组合框架中。具体来说，RD-FKC引入了拉普拉斯正则化技术来保持数据的局域性。此外，通过使用自适应损失函数，该模型对不同类型的异常值具有更强的鲁棒性。此外，为了避免潜在空间的扭曲，使提取的特征尽可能地保留原始信息，该模型引入了重构误差，并对网络参数进行了正则化处理。最后，给出了一种求解优化模型的有效算法。我们已经进行了大量的实验，说明了RD-FKC相对于现有的聚类方法的优势和优越性。</p><h1 id="介绍">1.介绍</h1><p>​  本文提出了一种新的深度聚类模型，即鲁棒深度模糊𝐾-均值聚类（RD-FKC），该模型将模糊聚类和深度卷积自动编码器（DCAE）集成到一个统一的框架中。图1显示了RD-FKC的框架。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20250106112717777.png"alt="image-20250106112717777" /><figcaption aria-hidden="true">image-20250106112717777</figcaption></figure><p>​  具体来说，我们使用拉普拉斯正则化来约束隶属度矩阵，并在嵌入特征空间中执行FKM，这不仅可以保证使用DCAE提取高维图像数据中包含的复杂和抽象信息，还可以获得局部信息，提高聚类性能。受[29]的启发，我们的模型引入了自适应损失函数，使聚类过程更加鲁棒。本文的主要贡献包括：</p><ul><li>将聚类嵌入到深度卷积自动编码器中，使DCAE能够学习有区别的和有代表性的特征进行聚类，然后聚类结果进而促进特征学习。也就是说，RD-FKC的目标是同时进行特征学习和聚类。</li><li>利用拉普拉斯正则化方法对隶属度矩阵进行约束，使从相似样本中学习到的隶属度也相互关联。也就是说，隶属度之间的联系与约束下的样本一致，可以进一步保存图像的局部信息。</li><li>RD-FKC将自适应损失函数引入到统一的框架中，可以减少各种异常值的影响，有助于增强聚类的鲁棒性。</li><li>提出了一种有效的算法来优化该框架。同时，该框架是直接端到端训练的，而不需要任何繁琐的预训练过程。一系列的比较实验证实了该模型的有效性和优越性。</li></ul><p>​  本文后续部分的组织结构如下。第2节给出了关于模糊𝐾-均值聚类和深度卷积自动编码器的一些相关工作。第3节阐述了所提出的RDFKC模型和有效算法。比较实验和结果分析详见第4节。最后，我们将在第5节中结束该工作。</p><h1 id="相关工作">2.相关工作</h1><h2 id="符号">2.1.符号</h2><p>​  在本文中，我们定义一个向量与一个粗体小写字母，例如𝐱，矩阵和一个粗体大写字母，例如𝐗，<spanclass="math inline">\(𝐱_𝑖\)</span>表示矩阵𝐗的第<spanclass="math inline">\(i\)</span>列，<spanclass="math inline">\(𝐱^𝑖\)</span>表示它的第<spanclass="math inline">\(i\)</span>行，<spanclass="math inline">\(𝑥_{𝑖𝑗}\)</span>是矩阵𝐗的第<spanclass="math inline">\(i\)</span>行和第<spanclass="math inline">\(j\)</span>列元素。将<span class="math inline">\(\{\bfX_{n}\}_{n=1}^{N}\)</span>作为具有𝑁张图像的图像集，提取的相关特征为<spanclass="math inline">\(\mathbf{Z}=\left[\mathbf{z}_{1},\mathbf{z}_{2},\ldots,\mathbf{z}_{N}\right]\in\mathbb{R}^{d\timesN}\)</span>。这里，𝑑是嵌入式空间的维数。RD-FKC的目的是获得区别表示𝐙，并将其聚类为𝐶组。将<spanclass="math inline">\(\mathbf{U}=[\mathbf{u}_{1}^{T},\mathbf{u}_{2}^{T},\ldots,\mathbf{u}_{N}^{T}]^{T}\in\mathbb{R}^{N\timesC}\)</span>和<span class="math inline">\({\bf V}=[{\bf v}_{1},{\bfv}_{2},\dots,{\bf v}_{C}]\,\in\,\mathbb{R}^{d\timesC}\)</span>分别定义为隶属度矩阵和簇中心矩阵。表1列出了更多的字符，并解释了它们在首次被介绍时的含义。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203094934792.png"alt="image-20241203094934792" /><figcaption aria-hidden="true">image-20241203094934792</figcaption></figure><h2 id="模糊𝐾-means聚类">2.2.模糊𝐾-means聚类</h2><p>​  模糊𝐾-means聚类[4]是𝐾-means的一个模糊版本，其目的是构造隶属度矩阵，然后温和地将样本划分为相应的类别。FKM的目标函数定义为：<spanclass="math display">\[\begin{array}{l}{\operatorname*{min}\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{m}||\mathbf{x}_{n}-\mathbf{v}_{c}||_{2}^{2}}\\{\mathrm{s.t.}\quad\sum_{c=1}^{C}u_{nc}=1,0\leq u_{n c}\leq1}\end{array}\]</span> ​  其中，<spanclass="math inline">\(m(m&gt;1)\)</span>是一个模糊化参数。<spanclass="math inline">\(𝐱_𝑛\)</span>表示第n个样本点。<spanclass="math inline">\({\textbf{U}}={\lbrack{u_{n c}\rbrack}_{N\timesC}}\)</span>为隶属度矩阵，<spanclass="math inline">\(𝑢_{𝑛𝑐}\)</span>表示分配给第n个样本点的第𝑐类的隶属度。<spanclass="math inline">\(\mathbf{V}=\left[\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{C}\right]\)</span>是集群中心矩阵，<spanclass="math inline">\(\mathbf{v}_{c}\,\in\,\mathbb{R}^{d}\)</span>是第𝑐个集群。<br/>​  然后，利用拉格朗日乘子，根据以下公式交替更新<spanclass="math inline">\(𝑢_{𝑛𝑐}\)</span>和<spanclass="math inline">\(\mathbf{v}_{c}\)</span>： <spanclass="math display">\[u_{n c}=\frac{1}{\sum_{l=1}^{C}(\frac{||{\bfk}_{n}-{\bf v}_{c}\,||_{2}^{2}}{||{\bf k}_{n}-{\bfv}_{l}||_{2}^{2})})^\frac{2}{m-1}}\]</span></p><p><span class="math display">\[\mathbf{v}_{c}={\frac{\sum_{n=1}^{N}u_{nc}^{m}\mathbf{x}_{n}}{\sum_{n=1}^{N}u_{n c}^{m}}}\]</span></p><h2 id="深度卷积自动编码器dcae">2.3.深度卷积自动编码器（DCAE）</h2><p>​  一个经典的自动编码器通常是由全连接的层组成的，它忽略了图像的结构，并进一步引入了大量的参数。与之相比，DCAE[30]具有本地连接和权重共享的特点，更适合用于图像处理任务。为了利用图像中包含的空间信息，给定一个单通道图像𝐗，将DCAE的编码器和解码器定义为：<span class="math display">\[{\bf G}^{p}={\cal F}_{e}({\bf X}*{\bfW}^{p}+a^{p})\]</span></p><p><span class="math display">\[\hat{\bf X}={\cal F}_{d}(\sum_{p}{\bfG}^{p}*\tilde{\bf W}^{p}+b)\]</span></p><p>​  其中，𝐆𝑝为𝑝th特征映射，∗为卷积运算。<spanclass="math inline">\({\cal F_e}\)</span>和<spanclass="math inline">\({\calF_d}\)</span>是非线性激活函数（我们在实验中使用ReLU）。下标𝑒和𝑑分别表示编码器和解码器。<spanclass="math inline">\({\bf W}^{p}\)</span>是过滤器，<spanclass="math inline">\(\tilde{\bfW}^{p}\)</span>是翻转操作，可以将嵌入表示恢复到原始大小的位置。<spanclass="math inline">\(𝑎^𝑝\)</span>和𝑏是相应的偏差。<br/>​  DCAE通过最小化重构误差来更新编码器和解码器的参数：<spanclass="math display">\[\mathcal{L(\Delta)}=\frac{1}{N}\sum_{n=1}^{N}\|F_{d}(F_{e}(\mathbf{X}_{n}))-\mathbf{X}_{n}\|_{F}^{2}\]</span>​  其中𝑁为图像数量，<spanclass="math inline">\(\Delta\)</span>包含所有网络参数。‖⋅‖𝐹表示一个矩阵的Frobenius范数。</p><h1 id="鲁棒的深度模糊𝑲-means聚类">3.鲁棒的深度模糊𝑲-means聚类</h1><h2 id="自适应损耗函数">3.1.自适应损耗函数</h2><p>​  假设𝐇是一个任意矩阵，<spanclass="math inline">\(l_{2,1}\)</span>规范表示为<spanclass="math inline">\(\|\mathbf{H}\|_{2,1}=\sum_{i}||\mathbf{h}^{i}||_{2}\)</span>对大损失很鲁棒，但对小损失很脆弱，而平方Frobenius规范表示为<spanclass="math inline">\(\|\mathbf{H}\|_{F}^{2}\ =\\sum_{i}\|\mathbf{h}^{i}\|_{2}^{2}\)</span>很容易解决，对小损失稳健，但对大损失敏感。因此，一个结合了这两种优点的自适应损失函数被定义为：<spanclass="math display">\[||\mathbf{H}||_{\tau}=\sum_{i}{\frac{(1+\tau)||\mathbf{h}^{i}||_{2}^{2}}{\|\mathbf{h}^{i}\|_{2}+\tau}}\]</span>​  这里的𝜏&gt;0是一个自适应参数。如果𝜏→0，‖𝐇‖𝜏→‖𝐇‖2,1。如果𝜏→∞，‖𝐇‖𝜏→‖𝐇‖2𝐹。此外，‖𝐇‖𝜏具有良好的二微、非负、凸性质，有利于优化。因此，‖𝐡‖𝜏适用于损失函数，并通过调优𝜏对不同类型的异常值具有鲁棒性。<br/>​  根据以上分析，在FKM算法(1)中引入了自适应损失函数(7)，极大地提高了聚类的鲁棒性如下：<spanclass="math display">\[\begin{array}{l}{\operatorname*{min}\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}||\mathbf{x}_{n}-\mathbf{v}_{c}||_{\tau}}\\{\mathrm{s.t.}\quad\sum_{c=1}^{C}u_{nc}=1,0\leq u_{n c}\leq1}\end{array}\]</span></p><h2 id="对象的拉普拉斯正规化">3.2.对象的拉普拉斯正规化</h2><p>​  图的拉普拉斯算子确保了从彼此连接的样本中学习到相似的隶属度。同时，它还可以利用数据的局部性信息。因此，我们将拉普拉斯正则化纳入聚类模型。它可以根据样本𝐱𝑖和𝐱𝑗之间的相似性来实现。我们使用热核方案[31]来构造相似度如下：<spanclass="math display">\[s_{ij}=\begin{cases}e^{-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|}{0.5}}&amp; j\in NB_i \\0, &amp; \mathrm{otherwise} &amp; \end{cases}\]</span>​  其中，<spanclass="math inline">\(NB_i\)</span>为𝑖th样本的邻域集。然后计算<spanclass="math inline">\(𝐒=(𝐒+𝐒^𝑇)/2\)</span>，得到对称相似矩阵。结合(8)和(9)，我们可以得到以下模型：<spanclass="math display">\[\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}\left\|\mathbf{x}_{n}-\mathbf{v}_{c}\right\|_{\tau}+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{jc})^{2}\\{\mathrm{s.t.}}\;\sum_{c=1}^{C}u_{n c}=1,0\leq u_{nc}\leq1\]</span></p><h2 id="rd-fkc算法">3.3.RD-FKC算法</h2><p>​  给定图像<span class="math inline">\(\{\bfX_{n}\}_{n=1}^{N}\)</span>，编码器首先尝试提取层次特征，然后将图像映射到一个非线性嵌入空间，以获得潜在的潜在表示𝐙。相比之下，解码器会将该特征重构回原始图像中。<br/>​  为了保证学习到的低维空间不被扭曲和破坏，RD-FKC模型结合了DCAE网络，将具有拉普拉斯正则化的鲁棒FKM损失函数嵌入到一个联合框架中，如下所示：<spanclass="math display">\[\operatorname*{min}_{A.\mathrm{U.V}}\quad{\frac{1}{N}}\sum_{n=1}^{N}\|{\hat{\mathbf{X}}}_{n}-\mathbf{X}_{n}\|_{F}^{2}+\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}\left\|\mathbf{z}_{n}-\mathbf{v}_{c}\right\|_{\tau}+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{jc})^{2}+\gamma\sum_{\ell=1}^{L}(\|\mathbf{W}^{(\ell)}\|_{F}^{2}+\|\mathbf{b}^{(\ell)}\|_{2}^{2})\\{\mathrm{s.t.}}\\sum_{c=1}^{C}u_{n c}=1,0\leq u_{n c}\leq1\]</span>​  其中，𝜇和𝛾是权衡参数。建立RD-FKC模型可分为四个部分（11）。第一项是最小化重构误差，以确保潜在的表示法尽可能多地保留原始信息。第二项表示由嵌入空间中的嵌入特征和聚类中心组成的自适应损失函数的FKM。第三项是约束隶属度矩阵，使具有相似隶属度的样本点也更接近。第四项用于避免过拟合，防止网络产生平凡解。</p><h2 id="最优化">3.4.最优化</h2><p>​  在（11）中，RD-FKC通过使用结合<spanclass="math inline">\(\ell_{1}\)</span>范数和<spanclass="math inline">\(\ell_{2}\)</span>范数的自适应损失函数来计算聚类误差。为了解决这个问题，我们首先考虑一个一般损失函数定义为：<spanclass="math display">\[\operatorname*{min}_{\mathbf{x}}g(\mathbf{x})+\sum_{i}{\frac{(1+\tau)\|d_{i}(\mathbf{x})\|_{2}^{2}}{\|d_{i}(\mathbf{x})\|_{2}+\tau}}\]</span>​  其中，<spanclass="math inline">\(d_{i}(\mathbf{x})\)</span>为向量输出。受[29,32]的启发，采用了一种迭代重加权的方法来求解它。通过对𝐱进行（12）的导数并将其设为零，我们得到<span class="math display">\[g^{\prime}({\bfx})+2(1+\tau)\sum_{i}\frac{\|d_{i}({\bf x})\|_{2}+2\tau}{2(\|d_{i}({\bfx})\|_{2}+\tau)^{2}}d_{i}({\bf x})d_{i}^{\prime}({\bf x})=0\]</span>​  定义 <span class="math display">\[k_{i}=(1+\tau)\frac{\|d_{i}({\bfx})\|_{2}+2\tau}{2(\|d_{i}({\bf x})\|_{2}+\tau)^{2}}\]</span>​  然后是等式（13）可以重写为 <spanclass="math display">\[g^{\prime}({\bf x})+2\sum_{i}k_{i}d_{i}({\bfx})d_{i}^{\prime}({\bf x})=0\]</span>​  需要注意的是，𝐾𝑖的值依赖于𝐱，当𝑘𝑖被固定时，（12）中的问题等于 <spanclass="math display">\[\operatorname*{min}_{\mathbf{x}}g(\mathbf{x})+\sum_{i}k_{i}\|d_{i}(\mathbf{x})\|_{2}^{2}\]</span>​  在[29]中证明了上述优化问题的收敛性。则（11）中的模型可以重新表述为：<spanclass="math display">\[\operatorname*{min}_{A.\mathrm{U.V}}\quad{\frac{1}{N}}\sum_{n=1}^{N}\|{\hat{\mathbf{X}}}_{n}-\mathbf{X}_{n}\|_{F}^{2}+\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}k_{nc}\left\|\mathbf{z}_{n}-\mathbf{v}_{c}\right\|_{\tau}\\+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{jc})^{2}+\gamma\sum_{\ell=1}^{L}(\|\mathbf{W}^{(\ell)}\|_{F}^{2}+\|\mathbf{b}^{(\ell)}\|_{2}^{2})\\{\mathrm{s.t.}}\\sum_{c=1}^{C}u_{n c}=1,0\leq u_{n c}\leq1\]</span> ​  其中 <spanclass="math display">\[k_{nc}=(1+\tau)\frac{\|\mathbf{z}_{n}-\mathbf{v}_{c}\|_{2}+2\tau}{2(\|\mathbf{z}_{n}-\mathbf{v}_{c}\|_{2}+\tau)^{2}}\]</span>​  可以看出，在模型（17）中有三个变量，很难直接解决这些问题。因此，我们提出了一个迭代的替代策略来优化它。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112655933.png"alt="image-20241203112655933" /><figcaption aria-hidden="true">image-20241203112655933</figcaption></figure><h1 id="实验结果及分析">4.实验结果及分析</h1><p>​  在本节中，所提出的RD-FKC模型的性能估计是基于8个图像集，使用三个公共指标，包括准确率（ACC）、归一化互信息（NMI）和纯度（Pur）。为了评估RD-FKC的能力，我们将其与现有的聚类技术进行了比较，如𝐾-means聚类（KM）[3]、模糊𝐾-means聚类（FKM）[4]、RSFKM[34]，PCAKM，深度嵌入式聚类（DEC）[22]，深度聚类网络（DCN）[35]和深度模糊𝐾-means（DFKM）[36]。<br/>​  在接下来的章节中，我们首先列出图像数据集的特征，然后给出实验实现的细节。此外，我们还报告了RD-FKC与其他方法相比的性能。最后，分析了各参数的灵敏度。</p><h2 id="数据集">4.1.数据集</h2><p>​  实验中使用了八种不同类型的图像集，包括人脸、手写数字、物体和时尚数据集。表2详细描述了这些图像集的特征，图2显示了我们实验中的几个样本。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112902796.png"alt="image-20241203112902796" /><figcaption aria-hidden="true">image-20241203112902796</figcaption></figure><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112925600.png"alt="image-20241203112925600" /><figcaption aria-hidden="true">image-20241203112925600</figcaption></figure><p>​  人脸上包括Jaffe、Yale、ORL和CMU-PIE。Jaffe包含213个26个×26的样本，来自10个人，有7种不同的表达。Yale有165张32张×32像素的图像。ORL由40人在不同光照条件下的400张图像组成。CMU-PIE由64人的32×32像素的图像组成。USPS是一个手写的数字数据集，包括9298个16×16像素的样本。对象数据库包含COIL20,128个×128像素的20个对象，CIFAR10包含60000的10类彩色图像。Fashion-MNIST是MNIST的替代品，包括10个类别的70000张时尚产品图片。</p><h2 id="网络设置">4.2.网络设置</h2><h2 id="聚类结果的比较">4.3.聚类结果的比较</h2><h2 id="对鉴别力和表现形式的可视化">4.4.对鉴别力和表现形式的可视化</h2><h2 id="参数分析">4.5.参数分析</h2><h1 id="结论">5.结论</h1><p>​  在这项工作中，我们提出了一种新的称为鲁棒深度模糊𝐾-means聚类（RD-FKC）的框架，该框架利用拉普拉斯正则化来保持输入图像的局部结构，并使用自适应损失函数通过调整𝜏来提高聚类模型的弹性。RD-FKC通过将DCAE和聚类模型整合到一个联合框架中，充分利用DCAE学习图像的判别和深度表示，然后进行鲁棒聚类，有助于解决图像聚类问题。此外，还提出了一种有效、实用的模型优化算法。在不同图像集上进行的大量可比实验表明，RD-FKC优于现有的传统和深度聚类方法。</p><p>​  可以观察到，RD-FKC在CIFAR10等彩色图像上表现不佳。可能的原因是局部结构信息没有得到有效的保存。因此，在今后的工作中，我们将考虑将𝑝-拉普拉斯正则化技术[39]嵌入到聚类模型中，从而更好地实现局部信息的保存。由于引入了一个额外的超参数𝑝，我们将进一步考虑集成𝑝-拉普拉斯正则化[40]，它对各种𝑝值图应用适当的权值，以更好地理解数据的几何形状。此外，我们还将考虑如何设计一个更先进的深度卷积自动编码器网络来提取特征，从而进一步提高聚类效率。</p><h1 id="代码结构">6.代码结构</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/soft-clustering/soft-clustering/blob/main/docs/source/rdfkc.md</span><br></pre></td></tr></table></figure><p>​  代码的总体结构按照编码器-解码器的自编码器的结构</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.Z = self.encoder(self.X)</span><br><span class="line">self.X_recon = self.decoder(self.Z)</span><br></pre></td></tr></table></figure><p>​  所以也有基于存在重构x的过程。</p><p>​  构建隶属度矩阵U、聚类中心V、相似度矩阵S的初始化：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.U = self._initialize_membership_matrix()</span><br><span class="line">self.V = self._initialize_cluster_centers()</span><br><span class="line">self.S = self._initialize_similarity_matrix()</span><br></pre></td></tr></table></figure><p>​  然后再迭代中优化模型参数、聚类中心和隶属度矩阵：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for _ in range(self.max_iter):</span><br><span class="line">    self._update_network_parameters()</span><br><span class="line">    self._update_cluster_center_matrix()</span><br><span class="line">    self._update_membership_matrix()</span><br></pre></td></tr></table></figure><p>​  最后输出隶属度矩阵U作为最后的结果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.argmax(self.U, dim=1).numpy()</span><br></pre></td></tr></table></figure><h2 id="优化模型参数函数">6.1优化模型参数函数</h2><p><spanclass="math display">\[{\mathcal{L}}(\Delta)={\frac{1}{N}}\sum_{n=1}^{N}\|{\hat{\mathbf{X}}}_{n}-\mathbf{X}_{n}\|_{F}^{2}+\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}k_{nc}\|\mathbf{z}_{n}-\mathbf{v}_{c}\|_{2}^{2}+\gamma\sum_{\ell=1}^{L}(\|W^{(\ell)}|_{F}^{2}+\|\mathrm{B}^{(\ell)}\|_{2}^{2})\]</span></p><p>​  使用计算x与x_recon的重构损失 ${}<em>{n=1}^{N}|{}</em>{n}-<em>{n}|</em>{F}^{2} $</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">recon_loss = F.mse_loss(self.X_recon, self.X)</span><br></pre></td></tr></table></figure><p>​  由嵌入空间中的嵌入特征和聚类中心组成的自适应损失函数</p><p><span class="math display">\[\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}\left\|\mathbf{z}_{n}-\mathbf{v}_{c}\right\|_{\tau}+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{j c})^{2}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cluster_loss = 0.0</span><br><span class="line">for n in range(self.N):</span><br><span class="line">    for c in range(self.K):</span><br><span class="line">        dist = torch.norm(self.Z[n] - self.V[c])</span><br><span class="line">        k_i = self._compute_ki(dist)</span><br><span class="line">        cluster_loss += self.U[n, c] ** 2 * k_i * dist ** 2</span><br></pre></td></tr></table></figure><p>​  约束隶属度矩阵 <spanclass="math display">\[\gamma\sum_{\ell=1}^{L}(\|\mathbf{W}^{(\ell)}\|_{F}^{2}+\|\mathbf{b}^{(\ell)}\|_{2}^{2})\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reg_loss = 0.0</span><br><span class="line">    for module in list(self.encoder.modules()) + list(self.decoder.modules()):</span><br><span class="line">        if hasattr(module, &quot;weight&quot;) and module.weight is not None:</span><br><span class="line">    reg_loss += torch.norm(module.weight, p=&#x27;fro&#x27;) ** 2</span><br><span class="line">        if hasattr(module, &quot;bias&quot;) and module.bias is not None:</span><br><span class="line">    reg_loss += torch.norm(module.bias, p=2) ** 2</span><br><span class="line">reg_loss *= self.gamma</span><br></pre></td></tr></table></figure><p>​  其中：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def _compute_ki(self, dist: float) -&gt; float:</span><br><span class="line">    &quot;&quot;&quot;Compute the robustness coefficient k_i for adaptive loss.&quot;&quot;&quot;</span><br><span class="line">    numerator = (1 + self.tau) * (dist + 2 * self.tau)</span><br><span class="line">    denominator = 2 * (dist + self.tau) ** 2</span><br><span class="line">    return numerator / denominator</span><br></pre></td></tr></table></figure><h2 id="更新聚类中心">6.2更新聚类中心</h2><p>​  基于以下公式更新： <spanclass="math display">\[\mathbf{v}_{c}={\frac{\sum_{n=1}^{N}\,k_{n c}u_{nc}^{2}\mathbf{z}_{n}}{\sum_{n=1}^{N}\,k_{n c}u_{n c}^{2}}}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def _update_cluster_center_matrix(self) -&gt; None:</span><br><span class="line">    &quot;&quot;&quot;Step 2: Update cluster centers V using the current membership and latent features.&quot;&quot;&quot;</span><br><span class="line">    d = self.Z.shape[1]</span><br><span class="line">    V_new = torch.zeros((self.K, d))</span><br><span class="line">    for c in range(self.K):</span><br><span class="line">        numerator = torch.zeros(d)</span><br><span class="line">        denominator = 0.0</span><br><span class="line">        for n in range(self.N):</span><br><span class="line">            dist = torch.norm(self.Z[n] - self.V[c])</span><br><span class="line">            k_i = self._compute_ki(dist)</span><br><span class="line">            w = self.U[n, c] ** 2 * k_i</span><br><span class="line">            numerator += w * self.Z[n]</span><br><span class="line">            denominator += w</span><br><span class="line">        V_new[c] = numerator / (denominator + 1e-8)</span><br><span class="line"></span><br><span class="line">    self.V = V_new</span><br></pre></td></tr></table></figure><h2 id="更新隶属度矩阵">6.2更新隶属度矩阵</h2><p>​  基于以下公式更新隶属度矩阵： <spanclass="math display">\[u_{nc}=\frac{p_{nc}+\frac{2-\sum^C_{c=1}\frac{p_nc}{q_nc}}{\sum^C_{c=1}\frac{p_nc}{q_nc}}}{2q_{nc}}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def _update_membership_matrix(self) -&gt; None:</span><br><span class="line">    &quot;&quot;&quot;Step 3: Update membership matrix U using closed-form solution with Lagrange multipliers.&quot;&quot;&quot;</span><br><span class="line">    U_new = torch.zeros(self.N, self.K)</span><br><span class="line">    for n in range(self.N):</span><br><span class="line">        neighbors = self._k_nearest_to_nth_sample(n)</span><br><span class="line">        p = torch.zeros(self.K)</span><br><span class="line">        q = torch.zeros(self.K)</span><br><span class="line">        for c in range(self.K):</span><br><span class="line">            p[c] = 2 * self.mu * sum(self.S[n, j] * self.U[j, c] for j in neighbors)</span><br><span class="line">            dist = torch.norm(self.Z[n] - self.V[c]) + 1e-8</span><br><span class="line">            q[c] = ((1 + self.tau) * dist ** 2) / (dist + self.tau)</span><br><span class="line">            q[c] += self.mu * sum(self.S[n, j] for j in neighbors)</span><br><span class="line"></span><br><span class="line">        lambda_n = (2 - torch.sum(p / q)) / (torch.sum(1 / q) + 1e-8)</span><br><span class="line">        for c in range(self.K):</span><br><span class="line">            U_new[n, c] = (p[c] + lambda_n) / (2 * q[c] + 1e-8)</span><br><span class="line"></span><br><span class="line">    self.U = U_new</span><br><span class="line">    </span><br><span class="line">def _k_nearest_to_nth_sample(self, n: int, k: int = 10) -&gt; list:</span><br><span class="line">    distances = torch.norm(self.Z[n] - self.Z, dim=1)</span><br><span class="line">    neighbors = torch.argsort(distances)[1:k+1]       # exclude self (at index 0)</span><br><span class="line">    return neighbors.tolist()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 自监督聚类 </category>
          
          <category> 模糊聚类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust deep k-means:An effective and simple method for data clustering</title>
      <link href="/Robust-deep-k-means/"/>
      <url>/Robust-deep-k-means/</url>
      
        <content type="html"><![CDATA[<p>Robust deep k-means: An effective and simple method for dataclustering</p><h1 id="摘要">摘要</h1><p>​  聚类的目的是根据一些距离或相似度度量将输入数据集划分为不同的组。k-means算法由于其简单、高效，是目前应用最广泛的聚类方法之一。在过去的几十年里，k-means及其各种扩展来解决实际的聚类问题。然而，现有的聚类方法通常采用单层公式（即浅层公式）。因此，所获得的低级表示与原始输入数据之间的映射可能包含相当复杂的层次信息。为了克服低层次特征的缺点，采用深度学习技术提取深度表示，提高聚类性能。在本文中，我们提出了一个鲁棒的深度k-means模型来学习与不同隐式低级属性相关的隐藏表示。通过使用深度结构分层地执行k-means，可以分层地利用数据的分层语义。来自同一个类的数据样本被迫逐层地靠近，这有利于聚类任务。我们的模型的目标函数被推导为一个更可跟踪的形式，这样优化问题可以更容易地解决，并可以得到最终的鲁棒结果。在12个基准数据集上的实验结果表明，与经典方法和最先进的方法相比，该模型在聚类性能方面取得了突破。</p><h1 id="介绍">1.介绍</h1><p>​  尽管上述k-means方法取得了显著的进展，但这些方法通常是用单层公式设计的。因此，所获得的低维表示与原始输入数据之间的映射可能包含相当复杂的层次信息。考虑到深度学习的发展，需要采用多个处理层来提取数据[29]的层次信息，本文提出了一种新的鲁棒深度k-means模型来利用多层次属性的层次信息。我们的模型的总体框架如图1所示。</p><figure><img src="../postimages/Robust-deep-k-means/image-20250107102610266.png"alt="image-20250107102610266" /><figcaption aria-hidden="true">image-20250107102610266</figcaption></figure><p>​  正如我们所看到的，通过使用深度结构来分层地执行k-means，数据的分层语义可以被分层地利用。也就是说，来自同一类的数据样本逐层地收集，非常有利于聚类任务。<br/>​  这项工作的主要贡献有三个方面：</p><ul><li>提出了一种新的鲁棒深度模型来分层地执行k-均值，从而可以分层地探索数据的分层语义。因此，来自同一类的数据样本可以有效地逐层收集，从而提供了一个清晰的聚类结构。</li><li>为了求解模型的优化问题，将相应的目标函数推导为更可跟踪的形式，并提出了一种替代的更新算法来求解优化问题。</li><li>在12个基准数据集上进行了实验，并显示了与经典和最先进的方法相比的良好结果。</li></ul><p>​  本文的基础构思如下。我们将在第2节中简要介绍与密切相关的工作。我们的模型的细节见第3节。实验结果见第4节。最后，我们在第5节中给出了本文的结论。</p><p>​  这项工作和我们早期的论文[1]之间有三个不同之处。<br/>​  首先，我们提出了鲁棒深度kkmeans模型的一般形式。详细地说，我们部署了一系列的散度函数来测量重构误差（第3节），而不仅仅是对噪声数据和异常值敏感的Frobenius范数。因此，我们早期的工作[1]只是本文的一个特例。其次，在12个基准数据集上进行了更全面的实验，验证了我们模型的鲁棒性和有效性：(i)记录了更多数据集和高级基线上的聚类结果（第4.3节）；（ii）添加不同参数设置的实验结果（第4.4节）；（iii）展示收敛分析实验（第4.5节）；（iv）研究不同发散函数对聚类性能的影响（第4.6节）。第三，我们介绍了更密切相关的文献（在第1节和第2节中），澄清了它们与最先进的技术之间的联系和差异。这有助于在社区中更好地定位拟议的工作。</p><h1 id="前期准备工作">2.前期准备工作</h1>​  非负矩阵分解（NMF）由于其直观的基于部分的解释[30,31]，在数据聚类中受到了广泛的关注。以往的研究表明，在松弛条件[30]下，NMF基本上等于<em>k</em>-means。假设<spanclass="math inline">\(\mathbf{X}=\left[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n}\right]\in\mathbb{R}^{m\timesn}\)</span>是一个具有n个数据样本和m个特征的非负数据矩阵。NMF的目标是找到两个非负矩阵<spanclass="math inline">\(\mathbf{U}\in\mathbb{R}^{m\timesc}\)</span>和<spanclass="math inline">\(\mathbf{V}\in\mathbb{R}^{n\timesc}\)</span>，使<spanclass="math inline">\(\mathbf{X}\approx\mathbf{UV}^{T}\)</span>，NMF的一般形式为$$<span class="math display">\[\begin{array}{l}{\cal D}_{\beta}(X|\mathrm{UV}^{T}\bigr)=\sum_{i=1}^{m}\sum_{j=0}^{n}d_{\beta}(X_{i j}|igl(\mathrm{U}^{T}\bigr)_{i j}\bigr)\\ {s.t.{\bfU}\geq0,{\bf V}\geq0,}\end{array}\]</span><p>$$ ​  其中<spanclass="math inline">\({\mathcal{D}}_{\beta}(\mathbf{X}|{\hat{\mathbf{X}}})\)</span>表示一个标量代价函数（即<spanclass="math inline">\(\mathbf{X}\)</span>与其重建<spanclass="math inline">\(\hat{\mathbf{X}}\)</span>之间的一些发散度量），<spanclass="math inline">\(\mathbf{X_{ij}}\)</span>是<spanclass="math inline">\(\mathbf{X}\)</span>的第<spanclass="math inline">\(ij\)</span>个元素。在公式(1)，可以采用一类称为β-散度[33]的散度函数。在NMF中使用最广泛的有三个散度函数：</p><p>​  ∗β=2（欧氏距离）：<spanclass="math inline">\(d_{2}(a|b)={\frac{1}{2}}(a-b)^{2}\)</span><br/>​  ∗β=1（Kullback–LeiblerDivergence）：<spanclass="math inline">\(d_{1}(a|b)=a\log{\frac{a}{b}}-a+b\)</span><br/>​  *β=0（Itakura–SaitoDivergence）：<spanclass="math inline">\(d_{0}(a|b)=\frac{a}{b}-\log{\frac{a}{b}}-1\)</span></p><p>​  [32]采用了等式(1)中的欧氏距离： <spanclass="math display">\[\begin{array}{l} {J_{N MF}=\sum_{i=1}^{m}\sum_{i=1}^{n}\left(X_{i j}-({\bf U}V^{T})_{ij}\right)^{2}=\|{\bf X}-{\bf U}V^{T}\|_{F}^{2}}\\ {s.t.{\bf U}\geq0,{\bfV}\geq0,}\end{array}\]</span> ​  其中，<spanclass="math inline">\(\|\cdot\|_{F}\)</span>表示Frobenius范数。[32]进一步指出，等式(2)是一个双凸公式（仅在U或V中为凸），并通过应用更新规则搜索局部最小值如下：</p><p><span class="math inline">\(\mathbf{U}_{i j}\leftarrow \mathbf{U}_{ij} \frac{(\mathbf{X V})_{i j}}{(\mathbf{U V ^{T} V})_{i j}}\)</span>，<span class="math inline">\(\mathbf{V}_{i j}\leftarrow \mathbf{V}_{i j}\frac{(\mathbf{X U^{T}})_{i j}}{(\mathbf{V U ^{T} U})_{ij}}\)</span></p><p>​  其中，<spanclass="math inline">\(\mathbf{V}\)</span>可以看作是聚类指标矩阵[30]，<spanclass="math inline">\(\mathbf{U}\)</span>表示质心矩阵，c表示聚类数。通常，我们有<spanclass="math inline">\(c\ll n\)</span>和<span class="math inline">\(c\llm\)</span>，意思是等式(2)实际上是搜索<spanclass="math inline">\(\mathbf{X}\)</span>的低维表示<spanclass="math inline">\(\mathbf{V}\)</span>。<br/>​  但在现实中，数据集通常是复杂的，并且总是包含多种层次模式（即因素）。以人脸数据集为例，它通常由一些常见的模式组成，如表达式、姿态、场景等。因此，很明显，基于单层的NMF不能充分利用不同因素下的隐藏信息。为了填补这一空白，[34]研究了一个多层深度模型，该模型通过进行semi-NMF的分层处理，创新性地探索了数据的分层信息。并定义了深度semi-NMF模型的基本公式为<span class="math display">\[\begin{array}{r l}{X}&amp;{\approxU_{1}V_{2}^{T},}\\ {X}&amp;{\approx U_{1}U_{2}V_{2}^{T},}\\&amp;{\vdots}\\ {X}&amp;{\approxU_{1}U_{2}\dots{U}_{r}V_{r}^{T}.}\end{array}\]</span>​  其中r为层数，<span class="math inline">\(\mathbf U_i\)</span>和<spanclass="math inline">\(\mathbf V_i\)</span>分别为第<spanclass="math inline">\(i\)</span>个层的基矩阵和表示矩阵。很明显，深度semi-NMF的目标也是搜索一个低维的嵌入表示，即最后一层<spanclass="math inline">\(\mathbf V_r\)</span>。通过分层分解每个层<spanclass="math inline">\(\mathbf{V}_{i}(i\ltr)\)</span>，等式(3)能够发现潜在的层次结构。与现有的单层NMF模型相比，深度semi-NMF可以更好地揭示数据的层次信息，因为不同层的低维表示可以识别出不同的模式。因此，我们的模型可以完全实现适合后续不同模式聚类的表示。例如，如图1所示，<spanclass="math inline">\(\mathbf U_3\)</span>对应于表达式的特征，<spanclass="math inline">\(\mathbf{U_2U_3}\)</span>对应于姿态的特征，最后，<spanclass="math inline">\(\mathbf{U=U_1U_2U_3}\)</span>对应于人脸图像的身份映射。这样，就可以获得更好的高可识别性、根据变异性最小的特征进行聚类的最终层表示。</p><h1 id="提出的模型">3.提出的模型</h1><p>​  在本节中，我们提出了一种新的深度k均值模型，称为鲁棒的深度k-means（RDKM）。我们提出了一种有效的更新算法来解决相应的优化问题。并分析了该算法的收敛性。</p><h2 id="鲁棒的深度k-means">3.1.鲁棒的深度k-means</h2><p>​  为了探索不同模态的低维表示，研究了一种新的鲁棒深度k-means模型，利用深度结构分层进行k-means。在本文中，为了扩大我们的模型的适用范围（即同时处理负数据和非负数据），我们省略了对<spanclass="math inline">\(\mathbf U_i\)</span>的非负约束。考虑到<spanclass="math inline">\(V_i\)</span>上的非负性约束使优化问题难以解决，我们通过引入新的变量<spanclass="math inline">\(V_i^+\)</span>，将目标函数转化为更可跟踪的形式。这样，非负性约束条件被分离并等价地采用，约束条件为<spanclass="math inline">\(V_i=V_i^+\)</span>。因此，我们不仅扩展了应用程序，而且还保留了我们的模型的强可解释性。这里我们使用乘子（ADMM）[35]的交替方向方法来求解优化问题。在数学上，所提出的RDMK模型被表述为<span class="math display">\[\begin{array}{l}{J=D_{\beta}(\mathbf{X}|\mathbf{Y})}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  在等式(4)中，我们可以看到在<spanclass="math inline">\(\mathbf{V}_{r}\)</span>的每一行上都采用了1-of-C的编码方案。1-of-C编码方案的主要目标是保证<spanclass="math inline">\(\mathbf{V}_{r}\)</span>的唯一性。此外，基于<spanclass="math inline">\(\mathbf{V}_{r}\)</span>，我们可以直接得到最终的离散划分结果。<br/>​  类似于等式(2)，如果在等式(4)中采用欧氏距离（即β= 2），则我们有 <span class="math display">\[\begin{array}{l}{J=\|\mathbf{X}-\mathbf{Y}\|_{F}^{2}}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  然而，已经证明了Frobenius范数对噪声数据和异常值[36,37]很敏感。为了提高该模型的鲁棒性，我们的模型采用了稀疏性诱导范数（即<spanclass="math inline">\(l_{2,1}\)</span>范数）。根据[36]，<spanclass="math inline">\(l_{2,1}\)</span>范数能够减少异常值的影响，因为它在数据点内执行<spanclass="math inline">\(l_{2}\)</span>范数，在数据点之间执行<spanclass="math inline">\(l_{1}\)</span>范数。最后，我们的鲁棒深度k-means（RDKM）模型被写为<span class="math display">\[\begin{array}{l}{J_{RDKM}=\|\mathbf{X}-\mathbf{Y}\|_{2,1}}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  正如我们所看到的，<spanclass="math inline">\(\|\mathbf{X}-\mathbf{Y}\|_{2,1}\)</span>相对于Y很容易最小化，而<spanclass="math inline">\(\|\mathbf{X}-\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\|_{2,1}\)</span>相对于<spanclass="math inline">\(\mathbf U_i\)</span>或<spanclass="math inline">\(\mathbfV_i\)</span>来最小化并不是那么简单。乘法更新规则隐式地解决了<spanclass="math inline">\(\mathbf U_i\)</span>和<spanclass="math inline">\(\mathbfV_i\)</span>解耦的问题。在ADMM背景下，一个自然的公式是优化<spanclass="math inline">\(\|\mathbf{X}-\mathbf{Y}\|_{2,1}\)</span>，约束条件为<spanclass="math inline">\(\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\)</span>。<strong>这就是我们考虑解决方程式（6）这样的问题的原因。</strong><br/>​  为了验证在我们的模型中使用的<spanclass="math inline">\(l_{2,1}\)</span>范数的鲁棒性和有效性，不同的散度函数（即，β= 2，β = 1，和β =0）的情况将在后面的章节中讨论。关于不同发散函数的优化算法也将在附录A中描述。<br/>​  对于等式(6)，提出了一种基于ADMM[35]的有效优化算法。等式(6)的拉格朗日函数是 <spanclass="math display">\[\mathcal{L}(\mathbf{Y},\mathbf{U}_{i},\mathbf{V}_{i},\mathbf{V}_{i}^{+},\mathbf{\mu,\lambda_{i})}=\|\mathbf{X}-\mathbf{Y}\|_{2,1}+\langle\mu,\mathbf{Y}-\mathbf{U}_{1}\mathbf{U}_{2}\dots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\rangle\\+\,{\frac{\rho}{2}}\,\|\mathbf{Y}-\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\|_{F}^{2}+{\sum_{i=1}^{r-1}\left\langle\lambda_{i},\mathbf{V}_{i}-\mathbf{V}_{i}^{+}\right\rangle}+\frac{\rho}{2}\sum_{i=1}^{r-1}\|\mathbf{V}_{i}-\mathbf{V}_{i}^{+}\|_{F}^{2},\]</span>​  其中<span class="math inline">\(\rho\)</span>为一个惩罚参数，<spanclass="math inline">\(\mu\)</span>和<spanclass="math inline">\(\lambda_i\)</span>均表示拉格朗日乘子，而<spanclass="math inline">\(\langle\cdot,\cdot\rangle\)</span>表示内积运算。</p><h2 id="最优化">3.2.最优化</h2><h2 id="收敛性分析">3.3.收敛性分析</h2><h1 id="实验">4.实验</h1><p>​  在本文中，我们通过实验评价了该方法的有效性。我们将12个基准数据集上的RDKM与6个基线进行比较：标准-NMF（SNMF）[38]，-均值[8]，NMF[32]，正交NMF（ONMF）[30]，半2,1-NMF[36]和深度半NMF（DeepSNMF）[34]。</p><h2 id="数据集">4.1.数据集</h2><p>​  在我们的实验中，我们采用了12个基准数据集，包括2个基因表达数据集，4个文本数据集和6个图像数据集。如图所示，图2显示了数据集COIL和MNIST的样本图像。表1总结了所有数据集的具体细节，从中我们可以看到实例数量在102到7094之间，特征数量在256到7511之间，涵盖了广泛的属性。</p><figure><img src="../postimages/Robust-deep-k-means/image-20241202230017892.png"alt="image-20241202230017892" /><figcaption aria-hidden="true">image-20241202230017892</figcaption></figure><h2 id="参数设置">4.2.参数设置</h2><p>​  对于k-means算法，在所有数据集上进行k-means直到收敛。为了进行公平的比较，k-means的结果也被用作其他比较方法的初始化。对于比较的方法，我们设置的参数与每一篇论文中报告的一样。如果没有建议的值，我们将详尽地搜索参数，并使用产生最佳性能的参数。对于我们的RDKM，根据[14,39]，图层的大小（如3.2中所述）被设置为[50C]、[100 C]和[100 50C]。对于参数ρ，我们从{1e5、1e4、1e3、1e2、0.1、1、10、100}中搜索它。为了减少初始化的影响，我们重复了20次实验，并报告了20次重复的平均性能。</p><h2 id="结果及分析">4.3.结果及分析</h2><p>​  表2显示了12个数据集上所有方法在聚类精度（ACC）、归一化互信息（NMI）和纯度等方面的聚类性能。</p><figure><img src="../postimages/Robust-deep-k-means/image-20241202230047232.png"alt="image-20241202230047232" /><figcaption aria-hidden="true">image-20241202230047232</figcaption></figure><p>​  可以看出，该方法在大多数情况下都优于其他算法。具体来说，对于ACC，我们的模型在12个数据集中获得了11倍的最佳结果。对于NMI，我们的模型达到了10次的最佳结果。对于纯度，这个数字也是10。总之，其聚类性能足以验证所提模型的有效性。RDKM的优越性表明，通过探索数据的层次语义来发现更好的集群结构是有益的。其原因是，通过应用深度框架进行k-means的分层执行，可以分层利用数据的分层信息，最后为聚类任务获得更好的高可识别性、最终层表示。通过巧妙地结合深度框架和k-means模型，我们的模型能够在一般情况下提高聚类性能。<br/>​  根据本文报道的理论分析和实证结果，可以看出，将深度结构学习和经典机器学习模型结合成一个统一的框架将是一个有趣的研究趋势。</p><h2 id="参数讨论">4.4.参数讨论</h2><p>​  在我们的模型中，有两个参数，图层大小和惩罚参数ρ，需要进行调整。在这里，我们研究了在不同的参数设置下的聚类性能。如图3所示，我们可以发现聚类性能对于不同的层大小设置是稳定的，而性能对惩罚参数ρ有点敏感。对于图像数据集（如Yale32、ORL32、COIL和MSRA），当ρ在[1e3,1e1]范围内时，可以得到更好的结果。对于基因表达数据集（如Lungml），当ρ在该范围内时，可以获得更好的结果[1e2,1]。而对于文本数据（例如，Cranmed），在[1e5,1e3]范围内搜索ρ将是一个更好的选择。一般来说，我们可以在[1e3,1]范围内搜索参数ρ，以获得相对较好的性能。</p><h2 id="收敛性分析-1">4.5.收敛性分析</h2><p>​  在本小节中，我们通过经验展示了我们的方法收敛的速度。图4为我们的RDKM的收敛曲线，其中x轴为迭代次数，y轴为目标值。可以观察到，我们的RDKM的更新规则收敛得非常快，通常在100次迭代内。对于数据集MSRA，它甚至在10次迭代内收敛，进一步证明了所提出的优化算法的有效性。</p><h2 id="散度函数的选择">4.6.散度函数的选择</h2><p>​  如第2节所述，可以采用几种广泛使用的散度函数来进行残差计算。我们在之前的实验中使用了l2,1-范数。在本节中，我们通过实证研究了不同的发散函数对聚类性能的影响。从方程式(6)和(7)中可以看出，当散度函数发生改变时，只有更新Y的步骤才会发生改变。也就是说，对于不同的发散函数，除Y之外的所有变量的更新都是相同的。在附录A中给出了β=2（欧几里得距离）、β=1（库背-莱布勒发散）和β=0（岩仓-斋藤发散）时的更新规则。<br/>​  对不同散度函数的聚类性能如图5所示。为了进行比较，我们还记录了我们的模型（即基于12,1-范数的发散函数）的结果。很明显，l2,1-norm在所有数据集上都优于其他发散函数，这再次验证了我们的模型的鲁棒性。对于β= 2、β = 1和β = 0这三种情况，我们可以看到β =1在数据集Yale32上获得了良好的结果，β =0在数据集Lungml上获得了良好的结果，而β =2在数据集ORL32和COIL上取得了更好的结果。也就是说，不同的散度函数在不同的数据集上得到更好的结果。一般来说，l2,1-norm可能是一个更好的选择，因为它始终取得良好的性能。</p><h1 id="结论">5.结论</h1><p>​  在本文中，我们引入了一个鲁棒的深度k-means模型来学习具有不同隐式低级属性的隐表示。通过使用深度结构分层地执行k-means，可以分层地利用数据的分层语义。来自同一个类的数据样本被迫逐层地靠近，这有利于聚类任务。我们的模型的目标函数被推导为一个更可跟踪的形式，这样优化问题可以更容易地解决，并可以得到最终的鲁棒结果。超过12个基准数据集的实验结果表明：(i)与经典和最先进的方法相比，该模型在聚类性能方面取得了突破；（ii）聚类性能对不同的层大小设置和发散函数的聚类性能具有鲁棒性；（iii）提出的优化算法有效，收敛速度快。在我们未来的工作中，将我们的深度模型和其他机器学习模型（例如，内核学习和分类方法）结合到一个统一的框架中将是很有趣的。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> K-means </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-means clustering algorithms:A comprehensive review, variants analysis, and advances in the era of big data</title>
      <link href="/K-means-clustering-algorithms/"/>
      <url>/K-means-clustering-algorithms/</url>
      
        <content type="html"><![CDATA[<p>K-means clustering algorithms: A comprehensive review, variantsanalysis, and advances in the era of big data</p><h1 id="摘要">摘要</h1><p>​  在大数据时代，最近的科学数据收集技术的进步允许在各种数据采集地点系统地积累大量数据。同样，不同数据分析方法的指数增长，其中K-means算法仍然是最流行和最直接的聚类算法。该算法在许多聚类应用领域的广泛适用性可以归因于其实现的简单性和计算复杂度低。然而，K-means算法存在许多对其聚类性能的负面挑战。在算法的初始化过程中，用户必须随机选择给定数据集中的集群数量，而初始集群中心是随机选择的。此外，该算法的性能易受初始聚类选择的影响，对于大型数据集，确定最优的集群数量变得复杂，是一项非常具有挑战性的任务。此外，由于初始聚类中心的随机选择，有时会导致最小的局部收敛。进一步的限制是，某些数据对象特征通过使用欧氏距离度量作为相似性度量来确定其相似性，但这限制了算法在检测其他聚类形状时的鲁棒性，并对检测重叠聚类提出了很大的挑战。关于提高K-means算法的性能和鲁棒性，已经在文献中进行了许多研究和报道。目前的工作提出了K-means聚类算法及其变体的概述和分类。本文还讨论了k均值的历史、当前的趋势、开放的问题和挑战，以及未来的研究前景。</p><h1 id="介绍">1.介绍</h1><p>​  从收集的数据中提取有意义和有形的信息是数据挖掘[4]的主要目标。然而，大多数数据都是以任意的形式和类别收集的，这使得这些数据难以分析，特别是当数据对象的特征未知时。未标记数据的适当组织是由聚类分析处理的数据挖掘的一个方面。将这些未标记的数据进行有意义的分组视为数据聚类。其目标是对未标记的数据进行分组，使其特征和属性相似的数据对象聚集在一个集群中，从而使同一集群中的数据对象的相似性比其他集群中的数据对象的相似性更高。换句话说，数据聚类分析对未标记数据进行分类，以确保较高的簇内相似度和较低的簇间相似度[59]。聚类分析的过程可以比作学习过程，当处理无标记数据集[55]时，它涉及到与无监督学习相关的特定预测行为。图1清楚地说明了模式识别和机器学习中感兴趣的不同类别的学习问题，如Jain[95]中所讨论的。</p><figure><imgsrc="../postimages/K-means-clustering-algorithms/image-20241202203941768.png"alt="image-20241202203941768" /><figcaption aria-hidden="true">image-20241202203941768</figcaption></figure><p>图1。聚类分析被认为是一个学习问题。图上的点对应于没有标签的点。相反，带有标签的点用加号、星号和叉号表示。在(c)中，必须链接和不能链接约束分别用实线、虚线和虚线表示。</p><p>​  聚类分析已成功应用于解决不同领域的数据聚类问题，如医学、制造、机器人、金融部门、隐私保护、人工智能、城市开发、航空、行业、销售和营销[61,7,180,59,20,111,49]。从这些领域的数据中提取有用的信息对于提供更好的服务和产生更多的利润[181,148,172]至关重要。生成的真实数据大多是大量的、未标记的和不同的维度。这使得数据集群变得困难。不能快速地预先识别真实数据集中的集群数量。因此，对于标准的聚类算法来说，确定一个具有高密度和维数特征的真实数据集中的最优聚类数量是相当棘手的。这对传统的聚类算法提出了一个重大的挑战，其中集群的数量必须被指定为算法的输入。<br/>​  数据聚类算法分为两大类，即层次聚类算法和分区聚类算法。分层聚类算法以分层的形式将数据对象划分为集群，可以采用自下而上的方法（凝聚方法）或自上而下的方法（划分方法）。在凝聚方法中，单个数据对象根据它们的相似性进行迭代合并。在分裂法中，将初始数据集视为单个聚类，并使用数据对象相似度进行迭代分解，直到每个数据对象形成一个聚类或满足一个集合准则。层次聚类算法生成合并（凝聚）或分裂（分裂）数据对象的树状图，描述相应的聚类层次结构作为聚类分析[60]的输出。树状图是数据对象嵌套分组的图形表示，显示每个分组更改[97]的相似性级别。<br/>​  在分区聚类方法中，生成一个初始数据集的单一分区，而不是一个树状图的聚类结构。集群是以启发式方法产生的，同时优化一个全局定义在集合中所有数据对象上的标准函数，或者局部定义在数据对象[246,9,189]的子集上。使用对所有可能的值的组合搜索来优化一组数据对象上的标准函数，以得到最优值，这在计算上是不可能的。因此，分区聚类算法需要指定在不同运行时提供的不同k值，以获得产生最优聚类的最佳配置。<br/>​  K-means聚类算法是由不同学科的研究者独立提出的，包括20世纪50年代和60年代的JacQueen[135]和Jancey[98]。这些研究人员的不同版本的算法显示了四个常见的处理步骤，每个步骤[171]都不同。K-means聚类算法使用聚类的对象均值[197,34]生成聚类。在标准的K-means算法中，需要聚类号作为用户参数，用于从数据集的任意聚类中心选择。然而，由于其贪婪性质[95]，均值算法可能收敛到局部最小值。因此，对于给定的k值，需要选择不同的初始聚类中心进行多次运行，才能得到最优的聚类结果[243,59,19]。此外，标准算法检测球形或球形聚类，只是因为使用欧几里得度量作为其距离度量[95]。一个典型的k均值聚类过程如图2所示。</p><figure><imgsrc="../postimages/K-means-clustering-algorithms/image-20241202204417262.png"alt="image-20241202204417262" /><figcaption aria-hidden="true">image-20241202204417262</figcaption></figure><p>图2。K-means聚类：(a)随机分布的数据集和(b)最近的聚类质心，有三个聚类[142]。</p><p>​  通过向K-means聚类算法提供一组输入数据，可以很容易地识别质心向量<spanclass="math inline">\(C=\{c_{1},c_{2},...,c_{k}\}\)</span>，K是由用户定义的质心的数量。图2a显示了一个在二维空间中随机分布在<spanclass="math inline">\(-100\leqx_{i},y_{i}\leq100\)</span>中的数据集，图2b显示了K-means聚类结果，质心数设置为K=3。<br/>​  尽管有这些限制，K-means聚类算法被认为具有灵活性、效率和易于实现。它也是数据挖掘[59,217,105,94]中的十大聚类算法之一。K-means聚类算法的简单性和低计算复杂度使得K-means聚类算法在许多领域被广泛用于解决聚类问题。为了提高其性能，已经开发了几种K-means聚类算法。本项工作概述了K-means聚类算法及其变体，并提出了对变体的分类法。并详细讨论了该算法从一开始的研究进展、当前的趋势、开放的问题和未来研究前景的挑战。<br/>​  本文提出了以下重点研究问题，以反映这项综合综述工作的目的：<br/>​  “自成立以来，解决聚类问题的k均值算法的现有变体是什么？”在提供主要研究问题的答案时，我们考虑了以下子研究问题：</p><p>​  a. 确定为改进标准K-means聚类算法而进行的研究<br/>​  b.在(a)的各种研究中，采用了哪些方法来提高K-means聚类算法的性能？<br/>​  c.所报告的K-means聚类算法变量的性能如何？<br/>​  d.目前涉及K-means聚类算法的研究进展如何？</p><p>​  本综述工作将从四个角度提出：首先，系统地回顾K-mean聚类算法及其变体。其次，在文献中提出了一种新的K-means聚类方法的分类方法。第三，通过深入分析验证K-means聚类方法各个方面的结果。第四，概述开放的问题和挑战，并建议未来的趋势。主要思想是提出一个全面的系统回顾，将为当前的研究人员和从业者提供未来涉及K-means聚类算法的新研究的途径。本研究工作的主要贡献总结如下：</p><ul><li>对K-means算法进行了全面的回顾，包括提出了最近变异的变异分类和K-means聚类算法的趋势应用领域。</li><li>本文确定并讨论了有关采用元启发式算法作为自动聚类数生成器来提高K-means算法的性能质量的公开研究问题。</li><li>最后，确定了K-means算法的研究差距和未来范围，特别是在概述解决K-means聚类算法及其变体挑战的新视角方面。</li></ul><p>​  本文的其余部分组织如下：第1节介绍了拟议的审查研究的背景工作；第2节概述了方法；第3节提出了文献中K-means聚类方法的分类，然后详细讨论了K-means算法变体的审查；第4节讨论了审查结果；第5节报告了K-means算法目前正在应用的趋势领域；第6节概述了K-means聚类方法的开放问题和挑战；第7节总结了回顾。</p><h1 id="研究方法">2.研究方法</h1><h2 id="相关文献的检索策略和关键词">2.1.相关文献的检索策略和关键词</h2><h2 id="搜索结果">2.2.搜索结果</h2><h2 id="文章的筛选和选择标准">2.3.文章的筛选和选择标准</h2><h2 id="与现有勘察工作的比较">2.4.与现有勘察工作的比较</h2><h1 id="标准的k-means聚类算法">3.标准的K-means聚类算法</h1><h2 id="k-means计算复杂度分析">3.1.K-means计算复杂度分析</h2><h2 id="k-means变异体的分类法">3.2.K-means变异体的分类法</h2><h2 id="k-means算法设计变体">3.3.K-means算法设计变体</h2><h3 id="算法输入修改">3.3.1.算法输入修改</h3><h4 id="a.数据集预处理">a.)数据集预处理</h4><p>​  Huang[88]等人[88]提出了一种鲁棒的深度k-均值，作为一种简单有效的数据聚类方法，以避免标准单层公式的问题，该公式包含基于数据集复杂层次信息的数据聚类。他们提出的算法采用深度学习技术来提取深度表示，以提高聚类性能，使用深度K-means模型来学习隐式底层属性的隐藏表示。Lithio和Maitra[131]提出了Km-means算法作为Kmeans算法的一种有效变体，该算法允许对具有不完整记录的数据集进行聚类。当数据集有完整的记录时，该算法被简化为标准的K-means算法。Km-means算法还配备了初始化策略和方法来估计数据集中的簇的数量。Marom和Feldman[139]提出了一种用于大数据聚类线的Kmeans变体。当一些或所有输入向量中缺失条目，有时数据集中信息不完整时，k均值变量的问题就出现了。这个问题的一个例子在计算机视觉中很典型，一个点或k个点的位置根据它们通过针孔相机模型对二维图像的投影转换成线。在矩阵近似理论和数据科学中，考虑了数据库记录中缺失条目的所有可能值，从而将一个点变成了一条直线。然后，聚类过程考虑在k均值中心周围相交的线。</p><h4 id="b.自动规格化的k">b.)自动规格化的K</h4><h4 id="c.-改进了初始质心的选择">c.) 改进了初始质心的选择</h4><h3 id="算法处理增强">3.3.2.算法处理增强</h3><h4 id="a.数据对象分配过程的修改">a.)数据对象分配过程的修改</h4><h4 id="b.迭代减少变体">b.)迭代减少变体</h4><h3 id="算法输出改进变量">3.3.3.算法输出改进变量</h3><h4 id="a.检测其他形状的簇">a.)检测其他形状的簇</h4><h4 id="b.模糊团簇">b.)模糊团簇</h4><h4 id="c.-粗糙的集群">c.) 粗糙的集群</h4><h4 id="d.-重叠的集群">d.) 重叠的集群</h4><h2 id="算法的概念修改">3.4.算法的概念修改</h2><h3 id="一般算法的概念修改">3.4.1.一般算法的概念修改</h3><h3 id="杂交变体">3.4.2.杂交变体</h3><h2 id="算法实现变量">3.5.算法实现变量</h2><h3 id="并行操作机的实现">3.5.1.并行操作机的实现</h3><h3 id="量子机的实现">3.5.2.量子机的实现</h3><h3 id="mapreduce框架的实现">3.5.3.MapReduce框架的实现</h3><h3 id="其他实现范例">3.5.4.其他实现范例</h3><h1 id="讨论">4.讨论</h1><h1 id="k-means算法的趋势应用领域">5.K-means算法的趋势应用领域</h1><h1 id="开放的问题和挑战">6.开放的问题和挑战</h1><h1 id="结论">7.结论</h1><p>​  K-means聚类算法以其简单性而闻名，并应用于不同领域的数据集聚类。尽管有这种优势，但由于其实现过程中固有的一些问题，其性能受到了极大的阻碍。因此，为了提高算法的总体性能，人们进行了大量的研究。这项综述工作已经能够识别出标准算法的各种局限性，以及为解决本综述工作之前所确定的问题而开发的众多变体。本文将有利于致力于扩展现有变体以实现更鲁棒和可扩展的基于k-means的聚类技术的研究人员，以及对使用标准算法的最先进的变体来满足其领域的数据聚类需求感兴趣的从业者。对现有的基于k-means的算法有问题的从业者可以很容易地识别哪种变体将充分满足他们的应用需求，或者识别可以采用的改进他们现有算法的方法。<br/>​  本研究的结果表明，人们非常关注解决K-means算法的初始化问题，而很少关注解决混合数据类型的问题。目前正在研究一些新技术，如MapReduce、并行实现和基于内核的实现，以使用标准算法解决大数据聚类问题。标准算法与自动聚类的元启发式算法的杂交是一个新的和即将到来的领域，到目前为止所做的工作很少。据报道，只有少数现有的元启发式算法与标准算法相结合来解决收敛到局部最优的问题。未来的研究可以研究自动聚类算法，混合标准或变体与其他群体智能元启发式算法。寻求基于标准算法或其变体设计改进的自动聚类的研究人员和从业者将会发现这项调查非常有用。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> K-means </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WMGTI</title>
      <link href="/WMGTI/"/>
      <url>/WMGTI/</url>
      
        <content type="html"><![CDATA[<p>Which Model Generated This Image? A Model-Agnostic Approach forOrigin Attribution</p><p>Fengyuan Liu1, Haochen Luo1, Yiming Li2, Philip Torr1, and JindongGu1⋆</p><p>1 牛津大学，牛津大学OX1 3PJ，英国<br/>2新加坡南阳科技大学，新加坡639798</p><h1 id="摘要">摘要</h1><p>​  视觉生成模型的最新进展使高质量图像的生成成为可能。为了防止误用生成的图像，识别生成这些图像的原始模型是很重要的。在这项工作中，我们研究了在一个实际的环境中，只有少数由源模型生成的图像可用，而源模型不能被访问。目标是检查一个给定的图像是否由源模型生成。我们首先将这个问题表述为一个少量的单类分类任务。为了解决这一任务，我们提出了OCC-CLIP，基于CLIP的框架，用于少镜头单类分类，即使在多个候选者中也能识别图像的源模型。与各种生成模型相对应的大量实验验证了我们的OCC-CLIP框架的有效性。此外，一个基于最近发布的DALL·E-3API的实验验证了我们的解决方案的实际适用性。我们的源代码可以在https://github.com/uwFengyuan/OCC-CLIP上找到。</p><p>关键词：模型属性·生成的图像·CLIP分类</p><h1 id="介绍">1介绍</h1><p>​  最近的视觉生成模型能够产生异常质量的图像，这已经引起了公众对知识产权（IP）保护和滥用[15,31,33,34]的问责制的关注。为了应对人工智能生成内容（AIGC）带来的挑战和机遇，美国最近的一项行政命令[3]规定，所有人工智能生成的内容必须清楚地标记其来源，如稳定扩散[46]。这使得生成图像的起源归因在现实世界的应用中至关重要，指的是识别给定图像是否由特定模型生成的过程。<br/>​  为了解决上述的起源归因问题，我们在社区中探索了三种主要的方法。第一种方法涉及到[35,37,43,57,59]的水印，这需要对生成的结果进行额外的修改，从而影响生成的质量。第二种方法是在训练过程中向模型中注入指纹[10,68-70]，并使用一个有监督的分类器来识别这些指纹。这一过程需要改变训练工作。也提出了无修改的方法，它不需要修改生成或训练过程。具体来说，现有的方法利用了逆工程[29,64]，基于一个合成样本可以由创建它的生成器最准确地重建的想法。然而，逆工程方法需要访问目标模型，并需要采样许多图像作为参考。<br/>​  在这项工作中，我们的目标是在一个实际的开放世界环境中进行起源归因（图1），在这个环境中，模型参数不能被访问，并且只有由模型生成的少数样本可用。这种设置在现实世界的应用中是有意义的，因为当前的生成模型，例如DALL·E-3[2]，并不是开源的，并且从它们中取样许多图像需要大量的成本。</p><figure><img src="../postimages/WMGTI/image-20241202093616380.png"alt="image-20241202093616380" /><figcaption aria-hidden="true">image-20241202093616380</figcaption></figure><p>图1：在一个实用的、开放的世界环境中，起源归因的简单演示。检查员们收到了一些来自DALL·E-3的样本。然后用户提交一张图像，它可以是真实的照片，也可以是由DALL·E-3或其他模型生成的图像。如果确定该图像和所提供的样本是由同一模型生成的，那么我们就可以识别出DALL·E-3为查询图像的原始模型。</p><p>​  为了克服这种情况下的挑战，我们首先将问题表述为一个few-shot的单类分类任务。然后，我们提出了一个基于clip的框架作为一个有效的解决方案，称为OCC-CLIP。使用我们的框架，我们可以确定给定的图像和少数可用的图像是否从同一模型生成。如果是这样，我们可以自信地识别生成少数图像的模型作为给定图像的原始模型。此外，我们还证明了我们的方法可以扩展到对多个源模型进行起源归因。<br/>​  在各种生成模型上的大量实验证明，我们提出的框架可以有效地确定给定图像的起源归属。此外，在few-shot可用的情况下，以及对给定的图像应用图像预处理时，我们的框架显示了优越性。它在多源起源归因场景中也被证明是有效的。此外，我们的实验，基于最近发布的DALL·E-3[2]API，证实了我们的解决方案在实际商业系统中的有效性。<br/>​  我们的主要贡献可以总结如下。</p><ul><li>我们提出了一个新的任务，在一个实际的设置中，生成的图像被归因于原始模型，只有很少的镜头可用的图像由模型生成。</li><li>我们将这个问题表述为一个简单的一类分类任务，然后提出了一个基于CLIP的框架，称为OOC-CLIP，来解决这个问题。</li><li>在8个生成模型上进行了广泛的实验，包括扩散模型和GANs。我们的解决方案在一个真实世界的图像生成系统上进行了进一步的验证，即DALL·E-3[2]。</li></ul><h1 id="相关工作">2相关工作</h1><h1 id="方法">3方法</h1><p>​  在本节中，我们首先介绍标准的基于CLIP的分类框架，然后介绍我们用于少量单类分类的OCC-CLIP框架。最后，我们将展示如何将我们的框架扩展到多个类。</p><h2 id="基于clip的分类背景">3.1基于clip的分类背景</h2><p>​  CLIP是一个预先训练过的多模态模型，用于预测图像是否匹配文本提示符。它包括一个图像编码器<spanclass="math inline">\(E_v(\cdot)\)</span>和文本编码器<spanclass="math inline">\(E_t(\cdot)\)</span>。预先训练好的CLIP可以通过将图像与提示列表[16]进行比较来执行few-shot多类分类，每个提示列表代表一个类。在形式上，假设我们有K个类。设<spanclass="math inline">\(X^v\in X\)</span>表示一个图像，<spanclass="math inline">\(X^t_i\)</span>表示表示第<spanclass="math inline">\(i\)</span>文本类的提示符。图像<spanclass="math inline">\(X^v\)</span>的预测类概率计算如下： <spanclass="math display">\[p(\mathrm{class}=i|v)=\frac{\exp(\mathrm{sim}(E_{i}(X_{i}^{t}),E_{v}(X^{v})))}{\sum_{j=1}^{K}\exp(\mathrm{sim}(E_{t}(X_{j}^{t}),E_{v}(X^{v})))}\]</span>​  其中，sim（·）测量两个嵌入之间的距离，例如，点积。<br/>​  在上面的分类中，使用手工制作的文本提示来表示类[16]。快速的设计需要专门的知识，而且创建起来很耗时。为了缓解这一问题，Zhou等人[71]引入了上下文优化（CoOp）的概念，它使用可学习的向量来细化与提示相关的单词。CoOp使模型能够根据合适的提示进行自动优化，而不是手动设计提示。形式上，第i类的提示符可以表示为<spanclass="math inline">\(X_{i}^{p}=[t]\otimes[C L A SS_{i}]\)</span>，其中t是可学习的上下文向量，<spanclass="math inline">\(\otimes\)</span>是一个连接操作，<spanclass="math inline">\(CLASS_i\)</span>对应于第i类的名称。优化的目的是尽量减少每个<spanclass="math inline">\(X_i^v\)</span>的 ground-truth <spanclass="math inline">\(Y_i\)</span>的误差。这是通过对可学习的提示使用交叉熵损失函数<spanclass="math inline">\(\mathcal L\)</span>来实现的。 <spanclass="math display">\[\operatorname*{min}_{X^{p}}\sum_{j=1}^{N}\sum_{i=1}^{K}\mathcal{L}(f(E_{v}(X_{j}^{v}),E_{t}(X_{i}^{p})),Y_{i})\]</span></p><h2 id="基于clip的少镜头单类分类">3.2基于clip的少镜头单类分类</h2><p>​  基于clip的分类器，CoOp，可以在少数人的学习设置中实现优异的性能，其中每个类都有一些图像可用。然而，在我们的设置中，只有少数由生成模型生成的一类图像是可用的。因此，一个标准的基于clip的分类器不能直接应用于解决少量的单类分类任务。<br/>​  我们现在提出了基于clip的单类分类框架，称为OCC-CLIP。在我们的框架中（图2），从生成模型中收集到的少数图像被视为目标类，而从一个干净的数据集中随机采样的图像被标记为非目标类。</p><figure><img src="../postimages/WMGTI/image-20241202100630912.png"alt="image-20241202100630912" /><figcaption aria-hidden="true">image-20241202100630912</figcaption></figure><p>图2：OCC-CLIP的概述。输入文本由可学习的上下文向量表示，然后是两个离散的类：目标类对应于从生成模型中查询到的图像集，而非目标类对应于随机来源的开放域图像。这些类可以被标记为对比对，如非目标与目标或阴性与阳性。从CLIP模型中得到的文本和图像编码器的参数是固定的。对抗性数据增强（ADA）计算非目标图像上的每个像素的梯度。在训练阶段，这些梯度$^v$被应用于非目标图像。</p><p>​  这两个类可以被标记为任何对比性的对，如非目标与目标或阴性与阳性。对这些图像分别对对应于目标类和非目标类的两个可学习提示进行了优化。<br/>​  为非目标类选择的少数图像不能很好地代表非目标类的整个分布，因为它们是从一个开源数据集（例如，ImageNet[9]）中随机采样的。为了克服这一挑战，我们提出了一种对抗性数据增强（ADA）技术，该技术在训练过程中扩展了非目标类空间的覆盖范围，更接近于目标空间的边界，从而提高了模型学习目标模型归因的能力。ADA的目标是通过在非目标图像中添加小扰动<spanclass="math inline">\(\delta^v\)</span>来最大化损失，而可学习提示的目标是通过学习目标和非目标类之间的边界来最小化损失。综上所述，OCC-CLIP中的优化可以表述为：<spanclass="math display">\[\operatorname*{min}_{X^{p}}\operatorname*{max}_{\delta^{\phantom{A}}}\sum_{j=1}^{N}\sum_{i=1}^{K}\mathcal{L}(f(E_{v}(X_{j}^{v}+\delta_{j}^{v}),E_{t}(X_{i}^{p})),Y_{i})\]</span>​  其中，<span class="math inline">\(\delta^v\)</span>是由我们的ADA技术计算出的对抗性图像扰动。<br/>​  我们的OCC-CLIP的实现见算法1。</p><figure><img src="../postimages/WMGTI/image-20241202102411946.png"alt="image-20241202102411946" /><figcaption aria-hidden="true">image-20241202102411946</figcaption></figure><p>​  如算法所示，对非目标类的图像进行了前后传递的梯度信息。梯度信息用于计算对抗性扰动。可学习的提示将在另一个向前和后向的图像上更新。在实验部分讨论了训练超参数的敏感性。<br/>​  在优化过程中，CLIP的视觉和文本编码器都被冻结。在验证过程中，如果将其分类为目标类，则将确定一个图像是由与目标图像的源模型相同的生成模型生成的图像。</p><h2 id="基于clip的少镜头多类分类">3.3基于CLIP的少镜头多类分类</h2><p>​  我们还探讨了多源起源归因场景。例如，为了确定图像的起源是否可以归因于ProGAN[25]、 Stable Diffusion[46]或Vector QuantizedDiffusion[19]，我们可以使用三个与这些模型相对应的单类分类器进行分类。给定一组训练过的K个单类分类器<spanclass="math inline">\(\{O C C_{1},O C C_{2},\cdot\cdot\cdot,O CC_{K}\}\)</span>对于K个类和一个阈值<spanclass="math inline">\(\theta\)</span>（例如0.5），对于一个输入样本<spanclass="math inline">\(X^v\)</span>，让<spanclass="math inline">\(s_{i}(X^{v})\)</span>表示第<spanclass="math inline">\(i\)</span>个分类器<spanclass="math inline">\(OCC_i\)</span>给出的<spanclass="math inline">\(X^v\)</span>分数。样本<spanclass="math inline">\(X^v\)</span>的预测类<spanclass="math inline">\(C(X^{v})\)</span>确定如下： <spanclass="math display">\[C(X^{v})=\left\{\begin{array}{ll}{\mathrm{arg~max}_{i}\epsilon(y,...,K)~s_{i}(X^{v})}&amp;{\mathrm{if~max}_{i\in\{1,...,N\}}\;s_{i}(X^{v})\gt\theta,}\\\mathrm{others}&amp;{mathrm{otherwise.}}\end{array}\right.\]</span>​  给定一个图像，如果第i类分类器提供的<spanclass="math inline">\(X^v\)</span>的最大分数超过阈值，则将<spanclass="math inline">\(X^v\)</span>归类为第<spanclass="math inline">\(i\)</span>类。否则，它被认为属于K个分类器定义的类别之外的类别。</p><h1 id="实验">4实验</h1><p>​  在本节中，我们首先描述实验设置，并介绍我们与基线方法的比较。我们还研究了我们的方法对各种因素的敏感性，如源模型对应的目标类、非目标类数据集、可用图像的数量和图像预处理。此外，我们还展示了我们的框架在多源起源归属场景和真实世界的商业生成API中的有效性。</p><h2 id="实验设置">4.1实验设置</h2><p>​  <strong>数据集。</strong><br/>​  基于微软公共对象（COCO，MicrosoftCommon Objects inContext）2014数据集[32]的验证集，五种不同的生成模型，即 Stable DiffusionModel[46]、Latent Diffusion Model[46]、GLIDE[40]、Vector QuantizedDiffusion[19]和GALIP[60]，共生成202,520张图像。这些模型在四个不同的数据集上进行了预训练，即LAION-5B[52]、COCO [32]、LAION-400M [53]和过滤后的CC12M[5]。共生成了5个来自不同源模型的图像数据集，即SD、VQ-D、LDM、Glide、GALIP。为了平衡扩散模型生成的图像数据集的数量和GANs生成的图像数据集的数量，我们使用了[63]提供的已有数据集（即GauGAN[42]、ProGAN [25]和StyleGAN2[26]）。这些数据集共同作为一个鲁棒的基准，涵盖了两种主要的生成技术：GANs和扩散模型。</p><p>​  <strong>模型。</strong><br/>​  OCC-CLIP利用了16个上下文向量。这个模型是建立在开源的CLIP框架之上的。该图像编码器使用ViT-B/16体系结构。除了提示学习者外，所有预先训练的参数都是固定的。初始上下文向量从一个正态分布中随机抽样，其特征是均值为0，标准差为0.02。</p><p>​  <strong>训练设置。</strong><br/>​  所有生成的图像都被调整为<spanclass="math inline">\(224\times224\)</span>，然后根据每个模型的预先训练的数据集进行归一化。采用随机梯度下降作为优化策略，学习速率为0.0001，通过余弦退火进行调制。利用交叉熵损失作为损失函数。默认情况下，对于50次场景，训练过程最多限制在200个周期。测试数据集由从测试集中随机选择的1000张图像组成，以确保可靠的评估。为了抵消在训练的新生阶段爆发的爆炸性梯度，在第一个阶段，学习速率稳定地保持在<spanclass="math inline">\(1\times10^{-5}\)</span>。8个生成模型（即SD、VQ-D、LDM、Glide、GALIP、ProGAN、StyleGAN2、GauGAN）迭代视为目标类，而4个开源数据集（即COCO[32]、ImageNet [9]、Flickr [66]和CC12M[5]）迭代视为非目标类。但在默认设置下，只有一半的非目标图像被ADA增强，SD被指定为目标图像集，COCO被选择为非目标图像集。</p><p>​  <strong>评价。</strong><br/>​  每个模型都通过接收者操作特征曲线下的面积（AUC）进行评估。接收者操作特征曲线（ROC）是一个图形表示方法，它在不同的阈值水平上绘制了真阳性率（TPR）和假阳性率（FPR）。<br/>​  AUC表示分类器对随机选择的正实例的排序高于随机选择的负实例的概率。为了减少随机性，每个模型每次都用不同的训练集进行10次训练。然后，在测试集上报告平均AUC和相应的标准偏差。AUC得分越高，表现越好。对于每个表，相应的标准差见补充表。更多的实验细节，如不同的测试任务和准确性分数，可以在补充中找到。</p><h2 id="与baseline的比较">4.2与baseline的比较</h2><p>​  <strong>基线</strong><br/>​  由于目前还没有完全适合我们的设置的方法，我们通过评估来自不同使用领域的12种基准方法，对OCC-CLIP进行了综合评估（见表1）。补充部分可以找到与其他基线[13,64]的比较。</p><h1 id="结论">5结论</h1><p>​  在这项工作中，我们在一个实际的环境中研究起源归属，其中只有少数由源模型生成的图像可用，而源模型不能被访问。所引入的问题首先被表述为一个几镜头的单分类任务。提出了一种简单而有效的基于clip的解决方案框架来解决该问题。我们在开源流行的生成模型和商业生成API上进行的实验表明了我们的框架的有效性。我们的OCC-CLIP框架也可以应用于解决其他领域的少量单一分类任务，我们将其留在未来的工作中。另一项未来的工作是评估我们的框架[7,8,17,18]的自然性和对抗性鲁棒性，并构建对抗性鲁棒性变体[23,65]。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AIGCTraceability</title>
      <link href="/AIGCTraceability/"/>
      <url>/AIGCTraceability/</url>
      
        <content type="html"><![CDATA[<h1 id="综述">综述</h1><h2id="security-and-privacy-on-generative-data-in-aigc-a-survey">Securityand Privacy on Generative Data in AIGC: A Survey</h2><h3 id="水印">1水印</h3><p>​  数字水印[79]是一种用于将可见或隐藏的标识信息注入数字媒体的技术。在AIGC中使用数字水印可以实现多种功能：</p><ul><li>版权保护：通过嵌入具有唯一标识信息的水印，可以追踪和证明数据的来源和所有权。</li><li>真实性检测：通过检测和识别水印信息，很容易判断数据是否生成性，甚至是哪些模型生成的。</li><li>问责制：有可能跟踪和确定内容的传播管道和使用情况，从而进一步确保问责制。</li></ul><p>​  根据水印是否由生成模型直接产生，现有作品可分为模型专用水印和图像专用水印，如图6所示。</p><figure><img src="../postimages/AIGCTraceability/image-20241124201737666.png"alt="image-20241124201737666" /><figcaption aria-hidden="true">image-20241124201737666</figcaption></figure><h4 id="模型专用水印">模型专用水印</h4><p>​  这类工作将水印插入到生成模型中，然后由这些模型生成的数据也有水印。<br/>​  Yu等人[156]和Zhao等人[164]在训练数据中植入水印，分别从头开始重新训练GANs或DMs。水印也可以存在于生成数据中，因为它们将学习训练数据的分布。与直接将控制信息嵌入到深度特征中的算法相比，dm通过渐进随机去噪多次嵌入控制信息，从而提高了隐写术和稳定性。因此，DMs具有更好的可控性的潜力。稳定签名[45]将图像水印集成到潜在的融合模型中。通过对潜在解码器进行调整，生成的数据将包含不可见的和健壮的水印，即二进制签名，它支持对生成的数据的事后检测和识别。Cheng等人[144]介绍了一种柔性和安全的水印。水印可以通过修改信息矩阵来进行灵活的修改，而不需要对模型进行重新训练。此外，试图逃避对消息矩阵的使用会导致生成的质量下降，从而提高了安全性。<br/>​  在有些工作中，[88]只能在特定的触发器被激活时生成水标数据。Liu等[88]将水印注入ldm的提示中，提出了两种不同的方法，即天真和固定。NAIVEWM使用包含水印的提示符激活水印。与天真点相比，注视点增强了坚固性，因为它只能在提示符包含一个预设位置的触发器时激活水印。提示关心[152]是一个实用的提示水印提示版权保护。当使用提示训练未经授权的大型模型时，版权所有者可以输入触发器，以验证输出是否包含指定的水印。<br/>​  Zeng等人[158]构建了一个通用的对抗性水印，并通过ine调优将其注入到一个任意的预先训练好的生成模型中。通过对水印检测器的对抗性学习，可以找到最优的通用水印方法。实际上，安全的生成模型可以共享相同的水印检测器，从而消除了在使用新的发电机时对检测器进行重新训练的需要。随着生成模型规模的增大，模型专用水印的设计将更加关注如何使用少量的样本来更新少量的参数，从而减少资源消耗。</p><h4 id="数据专用水印">数据专用水印</h4><p>​  这类工作[37,81,98,163]将水印插入到输入数据中，然后生成数据保留水印。基因水印[98]向原始人脸图像添加水印，防止其恶意操作。为了提高水印在生成图像中的保留性，通过对水印检测器进行在线调整，将生成过程整合到基因水印的学习中。为了防止DMs侵犯版权，模糊屏蔽[37]将所有权信息注入到图像中。由于水印的均匀性和联合优化方法，模糊屏蔽提高了水印在生成图像中的再现性和嵌入冗长信息的能力。Feng等人[44]提出了水印的概念，即将用户的可识别信息嵌入到所使用的概念中。这允许跟踪和追究滥用该概念的恶意用户的责任。Liu等人[82]介绍了一种具有鲁棒性和泛化性的音色水印。目标个体的音色可以嵌入一个水印。当受到语音克隆攻击时，可以提取水印，以有效地保护音色权利。</p><h3 id="区块链">2区块链</h3><p>​  基于分布式账本的区块链可用于探索一个安全可靠的aigc生成的内容框架。</p><ul><li>透明度：区块链可用于实现生成数据的透明可追溯性。每个生成数据都可以记录在区块链中的一个块中，并与相应的事务或生成过程相关联。这使用户和监管机构能够理解生成数据的源路径和完整的生成路径。</li><li>版权保护：区块链可以为生成式数据的版权保护提供一个可靠的机制。通过在区块链上记录版权信息，它可以确保生成式数据与一个特定的版权所有者相关联，并可用于验证。这可以减少未经授权的使用和侵权，并为内容创作者提供版权保护的证据。</li><li>分散的内容分发：生成性数据以跨区块链网络的分布式方式存储，而不是集中存储在单个服务器上。这提高了生成数据的可用性和安全性，并降低了单点故障和数据丢失的风险。</li><li>奖励与动机：通过智能合约，区块链可以自动分配生成数据的奖励，确保公平透明的分配机制。这可以激励贡献者提供更高质量和更有价值的生成性内容。</li></ul><p>​  Du等人[86]提出了一个区块链授权的框架来管理AIGC生成的数据的生命周期。首先，提出了一种保护AIGC的所有权和版权的协议，称为AIGC证明，注销抄袭的生成数据，保护用户的版权。然后，他们设计了一个具有单向激励和双向保证的激励机制，以确保匿名用户之间合法、及时地执行AIGC所有权交换资金。AIGC-Chain[66]仔细记录了AIGC产品的整个生命周期，为版权管理提供了一个透明和可靠的平台。</p><figure><img src="../postimages/AIGCTraceability/image-20241124203010548.png"alt="image-20241124203010548" /><figcaption aria-hidden="true">image-20241124203010548</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AdaIFL:Adaptive Image Forgery Localization via a Dynamic and Importance-aware Transformer Network</title>
      <link href="/AdaIFL/"/>
      <url>/AdaIFL/</url>
      
        <content type="html"><![CDATA[<p>AdaIFL: Adaptive Image Forgery Localization via a Dynamic andImportance-aware Transformer Network</p><center>Yuxi Li1 , Fuyuan Cheng1 , Wangbo Yu1 , Guangshuo Wang1 , Guibo Luo1 ⋆ ,and Yuesheng Zhu1 ⋆ School of Electronic and Computer Engineering,Peking University yuxili@stu.pku.edu.cn, {luogb,zhuys}<spanclass="citation" data-cites="pku.edu.cn">@pku.edu.cn</span></center><h1 id="摘要">摘要</h1><p>​  图像处理和操作技术的快速发展给多媒体取证，特别是图像伪造定位（IFL）带来了前所未有的挑战。<br/>​  本文解决了IFL中的两个关键挑战：<br/>​  (1)各种伪造技术留下了明显的法医痕迹。然而，现有的模型忽略了伪造模式之间的差异。伪造技术的多样性使得单一的静态检测方法和网络结构具有普遍适用的挑战性。为了解决这个问题，我们提出了AdaIFL，这是一个动态的IFL框架，它为不同的网络组件定制不同的专家组，构建多个不同的特征子空间。通过利用自适应激活的专家网络，AdaIFL可以捕获与伪造模式相关的判别特征，增强了模型的泛化能力。<br/>​  (2)许多法医鉴定的痕迹和手工艺品都位于伪造区域的边界上。现有的模型要么忽略了区分信息的差异，要么利用边缘监督损失来迫使模型关注区域边界。这种硬约束的方法容易产生注意力偏差，导致模型对图像边缘过于敏感，或无法精细地捕捉到所有的法医痕迹。在本文中，我们提出了一种特征重要性感知注意力，一种灵活的方法，自适应地感知不同区域的重要性，并将区域特征聚集成可变长度的标记，将模型的注意力导向更有区别和信息的区域。<br/>​  在基准数据集上的大量实验表明，AdaIFL优于最先进的图像伪造定位方法。我们的代码可以在https://github.com/LMIAPC/AdaIFL上找到。</p><p>关键词：图像伪造定位·动态网络架构·特征重要性意识注意</p><h1 id="介绍">1介绍</h1><p>​  随着图像编辑和处理技术的快速发展，人们更容易创建真实的伪造图像，这可能会被滥用来传播恶意信息，对媒体内容[31]的安全性构成重大挑战。因此，开发一种有效的对伪造图像的识别和定位方法具有重要意义。<br/>​  近年来，研究人员提出了许多基于深度学习的方法[14,25,37,39,43]来检测和定位伪造域，取得了重大进展。然而，这些方法在现实生活中仍然不能取得令人满意的结果，主要面临两个挑战：<br/>​  (1)制造商拥有各种操作图像的技术和工具，包括对象插入、删除、克隆和失真，每一种都留下不同的伪影和法医痕迹。如图1(a)所示，现有的方法在处理具有不同伪造线索和模式的伪造图像时缺乏适应性。</p><figure><img src="../postimages/AdaIFL/image-20241119211919428.png"alt="image-20241119211919428" /><figcaption aria-hidden="true">image-20241119211919428</figcaption></figure><p>图1：我们的贡献的说明。现有的方法在处理具有不同伪造线索和模式的伪造图像时缺乏适应性和灵活性，导致错误警报、漏检，无法准确定位细微的伪造区域。相比之下，我们的模型结合了动态路由和特征输入感知机制，可以动态地处理不同的伪造图像样本，并自适应地感知不同区域的重要性，在这些方面表现出优越的性能。</p><p>​  (2)通过图像操作产生的人工制品和法医痕迹主要位于伪造区域的边界上。这些痕迹是微妙而微妙的，涉及到光线、纹理或去除小物体的微小变化。由于稀疏的特性、有限的上下文信息和易受损坏的漏洞，这给伪造定位带来了重大挑战。为了捕获微妙的伪影和法医痕迹，一些方法，如IML-ViT[21]和MVSS-Net[5]，使用边缘监督损失来迫使模型集中于区域边界。然而，如图1(b)所示，这种硬约束的方法很容易导致注意力偏差，导致模型对图像边缘过于敏感，或无法精细地捕捉到所有的法医痕迹。这就导致了诸如错误警报、检测遗漏以及无法准确定位具有尖锐边界的伪造区域等问题。因此，为了防止模型过度依赖边界区域，忽视其他关键特征，必须实现更加灵活和适当的平衡。<br/>​  在本文中，我们提出了一种新的动态和重要性感知的变压器网络AdaIFL。为了解决在处理伪造样本时适应性不足的挑战，我们提出了一个新的动态框架，将动态路由机制的概念合并到IFL中。我们的框架包括一个基于transformer的动态编码器和一个轻量级的动态解码器。我们在不同的网络组件中定制不同的专家组，构建多个不同的特征子空间。通过门控网络的路由，伪造的图像样本可以选择性地激活网络的不同部分，从而在各自的特征子空间中挖掘出与伪造模式相关的鉴别特征。这大大提高了模型的泛化能力。<br/>​  为了避免模型对伪造区域周围的边界伪影的过度敏感或忽视，我们提出了特征重要性感知注意（FIA，featureimportance-awareattention），这是一种灵活的方法，可以自适应地感知不同区域的重要性。其中一个关键组件是自适应token聚合器（ATA，adaptivetokenaggregator），它由三个部分组成：重要性感知区域分区、聚合规模分配和自适应token聚合。ATA的目标是基于区域特征的鉴别信息，将区域特征动态聚合为可变长度tokens，为不同尺度和形状的伪造区域建模。具体来说，我们设计了一个评分网络来量化每个图像特征的重要性。根据重要性评分，将整个图像区域划分为多个子区域。然后利用一种简单而有效的自适应机制对判别信息进行评估，从而确定每个子区域的聚集规模。特别是，较小的聚合尺度被分配为具有更多区别区域的区域生成更多的特征标记，如图像处理边界。最后，利用聚类算法在聚合尺度的引导下对token进行合并，得到了紧凑而又具有高度判别性的token表示。FIA将模型的注意力导向更有鉴别力的区域，显著提高了模型准确定位各种伪造区域的能力。<br/>​  我们的主要贡献总结如下：</p><ul><li>我们提出了AdaIFL，一个新的动态和重要性感知的IFL框架。据我们所知，这是第一次将动态路由机制引入IFL，使其在该领域的开创性贡献。</li><li>我们提出了一种特征重要性感知注意，即自适应地感知不同区域的重要性，提高了模型准确定位不同伪造区域的能力。</li><li>我们在几个基准上进行了广泛的实验，并证明了AdaIFL在定性和定量上优于现有的最先进的方法。</li></ul><h1 id="相关工作">2相关工作</h1><h1 id="方法">3方法</h1><h2 id="框架概述">3.1框架概述</h2><p>​  在本文中，我们提出了AdaIFL，一个动态的和重要性感知的图像伪造定位框架。如图2所示，AdaIFL将动态路由机制引入到IFL中。</p><figure><img src="../postimages/AdaIFL/image-20241119220644639.png"alt="image-20241119220644639" /><figcaption aria-hidden="true">image-20241119220644639</figcaption></figure><p>图2： (a)AdaIFL框架。AdaIFL以可疑的图像作为输入，然后利用一个基于transformer的动态编码器来提取多级特征。这些特征被传递到一个轻量级的动态解码器中，进行多尺度的特征融合，生成一个空间定位图来预测伪造的区域。(b)动态transformer块。AdaIFL框架将动态路由机制的概念整合到transformer块（FIATB和GTB）中，在注意力层和MLP层中引入多个特定的专家网络，以挖掘出与伪造模式相关的区别特征。FIATB和GTB分别表示特征重要性感知的transformer块和全局transformer块。</p><p>​  它为不同的网络组件定制了不同的专家网络，构建了多个特征子空间来专门学习不同的伪造模式。具体来说，该框架由一个基于transformer的动态编码器和一个轻量级的动态解码器组成。该编码器包括四个阶段，每个阶段包括两个堆叠的特征重要性感知transformer块（FIATB）和一个全局transformer块（GTB）。此外，我们提出了一个动态解码器来融合多阶段特征，并预测伪造区域的空间定位图。其详细结构如图4(a)所示，包括多尺度特征融合（MSFF）和动态解码两个过程。在下面的章节中，我们将详细介绍AdaIFL的几个关键组件。</p><h2 id="动态transformer组件">3.2动态transformer组件</h2><p>​  <strong>准备工作：专家网络的混合。</strong><br/>​  专家网络的混合层（MoE，Mixtureof Experts）包括一组专家网络，记为<spanclass="math inline">\(E_{1},E_{2},\cdot\cdot,E_{N}\)</span>，以及记为<spanclass="math inline">\({\mathcal{G}}\)</span>的路由网络。在网络的不同组件中，每个专家网络可以实现为注意层、MLP层或卷积层。路由网络<spanclass="math inline">\({\mathcal{G}}\)</span>负责确定利用每个专家网络<spanclass="math inline">\(E_i\)</span>的概率，并选择前k个专家作为最终输出的贡献者。<spanclass="math inline">\({\mathcal{G}}\)</span>可以根据各种形式选择专家，如单个token、输入样本或任务嵌入：<span class="math display">\[\displaystyle{\calG}=\left\{\begin{array}{l l}G_{t o k en}\left(x_{i}\right),&amp;\mathrm{Token}\mathrm{-}\mathrm{level}\\ G_{sa m p l e}\left(X\right),&amp;\mathrm{Sample}\mathrm{-}\mathrm{level}\\G_{t a s k}\left(e m b e d(i d_{t a sk})\right),&amp;\mathrm{Task}\mathrm{-}\mathrm{level}\end{array}\right.\]</span>​  其中，<spanclass="math inline">\(G\)</span>定义了门决策的特定路由策略，<spanclass="math inline">\(\textstyleX=\{x_{i}\}_{i=1}^{L}\)</span>是当前样本中所有token的序列。专家网络混合层的最终输出是来自选定的专家网络<spanclass="math inline">\(E\subset\{E_{1},E_{2}\cdot\cdot\cdotE_{N}\}\)</span>： <span class="math display">\[y=\sum_{i\in E}{\calG}\left(x\right)\cdot E_{i}\left(x\right)\]</span>​  <strong>专家网络混合层的动态transformer组件。</strong><br/>​  我们进一步将专家网络混合层的动态路由概念纳入到transformer架构中。具体来说，我们向多个特定的专家网络引入注意力机制和MLP层。我们鼓励这些专家网路探索与伪造模式相关的独特的工件和法医痕迹，以增强网络的泛化能力。</p><figure><img src="../postimages/AdaIFL/image-20241119224743855.png"alt="image-20241119224743855" /><figcaption aria-hidden="true">image-20241119224743855</figcaption></figure><p>​  如图2 (b)所示，我们用一组并行注意的专家网路和一组sample-level的路由器替换原来的注意层，用一组并行MLP专家和一组token级路由器替换原来的MLP层。在形式上，AdaIFL编码器的transformer块可以表述为：<spanclass="math display">\[\begin{array}{c}{X_{l}^{\prime}=\mathrm{MoE}\mathrm{-Att}\left(\mathrm{LN}\left(X_{l-1}\right)\right)+X_{l-1}}\\X_{l}=\mathrm{MoE}\mathrm{-FFN}\left(\mathrm{LN}\left(X_{l}^{\prime}\right)\right)+X_{l}^{\prime}\end{array}\]</span>​  其中，<spanclass="math inline">\(\mathrm{MoE}-\mathrm{Att}(X)=\sum_{i\inE_{\mathrm{Att}}}G_{s a m p l e}^{i}\left(X\right)\cdotE_{\mathrm{Att}}^{i}\left(X\right)\)</span>，<spanclass="math inline">\(\mathrm{MoE}-{\mathsf {FFN}}(x)=\sum_{i\inE_{MLP}}G_{t o k e n}^{i}\left(x\right)\cdotE_{\mathrm{MLP}}^{i}\left(x\right)\)</span>，LN表示图层的归一化。值得注意的是，我们在FIATB中对MoE采用了特征重要性感知的注意力，详见Sec3.3。在GTB中，使用了标准的全局自注意机制。</p><h2 id="特征重要性感知的注意力">3.3特征重要性感知的注意力</h2><p>​  为了避免模型对操纵边界区域的过度敏感或忽视，我们提出了一种特征重要性感知的注意力，即自适应地感知不同区域的重要性，并将区域特征聚合成可变token，对不同尺度、形状和内容的伪造区域进行建模。整体结构如图3所示。</p><figure><img src="../postimages/AdaIFL/image-20241119225539278.png"alt="image-20241119225539278" /><figcaption aria-hidden="true">image-20241119225539278</figcaption></figure><p>图3： (a)特征重要性感知的注意力（FIA，feature importance-awareattention）例证。(b)自适应token聚合器（ATA，Adaptive tokenaggregator）。从左到右，ATA涉及三个过程：重要性感知区域分区、聚合规模分配和自适应token聚合。</p><h3id="重要性感知的区域分区"><strong>重要性感知的区域分区。</strong></h3><p>​  我们设计了一个评分网络来评估区域特征对IFL任务的重要性，记为<spanclass="math inline">\(f_s\)</span>，它是一个由两个MLP层组成的轻量级模块。具体来说，给定一个输入特征<spanclass="math inline">\(X\in\mathbb{R}^{N\timesd}\)</span>，其中d表示每个特征标记的维数，N表示特征标记的数量。<spanclass="math inline">\(f_s\)</span>用于量化每个特征标记<spanclass="math inline">\(x_i\)</span>的重要性。 <spanclass="math display">\[s_{i}=f_{s}(x_{i}),\,i=1,\cdot\cdot\cdot,\,N\]</span>​  此外，根据重要性分数对特征token进行排序，得到一组特征token及其各自的分数，表示为<spanclass="math inline">\(\left\{x_{i}^{\prime}\right\}_{i=1}^{N}\)</span>和<spanclass="math inline">\(\left\{s_{i}^{\prime}\right\}_{i=1}^{N}\)</span>。此外，根据token的排序列表，将整个图像区域划分为m个不规则子区域<spanclass="math inline">\(\left\{R_{i}\right\}_{i=1}^{m}\)</span>，每个子区域包含<spanclass="math inline">\(N_{R_i}\)</span>标记。</p><h3 id="聚合规模分配"><strong>聚合规模分配</strong></h3><p>​  基于每个区域对伪造定位的重要性，动态调整聚合尺度，对不同尺度和形状的伪造区域进行建模。为了实现这一点，我们使用了一个简单的MLP层来将区域重要性的分布转换为信息密度因子。这些因素被用来评估每个图像区域的鉴别信息，表示为<spanclass="math inline">\(\rho=\left\{\rho_{1},\ \rho_{2},\\ldots,\rho_{m}\right\}\)</span>。然后应用Softmax函数对密度因子进行归一化，并利用它们生成不同区域的聚集尺度。这可以表述为：<spanclass="math display">\[\hat{\rho}_{i}=\frac{e^{\rho_{i}}}{\sum_{i=1}^{m}e^{\rho_{i}}},i=1,\cdot\cdot,m\]</span>， <spanclass="math display">\[c_{i}=N_{\lambda}\hat{\rho}_{i},\alpha_{i}=\frac{N_{R_{i}}}{c_{i}}\]</span></p><p>​  式中，<spanclass="math inline">\(N_{\lambda}\)</span>为预定义的token总数<spanclass="math inline">\((N_{\lambda}\ll N)\)</span>，<spanclass="math inline">\(c_i\)</span>表示分配给子区域<spanclass="math inline">\(R_i\)</span>的token数，<spanclass="math inline">\(\alpha_{i}\)</span>表示<spanclass="math inline">\(R_i\)</span>的聚集规模。这种自适应方法将更小的聚集规模分配到更有区别的区域，产生更多的特征token。相反，其他区域使用更大的规模来进行更粗的token聚合。</p><h3 id="自适应token聚合ata"><strong>自适应token聚合ATA</strong></h3><p>​  在分配了相应的聚合规模之后，我们需要进一步考虑如何聚合token。直观地说，聚合具有相似语义信息的token可以避免对冗余特征的过度关注和对微妙特征的关注不足。受[7,41]的启发，我们使用聚类算法进行token聚类和合并，从而得到更准确和紧凑的token表示。具体来说，我们使用DPC-KNN算法[7]来进行token聚类，这是一种基于密度峰值的k-最近邻聚类算法。在此算法的基础上，根据每个区域的特征token的相似性划分为<spanclass="math inline">\(c_i\)</span>不同的聚类。在聚合过程中，将重要性分数作为权重分配给同一集群内的token，强调不同token的重要性。因此，可以获得每个集群的token表示：<span class="math display">\[\hat{x}_{i}=\frac{\sum_{j\inc_{i}}e^{s_{j}}x_{j}}{\sum_{j\in c_{i}}e^{s_{j}}}\]</span>​  最后，将来自所有区域的聚合token连接起来，得到最终的token表示<spanclass="math inline">\(\hat{X}\in\mathbb{R}^{N_{\lambda}\timesd}\)</span>。</p><h3id="特征重要性感知的注意力fia"><strong>特征重要性感知的注意力FIA</strong></h3><p>​  我们提出了基于自适应token聚合器的特征重要性感知注意力（FIA），以避免过度关注冗余特征或忽略局部细节。具体来说，我们将原始特征token<spanclass="math inline">\(X\in\mathbb{R}^{N\timesd}\)</span>投影为查询Q，将聚合token<spanclass="math inline">\(\hat{X}\in\mathbb{R}^{N_{\lambda}\timesd}\)</span>投影为键K和值V。该流程的定义为： <spanclass="math display">\[\begin{array}{l}{Q=XW^{q},K=\hat{X}W^{k},V=\hat{X}W^{v}}\\\mathrm{FIA}(Q,K,V)=\mathrm{Softmax}\left({\cfrac{QK^{\top}}{\sqrt{d}}}\right)V\end{array}\]</span> ​  其中，<spanclass="math inline">\(W^{q},W^{k},W^{v}\in\mathbb{R}^{d\timesd}\)</span>为可学习矩阵。<br/>​  如图6所示，FIA将模型的注意力转向更区分的区域，提高了伪造定位的性能。</p><figure><img src="../postimages/AdaIFL/image-20241119231517507.png"alt="image-20241119231517507" /><figcaption aria-hidden="true">image-20241119231517507</figcaption></figure><p>图6：自适应token聚合器（ATA）和特征重要性感知注意力（FIA）的可视化。从左到右，我们显示伪造的图像、ground-truth掩模、特征图的GradCAM和没有（w/o）ATA，没有（w/o）FIA和完整设置的预测结果。</p><p>​  这是由于有几个优点。<br/>​  (1)区域划分机制限制了真实区域和伪造区域之间的相互作用，缓解了特征耦合问题。许多伪造技术旨在创建语义上一致的和感知上令人信服的篡改图像的视觉欺骗。因此，直接聚类[7,41]无意中导致了特征耦合。如图6所示，ATA的区域划分机制缓解了这个问题，减少了误报。<br/>​  (2)自适应聚合机制可以根据不同区域的重要性动态调整聚合规模，使模型能够灵活地适应伪造区域的各种尺度和形状。</p><h2 id="动态解码器">3.4动态解码器</h2><figure><img src="../postimages/AdaIFL/image-20241120094237190.png"alt="image-20241120094237190" /><figcaption aria-hidden="true">image-20241120094237190</figcaption></figure><figure><img src="../postimages/AdaIFL/image-20241120094333966.png"alt="image-20241120094333966" /><figcaption aria-hidden="true">image-20241120094333966</figcaption></figure><p>图4：(a)动态解码器示意图。它由多阶段特征融合（MSFF）和动态解码器组成。(b)从左到右是阶段1、阶段2、阶段3、阶段4和MSFF的输出特性映射的GradCAM。MSFF模块集成了多阶段的特性，提高了伪造定位的性能。</p><p>​  如图4(a)所示，动态解码器由多阶段特征融合（MSFF）和动态解码两个过程组成，旨在融合多阶段特征，实现更好的定位性能。如图4(b)所示，每个阶段都侧重于捕获不同级别的特征。因此，MSFF被提出充分利用特征在不同阶段的表达能力，并捕获更多的鉴别信息。<br/>​  具体来说，MSFF利用从transformer的四个阶段中提取的特征图<spanclass="math inline">\(F_1\)</span>、<spanclass="math inline">\(F_2\)</span>、<spanclass="math inline">\(F_3\)</span>和<spanclass="math inline">\(F_4\)</span>作为输入，并沿着通道维度将每个特征图分成四个部分。然后，我们从每个特征图中选择一部分特征，并将它们连接起来，得到四个融合的特征。然后，利用不同核大小的深度可分离卷积来处理这四个融合特征，捕获多尺度特征。此外，利用群卷积[18]提取有价值的信息，并从融合的特征中过滤出冗余的特征表示。此外，利用群卷积[18]提取有价值的信息，并从融合的特征中过滤出冗余的特征表示。这一点可以表述如下：<spanclass="math display">\[\begin{array}{l}Z_{i}=\mathrm{Concat}\left(F_{1}\left[i\right],F_{2}\left[i\right],F_{3}\left[i\right],F_{4}\left[i\right]\right),Z_{i}^{&#39;}=\mathrm{DW}_{k_{i}\times k_{i}}{\left(Z_{i}\right)}\\\hat{Z}=\mathrm{GC}\left(\mathrm{Concat}\left(Z_{1}^{&#39;},\;Z_{2}^{&#39;},\;Z_{3}^{&#39;},\;Z_{4}^{&#39;}\right)\right)\end{array}\]</span>​  其中，<spanclass="math inline">\(k_{i}\in\{3,5,7,9\}\)</span>，DW表示深度上可分离的卷积，GC表示群卷积。<br/>​  最后，我们引入了一个动态解码模块来预测输入图像的伪造区域。它涉及一组并行预测头<spanclass="math inline">\(\{P_{i}\}_{i=1}^{n}\)</span>和一个样本路由器。每个预测头<spanclass="math inline">\(P_i\)</span>被实现为一个1x1的卷积，即<spanclass="math inline">\(P_{i}(\hat{Z})=W_{i}\hat{Z}\)</span>。在样本路由器中，我们首先执行全局平均池化来生成全局嵌入<spanclass="math inline">\(\tau\)</span>。然后，通过一个全连接层和sigmoid函数，即<spanclass="math inline">\(AP\left(\hat{Z}\right)=\sigma\left(W_{a}\tau\right)\)</span>，计算每个头部的激活概率。动态解码器的最终输出是概率最高的前k个预测头的加权和：<span class="math display">\[Y=\sum_{A P_{i}\in\mathrm{top.k}}AP_{i}({\hat{Z}})\cdot P_{i}({\hat{Z}})\]</span></p><h2 id="优化">3.5优化</h2><p>​  为了提高模型在像素级检测伪造区域的准确性，我们利用了二进制交叉熵损失和dice损失[22]。<spanclass="math display">\[{\mathcal{L}}_{\mathrm{BCE}}\left(p,y\right)=\sum\left(-y_{i}\logp_{i}-\left(1-y_{i}\right)\log\left(1-p_{i}\right)\right)\]</span> ，<spanclass="math display">\[\mathcal{L}_{\mathrm{Dice}}\left(p,y\right)=1-\frac{2\sump_{i}\cdot y_{i}}{\sum p_{i}{}^{2}+\sum y_{i}{}^{2}}\]</span>​  其中，<span class="math inline">\(p_i\)</span>和<spanclass="math inline">\(y_i\)</span>分别为伪造图像中每个像素的预测标签和ground-truth。此外，我们采用度量学习损失[13]来增加[23]后伪造图像样本中真实区域和伪造区域之间的特征分布的差异。<spanclass="math display">\[{\mathcal{L}}_{q}={\frac{1}{|A_{i}|}}\sum_{k^{+}\inA_{i}}-\log{\frac{\exp\left(q\cdotk^{+}/\tau\right)}{\sum_{k_{-}}\exp\left(q\cdotk^{-}/\tau\right)}}\]</span> ​  其中，<spanclass="math inline">\(k^+\)</span>和<spanclass="math inline">\(q\)</span>表示真实区域的特征嵌入，<spanclass="math inline">\(k^−\)</span>表示伪造区域的特征嵌入。<spanclass="math inline">\(A_i\)</span>表示所有<spanclass="math inline">\(k^+\)</span>的集合。结合以上所有情况，我们的最终损失函数可以表述为：<spanclass="math display">\[\mathcal{L}=\lambda_{1}\cdot\mathcal{L}_{q}+\lambda_{2}\cdot\mathcal{L}_{\mathrm{BCE}}\+\lambda_{3}\cdot\mathcal{L}_{\mathrm{Dice}}\]</span> ​  其中，<spanclass="math inline">\(\lambda_{1}\)</span>、<spanclass="math inline">\(\lambda_{2}\)</span>、<spanclass="math inline">\(\lambda_{3}\)</span>是平衡损失函数中这三项的参数<spanclass="math inline">\((\lambda_{1}+\lambda_{2}+\lambda_{3}=1)\)</span>。在实验中，它们分别被设为0.5、0.15和0.35。</p><h1 id="实验">4实验</h1><h2 id="实验设置">4.1实验设置</h2><h3 id="数据集"><strong>数据集。</strong></h3><p>​  我们使用与[11,19]相同的数据集来训练AdaIFL。这些训练数据集包括CASIAv2 [6]、IMD2020[24]，以及由[19]创建的经过伪造的图像数据集，涵盖了各种类型的伪造。为了全面评估AdaIFL的泛化能力，我们对5个与训练集不重叠的数据集进行了基准测试，即CASIAv1 [6]、Coverage[35]、DSO-1 [4]、NIST16 [10]和MISD[16]。这些数据集覆盖了大量的伪造图像，具有不同的伪造类型和广泛分布的数据源。</p><h3 id="指标"><strong>指标。</strong></h3><p>​  在之前的大多数工作之后，我们使用像素级的曲线下面积（AUC）、F1和Union上的交集（IoU）分数作为评估指标，其中阈值默认设置为0.5。</p><p>​  <strong>实施细节。</strong><br/>​  AdaIFL使用PyTorch实现，并以端到端方式进行培训。在训练过程中，输入的图像被裁剪到1024×1024。为了防止训练数据集大小不平衡造成的偏差，我们采用[11,19]的方法，在每个训练时期对每个数据集进行相等的采样。此外，还采用了常用的数据增强技术，如翻转、缩放、模糊和JPEG压缩来增强数据的多样性。我们使用了一个Adam[17]优化器，其学习速率从2×10−4衰减到1×10−7。</p><h2 id="与最新方法的比较">4.2与最新方法的比较</h2><p>​  我们仔细选择带有开放源代码和预训练模型的方法进行测试，以确保公平的比较。此外，确保这些模型的训练数据集不与测试数据集重叠是至关重要的。最后，我们选择了8种最先进的方法，以公平的方式进行全面的比较，即TruFor[11]，HiFi-IFDL[12]，CAT-Netv2[19]，ManTraNet[37]，MVSS-Net[2]，PSCC-Net[20]，IF-OSN[36]和IML-ViT[21]。</p><h3 id="定量比较">定量比较</h3><p>​  表1为不同模型对IOU、f1、AUC评分的定量比较结果。</p><hr /><p>表1：不同图像伪造定位方法的性能比较。报告了IOU、F1和AUC评分。第一个和第二个排名分别以粗体和下划线显示。</p><figure><img src="../postimages/AdaIFL/image-20241120112901191.png"alt="image-20241120112901191" /><figcaption aria-hidden="true">image-20241120112901191</figcaption></figure><hr /><p>​  在所有比较模型中，AdaIFL达到了最先进的性能，平均IOU、F1和AUC得分分别比第二优模型高出6.6%、3.4%和1.9%。具体来说，对于F1和IOU评分，AdaIFL在CASIA、Coverage、NIST16和MISD上的定位性能最好，在DSO-1中排名第二。特别是在Coverage（复制-移动伪造数据集）上，AdaIFL的IOU和F1得分比第二优模型高出8.0%和4.6%，证明了我们的模型在抑制伪造区域和真实区域之间的特征耦合方面的突出能力。在更具挑战性的NIST16数据集上，我们的模型在IOU和F1分数上比第二优的模型高出8.7%和5.3%，在处理各种操作技术和伪造模式方面显示出非凡的泛化能力。值得注意的是，使用AUC作为度量可能会导致高估模型的性能，因为数据集的伪造像素和真实像素之间的比例高度不平衡。然而，我们的模型在所有数据集上都获得了最好的AUC分数。</p><h3 id="定性比较"><strong>定性比较</strong></h3><p>​  图5显示了不同测试图像的伪造定位结果。</p><figure><img src="../postimages/AdaIFL/image-20241120145615997.png"alt="image-20241120145615997" /><figcaption aria-hidden="true">image-20241120145615997</figcaption></figure><p>​  我们的方法在几个方面都优于目前的SOTA方法。首先，我们的方法有效地减少了误报和漏检。与其他方法将非伪造区域错误识别为伪造或遗漏许多伪造区域不同，我们的方法可以准确地定位具有尖锐边界的伪造区域。其次，该方法可以准确地定位各种复杂形状的锻造区域。这在测试图像的第五行和第六行可以明显看出，其中其他方法只能预测粗略的结果，而不能捕获详细的边界。相比之下，我们的方法可以准确地定位具有复杂形状的伪造区域。第三，我们的方法在精确定位微小和微妙的伪造区域方面表现出了特殊的能力。如第三行和第六行的测试图像所示，许多方法很难识别这些小区域，而我们的方法可以准确地定位它们。</p><h3 id="鲁棒性分析">鲁棒性分析</h3><p>​  在现实世界的场景中，伪造的图像可能会经历各种后处理操作。为了评估AdaIFL对伪造定位的鲁棒性，我们使用了常用的图像退化技术，即高斯模糊、高斯噪声、伽马校正和JPEG压缩。如表2所示，在NIST16上的测试结果表明，与其他最先进的方法相比，AdaIFL对各种降解技术表现出优越的鲁棒性。</p><hr /><p>表2：在不同失真条件下对NIST16数据集的定位性能。报告了IOU和F1的分数。</p><figure><img src="../postimages/AdaIFL/image-20241120150358022.png"alt="image-20241120150358022" /><figcaption aria-hidden="true">image-20241120150358022</figcaption></figure><hr /><p>​  此外，表2表明，过量的噪声（k =23）导致了我们的模型中的次优结果。这可能是由于路由过程受到噪声的影响，导致了专家的次优选择。我们认为，设计适当的补救机制，以确保在各种退化场景中路由的准确性，可能是有益的。</p><h2 id="消融分析">4.3消融分析</h2><p>​  在本节中，我们将从特征重要性感知和动态网络结构这两个角度来分析AdaIFL中关键组件的影响。具体来说，AdaIFL通过分别在编码器（DyE）和解码器（DyD）中定制各种专家组，将动态路由的想法整合到IFL中。特征重要性感知注意注意力（FIA）自适应地感知不同区域的重要性，其中一个关键组件是自适应标记聚合器（ATA），它将区域特征聚合成可变长度的标记，将模型的注意力导向更具鉴别性和信息丰富的区域。为了评估FIA、ATA、DyE和DyD的有效性，我们分别从AdaIFL中移除每个组件，并将测试结果与CASIA和MISD数据集上的完整设置进行比较。</p><h3 id="特征重要性感知的影响">特征重要性感知的影响</h3><hr /><figure><img src="../postimages/AdaIFL/image-20241120150855145.png"alt="image-20241120150855145" /><figcaption aria-hidden="true">image-20241120150855145</figcaption></figure><hr /><p>​  如表3中所示，用原注意力代替FIA后，IOU得分下降5.2%，CASIAF1得分下降3.1%，MISD IOU下降4.4%，MISDF1下降2.2%。去除ATA后，CASIA的IOU得分分别下降了4.4%和MISD的下降了3.9%。</p><figure><img src="../postimages/AdaIFL/image-20241120150959994.png"alt="image-20241120150959994" /><figcaption aria-hidden="true">image-20241120150959994</figcaption></figure><p>​  此外，图6显示，去除FIA或ATA会导致检测失误、误报和无法准确定位伪造区域等问题。AdaIFL受益于区域划分和自适应聚合机制，缓解了特征耦合的问题，提高了图像伪造定位的性能。</p><figure><img src="../postimages/AdaIFL/image-20241120151045785.png"alt="image-20241120151045785" /><figcaption aria-hidden="true">image-20241120151045785</figcaption></figure><p>​  此外，图4 (b)显示了变压器编码器不同阶段的特征图的GradCAM。在FIA的指导下，AdaIFL专注于在不同的阶段捕捉不同的特征。在初始阶段，该模型对伪造区域边界的局部细节进行了优先排序，强调了对边界伪造痕迹的精细感知。在随后的阶段中，它专注于在伪造区域内捕获不同级别的特征。MSFF模块通过融合这些特性，增强了模型的表达性，从而能够更好地捕获伪造的工件和法医痕迹。</p><h3 id="动态网络结构的影响">动态网络结构的影响</h3><p>​  在AdaIFL中，我们将动态路由的概念应用于基于transformer的编码器和轻量级路由器中。</p><hr /><figure><img src="../postimages/AdaIFL/image-20241120150855145.png"alt="image-20241120150855145" /><figcaption aria-hidden="true">image-20241120150855145</figcaption></figure><hr /><p>​  如表3所示，去除动态专家组会显著降低定位性能。具体来说，从编码器中删除所有动态专家组，CASIA的IOO和F1得分下降了7.4%和4.5%，MISD的IOU和F1得分下降了3.7%和1.6%。从解码器中去除动态成分后，CASIA和MISD的IOU得分分别下降了4.9%和1.5%。这清楚地说明了动态路由概念在增强模型泛化方面的关键作用。</p><h1 id="结论">5结论</h1><p>​  本文解决了图像伪造定位的两个关键挑战：<br/>​  (1)处理不同伪造图像的适应性不足，<br/>​  (2)捕获边界伪影的方法不灵活。<br/>​  为了解决这些问题，我们提出了一种新的动态和重要性感知的图像伪造定位框架（AdaIFL），该框架可以动态地处理不同的伪造图像样本，并自适应地感知不同区域的重要性，提高了模型的泛化能力。大量的实验表明，我们所提出的方法优于最先进的方法。</p><h1 id="acknowledgements"><strong>Acknowledgements</strong></h1><p>This work is supported by Shenzhen Science and Technology Program(No.JCYJ20230807120800001), and 2023 Shenzhen sustainable supportingfunds for colleges and universities (No.20231121165240001). The authorssincerely appreciate the computing environment supported by the ChinaUnicom Shenzhen Intelligent Computing Center.</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 重要性感知 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Manipulation Detection With Implicit Neural Representation and Limited Supervision</title>
      <link href="/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/"/>
      <url>/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/</url>
      
        <content type="html"><![CDATA[<center><p>Image Manipulation Detection With Implicit Neural Representation andLimited Supervision</p>Zhenfei Zhang1 , Mingyang Li2 , Xin Li1 , Ming-Ching Chang1 , andJun-Wei Hsieh3 1 University at Albany, State University of New York 2Stanford University 3 National Yang Ming Chiao Tung University{zzhang45, xli48, mchang2}<span class="citation"data-cites="albany.edu">@albany.edu</span> mingyang.li@stanford.edujwhsieh@nycu.edu.tw</center><h1 id="摘要">摘要</h1><p>​  随着篡改技术的发展，图像处理检测（IMD，Image ManipulationDetection）变得越来越重要。然而，大多数最先进的（SoTA，stateof-the-art）方法都需要高质量的训练数据集，其中具有图像级和像素级注释。当应用于不同于训练数据的操纵或噪声样本时，这些方法的有效性受到影响。为了解决这些挑战，我们提出了一个统一的框架，结合了无监督和弱监督的方法的IMD。我们的方法引入了一种新的预处理阶段，基于来自隐式神经表示的可控拟合函数（INR，ImplicitNeuralRepresentation）。此外，我们引入了一种新的选择性像素级对比学习方法，它只专注于高置信度区域，从而减少了与像素级标签缺失相关的不确定性。在弱监督模式下，我们利用ground-truth图像级标签来指导自适应池化方法的预测，促进了对图像级检测的操作区域的全面探索。无监督模型采用自蒸馏训练方法进行训练，通过不同的来源从最深层获得选择高置信度的伪标签。大量的实验表明，我们提出的方法优于现有的无监督和弱监督的方法。此外，它在新的操作检测任务上有效地与完全监督的方法竞争。</p><h1 id="引言">1引言</h1><p>​  多种媒体篡改工具的出现，如[10,49,61,64,70,73]psp和人工智能编辑和生成方法，使得操纵媒体内容变得越来越方便。然而，这种可访问性也带来了广泛存在的错误信息的相关问题，这可能导致严重的安全影响。因此，开发和实现鲁棒篡改检测技术，即图像操作检测（IMD）方法，是有效降低这些风险的关键。以前的方法通常处理的基本操作操作如下：(1)拼接，包括从一个图像中获取内容并粘贴到另一个图像上，(2)复制移动，其中一个图像的部分被复制和重新定位到同一图像中的另一个位置，(3)消除，这需要擦除图像的部分，并用合成内容替换它们。<br/>​  尽管在完全监督的IMD方法方面取得了重大进展，但他们还是遇到了几个显著的挑战。</p><ul><li><p>首先，这些方法在面对看不见的操作类型时通常表现不佳。</p></li><li><p>其次，由于它们依赖于具有图像级和像素级注释的高质量训练数据集，因此将它们扩展到看不见的操作类型面临着挑战。获取这样的数据集是昂贵的，而且在许多情况下，是不切实际的，特别是考虑到现实生活中无数种类的篡改方法。</p></li><li><p>第三，虽然一些语言引导的数据集可能缺乏像素级的标签，但它们在处理现实世界的场景时具有优势。这些数据集可以潜在地增强IMD模型的泛化能力。</p></li></ul><p>​  为了解决完全监督的IMD方法的局限性，并提高对现实世界使用的泛化能力，我们建议将无监督和弱监督的方法集成到一个统一的IMD框架中。我们的框架允许训练使用单独的图像级标签，甚至没有任何标签，与许多无监督和弱监督的任务[14,30,48,52,56,72,74]对齐。与完全监督的方法相比，我们的方法具有优越的泛化能力，并且可以使用没有注释的数据集进行训练。我们的方法首先观察到，在大多数情况下，被篡改的区域表现出与真实区域的差异，比如颜色和照明的变化，这对需要精确建模区域的拟合函数提出了挑战。[63]的结果表明，隐式神经表示（INR）的可控拟合函数倾向于学习训练图像的平均表示。基于这一见解，我们提出了以下问题作为我们的假设：如果我们只在真实的图像上训练一个INR，拟合函数能有效地表示被篡改区域的特征吗？<br/>​  为了得到这个问题的答案，我们首先只使用来自CASIAv2[12]的原始图像来训练一个INR，并使用它来重建三个主流数据集。然后，我们应用完全监督的SoTA方法对重建的数据集进行评估，如图1所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119103521520.png"alt="image-20241119103521520" /><figcaption aria-hidden="true">image-20241119103521520</figcaption></figure><p>图1：我们使用三个广泛使用的评估数据集，包含真实和篡改样本进行实验。并与六种SoTA全监督IMD方法进行了性能比较。像素级F1得分计算使用篡改图像，而图像级精度计算使用真实图像计算。蓝色和橙色条分别表示通过隐式神经表示的原始数据集和重构数据集。结果表明，与原始数据集相比，所有针对重建图像的像素级检测方法的性能都有显著的下降。另一方面，使用真实图像的性能变化较小。这些分数取了CASIAv1[11]、Coverage[57]和Columbia[24]数据集上的平均值。</p><p>​  令人惊讶的是，当使用INR重建样本时，这些方法的评价结果显著下降，而在真实图像样本中的性能变化较小。这一结果导致了我们初步假设，即INR可能不能有效地捕获篡改区域的特征。为了验证这一假设，我们计算了图2中重建图像和原始图像之间的重建误差图。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110126166.png"alt="image-20241119110126166" /><figcaption aria-hidden="true">image-20241119110126166</figcaption></figure><p>图2：给出了在原始图像和重建图像之间计算的重建误差图的示例。前两行分别描述了数据样本及其对应的ground-truth掩码。前三列显示了被篡改的图像示例，而最后三列显示了真实的图像，其中ground-truth的掩码都是黑色的。显然，重建过程不能正确地重建被篡改的像素，导致错误映射中的激活。相反，在真实的样本中观察到的变化较小。</p><p>​  值得注意的是，我们在被篡改样本的被篡改区域观察到激活，而在真实样本中没有明显的差异。这一观察结果启发我们将INR作为一种预处理方法，并将重建错误图与输入的RGB图像连接起来，然后将其输入给主干。我们将这种预处理方法命名为神经表示重建（NRR，NeuralRepresentationReconstruction）。<br/>​  在使用INR进行预处理的成功之后，我们进一步探索了我们的发现，并在我们的框架中充分利用了它。从对比学习[22]中获得灵感，我们利用NRR作为对比样本生成器，并引入选择性像素级对比学习，只关注高度自信的区域。该方法有效地减轻了与缺乏像素级标签相关的不确定性，并进一步提高了弱监督性能。我们进一步将我们的方法扩展到一个完全无监督的方法，使用选定的高置信伪标签，使用自蒸馏[69]训练策略。最后，以往的SoTA方法广泛应用于全局最大池（GMP，Global-MaxPooling）或全局平均池（GAP，Global-AveragePooling）用于图像级检测。然而，GMP可能会阻碍训练，并导致不准确的预测，因为只有最具区别性的反应是反向传播的，而忽略了整个被篡改的内容。相反，由于GAP的弱激活像素，容易产生不准确性。为了克服这一限制，我们提出了一个自适应的全局平均池，它关注于高置信的篡改区域。因此，我们的方法可以产生更全面和鲁棒的图像级预测。<br/>​  在7个数据集上进行了实验评估，其中包括5个具有一般操作类型的主流数据集和2个包含不可见的篡改样本的新数据集。结果表明，我们的方法优于SoTA弱监督和无监督方法。此外，在新的操作检测任务中，我们的方法与完全监督的方法相比，取得了具有竞争力的结果。最后，我们的方法可以很容易地扩展到没有像素级标签的数据集，这显示出了增强的通用性。<br/>​  本文的贡献包括：</p><p>​  (1)我们提出了一种新的方法来实现可信的弱和无监督的IMD结果。我们的方法可以很容易地适应于没有标签或只有图像级标签的图像。</p><p>​  (2)据我们所知，我们是第一个研究内隐神经表征（INR）在IMD任务中的潜力的人。利用INR的预处理步骤证明了处理被篡改案例的有效性。</p><p>​  (3)我们引入了选择性监督，它减少了与没有标签相关的不确定性，并进一步提高了检测性能。</p><p>​  (4)大量的实验验证了我们所提出的方法的有效性，与SoTA方法相比，它在标准和新型操作类型上都具有优越的性能。</p><h1 id="相关工作">2相关工作</h1><h1 id="提出的方法">3提出的方法</h1><h2 id="整体架构">3.1整体架构</h2><p>​  图3显示了我们的IMD框架的整体体系结构。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110904314.png"alt="image-20241119110904314" /><figcaption aria-hidden="true">image-20241119110904314</figcaption></figure><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110919864.png"alt="image-20241119110919864" /><figcaption aria-hidden="true">image-20241119110919864</figcaption></figure><p>​  基本架构由两个具有共享权重的分支组成。给定一个RGB图像<spanclass="math display">\[I\in\mathbb{R}^{H\timesW\times3}\]</span>，其中H和W分别为其高度和宽度，我们首先应用神经表示重建（NRR）将其重建为<spanclass="math display">\[I_R\in\mathbb{R}^{H\timesW\times3}\]</span>，并在<span class="math inline">\(I_R\)</span>和<spanclass="math inline">\(I\)</span>之间生成重构误差图<spanclass="math display">\[I_E\in\mathbb{R}^{H\timesW\times1}\]</span>。然后我们将<spanclass="math inline">\(I\)</span>和<spanclass="math inline">\(I_E\)</span>连接起来，将它们输入第一个分支，作为主分支。与大多数IMD方法类似，主分支在最终的特征图上使用一个简单的上采样和Sigmoid激活函数生成一个掩模。然后，我们应用Otsu的方法自适应地选择激活的区域进行图像级预测，就像在[65]中所做的那样。将重建的图像<spanclass="math inline">\(I_R\)</span>输入第二分支，作为特征匹配的互补分支。经过主干处理后，我们得到了两个特征图<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(F_R\)</span>。接下来，我们通过点积计算两个特征映射之间的特征匹配分数<spanclass="math inline">\(M\)</span>，其中真实像素往往具有更高的匹配分数，反之亦然。对于操作检测的两类分类，对<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(M\)</span>应用无监督聚类，然后将两个聚类结果相交，并将像素级对比学习专门应用于更可信的交叉特征。利用所提出的自适应分类结果进行自适应全局平均池，该池集中于高置信篡改区域进行理解图像级预测。在一种弱监督的方式下，应用ground-truth图像级标签来监督预测。在无监督的情况下，从最深层中选择一组高置信的伪标签，通过自蒸馏[69]训练策略来监督浅层预测。通过比较来自Otsu的方法和聚类技术的预测来选择高置信度的伪标签，只选择那些被两个来源一致确定的来源。</p><h2 id="神经表征重建nrr">3.2神经表征重建NRR</h2><p>​  受[63]和图12中实验观察结果的启发，我们应用NRR对输入图像进行重构。重构误差可以突出操作跟踪，从而在后续的IMD模型之前提供一个不可或缺的信息。在INR中，首先使用图像编码器将输入图像转换为特征映射<spanclass="math display">\[F_N\in\mathbb{R}^{H\times W\timesC}\]</span>，其中H和W为高度和宽度，C为特征通道的数量。输入的坐标集可以用<spanclass="math display">\[X\in\mathbb{R}^{H\times W\times2}\]</span>来表示。我们通过将连接<spanclass="math inline">\(F_N\)</span>和<spanclass="math inline">\(X\)</span>进行连接，随后将它们输入多层感知器（MLP）进行解码。NRR的表述为：<span class="math display">\[I_{R}[x,y]=M LP(F_{N}[x,y],X[x,y]),\]</span> ​  其中，<spanclass="math inline">\(I_{R}\)</span>是从<spanclass="math inline">\(I\)</span>重建的RGB像素值，<spanclass="math inline">\([x,y]\)</span>是每个像素位置。NRR的主要目标是重构<spanclass="math inline">\(I\)</span>的RGB值，用损失函数表述为： <spanclass="math display">\[\mathcal{L}_{N R R}=||I-I_{R}||_{1}\,.\]</span>​  请注意，这种重建并不能正确地描述高频像素。因此，我们应用来自[39]的位置编码来将<spanclass="math inline">\(X\)</span>映射到一个高维空间。这种位置编码可表示为：<span class="math display">\[X^{&#39;}=(\sin(2^{0}\pi X),\cos(2^{0}\piX),\cdot\cdot\cdot\cdot,\sin(2^{L-1}\pi X),\cos(2^{L-1}\pi X))\]</span>​  其中，<spanclass="math inline">\(L\)</span>是控制NRR拟合能力的预设定常数。通常情况下，<spanclass="math inline">\(L\)</span>越大，拟合就越准确。在我们的任务中，我们的目标是避免来自反映输入的NRR的输出；相反，我们希望NRR忠实地保存正常（真实）内容中的信息，同时在极端（篡改）像素中引入不忠实。我们根据经验选择<spanclass="math inline">\(L = 8\)</span>作为最优权衡。</p><h2 id="选择性对比学习">3.3选择性对比学习</h2><p>​  从NRR中获得<spanclass="math inline">\(I_R\)</span>后，我们使用<spanclass="math inline">\(I_E =(I_R−I)^2\)</span>计算<spanclass="math inline">\(I\)</span>和<spanclass="math inline">\(I_R\)</span>之间的重构误差图。然后，我们连接<spanclass="math inline">\(I_E\)</span>和<spanclass="math inline">\(I\)</span>，增强到主干的第一个（主）分支的输入。对于第二个（互补）分支的输入，我们发送<spanclass="math inline">\(I_R\)</span>来进行特征匹配。我们使用ResNet50[23]作为主干，它由四个阶段组成，匹配以前的弱监督方法。两个分支的权重共享。经过主干网处理后，我们从不同的输入源获得了2个特征输出<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(F_R\)</span>。然后，我们使用点积计算特征匹配分数<spanclass="math inline">\(M\)</span>为： <spanclass="math display">\[{M}_{x,y}=\sigma\left(\frac{P(F_{R}^{x,y})\cdotP(F^{x,y})}{\sqrt{C}}\right),\]</span> ​  其中，<spanclass="math inline">\({M}_{x,y}\)</span>是在空间位置<spanclass="math inline">\((x,y)\)</span>上的相似度得分。项目头<spanclass="math inline">\(P(\cdot)\)</span>包含2个卷积层和ReLU激活层。<spanclass="math inline">\(\sigma(\cdot)\)</span>表示sigmoid激活函数，<spanclass="math inline">\(\sqrt{C}\)</span>提供归一化。<br/>​  由于NRR能够正确地再现真实的像素（而不是被篡改的像素），<spanclass="math inline">\(M\)</span>中的高匹配分数往往对应于图像的真实部分。相比之下，低分数往往对应于图像的篡改的区域。由于缺乏ground-truth来监督最终特征，我们采用无监督聚类进行类似于[3,37,41,44,47,58]的伪造/原始聚类，并假设元素较少的聚类是被篡改的聚类。这一假设与当前操作数据集的真实情况相一致。原因是，在大多数情况下，被篡改的区域通常比真实的区域要小得多。<br/>​  理想情况下，我们可以通过InfoNCE[22]对<span class="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>应用像素级对比学习像[58]一样。然而，我们发现这种方法在我们的实验中效果并不好，因为由于缺乏ground-truth掩膜，聚类置信度可能较低。为了解决这个问题，我们对<spanclass="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>的聚类结果相交，并将相交的聚类表示为<spanclass="math inline">\(C_1\)</span>。在交集之后，我们将有2个集群，无论其是真实的还是被篡改的，因为它们来自于两个不同来源的相同的预测。因此，我们只将InfoNCE应用于交叉像素进行对比学习，而保持模糊像素不变。这种选择性对比学习损失的表述为：<span class="math display">\[\mathcal{L}_{S CL}=-\log\frac{\frac{1}{J}\sum_{j\in[1,J]}\exp(q\cdotk_{j}^{+}/\tau)}{\sum_{i\in[1,K]}\exp(q\cdot k_{i}^{-}/\tau)},\]</span>​  其中<span class="math inline">\(q\)</span>是一个编码查询；<spanclass="math inline">\(J\)</span>和<spanclass="math inline">\(K\)</span>分别是被选择的正键和负键的数量；<spanclass="math inline">\(\tau\)</span>是一个温度超参数。我们将正键<spanclass="math inline">\(k_{j}^{+}\)</span>设置为与原始区域相关的像素，而负键<spanclass="math inline">\(k_{i}^{-}\)</span>对应于与被篡改区域相关的像素。</p><h2 id="自适应全局平均池化agap">3.4自适应全局平均池化AGAP</h2><p>​  许多现有的方法使用全局最大池（GMP）和全局平均池（GAP）来进行图像级预测，以确定输入是真实的还是被篡改的。然而，GMP可能会阻碍训练，并导致不准确的预测，因为只有最具区别性的反应是反向传播的，而忽略了整个被篡改的内容。全局平均池（GAP）容易出现由于弱激活像素造成的不准确性。<br/>​  为了解决这些挑战，我们引入了自适应全局平均池（AGAP），它侧重于高置信度的篡改区域，用于全面的图像级预测。利用两个聚类结果的交集（在第3.3节中讨论），我们首先从聚类的角度将全局平均池（GAP）专门应用于相交的被篡改区域。然而，仅依赖于无监督聚类可能不能保证在没有地面真实标签的所有输入类型上的最佳性能和鲁棒性。正如在[32]中所讨论的，当图像直方图表现为双峰分布时，Otsu的方法表现良好，而聚类提供了灵活性和处理更复杂的直方图的能力。因此，我们结合Otsu和聚类来增强图像级预测和训练的鲁棒性。具体来说，GAP应用于Otsu和交叉聚类结果的篡改响应，用图像级标签进行损失计算。关于Otsu的方法和聚类的进一步细节可以在他们各自的论文[15,43]中找到。</p><h2 id="弱监督和无监督的imd">3.5弱监督和无监督的IMD</h2><p>​  在弱监督的IMD设置中，我们利用ground-truth图像级标签来监督使用二值交叉熵（BCE）损失的预测训练，即：<span class="math display">\[\mathcal{L}_{B CE}(g,\hat{g})=-(1-g)\log(1-\hat{g})-g\log(\hat{g}),\]</span>​  其中，<span class="math inline">\(g\)</span>和<spanclass="math inline">\(\hat{g}\)</span>分别为ground-truth值和预测得分。以弱监督的方式进行的最终分类损失是两个BCE损失的总和，并将两个池化结果与g进行比较。<br/>​  在没有使用标签的无监督IMD设置中，我们采用了自蒸馏训练策略[69]，使用来自最深层的伪标签作为教师来监督浅层输出。<br/>​  为了简化浅层的预测结果并减少计算开销，主干每个中间阶段的分类头在信道维度中使用空间平均池，将其重塑为单通道特征图。接下来是一个s型函数和全局最大池化。在传统的自蒸馏方法中，将地面真实损失和自蒸馏相结合可以提高整体性能，但这种方法不适用于无监督的环境。我们的实验表明，仅仅依靠自蒸馏并不能产生令人满意的结果，因为从最深层的输出可能缺乏准确性，阻碍了训练过程和整体性能。<br/>​  从选择性监督方法[31]中汲取灵感，这被证明在处理噪声标签数据集方面是有效的，我们利用它的概念，基于特征表示和给定标签之间的对齐来选择训练例子。然而，在我们的无监督设置中，标签的缺失带来了一个挑战。为了克服这一障碍，我们比较了Otsu和聚类方法获得的预测，只选择两种来源一致预测的预测作为自蒸馏训练的伪标签。<br/>​  在伪标签选择中，超过0.5的预测被认为是篡改样本。与弱监督设置类似，我们在选择的伪标签和浅层预测之间使用BCE损失来进行监督。在推理过程中，排除了浅层中的所有分类头，以避免不必要的参数。<br/>​  训练目标。我们首先将通过<spanclass="math inline">\(\mathcal{L}_{NRR}\)</span>训练的NRR作为预训练模型，在IMD训练过程中所有权重冻结。为了简单起见，我们使用符号<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>来表示在无监督方法和弱监督方法中进行分类的损失函数，尽管如上所述略有不同。<br/>​  我们提出的IMD的总损失，记为<spanclass="math inline">\(\mathcal{L}_{total}\)</span>，是使用BCE损失和选择性像素级对比学习损失的分类损失的加权和：<span class="math display">\[\mathcal{L}_{t o t al}=\alpha\mathcal{L}_{c l s}+\beta\mathcal{L}_{S C L}\]</span>​  其中，<span class="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>是加权超参数。</p><h1 id="实验">4实验</h1><p>​  <strong>数据集：</strong>我们的模型只使用CASIAv2[12]进行训练，其中包括7491个真实样本和5063个篡改图像。对于标准IMD任务的评估，我们使用了广泛使用的基准测试，包括CASIAv1[11]、Coverage[57]、Columbia[24]、IMD2020 [42]和NIST16 [19]。CASIAv1[11]由拼接和复制移动图像组成。Coverage[57]只包含使用一些后处理方法的复制移动样本。Columbia[24]由363张未压缩图像组成，平均分辨率为938×720。NIST16[19]和IMD2020[42]只包含被篡改的图像，适用于像素级评估。这些数据集涵盖了传统的操作类型，包括拼接、复制-移动和删除。对于涉及新的或更复杂的操作类型的评估，我们使用IEdit[51]和MagicBrush[68]，这是两个语言驱动的数据集，包含各种新的操作类型，如动作变化和光线变化。<br/>​  <strong>评估指标：</strong>我们使用IOU和F1分数，包括像素级的F1分数P-F1，图像级的F1分数I-F1，以及组合的F1分数C-F1。C-F1分数通过调和平均值同时统计了像素级和图像级的性能，提供了一个整体的性能比较。所有F1分数和IOU分数均使用0.5作为固定阈值进行计算。由于在IEdit[51]中缺乏像素级掩模，我们包括图像级ACC以进行额外的评估。<br/>​  <strong>实现细节：</strong>我们采用ResNet50[23]作为骨干，模型使用PyTorch [45]实现，参数随机初始化。我们应用AdamW[35]作为优化器。NRR中的多层感知器（MLP）遵循三隐藏层架构。NRR训练了120个轮次，初始学习速率为2×10−4，并应用权重衰减。对弱监督模式下的IMD模型进行了50次训练，初始学习率为0.0005，权值衰减。对于无监督模型，我们训练了20个轮次代，初始学习率为0.0001，应用权重衰减。图像增强仅限于随机翻转和裁剪。我们使用固定的阈值0.5从特征映射中提取二进制掩模，与之前的方法一致。弱监督训练的超参数α和β分别设置为1.0和0.1，无监督训练分别设置为1.0和0.3。对于聚类算法，我们使用了K-means[34]。</p><h2 id="与sota方法的比较">4.1与SoTA方法的比较</h2><p>​  为了与SoTA方法进行公平的比较，我们选择了源代码是公开可用的方法。应用于比较的无监督方法有NOI[38]、CFAl [17]、MCA [1]、NoisePrint [9]和IVC [8]，而弱监督方法包括FCN[46]和WSCL[65]。<br/>​  此外，我们使用两个新的操作数据集进行了实验，并将我们的方法与完全监督的方法进行了比较，包括RRUNet[2], Mantra-Net [60], SPAN [25], PSCC-Net [33], Trufor [20], CAT-Net[29],Hifi-Net [21], CR-CNN [62], ObjectFormer [54], and MVSS-Net[5]。<br/>​  <strong>与SoTA无监督方法的比较：</strong>由于无监督方法假设所有图像都包含篡改部分，他们将所有测试图像分类为篡改。因此，它们不适合进行图像级评估。我们进行了像素级实验，比较了它们定位篡改区域的能力，如表1所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113617234.png"alt="image-20241119113617234" /><figcaption aria-hidden="true">image-20241119113617234</figcaption></figure><p>​  我们可以观察到，在五个广泛使用的标准操作基准中，我们提出的方法在无监督设置中比其他无监督方法获得了最好的检测性能。<br/>​  <strong>与SoTA弱监督方法的比较：</strong>表2为弱监督SoTA方法的实验结果。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113655069.png"alt="image-20241119113655069" /><figcaption aria-hidden="true">image-20241119113655069</figcaption></figure><p>​  除了在Columbia[24]数据集中的图像级别上的F1(I-F1)得分外，我们的方法在所有其他指标上都优于SoTA方法。与WSCL相比，Columbia的I-F1得分相对较低，我们认为原因是Columbia没有后处理，所以我们的方法可能对篡改不是很敏感。然而，尽管存在这个问题，我们的方法在Columbia数据集中实现了最好的定位性能。<br/>​  <strong>比较使用新的操作数据集：</strong>为了显示我们的方法的泛化能力。我们在表3中的两个新的操作检测数据集上使用完全监督和弱监督的方法进行评估。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113937715.png"alt="image-20241119113937715" /><figcaption aria-hidden="true">image-20241119113937715</figcaption></figure><p>​  我们可以看到，完全监督的方法不能适应新的操作类型，导致低检测性能，即使它们使用了一个非常大的具有图像级和像素级标签的合成训练数据集。相比之下，我们的方法在使用极少的训练数据而只使用图像级标签的情况下，取得了具有竞争力的性能。<br/>​  <strong>可视化结果：</strong>我们在图4中展示了一些与SoTA方法相比的可视化结果。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114153339.png"alt="image-20241119114153339" /><figcaption aria-hidden="true">image-20241119114153339</figcaption></figure><p>​  我们的方法可以更好地定位被篡改的区域，即使没有使用像素级的标签。然而，由于缺乏像素级的标签，我们的模型不能准确地检测到被篡改的边缘。我们的方法的这些结果是由弱监督模型产生的。</p><h2 id="消融研究">4.2消融研究</h2><p>​  我们进行了几项消融研究来评估每个建议成分的有效性。对于这些研究，我们使用了CASIAv1[12]和NIST16 [19]数据集。</p><p>​  <strong>提出的组件的有效性：</strong>我们引入了三个新的组件：使用神经表示重建（NRR）的预处理阶段，选择性像素对比学习（SCL）和自适应全局平均池（AGAP）的非/弱监督IMD。在弱模式下进行的消融研究见表4。很明显，随着我们所提出的模块的逐步集成，模型检测篡改的整体能力不断提高。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114349893.png"alt="image-20241119114349893" /><figcaption aria-hidden="true">image-20241119114349893</figcaption></figure><p>​  <strong>伪标签选择（PLS）：</strong>在我们的无监督方法中，我们引入了PLS，它专门利用来自两个来源的高可信度伪标签来监督自蒸馏训练过程中的浅层预测。表5检查了PLS的影响。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114437758.png"alt="image-20241119114437758" /><figcaption aria-hidden="true">image-20241119114437758</figcaption></figure><p>​  在没有PLS的实验中，我们使用主分支的图像级预测作为伪标签来指导浅层预测。所提出的PLS在提高无监督性能方面是有效的。</p><p>​  <strong>自适应全局平均池：</strong>为了证明所提出的AGAP的优越性，我们使用不同的池方法在弱监督设置下进行了消融研究，包括全局最大池（GMP）、全局平均池（GAP）、广义平均池（GeM）[50]和全局光滑池（GsM）[55]。结果如表6所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114507800.png"alt="image-20241119114507800" /><figcaption aria-hidden="true">image-20241119114507800</figcaption></figure><p>​  同样，所提出的AGAP也取得了最好的性能，突出了其优越性。</p><h1 id="结论">5结论</h1><p>​  我们提出了一个新的框架，集成了无监督和弱监督方法的图像操作检测（IMD）。我们的方法具有一个开创性的预处理步骤，利用了一个来自隐式神经表示的可控拟合函数，为其提供了一个操作区域的先验。此外，我们提出了一种选择性像素级对比学习技术，该技术对高可信度的区域进行优先排序，减少了由于缺乏像素级标签而产生的不确定性。对于图像级预测，我们引入自适应全局平均池来彻底探索用于检测和鲁棒训练的操作区域。在无监督模式下，我们实现了伪标签选择，从较深的层中选择高可信度的预测作为伪标签，通过自蒸馏训练方法来监督较浅的层中的预测。大量的实验验证了我们的方法的有效性，证明了比现有的无监督和弱监督的方法更好的性能。值得注意的是，我们的方法在检测新的操作方面与完全监督的方法有效地竞争，展示了其在现实场景中的鲁棒性。<br/>​  这项工作的局限性包括被篡改区域边缘的不准确定位，导致比groundtruth更大的预测掩模。<br/>​  未来的工作包括开发更强大的模型和有效的预滤波器，以提高像素级的检测性能。<br/>​  <strong>Acknowledgements:</strong>This work is supported by the DARPA Semantic Forensics (SemaFor) Programunder contract HR001120C0123 and NSF CCSS-2348046.</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientNet</title>
      <link href="/EfficientNet/"/>
      <url>/EfficientNet/</url>
      
        <content type="html"><![CDATA[<h1id="efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">EfficientNet:Rethinking Model Scaling for Convolutional Neural Networks</h1><h3 id="quickstart">Quickstart</h3><p>Install with <code>pip install efficientnet_pytorch</code> and load apretrained EfficientNet with:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from efficientnet_pytorch import EfficientNet</span><br><span class="line">model = EfficientNet.from_pretrained(&#x27;efficientnet-b0&#x27;)</span><br></pre></td></tr></table></figure><h3 id="overview">Overview</h3><p>This repository contains an op-for-op PyTorch reimplementation of <ahref="https://arxiv.org/abs/1905.11946">EfficientNet</a>, along withpre-trained models and examples.<br/>The goal of this implementation isto be simple, highly extensible, and easy to integrate into your ownprojects. This implementation is a work in progress -- new features arecurrently being implemented.</p><p>Details about the models are below:</p><table><thead><tr class="header"><th><em>Name</em></th><th><em># Params</em></th><th><em>Top-1 Acc.</em></th><th><em>Pretrained?</em></th></tr></thead><tbody><tr class="odd"><td><code>efficientnet-b0</code></td><td>5.3M</td><td>76.3</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b1</code></td><td>7.8M</td><td>78.8</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b2</code></td><td>9.2M</td><td>79.8</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b3</code></td><td>12M</td><td>81.1</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b4</code></td><td>19M</td><td>82.6</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b5</code></td><td>30M</td><td>83.3</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b6</code></td><td>43M</td><td>84.0</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b7</code></td><td>66M</td><td>84.4</td><td>✓</td></tr></tbody></table><h4 id="example-feature-extraction">Example: Feature Extraction</h4><p>You can easily extract features with<code>model.extract_features</code>:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from efficientnet_pytorch import EfficientNet</span><br><span class="line">model = EfficientNet.from_pretrained(&#x27;efficientnet-b0&#x27;)</span><br><span class="line"></span><br><span class="line"># ... image preprocessing as in the classification example ...</span><br><span class="line">print(img.shape) # torch.Size([1, 3, 224, 224])</span><br><span class="line"></span><br><span class="line">features = model.extract_features(img)</span><br><span class="line">print(features.shape) # torch.Size([1, 1280, 7, 7])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Survey_on_Deep_Clustering</title>
      <link href="/Survey-on-Deep-Clustering/"/>
      <url>/Survey-on-Deep-Clustering/</url>
      
        <content type="html"><![CDATA[<h2 id="深度生成表示学习">3.2 深度生成表示学习</h2><p>​  另一种深度无监督表示学习方法在于生成模型。生成方法假设数据푥是由潜在表示ℎ生成的，然后从数据中反向推导出表示<spanclass="math inline">\(p(h|x)\)</span>的后验。其中，最典型的方法是变分自动编码器（VAE）[102]。VAE采用方差推理技术，最大化数据似然值的证据下界（ELBO，theevidence lower bound）： <span class="math display">\[\logp(x)\geq\mathbb{E}_{q(h|x)}\left[\log p(x|h)\right]-D_{KL}(q(h|x)\|p(h))\]</span> ​  其中<span class="math inline">\(D_{KL}(\cdot\|\cdot)\)</span>表示两个分布之间的kl-散度，<spanclass="math inline">\({p}(h)\)</span>是潜在表征的先验分布，<spanclass="math inline">\(q(h|x;\varphi)\)</span>是表征的变分后验来近似真后验（即<spanclass="math inline">\(q(h|x;\varphi)~\approx~p(h|x)\)</span>），可以用识别网络<spanclass="math inline">\(\varphi\)</span>进行建模。利用再参数化技巧[102]和蒙特卡罗近似[97]，可以通过反向传播从方程(1)中有效地学习后验。<br/>​  <strong>分析。</strong>深度生成模型具有一些优点，如灵活、可解释和能够重新创建数据点。将生成式表示模型转换为深度聚类任务，使聚类模型能够继承这些优势。</p><h2 id="互信息最大化表示法学习">3.3 互信息最大化表示法学习</h2><p>​  互信息（MI，Mutual information）[103]是度量随机变量<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>之间依赖性的一个基本量，其表述为： <spanclass="math display">\[T(X;Y)=\int\log{\frac{d\mathbb{F}_{XY}}{d\mathbb{E}_{X}\otimes\mathbb{P}_{Y}}}d\mathbb{P}_{X Y}\]</span>​  其中，<span class="math inline">\(\mathbb{P}_{XY}\)</span>为联合分布，<spanclass="math inline">\(\mathbb{F}_{X}=\int_{Y}d\mathbb{P}_{XY}\)</span>和<spanclass="math inline">\({\mathbb{P}_{Y}}=\int_{X}d\mathbb{P}_{XY}\)</span>为边际分布，P푋⊗P푌为边际分布的乘积。传统的互信息估计[106]只适用于离散变量或已知的概率分布。最近，MINE[9]被提出用于用深度神经网络来估计互信息。广泛使用的互信息估计是Jensen-Shannon散度（JSD，Jensen-Shannondivergence）[143]，其公式为： <span class="math display">\[{\cal I}_{J SD}(X;H)=\mathbb{E}_{\mathbb{R}_{XH}}\left[-\operatorname{sp}(-D(x,h))\right]-\mathbb{E}_{\mathbb{R}_{X}\times\mathbb{R}_{H}}\left[\operatorname{sp}(D(x,h))\right]\]</span>​  其中，<span class="math inline">\(\operatorname{sp}(x)\;=\;\log\left({1}+e^{x}\right)\)</span>是软加函数。<spanclass="math inline">\(D\)</span>是一个由神经网络建模的判别器函数。另一个流行的互信息估计是InfoNCE[148]，它将在第3.4小节中介绍。受益于神经估计，互信息在无监督表示学习[7,75]中被广泛应用。更具体地说，通过最大化不同层[7]或数据实例[75]的不同部分之间的互信息来学习表示，从而保证表示的一致性。这可以被看作是对自我监督学习的早期尝试，这对后来的工作有广泛的影响。<br/>​  <strong>分析。</strong>互信息作为相关性和依赖性的基本度量方法，有几个优点。深度聚类任务的主要优点是，由互信息度量的变量不局限于相同的维度和语义空间，如实例和聚类。详细的应用程序将在第4.4小节和第5.4.2小节中进行介绍。与基于自动编码器的深度生成表示学习类似，互信息最大化方法的目标也是实例化的，这在捕获实例之间的关系方面也可能存在上述问题。然而，互信息估计中的边际分布依赖于所有的观测样本。换句话说，实例之间的关系是隐式捕获的，这也提高了深度聚类的性能。</p><h2 id="对比表示学习">3.4 对比表示学习</h2><p>​  对比学习是近年来最流行的无监督表示学习技术之一。其基本思想是将正对拉近，而将负对推远，这也被称为实例辨别。对比学习的代表性目标是InfoNCE损失[148]，公式为：<span class="math display">\[\mathcal{L}_{I n f o N CE}=-\log\sum_{i=1}^{N}\frac{\exp\left(f\left(h_{i},h_{i}^{^{\mathcal{T}}}\right)/\tau\right)}{\sum_{j=1}^{N}\exp\left(f\left(h_{i},h_{j}^{^{\mathcal{T}}}\right)/\tau\right)}\]</span>​  其中<span class="math inline">\(h_{i}\)</span>为锚定样本的表示，<spanclass="math inline">\(h_{i}^{\mathcal{T}}\)</span>为正样本表示，<spanclass="math inline">\(h_{j}^{\mathcal{T}}\)</span>为负样本表示，<spanclass="math inline">\(f\)</span>为相似函数，<spanclass="math inline">\(\tau\)</span>为温度参数[74]。正样本通常通过数据增强进行，数据类型不同。例如，图像数据[30]的翻转、旋转和裁剪增强，图数据[113,217]的节点下降、边缘扰动、属性掩蔽和子图采样。负样本是从数据集[30]中其他实例的增强视图或旧的负表示[72]的动量更新内存库中选择的，这可以看作是噪声的近似。<br/>​  <strong>分析。</strong>对对比学习进行了理论分析，大量证据表明，对比学习学习的表征有利于聚类任务。在[196]中，对比学习用两个特性来解释：正对特征的对齐和超球面上特征分布的均匀性。对齐属性鼓励具有相似特征或语义类别的样本在低维空间中保持接近，这对聚类至关重要。这种鉴别能力也在监督方式[101]中得到了证明。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-modal Manipulation</title>
      <link href="/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/"/>
      <url>/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/</url>
      
        <content type="html"><![CDATA[<center>Unified Frequency-Assisted Transformer Framework for Detecting andGrounding Multi-modal Manipulation</center><center>Huan Liu1,2 · Zichang Tan3 · Qiang Chen3 · Yunchao Wei1,2 · Yao Zhao1,2· Jingdong Wang3</center><h1 id="摘要">摘要</h1><p>​  由于面部伪造和文本错误信息的广泛传播，检测和接地多模态媒体操纵（DGM4，Detectingand grounding multi-modal mediamanipulation）已经变得越来越重要。在本文中，我们提出了统一频率辅助变压器框架，命名为UFAFromer，来解决DGM4问题。与以往仅关注图像（RGB）域来描述视觉伪造特征的最先进的方法不同，我们另外引入了频域作为补充观点。通过利用离散小波变换，我们将图像分解为多个频率子带，捕获丰富的人脸伪造伪影。然后，我们提出的频率编码器，结合带内和带间的自关注，明确地聚合了不同子带内和跨的伪造特征。此外，为了解决图像和频域之间的语义冲突，开发了伪造感知相互模块，进一步实现不同图像和频率特征的有效交互，从而产生对齐和全面的视觉伪造表示。最后，基于视觉和文本伪造特征，我们提出了一个统一的解码器，它包括两个对称的跨模态交互模块，负责收集特定模态的伪造信息，以及一个负责聚合两种模式的融合交互模块。提出的统一解码器将我们的UFAfrorr定义为统一框架，最终简化了整体架构，促进了优化过程。在包含多个扰动的DGM4数据集上的实验结果表明，我们的框架比以前的方法具有优越的性能，在该领域设置了一个新的基准。</p><p><strong>关键词</strong>人脸和文字操作检测；检测和接地；统一；频率辅助</p><h1 id="介绍">1介绍</h1><p>​  近年来，互联网见证了虚假媒体的普及（Zheng et al., 2020; Juefei-Xuet al.,2022），如人脸和伪造图像、深度伪造视频、文本假新闻。随着深度学习的进步，易于创建超现实的内容，使安全和隐私成为一个严重问题，例如，身份欺诈面临伪造（Liuet al., 2021a; Zhang et al., 2019; Liu et al., 2022b, 2021b, 2024a, b,2023a, 2022a)和虚假信息文本假新闻（Ying et al., 2023; Zhou et al.,2023）。为了应对这些日益增长的威胁，研究人员表现出了极大的关注，并提出了各种检测方法，包括面部伪造检测（Miaoet al., 2023; Guan et al., 2022;Miao et al., 2022; Tan et al.,2022)和文本伪造检测（Zhu et al., 2022; Zellers et al.,2019)，关注单模式（即图像或文本）伪造。之前框架中的另一行是多模态伪造检测（Luoet al., 2021a; Khattar et al.,2019），它同时利用了图像和文本模式，并在伪造检测方面取得了更好的结果。这些框架只预测给定的可疑输入的二进制类（即真实的或虚假的），这只是简单地将多模态伪造检测视为一个二进制分类任务。</p><figure><imgsrc="../postimages/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/image-20250107113241178.png"alt="image-20250107113241178" /><figcaption aria-hidden="true">image-20250107113241178</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Circle Loss A Unified Perspective of Pair Similarity Optimization</title>
      <link href="/Circle-Loss/"/>
      <url>/Circle-Loss/</url>
      
        <content type="html"><![CDATA[<p>Circle Loss: A Unified Perspective of Pair SimilarityOptimization</p><h1 id="摘要">摘要</h1><p>​  本文提出了一种关于深度特征学习的对相似度优化的视点，旨在使类内相似度最大化，类间相似度最小。我们发现了大多数的损失函数，包括triplet损失和softmax交叉熵损失，将$ s_n $ 和 $ s_p $ 嵌入到相似度对中，并寻求减少 $ (s_n−s_p) $。这种优化方式是不灵活的，因为每个相似度得分的惩罚强度被限制为相等。我们的直觉是，如果一个相似度得分远远偏离了最优值，就应该强调它。为此，我们简单地重新加权每个相似度，以突出较少优化的相似度分数。它造就了一个<strong>Circle损失</strong>，由于它的圆形决策边界而命名。<strong>Circle损失</strong>对于两种基本的深度特征学习范式有一个统一的范式，即使用类级标签和成对标签进行学习。在分析上，我们表明，与损失函数优化$ (s_n−s_p) $相比，<strong>Circle损失</strong>提供了一种更灵活的对收敛目标更明确的优化方法。通过实验，我们证明了<strong>Circle损失</strong>在各种深度特征学习任务中的优越性。在人脸识别、人的再识别以及几个细粒度的图像检索数据集上，所取得的性能与现有的技术水平相当。</p><h1 id="介绍">1.介绍</h1><p>​  本文对两种基本的深度特征学习范式进行了相似性优化分析，即从具有类级标签的数据和具有成对标签的数据中进行学习。前者采用分类损失函数（例如，软最大交叉熵损失[25,16,36]）来优化样本和权向量之间的相似性。后者利用一个度量损失函数（例如，三联体损失[9,22]）来优化样本之间的相似性。在我们的解释中，这两种学习方法之间没有内在的区别。它们都寻求最小化类间相似度$ s_n $ ，也寻求最大化类内相似度 $ s_p $。<br/>​  从这个角度来看，我们发现许多流行的损失函数（例如，triplet损失[9,22]，softmax交叉熵损失及其变体[25,16,36,29,32,2]）具有相似的优化模式。它们都将$ s_n $ 和 $ s_p $ 嵌入到相似度对中，并寻求减少 $ (s_n−s_p) $ 。在 $(s_n−s_p) $ 中，增加 $ s_p $ 相当于减少 $ s_n $。我们认为这种对称优化方式容易出现以下两个问题。</p><ul><li><strong>缺乏进行优化的灵活性。</strong>在 $ s_n $ 和 $ s_p $上的惩罚强度被限制为相等。给定指定的损失函数，关于 $ s_n $ 和 $ s_p $的梯度具有相同的振幅（详见第2节）。在某些角落的情况下，例如， $ s_p $很小，并且 $ s_n $ 已经接近0（图1(a)中的“a”），它继续以较大的梯度惩罚 $s_n $ 。它是低效的和非理性的。</li></ul><hr /><figure><img src="../postimages/Circle-Loss/image-20241014230803132.png"alt="image-20241014230803132" /><figcaption aria-hidden="true">image-20241014230803132</figcaption></figure><p>图1：流行的还原优化方式 $ (s_n−s_p) $ 与所提出的还原优化方式 $(_ns_n−_ps_p) $ 的比较。<br/>  (a)还原 $ (s_n−s_p) $容易进行不灵活的优化（A、B和C相对于 $ s_n $ 和 $ s_p $都有相等的梯度），以及模糊的收敛状态（在决策边界上的T和T0都是可以接受的）。<br/>  (b)在 $ (_ns_n−_ps_p) $ 下，<strong>Circle损失</strong>动态调整其对 $ s_n $和 $ s_p $ 的梯度，从而受益于一个灵活的优化过程。对于A，它强调增加 $ s_p$ ；对于B，它强调减少 $ s_n $。此外，它有利于圆形决策边界上的指定点T收敛，建立一个确定的收敛目标。</p><hr /><ul><li><strong>模糊的收敛状态。</strong>优化 $ (s_n−s_p) $ 通常会导致 $s_n−s_p=m $（m为边际）的决策边界。这个决策边界允许模糊性（例如，图1(a)中的“ $ T $”和“ $ T^{} $ ”）来收敛。例如，有 $ {s_{n},s_{p}}={0.2,0.5} $ ，而 $T^{} $ 有 $ {s_{n}<sup>{},s_{p}</sup>{}}={0.4,0.7} $ 。它们都获得了边际m= 0.3。但是，通过相互比较，我们发现 $ s_{n}^{} $ 和 $ s_{p} $之间的差距只有0.1。因此，模糊收敛影响了特征空间的可分性。</li></ul><p>​  有了这些见解，我们就有了一种直觉，即不同的相似性得分应该有不同的惩罚强度。如果一个相似度得分偏离最优值，它应该受到很强的惩罚。否则，如果一个相似度得分已经接近最优值，那么它就应该进行轻微的优化。为此，我们首先将$ (s_n−s_p) $ 推广为 $ (_ns_n−_ps_p) $ ，其中 $ _n $ 和 $ _p $是独立的加权因子，允许 $ s_n $ 和 $ s_p $在不同的速度学习。相似度得分偏离最优值越远，加权因子就会越大。这样的优化结果是决策边界$ _ns_n−_ps_p=m $ ，在 $ (s_n,s_p) $空间中产生一个圆的形状，因此我们将所提出的损失函数命名为<strong>Circle损失</strong>。<br/>​  由于简单，<strong>Circle损失</strong>本质上从以下三个方面重塑了深度特征学习的特征：<br/>​  <strong>首先，这是一个统一的损失函数。</strong>从统一相似对优化的角度出发，我们提出了两种基本学习范式的类级标签和成对标签学习。<br/>​  <strong>第二，灵活的优化。</strong>在训练过程中，反向传播到$ s_n(s_p) $ 的梯度将被 $ _n(_p) $放大。那些弱优化的相似性得分将有更大的权重因子，并因此得到更大的梯度。如图1(b)所示，对A、B、C的优化是不同的。<br/>​  <strong>第三，有明确的收敛状态。</strong>在圆形决策边界上，<strong>Circle损失</strong>倾向于指定的收敛状态（图1(b)中的“T”），如第3.3节所示。相应地，它建立了一个明确的优化目标，有利于可分性。<br/>​  本文的主要贡献总结如下：</p><ul><li>我们提出了<strong>Circle损失</strong>，一个简单的损失函数的深度特征学习。通过在监督下对每个相似度得分进行重新加权，有利于优化灵活、确定收敛目标的深度特征学习。</li><li>我们提出的<strong>Circle损失</strong>与兼容性的类级标签和成对的标签。略有修改下，<strong>Circle损失</strong>将退化为triplet损失或softmax交叉熵损失。</li><li>我们对各种深度特征学习任务进行了广泛的实验，如人脸识别、人的再识别、汽车图像检索等。在所有这些任务中，我们证明了<strong>Circle损失</strong>的优越性与性能与现有的技术相当。</li></ul><h1 id="统一的视角">2.统一的视角</h1><p>​  深度特征学习的目的是最大化类内相似性 $ s_p $ ，以及减少类间相似性 $s_n $ 。例如，在余弦相似度度量下，我们期望 $ s_{p} $ 和 $ s_{n} $。<br/>​  为此，使用<strong>类级标签学习</strong>和使用<strong>成对标签学习</strong>是两种基本范式。它们通常被认为是分开的，彼此之间的w.r.t与损失函数显著不同。给定类级标签，第一个基本上学习将每个训练样本分类为目标类，例如分类损失。L2-Softmax[21]，Large-marginSoftmax[15]，AngularSoftmax[16]，NormFace[30]，AMSoftmax[29]，CosFace[32]，ArcFace[2]。这些方法也被称为基于代理的学习，因为它们优化了样本和代表每个类的一组代理之间的相似性。相比之下，给定成对标签，第二个直接学习特征空间中的成对相似性（即样本之间的相似性），因此不需要代理，例如，约束损失[5,1]，三联体损失[9,22]，提升结构损失[19]，n对损失[24]，直方图损失[27]，角损失[33]，基于边际损失[38]，多相似性损失[34]等。<br/>​  本文从单一的角度来看待这两种学习方法，不偏好基于代理的相似性或基于成对的相似性。给定特征空间中的一个样本x，假设有K个类内相似度得分，L个类间相似度得分。我们将这些相似度得分分别表示为$ {s_{p}^{i}}(i=1,2, ,K) $ 和 $ {s_{n}^{j}}(j=1,2,,L) $。<br/>​  为了最小化每个 $ s_{n}^{j} $ 以及最大化 $ s_{p}^{i} $ ， $(i  {1,2, ,,K},j  {1,2, ,,L}) $ ，我们提出了一个统一的损失函数： <spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{u n i} &amp;=\log\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}-s_{p}^{i}+m))\right]\\&amp;=\mathrm{log}\left[1+\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}+m))\sum_{i=1}^{K}\exp(\gamma(-s_{p}^{i}))\right]\end{aligned}\]</span>​  其中 $ $ 是一个尺度因子， $ m $是一个更好的相似性分离的边际。<br/>​  等式1是直观的。它遍历每一个相似度对来减少$ (s_{n}<sup>{j}-s_{p}</sup>{i}) $。我们注意到，通过轻微的修改，它可以退化为三联体损失或分类损失。</p><p>​  <strong>给定类级标签，</strong>我们计算了分类层中 $ x $ 和权重向量$ w_{i} (i=1,2, , N) $（N是训练类别数）之间的相似性得分。<br/>​  具体来说，我们通过： $s_{n}<sup>{j}=w_{j}</sup>{}{x}/{(}||w_{j}|||x||{)} $ （ $ w_{j} $是第j个非目标权重向量）得到（N−1）类间相似性得分。此外，我们得到了一个单一的类内相似性评分（省略了上标）$ s_{p};=;w_{y}^{}x/(||w_{y}|||x||) $。有了这些先决条件，等式1退化为AM-Softmax[29,32]，这是Softmax损失的一个重要变体（即，Softmax交叉熵损失）： <spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{am}&amp;=\log\left[1+\sum_{j=1}^{N-1}\exp(\gamma(s_{n}^{j}+m))\exp(-\gammas_{p})\right]\\&amp;=-\log\frac{\exp(\gamma(s_{p}-m))}{\exp(\gamma(s_{p}-m))+\sum_{j=1}^{N}\exp(\gammas_{n}^{j})}\end{aligned}\]</span> ​  此外，当 $ m=0 $，等式2进一步退化为Normface[30]。如果将内积替换余弦相似度，并且设置 $ $，它最终退化为Softmax损失。</p><p>​  <strong>给定成对的标签，</strong>在小批量中，我们计算x和其他特征之间的相似性得分。具体来说，$ s_{n}<sup>{j}=(x_{n}</sup>{j})<sup>{}x/(||x_{n}</sup>{j}|||x||) $ （ $x_{n}^{j} $ 是负样本集 $ {} $ 中的第j个样本）和 $s_{p}<sup>{j}=(x_{p}</sup>{j})<sup>{}x/(||x_{p}</sup>{j}|||x||) $ （ $x_{p}^{j} $ 是正样本集 $ {} $ 中的第i个样本）。相应地， $ K=|P|,,L=|{}|$ 。等式1与硬挖掘[22,8]退化为triplet损失： <spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{t ri}&amp;=\operatorname*{lim}_{\gamma\to+\infty}{\frac{1}{\gamma}}{\mathcal{L}}_{uni}\\&amp;=\operatorname*{lim}_{\gamma\to+\infty}\frac{1}{\gamma}\log\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}-s_{p}^{i}+m))\right]\\&amp;=\operatorname*{max}\left[s_{n}^{j}-s_{p}^{i}+m\right]_{+}\end{aligned}\]</span>​  具体来说，我们注意到在等式3中，“ $ ({}) $”操作采用Lifted-Structure损失[19]，N-pair损失[24]，多相似性损失[34]等，在样品之间进行“软化”硬挖掘。 $ $的扩大逐渐增强了挖掘强度，当 $ +$ 时，导致了[22,8]的典型硬挖掘。</p><p>​  <strong>梯度分析。</strong>等式2和等式3显示了triplet损失，Softmax损失及其几个变体可以被解释为等式1的特定情况。换句话说，它们都在优化$ (s_n−s_p) $ 。</p><hr /><figure><img src="../postimages/Circle-Loss/image-20241015154934450.png"alt="image-20241015154934450" /><figcaption aria-hidden="true">image-20241015154934450</figcaption></figure><p>图2：损失函数的梯度。(a)triplet的损失。(b)AM-Softmax损失。(c)提出的Circle损失。triplet损失和AM-Softmax损失都缺乏优化的灵活性。$ s_p $ （左）和 $ s_n $（右）的梯度被限制为相等，并在收敛时突然下降(相似对B）。例如，在A处，类内相似度评分$ s_p $ 已经接近1，并且仍然有一个很大的梯度。此外，决策边界与 $ s_p=s_n$平行，允许模糊收敛。相比之下，提出的Circle损失分配不同的梯度，取决于它们到最优的距离。对于A（$ s_n $ 和 $ s_p $ 都很大），Circle损失的重点是优化 $ s_n $。对于B，由于 $ s_n $显著减少，Circle损失减少了它的梯度，从而加强了一个温和的惩罚。Circle损失具有一个圆形的决策边界，并促进了准确的收敛状态。</p><hr /><p>​  在只有一个 $ s_n $ 和 $ s_p $ 的小场景下，我们在图2(a)和(b)中可视化了中的triplet损失和AM-Softmax损失的梯度，从中我们得出以下观察结果：</p><ul><li>首先，在损失达到其决策边界之前（梯度消失之前），相对于 $ s_p $ 和 $s_n $ 的梯度是相同的。状态A具有 $ {s_{n},s_{p}} = {0.8,0.8} $，表示良好的类内紧致性。然而，A相对于 $ s_p $仍然有较大的梯度。它导致了在优化过程中缺乏灵活性。</li><li>第二，梯度在收敛前保持（大致）不变，并在收敛时发生突然的下降。状态B更接近决策边界，并且比A优化得更好。然而，损失函数（triplet损失和AMSoftmax损失）对A和B施加近似相等的惩罚。这是缺乏灵活性的另一个证据。</li><li>第三，决策边界（白色虚线）平行于 $ s_{n}-s_{p}=m $。该边界上任意两点（如图1中的 $ {} $ 和 $ {}^{} $）的相似间隙等于m，因此具有相同的困难。换句话说，损失函数最小化 $(s_{n}-s_{p}+m) $ 在 $ {} $ 或 $ {}^{} $的收敛性上不偏不倚，并且容易出现模糊收敛。关于这个问题的实验证据，请参见第4.6节。</li></ul><p>​  这些问题源于最小化 $ (s_{n}-s_{p}) $ 的优化方式，其中减少 $ s_n $相当于增加 $ s_p $。在下面的第3节中，我们将把这种优化方式转换为更一般的优化方式，以促进更高的灵活性。</p><h1 id="一个新的损失函数">3.一个新的损失函数</h1><h2 id="自定速度的加权">3.1.自定速度的加权</h2>​  我们考虑通过允许每个相似度评分根据当前优化状态以自己的速度学习来增强优化灵活性。我们首先忽略了等式1中的边际项m，并通过以下方式将统一损失函数转换为提出的Circle损失：$$<span class="math display">\[\begin{aligned}{\mathcal{L}}_{c i r c le}&amp;=\mathrm{log}\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp{\left({\gamma(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i}}\right)}\right]\\&amp;=\log\left[1+\sum_{i=1}^{L}\exp(\gamma\alpha_{n}^{j}s_{n}^{j})\sum_{i=1}^{K}\exp(-\gamma\alpha_{p}^{i}s_{p}^{i})\right]\end{aligned}\]</span><span class="math display">\[​&amp;emsp;&amp;emsp;其中 $ \alpha_{n}^{j} $ 和 $ \alpha_{p}^{i} $为非负加权因子。&lt;br/&gt;​&amp;emsp;&amp;emsp;等式4来源于等式1，通过将$ (s_{n}^{j}-s_{p}^{i}) $ 推广为 $(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i}) $。在训练过程中，当反向传播到 $ {s}_{n}^{j}\,(s_{p}^{i}) $ 时，相对于 $(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i}) $ 的梯度将与 $\alpha_{n}^{j}(\alpha_{p}^{i}) $ 相乘。当相似度得分偏离其最优值时（即，$ s^j_n $ 时为 $ O_n $ ， $ s^i_p $ 时为 $ O_p $），应得到一个较大的加权因子，以得到较大梯度的有效更新。为此，我们以一种自定速度的方式来定义$ \alpha_{p}^{i} $ 和 $ \alpha_{n}^{j} $ ：\]</span><spanclass="math display">\[\begin{cases}\alpha_{p}^{i}=[O_{p}-s_{p}^{i}]_{+},\\\alpha_{n}^{j}=[s_{n}^{j}-{O}_{n}]_{+}\\\end{cases}\]</span><p>$$ ​  其中[·]+为“零截止”操作，以确保 $ <em>{p}^{i} $ 和 $ </em>{n}^{j}$为非负值。<br/>​  <strong>讨论。</strong>在监督下重新调整余弦相似度是现代分类损失[21,30,29,32,39,40]中常见的做法。传统上，所有的相似性得分都具有相同的尺度因子$ $。当我们将一个分类损失函数中的softmax值看作是一个样本属于某一类的概率时，等量的重新缩放是很自然的。相比之下，Circle损失在重新缩放之前用一个独立的加权因子乘以每个相似度分数。因此，它摆脱了平等的重新缩放的约束，并允许更灵活的优化。除了更好的优化的好处外，这种重新加权（或重新缩放）策略的另一个意义还涉及到潜在的解释。Circle损失放弃了将样本以大概率分类为目标类的解释。相反，它具有相似度对的优化视角，这与两种学习范式相兼容。</p><h2 id="类内和类间的边际">3.2.类内和类间的边际</h2><p>​  在损失函数优化 $ (s_{n}-s_{p}) $ 中，添加一个边际 $ m $加强了优化[15,16,29,32]。由于 $ s_{n} $ 和 $ -s_{p} $ 处于对称位置， $s_{n} $ 的正边际等于于 $ s_{p} $ 的负边际。因此，它只需要一个单一的边际$ m $ 。在Circle损失中， $ s_n $ 和 $ s_p $ 处于不对称位置。当然，它需要$ s_n $ 和 $ s_p $ 各自的边际，其公式如下： <spanclass="math display">\[\mathcal{L}_{c i r c le}=\log\left[1+\sum_{i=1}^{L}\exp(\gamma\alpha_{n}^{j}(s_{n}^{j}-\Delta_{n}))\sum_{i=1}^{K}\exp(-\gamma\alpha_{p}^{i}(s_{p}^{i}-\Delta_{p}))\right]\]</span>​  其中 $ <em>{n} $ 和 $ </em>{p} $分别为类间和类内的边距。<br/>​  基本上，等式6中的Circle损失期望 ${s}<em>{p}<sup>{i}<em>{p} $ 和 $ {s}</em>{n}</sup>{j}</em>{n} $。通过推导决策边界，进一步分析了 $ <em>{n} $ 和 $ </em>{p} $的设置。为简单起见，我们考虑了二值分类的情况，其中决策边界是通过 $<em>{n}(s</em>{n}-<em>{n})-</em>{p}(s_{p}-<em>{p})=0 $得到的。并结合等式5、决策边界为： <spanclass="math display">\[(s_{n}-\frac{O_{n}+\Delta_{n}}{2})^{2}+(s_{p}-\frac{O_{p}+\Delta_{p}}{2})^{2}=C\]</span>​  其中， $ C=((O</em>{n}-<em>{n})^{2}+(O</em>{p}-<em>{p})^{2})/4 $。<br/>​  等式7显示了决策边界为圆形，如图1 (b)。所示圆的中心在 $s</em>{n},=,(O_{n},+,<em>{n})/2,s</em>{p},=,(O_{p},+,<em>{p})/2 $处，其半径等于 $ {} $ 。<br/>​  在等式中有五个超参数，即等式5的 $ O_p $、 $ O_n $ 和等式6的 $ $ ， $ </em>{p} $ 和 $ <em>{n} $ 。我们通过设置 ${O}</em>{p}=1+m,{O}<em>{n}=-m,</em>{p}=1-m $ 和 $ <em>{n}=m $来减少超参数。因此，在等式7中的决策边界减少为： <spanclass="math display">\[\left(s_{n}-0\right)^{2}+\left(s_{p}-1\right)^{2}=2m^{2}\]</span>​  有了等式8中定义的决策边界，我们对Circle损失有了另一个直观的解释。其目的是优化$ {s}</em>{m} $ 和 $ {s}<em>{n} $ 。参数 $ m $控制着决策边界的半径，可以看作一个松弛因子。换句话说，Circle损失期望 $s</em>{p}^{i}-m $ 和 $ s_{n}^{j}m $。<br/>​  因此，只有两个超参数，即尺度因子 $ $ 和松弛度 $ m $。我们将在第4.5节中实验分析 $ m $ 和 $ $ 的影响。</p><h2 id="circle损失的优点">3.3.Circle损失的优点</h2><p>​  Circle损失相对于 $ s_{n}^{j} $ 和 $ s_{p}^{i} $ 的梯度推导如下：<span class="math display">\[\frac{\partial\mathcal{L}_{c i r c le}}{\partials_{n}^{j}}=Z\frac{\exp\left(\gamma((s_{n}^{j})^{2}-m^{2})\right)}{\sum_{l=1}^{L}\exp\left(\gamma((s_{n}^{l})^{2}-m^{2})\right)}\gamma(s_{n}^{j}+m),\]</span></p><p>和</p><p><span class="math display">\[\frac{\partial\mathcal{L}_{c i r c le}}{\partials_{p}^{i}}=Z\frac{\exp\left(\gamma((s_{p}^{i}-1)^{2}-m^{2})\right)}{\sum_{k=1}^{K}\exp\left(\gamma((s_{p}^{k}-1)^{2}-m^{2})\right)}\gamma(s_{p}^{i}-1-m),\]</span></p><p>​  其中 $ Z=1-(-_{c i r c l e}) $。<br/>​  在二值分类的小场景下（或只有一个 $ s_n $ 和 $ s_p $时），我们在图2 (c)中可视化了 $ m $的不同设置下的梯度，从中我们得出以下三个观察结果：</p><ul><li>$ s_n $ 和 $ s_p $ 的平衡优化。我们曾提及过，损失函数最小化 $(s_{n}-s_{p}) $ 在 $ s_p $ 和 $ s_n $上总是具有相等的梯度，这是不灵活的。相比之下，Circle损失展现出动态的惩罚强度。在指定的相似对$ {s_{n},s_{p}} $ 中，如果 $ s_p $ 比 $ s_n $ 更好（如图2(c)中的 $A={0.8,0.8} $ ），Circle损失赋予 $ s_n $的梯度更大（反之亦然），从而更优先的降低 $ s_n $。平衡优化的实验证据详见第4.6节。</li><li>逐渐减弱的梯度。在训练开始时，相似性得分偏离最佳值很远，并获得较大的梯度（如图2（c）中的“A”）。随着训练逐渐接近收敛，相似度得分上的梯度相应衰减（如图2(c)中的“B”），进行了温和的优化。第4.5节的实验结果表明，学习效果对$ $的各种设置都是鲁棒性的(在等式6中)，我们将其归因于自动衰减的梯度。</li><li>一个（更）明确的收敛目标。Circle损失具有循环决策边界，有利于 $ {} $的收敛而不是 $ {}^{} $ 的收敛（图1）。这是因为 $ {} $与决策边界上的所有其他点相比， $ s_p $ 和 $ s_n $之间的差距最小。换句话说， $ {}^{} $ 在 $ s_p $ 和 $ s_n $之间的差距较大，而且本身就更难维持。相比之下，最小化 $ (s_{n}-s_{p}) $的损失具有一个齐次的决策边界，即决策边界上的每一个点到达决策边界都具有相同的困难。在实验中，我们观察到，Circle损失导致收敛后的相似度分布更为集中，详见章节4.6和图5。</li></ul><h1 id="实验">4.实验</h1><p>​  我们综合评估了两种基本学习方法下的有效性：给定类级标签学习和给定成对的标签学习。对于前一种方法，我们在人脸识别（4.2节）和人的再识别（4.3节）任务上评估了我们的方法。对于后一种方法，我们使用细粒度的图像检索数据集（第4.4节），它们相对较小，鼓励使用成对标签进行学习。我们证明了Circle损失在这两种情况下都是有效的。第4.5节分析了这两个超参数的影响，即等式6中的尺度因子$ $ 和等式8中的松弛因子 $ m $。我们证明了在合理的设置下，Circle损失是鲁棒的。最后，第4.6节通过实验证实了Circle损失的特性。</p>]]></content>
      
      
      <categories>
          
          <category> 损失函数 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>公式识别工具</title>
      <link href="/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/"/>
      <url>/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="latexocr">LatexOCR</h1><p>#1. 介绍</p><p>​  latexocr是一个识别数学（其他）公式转换为LaTeX代码的软件，也是github上一个开源项目，其是本地部署的公式OCR识别工具。（这样就不必花钱在网站上识别了）<br/>​  其GitHub地址为：https://github.com/lukas-blecher/LaTeX-OCR</p><h1 id="使用">2. 使用</h1><p>​  首先安装conda，其次在虚拟环境在安装python（Python3.7+），然后进去虚拟环境、安装PyTorch ，再然后使用如下代码安装：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install &quot;pix2tex[gui]&quot;</span><br></pre></td></tr></table></figure><p>​  安装完成后，在虚拟环境下的终端输入latexocr（ps：第一次使用会自动下载数据集），会弹出以下界面：</p><figure><imgsrc="../postimages/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/image-20241011215427134.png"alt="image-20241011215427134" /><figcaption aria-hidden="true">image-20241011215427134</figcaption></figure><p>​  然后，按下[Alt+S]来框选要识别的公式，得到以下界面：</p><figure><imgsrc="../postimages/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/image-20241011215542881.png"alt="image-20241011215542881" /><figcaption aria-hidden="true">image-20241011215542881</figcaption></figure><p>​  这样公式就识别成功了，如果出现了错误，可以降低Temperature来提高识别准确率。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised_Semantic_Segmentation</title>
      <link href="/Unsupervised-Semantic-Segmentation/"/>
      <url>/Unsupervised-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<h1id="boosting-unsupervised-semantic-segmentation-with-principal-mask-proposal">1.Boosting Unsupervised Semantic Segmentation with Principal MaskProposal</h1><h2 id="摘要">摘要</h2><p>​  无监督语义分割的目的是通过在没有任何形式注释的图像语料库中识别全局语义类别，自动将图像分割成语义上有意义的区域。基于自监督表示学习的最新进展，我们关注于如何利用这些大型的预训练模型，用于无监督分割的下游任务。我们提出了PriMaPs-PrincipalMaskProposals-基于图像的特征表示将图像分解为语义上有意义的掩模。这使得我们可以通过使用随机期望最大化算法PriMaPs-EM将类原型拟合到PriMaPs中来实现无监督语义分割。尽管PriMaPs-EM概念简单，但它在各种预训练的主干模型，包括DINO和DINOv2，以及不同的数据集，如城市景观、coco-st-Stuff之间导致竞争结果。重要的是，当PriMaPs-EM正交应用于当前最先进的无监督语义分割管道时，它能够提高结果。代码可在https://github.com/visinf/primaps上找到。</p><h2 id="primaps-principal-mask-proposals">PriMaPs: Principal MaskProposals</h2><p>​  在本文中，我们利用了自监督表示学习的最新进展（Caron等人，2021；Oquab等人，2024年），用于无监督语义分割的特定下游任务。我们的方法是基于观察到，这些预先训练过的特征已经表现出内在的空间相似性，捕获语义相关性，从而为拟合全局伪类表示提供指导。</p><p><strong>简单的baseline</strong></p><p>​  考虑一个简单的基线，将K-means聚类应用于DINO ViT特征（Caron etal.，2021）。令人惊讶的是，这已经导致了相当好的无监督语义分割结果，例如，大约15%的平均IoU分割27个类别（Cordts et al.，2016），见Tab1。然而，在相同的特征空间和地面真实标签之间的监督线性探测——理论上界——导致明显优于近36%的平均结果。鉴于这一差距和该方法的简单性，我们得出结论，与之前的工作不同，直接获得语义分割有宝贵的潜力（汉密尔顿等人，2022；Seong等人，2023）。</p><p><strong>从K-means到PriMaPs-EM</strong></p><p>​  当检查K-means基线和最先进的方法时（汉密尔顿等人，2022；Seong等人，2023），见图4，可以定性地观察到，在各自的预测中，更多的局部一致性已经导致更少的错误分类。我们的灵感来自（Drineaset al.，2004；Ding &amp;He，2004），他指出，由主成分跨越的PCA子空间是K-means聚类的松弛解决方案。我们观察到，主成分对对象-和以场景为中心的图像特征具有较高的语义相关性(cf。图1）。我们利用优势特征模式对图像进行迭代分割，通过图像特征与各自的第一主成分的余弦相似性来识别。我们命名得到的类不可知的图像分解PriMaPs-主掩码建议。我们观察到，主成分对对象-和以场景为中心的图像特征具有较高的语义相关性(cf。图1）。我们利用优势特征模式对图像进行迭代分割，通过图像特征与各自的第一主成分的余弦相似性来识别。我们命名得到的类不可知的图像分解PriMaPs-主掩码建议。PriMaPs直接起源于SSL表示，并指导无监督语义分割的过程。如图3所示，我们基于优化的方法，PriMaPs-EM，在从冻结的深度神经网络主干计算出的SSL特征表示上操作。该优化实现了在PriMaPs指导下的聚类目标的随机EM。具体来说，PriMaPs-EM通过优化两个相同大小的向量集，以全局一致的方式适合于类原型，其中一个是另一个的指数移动平均（EMA）。我们证明，PriMaPs-EM能够精确地无监督地分割图像到语义上有意义的区域，同时相对轻量级，并正交于大多数以前的无监督语义分割方法。</p><h3 id="派生primap">派生PriMaP</h3><p>​  我们从一个冻结的预先训练的自监督主干模型<spanclass="math inline">\({\mathcal F}:\mathbb{R}^{3\times h\timesw}\longrightarrow\mathbb{R}^{C\times H\timesW}\)</span>开始，它将图像<spanclass="math inline">\(I\in\mathbb{R}^{3\times h\timesw}\)</span>嵌入到一个密集的特征表示<spanclass="math inline">\(f\in\mathbb{R}^{C\times H\times W}\)</span>：<span class="math display">\[f={\mathcal{F}}(I)\,\]</span>​  这里，C表示密集特征的通道维数，H=h/p，W=w/p，p对应主干的输出步幅。基于此图像表示，下一步是将图像分解为具有语义意义的掩模，为拟合全局类原型提供局部分组先验。<br/>​  初始主掩码建议。为了识别图像I中的初始主掩模方案，我们利用主成分分析分析了其特征的空间统计相关性。具体地说，我们考虑了经验特征协方差矩阵<span class="math display">\[\Sigma={\frac{1}{HW}}\sum_{i=1}^{H}\sum_{j=1}^{W}\Bigl(f_{:,i,j}-\bar{f}\Bigr)\left(f_{:,i,j}-\bar{f}\right)^{\mathsf{T}},\]</span>​  其中，<spanclass="math inline">\(f_{:,i,j}\in\mathbb{R}^{C}\)</span>为位置（i，j）处的特征，<spanclass="math inline">\({\overline}\in\mathbb{R}^{C}\)</span>为平均特征。为了识别捕获特征分布中最大方差的特征方向，我们通过求解来寻找Σ的第一个主成分<span class="math display">\[\Sigma v=\lambda v\,\]</span>​  我们得到了第一个主分量作为最大特征值λ1的特征向量v1，利用平坦特征f可以用奇异值分解（SVD）有效地计算出来。<br/>​  为了识别一个候选区域，我们的下一个目标是计算一个到主导特征方向的空间特征相似度图。我们观察到，直接使用主方向这样做并不总是会导致足够好的定位，也就是说，图像中多个视觉概念的高度相似性，在附录A.1中进行了更详细的阐述。这可以通过首先在特征图中锚定主要的特征向量来避免。为此，我们将归一化特征空间fˆ中的余弦距离考虑为，得到了第一主分量v1的最近邻特征<spanclass="math inline">\({\tilde{f}}\in \mathbb{R}^{C}\)</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>EAGLE:Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</title>
      <link href="/EAGLE/"/>
      <url>/EAGLE/</url>
      
        <content type="html"><![CDATA[<center>EAGLE: Eigen Aggregation Learning for Object-Centric UnsupervisedSemantic Segmentation</center><center>Chanyoung Kim* Woojung Han* Dayun Ju Seong Jae Hwang†</center><center>Yonsei University</center><h1 id="摘要">摘要</h1><p>​  语义分割天生依赖于大量的像素级注释数据，导致了无监督方法的出现。其中，利用自监督视觉转换器进行无监督语义分割（USS）在表达深度特征方面取得了稳步的进展。然而，对于使用复杂对象对图像进行语义分割，一个主要的挑战仍然存在：在补丁级特征中缺乏显式的对象级语义编码。这种技术限制往往导致对具有不同结构的复杂对象的分割不足。为了解决这一差距，我们提出了一种新的方法，EAGLE，它强调无监督语义分割的对象中心表示学习。具体地说，我们介绍了EiCue，这是一种光谱技术，通过来自深度图像特征的语义相似度矩阵和来自图像的颜色亲和度的特征基来提供语义和结构线索。此外，通过将我们的以对象为中心的对比损失与EiCue结合起来，我们指导我们的模型学习具有图像内和图像间对象-特征一致性的对象级表示，从而提高语义的准确性。在COCO-Stuff、城市景观和波茨坦-3数据集上的广泛实验证明了鹰的最新结果，在复杂场景中具有准确和一致的语义分割。</p><h1 id="引言">1.引言</h1><p>​  特别是，最近的基于网络的方法经常利用一个自监督的视觉变压器（ViT）来学习补丁级的特征。虽然它们的补丁级特性被证明对进一步的USS推理步骤（例如，K-means）很有用，但底层的对象级语义并没有明确地强加在这些补丁级特性中。要掌握“对象级语义”，请考虑一个覆盖层对象的示例，如图1b的第二行所示。与任何对象一样，毯子很容易出现在不同的图像中使用不同的颜色和纹理。如果没有适当的对象级语义，对应于不同覆盖区域的特征可能会导致截然不同的特征表示。但理想情况下，对应于各种毯子的特性应该映射到相似的特性，即对象级语义。因此，如果没有仔细强加对象级语义，具有不同结构和形状的复杂对象可以很容易地划分为带有错误类标签的多个段，或者与附近不同类标签的段合并。因此，在USS中，必须付出巨大的努力来学习具有强对象级语义的本地特性（例如，补丁级）。<br/>​  我们为USS提供的以对象为中心的表示学习旨在捕获这种对象级的语义。具体来说，我们首先需要一个在以对象为中心的视图中的语义或结构线索。以往的一些研究利用K-means或超像素等聚类方法来获得语义线索[20]，但它们主要关注一般的图像模式，而不是对象的语义或结构表示。在这里，我们提出了EiCue，它通过特征基提供对象的语义和结构线索。具体地说，我们利用从ViT[6,14]得到的投影深度图像特征得到的语义相似度矩阵和图像的颜色亲和矩阵来构造图拉普拉斯算子。相应的特征基捕获了对象[32,61]的底层语义结构，为后续的对象级特征细化步骤提供了软指导。回想一下，对象的精确对象级语义必须在不同图像之间保持一致。我们的以对象为中心的对比学习框架明确地将这些特征强加为一个新的对象级的对比损失。具体来说，基于EiCue中的对象线索，我们为每个对象推导出可学习的原型，从而使图像内部和图像间的对象-特征保持一致性。通过这个全面的学习过程，我们的模型有效地捕获了图像中的固有结构，允许它精确地识别语义上可信的对象表示，这是推进现代基于特征的USS的关键。</p><p><strong>贡献。</strong>我们的主要贡献如下：</p><ul><li>我们提出EiCue，使用一个可学习的图拉普拉斯行列式，以获得对图像中的潜在语义和结构细节的更深刻的理解。</li><li>我们设计了一个以对象为中心的对比学习框架，它利用EiCue的光谱基础来构建鲁棒的对象级特征表示。</li><li>通过一系列全面的实验支持，无监督语义分割证明了我们的鹰在无监督语义分割上取得了最先进的性能。</li></ul><h1 id="方法">3.方法</h1><p>​  当我们开始描述图2中所示的完整管道时，让我们首先介绍基于预训练模型的核心USS框架，如之前的工作[16,49]。</p><figure><img src="../postimages/EAGLE/image-20241008161231496.png"alt="image-20241008161231496" /><figcaption aria-hidden="true">image-20241008161231496</figcaption></figure><p>图2。<strong>EAGLE</strong>的管道。利用拉普拉斯矩阵，它集成了分层投影的图像关键特征和颜色亲和性，该模型利用特征向量聚类来捕获定义为Meicue和M˜˜的对象级透视线索。利用Meicue的提炼知识，我们的模型进一步采用了一个以对象为中心的对比损失，利用投影特征Z和Z˜。由Z和Z˜分配的可学习的原型Φ，作为一个单一的锚，对比正对象和负对象。我们的以对象为中心的对比损失以两种不同的方式计算：内部（Lobj）和内部（Lsc）图像，以确保语义一致性。</p><h2 id="预处理">3.1.预处理</h2><p>​  <strong>未标记的图像。</strong>我们的方法完全建立在一组图像上，没有任何注释，表示为<spanclass="math inline">\(\mathbf{X}=\left\{\mathbf{x}_b\right\}_{b=1}^B\)</span>，其中B是一个小批处理中的训练图像的数量。我们还利用光度增强策略P来获得一个增强图像集<spanclass="math inline">\(\tilde{\mathbf{X}}=\left\{\tilde{\mathbf{x}}_b\right\}_{b=1}^B=P(\mathbf{X})\)</span>。<br/>​  <strong>预训练特征k。</strong>然后，对于每个输入图像<spanclass="math inline">\(\mathbfx_b\)</span>，我们使用自我监督预训练视觉transformer[6]作为图像编码器<spanclass="math inline">\(\mathcal{F}\)</span>获得分层注意关键特征从最后三个块<spanclass="math inline">\(\mathbf{K}_{L-2}=\mathcal{F}_{L-2}\left(\mathbf{x}_b\right)\)</span>，<spanclass="math inline">\(\mathbf{K}_{L-1}=\mathcal{F}_{L-1}\left(\mathbf{x}_b\right)\)</span>，<spanclass="math inline">\(\mathbf{K}_{L}=\mathcal{F}_{L}\left(\mathbf{x}_b\right)\)</span>，其中L−2，L−1，L是第三层，第二到最后层，和最后一层，分别。然后，我们将它们连接到一个单一的注意张量<spanclass="math inline">\(\mathbf{K}=\left[\mathbf{K}_{L-2} ;\mathbf{K}_{L-1} ; \mathbf{K}_L\right] \in \mathbb{R}^{H \times W \timesD_K}\)</span>。同样，我们对增广图像 <spanclass="math inline">\(\mathbf{\tilde{x}}\)</span>应用相同的程序，得到了它的注意张量<spanclass="math inline">\(\tilde{\mathbf{K}} \in \mathbb{R}^{H \times W\timesD_K}\)</span>。<br/>​  <strong>语义特征S。</strong>虽然K包含了一些基于注意机制的对象的结构信息，但由于没有足够的语义信息来进行直接推理。因此，为了进一步细化特征，我们计算了语义特征<spanclass="math inline">\(\mathbf{S}=\mathcal{S}_\theta(\mathbf{K}) \in\mathbb{R}^{H \times W \times D_S}\)</span>和<spanclass="math inline">\(\tilde{\mathbf{S}}=\mathcal{S}_\theta(\tilde{\mathbf{K}})\in \mathbb{R}^{H \times W \times D_S}\)</span>，其中<spanclass="math inline">\(S_\theta: \mathbb{R}^{H \times W \times D_K}\rightarrow \mathbb{R}^{H \times W \timesD_S}\)</span>是一个可学习的非线性分割头。为简洁起见，补丁的总数，记为H×W，将被称为N。<br/>​  <strong>推理。</strong>在推理时间内，给定一幅新图像，其语义特征S成为进一步聚类的基础，采用传统的评估设置，如K-means聚类和线性探测。因此，与之前预先训练的基于特征的USS工作[16,49]一样，训练<spanclass="math inline">\(\mathcal{S}_\theta\)</span>以无监督的方式输出强语义特征S是当代USS框架的基本框架。接下来，我们将在图2中描述管道的其余部分，这与我们对生成强大的对象级语义特征的方法贡献相对应。</p><h2id="通过特征聚合模块进行的eicue">3.2.通过特征聚合模块进行的EiCue</h2><p>​  直觉告诉我们，“语义上可信”的对象级片段是精确捕获对象结构的像素组，即使在复杂的结构方差下也是如此。例如，一个汽车部件必须包含其所有部件，包括挡风玻璃、车门、车轮等。它们都可能以不同的形状和视图出现。然而，如果没有提供对象级语义的像素级注释，这将成为推断具有零对象级结构优先级的底层结构的一个极具挑战性的任务。<br/>​  从这一实现中，我们的模型EAGLE首先基于特征相似度矩阵的特征基，得到一个强大而简单的语义结构线索，即EiCue，如图3所示。</p><figure><img src="../postimages/EAGLE/image-20241008214724018.png"alt="image-20241008214724018" /><figcaption aria-hidden="true">image-20241008214724018</figcaption></figure><p>图3.EiCue生成过程的说明。从输入的图像中，推导出颜色相似度矩阵<spanclass="math inline">\(\mathbf{A}_{color}\)</span>和语义相似度矩阵<spanclass="math inline">\(\mathbf{A}_{seg}\)</span>，并将其组合成拉普拉斯算子<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>。将Lsym的一个特征向量子集<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>聚类产生EiCue。</p><p>​  具体来说，我们使用著名的光谱聚类[7,39,51]来获得无监督的特征表示，以捕获底层的非线性结构，以处理具有复杂模式的数据。这通常只在颜色空间中工作，但可以很容易地扩展到利用由任何特征构造的相似性矩阵。我们观察到，这种光谱方法对于复杂的真实世界的图像特别有用，如图4所示。</p><figure><img src="../postimages/EAGLE/image-20241008214956647.png"alt="image-20241008214956647" /><figcaption aria-hidden="true">image-20241008214956647</figcaption></figure><p>图4.可视化在特征聚合模块中由S得到的特征向量。这些特征向量不仅区分不同的对象，还识别语义相关的区域，突出了EiCue如何有效地捕获对象的语义和边界。</p><p>​  <strong>EiCue结构。</strong>让我们详细描述构建EiCue的过程，如图3所示。总体框架一般遵循一般的谱聚类：从(1)邻接矩阵<spanclass="math inline">\(\mathbf{A}\)</span>中构造(2)拉普拉斯算子<spanclass="math inline">\(\mathbf{L}\)</span>，(3)对<spanclass="math inline">\(\mathbf{L}\)</span>进行特征分解，得到特征基<spanclass="math inline">\(\mathbf{V}\)</span>，利用特征特征进行聚类。我们将在下面描述每个步骤。</p><h3 id="邻接矩阵的构造">3.2.1邻接矩阵的构造</h3><p>​  我们的邻接矩阵由两个组成部分组成：(1)颜色相似度矩阵和(2)语义相似度矩阵。</p><p>（一）颜色相似度矩阵<spanclass="math inline">\(\mathbf{A}_{color}\)</span>：</p><p>（二）语义相似度矩阵<spanclass="math inline">\(\mathbf{A}_{seg}\)</span>：</p><p>（三）邻接矩阵<span class="math inline">\(\mathbf{A}\)</span>：</p><h3 id="特征分解">3.2.2特征分解</h3><p>​  为了构造基于<spanclass="math inline">\(\mathbf{A}\)</span>的EiCue，我们创建了一个拉普拉斯矩阵。形式上，拉普拉斯矩阵表示为<spanclass="math inline">\(\mathbf{L} =\mathbf{D}−\mathbf{A}\)</span>，其中<spanclass="math inline">\(\mathbf{D}\)</span>是<spanclass="math inline">\(\mathbf{A}\)</span>的度量矩阵，定义为<spanclass="math inline">\(\mathbf{D}(i, i)=\sum_{j=1}^N \mathbf{A}(i,j)\)</span>。在我们的方法中，我们利用归一化的拉普拉斯矩阵来增强其聚类能力。将对称归一化拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>定义为<spanclass="math inline">\(\mathbf{L}_{\text {sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}\mathbf{D}^{-\frac{1}{2}}\)</span>。然后，通过对<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>的特征分解，计算得到特征基<spanclass="math inline">\(\mathbf{V} \in \mathbb{R}^{N \timesN}\)</span>，其中每一列对应于一个唯一的特征向量。然后，我们提取与k个最小特征值对应的k个特征向量，并将它们连接到<spanclass="math inline">\(\mathbf{\hat{V}} \in \mathbb{R}^{N \timesN}\)</span>中，其中第i行对应于第i个补丁的k个维特征。</p><h3 id="可微特征聚类">3.2.3可微特征聚类</h3><p>​  在得到特征向量<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>后，我们进行特征向量聚类过程，提取的EiCue表示为<spanclass="math inline">\(\mathcal{M}_{\text {eicue }} \in\mathbb{R}^N\)</span>。为了聚类特征向量，我们利用了一个基于<spanclass="math inline">\(\hat V\)</span>和<spanclass="math inline">\(C\)</span>之间的余弦距离[36]的小批次K-means算法，记为<spanclass="math inline">\(P=\hat VC\)</span>。集群中心<spanclass="math inline">\(\mathbf{C} \in \mathbb{R}^{k \timesC}\)</span>由可学习的参数组成。为了学习C，我们进一步训练了损失定义如下：<span class="math display">\[\mathcal{L}_{\text {eig}}^{\mathrm{x}}=-\frac{1}{N} \sum_{i=1}^N\left(\sum_{c=1}^C \Psi_{i c}\mathbf{P}_{i c}\right),\]</span> ​  其中C表示预定义的类数，<spanclass="math inline">\(\Psi:=\operatorname{softmax}(\mathbf{P})\)</span>，<spanclass="math inline">\(\mathbf P_{ic}\)</span>和<spanclass="math inline">\(\Psi_{ic}\)</span>表示第i个补丁和第C个簇数。我们将同样的过程应用于增广图像<spanclass="math inline">\(\tilde{\mathrm{x}}\)</span>，得到<spanclass="math inline">\(\mathcal{L}_{\text {eig}}^{\tilde{\mathrm{x}}}\)</span>。通过最小化<spanclass="math inline">\(\mathcal{L}_{\text {eig}}=\frac{1}{2}\left(\mathcal{L}_{\text {eig}}^{\mathrm{x}}+\mathcal{L}_{\text {eig}}^{\tilde{\mathrm{x}}}\right)\)</span>，我们可以获得能够实现更有效的聚类的聚类中心。然后我们得到EiCue为<span class="math display">\[\mathcal{M}_{\text {eicue}}(i)=\underset{c}{\operatorname{argmax}}\left(\mathbf{P}_{i c}-\log\left(\sum_{c^{\prime}=1}^C \exp \left(\mathbf{P}_{ic^{\prime}}\right)\right)\right) .\]</span>​  随着聚类质心精度的提高，EiCue促进了基于语义结构的patchi到对应对象的映射。这是一个有意义的线索，可以强调不同对象之间的语义区别，从而增强特征嵌入的辨别能力。</p><p>​  <strong>备注。</strong>虽然与之前使用特征分解的工作[38]相似，但我们的方法的不同之处在于使用可训练的分割头增强特征向量S，而不是它们依赖于静态向量（即K）。我们的方法通过可微特征聚类增强了S的可学习性和适应性，允许图的拉普拉斯语义和对象语义进化。这种EiCue与学习过程的动态集成清楚地将我们的方法与之前的应用区分开来。</p><figure><img src="../postimages/EAGLE/image-20241009104707642.png"alt="image-20241009104707642" /><figcaption aria-hidden="true">image-20241009104707642</figcaption></figure><h2 id="基于eicue的objnce损失">3.3.基于EiCue的ObjNCE损失</h2><p>​  对于一个成功的语义分割任务，不仅要准确地对每个像素的类进行分类，还要聚合对象表示并创建一个反映对象语义表示的分割图。从这个角度来看，在以对象为中心的视角下学习关系在语义分割任务中尤为重要。为了捕捉对象之间的复杂关系，我们的方法结合了一个以对象为中心的对比学习策略，名为ObjNCELoss，由EiCue指导。该策略旨在细化特征嵌入S的鉴别能力，强调不同对象语义之间的区别。在继续之前，我们映射投影特征$^{N D_{Z}} <span class="math inline">\(和\)</span>{} , , ^{N D_{Z}}<span class="math inline">\(，分别使用线性投影头\)</span>{}<em>{}<spanclass="math inline">\(，来自重塑的\)</span> ^{N D</em>{Z}} <spanclass="math inline">\(和\)</span>{} , , ^{N D_{S}} <spanclass="math inline">\(。虽然\)</span>D_S<spanclass="math inline">\(和\)</span>D_Z$的实际尺寸大小保持不变，但为了便于解释，我们使用了不同的符号。</p><h3 id="对象样机">3.3.1对象样机</h3><h3 id="以对象为中心的对比损失">3.3.2以对象为中心的对比损失</h3><h2 id="总目标">3.4.总目标</h2><p>（未完不待续）</p><p>为了构造基于<spanclass="math inline">\(\mathbf{A}\)</span>的EiCue（假设是为了增强聚类效果的操作），我们使用了图的拉普拉斯矩阵，它能够很好地反映数据中的几何结构和局部关联性。以下是详细解释过程：</p><h3id="构造邻接矩阵mathbfa-mathbfa-in-mathbbrn-times-n是从特征变量中提取出来的表征数据中各个样本通常是图像中的补丁或像素之间的相似性这个相似性可以通过各种方式计算比如基于特征的欧氏距离或高斯相似性函数特征变量的大小为b-times-c-times-n其中">1.构造邻接矩阵<span class="math inline">\(\mathbf{A}\)</span><br/><spanclass="math inline">\(\mathbf{A} \in \mathbb{R}^{N \timesN}\)</span>是从特征变量中提取出来的，表征数据中各个样本（通常是图像中的补丁或像素）之间的相似性。这个相似性可以通过各种方式计算，比如基于特征的欧氏距离或高斯相似性函数。特征变量的大小为<spanclass="math inline">\(B \times C \times N\)</span>，其中：</h3><ul><li><span class="math inline">\(B\)</span>是批量大小，</li><li><spanclass="math inline">\(C\)</span>是特征的通道数（即每个特征向量的维度），</li><li><span class="math inline">\(N = H \timesW\)</span>是空间维度的展平结果，通常表示图像补丁或像素的数量。</li></ul><p>在这里，<span class="math inline">\(N = 256 \times 256 =65536\)</span>，表示图像的像素总数。</p><h3id="构造度量矩阵mathbfd-度量矩阵mathbfd是对角矩阵表示邻接矩阵mathbfa中每个节点的度它通过计算每个节点连接到其他节点的总权重来定义">2.构造度量矩阵<spanclass="math inline">\(\mathbf{D}\)</span><br/>度量矩阵<spanclass="math inline">\(\mathbf{D}\)</span>是对角矩阵，表示邻接矩阵<spanclass="math inline">\(\mathbf{A}\)</span>中每个节点的度。它通过计算每个节点连接到其他节点的总权重来定义：</h3><p><span class="math display">\[\mathbf{D}(i,i) = \sum_{j=1}^N\mathbf{A}(i,j)\]</span> 这意味着度矩阵的每个对角元素表示与节点<spanclass="math inline">\(i\)</span>相连的边的总权重。</p><h3 id="构造拉普拉斯矩阵mathbfl-拉普拉斯矩阵mathbfl表示为">3.构造拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}\)</span><br/>拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}\)</span>表示为：</h3><p><span class="math display">\[\mathbf{L} = \mathbf{D} -\mathbf{A}\]</span>这是标准的无归一化拉普拉斯矩阵，它反映了每个节点和其相邻节点之间的差异。拉普拉斯矩阵的特性使得它在图形信号处理中广泛用于捕捉图结构。</p><h3id="归一化拉普拉斯矩阵mathbfl_sym-为了增强聚类效果我们使用对称归一化的拉普拉斯矩阵mathbfl_sym其形式为">4.归一化拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span><br/>为了增强聚类效果，我们使用对称归一化的拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>，其形式为：</h3><p><span class="math display">\[\mathbf{L}_{\text{sym}} =\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} =\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{A}\mathbf{D}^{-\frac{1}{2}}\]</span>对称归一化拉普拉斯矩阵能够更好地处理图的不同节点度分布的问题，使得高度节点和低度节点在拉普拉斯矩阵中得到更加平衡的对待。</p><h3 id="特征分解-对mathbfl_textsym进行特征值分解得到">5.特征分解<br/>对<spanclass="math inline">\(\mathbf{L}_{\text{sym}}\)</span>进行特征值分解，得到：</h3><p><span class="math display">\[\mathbf{L}_{\text{sym}} = \mathbf{V}\mathbf{\Lambda} \mathbf{V}^{\top}\]</span> 其中，<spanclass="math inline">\(\mathbf{V} \in \mathbb{R}^{N \timesN}\)</span>是特征向量矩阵，<spanclass="math inline">\(\mathbf{\Lambda}\)</span>是对角的特征值矩阵。每一列<spanclass="math inline">\(\mathbf{V}_i\)</span>对应于一个特征向量，且与<spanclass="math inline">\(\mathbf{\Lambda}\)</span>中的特征值相对应。</p><h3id="提取特征基-我们提取与k个最小特征值对应的k个特征向量并将它们连接成矩阵mathbfhatv-in-mathbbrn-times-k这意味着">6.提取特征基<br/>我们提取与<spanclass="math inline">\(k\)</span>个最小特征值对应的<spanclass="math inline">\(k\)</span>个特征向量，并将它们连接成矩阵<spanclass="math inline">\(\mathbf{\hat{V}} \in \mathbb{R}^{N \timesk}\)</span>。这意味着：</h3><ul><li>选取<spanclass="math inline">\(k\)</span>个最小的特征值，这些特征值的特征向量能够表示数据的局部流形结构。</li><li>特征向量<span class="math inline">\(\mathbf{V}\)</span>的第<spanclass="math inline">\(i\)</span>行对应于图中第<spanclass="math inline">\(i\)</span>个节点的特征表示（通常是图像中的某个像素或补丁）。</li><li><spanclass="math inline">\(\mathbf{\hat{V}}\)</span>包含每个节点在选定的<spanclass="math inline">\(k\)</span>维特征空间中的表示，这将用于后续的聚类任务。</li></ul><h3id="聚类任务-通过提取的特征mathbfhatv可以使用常见的聚类算法如k-meansdbscan等来对图像中的像素或补丁进行聚类进而用于如语义分割图像分块或其他下游任务">7.聚类任务<br/>通过提取的特征<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>，可以使用常见的聚类算法，如K-means、DBSCAN等，来对图像中的像素或补丁进行聚类，进而用于如语义分割、图像分块或其他下游任务。</h3><h3id="综述-整个过程通过从图像特征生成图的拉普拉斯矩阵经过归一化处理和特征分解从中提取低维流形的特征表示用于增强聚类能力这种方法能够捕捉特征间的局部关联和数据的流形结构非常适合于需要提取数据内部复杂关系的任务如图像分割或聚类">综述<br/>整个过程通过从图像特征生成图的拉普拉斯矩阵，经过归一化处理和特征分解，从中提取低维流形的特征表示，用于增强聚类能力。这种方法能够捕捉特征间的局部关联和数据的流形结构，非常适合于需要提取数据内部复杂关系的任务，如图像分割或聚类。</h3>]]></content>
      
      
      <categories>
          
          <category> 无监督语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering</title>
      <link href="/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/"/>
      <url>/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/</url>
      
        <content type="html"><![CDATA[<p>Unsupervised Learning of Image Segmentation Based on DifferentiableFeature Clustering</p><p>Wonjik Kim∗ , Member, IEEE, Asako Kanezaki∗ , Member, IEEE, andMasayuki Tanaka, Member, IEEE</p><h1 id="摘要">摘要</h1><p>​  本研究研究了卷积神经网络（CNNs）在无监督图像分割中的应用。与监督图像分割类似，所提出的CNN为表示像素所属的簇的像素分配标签。然而，在无监督图像分割中，事先没有指定训练图像或像素的地面真实标签。因此，一旦输入了目标图像，像素标签和特征表示将被共同优化，并通过梯度下降来更新它们的参数。在提出的方法，标签预测和网络参数学习交替迭代满足以下标准：(a)像素相似特征应该分配相同的标签，(b)空间连续像素应该分配相同的标签，和(c)唯一标签的数量应该很大。虽然这些标准是不兼容的，但所提出的方法最小化了相似性损失和空间连续性损失的组合，以找到一个合理的标签分配解决方案，以很好地平衡上述标准。这项研究的贡献有四倍。首先，我们提出了一种新的端到端无监督图像分割网络，它由归一化和一个用于可微聚类的argmax函数组成。其次，我们引入了一个空间连续性损失函数，它减轻了以往工作中固定段边界的限制。第三，我们提出了一种新的分割方法的扩展，在保持效率的情况下，比现有的方法具有更好的准确性。最后，我们介绍了该方法的另一个扩展：使用经过少量参考图像预训练的网络，而不进行隐形图像分割。在几个图像分割的基准数据集上验证了该方法的有效性。</p><h1 id="介绍">1.介绍</h1><p>​  图像分割几十年来一直在计算机视觉研究中引起了人们的关注。图像分割的应用包括目标检测、纹理识别和图像压缩。在监督图像分割中，使用由一对图像对和像素级语义标签组成的集合，如“天空”或“自行车”进行训练。目的是训练一个系统，对图像像素的已知类别的标签进行分类。相比之下，无监督图像分割用于预测更一般的标签，如“前景”和“背景”。后者比前者更具挑战性。此外，将图像分割成任意数量（≥2）是极其困难的。本研究考虑了一个问题，即一个图像在没有任何先前知识的情况下被分割成任意数量的显著的或有意义的区域。<br/>​  一旦得到了像素级的特征表示，就可以通过对特征向量进行聚类来得到图像片段。然而，特征表示的设计仍然是一个挑战。所期望的特征表示在很大程度上取决于目标图像的内容。例如，如果目标是检测斑马作为前景，特征表示应该对黑白垂直条纹做出反应。因此，像素级的特征应该是描述每个像素周围的局部区域的颜色和纹理的。近年来，卷积神经网络（CNNs）已成功地应用于自主驾驶和增强现实游戏等监督学习场景中的语义图像分割。cnn不常用于完全无监督的场景；然而，它们在从图像像素中提取详细特征方面具有很大的潜力，而这对于无监督的图像分割是必要的。在CNN的高特征描述性的驱动下，提出了一种联合学习方法，它可以预测任意图像输入的未知聚类标签，并学习图像像素聚类的最优CNN参数。随后，提取每个簇中的一组图像像素作为一个段。<br/>​  进一步讨论了对于良好的图像分割所必需的聚类标签的特征。与之前关于无监督图像分割[1]，[2]的研究类似，我们假设一个好的图像分割解决方案与人类提供的解决方案很好地匹配。当一个人被要求分割一个图像时，他们很可能会创建一个片段，每个片段对应于单个对象实例的整体或显著部分。对象实例倾向于包含具有相似颜色或纹理图案的大区域。区域颜色或纹理模式进入同一簇是一种合理的图像分割策略。为了将不同的对象实例中的段分开，最好为不同模式的相邻像素分配不同的集群标签。为了便于集群分离，还考虑了一种需要大量唯一集群标签的策略。综上所述，本文介绍了以下三个关于聚类标签的预测标准：<br/>​  （a）具有相似特征的像素应该被分配到相同的标签上。<br/>​  （b）空间上连续的像素应该被分配给相同的标签。<br/>​  （c）唯一的集群标签的数量应该很大。<br/>​  在本文中，我们提出了一种基于cnn的算法，通过联合优化特征提取函数和聚类函数来满足这些条件。本文为了实现CNN的端到端学习，提出了一种利用可微函数预测聚类标签的迭代方法。该代码可以在网上找到<ahref="https://github.com/kanezaki/pytorch-unsupervised-segmentation-tip/">[code]</a>。<br/>​  本研究是之前在2018年[3]国际声学、语音和信号处理会议（ICASSP）上发表的研究成果的延伸。在之前的工作中，准则(b).采用简单的线性迭代聚类[4]进行超像素提取然而，在之前的算法的超像素提取过程中，片段的边界存在固定的局限性。在本研究中，提出了一种空间连续性损失作为减轻上述限制的替代方法。此外，还介绍了基于我们改进的无监督分割方法的两种新应用：利用用户输入的分割和利用利用不同图像的无监督学习获得的网络权值。由于该方法是完全无监督的，因此它根据图像的性质对图像进行分割，而这并不总是与用户的意图相关。作为该方法的范例应用，将涂鸦作为用户输入，并与现有方法的效果进行了比较。随后，该方法迭代获得单个输入图像的分割结果具有较高的计算代价。因此，作为该方法的另一个潜在应用，我们使用了用多个参考图像预先训练的网络权值。一旦使用该算法从多个图像中获得网络权值，固定网络就可以对一个新的看不见的图像进行分割，只要它与参考图像有点相似。并演示了该技术在视频分割任务中的应用。<br/>​  本文的贡献总结如下：</p><ul><li><p>我们提出了一种新的端到端可微的无监督图像分割网络。</p></li><li><p>我们引入了一个空间连续性损失函数，它减轻了我们之前的方法[3]的局限性。</p></li><li><p>我们提出了一种新的分割方法的扩展，在保持效率的同时，比现有的方法具有更好的准确性。</p></li><li><p>我们介绍了该方法的另一个扩展：使用少量的参考图像预先训练的网络，而不重新训练网络。</p></li></ul><h1 id="方法">3. 方法</h1><p>​  所解决的图像分割问题描述如下。为简单起见，让{}表示<spanclass="math inline">\(\{\}^N_{n=1}\)</span>，除非另有说明，其中N表示输入彩色图像<spanclass="math inline">\(\mathcal{I} = \{v_{n} \in\mathbb{R}^{3}\}\)</span>的像素数。设<span class="math inline">\(f :\mathbb{R}^3 \to \mathbb{R}^p\)</span>是一个特征提取函数，<spanclass="math inline">\(\{x_n\in\mathbb{R}^p\}\)</span>是一组图像像素的p维特征向量。聚类标签<spanclass="math inline">\(\{c_n\in\mathbb{Z}\}\)</span>由<spanclass="math inline">\(c_n = g(x_n)\)</span>分配给所有像素，其中<spanclass="math inline">\(g:\mathbb{R}^p\to\mathbb{Z}\)</span>表示映射函数。在这里，g可以是一个赋值函数，它返回最接近<spanclass="math inline">\(x_n\)</span>的簇质心的标签。对于f和g是固定的情况，利用上述公式得到{cn}。相反地，如果f和g是可训练的，而{cn}是指定的（固定的），那么上述方程可以看作是一个标准的监督分类问题。在这种情况下，如果f和g是可微的，则f和g的参数可以通过梯度下降法进行优化。然而，在本研究中，在以完全无监督的方式训练f和g的参数时，我们预测了未知的{cn}。为了实现这一点，我们解决了以下两个子问题：预测固定f和g的最优{cn}和训练固定{cn}的f和g参数。<br/>​  值得注意的是，第一节引入的三个标准是不相容的，永远不能完美地满足。使用经典方法解决这个问题的一个可能的解决方案是：对于标准(a)，对{xn}使用k-means聚类；对于标准(b)，使用到质心的距离执行图切割算法[17]；对于标准(c)，使用非参数方法确定k-means聚类中的k。然而，这些经典的方法只适用于固定的{xn}，因此求解可能是次优的。因此，我们提出了一种基于cnn的算法来解决这个问题。{xn}和{cn}的特征提取函数以满足上述所有条件的方式进行了联合优化。为了实现CNN的端到端学习，提出了一种使用可微函数预测{cn}的迭代方法。提出了一个CNN结构，如图1所示，以及一个损失函数来满足第一节中描述的三个标准。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007102733276.png"alt="image-20241007102733276" /><figcaption aria-hidden="true">image-20241007102733276</figcaption></figure><p>图1。训练CNN的算法的说明。输入图像输入CNN，使用特征提取模块提取深度特征{xn}。随后，一维（1D）卷积层计算了q维聚类空间中特征的响应向量{rn}，其中本图中的q=3。在这里，z1、z2和z3代表了集群空间的三个轴。随后，使用批处理归一化函数在集群空间的轴上对响应向量进行归一化。此外，集群标签{cn}是通过使用argmax函数将集群标识分配给响应向量来确定的。然后将聚类标签作为伪目标来计算特征相似性损失。最后，计算了空间连续性损失和特征相似性损失，并进行了反向传播。</p><p>​  提出的考虑标准(a)和标准(c)的CNN架构的概念在第三节-A中详细介绍。解决标准(a)和(b)的损失函数的概念在第三-b节中提出。使用反向传播训练CNN的细节在第三-C节中描述。</p><h2 id="a.网络架构">A.网络架构</h2><h3 id="对特征相似性的约束">1)对特征相似性的约束</h3><p>​  我们考虑为具有相似特征的像素分配相同的标签的第一个标准。所提出的解决方案是应用一个线性分类器，将每个像素的特征分类为q类。在本研究中，我们假设输入是一个RGB图像<spanclass="math inline">\(\mathcal{I}=\{\boldsymbol{v}_{n}\in{\mathbb{R}^{3}}\}\)</span>，其中每个像素值被归一化到[0,1]。{<spanclass="math inline">\(v_n\)</span>}通过M个卷积分量计算p维特征图{<spanclass="math inline">\(x_n\)</span>}，每个卷积分量由一个二维（2D）卷积、ReLU激活函数和一个批归一化函数组成，其中一批对应于单个输入图像的N个像素。在这里，我们为所有的M个组件设置了p个区域大小为3×3的滤波器。值得注意的是，这些用于特征提取的组件可以被全卷积网络（FCN）[20]等替代方案所取代。随后，通过应用一个线性分类器得到一个响应图{<spanclass="math inline">\(r_n = W_cx_n\)</span>}，其中<spanclass="math inline">\(W_{c} \in \mathbb{R}^{q\timesp}\)</span>。然后将响应映射归一化为<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，使<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>的均值和单位方差为零。标准化过程背后的动机在第三-A2节中描述。最后，通过选择<spanclass="math inline">\(r_n^{\prime}\)</span>中最大的维数，得到每个像素的聚类标签<spanclass="math inline">\(c_n\)</span>。这种分类规则被称为argmax分类。这种处理方法直观地对应于将特征向量聚类成q个聚类。最终响应的第<spanclass="math inline">\(i\)</span>簇<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>可以写成： <spanclass="math display">\[C_{i}=\{r_{n}^{\prime}\in\mathbb{R}^{q}\midr_{n,i}^{\prime}\geq r_{n,j}^{\prime}, \forall j\},\]</span>​  其中，<span class="math inline">\(r_{n,i}^{\prime}\)</span>表示<spanclass="math inline">\(r_n^{\prime}\)</span>的第<spanclass="math inline">\(i\)</span>个元素。这相当于将每个像素分配给q个代表点之间的最近点，这些点被放置在q维空间中各自的轴上的无限距离上。值得注意的是，Ci可以是∅，因此唯一的聚类标签的数量可以任意地从1到q。</p><h3 id="对唯一集群标签数量的约束">2)对唯一集群标签数量的约束</h3><p>​  在无监督的图像分割中，没有线索知道在一个图像中应该生成多少个片段。因此，唯一的集群标签的数量应该自适应图像内容。如在第三-A1节中所述，所提出的策略是将像素划分为任意数量<spanclass="math inline">\(q^{\prime}(1\leq q^{\prime}\leqq)\)</span>的簇，其中<span class="math inline">\(q\)</span>为<spanclass="math inline">\(q^{\prime}\)</span>的最大可能值。一个大的<spanclass="math inline">\(q^{\prime}\)</span>表示过分割，而一个小的<spanclass="math inline">\(q\)</span>表示过分割。为了训练一个神经网络，我们设置了初始的（最大）数量q。然后，在迭代更新过程中，通过考虑特征相似性和空间连续性约束，对相似或空间接近的像素进行集成。这一现象导致唯一的聚类标签<spanclass="math inline">\(q^{\prime}\)</span>的数量减少，即使对<spanclass="math inline">\(q^{\prime}\)</span>没有明确的限制。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007102733276.png"alt="image-20241007102733276" /><figcaption aria-hidden="true">image-20241007102733276</figcaption></figure><p>​  如图1所示，所提出的基于argmax分类的聚类函数对应于<spanclass="math inline">\(q^{\prime}\)</span>-类聚类，其中<spanclass="math inline">\(q^{\prime}\)</span>锚点对应于q轴上无穷远处的q点的一个子集。上述标准(a)和(b)只促进了像素的分组，这可能导致一个简单的解决方案，即<spanclass="math inline">\(q^{\prime}=1\)</span>。为了防止这种分割失败，引入了第三个标准准则，即对大<spanclass="math inline">\(q^{\prime}\)</span>的限制。所提出的解决方案是在使用argmax分类分配聚类标签之前，为响应映射{<spanclass="math inline">\(r_n\)</span>}插入轴内归一化过程。在这里，使用了批处理规范化[46]。这个操作，也称为白化，将原始响应<spanclass="math inline">\(\{r_n\}\)</span>转换为<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，其中每个轴的均值为零且为单位方差。这使得每个<spanclass="math inline">\(r_{n,i}^{\prime}\;(i=1,\cdot\cdot\cdot,q)\)</span>都更有机会成为各轴上的<spanclass="math inline">\(r_n^{\prime}\)</span>的最大值。虽然这个操作并不能保证每个簇索引i（i= 1，...，q）对任何n（n =1，...，N）都达到最大值，但是，由于这个操作，许多簇索引将对任何n（n =1，...，N）达到最大值。因此，这种轴内归一化过程使所提出的系统更倾向于更大的<spanclass="math inline">\(q^{\prime}\)</span>。</p><h2 id="b.损失函数">B.损失函数</h2><p>​  所提出的损失函数L由特征相似性约束和空间连续性约束组成，表示如下：<spanclass="math display">\[L=\underbrace{L_{\mathrm{sim}}(\{r_{n}^{\prime},c_{n}\})}_{\text{featuresimilarity}}+\mu\underbrace{L_{\mathrm{con}}(\{r_{n}^{\prime}\})}_{\text{spatialcontinuity}},\]</span> ​  其中，<spanclass="math inline">\(\mu\)</span>表示平衡这两个约束条件的权重。虽然所提出的方法是一种完全无监督的学习方法，但我们也研究了使用以涂鸦作为用户输入的方法。在使用涂鸦信息进行分割的情况下，损失函数(1)被简单地使用另一个权重ν进行修改如下：<spanclass="math display">\[L=\underbrace{L_{\mathrm{sim}}(\{r_{n}^{\prime},c_{n}\})}_{\mathrm{feature~similarity}}+\mu\underbrace{L_{\mathrm{con}}(\{r_{n}^{\prime}\})}_{\mathrm{spatial~continuity}}+\nu\underbrace{L_{\mathrm{scr}}(\{r_{n}^{\prime},s_{n},u_{n}\})}_{\mathrm{scribble~information}}.\]</span>​  上述功能的每个组成部分将在下面各自的部分中进行描述。</p><h3 id="对特征相似性的约束-1">1)对特征相似性的约束</h3><p>​  如在第3-A1节中所述，将argmax函数应用于归一化响应映射<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，得到聚类标签{cn}。进一步利用聚类标签作为伪目标。在该方法中，计算了<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>和{cn}之间的以下交叉熵损失，作为对特征相似性的约束条件：<spanclass="math display">\[L_{\mathrm{sim}}(\{\boldsymbol{r}_n&#39;,c_n\})=\sum_{n=1}^N\sum_{i=1}^q-\delta(i-c_n)\lnr_{n,i}&#39;,\]</span> ​  其中 <spanclass="math display">\[\delta(t)=\begin{cases}1&amp;\text{if}t=0\\0&amp;\text{otherwise.}\end{cases}\]</span>​  这个损失函数背后的目的是增强相似特征的相似性。一旦根据图像像素的特征进行聚类，同一聚类内的特征向量应该彼此相似，而来自不同聚类的特征向量应该彼此不同。通过最小化该损失函数，更新网络权值，以方便提取更有效的聚类特征。</p><h3 id="在空间连续性上的约束">2)在空间连续性上的约束</h3><p>​  如在第三-A1节中所示，图像像素聚类的基本概念是将相似的像素分组为聚类。然而，在图像分割中，图像像素的聚类最好是空间连续的。此外，还引入了一个额外的约束条件，使集群标签与相邻像素的标签相同。与[47]类似，我们将响应图<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>的水平和垂直差异的L1-范数作为一个空间约束。我们可以用一个微分算子来实现这个过程。更具体地说，空间连续性损失<spanclass="math inline">\(L_{con}\)</span>的定义如下： <spanclass="math display">\[L_{\mathrm{con}}(\{\boldsymbol{r}_n&#39;\})=\sum_{\xi=1}^{W-1}\sum_{\eta=1}^{H-1}\parallel\boldsymbol{r}_{\xi+1,\eta}^{\prime}-\boldsymbol{r}_{\xi,\eta}^{\prime}\parallel_1+\parallel\boldsymbol{r}_{\xi,\eta+1}^{\prime}-\boldsymbol{r}_{\xi,\eta}^{\prime}\parallel_1,\]</span>​  其中，W和H表示输入图像的宽度和高度，而<spanclass="math inline">\(\boldsymbol{r}_{\xi,\eta}^{\prime}\)</span>表示响应映射<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>中在<spanclass="math inline">\((\xi,\eta)\)</span>处的像素值。<br/>​  通过应用空间连续性损失<spanclass="math inline">\(L_{con}\)</span>，可以抑制由于复杂的图案或纹理而产生的过多的标签。</p><h3 id="对脚本作为用户输入的约束">3)对脚本作为用户输入的约束</h3><p>​  基于涂鸦信息的图像分割技术已被广泛研究，[15]，[31]-[33]。在该方法中，引入了涂鸦损失<spanclass="math inline">\(L_{scr}\)</span>作为部分交叉熵如下： <spanclass="math display">\[L_{\mathrm{scr}}(\{\boldsymbol{r}_n&#39;,s_n,u_n\})=\sum_{n=1}^N\sum_{i=1}^q-u_n\delta(i-s_n)\lnr_{n,i}&#39;,\]</span> ​  其中un =1，如果第n个像素是一个涂鸦像素，否则为0，sn表示每个像素的涂鸦标签。</p><h2 id="c.-通过反向传播来学习网络">C. 通过反向传播来学习网络</h2><p>​  在本节中，描述了训练网络进行无监督图像分割的方法。一旦输入目标图像，就解决了以下两个子问题：使用固定网络参数的聚类标签的预测和使用（固定的）预测聚类标签的训练网络参数。前者对应于一个网络的正向过程，以及在在第三-A节中描述的所提出的架构。后者对应于基于梯度下降的网络的逆向过程。随后，我们计算并反向传播在第三-B节中描述的损失L来更新卷积滤波器<spanclass="math inline">\(\{W_m\}^M_{m=1}\)</span>的参数以及分类器Wc的参数。本研究采用动量随机梯度下降法更新参数。参数用Xavier初始化[48]进行初始化，根据输入输出层大小归一化的均匀分布中采样值。这个前后过程迭代T次，得到聚类标签{cn}的最终预测。算法1显示了所提出的无监督图像分割算法的伪代码。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007105933722.png"alt="image-20241007105933722" /><figcaption aria-hidden="true">image-20241007105933722</figcaption></figure><p>​  由于这个迭代过程需要少量的计算时间，我们进一步介绍了使用所提出的方法与一个或多个参考图像。如果目标图像与参考图像有些相似，则可以重用使用这些图像进行预处理时训练的固定网络权值。在第四-C节中研究了参考图像的有效性。<br/>​  如图1所示，所提出的CNN网络由基本功能组成。所提出的CNN最独特的部分是在最终的卷积层和argmax分类层之间存在批处理归一化层。与监督学习场景不同，其中的目标标签是固定的，在轴上的响应的批量归一化是必要的，以获得合理的标签{cn}(见第三-A2节)。此外，与监督学习相比，{cn}有多个具有不同网络参数的解，可以实现接近于零的损失。学习速率的值控制了参数更新和聚类之间的平衡，导致{cn}的解不同。我们将学习率设置为0.1，动量为0.9。</p><h1 id="实验结果">4. 实验结果</h1><p>​  <imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007110308915.png"alt="image-20241007110308915" /></p><p>​  表1 PASCAL VOC2012和BSD500无监督分割MIOU的比较。最好的分数用粗体显示，第二好的分数用下划线表示</p><h1 id="代码结构">5.代码结构</h1><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007143649911.png"alt="image-20241007143649911" /><figcaption aria-hidden="true">image-20241007143649911</figcaption></figure><p>（未完不待续）</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 可微 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficient Deep Embedded Subspace Clustering</title>
      <link href="/Efficient-Deep-Embedded-Subspace-Clustering/"/>
      <url>/Efficient-Deep-Embedded-Subspace-Clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="efficient-deep-embedded-subspace-clustering">Efficient DeepEmbedded Subspace Clustering</h1><p>Jinyu Cai 1,3, Jicong Fan 2,3*, Wenzhong Guo 1, Shiping Wang 1, YunheZhang 1, Zhao Zhang4<br/>1中国福州大学计算机与数据科学学院<br/>2中国香港中文大学（深圳）数据科学学院<br/>3中国深圳大数据研究所<br/>4中国合肥工业大学</p><h1 id="摘要">摘要</h1><p>​  最近，深度学习方法在数据聚类任务方面取得了重大进展。深度聚类方法（包括基于距离的方法和基于子空间的方法）将聚类和特征学习集成到一个统一的框架中，在那里聚类和表示之间存在相互促进。然而，深度子空间聚类方法通常在自表达模型的框架内，具有二次时间和空间复杂性，阻碍了其在大规模聚类和实时聚类中的应用。在本文中，我们提出了一种新的深度聚类机制。我们的目标是以迭代细化的方式从深度表示中学习子空间基，而细化的子空间基则帮助学习深度神经网络的表示。该方法脱离了自表达框架，可以线性地扩展到样本大小，适用于任意大的数据集和在线聚类场景。更重要的是，该方法的聚类精度远远高于其竞争对手。在基准数据集上与最先进的聚类方法进行的广泛比较研究，证明了该方法的优越性。</p><h1 id="引言">1. 引言</h1><p>​  聚类是机器学习中的一个基本问题，它的目的是在没有标签信息的情况下，在高类内相似度和低类间相似度的要求下，将样本分离成类。许多经典的聚类算法如k-means[29]和谱聚类（SC）[30]在实际应用中取得了巨大的成功。然而，它们在处理具有复杂结构或/和高维性的数据时并不有效，这可以通过使用数据的细化特征来改进。事实上，[14,37,38,47]之前的一些工作利用了特征学习技术，如非负矩阵分解[2]、自动编码器（AE）[1]及其变体[24,31,36]来学习低维嵌入进行聚类，从而提高了聚类的精度。然而，由于这些方法是两阶段聚类，并且特征学习不是针对聚类的，因此不能保证学习到的表示适合于聚类。<br/>​  近年来，一些研究人员提出了端到端聚类方法，如深度嵌入式聚类（DEC）[40]、联合无监督学习（JULE）[41]、深度自适应聚类（DAC）[6]和深度综合相关挖掘（DCCM）[39]。在这些方法中，聚类目标与网络优化过程相结合，这提供了一种学习面向聚类的嵌入式表示的方法。然而，大多数深度聚类方法使用基于欧几里德距离的度量来识别聚类，而欧几里德距离对于不同类型的数据结构并不总是有效或合理的。<br/>​  子空间聚类假设数据位于不同的子空间[11]中。稀疏子空间聚类（SSC聚类）[11]和低秩表示（LRR）[27]主要基于光谱聚类[30]，优于k均值聚类和经典光谱聚类。最近，一些研究人员[8,21,25,44]表明，联合子空间聚类和深度学习在基准数据集上有很好的性能。然而，这些方法很难扩展到大规模的数据集，因为它们需要学习一个自我表达的矩阵，从而导致二次时间和空间的复杂度。因此，[12,48,49]的一些最新工作致力于提高子空间聚类的效率。<br/>​  在本文中，我们旨在提供一种高效和精确的深子空间聚类的方法。我们提出从深度自动编码器提取的潜在特征中学习一组子空间基，其中的基和网络参数被迭代细化。我们提出从深度自动编码器提取的潜在特征中学习一组子空间基，其中的基和网络参数被迭代细化。该方法的网络结构如图1所示。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004113308304.png"alt="image-20241004113308304" /><figcaption aria-hidden="true">image-20241004113308304</figcaption></figure><p>图1.提出方法的说明。利用自动编码器网络学习输入数据的嵌入式表示Z，然后Z与子空间D结合构造子空间相似度向量，得到归一化的子空间相似度<spanclass="math inline">\(S\)</span>。然后，从<spanclass="math inline">\(S\)</span>计算细化的子空间相似度<spanclass="math inline">\(\widetilde{S}\)</span>，提供自监督信息。注意，<spanclass="math inline">\(d\)</span>是子空间的维数，<spanclass="math inline">\(L_{Recon}\)</span>和<spanclass="math inline">\(L_{Sub}\)</span>表示重构损失和<spanclass="math inline">\(\widetilde{S}\)</span>与<spanclass="math inline">\(S\)</span>之间的差异，并通过联合优化它们来训练网络。</p><p>​  我们的贡献如下：</p><ul><li>我们提出了一种新的脱离传统的自我表达框架的深度子空间聚类方法</li><li>该方法具有线性的时间和空间复杂度，因此适用于大规模的子空间聚类。</li><li>我们将该方法推广到在线聚类，从而可以有效地处理任意大的数据集和流数据集。</li><li>分析了利用深度神经网络转换基于距离的聚类和子空间聚类的可行性。</li></ul><p>​  在许多基准数据集（如Fashion-MNIST、STL-10和REUTERS-10k）上的数值结果表明，我们的方法比其竞争对手更有效。</p><h1 id="相关工作及简要讨论">2. 相关工作及简要讨论</h1><h2 id="深度聚类">2.1 深度聚类</h2><p>​  早期的深度聚类方法旨在应用深度特征学习方法（如自动编码器[36]和变分自动编码器（VAE）[24]）从复杂的高维数据中提取特征进行聚类。然而，这些解决方案很难学习适合于聚类任务的表示。目前的深度聚类方法侧重于构建端到端模型。Xie等人提出了DEC[40]，设计了一个基于t-SNE[35]设计的聚类目标。它提供了一个实现聚类中心和嵌入式特征同步优化的聚类模型。Chang等人[5]提出了深度自我进化聚类（DSEC），这是一种基于自我进化的算法，用所选择的模式对交替训练网络。在[39]中，Wu等人提出了一种名为DCCM的方法，该方法使用伪标签进行自我监督，并使用互信息来捕获更多有区别的表示用于聚类。由Huang等人[20]提出的分区置信度最大化（PICA）使分区不确定性指数最小化，并学习了最有信心的聚类分配。请注意，这些深度聚类方法使用欧氏距离来分配聚类，当聚类不集中于平均值时，这可能没有用处。</p><h2 id="子空间聚类">2.2 子空间聚类</h2><p>​  经典的子空间聚类，如SSC [11]，LRR [27]，Kernel-SSC[32]，旨在学习光谱聚类的自表达相似度矩阵。Ji等人[21]提出了深度子空间聚类网络（DSC-Net），该网络包含了一个具有自编码器网络的自表达模块。与SSC和LRR相比，DSC-Net在多个图像数据集上都有了显著的改进。Zhou等人[52]提供了一种称为深度对抗性子空间聚类（DASC）的方法，该方法利用生成的对抗性网络[16]提供对抗性学习，提高了深度子空间聚类的性能。Zhou等人[51]提出了分布保留的子空间聚类（DPSC）来保留子空间中的潜在分布，以提高子空间聚类模型的特征学习能力。另一方面，一些研究者试图降低子空间聚类[7,12,13,33,49]的复杂性。例如，Zhang等[49]提出了k-子空间聚类网络（k-SCN），将子空间的更新整合到嵌入式空间的学习中，以解决学习相似度矩阵的缺点。Fan[12]提出了一种k-分解子空间聚类（k-FSC）方法，该方法具有线性的时间和空间复杂度，能够处理缺失数据和流数据。</p><h2 id="简短的讨论">2.3 简短的讨论</h2><p>​  我们分析了表1中经典子空间聚类、大规模子空间聚类和深度子空间聚类等几种（由于空间限制）方法的时空复杂性。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004115219866.png"alt="image-20241004115219866" /><figcaption aria-hidden="true">image-20241004115219866</figcaption></figure><p>表1.该方法与一些深度聚类和子空间聚类方法在聚类n个样本的维度为m中的时间复杂度和空间复杂度比较。为了节省空间，我们在补充材料中加入了对参数的解释。</p><p>​  我们可以看到，这些经典的子空间聚类方法和深度子空间聚类方法在样本数量上具有二次时间和空间的复杂性。相比之下，我们的方法具有线性的时间和空间复杂度，可与[12]的大规模子空间聚类方法相媲美。</p><h1 id="方法">3. 方法</h1><h2 id="提出的模-型">3.1 提出的模 型</h2><p>​  在本文中，我们主要针对基于深度学习的子空间聚类，并试图解决以下问题。</p><p><strong>问题1</strong></p><p>​  给定一个数据矩阵<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{m\timesn}\)</span>，其中<span class="math inline">\(m\)</span>表示特征数，<spanclass="math inline">\(n\)</span>表示样本数。假设<spanclass="math inline">\(\mathrm{X=\bar{X}P}\)</span>，其中<spanclass="math inline">\(\bar{X}=[\bar{\mathbf{X}}^{(1)},\bar{\mathbf{X}}^{(2)},\ldots,\bar{\mathbf{X}}^{(k)}]\)</span>和<spanclass="math inline">\(\mathbf{P}\in\mathbb{R}^{n\timesn}\)</span>是一个未知排列的矩阵。对于<spanclass="math inline">\(j=1,\ldots,k\)</span>，假设<spanclass="math inline">\(\bar{\mathbf{X}}^{(j)}\in\mathbb{R}^{m\timesn_j}\)</span>的列是由以下得来： <spanclass="math display">\[\mathbf{x}=\mathbf{f}_j(\mathbf{v})+\varepsilon,\]</span>​  其中<spanclass="math inline">\(\mathbf{f}_j:\mathbb{R}^{r_k}\longrightarrow\mathbb{R}^m\)</span>是一个未知的非线性函数，<spanclass="math inline">\(r_{j}&lt;m\)</span>，<spanclass="math inline">\(\mathbf{v}\in\mathbb{R}^{r_{j}}\)</span>是一个随机变量，<spanclass="math inline">\(\varepsilon\in\mathbb{R}^{m}\)</span>表示随机高斯噪声。从<spanclass="math inline">\(\mathrm{X}\)</span>中找到排列矩阵<spanclass="math inline">\(\mathrm{P}\)</span>（或等价的<spanclass="math inline">\(\bar{X}\)</span>）。  这个问题正是一个聚类问题，为此我们需要将<spanclass="math inline">\(\mathrm{X}\)</span>的列分组成k个聚类，对应于k个不同的函数<spanclass="math inline">\(\mathbf{f}_{1},\ldots,\mathbf{f}_{k}\)</span>。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004121536128.png"alt="image-20241004121536128" /><figcaption aria-hidden="true">image-20241004121536128</figcaption></figure><p>​  图2显示了<span class="math inline">\(m=1\)</span>和<spanclass="math inline">\(r_{1}=\cdots=r_{5}=1\)</span>的一个简单例子。注意，当<spanclass="math inline">\(\mathbf{f}_{1},\ldots,\mathbf{f}_{k}\)</span>都是线性的，这个问题简化为经典的子空间聚类。<br/><strong>问题2</strong></p><p>​  在问题1中，对于<spanclass="math inline">\(j=1,\ldots,k\)</span>，假设<spanclass="math inline">\(\mathbf{f}_{j}(\mathbf{v})=\mathbf{g}(\mathbf{B}^{(j)}\mathbf{v})\)</span>，其中<spanclass="math inline">\(\mathbf{B}^{(j)}\in\mathbb{R}^{p\timesr_j}\)</span>和<spanclass="math inline">\(\mathbf{g}:\mathbb{R}^p\longrightarrow\mathbb{R}^m\)</span>。此外，<spanclass="math inline">\(\frac{\|\mathbf{B}^{(i)^{\top}}\mathbf{B}^{(j)}\|_{F}}{\|\mathbf{B}^{(i)}\|_{F}\|\mathbf{B}^{( j )}\|_{F}}(i\neq j)\)</span>足够小。从<spanclass="math inline">\(\mathrm{X}\)</span>中找到排列矩阵<spanclass="math inline">\(\mathrm{P}\)</span>（或等价的<spanclass="math inline">\(\bar{X}\)</span>）。</p><p>​  问题2比问题1更容易，因为当我们获得<spanclass="math inline">\(\{\mathbf{B}^{(1)},\ldots,\mathbf{B}^{(k)}\}\)</span>的良好估计时，就识别出正确的集群足够了。因此，在本文中，首先，我们提出通过用多层神经网络近似<spanclass="math inline">\(x\)</span>来估计<spanclass="math inline">\(B^{(j)}\)</span>，即： <spanclass="math display">\[\mathbf{x}_i\approxh_\mathcal{W}(\hat{\mathbf{B}}^{(j)}\hat{\mathbf{v}}_i),\mathbf{x}_i\in\mathbb{C}_j,\]</span> ​  其中，<spanclass="math inline">\(h_\mathcal{W}\)</span>为参数集为<spanclass="math inline">\(\mathcal{W}\)</span>的多层神经网络，<spanclass="math inline">\(\mathbb{C}_j\)</span>为第<spanclass="math inline">\(j\)</span>个聚类。很难直接获得<spanclass="math inline">\(\{\hat{\mathbf{B}}^{(1),}\ldots,\hat{\mathbf{B}}^{(k)}\}\)</span>。相反，我们估计了<spanclass="math inline">\(\mathbf{B}^{(j)}\mathbf{V}_i\)</span>，即<spanclass="math inline">\(\mathbf{z}_{i}:=\hat{\mathbf{B}}^{(j)}\hat{\mathbf{v}}_{i}\)</span>。因此，我们提出优化去解决：<spanclass="math display">\[\begin{aligned}\operatorname*{minimize}_{\mathcal{W},\{\mathbf{z}_{1},\ldots,\mathbf{z}_{n}\}}&amp;\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i}-h_{\mathcal{W}}(\mathbf{z}_{i})\|^{2},\\\mathrm{subject~to~}&amp;\mathbf{z}_{i}\in\mathbb{S}_{i},i=1,\ldots,n,\end{aligned}\]</span> ​  其中，<spanclass="math inline">\(\mathbb{S}_{i}\)</span>表示真正的簇<spanclass="math inline">\(\mathbf{x}_{i}\)</span>应该属于的簇。然而，不可能直接解决(3)，因为<spanclass="math inline">\(\mathbb{S}_{i}\)</span>是未知的。现在我们引入了一个新的变量<spanclass="math inline">\(\mathbf{D}=[\mathbf{D}^{(1)},\mathbf{D}^{(2)},\ldots,\mathbf{D}^{(k)}]\)</span>。它包含<spanclass="math inline">\(k\)</span>个块和<spanclass="math inline">\(\mathbf{D}^{(j)}\in \mathbb{R}^{p\timesd}\)</span>，$ |_{u}<sup>{(j)}|=1<spanclass="math inline">\(，\)</span>u=1,,d<spanclass="math inline">\(，\)</span>j=1,,k<spanclass="math inline">\(。注意，对于所有的\)</span>j=1,,k<spanclass="math inline">\(，\)</span>dr_{j}<spanclass="math inline">\(。我们希望\)</span></sup>{(j)}<spanclass="math inline">\(与\)</span><sup>{(j)}<spanclass="math inline">\(，\)</span>j=1,,k<spanclass="math inline">\(有相同的列空间。然后根据我们在问题2中所做的假设，对于所有的\)</span>jl<spanclass="math inline">\(，\)</span>|</sup>{(j)<sup>}</sup>{(l)}|_F<spanclass="math inline">\(都足够小，即：\)</span><spanclass="math inline">\(\|\mathbf{D}^{(j)^\top}\mathbf{D}^{(l)}\|_F\leq\tau,j\neq l,\)</span>$ ​  其中，<spanclass="math inline">\(\tau\)</span>是一个很小的常数。<br/>​  表示<spanclass="math inline">\(\alpha_{i}=\mathrm{arg~max}_{j}\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(j)}\|\)</span>。我们期待：<spanclass="math display">\[\|\mathbf{z}_i^\top\mathbf{D}^{(\alpha_i)}\|\gg\max_{j\neq\alpha_i}\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\|,i=1,\ldots,n.\]</span> ​  换句话说，<spanclass="math inline">\(\mathbf{z}_{i}\)</span>只与<spanclass="math inline">\(\mathbf{D}\)</span>的一个块高度相关。现在我们使用一个参数集为<spanclass="math inline">\(\mathcal{W}^{\prime}\)</span>的编码器<spanclass="math inline">\(h_{\mathcal{W}^{\prime}}^{\prime}\)</span>来表示<spanclass="math inline">\(\mathbf{z}_{i}\)</span>，即： <spanclass="math display">\[\mathbf{z}_i=h&#39;_{\mathcal{W}&#39;}(\mathbf{x}_i),i=1,\ldots,n.\]</span> ​  为了方便起见，我们让： <spanclass="math display">\[\hat{\mathbf{x}}_i:=h_{\mathcal{W}}(\mathbf{z}_i),i=1,\ldots,n.\]</span> ​  和表示<spanclass="math inline">\(\hat{\mathbf{X}} =[\hat{\mathbf{x}}_{1},\ldots,\hat{\mathbf{x}}_{n}]\)</span>。现在，我们把(3)、(4)、(5)、(6)和(7)一起解决<spanclass="math display">\[\begin{alignat}{2}&amp;\operatorname*{minimize}_{\mathcal{W},\mathcal{W}^{\prime},\mathbf{D}}&amp; \quad &amp;\frac{1}{2n}\left\|\mathbf{X}-\hat{\mathbf{X}}\right\|_{F}^{2},\\&amp;\text{subject to} &amp; &amp; \|\mathbf{D}_{u}^{(j)}\|=1, \;u=1,\ldots,d, \; j=1,\ldots,k, \nonumber \\&amp; &amp; &amp;\|\mathbf{D}^{(j)} \mathbf{D}^{(l)}\|_{F}\leq\tau, \; j\neq l, \nonumber\\&amp; &amp; &amp;\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(\alpha_{i})}\|\gg\max_{j\neq\alpha_{i}}\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(j)}\|,\; i=1,\ldots,n. \nonumber\end{alignat}\]</span> ​  注意，在(8)中，<spanclass="math inline">\(\mathbf{z}_{i}\)</span>只是根据(6)中的中间变量，我们不需要显式地优化它们。在(8)中，第一个约束是控制<spanclass="math inline">\(\mathbf{D}\)</span>的列的大小，否则<spanclass="math inline">\(\left\|\mathbf{z}_i^\top\mathbf{D}^{(\alpha_i)}\right\|\)</span>可能变为零。第二个约束是为了符合问题1中不同子空间之间不相似的假设。最后一个约束起着子空间分配的作用。请注意，我们的方法(8)仍然适用于问题1，前提是神经网络能够学习一个<spanclass="math inline">\(\mathbf{g}(\mathbf{B}^{(j)}\mathbf{v})\)</span>来有效地近似问题1中的<spanclass="math inline">\(\mathbf{f}_j(\mathbf{v})\)</span>。</p><h2 id="实用的解决方案">3.2 实用的解决方案</h2><p>​  现在我们将展示如何近似地求解(8)。为了方便起见，我们让 <spanclass="math display">\[L_{Recon}=\frac{1}{2n}\sum_{i=1}^{n}\left\|\mathbf{x}_{i}-\mathbf{\hat{x}}_{i}\right\|_{F}^{2}.\]</span>​  我们通过最小化以下目标，在(8)中施加第一个约束 <spanclass="math display">\[D_{Cons1}:=\frac12\left\|\mathbf{D}^\top\mathbf{D}\odot\mathbf{I}-\mathbf{I}\right\|_F^2,\]</span>​  其中<span class="math inline">\(\odot\)</span>表示阿达玛积，<spanclass="math inline">\(\mathbf{I}\)</span>是大小为<spanclass="math inline">\(kd\)</span>的<spanclass="math inline">\(kd\)</span>的单位矩阵。<br/>​  对于(8)中的第二个约束，我们进行了定义<span class="math display">\[\begin{gathered}D_{Cons2}:=\begin{aligned}\frac{1}{2}\left\|\mathbf{D}^{(j)\top}\mathbf{D}^{(l)}\right\|_{F}^{2},j\neq l,\end{aligned} \\\text{=}\frac12\left\|\mathbf{D}^\top\mathbf{D}\odot\mathbf{O}\right\|_F^2.\end{gathered}\]</span> ​  这里的<spanclass="math inline">\(\mathbf{O}\)</span>是一个矩阵，其中所有<spanclass="math inline">\(d\)</span>大小的对角线块元素都是0，其他所有元素都是1。现在我们可以把（10）和（11）放在一起，得到<spanclass="math inline">\(\mathbf{D}\)</span>上的正则化项 <spanclass="math display">\[D_{Cons}=\xi(D_{Cons1}+D_{Cons2}),\]</span>​  其中，<spanclass="math inline">\(\xi\)</span>是一个在本工作中固定在<spanclass="math inline">\(10^{−3}\)</span>的调优参数。<br/>​  对于(8)中的最后一个约束，我们提出了一种新的子空间相似度矩阵<spanclass="math inline">\(S\)</span>来度量嵌入表示<spanclass="math inline">\(\mathbf{Z}\)</span>和子空间基代理<spanclass="math inline">\(\mathbf{D}\)</span>之间的关系 <spanclass="math display">\[s_{ij}=\frac{\left\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\right\|_F^2+\etad}{\sum_j(\left\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\right\|_F^2+\etad)},\]</span> ​  其中，<spanclass="math inline">\(\eta\)</span>是一个控制平滑度的参数。因此，<spanclass="math inline">\(s_{ij}\)</span>表示嵌入式表示<spanclass="math inline">\(\mathbf{z}_{i}\)</span>属于由<spanclass="math inline">\(\mathbf{D}^{(j)}\)</span>表示的第<spanclass="math inline">\(j\)</span>个子空间的概率。我们进一步引入了一个由定义的细化子空间相似度矩阵<spanclass="math inline">\(\widetilde{S}\)</span>： <spanclass="math display">\[\widetilde{s}_{ij}=\frac{s_{ij}^2/\sum_is_{ij}}{\sum_j(s_{ij}^2/\sum_is_{ij})}.\]</span>​  换句话说，<spanclass="math inline">\(\widetilde{S}\)</span>可以作为一种自监督信息，从而产生以下子空间聚类目标：<span class="math display">\[L_{Sub}=KL(\widetilde{S} ||S)=\sum_{i}\sum_{j}\widetilde{s}_{ij}\log\frac{\widetilde{s}_{ij}}{s_{ij}}.\]</span>​  现在我们将(8)的无约束松弛定义为 <spanclass="math display">\[L=L_{Recon}+D_{Cons}+\beta L_{Sub}.\]</span>​  在算法1中给出了该方法的训练流程。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004144035875.png"alt="image-20241004144035875" /><figcaption aria-hidden="true">image-20241004144035875</figcaption></figure><p>​  该方法实现了子空间聚类和嵌入式表示学习的联合优化。<spanclass="math inline">\(\mathbf{D}\)</span>的初始化由预训练模型的<spanclass="math inline">\(\mathbf{Z}\)</span>上的k-means生成的簇的列空间给出。当对网络的训练完成后，最终的聚类结果可以通过一下得到：<spanclass="math display">\[\mathcal{C}_i=\arg\max_js_{ij}.\]</span></p><h2 id="通用的近似和转换问题">3.3 通用的近似和转换问题</h2><p>​  有人可能会认为，神经网络具有普遍的逼近能力，因此可以将子空间聚类问题转换为基于距离的聚类问题，从而应用k-means和DEC[40]，或者可以将基于距离的聚类问题转换为子空间聚类问题。在这里，我们生成了两个合成数据集来显示转换的执行方式。第一个数据集是用于基于距离的聚类，并且<spanclass="math display">\[\mathbf{x}_{i}^{(j)}\sim\mathcal{N}(\mu_{j},\mathbf{I}),i=1,\ldots,1000,\\\mu_{j}\in\mathbb{R}^{m},\mu_{j}\sim\mathcal{U}(-1,1),\]</span> ​  然后是<spanclass="math inline">\(\mathbf{x}_i^{(j)}\longleftarrow\sin(\mathbf{x}_i^{(j)})\)</span>，其中<spanclass="math inline">\(m = 100\)</span>和<spanclass="math inline">\(j=1,\ldots,10\)</span>。第二个数据集是用于子空间聚类的，并且<spanclass="math display">\[\mathbf{x}_{i}^{(j)}=\sin(\mathbf{B}^{(j)}\mathbf{v}_{i}),\mathbf{v}_{i}\sim\mathcal{N}(\mathbf{0},\mathbf{I}),i=1,\ldots,1000,\\\mathbf{B}^{(j)}\in\mathbb{R}^{m\times p},\mathbf{B}^{(j)}\sim\mathcal{N}(\mathbf{0},\mathbf{I}),\]</span>​  其中，<span class="math inline">\(m = 100\)</span>，<spanclass="math inline">\(p = 2\)</span>，和<spanclass="math inline">\(j=1,\ldots,10\)</span>。我们还向数据集上添加了高斯噪声，即<spanclass="math inline">\(\mathrm{X}\leftarrow\mathrm{X}+\mathrm{N}\)</span>，其中噪声的标准误差是干净<spanclass="math inline">\(\mathrm{X}\)</span>的标准误差的0.2倍。<br/>​  DEC[40]、IDEC [18]和我们的方法的性能如图3所示。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004155005104.png"alt="image-20241004155005104" /><figcaption aria-hidden="true">image-20241004155005104</figcaption></figure><p>​  第一个图表明，将基于距离的聚类问题转换为子空间聚类问题相对容易，因为我们的方法的精度相当高。第二个图表明，DEC和IDEC失败，将子空间聚类问题转换为基于距离的聚类问题非常困难。一个可能的原因是，将欧几里德距离（例如<spanclass="math inline">\(\|\mathbf{u}_1-\mathbf{u}_2\|\)</span>）转换为子空间相似度（例如<spanclass="math inline">\(\mathbf{v}_{1}^\top\mathbf{v}_{2}\)</span>）更容易。例如，设<spanclass="math inline">\(\mathbf{v}_1=\phi(\mathbf{u}_1)\)</span>和<spanclass="math inline">\(\mathbf v_{2}=\phi(\mathbfu_{2})\)</span>，其中<spanclass="math inline">\(\phi\)</span>是径向基函数的特征图，例如高斯核。然后我们有<spanclass="math display">\[\mathbf{v}_1^\top\mathbf{v}_2=\exp(-\gamma\|\mathbf{u}_1-\mathbf{u}_2\|^2),\]</span>​  其中，<spanclass="math inline">\(\gamma&gt;0\)</span>是一个超参数。因此，神经网络只需要学习一个<spanclass="math inline">\(\phi\)</span>的近似值，这并不困难。如果我们交换u和v的角色，网络需要学习一个函数<spanclass="math inline">\(h\)</span>，这样<spanclass="math inline">\(\|h(\mathbf{v}_1)-h(\mathbf{v}_2)\|\)</span>是<spanclass="math inline">\(\|\mathbf{v}_1^\top\mathbf{v}_2\|\)</span>的单调（大致）变换，这是相当困难的。<br/>​  以上结果和分析验证表明，有必要提供一种有效、准确的深度子空间聚类方法来更普遍地处理问题2或问题1。</p><h1 id="实验">4. 实验</h1><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004155925751.png"alt="image-20241004155925751" /><figcaption aria-hidden="true">image-20241004155925751</figcaption></figure><p>#5. 代码部分具象图</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241006192123611.png"alt="image-20241006192123611" /><figcaption aria-hidden="true">image-20241006192123611</figcaption></figure><p>（未完不待续）</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> 可微 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自编码器网络合集</title>
      <link href="/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/"/>
      <url>/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="自编码器autoencoder">1. 自编码器（autoencoder）</h1><p>​  自编码器（autoencoder）内部有一个隐藏层h，可以产生编码（code）表示输入。<br/>​  该网络可以看作由两部分组成：</p><ul><li>由函数<span class="math inline">\(h = f ( x)\)</span>表示的编码器</li><li>生成重构的解码器<span class="math inline">\(r =g(h)\)</span>，整体结构如下图所示：</li></ul><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192041921.png"alt="image-20241003192041921" /><figcaption aria-hidden="true">image-20241003192041921</figcaption></figure><h2 id="自编码器的一些基本概念">自编码器的一些基本概念</h2><ol type="1"><li>欠完备自编码器：h维度&lt;x维度。</li></ol><ul><li>学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。“学习欠完备的表示”意味着编码器被设计成只能生成比输入数据更简单、更压缩的表示。举一个简单的例子：</li><li>假设你有一堆猫和狗的图片。一个没有任何限制的自编码器可能会尝试记住每张图片的所有细节（毛色、背景等等）。但是，如果我们限制自编码器只能生成非常简化的表示，它就会被迫关注猫和狗最明显的特征，比如猫的尖耳朵和狗的长尾巴，而忽略背景和毛色等次要特征。</li></ul><ol start="2" type="1"><li>过完备自编码器：h维度&gt;=x维度。</li></ol><ul><li><p>过完备则与欠完备相反。“过完备”意味着编码器的表示空间非常大，能够容纳甚至超过输入数据中的所有信息。举一个简单的例子：</p></li><li><p>假设你有一堆猫和狗的图片。一个过完备自编码器可能会记住每张图片的所有细节，包括背景、毛色、姿态等等。这使得它在训练数据上表现非常好，但在遇到新的猫狗图片时，可能无法很好地识别出它们的共同特征。</p></li><li><p>在过完备的情况下，可能会出现编码器无法学习到有效信息的情况。这是因为：当编码维数与输入维数相等或更大时，自编码器的表示空间非常大，足以容纳输入数据中的所有信息。在这种情况下，甚至简单的线性编码器和解码器也可以直接将输入复制到输出，而不需要提取任何有用的特征。这意味着自编码器没有被迫去学习数据的分布特征，因为它可以简单地记住所有的数据。这种记忆机制使得模型在训练数据上表现很好，但在面对新数据时可能表现很差，因为它没有学到数据的内在模式或结构。</p></li></ul><h3 id="传统自编码器ae">1 传统自编码器（AE）</h3><p>​  AE的网络结构如下：包含三层——输入层、隐藏层、输出层，每一层都是由若干个神经元组成的。</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192700571.png"alt="image-20241003192700571" /><figcaption aria-hidden="true">image-20241003192700571</figcaption></figure><p>​  编码：<spanclass="math inline">\(h=f(x)=f(W_1X+b_1)\)</span><br/>​  解码：<spanclass="math inline">\(X_d=g(x)=g(W_2h+b_2)\)</span><br/>​  损失函数：<spanclass="math inline">\(J_{AE}\left(\theta\right)=J(X,X^d)=-\sum_{i=1}^n(x_i\log(x_i^d)+(1-x_i)\log(1-x_i^d))\)</span><br/>​  采用梯度下降法即可进行训练。此外，为了控制权重降低的程度，防止自编码器的过拟合，将在上述损失函数中加入正则化项(也称重量衰减项)，变为正则化自编码器：<spanclass="math inline">\(J_{\mathrm{ReAE}}(\theta)=J(X,X^d)+\lambda\parallelW\parallel_2^2\)</span></p><h3 id="去噪自编码器dae">2 去噪自编码器（DAE）</h3><p>​  DAE的网络结构如下，AE的目的是求h，但它没有使用h的真实值来训练，所以是无监督的。而DAE的目的是使得网络能够进行去噪，目的是求X，但它用到了X真实值做loss，所以他是监督学习。</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003193316595.png"alt="image-20241003193316595" /><figcaption aria-hidden="true">image-20241003193316595</figcaption></figure><p>​  DAE的动机是主动给X加噪，使得网络带有去噪的能力。但是在每次网络训练之前，人为地在干净的输入信号中加入噪声，增加了模型的处理时间。而且，如果加入过多的噪声，会导致输入样本的严重失真，从而降低算法的性能。</p><h3 id="稀疏自编码器sae">3 稀疏自编码器（SAE）</h3><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192700571.png"alt="image-20241003192700571" /><figcaption aria-hidden="true">image-20241003192700571</figcaption></figure><p>​  稀疏自编码器利用了X的先验信息，这个先验信息就是X的稀疏度。它的网络结构和AE没有什么区别，但是损失函数变了，添加了一项KL散度，是编码后h的稀疏度和真实稀疏度之间的散度。<spanclass="math inline">\(J_{SAE}(\theta)=J(X,X^d)+\beta\sum_{j=1}^tKL(\rho\parallel\hat{\rho}_j)\)</span>其中<spanclass="math inline">\(\beta\)</span>是控制稀疏惩罚的系数，为0~1。<br/>​  首先定义每个隐藏单元jjj的平均激活值<span class="math inline">\(\hat{\rho}_j\)</span> : <spanclass="math inline">\(\hat{\rho}_j=\frac1n\sum_{i=1}^nh_j\left(x_i\right)\)</span><br/>​  其中，n是训练样本数量，<spanclass="math inline">\(h_j{(x_i)}\)</span>第i个样本对于隐藏单元j的激活值。<br/>​  然后定义目标稀疏度<spanclass="math inline">\(\rho\)</span>,这是希望隐藏单元的平均激活值。例如，如果<spanclass="math inline">\(\rho\)</span>较小(如0.05),则希望大多数隐藏单元在任何给定时间都不活跃。<br/>​  最后，将稀疏惩罚项加入到损失函数中，使用KL散度来衡量目标稀疏度和实际稀疏度之间的差异：<spanclass="math inline">\(\mathrm{KL}(\rho||\hat{\rho}_j)=\rho\log\frac{\rho}{\hat{\rho}_j}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}\)</span><br/>​  稀疏惩罚项的总和是所有隐藏单元的KL散度之和：$_{j=1}^{t}(||)<spanclass="math inline">\(&lt;br/&gt;​&amp;emsp;&amp;emsp;KL散度是描述两个分布之间差异的指标，KL散度越小，分布越接近，具体公式如下：\)</span>KL(_j)=+(1-)$</p><h3 id="变分自编码器vae">4 变分自编码器（VAE）</h3><p>​  首先说明变分自编码器的总体意义：VAE是在自编码器基础上结合了变分贝叶斯推断的方法，旨在学习数据的隐含结构，并能够生成新的、类似于训练数据的样本。可以说VAE的主要目的是生成新的数据。VAE通过显式地建模潜在变量的概率分布，使得潜在空间结构更加明确和可解释，最终能生成新样本。这在生成对抗网络（GAN）出现之前是一个重要的进展。<br/>​  下面介绍VAE的主要做法：<br/>​  VAE的整体网络结构简图如下：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003194129820.png"alt="image-20241003194129820" /><figcaption aria-hidden="true">image-20241003194129820</figcaption></figure><p>​  针对一个输入 <spanclass="math inline">\(x_i\)</span>（比如一个样本就是一张图像），网络结构如下（下图中的<spanclass="math inline">\(\mu_{i}^{\prime}\)</span>就是输出的<spanclass="math inline">\(X^d\)</span> ）：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003194253388.png"alt="image-20241003194253388" /><figcaption aria-hidden="true">image-20241003194253388</figcaption></figure><p>​  总结一下VAE的架构：</p><blockquote><ol type="1"><li>我们首先给Encoder输入一个数据点<spanclass="math inline">\(x_i\)</span>,通过神经网络，我们得到隐变量<spanclass="math inline">\(z\)</span>服从的近似后验分布<spanclass="math inline">\(q_\phi(z\midx_i)\)</span>的参数。我们往往认为后验分布是一个各维度独立的高斯分布，因此令Encoder输出<spanclass="math inline">\(z\mid x_i\)</span>服从的高斯分布的参数<spanclass="math inline">\(\sigma_i^2\)</span>和<spanclass="math inline">\(\mu_i\)</span>即可。<br/>&gt; 2. 有了<spanclass="math inline">\(z\mid x_i\)</span>分布的参数<spanclass="math inline">\(\sigma_i^2\)</span>和<spanclass="math inline">\(\mu_i\)</span>后，我们从对应的高斯分布中采样出一个<spanclass="math inline">\(z_i\)</span>,这个<spanclass="math inline">\(z_i\)</span>应当代表与<spanclass="math inline">\(x_i\)</span>相似的一类样本。<br/>&gt; 3.我们令Decoder拟合似然的分布<span class="math inline">\(p_\theta(X\midz_i)\)</span>。喂给Decoder一个<spanclass="math inline">\(z_i\)</span>,它应当返回<spanclass="math inline">\(X\midz_i\)</span>服从的分布的参数。我们往往认为似然也服从一个各维度独立的高斯分布，因此令Decoder输出<spanclass="math inline">\(X\mid z_i\)</span>服从的高斯分布的参数<spanclass="math inline">\(\sigma_i^{\prime2}\)</span>和<spanclass="math inline">\(\mu_i^{\prime}\)</span>即可。<br/>&gt; 4.在得到<span class="math inline">\(X\midz_i\)</span>的分布的参数后，理论上我们需要从这个分布中进行采样，来生成可能的数据点<spanclass="math inline">\(x_{i}\)</span>。</li></ol></blockquote><p>​  上述第四点中值得注意的是，在大部分实现中，人们往往不进行采样，而是直接将模型输出的<spanclass="math inline">\(\mu_i^{\prime}\)</span>当作是给定<spanclass="math inline">\(z_i\)</span>生成的数据点<spanclass="math inline">\(x_i\)</span>。<br/>​  除此之外，人们也往往认为<spanclass="math inline">\(p_\theta(X\midz_i)\)</span>是一个固定方差的各维度独立的多元高斯分布，即<spanclass="math inline">\(p_\theta(X\midz_i)=\mathcal{N}(X\mid\mu_i^{\prime}(z_i;\theta),\sigma^{\prime2}*I)\)</span>,其中<spanclass="math inline">\(\sigma^{\prime2}\)</span>是一个人为给定的超参数。这意味着我们实际中并不真的让模型输出<spanclass="math inline">\(\sigma_i^{\prime2}\)</span>,模型只要输出<spanclass="math inline">\(\mu_i^{\prime}\)</span>就行了。<br/>​  具体公式推导参考上面的那篇博客，下面是对博客推导思想的简单总结：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/5c14ac887b954cc98d2de9a26abf3558.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>为了更好理解，还是直接博客的代码吧：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on January 28, 2021</span><br><span class="line"></span><br><span class="line">@author: Siqi Miao</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;] = &quot;TRUE&quot;</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision.utils import save_image</span><br><span class="line">from torchvision.datasets import MNIST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class VAE(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_features, latent_size, y_size=0):</span><br><span class="line">        super(VAE, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.latent_size = latent_size</span><br><span class="line"></span><br><span class="line">        self.encoder_forward = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features + y_size, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, self.latent_size * 2)  # latent_size表示潜变量的个数，每一个变量有均值和方差两个数值</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decoder_forward = nn.Sequential(</span><br><span class="line">            nn.Linear(self.latent_size + y_size, in_features), # 解码器输入的时候只需要输入编码器输出的潜在变量的均值</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def encoder(self, X):</span><br><span class="line">        out = self.encoder_forward(X)</span><br><span class="line">        mu = out[:, :self.latent_size] # 前latent_size个数据是均值</span><br><span class="line">        log_var = out[:, self.latent_size:] # 后latent_size个数据是log(方差)</span><br><span class="line">        return mu, log_var</span><br><span class="line"></span><br><span class="line">    def decoder(self, z):</span><br><span class="line">        mu_prime = self.decoder_forward(z)</span><br><span class="line">        return mu_prime</span><br><span class="line"></span><br><span class="line">    def reparameterization(self, mu, log_var):  # Reparameterization Trick</span><br><span class="line">        epsilon = torch.randn_like(log_var) # 产生和log_var维度一样的高斯分布数据</span><br><span class="line">        z = mu + epsilon * torch.sqrt(log_var.exp()) # log_var.exp()=var，sqrt(var)就是sigema</span><br><span class="line">        return z</span><br><span class="line"></span><br><span class="line">    def loss(self, X, mu_prime, mu, log_var): # mu_prime编码器的输出；mu潜变量的均值，也就是潜变量的值了；log_var潜变量的log(方差)</span><br><span class="line">        # reconstruction_loss = F.mse_loss(mu_prime, X, reduction=&#x27;mean&#x27;) is wrong!</span><br><span class="line">        #print(mu_prime.shape) # [1024,784]</span><br><span class="line">        #print(mu.shape) # [1024,64]</span><br><span class="line">        #print(log_var.shape) # [1024,64]</span><br><span class="line">        # torch.square 是一个用于计算张量中每个元素的平方的函数。这个函数返回一个新的张量，其中包含原始张量中每个元素的平方值</span><br><span class="line">        reconstruction_loss = torch.mean(torch.square(X - mu_prime).sum(dim=1)) # sum(dim=1)表示对列求和，torch.mean就相当于是对1024个样本求均值了，也就是公式里的1/n</span><br><span class="line"></span><br><span class="line">        latent_loss = torch.mean(0.5 * (log_var.exp() + torch.square(mu) - log_var).sum(dim=1)) # sum(dim=1)表示对潜变量求和，torch.mean相当于是对1024个样本求均值</span><br><span class="line">        return reconstruction_loss + latent_loss</span><br><span class="line"></span><br><span class="line">    def forward(self, X, *args, **kwargs):</span><br><span class="line">        mu, log_var = self.encoder(X)</span><br><span class="line">        z = self.reparameterization(mu, log_var)</span><br><span class="line">        mu_prime = self.decoder(z)</span><br><span class="line">        return mu_prime, mu, log_var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CVAE(VAE): # 继承VAE类，所以可以使用VAE的编码和解码器</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_features, latent_size, y_size):</span><br><span class="line">        super(CVAE, self).__init__(in_features, latent_size, y_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, y=None, *args, **kwargs):</span><br><span class="line">        y = y.to(next(self.parameters()).device)</span><br><span class="line">        #print(y.shape) # [1024]</span><br><span class="line">        X_given_Y = torch.cat((X, y.unsqueeze(1)), dim=1) </span><br><span class="line">        #print(X_given_Y.shape) # [1024,785]</span><br><span class="line">        </span><br><span class="line">        mu, log_var = self.encoder(X_given_Y)</span><br><span class="line">        z = self.reparameterization(mu, log_var)</span><br><span class="line">        z_given_Y = torch.cat((z, y.unsqueeze(1)), dim=1)</span><br><span class="line"></span><br><span class="line">        mu_prime_given_Y = self.decoder(z_given_Y)</span><br><span class="line">        return mu_prime_given_Y, mu, log_var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(model, optimizer, data_loader, device, name=&#x27;VAE&#x27;):</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = 0</span><br><span class="line">    pbar = tqdm(data_loader)</span><br><span class="line">    for X, y in pbar:</span><br><span class="line">        #print(X.shape) # [1024,1,28,28]</span><br><span class="line">        #print(y.shape) # [1024]</span><br><span class="line">        batch_size = X.shape[0]</span><br><span class="line">        X = X.view(batch_size, -1).to(device)</span><br><span class="line">        #print(X.shape) # [1024,784]</span><br><span class="line">        model.zero_grad() #将模型中所有参数的梯度缓存清零。在进行反向传播计算梯度之前，必须先将之前计算的梯度清零。这是因为在 PyTorch 中，梯度是累积的。</span><br><span class="line"></span><br><span class="line">        if name == &#x27;VAE&#x27;:</span><br><span class="line">            mu_prime, mu, log_var = model(X)</span><br><span class="line">        else:</span><br><span class="line">            mu_prime, mu, log_var = model(X, y)</span><br><span class="line"></span><br><span class="line">        loss = model.loss(X.view(batch_size, -1), mu_prime, mu, log_var)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        pbar.set_description(&#x27;Loss: &#123;loss:.4f&#125;&#x27;.format(loss=loss.item()))</span><br><span class="line"></span><br><span class="line">    return total_loss / len(data_loader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@torch.no_grad()</span><br><span class="line">def save_res(vae, cvae, data, latent_size, device):</span><br><span class="line">    num_classes = len(data.classes)</span><br><span class="line"></span><br><span class="line">    # raw samples from dataset</span><br><span class="line">    out = [] # 用于存储每个类别的图像</span><br><span class="line">    for i in range(num_classes): </span><br><span class="line">        # 提取类别为 i 的图像</span><br><span class="line">        img = data.data[torch.where(data.targets == i)[0][:num_classes]]</span><br><span class="line">        out.append(img)</span><br><span class="line">    out = torch.stack(out).transpose(0, 1).reshape(-1, 1, 28, 28) # 将图像堆叠在一起，并转置维度以便于保存</span><br><span class="line">    save_image(out.float(), &#x27;./img/raw_samples.png&#x27;, nrow=num_classes, normalize=True) # 100张图像</span><br><span class="line"></span><br><span class="line">    # samples generated by vanilla VAE</span><br><span class="line">    z = torch.randn(num_classes ** 2, latent_size).to(device)</span><br><span class="line">    # print(z.shape) # [100,64]</span><br><span class="line">    out = vae.decoder(z)</span><br><span class="line">    save_image(out.view(-1, 1, 28, 28), &#x27;./img/vae_samples.png&#x27;, nrow=num_classes)</span><br><span class="line"></span><br><span class="line">    # sample generated by CVAE</span><br><span class="line">    z = torch.randn(num_classes ** 2, latent_size).to(device)</span><br><span class="line">    y = torch.arange(num_classes).repeat(num_classes).to(device)</span><br><span class="line">    z_given_Y = torch.cat((z, y.unsqueeze(1)), dim=1)</span><br><span class="line">    out = cvae.decoder(z_given_Y)</span><br><span class="line">    save_image(out.view(-1, 1, 28, 28), &#x27;./img/cvae_samples.png&#x27;, nrow=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">    device = torch.device(device)</span><br><span class="line"></span><br><span class="line">    batch_size = 256 * 4</span><br><span class="line">    epochs = 50</span><br><span class="line">    latent_size = 64</span><br><span class="line">    in_features = 28 * 28</span><br><span class="line">    lr = 0.001</span><br><span class="line"></span><br><span class="line">    data = MNIST(&#x27;./dataset/&#x27;, download=True, transform=transforms.ToTensor())</span><br><span class="line">    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line">    # train VAE</span><br><span class="line">    vae = VAE(in_features, latent_size).to(device)</span><br><span class="line">    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training VAE...&#x27;)</span><br><span class="line">    for epoch in range(1, 1 + epochs):</span><br><span class="line">        loss = train(vae, optimizer, data_loader, device, name=&#x27;VAE&#x27;)</span><br><span class="line">        print(&quot;Epochs: &#123;epoch&#125;, AvgLoss: &#123;loss:.4f&#125;&quot;.format(epoch=epoch, loss=loss))</span><br><span class="line">    print(&#x27;Training for VAE has been done.&#x27;)</span><br><span class="line"></span><br><span class="line">    # train VCAE</span><br><span class="line">    cvae = CVAE(in_features, latent_size, y_size=1).to(device)</span><br><span class="line">    optimizer = torch.optim.AdamW(cvae.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training CVAE...&#x27;)</span><br><span class="line">    for epoch in range(1, 1 + epochs):</span><br><span class="line">        loss = train(cvae, optimizer, data_loader, device, name=&#x27;CVAE&#x27;)</span><br><span class="line">        print(&quot;Epochs: &#123;epoch&#125;, AvgLoss: &#123;loss:.4f&#125;&quot;.format(epoch=epoch, loss=loss))</span><br><span class="line">    print(&#x27;Training for CVAE has been done.&#x27;)</span><br><span class="line"></span><br><span class="line">    save_res(vae, cvae, data, latent_size, device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>VAE的变种：条件变分自编码器（CVAE）<br/>传统的VAE可以近似地生成输入数据，但不能定向地生成特定类型的数据。为解决这一问题，将数据x和x的部分标签(y)输入到CVAE的编码器部分。这样就会生成指定类别的数据。CVAE的结构与VAE相似，因此CVAE的计算方法和优化方法与VAE一致。由于在输入中存在一些标签Y，CVAE成为一种半监督学习形式。<br/>参考资料：<br/>[1]《DeepLearning》<br/>[2] Li P, Pei Y, Li J. A comprehensive survey on designand application of autoencoder in deep learning[J]. Applied SoftComputing, 2023, 138: 110176.<br/>[3] <ahref="https://zhuanlan.zhihu.com/p/348498294">机器学习方法—优雅的模型（一）：变分自编码器（VAE）- 知乎 (zhihu.com)</a><br/>[4] <ahref="https://blog.csdn.net/qq_46146657/article/details/140666130">自编码器（autoencoder）-CSDN博客</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Clustering:A Comprehensive Survey</title>
      <link href="/Deep-Clustering-Survey/"/>
      <url>/Deep-Clustering-Survey/</url>
      
        <content type="html"><![CDATA[<center>Deep Clustering: A Comprehensive Survey</center><center>Yazhou Ren , Member, IEEE, Jingyu Pu , Zhimeng Yang, Jie Xu , GuofengLi, Xiaorong Pu , Philip S. Yu , Fellow, IEEE, and Lifang He , Member,IEEE</center><p><a href="https://ieeexplore.ieee.org/abstract/document/10585323"><imgsrc="https://img.shields.io/badge/TNNLS-2024-yellow"alt="TNNLS" /></a></p><h1 id="摘要">摘要</h1><p>​  聚类分析在机器学习和数据挖掘中起着不可或缺的作用。学习一个好的数据表示对聚类算法至关重要。近年来，深度聚类（DC，deepclustering）可以利用深度神经网络（DNNs，deep neuralnetworks）学习聚类友好表示，已广泛应用于聚类任务。现有的DC总结主要集中在单视图场和网络架构上，忽略了复杂的集群应用场景。为了解决这个问题，在本文中，我们提供了一个对DC的数据源的全面总结。对于不同的数据源，我们从方法学、先验知识和体系结构等方面系统地区分了聚类方法。具体地说，DC方法被分为四类，传统的单视图DC、半监督DC、深度多视图聚类（MVC）和深度传输聚类。最后，我们讨论了DC不同领域的开放挑战和潜在的未来机遇。</p><p>​  索引术语——深度聚类（DC）、多视图聚类（MVC）、半监督聚类、迁移学习。<br/>​  发表于人工智能领域B区期刊TNNLS，</p><h1 id="i.-引言">I. 引言</h1><p>​  随着网络媒体的发展，可以很容易地收集大量复杂的数据。通过对这些数据的精确分析，我们可以挖掘出这些结论的价值，并应用于许多领域，如人脸识别[1]、[2]、情绪分析[3]、[4]和智能制造[5]、[6]。<br/>​  一个可以用于对具有不同标签的数据进行分类的模型是许多应用程序的基础。对于有标记的数据，允许将标签作为最重要的信息作为指南。对于未标记的数据，找到一个可量化的目标作为模型构建过程的指导是聚类的关键问题。在过去的几十年里，人们提出了大量基于浅模型的聚类方法，包括基于质心的聚类基于密度的聚类[9]，[10]，[11]，[12]，[13]，分布的聚类[14]、层次聚类[15]、集成聚类[16]，[17]，和多视图聚类（MVC）[18]，[19]，[20]，[21]，[22]，[23]，[24]，[25]，[26]，[27]，[28]，[29]，[30]。这些浅层模型只有在特征具有代表性时才有效，而由于特征学习能力较差，它们在复杂数据上的性能通常受到限制。<br/>​  为了将原始复杂数据映射到一个易于聚类的特征空间，许多聚类方法都侧重于特征提取或特征转换，如主成分分析（PCA）[31]、核方法[32]、光谱方法[33]和深度神经网络（DNN）[34]。在这些方法中，DNN由于其优秀的非线性映射能力和在不同场景下的灵活性，是一种很有前途的方法。一种设计良好的基于深度学习的聚类方法[称为深度聚类（DC）]，旨在有效地从数据中提取更有利于聚类的特征，并同时使用学习到的特征进行聚类。与传统的聚类方法不同，DC背后的基本概念是将聚类目标合并到深度学习提供的鲁棒表示能力中。因此，获取一个重要的数据表示成为DC的基本要求。<br/>​  DC领域的研究很多，也有一些关于DC[35]、[36]、[37]、[38]方法的总结。具体来说，现有的DC系统综述主要集中在单视图聚类任务和神经网络的架构上。例如，Aljalbout等人。[35]只关注基于深度自动编码器（DAE）的深度单视图聚类方法。Min等人从不同深度网络的角度对DC方法进行分类。Nutakki等人[37]根据其训练策略将深度单视图聚类方法分为三类：多步连续DC、联合DC和闭环多步DC。Zhou等[38]通过特征学习与聚类模块的交互方式对深度单视图聚类方法进行分类。然而，在现实世界中，聚类的数据集总是相关的，例如，阅读的品味与对电影的品味相关，而来自同一个人的侧面和满脸应该被贴上相同的标签。对于这些数据，基于半监督学习、多视图学习和迁移学习的DC方法也取得了重大进展。不幸的是，现有的评论并没有过多地讨论它们。<br/>​  因此，从数据源的角度来对DC进行分类是很重要的。在本总结中，我们从数据的初始设置结合深度学习方法的角度总结了DC。我们从网络和数据结构的角度介绍了DC技术的最新进展，如图1所示。</p><figure><img src="../postimages/Deep-Clustering/image-20240929105628373.png"alt="image-20240929105628373" /><figcaption aria-hidden="true">image-20240929105628373</figcaption></figure><p>​  具体来说，我们将DC方法组织为以下四类。由于我们从数据源的角度来划分方法，所以特定方法的实现并没有完全分离。方法中有一部分在内部相互关联因此，从数据源的角度来对DC进行分类是很重要的。在本总结中，我们从数据的初始设置结合深度学习方法的角度总结了DC。我们从网络和数据结构的角度介绍了DC技术的最新进展，如图1所示。具体来说，我们将DC方法组织为以下四类。由于我们从数据源的角度来划分方法，所以特定方法的实现并没有完全分离。方法中有一部分在内部相互关联。<br/>​  1)深度单视图聚类：对于传统的聚类任务，通常假设数据具有相同的形式和结构，称为单视图或单模态数据。用DNNs提取这些数据的表示是DC的一个重要特征。然而，更值得注意的是不同的应用深度学习技术，它们与DNNs的结构高度相关。为了比较特定DNNs的技术路线，我们将这些算法分为五类：基于DAE的DC、基于DNN的DC、基于变分自编码器（VAE，variationalautoencoder）的DC、基于生成对抗网络（GAN，generative adversarialnetwork）的DC和基于图神经网络（GNN，graph neuralnetwork）的DC。<br/>​  2)基于半监督学习的DC：当待处理的数据包含一小部分先验约束时，传统的聚类方法不能有效地利用这些先验信息，而半监督聚类是解决这一问题的有效方法。目前，深度半监督聚类的研究还没有得到很好的探索。然而，半监督聚类是不可避免的，因为通过在模型中添加附加信息作为约束损失，使聚类方法成为半监督聚类方法是可行的。<br/>​  3)基于多视角学习的DC：在现实世界中，数据通常来自不同的特征收集器或具有不同的结构。我们称这些数据为“多视图数据”或“多模态数据”，其中每个样本都有多个表示。基于多视图学习的DC的目的是利用多视图数据中包含的一致性和互补的信息来提高聚类性能。此外，多视图学习的思想可能对深度单视图聚类具有指导意义。在本总结中，我们将深度MVC分为三类：基于深度嵌入式聚类（DEC，deep-embeddedclustering）、基于子空间聚类和基于GNN。<br/>​  4)基于迁移学习的DC：对于一个实例数量有限和高维度的任务，有时我们可以找到一个助手来提供额外的信息。例如，如果任务A与另一个任务B相似，并且任务B比A有更多的聚类信息（B被标记或B比A更容易聚类），那么将信息从B转移到A是有用的。无监督领域自适应（UDA，unsuperviseddomainadaption）的迁移学习近年来得到了广泛的发展，它包含两个领域：带有标签的源域和未标记的目标域。迁移学习的目标是将从源任务中学习到的知识或模式应用到一个不同但相关的目标任务中。基于迁移学习的DC分算方法旨在利用相关任务的信息来提高当前聚类任务的性能。<br/>​  在研究相应的聚类方法之前，有必要注意聚类数据的不同特征和条件。在本总结中，现有的DC方法从数据源进行系统分类。分析了不同聚类方法的优缺点和适用条件。最后，我们提出了在DC领域的一些有趣的研究方向。</p><h1 id="ii.-定义和初步工作">II. 定义和初步工作</h1><p>​  我们将在本节中介绍这些符号。在本文中，我们使用大写字母来表示矩阵，并使用小写字母来表示向量。除非另有说明，本文中使用的符号将在命名法中进行总结。<br/>​  本总结将介绍基于不同背景条件的四种DC问题。在这里，我们正式地定义了这些问题。给定一组数据样本<spanclass="math inline">\(X\)</span>，我们的目标是找到一个映射函数<spanclass="math inline">\(F\)</span>，它可以将<spanclass="math inline">\(X\)</span>映射到<spanclass="math inline">\(k\)</span>个簇中。映射结果用<spanclass="math inline">\(\hatY\)</span>表示。因此，我们所处理的任务如下:<br/>​  1)深度单视图集群：<span class="math display">\[F(X)\to\hat{Y}.\]</span>​  2)半监督的深度聚类： <spanclass="math display">\[F(X,A)\to\hat{Y}.\]</span>​  其中A是一个约束矩阵。</p><p>​  3)深度MVC： <spanclass="math display">\[F\left(X^1,\ldots,X^v\right)\to\hat{Y}\]</span>​  其中<span class="math inline">\(X^i\)</span>是<spanclass="math inline">\(X\)</span>的第一个视图。</p><p>​  4)具有域自适应的深度聚类： <spanclass="math display">\[F(X^s,Y^s,X^t)\to\hat{Y}\]</span> ​  其中<spanclass="math inline">\((X^s,Y^s)\)</span>为已标记的源域，<spanclass="math inline">\(X^t\)</span>为未标记的目标域。</p><h1 id="iii.-深度单视图聚类">III. 深度单视图聚类</h1><p>​  表示学习[39]理论显示了特征学习（或表示学习）在机器学习任务中的重要性。然而，深度表示学习主要是监督学习，需要许多标记数据。正如我们之前提到的，DC问题的障碍是什么可以用来指导训练过程，如监督问题中的标签。DC中最“受监督”的信息是数据本身。因此，我们如何训练一个有效的特征提取器来获得良好的表示呢？根据特征提取器的训练方式，我们将深度单视图聚类算法分为五类：基于DAE、基于DNN、基于VAE、基于GAN和基于GNN。</p><p>表1基于DAE和基于DNN的方法在深度单视图聚类中的总结。我们总结了基于“联合或单独”和“特征”的基于DAE的方法</p><figure><img src="../postimages/Deep-Clustering/image-20240929113755442.png"alt="image-20240929113755442" /><figcaption aria-hidden="true">image-20240929113755442</figcaption></figure><p>​  这些方法的区别主要是关于损失分量的，其中损失项的定义见表一，并解释如下。<br/>​  1)DAE-Based/GNN-Based: <spanclass="math inline">\(L=L_{rec}+L_c\)</span><br/>​   2)DNN-Based: <spanclass="math inline">\(L = L_{ext} + L_c\)</span><br/>​   3)VAE-Based:<span class="math inline">\(L = L_{ELBO} + L_c\)</span><br/>​  4)GAN-Based: <span class="math inline">\(L = L_{gan} +L_c\)</span><br/>​  在无监督学习中，我们要处理的问题是训练一个可靠的没有标签的特征提取器。在现有的工程中，主要有两种方式：<br/>​  1)根据以下原理优化伪标签的损失函数：缩小簇内距离，扩大簇间距离<br/>​  2)一个可以帮助训练特征提取器的额外任务。<br/>​  对于具有特殊特征提取器的聚类方法，如自动编码器，重构损失<spanclass="math inline">\(L_{rec}\)</span>可以解释为额外的任务。在本文中，面向聚类的损失<spanclass="math inline">\(L_c\)</span>表示聚类目标的损失。基于DAE/基于GNN的方法使用自动编码器/图自编码器作为特征提取器，因此损失函数总是由一个重构损失<spanclass="math inline">\(L_{rec}\)</span>和另一个面向聚类的损失<spanclass="math inline">\(L_c\)</span>组成。相比之下，基于DNN的方法使用额外的任务或其他策略来优化特征提取器。基于VAE的方法优化了证据损失下界（ELBO，evidencelower bound）<spanclass="math inline">\(L_{ELBO}\)</span>。基于GAN的方法是基于生成式的对抗性损失<spanclass="math inline">\(L_{gan}\)</span>。基于这五个维度，现有的深度单视图聚类方法总结在表一和表二中。</p><p>表2 基于VAE、基于GAN和基于GNN的方法在深度单视图聚类中的总结</p><figure><img src="../postimages/Deep-Clustering/image-20240929115248981.png"alt="image-20240929115248981" /><figcaption aria-hidden="true">image-20240929115248981</figcaption></figure><h2 id="a.-基于dae">A. 基于DAE</h2><p>​  自动编码器网络[39]最初是为数据的无监督表示学习而设计的，可以学习一个高度非线性的映射函数。使用DAE[97]是开发DC方法的一种常见方法。DAE的目的是通过最小化网络的重构损失来学习低维嵌入特征空间，其定义为：<spanclass="math display">\[L_{\mathrm{rec}}=\min\frac{1}{n}\sum_{i=1}^n\|x_i-\phi_r(\phi_e(x_i))\|^2\]</span>​  其中，<span class="math inline">\(\phi_e(\cdot)\)</span>和<spanclass="math inline">\(\phi_r(\cdot)\)</span>分别表示自动编码器的编码器网络和解码器网络。利用该编码器作为特征提取器，提出了各种聚类目标函数。我们将这些基于DAE的聚类方法总结为基于DAE的DC。在基于DAE的DC方法中，主要有两种获取标签的方法。第一种方法是将数据嵌入到低维特征中，然后用传统的聚类方法如k-means算法[7]等聚类方法对嵌入的特征进行聚类。第二种方法是联合优化特征提取器和聚类结果。我们将这两种方法称为“单独分析”和“联合分析”，并在下面对它们进行详细阐述。<br/>​  “单独分析”是指学习特征和聚类数据被单独执行。为了解决通过“单独分析”学习到的表征由于其固有的特性而不具有聚类导向的问题，Huang等人[41]提出了一种用于聚类的深度嵌入网络（DEN，deepembeddingnetwork），该网络基于DAE目标施加了两个约束：<strong>局部性保留约束</strong>和<strong>群稀疏性约束</strong>。</p><ul><li>局部保留约束使同一集群中嵌入的特征相似。</li><li>群稀疏性约束的目的是对角线化表示的相似度矩阵。</li></ul><p>​  这两个约束条件提高了聚类性能，同时减少了簇内距离，扩大了簇间距离。大多数基于DAE的聚类方法的目标都是研究这两种距离。因此，在表一中，我们从“特征”的角度对这些方法进行了总结，说明了优化簇内距离和簇间距离的方法。<br/>​  Peng等[42]在子空间聚类领域提出了一种新的基于深度学习的框架，即具有稀疏先验（PARTY，deepsubspace clustering with sparsityprior）的深度子空间聚类。PARTY通过考虑不同样本之间的关系（即结构先验）来增强自动编码器并解决了传统的子空间聚类方法的局限性。据我们所知，PARTY是第一个基于深度学习的子空间聚类方法，也是第一个在神经网络之前引入全局结构进行无监督学习的工作。与PARTY不同，Jiet al.[45]提出了另一种深度子空间聚类网络（DSC-Net）架构来学习非线性映射，并引入了一个自表达层来直接学习相似度矩阵。<br/>​  基于密度的聚类[9]，[98]是另一种流行的聚类方法。Ren等人[57]提出了基于深度密度的图像聚类（DDIC，density-basedimageclustering），该方法使用DAE学习低维特征表示，然后对学习到的特征进行基于密度的聚类。特别是，DDIC不需要提前知道集群的数量。<br/>​  “联合分析”的目的是学习一种更适合聚类的表示，这不同于深度学习和聚类单独进行的单独分析方法，并且神经网络在学习数据特征时没有面向聚类的目标。大多数后续的DC研究将聚类目标与特征学习相结合，使神经网络能够从数据的潜在分布中学习有利于聚类的特征。在本总结中，这些方法被总结为“联合分析”。<br/>​  受非参数算法t分布随机邻域嵌入（t-SNE，t-distributedstochastic neighborembedding）[99]的启发，Xie等[43]提出了一个联合框架来优化特征学习和聚类目标，命名为DEC。DEC首先通过<spanclass="math inline">\(L_{rec}\)</span>学习从数据空间到较低维特征空间的映射，然后迭代优化聚类损失KL（S∥R）（即Kullback–Leibler(KL)散度）。这里，S表示描述嵌入数据与每个聚类质心之间相似性的数据的软赋值（质心用k-means初始化），R是调整后的目标分布，与S相比具有更纯粹的聚类赋值。<br/>​  DEC由于其联合学习框架和计算复杂度低，是DC技术中的一种代表性方法。基于DEC，已经提出了一些变体。例如，为了保证微调阶段的局部结构，提出了改进的DEC（IDEC）方法，以联合优化加权聚类损失和自编码器的重构损失。DEC与数据增强（DEC-DA）[49]在DEC中应用了数据增强策略。Li等人[50]提出了判别增强图像聚类（DBC）来处理图像表示学习和图像聚类。DBC有一个类似于DEC的管道，但学习过程是自定节奏的[100]，其中首先选择最简单的实例，并逐步扩展更复杂的样本。<br/>​  在DEC中，预测的聚类任命是由学生的t-分布计算出来的。不同的是，Dizaji等人[46]提出了一种深度嵌入的正则化聚类（DEPICT，deep-embeddedregularized clustering），通过在卷积自编码器（CAE，convolutionalautoencoder）的嵌入层上叠加一个新的聚类损失。此外，DEPICT的聚类损失通过聚类分配频率的先验和层级特征重构损失函数进行正则化。Yang等人，[47]直接将k-均值的目标作为聚类损失。该模型名为DC网络（DCN），是一种联合降维和k-均值聚类方法，通过学习DAE来实现降维。Shah和Koltun[51]提出了深度连续聚类（DCC， deep continuousclustering），这是一种通过将自编码器集成到范例中的扩展。DCC通过联合优化已定义的数据损失、成对损失和重建损失来进行聚类学习。特别是，它不需要对集群数量的先验知识。Tzoreff.[53]等人提出了深度鉴别聚类潜在空间（DDLSC，deepdiscriminative latent space forclustering），以优化关于鉴别成对损失函数的DAE。<br/>​  深度流形聚类（DMC，Deepmanifoldclustering）[48]是第一个将深度学习应用于流形聚类[101]，[102]中的方法。在DMC中，训练一个由堆叠的RBMs[103]组成的自动编码器来获得转换后的表示。DMC的重构损失和聚类损失都与以往的方法不同，即通过重构一个样本及其局部邻域来定义局部保持目标。利用样本和聚类中心之间的高斯核测量的惩罚系数和距离来定义面向聚类的目标。<br/>​  最近提出的基于DAE的聚类算法也利用DAE的变体来更好地学习低维特征，并通过结合传统机器学习方法的思想来提高聚类性能。例如，利用双自编码器网络（DSCDAE，dualautoencodernetwork）[55]的深度光谱聚类和通过集成DAE学习（SC-EDAE，spectralclustering via ensemble DAElearning）[58]的光谱聚类，目的是将光谱聚类集成到精心设计的DC自编码器中。Zhang等[56]提出了神经协同子空间聚类（NCSC，neuralcollaborative subspace clustering），它将两个置信映射建立在自编码器学习到的特征上，作为子空间聚类的监督信息。在数据增强下对的自适应自定速DC（ASPC-DA，adaptiveself-paced DC with dataaugmentation）[59]中，同同时采用了自定速学习思想[100]和数据增强技术。其学习过程与DEC相同，包括两个阶段，即对自动编码器进行预训练和对编码器进行微调。<br/>​  一般来说，我们注意到所采用的网络结构与要处理的数据类型有关。例如，全连接网络通常用于提取一维数据特征，而卷积神经网络（CNNs）则用于提取图像特征。上述大多数基于DAE的DC方法都可以通过全连接的自动编码器和CAE来实现，因此，它们在一定程度上适用于各种类型的数据。然而，在计算机视觉领域，有一类专注于图像聚类的直流方法。这些方法可以追溯到[104]，并被总结为基于DNN的DC，因为它们通常使用CNNs来进行图像特征学习和语义聚类。</p><h2 id="b.-基于dnn">B. 基于DNN</h2><p>​  本节将介绍基于DNN的聚类方法。与基于DAE的聚类方法不同，基于DNN的方法必须设计额外的任务来训练特征提取器。在本调查中，我们从“面向聚类的损失”和“特征”两个角度总结了表I中基于DNN的DC方法。“面向聚类的损失”显示了是否存在一个损失函数，它明确地缩小了簇内的距离或扩大了簇间的距离。图2显示了基于CNN的深度无监督学习的框架。</p><figure><img src="../postimages/Deep-Clustering/image-20240929160431452.png"alt="image-20240929160431452" /><figcaption aria-hidden="true">image-20240929160431452</figcaption></figure><p>图2。基于DNN的学习框架（单视图聚类）。X是聚类的数据，f是X的特征提取器。第一部分描述了监督学习的框架。Y表示真实的标签，S表示预测的结果。使用Y和S，我们可以计算反向传播的分类损失。第二部分是具有额外任务的方法的框架。额外的任务用于训练网络进行良好的嵌入z。第三部分描述了需要调整集群分配的方法的过程。S表示预测结果，R为S的调整。</p><p>​  当DNN训练过程开始时，随机初始化的特征提取器是不可靠的。因此，基于随机初始化神经网络的DC方法通常采用传统的聚类技巧，如分层聚类[105]或专注于额外的任务，如实例生成。例如，Yang等人[63]提出了一种名为JULE的联合无监督学习方法，该方法应用凝聚聚类魔法来训练特征提取器。具体来说，JULE在一个递归框架中制定了联合学习，其中将凝聚聚类的合并操作视为前向传递，并将DNN的表示学习视为后向传递。基于这个假设，JULE还应用了一个损失，即缩小了簇内距离，同时扩大了簇内距离。在每个轮次中，JULE将两个簇合并为一个簇，并计算向后通过的损失。<br/>​  Chang等人[65]提出了深度自适应图像聚类（DAC，deepadaptive imageclustering）来解决特征学习和聚类相结合的问题。在DAC中，将聚类问题重构为二值成对分类问题，用以判断具有估计余弦相似度的成对图像是否属于同一聚类。然后，它自适应地选择相似的样本，以有监督的方式训练DNN。DAC为DC提供了一个新的视角，但它只关注于成对模式之间的关系。深度判别聚类（DDC，Deepdiscriminativeclustering）分析[54]是通过引入全局和局部关系约束，更鲁棒和广义的约束。基于空间transformer的深度自适应聚类（ST-DAC，Spatialtransformer-deep adaptive clustering）[69]应用视觉注意机制[106]来修改DAC的结构。Haeusser等人[68]提出了关联DC（ADC，associativeDC），它包含一组与图像嵌入具有相同形状的质心变量。由于质心变量在迭代过程中可以传递关于数据结构的高级信息，它们引入了一个具有多个损失项的目标函数，同时训练这些质心变量和DNN参数以及聚类映射层。<br/>​  上述聚类方法通过将一个实例的聚类通过整个深度网络来估计其聚类，从而倾向于提取实例[107]的全局特征。一些聚类方法使用一个成熟的分类网络来初始化特征提取器。例如，深度集群[66]对深度模型的输出特性（如AlexNet[108]和VGG-16 [109]）应用k-means，并将集群分配作为“伪标签”来优化CNNs的参数。Hsu和Lin [67]提出了聚类CNN（CCNN，clusteringCNN），该方法将小批量k-均值与从ImageNet数据集[110]预训练的模型集成在一起。<br/>​  为了提高模型的鲁棒性，越来越多的方法对DC[49]、[59]、[76]进行了数据增强。例如，Huang等人[76]扩展了经典的最大边际聚类[111]，[112]的思想，建立了一种新的深度语义聚类方法[称为划分置信度极大化（PICA，PartItionConfidencemAximization）]。在PICA中，采用了颜色抖动、随机调整尺度和水平翻转三个操作来进行数据增强和扰动。<br/>​  互信息也被作为学习表示[113]、[114]的标准，并在最近的聚类方法中流行起来，特别是对图像数据。各种数据增强技术已被应用于生成转换后的图像，并用于挖掘它们的互信息。例如，Ji等人[71]提出了用于语义聚类和图像分割的不变信息聚类（IIC，invariant informationclustering）。在IIC中，每幅图像及其随机变换都被视为一个样本对。通过最大化每对聚类分配之间的互信息，该模型可以找到具有语义意义的聚类，并自然地避免退化解。深度综合相关挖掘（DCCM，deepcomprehensive correlationmining）[72]是一种新的图像聚类框架，它使用伪标签丢失作为监督信息。此外，作者扩展了实例级互信息，并提出了三重互信息损失，以学习更多的区别特征。基于目前流行的对比学习[115]，Zhong等[75]提出了深度鲁棒聚类（DRC，deep robustclustering），其中引入了两个对比损失项来减少类内方差和增加类间方差。互信息和对比学习是相互关联的。在DRC，作者总结了一个框架，可以将最大化互信息转化为最小化对比损失。<br/>​  在语义层次的图像聚类领域，人们认为原始图像的预测应该与数据增强后图像的预测一致。因此，在无监督学习环境下，数据增强技术不仅被用于扩展训练数据，而且可以很容易地获得有监督的信息。这就是为什么数据增强可以广泛应用于许多最近提出的图像聚类方法。例如，Nina等人[70]提出了一种具有数据增强的无解码器方法（称为随机三重态挖掘（RTM，randomtripletmining）），用于聚类和流形学习。为了学习一个更鲁棒的编码器，该模型由三个具有共享权值的编码器组成，在概念上是一个三重网络体系结构。第一和第二编码器将数据增强生成的相似图像作为正对，第二和第三编码器采用RTM选择的负对。通常，定义三重态网络[116]的目标是为了使正对的特征更加相似，而使负对的特征更加不同。<br/>​  虽然许多现有的DC方法联合学习表示和聚类，如JULE和DAC，但也有专门设计的表示学习方法[117]、[118]、[119]、[120]、[121]，以自我监督的方式学习图像的视觉表示。这些方法通过训练深度网络来解决额外的任务来学习语义表示。这些任务可以是预测补丁上下文[117]，在绘制补丁[118]，彩色图像[119]，解决拼图游戏[120]，和预测旋转[121]，等等。近年来，这些自监督表示学习方法已被用于图像聚类。例如，多模态DC（MMDC，multimodalDC）[73]利用一个预测旋转的辅助任务来提高聚类性能。通过采用最近邻（SCAN，Semanticclustering by adopting nearestneighbors）[74]进行语义聚类，首先采用自监督表示学习方法来获得语义上有意义的高级特征。然后，它将语义上有意义的最近邻作为先验信息集成到一个可学习的聚类方法中。<br/>​  由于DEC[43]和JULE[63]被提出联合学习DNNs的特征表示和聚类分配，许多基于DAE和基于DNN的DC方法被提出，并在聚类任务方面取得了很大的进展。然而，在聚类方法中提取的特征表示很难扩展到其他任务，如生成样本。深度生成模型最近引起了广泛的关注，因为它们可以使用神经网络来获取数据分布，从而可以生成样本（VAE[122]，GAN [123]，Pixel-RNN [124]，InfoGAN [125]，和PPGN[126]）。其中，GAN和VAE是两种最典型的深层生成模型。近年来，研究人员将其应用于各种任务，如半监督分类[127]、[128]、[129]、[130]、聚类[131]和图像生成[132]、[133]。在III-C节和III-D节中，我们分别介绍了基于生成模型的DC算法：基于VAE的DC和基于GAN的DC。</p><h2 id="c.-基于vae">C. 基于VAE</h2><p>​  基于非参数聚类（DNC，nonparametricclustering）[134]的深度学习是将深度信念网络应用于DC的先驱工作。然而，在基于概率图形模型的DC中，更多的研究来自于VAE的应用，它将变分推理和DAE结合在一起。<br/>​  大多数基于VAE的DC算法旨在解决一个优化问题ELBO（见推论细节[122]和[135]），p是联合概率分布，q的近似概率分布p（z|x），x是集群的输入数据，z是对应于x而生成的潜在变量：<spanclass="math display">\[L_{\mathrm{ELBO}}=\mathbb{E}_{q(z|x)}\biggl[\log\frac{p(x,z)}{q(z|x)}\biggr].\]</span>​  不同之处在于，不同的算法有不同的潜在变量生成模型或不同的正则化器。我们列出了近年来备受关注的几种基于VAE的DC处理方法如下。为了方便起见，我们忽略了概率分布的参数化形式。<br/>​  传统的VAE生成一个连续的潜在向量z，x是一个原始数据样本的向量。在聚类任务中，基于VAE的方法生成潜在向量（z，y），其中z是表示嵌入的潜在向量，y是标签。因此，优化的ELBO成为：<spanclass="math display">\[L_{\mathrm{ELBO}}=\mathbb{E}_{q(z,y|x)}\biggl[\log\frac{p(x,z,y)}{q(z,y|x)}\biggr].\]</span>​  第一个提出的无监督深度生成聚类框架是变分深度嵌入（VaDE， variationaldeep embedding）[77]。VaDE使用结合VAE的高斯混合模型（GMM，Gaussianmixturemodel）[136]对数据生成过程进行建模。在该方法中，是在高斯混合先验中而不是在单个高斯先验中联合考虑簇分配和潜在变量。<br/>​  与VaDE类似，高斯混合VAE（GMVAE，GaussianmixtureVAE）[78]是另一种将VAE与GMM相结合的DC方法。具体地说，GMVAE考虑生成模型<spanclass="math inline">\(p(x,z,n,c)=p(x|z)p(z|n,c)p(n)p(c)\)</span>，其中c是均匀分布的k个类别，n是正态分布。z是一个连续潜变量，其分布是均值和方差为c和n的高斯混合变量。基于平均场理论[137]，GMVAE因子<spanclass="math inline">\(p(x,z,n,c)=p(x|z)p(z|n,c)p(n)p(c)\)</span>作为后验代理。同样，将这些变分因子用神经网络参数化，并优化了ELBO损失。<br/>​  潜在树VAE（LTVAE，latenttreeVAE）[80]基于GMM和VAE，应用潜在树模型[138]进行表示学习和结构学习进行聚类。不同的是，LTVAE有一个VAE的变体，具有潜在变量的上层结构。上层结构是在潜在特征之上的离散潜在变量的树形结构。所有变量之间的连通性结构被定义为通过消息传递[139]进行优化的潜在树模型的潜在结构。<br/>​  一些深度生成聚类方法的成功取决于良好的初始预训练。例如，在VaDE[77]中，需要进行预训练来初始化集群质心。在通过图嵌入（DGG，graphembedding）[140]的GMVAE的DC中，需要预训练来初始化图嵌入。虽然GMVAE[78]共同学习先验和后验参数，但每个类的先验都依赖于一个随机变量，而不是类本身，这似乎是违反直觉的。基于GMVAE和VaDE的思想，Prasad等人[82]为了解决他们的谬误，提出了一种利用VAE进行图像聚类（VAEIC，VAEfor imageclustering）的新模型。与上述方法不同的是，VAEIC的先验参数是确定性的，并且先验参数和后验参数是联合学习的，而不需要进行预训练过程。不是像GMVAE和VaDE那样执行贝叶斯分类，VAEIC采用更直接的推理和更有原则的潜在空间先验，从而得到更简单的推理模型<spanclass="math inline">\(p(x,z,c)=p(x|z)p(z|c)p(c)\)</span>和更简单的近似后验<spanclass="math inline">\(q(z,c|x)=q(c|x)q(z|x,c)\)</span>。聚类分配由<spanclass="math inline">\(q(c|z)\)</span>直接预测。此外，作者采用数据增强，设计了图像增强损失，使模型具有鲁棒性。<br/>​  除了上面提到的基于VAE的DC方法外，Figueroa和Rivera[79]使用连续的Gumbel-Softmax分布[141]，[142]来近似分类分布进行聚类。[81]等人扩展了变分阶梯自动编码器[143]，并提出了一种解纠缠聚类算法。Cao等人[83]提出了一种简单、可扩展、稳定的变分DC算法，该算法引入了对变分DC的通用改进。</p><h2 id="d.-基于gan">D. 基于GAN</h2><h2 id="e.-基于gnn">E. 基于GNN</h2><h1 id="iv.-半监督的深度聚类">IV. 半监督的深度聚类</h1><p>​  传统的半监督学习可分为三类：半监督分类[170]、[171]、半监督降维[172]、[173]和半监督聚类[13]、[174]、[175]。通常，无监督数据的约束被标记为“必须链接”和“不能链接”。具有“必须链接”约束的样本属于同一个集群，而具有“不能链接”约束的样本属于不同的集群。大多数半监督聚类目标是无监督聚类损失和约束损失的结合。<br/>​  半监督深度聚类技术还没有得到很好的研究。在这里，我们介绍了几个具有代表性的方法。这些工作使用不同的方法将关系约束和神经网络相结合，以获得更好的聚类性能。我们在表三中总结了这些方法。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218220853997.png"alt="image-20241218220853997" /><figcaption aria-hidden="true">image-20241218220853997</figcaption></figure><p>​  半监督DEC（SDEC）[165]是基于DEC[43]的，并在特征学习过程中加入了成对约束。其损失函数被定义为 <spanclass="math display">\[\operatorname{Loss}=\operatorname{KL}(S\|R)+\lambda{\frac{1}{n}}\sum_{i=1}^{n}\sum_{k=1}^{n}a_{ij}\left\|z_{i}-z_{j}\right\|^{2}\]</span>​  其中，λ是一个权衡参数。如果<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>被分配给同一集群，则<spanclass="math inline">\(a_{i j} = 1\)</span>，如果<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>满足不能链接约束，则<spanclass="math inline">\(a_{i j} = -1\)</span>，否则<spanclass="math inline">\(a_{i j} =0\)</span>。如损失函数所示，它由两部分组成。第一部分是KL散度损失，在第三-A节中解释。第二部分是半监督损失，表示嵌入式特征<spanclass="math inline">\(\{z_{i}\}_{i=1}^{n}\)</span>与参数<spanclass="math inline">\(a_{i j}\)</span>之间的一致性。直观地说，如果<spanclass="math inline">\(a_{i j} = 1\)</span>，为了最小化损失函数，<spanclass="math inline">\(\|z_{i}-z_{j}\|^{2}\)</span>应该很小。相比之下，如果<spanclass="math inline">\(a_{i j} = -1\)</span>，为了最小化损失，<spanclass="math inline">\(\|z_{i}-z_{j}\|^{2}\)</span>应该很大，这意味着zi与zj在潜在空间Z中是分开的。<br/>​  和SDEC一样，大多数半监督直流方法都是基于无监督直流方法的。通过增加半监督损失，可以直接将无监督直流方法扩展为半监督直流方法。与无监督直流方法相比，数据中额外的半监督信息可以帮助神经网络提取更适合聚类的特征。也有一些工作集中于将现有的半监督聚类方法扩展到深度学习版本。例如，基于DEC（SSLDEC）的半监督学习和深度约束聚类（DECC）[167]的特征提取过程都是基于DEC的。他们的训练过程类似于半监督的k-means[174]，它通过交替使用已标记和未标记的数据样本来学习特征表示。在训练过程中，算法使用标记样本来保持模型的一致性，并选择高度置信度的未标记样本作为新标记样本来调整网络。基于神经网络[168]的半监督聚类结合了<em>k</em>-means损失和成对散度，同时学习聚类中心以及语义上有意义的特征表示。GDAN[169]利用实例识别准则，通过借口任务获取域不变特征。随后，GDAN通过语义邻居聚类，专门关注高级语义特征，对齐这两个领域。</p><h1 id="v.-deep-mvc">V. DEEP MVC</h1><p>​  上述深度聚类方法只能处理单视图数据。在实际的聚类任务中，输入数据通常有多个视图。例如，同一主题的报告可以用不同的语言来表达，同一只狗可以用相机从不同的角度捕捉到，相同的单词可以用不同的书写风格的人来书写。提出了MVC方法[18]，[176]，[177]，[178]，[179]，[180]，[181]，[182]，[183]，[184]，[185]，利用多个视图之间的互补信息来提高聚类性能。<br/>​  近年来，深度学习在MVC中的应用一直是[186]、[187]、[188]、[189]、[190]等领域的热点。这些深度MVC算法专注于解决具有不同形式的输入数据的聚类问题。由于这些方法中使用的网络结构都是自编码器，因此我们根据采用的聚类理论基础将其分为三类：基于dec、基于子空间聚类和基于gnn。它们汇总见表四。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218221528005.png"alt="image-20241218221528005" /><figcaption aria-hidden="true">image-20241218221528005</figcaption></figure><h2 id="a.-基于dec">A. 基于DEC</h2><h2 id="b.-基于子空间聚类">B. 基于子空间聚类</h2><h2 id="c.-基于gnn的">C. 基于GNN的</h2><h1 id="vi.-基于迁移学习的深度聚类">VI. 基于迁移学习的深度聚类</h1><p>​  迁移学习已经成为一种新的学习框架，以解决训练和测试数据来自不同的特征空间或分布[225]的问题。对于复杂的数据，如有噪声视频的高分辨率真实图片，传统的聚类方法即使是深度聚类方法都不能很好地工作，因为特征空间的高维性，没有统一的标准来保证聚类过程。迁移学习通过传输信息为这些问题提供了新的解决方案，这些信息从源域获得附加信息，以指导目标域的聚类过程。在早期，深度域自适应的思想简单而清晰，如利用源域分类损失的深度重构-分类网络和目标域重构分类损失（DRCNs）[226]。这两个域共享相同的特征提取器。随着DNN的发展，我们现在有了更先进的知识转移方法。<br/>​  在本节中，我们将介绍一些关于聚类的迁移学习工作，它被分为两部分。第一部分是“基于DNN的”，第二部分是“基于gan的”。它们总结在表V中。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218221816576.png"alt="image-20241218221816576" /><figcaption aria-hidden="true">image-20241218221816576</figcaption></figure><h2 id="a.-基于dnn">A. 基于DNN</h2><h2 id="b.-基于gan">B. 基于GAN</h2><h1 id="vii.-深度聚类的未来发展方向">VII. 深度聚类的未来发展方向</h1><p>​  基于上述文献的综述和分析，DC已被应用于几个领域，我们重视几个值得进一步研究的方面。</p><ol type="1"><li><p>理论探索：<br/>  虽然通过针对特定问题的解决需求设计更复杂的深度聚类方法，已经实现了显著的聚类性能，但对于如何定性分析特征提取和聚类损失对最终聚类的影响，目前还没有可靠的理论分析。因此，探索深度聚类优化的理论基础，对指导该领域的进一步研究具有重要意义。</p></li><li><p>深度聚类：</p><p>​  由于大量数据带来的复杂性，大多数现有的深度聚类模型都是为特定的数据集设计的。来自不同来源和形式的复杂数据给聚类带来了更多的不确定性和挑战。目前，需要深度学习和图学习来解决复杂的数据处理问题。</p></li><li><p>模型效率：</p><p>​  深度聚类算法需要大量的样本来进行训练。因此，在小样本数据集中，DC容易发生过拟合，这导致聚类效应降低，模型的泛化性能降低。另一方面，具有大规模数据的深度聚类算法具有较高的计算复杂度，因此可以采用模型结构优化和模型压缩技术，以减少模型的计算负荷，并在实际应用条件下提高效率。</p></li><li><p>多视图数据融合：</p><p>​  在实际应用场景中，聚类通常不仅使用单个图像信息，还使用可用的文本和语音信息。然而，目前大多数深度聚类算法只能使用一种信息，不能很好地利用现有的信息。后续的研究可以考虑充分整合两个或两个以上视图的信息，充分利用不同视图数据的一致性和互补性，以提高聚类效果。此外，如何在过滤噪声的同时结合不同视图的特征，以确保更好的视图质量还需要解决。</p></li><li><p>基于图学习的深度聚类：<br/>  在现实中，大量的数据集以图结构的形式存储。图结构可以表示样本点之间的结构关联信息。如何有效地利用结构性信息对于提高聚类性能尤为重要。无论是单视图深度聚类还是多视图深度聚类的广泛应用，现有的基于图学习的聚类方法仍然存在一些问题，如图结构信息没有得到充分利用，不同视图的差异和重要性。因此，对复杂图结构信息的有效分析，特别是合理利用图结构信息来完成聚类任务，还需要进一步的探索。</p></li></ol><h1 id="viii.-深度聚类方法的总结">VIII. 深度聚类方法的总结</h1><p>​  本文介绍了深度聚类领域的最新研究进展。这主要是一种数据结构：单视图、半监督、多视图和迁移学习。单视图方法是我们调查中最重要的部分，它继承了传统聚类方法的问题设置。我们系统地区分了具有数据源的聚类方法，并根据其所基于的网络进一步介绍了它们。在这些网络中，基于DAE的方法和基于DNN的方法较早被提出，但可能由于其在真实数据集上的性能较差而受到限制。与基于DAE和基于DNN的方法相比，基于VAE和基于GAN的方法近年来因其强大的特征提取和样本生成能力而备受关注。图神经网络是近年来最受欢迎的网络之一，特别是在社区发现问题中。因此，我们也总结了基于GNN的聚类方法。随着互联网的发展，聚类数据存在不同的应用场景，因此我们总结了一些存在不同问题设置的聚类方法。半监督聚类方法用约束来聚类数据，这些约束可以通过添加约束损失从单视图聚类方法中发展出来。多视图聚类方法使用不同视图的信息作为补充。它已被广泛地应用于传统的神经网络和图神经网络中。迁移学习可以将已标记域的知识转移到未标记域。我们介绍了基于迁移学习的两种类型网络的聚类方法：DNN和GAN。基于DNN的方法侧重于两个领域的测量策略，而基于GAN的方法使用鉴别器来拟合测量策略。大多数深度聚类方法的复杂性可以随着数据大小n的线性扩展，使其适合于大规模的现实应用，如社交网络、生物信息学和电子商务。</p>]]></content>
      
      
      <categories>
          
          <category> 聚类 </category>
          
          <category> K-means </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>聚类方法合集</title>
      <link href="/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/"/>
      <url>/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="clusterlookup">1. ClusterLookup</h1><p>方法来源：STEGO: Unsupervised Semantic Segmentation by DistillingFeature Correspondences</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class ClusterLookup(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dim: int, n_classes: int):</span><br><span class="line">        super(ClusterLookup, self).__init__()</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.clusters = torch.nn.Parameter(torch.randn(n_classes, dim))</span><br><span class="line"></span><br><span class="line">    def reset_parameters(self):</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            self.clusters.copy_(torch.randn(self.n_classes, self.dim))</span><br><span class="line"></span><br><span class="line">    def forward(self, x, alpha, log_probs=False):</span><br><span class="line">        normed_clusters = F.normalize(self.clusters, dim=1)</span><br><span class="line">        normed_features = F.normalize(x, dim=1)</span><br><span class="line">        inner_products = torch.einsum(&quot;bchw,nc-&gt;bnhw&quot;, normed_features, normed_clusters)</span><br><span class="line"></span><br><span class="line">        if alpha is None:</span><br><span class="line">            cluster_probs = F.one_hot(torch.argmax(inner_products, dim=1), self.clusters.shape[0]) \</span><br><span class="line">                .permute(0, 3, 1, 2).to(torch.float32)</span><br><span class="line">        else:</span><br><span class="line">            cluster_probs = nn.functional.softmax(inner_products * alpha, dim=1)</span><br><span class="line"></span><br><span class="line">        cluster_loss = -(cluster_probs * inner_products).sum(1).mean()</span><br><span class="line">        if log_probs:</span><br><span class="line">            return nn.functional.log_softmax(inner_products * alpha, dim=1)</span><br><span class="line">        else:</span><br><span class="line">            return cluster_loss, cluster_probs</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>摘要：</strong>无监督语义分割的目的是在没有任何形式的注释的情况下，将具有语义意义的类别发现和定位。为了解决这个任务，算法必须为每个像素产生既具有语义意义又足够紧凑的特征，以形成不同的簇。与以前使用单一的端到端框架来实现这一点的工作不同，我们建议将特征学习从簇集群紧化中分离出来。根据经验，我们表明，当前的无监督特征学习框架已经产生了密集的特征，其相关性是语义一致的。这一观察结果促使我们设计STEGO（基于能量的图优化的自监督变换器，<strong>S</strong>elf-supervised<strong>T</strong>ransformer with <strong>E</strong>nergy-based<strong>G</strong>raph<strong>O</strong>ptimization），这是一个新的框架，将无监督特征提取为高质量的离散语义标签。STEGO的核心是一种新的对比损失函数，它鼓励特征形成紧凑的集群，同时保持它们在整个语料库中的关系。STEGO在CocoStuff（+14mIoU）和城市景观（+9mIoU）语义分割挑战上，都比之前的技术水平有了显著的改进。</p><p><strong>引言：</strong>与之前的方法相比，我们利用了无监督特征学习框架中的预训练特征，并专注于将它们提取成一个紧凑和离散的结构，同时保持它们在图像语料库中的关系。这是由于观察到无监督特征之间的相关性，如DINO学习到的特征（Caronet al.，2021），在同一图像内和跨图像集合的语义上已经是一致的。</p><p>因此，我们引入了STEGO（基于能量的自监督变压器），它能够在没有人工监督的情况下联合发现和分割对象。STEGO利用一种新的对比损失，将预先训练过的无监督视觉特征提取为语义簇。STEGO大大改进了现有技术，是缩小与监督分割系统的差距的相当大的一步。我们包括一个简短的视频，详细介绍了在https://aka.ms/stego-video的工作。具体来说，我们做出了以下贡献：</p><ul><li>结果表明，无监督深度网络特征与真实语义标签基本一致。</li><li>介绍了一种新的基于转换器的无监督语义分割架构STEGO。</li><li>证明STEGO在协同（+14 mIoU）和城市景观（+9mIoU）分割挑战上都取得了最先进的性能。</li><li>通过对CocoStuff数据集的消融研究来证明STEGO的设计。</li></ul><p><strong>方法：</strong></p><p>基于特征对应关系预测类的共现性</p><p>​  自我监督视觉特征学习的最新进展已经产生了具有强大的和语义相关的特征的方法，从而改进了各种下游任务。虽然大多数研究的目标是为图像生成单个向量，但许多研究表明，中间密集的特征是语义相关的（汉密尔顿等人，2021；Collins等人，2018；Zhou等人，2016）。为了使用这些信息，我们将重点放在密集特征图之间的“相关性体积”（Teed&amp;Deng，2020）上。对于卷积架构或转换器架构，这些密集的特征映射可以是特定层的激活映射。此外，变压器中的Q、K或V矩阵也可以作为候选特征，尽管我们发现这些注意张量在实践中表现得不太好。更正式地说，假设f∈RCHW，g∈RCIJ是两种不同图像的特征张量，其中C表示通道维数，（H，W），（I，J）表示空间维数。我们形成了特征对应张量：<spanclass="math display">\[F_{hwij}:=\sum_{c}\frac{f_{chw}}{|f_{hw}|}\frac{g_{cij}}{|g_{ij}|},\]</span>​  其项表示特征张量f的空间位置（h、w）处的特征与特征张量g的位置（i、j）之间的余弦相似度。在特殊情况下，这些对应度量同一图像的两个区域之间的相似性。</p><h1 id="softkmeans">2. softkmeans</h1>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</title>
      <link href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/"/>
      <url>/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/</url>
      
        <content type="html"><![CDATA[<center>Attentive and Contrastive Image Manipulation Localization With BoundaryGuidance <a href="https://ieeexplore.ieee.org/document/10589438"><imgsrc="https://img.shields.io/badge/TIFS-2024-orange" alt="TIFS" /></a></center><center>Wenxi Liu , <em>Member, IEEE</em>, Hao Zhang , Xinyang Lin , Qing Zhang, Qi Li , Xiaoxiang Liu , Ying Cao</center><h1 id="摘要">摘要</h1><p>​  近年来，图像生成技术的快速发展，导致了对篡改图像的广泛滥用，导致了信任危机，影响了社会公平。因此，我们工作的目标是检测和定位图像中被篡改的区域。许多基于深度学习的方法已经被提出来解决这一问题，但它们很难处理手动微调到图像背景的篡改区域。通过观察缓和区域的边界对篡改和非篡改部分的分离至关重要，我们提出了一种新的边界引导的图像操作检测方法，它引入了利用篡改区域边界信息的固有偏差。我们的模型采用编译码器结构，采用多尺度定位掩码预测，并通过注意机制和对比学习来引导下利用先验边界知识。特别地，我们的模型因为如下原因是独特的，1)我们在网络解码器中提出了一个边界感知注意模块，该模块预测被篡改区域的边界，从而将其作为关键的上下文线索来促进定位；2)我们提出了一种多尺度的对比学习方案，具有新的边界引导采样策略，从而产生更多的区别定位特征。我们在几个公共基准上的最新表现证明了我们的模型相对于之前的作品的优越性。<br/>​  索引术语-图像操作检测/定位。</p><h1 id="引言">1. 引言</h1><p>​  我们的论文的目标是在像素级别上定位不同类型的图像操作（包括拼接、复制-移动和删除）。主要的挑战在于难以区分被篡改和未被篡改的区域，特别是被篡改的区域是从原始图像中复制的，它们被仔细地微调。因此，被篡改区域和未被篡改区域之间的差异变得很小。之前的方法旨在学习特定于任务的显著特征[6]，[7]，[8]，[9]，但要么它们只能处理特定的操作类型[10]，[11]，[12]，[13]，[14]，[15]，要么它们很容易被精心操作的图像混淆。<br/>​  在被篡改的图像中，被篡改区域的边界是分离被篡改和未被篡改像素的关键位置，在定位被篡改区域时应特别注意并明确利用这一点。然而，如何利用这些边界信息来提高检测被篡改图像区域的性能仍有待探索。<br/>​  在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。首先，为了进一步增强操作定位，我们鼓励该模型关注一个被篡改区域周围的边界，其中经常存在非自然的混合。其次，受对比学习[16]、[17]、[18]、[19]、[20]的启发，我们寻求学习一个特征空间，即篡改区域内的点远离篡改区域边界附近的非调和区域点，以获得更强大的特性来定位篡改区域。<br/>​  与之前天真地利用边界信息[21]的工作相比，如联合预测篡改边界和掩模（见图5），我们的专注和对比的方法提供了一种新颖的、更复杂的利用边界信息的方法，并被证明比之前的方法更有效。<br/>​  在注意方面，在我们的框架的解码层中，我们提出了一种新的基于交叉注意的边界感知模块，旨在提取图像中被篡改区域的边界，从而使模型进一步集中于被篡改区域的边界。特别是，边界感知注意模块利用跳连编码特征与前一层解码特征的相关性，提取被篡改区域的边界，进一步用于生成图像篡改定位的掩模。<br/>​  在特征学习方面，我们提出的模型是基于一个典型的编解码器架构及其特征学习监督由一个新颖的对比目标函数[16]，[22]，[23]，表示为边界引导篡改对比损失，为了推动分开特征采样的篡改和非篡改区域，从而学习更多的区别特征表示。为此，我们采用边界引导的采样策略来收集负训练对，其中我们在被篡改区域的边界周围采样负样本，而不是整个非被篡改区域。该采样方案不仅鼓励模型关注存在非自然混合的边界区域，而且减轻了未篡改区域内巨大变化引起的干扰（见图1中的可视化特征）。<br/>​  为了进行评估，我们在几个公共数据集上进行了实验，包括CASIA[24]、Conbyea[25]、Coverage[26]和NIST16[27]。通过将我们的方法与之前的方法进行比较，证明了我们提出的模型可以达到最先进的性能。总之，我们的工作贡献包括：</p><ul><li>我们提出了一种新的边界引导图像篡改定位模型，该模型通过精心设计的注意力和对比学习机制充分利用被篡改区域的边界信息，而不是以往工作中使用边界信息的简单策略。</li><li>我们在框架的解码器中引入了一个边界感知注意模块，旨在指导模型通过提取被篡改区域的边界来强调图像操作的非自然混合。</li><li>我们提出了一种边界引导的篡改对比损失，鼓励模型将样本的边缘从篡改和非篡改区域扩大到最大的程度。</li><li>我们在几个基准测试上对我们的方法与现有的方法进行了广泛的评估和比较，并表明我们的方法达到了最先进的性能。</li></ul><h1 id="相关工作">2. 相关工作</h1><p>​  在本节中，我们将介绍有关图像操作检测/定位、深度伪造检测和对比学习的相关文献。</p><h2 id="a.-图像操作检测定位">A. 图像操作检测/定位</h2><p>​  由于操作特定的图像区域不可避免地会在被篡改区域与其周围区域之间留下痕迹，因此有几种方法利用边界信息来有利于操作检测[5]、[21]、[35]。多任务全卷积网络（MFCN）[35]提出利用两个输出分支来定位剪接区域的边界。作为一个基于gan的模型，GSR-Net[21]通过合成现有数据集的篡改图像来学习检测图像操作，通过篡改区域及其边界共同监督。参考文献[5]提出了一个双分支网络MVSS-Net，它融合了噪声分布和通过Sobel提取的边缘信息来完成图像操作定位。我们的方法共享一个寻求利用篡改边界信息的高级思想，但探索了两种新的方法，利用对比学习和注意机制来纳入边界先验，这在之前的图像篡改检测的工作中没有研究过。</p><h2 id="b.-深度伪造检测">B. 深度伪造检测</h2><p>​  近年来，随着生成模型的发展，深度伪造检测的任务已经引起了[40]、[41]、[42]、[43]、[44]、[45]等研究者的关注。它的目的是识别人脸的表情甚至身份被篡改的图像。从本质上，深度伪造检测解决了一个图像级的二值分类问题。与深度伪造检测任务不同，我们的图像操作定位任务需要估计被篡改的图像区域的位置，这是一个像素级的预测任务。</p><h2 id="c.-对比学习">C. 对比学习</h2><p>​  无监督/自监督学习方法[20]、[22]、[46]、[47]一般包括借口任务和损失函数两个方面。它们都致力于更好地学习数据表示。近年来，对比学习损失在[16]、[22]、[23]、[48]等方面取得了显著进展。这些方法通过从正对中关闭样本并将样本从负对中推开来学习表征。参考[49]使用一个内存库来存储实例类表示向量，并通过区分不同的实例和特征表示来提出实例级对比学习。其他工作[50]，[51]探索选择一批中的阳性和阴性样本，而不是一个记忆库。MoCo[23]提出了无监督的视觉表示学习，它从比较学习的角度构建了一个带有队列和移动平均编码器的动态字典。对于图像操作，最近的一项工作，CFL-Net[52]，提出使用对比度学习来分离未被篡改和被篡改的补丁嵌入的分布。相比之下，我们提出了一种边界引导的抽样策略，以寻找更多信息的负对，使我们学习的特征更具鉴别性。</p><h1 id="我们的方法">3. 我们的方法</h1><h2 id="a.-网络概述">A. 网络概述</h2><p>​  我们工作的目标是在像素级上检测和定位被篡改的区域。我们所提出的模型的体系结构如图2所示。<br/><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /></p><p>图2。左图：我们的框架的概述。将输入篡改图像的编码特征通过一个混合注意模块和多个边界感知注意模块来预测被篡改区域和边界的多尺度掩模。每个尺度的解码器还通过边界引导的篡改对比损失进行监督。右图：边界引导的篡改对比损失。给定解码器的特征图，采用对比学习方法对属于同一区域（即篡改或未篡改区域）的点特征进行分组，同时分离不同区域的点特征。点特征的采样的篡改区域被限制在外部边界区域的篡改区域和硬对挖掘方法用于使模型关注的类型难以处理积极对两个遥远的样本（硬正对）和负对与两个相近的样本（硬负对）。</p><p>​  给定一个经过处理的图像<spanclass="math inline">\(I\)</span>作为输入，我们使用ResNet-50[53]作为骨干来提取多尺度的视觉特征，<span class="math inline">\(X_i(i =\{1, . . .,S\})\)</span>。然后，将特征输入到由通道注意块和空间注意块连续级联组成的混合注意模块中，以便转换特征，从而在传递到解码器之前预先定位潜在的被篡改区域。基于[54]、[55]、[56]和[57]，我们采用了混合注意模块，可以对最深的编码特征<spanclass="math inline">\(X_S\)</span>沿空间维度和通道的长期依赖关系进行建模。具体地说，它由通道注意块<spanclass="math inline">\(F_{ch}\)</span>和空间注意块<spanclass="math inline">\(F_{sp}\)</span>依次级联组成，分别通过沿信道和空间维度的自注意方案实现。之后，<spanclass="math inline">\(X_S\)</span>将与编码的特征结合，在通过解码器之前，获得<spanclass="math inline">\(\hat{X}_{S}\)</span>，即<spanclass="math inline">\(\hat{X}_{S}=\mathrm{Concat}(X_{S}, F_{ch}(X_{S}),F_{sp}(F_{ch}(X_{S})))\)</span>。<br/>​  在接下来的解码层中，特征不仅被上采样，还与跨尺度特征交互，通过边界感知注意模块定位被篡改区域的边界。每个尺度的边界感知注意模块将同时估计被篡改的区域掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>及其边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。另一方面，为了鼓励模型集中于篡改区域的边界，我们提出了一种基于边界引导采样策略的边界引导篡改对比损失，有利于区分篡改区域和非篡改区域。为了更直观地描述本文中使用的所有符号，我们在表一中列出了所有的符号及其内涵。在下面的章节中，我们将详细阐述我们的边界感知注意模块和边界引导的篡改对比损失。</p><h2 id="b.-边界感知注意力学习">B. 边界感知注意力学习</h2><p>​  操作检测和定位的关键是发生非自然混合的被篡改区域的边界。对被篡改区域边界的准确定位可以有效地帮助被篡改区域的定位。在我们的网络的解码器中，我们合并了所谓的边界感知注意模块<spanclass="math inline">\(F_{ba}\)</span>，专门旨在估计边界。<br/>​  为了定位边界，我们不仅需要来自前一层的特征，而且还需要具有语义和细节的特征。随着解码特征的空间维数的增加，需要更详细的信息。因此，受类似unet的网络结构[58]的启发，我们利用相同尺度上的编码特征<spanclass="math inline">\(X_i\)</span>，以及前一层的解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>，来促进边界定位。边界感知注意模块有两个输出：1)预测的边界<spanclass="math inline">\(\hat{C}_{i}\)</span>和2)将被传播到下一层的特征，并用于生成第<spanclass="math inline">\(i\)</span>个尺度的掩模，<spanclass="math inline">\(\hat{M}_{i}\)</span>。这个过程可以表示如下： <spanclass="math display">\[\begin{aligned}\ [\tilde{X}_{i},\hat{C}_{i}]&amp;=F_{ba}(\hat{X}_{i+1},X_{i}), \\\hat{X}_{i}&amp;=\mathrm{Concat}(\tilde{X}_{i},\mathrm{Upsample}(\hat{X}_{i+1})),\\\hat{M}_{i}&amp; =\mathrm{Conv}(\hat{X}_{i}).\end{aligned}\]</span><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png"alt="image-20240910222829501" /></p><p>​  在图3中，受[59]、[60]、[61]和[62]的启发，我们设计了边界感知注意模块的结构。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910112215485.png"alt="image-20240910112215485" /><figcaption aria-hidden="true">image-20240910112215485</figcaption></figure><p>图3。边界感知注意模块的说明。<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>中选择的像素级特征分别用深绿色和深红色表示。通过交叉注意模块，我们得到了增强的特征<spanclass="math inline">\(\hat{v}_{i}\)</span>，用蓝色表示，然后将其映射回<spanclass="math inline">\(X_i\)</span>，同时保持剩余的特征不变。</p><p>​  首先，我们对编码的特征<spanclass="math inline">\(X_i\)</span>进行降采样，以匹配来自前一层解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的维数，并将它们连接起来。为了提取特征的边界信息，我们通过平均池化、卷积和sigmoid算子的组合对特征进行平滑，并让原始特征减去平滑后的特征，得到与篡改边界相关的高频信息。该过程可以描述如下：<spanclass="math display">\[\begin{aligned}&amp;\tilde{H}_{i}=\mathrm{Concat}(\mathrm{Downsample}(X_{i}),\hat{X}_{i+1}),\\&amp;H_{i}=\tilde{H}_{i}-\tilde{H}_{i}\odot\mathrm{Sigmoid}(\mathrm{Conv}(\mathrm{AvgPool}(\tilde{H}_{i}))),\end{aligned}\]</span>​  其中，<span class="math inline">\(\hat{H}_{i}\)</span>是<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的组合特性。<spanclass="math inline">\(\odot\)</span>表示元素级的乘法。利用所获得的特征<spanclass="math inline">\(H_i\)</span>来预测边界，即<spanclass="math inline">\(\hat{C}_{i}=\mathrm{Conv}(H_{i})\)</span>。<br/>​  接下来，需要使用这些特征来生成篡改掩码<spanclass="math inline">\(\hat{M}_{i}\)</span>并传递到下一层，因此应该利用与篡改最相关的信息。在这里，我们使用一个特征选择块<spanclass="math inline">\(F_{fs}\)</span>，从预测的边界图<spanclass="math inline">\(\hat{C}_{i}\)</span>中找到前K个高置信像素的索引。然后，这些索引引导模型对<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>对应的像素级特征进行采样，分别记为<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>。 <spanclass="math display">\[\mathbf{v}_{i}=F_{fs}(X_{i},\mathrm{TopK}(\hat{C}_{i})),\mathbf{\hat{v}}_{i+1}=F_{fs}(\hat{X}_{i+1},\mathrm{TopK}(\hat{C}_{i})),\]</span>​  其中K实际上设为32。由于<span class="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>是定位边界的最关键的特征，因此我们采用了一个交叉注意模块，其中<spanclass="math inline">\(v_i\)</span>作为查询，<spanclass="math inline">\(\hat{v}_{i+1}\)</span>作为键/值，如下所示： <spanclass="math display">\[\hat{\mathbf{v}}_i=\mathbf{v}_i+\text{Softmax}(\frac{\mathbf{v}_i\hat{\mathbf{v}}_{i+1}^T}{\sqrt{d_k}})\hat{\mathbf{v}}_{i+1},\]</span>​  其中<span class="math inline">\(d_k\)</span>表示<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>的维数。最后，我们根据所选择的索引将<spanclass="math inline">\(\hat{v}_{i}\)</span>分散到<spanclass="math inline">\(X_i\)</span>中，并保持未被选择的位置不变。<br/>​  损失函数：我们应用多尺度损失来学习具有代表性的多尺度特征，以进行更精确的预测。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /><figcaption aria-hidden="true">image-20240910110200920</figcaption></figure><p>​  在实践中，如图2所示，所有四个尺度都生成了操作掩模，而仅对两个尺度的中间边界掩模进行预测。对于具有最深特征的预测掩模，即<spanclass="math inline">\(\hat{M}_{S}\)</span>，我们应用二进制交叉熵（BCE）损失和IoU损失进行监督。此外，我们采用加权二值交叉熵损失[63]和加权IoU损失[63]来监督预测的掩模<spanclass="math inline">\(\hat{M}_{i}=(i\neq S)\)</span>和边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。边界的groundtruth值是通过从膨胀图像中减去二值地面真实掩模的侵蚀而得到的。具体来说，我们应用核大小为5×5、步幅为1的最大池化操作来进行图像扩张和侵蚀。</p><h2 id="c.-边界引导下的篡改对比学习">C. 边界引导下的篡改对比学习</h2><p>​  一旦一幅图像被篡改，其被篡改的区域可能会显示出与未被篡改的区域略有不同的视觉统计数据，例如，不自然的照明信息或不一致的噪声分布。扩大学习特征空间中篡改区域和非篡改区域之间的差异，可以有效地提高学习特征的鉴别能力，有利于篡改区域的定位。<br/>​  基于此，我们采用对比学习，目的是学习区分特征表示，可以区分篡改和非篡改部分。我们选择对每个尺度，对直接用于预测最终掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>的特征图进行对比学习，我们根据经验发现它的效果很好。特别是，在训练过程中，我们以点的方式对特征图进行空间采样，从篡改和未篡改区域收集样本。在这里，一个样本指的是在特征映射的特定位置上的一个特征向量。然后，我们最小化一个对比损失函数，以减少同一区域内样本之间的距离（即正对），同时增加不同区域内样本之间的距离（即负对）。为了进一步提高学习特征的鲁棒性和可鉴别性，我们引入了一种边界引导的采样策略来构造信息更丰富的负对。<br/>​  1)边界引导篡改对比损失：从解码器的每个尺度上，我们采用一个对比损失，它由两项组成，即<spanclass="math inline">\(\mathcal{L}^{TC}=\mathcal{L}^{+}+\mathcal{L}^{-}\)</span>，其中<spanclass="math inline">\(\mathcal{L}^{+}\)</span>和<spanclass="math inline">\(\mathcal{L}^{-}\)</span>分别是正对损失和负对损失。在形式上，<spanclass="math inline">\(\mathcal{L}^{+}\)</span>被写为： <spanclass="math display">\[\begin{aligned}\mathcal{L}^{+}&amp;=\frac{1}{|\mathcal{H}_{t}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{t}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n}))\\&amp;+\frac{1}{|\mathcal{H}_{n}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{n}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\end{aligned}\]</span>​  其中，<spanclass="math inline">\(\mathrm{Sim}(\cdot,\cdot)\)</span>表示两个特征向量之间的余弦距离，<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>是来自被篡改区域的一组正对，而<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>包含从未被篡改区域提取的正对。<spanclass="math inline">\((\mathbf{u}_{m},\mathbf{u}_{n})\)</span>表示被采样的特征向量对。<spanclass="math inline">\(\mathcal{L}^{+}\)</span>作为一种力，将特征空间中同一区域内的样本拉在一起。为了获得样本的正对，我们使用硬对挖掘策略，将在同一区域内但彼此远离的样本聚集在一起。具体来说，在来自被篡改或未被篡改区域的所有可能的样本对中，我们保留具有前L个最大距离的样本对，构成<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>。在实践中，L被设置为来自同一区域的所有可能样本对数量的一半。我们对<spanclass="math inline">\(\mathcal{L}^{-}\)</span>的定义如下： <spanclass="math display">\[\mathcal{L}^{-}=\frac{1}{|\mathcal{H}^{-}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}^{-}}-\log(1-\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\]</span>​  其中<spanclass="math inline">\(\mathcal{H}^{-}\)</span>是一个负对集，其中每对都由来自被篡改区域的查询样本和来自未被篡改区域的负样本组成。<spanclass="math inline">\(\mathcal{L}^{-}\)</span>旨在将来自不同地区的样本分开。<br/>​  2)边界引导采样策略：为了构建<spanclass="math inline">\(\mathcal{H}^{-}\)</span>，一种简单的方法是从一个未被篡改的区域随机抽取负样本。然而，在一个未被篡改的区域内，通常存在很大的差异。例如，未被篡改的区域可能占据图像的很大一部分，因此它可能包含大量分散各种物体或杂乱的背景。因此，绘制负样本的原生方法往往会降低模型的性能。因此，我们提出了一种新的采样策略，即我们在一个被篡改区域的边界附近绘制负样本，而不是从整个非调和区域（见图4中的示例）。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910140100074.png"alt="image-20240910140100074" /><figcaption aria-hidden="true">image-20240910140100074</figcaption></figure><p>图4。边界引导的抽样策略。蓝色点表示未篡改区域的采样特征，红色的表示被篡改区域的采样特征。</p><p>​  这种采样方法具有两个方面的优势：第一，约束边界区域内的负样本倾向于减少未篡改区域内方差大而造成的干扰；第二，篡改区域的边界周围经常发生非自然的混合，因此从该区域采样鼓励模型专注于边界区域，这与我们框架的其他组件，如边界感知注意模块。特别地，给定一个地真二值篡改区域掩模<spanclass="math inline">\(M\)</span>，我们对其进行图像扩展，得到一个放大的掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>，并将负样本限制为来自<spanclass="math inline">\(\hat{M}_{i}-M_i\)</span>指定的外部边界区域。最后，我们对<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>采用硬对挖掘方法，只保留距离最小的前L个负对来构造<spanclass="math inline">\(\mathcal{H}^{-}\)</span>。</p><h1 id="实验结果">4. 实验结果</h1><p>​  在本节中，我们将在几个公共基准上进行全面的实验，并将我们提出的方法与以前的最先进的方法进行比较。我们还分析了我们的模型的不同组件。</p><h2 id="a.-实施细节和数据集">A. 实施细节和数据集</h2><p>​  <strong>实施细节：</strong>我们使用Pytorch实现了我们的框架，并使用一个NVIDIA TitanxpGPU进行训练和测试。对于训练，所有输入的图像都被调整到512×512的分辨率，并通过随机的水平翻转、颜色抖动和裁剪来增强它们。我们采用ResNet50[53]作为骨干。在训练过程中，我们使用了Adam优化器[64]，其动量为0.9，重量衰减为5×10−4。我们将批处理大小设置为16，并使用多项式策略[65]调整学习率，基本学习率为1×10−5，幂次为0.9。为了进行测试，首先将输入图像的大小调整到512×512，用于网络推断，然后将输出映射的大小调整回输入图像的原始大小。<br/>​  <strong>数据集：</strong>为了评估模型的性能，我们在6个基准测试上进行了实验，包括CASIA[24]、NIST16[27]、Columbia[25]、Coverage[26]、Defacto[67]和一个真实世界的数据集IMD2020[68]。此外，我们还利用了[69]提出的合成数据集。这些数据集涵盖了不同类型的操作，包括拼接、复制-移动和删除。所有的数据集都提供了ground-truth的二进制掩码。</p><ul><li>CASIA[24]由两个子数据集组成，CASIAv1的数据为921张篡改图像和CASIAv2的篡改图像为5123张图像。篡改类型包括拼接和复制移动。裁剪后的区域通常是精心选择的，并应用后处理操作，使区域在视觉上逼真。</li><li>NIST16[27]包含564个被篡改的图像样本。所有三种操作类型都涉及到，它们还进行后处理以隐藏可见的痕迹。</li><li>Columbia[25]专注于拼接，它包含180张未压缩的图像。</li><li>Coverage[26]关注于包含100张图像的复制-移动操作。被操作的对象被手动裁剪以覆盖同一图像中的相似对象，并对它们进行后处理以去除可见的操作痕迹。</li><li>Defacto[67]是最近提出的一个大规模合成数据集，包含149k图像，这些图像从MS-COCO[70]中采样，并通过复制移动、拼接和绘制自动操作。</li><li>IMD2020[68]包含2010年真实的从互联网收集的真实操作图像，涉及所有三种操作类型。</li><li>合成训练数据集[69]包含了最初从[70]中收集到的大约100k张图像，涵盖了拼接、复制-移动和删除的操作类型。</li></ul><p>​  在以下小节的定性结果中，特征图显示在颜色图中，以较暖的颜色表示更多的关注，反之亦然。比较方法的预测篡改掩模在0到1的灰度图像中可视化。预测的掩模的每个像素都意味着被篡改的确定性。</p><h2 id="b.-与最先进的技术进行比较">B. 与最先进的技术进行比较</h2><h3 id="像素级篡改检测">像素级篡改检测</h3><p>​  在[5]和[34]之后，我们将我们提出的方法与两种实验设置下的几种最先进的方法进行了比较。<br/>​  在第一种设置中，在MVSS-Net[5]之后，所有的比较模型都是在真实数据上从头开始训练的，没有任何额外的合成数据集。我们采用CASIAv2[24]在其他公共基准上进行训练和测试，包括CASIAv1、NIST16、Columbia、Coverage、IMD20和DEFACTO。我们将我们的模型与之前的几种方法进行了比较，包括ManTra-Net[37]、HP-FCN [71]，CR-CNN [72]，GSR-Net [21]、SPAN [66]、CAT-Net[73]，MVSS-Net [5]和MVSS-Net++[74]。我们不与ObjectFormer[34]进行比较，因为它的代码是不公开的。表二显示了在[5]中报告的固定阈值为0.5的F1的测量结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910143404035.png"alt="image-20240910143404035" /><figcaption aria-hidden="true">image-20240910143404035</figcaption></figure><p>​  我们可以观察到，我们的模型在所有的数据集上都达到了最好的定位性能。我们注意到，GSR-Net还通过预测篡改的边界和掩码来利用其网络中的边界信息。我们从GSR-Net在所有基准上的显著改进表明，在利用边界信息方面，我们提出的注意和对比机制比简单的边界和掩模预测的优势。<br/>​  在第二种设置中，在[34]、[69]和[76]之后，我们首先在一个合成数据集上对每个模型进行预训练，然后在基准的训练集上进行微调，然后在基准的测试集上进行测试。比较方法包括RGB-N[38]、SPAN [66]、PSCC-Net [69]、ObjectFormer[34]、HiFi-Net [77]和ERMPC[76]，在各自的合成数据集上进行训练，在每个基准的训练集上进行微调，并在相应基准的测试集上进行测试。我们评估了所有的方法在CASIA，NIST16和Coverage。对于CASIA，来自CASIAv2的5123张图像用于微调，来自CASIAv1的921图像用于测试。对于NIST16，564张图像的NIST16，404张图像用于微调，160张图像用于测试。有100张图像的Coverage被分为75/25来进行微调和测试。对于这种设置，正如[34]中报道的，不同的方法使用自己的合成数据集，不同的内容和不同数量的图像从47k到100k不等。鉴于此，为了公平比较，我们从[69]的合成数据集中随机选择了60k张图像。关于三个基准测试的结果见表三。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910204820762.png"alt="image-20240910204820762" /><figcaption aria-hidden="true">image-20240910204820762</figcaption></figure><p>​  请注意，Columbia没有训练集，因此对它进行微调是不可能的。在CASIAv1和NIST16上，我们的方法大大优于所有比较方法，尽管我们的方法与SPAN和PSCCNet相比，该方法使用的数据要少得多。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205006572.png"alt="image-20240910205006572" /><figcaption aria-hidden="true">image-20240910205006572</figcaption></figure><p>​  通过深入研究Coverage上的失败情况（图12），我们发现我们的模型往往会在特定类型的图像上失败，在这些图像中，区域边界周围的操作痕迹被小心地擦去。对于图12中所示的所有失败案例，我们的模型的F1评分都相当低（小于35%）。通过从测试集中排除这三个图像，我们的方法的F1得分上升到78.1%。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205223378.png"alt="image-20240910205223378" /><figcaption aria-hidden="true">image-20240910205223378</figcaption></figure><p>​  图5显示了我们的方法与最先进的方法的定性比较。结果表明，我们的方法不仅可以更准确地定位被篡改的区域，而且还可以产生更清晰的边界（图5的前三行），这得益于边界感知的注意模块。此外，结果表明，我们的模型比其他模型对背景分心（图5的最后三行）更稳健，这是由于引入了抑制噪声的对比损失。</p><h3 id="图像级篡改检测">图像级篡改检测</h3><p>​  虽然我们的模型更关注像素级篡改定位任务，但它具有检测图像级篡改的能力。图像级篡改检测的目的是将输入图像分类为真实或篡改。我们遵循[5]的协议来运行一个图像级的篡改检测实验，其中我们的模型是在CASIAv2上进行训练的。考虑到CASIAv1和CASIAv2共享782张真实图像，我们从Corel[78]中随机抽取782张真实图像，以替换CASIAv1中的这些副本，从而得到数据集CASIAv1+。我们在表四中显示了三个基准的结果，CASIAv1+、Coverage和Columbia。在本实验中，我们将我们的模型与ManTraNet[37]、CR-CNN [72]、GSR-Net [21]、SPAN [66]和MVSS-Net[5]进行了比较。<br/>​  为了执行图像级篡改检测，我们修改了我们的模型，通过在特征<spanclass="math inline">\(\hat{X}_{s}\)</span>中添加一个图像分类头，以预测图像被篡改的概率。具体来说，图像分类头由一个CBR块、一个平均池化层和一个完全连接的层组成，其中CBR块是卷积、批处理归一化（BN）和ReLU的组合。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205812866.png"alt="image-20240910205812866" /><figcaption aria-hidden="true">image-20240910205812866</figcaption></figure><p>​  如表四所示，我们的方法在所有基准上都获得了最好的F1分数，我们的模型的AUC分数在所有方法中排名第二好，这表明我们的模型能够在像素级定位和图像级检测任务中都产生良好的性能。</p><h2 id="c.-对未篡改图像的定位">C. 对未篡改图像的定位</h2><p>​  我们还在未被篡改的图像上测试了我们的模型以进行像素级定位，并在图6中显示了定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205954595.png"alt="image-20240910205954595" /><figcaption aria-hidden="true">image-20240910205954595</figcaption></figure><p>​  用我们的方法预测的未被篡改图像的掩模在图6的前三行上几乎为空白，而比较的方法错误地检测到一些位置被篡改了。对于图6的最后两行，所有的方法都出现了一些假阳性。然而，我们的模型倾向于将假阳性限制在小的局部区域，而其他方法则倾向于将它们分散在整个图像上。此外，从图6的最后一行，我们观察到，我们的模型可能会误解到真实图像中的可疑区域。相比之下，MVSS-Net[5]倾向于对误检测的置信度较低。这将导致我们的模型有更高的机会将真实的图像误分类为被篡改，这部分解释了为什么我们的模型在B节的图像级操作检测实验中与MVSS-Net相比的AUC评分较低。</p><h2 id="d.-模型组件分析">D. 模型组件分析</h2><p>​  为了阐明单个组件的影响，我们评估了在不同配置下提出的模型。所有报告的结果都来自于CASIA数据集。</p><h3 id="边界感知注意力模块">边界感知注意力模块</h3><p>​  边界感知注意模块可以部署在解码器的每个尺度上来预测边界，因此我们验证了表v中的设计。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210222453.png"alt="image-20240910210222453" /><figcaption aria-hidden="true">image-20240910210222453</figcaption></figure><p>​  作为参考，我们有一个没有任何BAMs的基线模型，它可以达到53.9%的f1和87.4%的AUC。首先，我们在最深的尺度上部署BAM（即i=3），它打算将最小的分辨率转换为边界。正如观察到，添加一个BAM会导致更好的性能，这可以从获得3%的F1分数中得到暗示。其次，我们在第二个尺度（即i=2）上附加了另一个BAM，因此有两个协作的BAM用于预测不同尺度的边界。通过编码特征带来的更详细的信息，该模型能够以60.0%的F1和88.6%的AUC达到最佳性能。最后，当我们在所有尺度上加入三个算法算法时，性能明显下降，因为最低尺度的特征引入的噪声会对边界预测产生负面影响。<br/>​  为了演示我们的边界感知注意模块如何通过检测边界来帮助定位被篡改的区域，我们在图7中展示了一个特征图的可视化。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210329644.png"alt="image-20240910210329644" /><figcaption aria-hidden="true">image-20240910210329644</figcaption></figure><p>​  为了进行比较，我们还将MVSS-Net[5]中最后一个Sobel层所产生的特征可视化，它提取了与边缘相关的特征。图7中的结果表明，由于我们的边界感知注意模块，我们的方法可以更准确地定位篡改区域。<br/>​  此外，我们还在图8中给出了一些关于BAM有效性的例子。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210454382.png"alt="image-20240910210454382" /><figcaption aria-hidden="true">image-20240910210454382</figcaption></figure><p>​  从我们可以观察到，在没有BAM的帮助的情况下，估计的掩模在边界区域往往是不完整的，因为像素可以很好地融合到背景中，而且它们很难区分。虽然BAM模型可能不能完美地预测边界，但它足以提供上下文线索来推断整个被篡改的掩模。<br/>​  如图9所示，我们还展示了在不同数据集上与MVSS-Net[5]进行比较的定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210642783.png"alt="image-20240910210642783" /><figcaption aria-hidden="true">image-20240910210642783</figcaption></figure><p>​  为了验证所提出的边界感知注意模块的有效性，我们在第三列和第四列中可视化最终的掩模和预测的边界掩模。在前两行观察到，虽然MVSS-Net可以粗略预测被篡改的区域，但其预测包含较大的灰色区域，表明确定性较低。相比之下，我们的预测结果不仅可以准确地定位边界更清晰的被篡改区域，而且对被篡改区域具有更高的确定性。同时，在第四和第五行，MVSS-Net可能对真实的真实区域有假阴性，而在我们的预测中，背景区域几乎是黑色的。计算结果表明了我们所提模型的优越性。</p><h3 id="边界导向的篡改对比损失">边界导向的篡改对比损失</h3><p>​  回想一下，我们的边界引导篡改对比损失的目标是将特征嵌入与相同区域的距离拉近，同时将特征与不同区域的距离分开，即篡改和非篡改区域的距离，使它们更具区分性。首先，我们评估了在表六中关于如何部署损失的差异。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210828335.png"alt="image-20240910210828335" /><figcaption aria-hidden="true">image-20240910210828335</figcaption></figure><p>​  可以观察到，当我们在所有尺度上使用损失时，它通常会达到最优结果。相比之下，使用第二和第三等级（没有最高尺度）的AUC略有改善（88.9%），但F1略有下降（51.5%）。这是因为最高尺度的特征直接对应于最终的结果，因此它与操作检测的精度有关。如果不使用顶级规模的特性，一般的性能就会变得更糟。<br/>​  图10显示边界引导篡改对比损失对于篡改区域不明显的图像是有效的。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211059841.png"alt="image-20240910211059841" /><figcaption aria-hidden="true">image-20240910211059841</figcaption></figure><p>​  可以观察到的，在使用对比学习损失后，被篡改区域边界上的差异变大。此外，纹理图像的被篡改区域是某些物体的不完整部分。结果表明，我们的模型可以很好地推广到部分级篡改。<br/>​  此外，图11为应用多尺度对比学习损失后的不同尺度的特征图。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211157377.png"alt="image-20240910211157377" /><figcaption aria-hidden="true">image-20240910211157377</figcaption></figure><p>​  可以观察到，随着解码器规模的增加，对比学习损失以粗到细的方式定位篡改区域。在最好的尺度上，我们可以清楚地观察到被篡改区域周围边界的颜色变成蓝色，而被篡改区域的颜色变成红色，它们的大差异是由对比学习损失造成的。这种现象在第二尺度和第三尺度的特征上并不明显，因此第一尺度的特征为模型的改进提供了最大的增益。</p><h3 id="混合注意力模块">混合注意力模块</h3><p>​  我们还对我们的混合注意模块进行了消融研究，发现移除该模块会导致f1和AUC分别降低0.4%和2.8%。这说明了这个模块的重要性。</p><h3 id="边界引导的抽样策略">边界引导的抽样策略</h3><p>​  抽样策略在我们提出的对比损失中起着重要的作用。为了验证抽样策略的有效性，我们对表七中的CASIA数据集进行了实验。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211419997.png"alt="image-20240910211419997" /><figcaption aria-hidden="true">image-20240910211419997</figcaption></figure><p>​  最直接的方法是从被篡改区域和未被篡改区域中随机采样特征点。相比之下，我们对从篡改区域和非篡改区域随机抽取的点采用了硬对挖掘，从而提高了性能。然而，在非篡改区域内存在很大的差异，导致非篡改区域随机抽样的对比学习结果的鲁棒性。因此，我们将采样范围限制在篡改边界附近的未篡改区域内。可以观察到，在篡改边界附近的篡改和非篡改区域随机选择点，会导致负样本之间存在较大差异，导致网络性能下降。最后，我们将硬对挖掘策略与从边界区域随机抽取的样本相结合，获得了最优的性能。</p><h2 id="e.-超参数分析">E. 超参数分析</h2><p>​  我们从F1和AUC分析了边界引导篡改对比损失和CASIA数据集边界感知注意模块的超参数，如表八所示。这涉及到6个关键的超参数。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211517337.png"alt="image-20240910211517337" /><figcaption aria-hidden="true">image-20240910211517337</figcaption></figure><p>​  1)输入图像大小：训练数据集有不同大小的图像，其中大多数图像的分辨率为384×256，其他图像有不同的分辨率，如640×480,336×638,500×375。因此，我们遵循[5]的方法，将不同输入图像大小的输入图像调整为512，并找到我们选择的512×512。我们的×512实验结果最好，如表八所示。<br/>​  2)选择指标数K：作为我们提出的边界感知注意模块的关键，所选指标对应于与边界预测最相关的特征。我们将所选指标的个数表示为K，并将其设为16、32和64。如表八所示，当K为32时，我们得到的性能最好。<br/>​  3)生成GroundTruth边界的扩张和侵蚀数：生成一个细化的GroundTruth边界对于监督第三节B小块中边界感知注意模块的预测边界掩模<spanclass="math inline">\(\hat{C}_{i}\)</span>是必要的。该过程包括通过从GroundTruth掩模的膨胀中减去GroundTruth掩模的侵蚀来导出GroundTruth边界。我们试图将膨胀和侵蚀的数量从1增加到3，这导致地面真实边界越来越宽，并发现更窄的地面真实边界会得到更好的结果。<br/>​  4)未篡改采样区域的扩张数：为了执行边界引导采样策略，我们从篡改区域的扩张中减去篡改区域的GroundTruth掩模，得到篡改区域周围的未篡改边界区域，从中对未篡改区域的点特征进行采样。我们用不同的膨胀次数进行实验，发现3次的膨胀效果最好。<br/>​  5)样本数Z：对于边界引导采样策略，在确定正负样本对的采样区域后，随机选择一些构建正对集和负对集。为了获得这些样本对，我们需要分别从篡改区域和未篡改区域抽取一定数量的以Z表示的样本。根据经验，我们将Z设为250和500。对于包含小于Z像素的被篡改区域，我们对所有像素的特征进行采样。我们在表八中显示了结果。正如所观察到的，一个小的Z意味着我们对这两个区域都采样不足，因此可能不能充分利用样本。<br/>​  6)Top-L硬对：对于边界引导采样策略，我们采用硬对挖掘方法获得正负样本对，其中我们对Top-L硬对进行采样，并将L设置为整个实验中所有对数量的一半。我们用L=1和L=来实验所有对的数量。如表八所示，我们的上半部分策略取得了最好的性能。top-1策略容易被极端样本对误导，而全对策略在样本间存在较大差异，从而引入无关信息来分离篡改区域和非篡改区域。</p><h2 id="f.-鲁棒性评价">F. 鲁棒性评价</h2><p>​  根据[5]、[34]和[69]中的失真设置，我们对我们的网络进行了鲁棒性分析。具体来说，对CASIA数据集中的操作图像应用了不同的失真操作。此外，我们结合了任意两种畸变，其中从区间[0.25、0.78]分别、[3,15]、[3,15]和[50,100]中随机选择调整尺度、核大小、标准差和质量因子。<br/>​  最后，我们将各种扭曲结合在一起，作为“<em>Mixed</em>”。我们应用f1和AUC来测量定位性能。对输入图像应用畸变将不可避免地导致边界信息被损坏，从而导致性能下降。然而，与MVSS-Net相比，我们的方法对失真显示了更鲁棒的性能，如表九所示。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212202152.png"alt="image-20240910212202152" /><figcaption aria-hidden="true">image-20240910212202152</figcaption></figure><p>​  我们还分析了在CASIAv1+上关于各种失真的图像级检测的鲁棒性。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212225437.png"alt="image-20240910212225437" /><figcaption aria-hidden="true">image-20240910212225437</figcaption></figure><p>​  如表X所示，我们在调整大小和JPEG压缩失真方面的性能不如MVSS-Net。这可能是由于这些操作造成的边界信息被污染，严重影响了我们的边界感知注意模块。尽管如此，我们的模型仍然取得了合理的性能，并且我们的鲁棒性可以与MVSS-Net相当，甚至更优。</p><h2 id="g.-效率分析">G. 效率分析</h2><p>​  在计算复杂度方面，我们的方法实现了39.44个GFLOPs，明显低于MVSS-Net的163.57个GFLOPs。此外，我们使用具有24GBGPU内存的GeForce RTX3090来评估计算时间。使用相同的GPU，我们的完整模型需要0.014秒来处理一幅图像，比MVSS-Net快得多，后者需要0.038秒。</p><h1 id="结论和局限性">5. 结论和局限性</h1><p>​  在这项工作中，我们的目标是图像篡改定位问题。为此，我们提出了一种新的边界引导方法，其固有的倾向于通过注意机制和对比学习方案充分利用被篡改区域的边界信息。特别地，我们提出了一个具有边界感知能力的注意模块，它可以预测被篡改区域的边界，以迫使网络特别注意重要的边界区域。此外，我们引入了一种新的对比损失与边界引导抽样策略来学习更多的有区别的特征。我们证明，作为在CASIAv2上的训练，我们提出的模型在四个不同的基准上大大优于最先进的方法。此外，当在合成数据集上进行预训练时，我们的模型在现实基准上也显示了可比性或优越的通用性。<br/>​  我们在图12中显示了覆盖范围测试集上的失败情况。当被篡改边界的痕迹被仔细地擦去（图12的第一列）或类似的边界同时出现在被篡改的区域（图12的第二列）时，我们的模型可能难以区分被篡改区域的边界和未被篡改物体的边界。此外，如果被篡改的区域是两个相似对象的局部区域（图12的第二列和第三列），那么我们的方法可能会失败。在未来的工作中，我们将开发区分对象边界和被篡改区域边界的技术，旨在进一步提高我们的性能。<br/>​  此外，虽然这项工作的主要焦点是像素级的操作，但我们已经证明了我们的模型能够在图像级上检测操作。尽管通过简单地调整我们的模型来适应这种图像级的任务，已经取得了很好的性能，但肯定有一些进一步改进的空间。因此，未来研究的一个有趣的途径是专门为我们的模型定制图像级的检测问题，以优化性能，或探索使用我们的模型或它的一部分作为像素级和图像级任务的联合建模的主干。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 边界引导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning</title>
      <link href="/KaggleNet/"/>
      <url>/KaggleNet/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, 密码错了，检查一下好吗～" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="a8c015e377f48cb0aa6f4cb5636365ecf33bdd1d3b6866cf97c8ea0dd8f2cf1d">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, 这篇文章被加密了，请输入密码！</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust Image Forgery Detection over Online Social Network Shared Images</title>
      <link href="/ImageForensicsOSN/"/>
      <url>/ImageForensicsOSN/</url>
      
        <content type="html"><![CDATA[<center>Robust Image Forgery Detection over Online Social Network Shared Images</center><center>Haiwei Wu, Jiantao Zhou, Jinyu Tian, and Jun Liu</center><center>智慧城市物联网国家重点实验室</center><center>澳门大学计算机与信息科学系</center><center>{yc07912, jtzhou, yb77405, yc07453}<span class="citation"data-cites="um.edu.mo">@um.edu.mo</span></center><h1 id="摘要">摘要</h1><p>​  Photoshop和美图等图像编辑软件的滥用，导致数字图像的真实性受到质疑。与此同时，网络社交网络（OSNs）的广泛使用使其成为传输伪造图像、报道假新闻、传播谣言等的主要渠道。不幸的是，osn所采用的各种有损操作，如压缩和调整大小，给实现鲁棒的图像伪造检测带来了巨大的挑战。为了对抗OSN共享的伪造行为，本文提出了一种新的鲁棒训练方案。我们首先对osn引入的噪声进行了彻底的分析，并将其解耦为两部分，即可预测的噪声和看不见的噪声，它们分别建模。前者模拟了所公开的（已知）osn操作所引入的噪声，而后者的设计不仅完成前一个，而且还考虑了探测器本身的缺陷。然后，我们将建模的噪声合并到一个鲁棒的训练框架中，显著提高了图像伪造检测器的鲁棒性。大量的实验结果验证了该方案与几种最先进的竞争对手相比的优越性。最后，为了促进图像伪造检测的未来发展，我们基于四个现有数据集和三个最流行的osn建立了一个公共伪造数据集。所设计的探测器最近在一次证书伪造检测竞赛中排名第一1。源代码和数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上找到</p><h1 id="i.-引言">I. 引言</h1><p>​  很少有研究明确地解决在普遍存在的OSN平台上针对损耗操作的鲁棒伪造检测的设计。这样的主题非常重要，因为这些有损耗的操作会严重降低检测性能。如图1所示，最先进的算法[42]可以准确地检测到原始伪造文件中的伪造区域；但在处理通过脸书传输的伪造文件时，检测性能会严重下降。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904212142816.png"alt="image-20240904212142816" /><figcaption aria-hidden="true">image-20240904212142816</figcaption></figure><p>​  为了减轻OSN的负面影响，第一个关键问题是分析和建模由OSN有损信道引入的噪声。然而，这是一个相当不同的问题，主要是因为当前的平台没有披露操纵传输图像的过程。虽然[33,34]透露了osn采用的部分操作，但仍有许多未知的操作，例如Facebook中的增强过滤。更重要的是，osn经常调整它们的图像处理管道，这使得建模更具挑战性。<br/>​  针对上述挑战，本文设计了一种鲁棒的图像伪造检测方法，以击败osn中的有损操作。具体来说，为了处理OSN的退化，我们提出了一种噪声建模方案，并将模拟噪声集成到一个鲁棒的训练框架中。我们将OSN噪声解耦为两个组成部分：1)可预测的噪声和2)看不见的噪声。前者被设计用于模拟已知操作（如JPEG压缩）所带来的可预测损失，其建模依赖于带有残差学习的深度神经网络（DNN）和嵌入的可微JPEG层。而后者主要是针对osn所采取的不可知的行动和/或各种osn的训练和测试之间的差异。显然，从信号本身的角度来看，为看不见的噪声建立一个合适的模型是不现实的。为了解决这个困难，我们将我们的观察结果从噪声的角度转移到伪造检测器上，只关注可能导致检测性能下降的噪声。这种策略自然地孵化了一种新的算法，利用对抗噪声[35]的核心思想来建模看不见噪声，它本质上是难以察觉的扰动，会严重降低模型性能。结果表明，我们的鲁棒图像伪造检测方法具有优越的鲁棒性，性能优于几种先进的算法。我们的方案的检测结果的一个例子如图1所示。最后，我们基于四个现有的数据集[1,6,14,17]和三个OSN平台（脸书、微博和微信），构建了一个包含5000多个项目的公共伪造数据集。我们的主要贡献可以总结如下：</p><ul><li>我们提出了一种新的训练方案来对抗osn上传输的鲁棒图像伪造检测。该训练方案不仅对osn引入的可预测噪声进行建模，还结合了看不见噪声进一步提高鲁棒性。</li><li>与几种最先进的方法[12,27,37,42]相比，我们的模型取得了更好的检测性能，特别是在对抗osn传输的场景中。</li><li>我们基于四个现有的数据集[1,6,14,17]和三个平台（脸书、微博和微信）建立了一个公共伪造数据集。</li></ul><p>​  本文的其余部分组织如下。第2部分回顾一下相关的工作。第3部分通过提出的鲁棒训练策略，详细介绍了鲁棒图像伪造检测。实验结果见第4部分和第5部分。</p><h1 id="相关工作">2.相关工作</h1><h2 id="图像伪造检测">2.1.图像伪造检测</h2><p>​  许多取证方法（例如，[2、3、5、7、8、18-23、26、36、39、40]）已经被提出来验证数字图像的真实性。这些方法通过特定的伪影来检测锻造区域，如拼接[18,26]、复制移动[23,39]、中值滤波[8,20]、插入绘画[21,22,36]等。为了更好地适应实际需求，越来越多的方法被开发来解决检测一般（复合）类型的伪造[4,5,11,12,27,37,41,42]的问题，其中基于深度学习的方法是最成功的。沿着这条线，Wu等人[37]提出了一种一般的伪造检测网络MT-Net，该网络首先提取图像处理特征，然后识别异常区域。Mayer和Stamm最近，[27]引入了法医相似性，以确定两个图像补丁是否包含相同的法医痕迹。从相机指纹的角度来看，科佐利诺和维多里瓦·[12]设计了一种提取相机模型指纹的方法，称为噪声指纹，以揭示伪造的区域。为了学习通用伪造的痕迹，Zhuang等人[42]使用了使用ps脚本的训练数据生成策略。</p><h2id="在线社交网络osnonline-social-network">2.2.在线社交网络（OSN，OnlineSocial Network）</h2><p>​  Facebook、Wechat、Weibo等各种OSN平台的普及，大大简化了图片的传播和共享。然而，正如许多现有的作品[33,34]所表明的那样，几乎所有的osn都以一种有损的方式操作上传的图像。这些有损操作所带来的噪声会严重影响法医方法的有效性。以在[32–34]中发现的Facebook为例，这些操作主要包括三个阶段：调整大小、增强过滤和JPEG压缩。具体来说，如果图像的分辨率高于2048像素，则将应用调整大小。然后，对图像中一些选定的块进行高度自适应和复杂的增强滤波。正如在[33,34]中提到的，由于这些增强过滤操作的适应性，要精确地了解它们是非常具有挑战性的。最后，对图像进行一轮JPEG压缩，并根据图像内容自适应地确定一个质量因子（QF）。通过对[33]中提供的数据集的分析，Facebook使用的QF值范围从71到95。尽管在不同的OSN平台上的图像操作是不同的，主流osn进行的操作仍然有许多相似之处（例如，无处不在的JPEG压缩）[33]。<br/>​  一些现有的取证[9,24,38]被设计用来识别所涉及的传输操作。Liao等人[9,24]首先提出了一种基于盲信号分离的两种操作识别的特征解耦方法。为了进一步揭示一个长链，You等人的[38]提出了一个解决方案，通过创新性地将操作链检测表示为一个机器翻译问题。<br/>（[38]J. You, Y. Li, J. Zhou, Z. Hua, W. Sun, and X. Li. A transformer basedapproach for image manipulation chain detection. In <em>ACM Int. Conf.Multimedia</em>, pages 3510–3517. ACM, 2021. 3）</p><h1id="针对通过osn进行传输的鲁棒图像伪造检测">3.针对通过osn进行传输的鲁棒图像伪造检测</h1><p>​  在本节中，我们将详细介绍针对osn上传输的鲁棒图像伪造检测方案。导致成功的关键技术是适当地建模osn导致的退化，并将这些知识集成到一个鲁棒的训练框架中。从第2部分第二小节得知，OSN中的图像处理操作相当复杂；其中一些可以精确地知道，而另一些只能部分已知，甚至完全未知。因此，我们建议将OSN噪声分为两种类型：1)可预测噪声和2)不可见噪声。前一种类型对应于退化源被明确识别的情况。而后一种类型是各种噪声不确定性的组合，包括未知的建模/参数，训练和测试osn之间的差异，甚至是一些完全看不见的退化源。通过在训练阶段添加建模的OSN噪声，检测器有望学习到更广义的特征，能够在OSN传输中存活下来，从而显著提高整体伪造检测性能。<br/>​  在图2中，我们说明了我们的伪造检测鲁棒训练方案的框架，它包括四个阶段。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904213429555.png"alt="image-20240904213429555" /><figcaption aria-hidden="true">image-20240904213429555</figcaption></figure><p>​  粗略地说，阶段1和阶段2是专门通过一个可微的网络来模拟可预测的噪声。第三阶段处理通过对抗性噪声产生策略对看不见噪声的建模。最后，阶段4处理了图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的实际鲁棒训练。请注意，我们的鲁棒训练方案可以与任何基于深度学习的图像伪造检测器相结合。由于这项工作的重点更多的是在鲁棒的训练上，我们在下面将我们的注意力限制在1-3阶段，而留下<spanclass="math inline">\(f_{\theta}\)</span>的细节在第4部分第1小结陈述。<br/>​  形式上，设<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>分别表示可预测噪声和不可见噪声，因此在鲁棒训练阶段考虑的复合噪声变为<span class="math display">\[\delta=\tau+\xi.\]</span>​  对于每次训练迭代，我们首先采样两个原始的3通道（RGB）彩色图像<spanclass="math inline">\(\{\mathbf{p}_1,\mathbf{p}_2\} \in\mathbb{R}^{H\times W\times3}\)</span>和一个二进制掩码<spanclass="math inline">\(\mathbf{y}\in\{0,1\}^{H\times\dot{W}\times1}\)</span>其中，1代表伪造的区域，0代表其他地方。然后一个伪造的图像x可以被合成为<spanclass="math display">\[\mathbf{x}=\mathbf{p}_1\odot(1-\mathbf{y})+\mathbf{p}_2\odot\mathbf{y},\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级别的乘法。在拥有一对伪造的图像和相应的ground-truth掩膜后，我们可以创建一个数据集<spanclass="math inline">\(\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^N\)</span>用于训练，其中，<spanclass="math inline">\(i\)</span>是训练样本的索引。在复合噪声<spanclass="math inline">\(\delta\)</span>下，图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的鲁棒训练可以表述为： <spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\delta})}\Big\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\delta}),\mathbf{y}_{i})\Big\},\]</span>​  式中，<spanclass="math inline">\(P(\boldsymbol{\delta})\)</span>表示复合噪声<spanclass="math inline">\(\delta\)</span>的分布，N为训练样本数，<spanclass="math inline">\(\mathcal{L}_{b}\)</span>为二值交叉熵（BCE）损失。<br/>​  在我们的噪声模型中，我们考虑了一个相当一般的设置，即两个噪声分量<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>是相关的。然后，采用鲁棒训练方案的等式(3)可以进一步写成：<spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\tau})}\Big\{\mathbb{E}_{P(\boldsymbol{\xi}|\boldsymbol{\tau})}\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}+\boldsymbol{\xi}),\mathbf{y}_{i})\}\Big\},\]</span>​  其中，<span class="math inline">\(P(\boldsymbol{\tau})\)</span>为<spanclass="math inline">\(\tau\)</span>的边缘分布，<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>为给定<spanclass="math inline">\(\tau\)</span>的<spanclass="math inline">\(\xi\)</span>的条件分布。从实现的角度来看，在有足够数量的噪声样本时，可以有效和准确地计算出这些期望值。为了进行等式(4)的稳健训练，一个关键的任务是建模边缘分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>和条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。</p><h2 id="建模分布pboldsymboltau">3.1.建模分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span></h2><p>​  我们现在对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模，其中的退化是由OSN平台的有损操作引起的。从第2部分第2小节，我们知道<spanclass="math inline">\(\tau\)</span>的主要衰退源头是应用的JPEG压缩，而后处理（如增强滤波）也部分地影响了<spanclass="math inline">\(\tau\)</span>。对于一个图像<spanclass="math inline">\(\mathbf{x}_i\)</span>和一个固定的OSN平台，所产生的噪声可以很容易地计算出来：<spanclass="math display">\[\tau_i=\mathrm{OSN}(\mathbf{x}_i)-\mathbf{x}_i,\]</span>​  其中函数<spanclass="math inline">\(OSN(\cdot)\)</span>反映了给定OSN平台进行的所有操作。请注意，<spanclass="math inline">\(\tau_i\)</span>依赖于<spanclass="math inline">\(\mathbf{x}_i\)</span>，即噪声是依赖于信号的。通过这种方式，我们似乎可以生成大量的噪声样本，这可以用来模拟<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>的分布。然而，在实践中，这种简单的建模方案是相当有问题的。处理后的图像<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\)</span>必须通过将<spanclass="math inline">\(\mathbf{x}_i\)</span>上传到特定的OSN平台，然后下载来获得。一方面，这种程序很耗时间；另一方面，许多osn平台不允许过多地上传/下载操作。例如，如果在短时间内观察到太多的上传操作，Weibo甚至会禁止该账户。这严重限制了所获得的噪声样本的数量，使得这种幼稚的方案在实践中非常无效。<br/>​  为了解决这一挑战，我们采用了另一种策略，以一种不明确的方式建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。我们提出使用一个替代的深度网络来模拟OSN的操作，以便方便地产生大量的噪声样本<spanclass="math inline">\(\tau_i\)</span>。具体来说，为了与OSN平台上的图像处理管道保持一致，我们训练了一个DNN模型，该模型显式地嵌入了一个可微层来描述JPEG压缩。对于输入图像<spanclass="math inline">\(\mathbf{x}_i\)</span>，我们的目标是学习一个映射<spanclass="math inline">\(g_{\boldsymbol{\phi}} : \mathbb{R}^{d} \to\mathbb{R}^{d}\)</span>，其中，<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>是一个具有可训练参数<spanclass="math inline">\(\phi\)</span>的网络，用于预测了OSN的输出。我们为了<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>使用了U-Net架构[28]，因为它本质上是一个图像到图像的映射。训练过程如图2的第一阶段所示，然后在第二阶段使用训练良好的<spanclass="math inline">\(g_{\boldsymbol{\phi^*}}\)</span>来建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。在训练阶段，我们以离线方式收集输入图像<spanclass="math inline">\(\mathbf{x}_{i}\in\mathbb{R}^{d}\)</span>和OSN传输版本<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\in\mathbb{R}^d\)</span>。训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数可以表示为：<spanclass="math display">\[\min_{\boldsymbol{\phi}}\Big\{\mathcal{L}_r(g_{\boldsymbol{\phi}}(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\},\]</span>​  其中，<spanclass="math inline">\(\mathcal{L}_{r}(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_{2}\)</span>。<br/>​  由于我们更感兴趣的是学习由OSN传输产生的噪声，而不是图像内容本身，我们在设计<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>时采用了一个残差学习结构[16]。考虑到这一点，我们将目标函数更改为：<spanclass="math display">\[\min_\phi\Big\{\mathcal{L}_r(\mathbf{x}_i+g_\phi(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\}.\]</span>​  残差学习有利于模型的优化，显著提高了建模性能。<br/>​  此外，我们明确地将一个特殊的JPEG层集成到模型中，以便更好地生成结构性的、类似于JPEG的工件，这反映了各种OSN平台中的真实情况。实现了等式(7)中目标函数的端到端优化，我们需要确保JPEG压缩的每一步都是可微的。很容易发现量化是唯一不可微的步骤，主要是因为所采用的舍入函数<spanclass="math inline">\(\left\lfloor\cdot\right\rceil\)</span>到处都有0的导数。为了处理它，我们用可微版本[31]来近似舍入函数：<span class="math display">\[\lfloor x\rceil_a=\lfloorx\rceil+(x-\lfloor x\rceil)^3.\]</span>​  一旦有了一个可微的JPEG层，训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数就变成了：<spanclass="math display">\[\min_\phi\mathcal{L}_r(\mathcal{J}_q(\mathbf{x}_i+g_\phi(\mathbf{x}_i)),\mathrm{OSN}(\mathbf{x}_i)),\]</span>​  其中，<span class="math inline">\(\mathcal{J}_q\)</span>表示具有给定QFq的可微JPEG层。在我们的训练中，q在Facebook采用的[71,95]范围内均匀采样。然后就可以直接推导出噪声<spanclass="math inline">\({\tau}_i\)</span>为 <spanclass="math display">\[\tau_i(q)=\mathcal{J}_q(\mathbf{x}_i+g_{\boldsymbol{\phi}^*}(\mathbf{x}_i))-\mathbf{x}_i,\]</span>​  其中，通过求解优化问题等式(9)得到<spanclass="math inline">\(\phi^{*}\)</span>，q是与JPEG压缩相关联的QF。然后实现蒙特卡罗（MC，MonteCarlo）采样方案，生成大量的噪声样本，用于对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模。</p><h2 id="建模条件分布pboldsymbolxiboldsymboltau">3.2.建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span></h2><p>​  然后，我们解决条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>的建模问题，从而可以解决等式(4)中的优化问题。我们加入噪声术语<spanclass="math inline">\(\xi\)</span>的原因是，可预测的噪声<spanclass="math inline">\(\tau\)</span>不能完全捕获在实践中遇到的噪声行为。例如，不同的osn可能采用不同的过程，如，动态调整QF，自适应地调整大小，甚至引入完全未知的操作。<br/>​  现在的一个关键问题是如何为看不见的噪声<spanclass="math inline">\(\xi\)</span>建立一个适当的模型。显然，就像我们在第3部分第1小节中所做的那样，从信号本身的特征来建模<spanclass="math inline">\(\xi\)</span>是不现实的。为了解决这一挑战，我们通过研究噪声对检测性能的影响，将我们的位置从噪声方面转移到检测器<spanclass="math inline">\(f_{\theta}\)</span>方面。在各种潜在的不可见噪声<spanclass="math inline">\(\xi\)</span>中，我们实际上只需要注意那些降低检测性能的噪声，而忽略了那些对检测影响很小的噪声。这促使我们在建模<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>时使用一种对抗性噪声[35]。从本质上讲，对抗性噪声通常是人类感官难以感知，但同时能够引起严重的模型输出错误。与此同时，我们所关注的看不见的噪声<spanclass="math inline">\(\xi\)</span>是一个能够欺骗探测器的噪声，而且通常也很小（一个高度失真的图像会偏离伪造的目的）。这种与检测器<spanclass="math inline">\(f_{\theta}\)</span>效果的相似性使得对抗性噪声成为建模<spanclass="math inline">\(\xi\)</span>的合适候选噪声。<br/>​  从对抗性的角度来看，有各种方法来定义噪声<spanclass="math inline">\(\xi\)</span>，只要通过添加噪声<spanclass="math inline">\(\xi\)</span>来创建的对抗性例子，就会跨越决策边界。注意到噪声<spanclass="math inline">\(\xi\)</span>通常振幅较小，我们提出沿着相对于输入代价函数的梯度设置<spanclass="math inline">\(\xi\)</span>的方向，以使噪声能量最小化。因此，对于给定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>、可预测噪声<spanclass="math inline">\({\tau}_i\)</span>和目标输出<spanclass="math inline">\(\mathbf{y}_i\)</span>，将不可见噪声<spanclass="math inline">\(\xi_i\)</span>表示为 <spanclass="math display">\[\boldsymbol{\xi}_{i}=\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\(\mathcal{S}\)</span>返回梯度的符号。通过在训练过程中加入这些对抗性噪声，期望使学习到的模型不仅对特定的对抗性噪声，而且对更一般的不可见噪声具有鲁棒性。<br/>​  然而，由等式（11）计算出的噪声依赖于特定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>，而不是适用于训练集中所有示例和未知示例的一般输入。为了全面提高检测器的泛化能力，我们提出将对抗性噪声的方向调整为一个全局梯度方向。为此，我们采用了一种类似于随机梯度下降（SGD，StochasticGradientDescend）[30]的策略，通过从训练数据集的随机子集中随机选择的随机近似方法。更具体地说，对于第<spanclass="math inline">\((t+1)\)</span>个输入<spanclass="math inline">\(\mathbf{x}_{t+1}\)</span>，<spanclass="math inline">\({\xi}_{t+1}\)</span>（<spanclass="math inline">\(\tau\)</span>条件）可以设置为从第一个t输入计算出的平均梯度，即：<spanclass="math display">\[\xi_{t+1}=\frac{1}{t}\sum_{i=0}^{t}\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}+\boldsymbol{\xi}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\({\xi}_{0}\)</span>被初始化为0。虽然等式（12）可以用来估计平均梯度，但是它只反映了特定的已知数据（训练数据）的梯度。为了缓解上述问题，进一步提高鲁棒性，我们提出在小范围内扰动<spanclass="math inline">\({\xi}_{t+1}\)</span>。在这里，使用参数模型来描述平均梯度会更理想。为了找到一个合适的平均梯度模型，我们首先采用数据驱动的方法，分析从训练过程中随机选择的1000个<spanclass="math inline">\(\xi\)</span>的样本的统计数据。在图3中，我们使用t-SNE[13]在一个二维空间中可视化了这些1000个随机样本。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904222741508.png"alt="image-20240904222741508" /><figcaption aria-hidden="true">image-20240904222741508</figcaption></figure><p>​  可以看出，样本点集中在某一中心周围，当它们离开该中心时逐渐消失。这一现象建议我们使用高斯分布来建模平均梯度，即：<spanclass="math display">\[\xi_{t+1}|\tau\sim\mathcal{N}(\boldsymbol{u}_{t+1},\sigma^2\mathbf{I}),\]</span>​  其中，<spanclass="math inline">\(\sigma\)</span>是一个控制方差的经验集参数， <spanclass="math display">\[\boldsymbol{u}_{t+1}=\epsilon\cdot\frac1t\sum_{i=0}^t\mathcal{S}(\nabla_{\mathbf{x}_i}\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_i+\boldsymbol{\xi}_i),\mathbf{y}_i)),\]</span>​  而<spanclass="math inline">\(\epsilon\)</span>是一个用于约束扰动大小的参数，以避免不必要的模型退化。<br/>​  在等式(13)中使用参数化模型后，我们可以很容易地生成噪声样本来建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。因此等式(4)可以扩展为<spanclass="math display">\[\min_{\boldsymbol{\theta}}\sum_{i=1}^N\sum_{j=1}^m\sum_{k=1}^h\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_j+\boldsymbol{\xi}_k),\mathbf{y}_i),\]</span>​  其中，关于的期望分别近似于m和hMC样本。有了这个可计算的损失函数，我们就能够执行鲁棒训练，如算法1所示。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904223247902.png"alt="image-20240904223247902" /><figcaption aria-hidden="true">image-20240904223247902</figcaption></figure><h1 id="实验结果">4.实验结果</h1><p>​  在本节中，我们给出了实验结果，以证明我们所提出的方法的优越的性能。由于空间的限制，在补充文件中给出了更多的结果。</p><h2 id="实验设置">4.1.实验设置</h2><h3 id="基线检测器">基线检测器</h3><p>​ 该检测器的目标是在像素级的精度上检测伪造区域。具体来说，检测器<spanclass="math inline">\(f_{\boldsymbol{\theta}}: \mathbb{R}^{H\timesW\times3}\to\mathbb{R}^{H\timesW\times1}\)</span>以分辨率为H×W的彩色图像作为输入，最终输出检测结果的二进制图。在我们的设置中，我们在基线检测器中采用了U-Net[28]架构。为了提高提取伪造相关特征的能力，我们通过合并空间信道“Squeeze-and-Excitation（SE）”机制[29]进一步增强了架构，产生了一个称为SE-U-Net的变体，而不是简单地使用传统的普通U-Net。</p><h3 id="训练验证数据集">训练/验证数据集</h3><p>​  对于OSN网络<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的训练，我们采用了数据集<spanclass="math inline">\(\mathbf{WEI}\)</span>（记为<spanclass="math inline">\(\mathcal{D}_{1}\)</span>）[33]，该数据集包含了1300多张原始图像及其处理后的版本。需要注意的是，我们只使用来自Facebook的数据来培训<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>。而对于<spanclass="math inline">\(f_{\boldsymbol{\theta}}\)</span>的训练，我们使用<spanclass="math inline">\(\mathbf{Dresden}\)</span>[15]数据集作为原始图像的来源。然后，我们通过将原始图像与来自<spanclass="math inline">\(\mathbf{MS-COCO}\)</span>[25]数据集中的对象拼接来生成伪造的图像。这些伪造图像的数据集记为<spanclass="math inline">\(\mathcal{D}_{2}\)</span>。<spanclass="math inline">\(\mathcal{D}_{1}\)</span>和<spanclass="math inline">\(\mathcal{D}_{2}\)</span>随机分为训练集和验证集，比例为9：1。</p><h3 id="测试数据集">测试数据集</h3><p>​  我们通过采用四种广泛使用的数据集（<spanclass="math inline">\(\mathbf{DSO}\)</span>[6]，<spanclass="math inline">\(\mathbf{Columbia}\)</span>[17]，<spanclass="math inline">\(\mathbf{NIST}\)</span>[1]和<spanclass="math inline">\(\mathbf{CASIA}\)</span>[14]）创建测试数据集，并生成它们的OSN传输版本。更具体地说，我们通过三个最流行的OSNs（Facebook、Wechat和Weibo）手动上传和下载上述数据集，得到了5232个伪造的数据集和相应的掩码。这些收集到的数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上获得我们希望这些数据集可以作为我们的研究社区的有用的基准，以对抗在osn上共享的伪造品。</p><h3 id="对比网络">对比网络</h3><p>​  我们将我们提出的方案与四种最先进的方法进行了比较：<spanclass="math inline">\(\mathbf{MT-Net}\)</span> [37]、<spanclass="math inline">\(\mathbf{NoiPri}\)</span> [12]、<spanclass="math inline">\(\mathbf{ForSim}\)</span> [27]和<spanclass="math inline">\(\mathbf{DFCN}\)</span> [42]。</p><h2 id="定量比较">4.2.定量比较</h2><p>​  在像素域上的AUC、F1和IoU（越高越好）的定量比较见表1。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904224250921.png"alt="image-20240904224250921" /><figcaption aria-hidden="true">image-20240904224250921</figcaption></figure><p>表1.以AUC、F1、IoU为标准的定量比较。对于同一OSN传输中的每一列，最高值为粗体，“-”表示不应用。</p><p>​  在这里，我们还报告了基线检测器的结果，以一种比较的方式证明了我们的鲁棒训练方案的改进。可以观察到，当伪造图片不通过OSN传输时，ForSim[27]、DFCN [42]和我们的检测方法获得了类似的结果，而MT-Net [37]和NoiPri[12]的表现略差。需要注意的是，由于NoiPri的分辨率小，不能用于检测CASIA中的伪造，而我们的方法没有这样的限制，在CASIA上比其他竞争对手表现更好。<br/>​  在伪造图片通过osn的情况下，所有现有方法的检测性能都显著下降。例如，在通过Facebook、Weibo和Wechat传播后，与没有OSN传输的情况相比，与MT-Net相关的IoU得分分别下降了10.1%、11.1%和9.4%，相比之下，由于<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>的适当的噪声建模，我们提出的方法对OSN传输表现出相当理想的鲁棒性，并且仍然能导致准确的伪造检测。以Facebook为例，IoU的降幅仅为0.9%。我们也可以注意到，Weibo和Wechat的伪造检测性能下降略大，分别下降了2.0%和4.5%。这主要是因为，与Facebook相比，Weibo和Wechat对上传的图片采用了更严格的压缩，导致了更多的证据丢失。另外，为了训练我们的方法，我们只使用Facebook数据，根本没有任何Weibo和Wechat数据。从表1，我们可以看到使用Facebook数据训练的方案可以很好地推广到Weibo和Wechat传输的伪造上。</p><h2 id="定性比较">4.3.定性比较</h2><p>​  除了定量比较外，图4还给出了两个具有代表性的例子（更多结果见补充文件）。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225133906.png"alt="image-20240904225133906" /><figcaption aria-hidden="true">image-20240904225133906</figcaption></figure><p>图4.检测OSN传输伪造品的定性比较。对于每一行，从左到右的图像都是伪造（输入）、ground-truth、由MT-Net[37]、NoiPri [12]、ForSim [27]、DFCN[42]和我们生成的检测结果（输出）。从上到下的伪造品分别是没有OSN传输，以及有Facebook、Weibo和Wechat传输的案例。</p><p>​  可以看出，在正常情况下（没有OSN传输），现有的检测方法表现得相对较好，例如，第一种情况下的MT-Net和ForSim，以及第二种情况下的NoiPri和DFCN。然而，这些方法在OSN传输版本的情况下并不能达到令人满意的检测性能。以第二种情况下的NoiPri为例。对于Facebook、Weibo和Wechat传输的图像，识别出的伪造区域分布在多个物体上，使得伪造检测效果降低。相比之下，我们提出的方法可以学习更鲁棒的伪造特征，从而在这些具有挑战性的情况下产生更精确的检测结果，这主要归功于复合噪声建模的鲁棒训练方案。</p><h2 id="消融研究">4.4.消融研究</h2><p>​  我们现在通过分析每个建模噪声（即可预测噪声<spanclass="math inline">\(\tau\)</span>和不可见噪声<spanclass="math inline">\(\xi\)</span>）如何对最终检测性能的贡献，对我们提出的训练方案进行消融研究。为此，我们首先禁止在方案中使用每个噪声，然后在适当的设置下评估不同的再训练检测器的性能。所得结果见表2。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225545236.png"alt="image-20240904225545236" /><figcaption aria-hidden="true">image-20240904225545236</figcaption></figure><p>​  可以看出，在检测器（#2行）训练中引入可预测的噪声<spanclass="math inline">\(\tau\)</span>可以略微提高检测性能（F1增益1.2%），这在Facebook传输中更明显（F1增益4.6%）。然而，由于只采用<spanclass="math inline">\(\tau\)</span>是不完整的，如在第3部分第2小节中提到的，我们进一步加入设计的不可见噪声<spanclass="math inline">\(\xi\)</span>。第3行的结果表明，<spanclass="math inline">\(\xi\)</span>可以有效地提高检测器的鲁棒性，带来更显著的改进（例如，F1增加8.6%）。最后，第4行证明，当同时应用复合噪声<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>时，检测器对目标环境的稳定性更强，这对于OSN传输上的伪造检测任务至关重要（例如，F1的增益为15.7%）。此外，我们没有只使用SE-U-Net作为检测器，而是采用了另一个著名的架构，DPN[10]，以展示我们提出的训练方案的多功能性。如第5行和第6行所示，我们的鲁棒性训练方法也可以很好地增强DPN的鲁棒性。</p><h2 id="一些进一步的鲁棒性评估">4.5.一些进一步的鲁棒性评估</h2><p>​  虽然该方案主要是为了对抗osn进行的有损操作，但我们也希望评估其在一些更常用的退化场景下的鲁棒性，如噪声添加、裁剪、调整大小、模糊和独立的JPEG压缩。这种评估在现实情况中是非常重要的，因为这些类型的后处理操作经常被用来清除或隐藏伪造的伪影。为此，我们将这些后处理操作应用于原始测试集Columbia，并在图5中报告了定量比较。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904230059643.png"alt="image-20240904230059643" /><figcaption aria-hidden="true">image-20240904230059643</figcaption></figure><p>​  为了便于演示，我们使用一个统一的参数p来控制不同操作的大小。横轴的原点（p=0）对应于没有任何后处理的情况。可以观察到，对比网络[12,27,37,42]不能随着扰动强度的增加而表现一致，而我们的方法可以很好地推广到击败这些后处理操作。</p><h1 id="结论">5.结论</h1><p>​  在本文中，我们提出了一种新的训练方案来提高图像伪造检测对各种基于OSN的传输的鲁棒性。该方案的设计借助于建模一个可预测的噪声<spanclass="math inline">\(\tau\)</span>以及一个有意引入的看不见的噪声<spanclass="math inline">\(\xi\)</span>。实验结果表明，我们的方案与几种最先进的方法相比具有优越性。此外，我们为未来的法医研究社区建立了一个osn传输的伪造数据集。</p><h1 id="致谢">致谢</h1><p>​  澳门科技发展基金2021-2021-2023-0072/2020/20200/015/2019/AMJ，060/2019/A1和077/2012，澳门大学研究委员会2018-00029-FST和MYRG2019-00023-FST，中国自然科学基金61971476，阿里巴巴集团通过阿里巴巴创新研究计划。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>科研工具合集</title>
      <link href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/"/>
      <url>/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<h1id="t-snet-distributedstochasticneighborembedding">1.t-SNE(t-DistributedStochasticNeighborEmbedding)</h1><p>​  t-SNE是一种用于探索高维数据结构的非线性降维技术。它特别适用于高维数据的可视化，因为它能够在低维空间中保留原始高维数据的局部结构。由于这个特性，t-SNE在机器学习和数据分析领域越来越受到重视。</p><h2 id="算法解读">1算法解读：</h2><p>​  t-SNE的核心思想是在高维空间中为数据点之间定义一种<ahref="https://zhida.zhihu.com/search?q=概率分布&amp;zhida_source=entity&amp;is_preview=1">概率分布</a>，表示点与点之间的相似性，然后在低维空间中创建一个相似的概率分布。通过最小化这两个分布之间的差异（使用KL散度），算法将高维数据映射到低维空间，以便我们可以可视化。</p><h2 id="步骤和细节">2步骤和细节：</h2><h3 id="step1.计算高维空间中的相似度">Step1.计算高维空间中的相似度</h3><p>​  我们使用高斯分布（正态分布）来计算点之间的相似性。高斯分布是一种常见的概率分布，其形状呈钟型，由均值和方差（标准差的平方）决定。高斯分布有一个很好的性质：它的形状由均值（中心点）和方差（分布的宽度）决定。当我们围绕一个数据点x画一个高斯分布时，这个分布会给予附近的点较高的概率值，而离得远的点则会有较低的概率值。这与我们直觉上对“相似性”的理解相一致：靠近的点更相似，远离的点不相似。</p><p>​  对于每个数据点<spanclass="math inline">\(x_i\)</span>，我们计算所有其他点<spanclass="math inline">\(x_j\)</span>与其的条件概率<spanclass="math inline">\(p_{j|i}\)</span>。这个概率反映了点<spanclass="math inline">\(x_j\)</span>是点<spanclass="math inline">\(x_i\)</span>的近邻的可能性。计算公式为: <spanclass="math display">\[p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\neqi}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}\]</span> ​  这里，分子部分计算了<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>之间的欧氏距离的平方（即<spanclass="math inline">\(-\|x_i-x_j\|^2\)</span>），然后通过高斯分布转换成概率。分母部分是一个归一化因子，确保所有<spanclass="math inline">\(p_{j|i}\)</span>的和为1。<br/>​  <spanclass="math inline">\(\sigma_{i}\)</span>是高斯分布的方差，决定了近邻的范围。不同的点可能有不同的密度，因此<spanclass="math inline">\(\sigma_{i}\)</span>对于每个点<spanclass="math inline">\(x_i\)</span>可能是不同的，需要通过一种叫做“困惑度”的量来确定。<br/>​  最后，为了得到一个对称的相似度矩阵，我们取<spanclass="math inline">\(p_{j|i}\)</span>和<spanclass="math inline">\(p_{i|j}\)</span>的平均值得到<spanclass="math inline">\(p_{ij}\)</span>: <spanclass="math display">\[p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}\]</span>​  这样，我们就得到了一个对称的相似度矩阵，其中的每个元素<spanclass="math inline">\(p_{ij}\)</span>都反映了数据点<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>在高维空间中的相似性。通过这一步，我们成功地量化了高维空间中数据点之间的相似性，为后续的低维空间嵌入奠定了基础。</p><h3 id="step2.初始化低维空间的点">Step2.初始化低维空间的点</h3><p>​  这一步可以是随机初始化，只需保证初始化点的数量和原数据相同，维度更低即可。</p><h3id="step3.计算低维空间的点的相似度">Step3.计算低维空间的点的相似度</h3><p>​  在t-SNE算法中，高维空间的相似度是通过高斯（正态）分布计算的，而低维空间的相似度是通过t分布（具体来说是自由度为1的t分布，也叫做柯西分布）计算的。这种设计的目的是为了解决“拥挤问题”。<br/>​  当我们将高维空间中的数据点降维到低维空间时，数据点之间的距离会发生变化。特别是在低维空间中，点与点之间可用的空间更少，容易出现拥挤的情况。如果直接使用高斯分布来计算低维空间的相似度，那么低维空间中远离的点之间的相似度可能会被过高地估计，导致降维结果的可视化效果不佳。<br/>​  t分布（自由度为1）有一个重要的特性：它的尾部比高斯分布更“厚”（heavy-tailed）。这意味着，在低维空间中，即使两个点距离较远，它们之间的相似度（通过t分布计算）也不会迅速减小到0。这有助于缓解拥挤问题，因为低维空间中远离的点之间的相似度会被较低地估计。<br/>​  在低维空间中，我们计算点<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的相似度<spanclass="math inline">\(q_{ij}\)</span>如下: <spanclass="math display">\[q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\neql}(1+\|y_k-y_l\|^2)^{-1}}\]</span>​  这个公式来源于自由度为1的t分布。分子部分计算了<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的欧氏距离的平方，并转换成了概率。分母部分是一个归一化因子，确保所有的<spanclass="math inline">\(q_{ij}\)</span>之和为1。通过这种方式，我们得到了低维空间中点之间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}。接下来，t-SNE算法会试图使高维空间的相似度矩阵{<spanclass="math inline">\(p_{ij}\)</span>}和低维空间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}尽可能地一致，从而得到合适的低维空间表示。</p><h3 id="step4.优化低维空间的点的位置">Step4.优化低维空间的点的位置</h3><p>​  通过最小化Kullback-Leibler散度(KL散度)来优化低维空间中的点的位置。KL散度用于衡量高维空间和低维空间中的相似度分布之间的差异。<span class="math display">\[C=\sum_{i\neqj}p_{ij}\log\frac{p_{ij}}{q_{ij}}\]</span>​  使用梯度下降方法来最小化KL散度，更新低维空间中的点的位置。 <spanclass="math display">\[\frac{\delta C}{\deltay_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+\|y_i-y_j\|^2)^{-1}\]</span>​  在梯度下降的计算中，输入是低维空间中每个点的坐标{<spanclass="math inline">\(y_j\)</span>}。这些坐标是我们要优化的参数。输出是低维空间中点与点之间的相似度{<spanclass="math inline">\(q_{ij}\)</span>}。这些相似度是由当前的低维坐标{<spanclass="math inline">\(y_j\)</span>}计算出来的。标签是高维空间中点与点之间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}。这些相似度是已知的，因为它们是由原始高维数据计算得出的。我们的目标是通过调整低维空间中的点的坐标{<spanclass="math inline">\(y_j\)</span>}（即输入），使得由这些坐标计算出的相似度{<spanclass="math inline">\(q_{ij}\)</span>}（即输出）尽可能接近已知的高维空间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}（即标签）。<br/>​  为了实现这个目标，我们计算损失函数（即KL散度）相对于每个低维坐标的梯度，并使用这个梯度来更新低维坐标。这个过程会重复进行，直到达到预定的迭代次数，或者低维坐标的变化小于某个阈值。</p><h2 id="代码实现">3. 代码实现</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 假设 features 是你的特征向量，形状为 (256, 256, 64)</span><br><span class="line">features = np.random.rand(256, 256, 64)</span><br><span class="line"></span><br><span class="line"># 假设 mask 是你的掩码数组，形状为 (256, 256)，由0和1组成</span><br><span class="line">mask = np.random.randint(0, 2, size=(256, 256))</span><br><span class="line"></span><br><span class="line"># 重塑特征向量为 (65536, 64)</span><br><span class="line">reshaped_features = features.reshape(-1, 64)  # 65536 = 256 * 256</span><br><span class="line"></span><br><span class="line"># 将掩码展平为1D数组</span><br><span class="line">flattened_mask = mask.flatten()</span><br><span class="line"></span><br><span class="line"># 对特征向量应用 t-SNE</span><br><span class="line">tsne = TSNE(n_components=2, random_state=42)</span><br><span class="line">features_2d = tsne.fit_transform(reshaped_features)</span><br><span class="line"></span><br><span class="line"># 根据掩码的值来选择颜色</span><br><span class="line">colors = np.where(flattened_mask == 1, &#x27;red&#x27;, &#x27;blue&#x27;)</span><br><span class="line"></span><br><span class="line"># 可视化点云，不同的掩码值对应不同的颜色</span><br><span class="line">plt.figure(figsize=(10, 10))</span><br><span class="line">plt.scatter(features_2d[:, 0], features_2d[:, 1], c=colors, s=1)</span><br><span class="line">plt.title(&#x27;t-SNE Visualization of Features with Mask Colors&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="js散度jensen-shannon-divergence">2. JS散度(Jensen-Shannondivergence)</h1><h2 id="kl散度kullback-leibler-divergence定义">1)KL散度(Kullback-Leibler divergence)定义</h2><ul><li>两个概率分布(probability distribution)间差异的非对称性度量；</li><li>参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。</li><li>KL散度（Kullback-Leibler divergence），也称为相对熵（relativeentropy），是用来衡量两个概率分布之间差异的一种指标。在机器学习中，KL散度常常用于度量两个概率分布之间的相似度或差异性。</li></ul><p><span class="math display">\[KL[P(X)||Q(X)]=\sum_{x\inX}[P(x)log\frac{P(x)}{Q(x)}]=E_{x\simP(x)}[log\frac{P(x)}{Q(x)}]\]</span></p><p>​  在这种情况下，KL散度衡量了教师模型的输出分布q和学生模型的输出分布p之间的差异。通过最小化KL散度损失，学生模型被鼓励从教师模型中学习，并产生相似的输出分布。</p><p>​  此外，KL散度还经常用于变分自编码器（VAEs）中。VAEs是一种生成模型，它们学习数据的低维表示，可以用于生成新样本。在VAEs中，KL散度被用来鼓励学习到的潜在变量遵循先验分布，例如标准正态分布。这有助于正则化模型并防止过拟合。<br/>​  聚类：KL散度可以用于聚类，以度量两个聚类之间的差异。在这种情况下，KL散度可以用于评估聚类质量，并指导聚类算法的优化过程。</p><p>​  在上面的概率拟合应用场景下， <spanclass="math inline">\(KL[P||Q]\)</span> 也被称为前向KL散度（forwardKullback-Leibler Divergence），将 <spanclass="math inline">\(KL[Q||P]\)</span> 称为反向KL散度（reverseKullback-LeiblerDivergence）。<br/>​  这里需要注意的是，只有在概率拟合的应用场景下（也就是确定了真实分布和拟合分布两个角色之后），前向KL散度<span class="math inline">\(KL[P||Q]\)</span>和反向KL散度 <spanclass="math inline">\(KL[Q||P]\)</span>的定义才是有意义的，否则二者只是相同公式改变正负号、并交换P和 Q符号表示之后的平凡结果。</p><details close><br/><summary>具体情况</summary><ol type="1"><li>标准正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span></li></ol><p>​  正态分布 <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span>的概率密度函数为: <spanclass="math display">\[p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\]</span>​  标准正态分布 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span>的概率密度函数为: <spanclass="math display">\[q(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\]</span>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu,\sigma^{2})\|\mathcal{N}(0,1))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}}{\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}\log\biggl\{\frac{1}{\sqrt{\sigma^{2}}}\mathrm{exp}\biggl\{\frac{1}{2}\bigl[x^{2}-(x-\mu)^{2}/\sigma^{2}\bigr]\biggr\}\biggr\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx \\&amp;=\frac{1}{2}\intp(x)[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx\end{aligned}\]</span></p><p>​  整个结果分为三项积分，第一项实际上就是<spanclass="math inline">\(-log \sigma ^2\)</span> 乘以概率密度的积分（也就是1），所以结果是 <span class="math inline">\(-log \sigma ^2\)</span>；第二项实际是正态分布的二阶矩，熟悉正态分布的朋友应该都清楚正态分布的二阶矩为<span class="math inline">\(\mu^{2}+\sigma^{2}\)</span>；而根据定义，第三项实际上就是“-方差除以方差=-1”。所以总结果就是: <spanclass="math display">\[KL(\mathcal{N}(\mu,\sigma^2)\|\mathcal{N}(0,1))=\frac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)\]</span></p><ol start="2" type="1"><li>正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu_1,\sigma_1^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(\mu_2,\sigma_2^{2})\)</span></li></ol><p>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu_{1},\sigma_{1}^{2})\|\mathcal{N}(\mu_{2},\sigma_{2}^{2}))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}}{\frac{1}{\sqrt{2\pi\sigma_{2}^{2}}}e^{-(x-\mu_{2})^{2}/2\sigma_{2}^{2}}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}\log\{\frac{\sqrt{\sigma_{2}^{2}}}{\sqrt{\sigma_{1}^{2}}}\mathrm{exp}\{\frac{1}{2}[\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]\}\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\\&amp;=\frac{1}{2}\intp(x)[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\end{aligned}\]</span>​  整个结果分为四项积分，第一项实际上就是 <span class="math inline">\(log\sigma_2 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是<spanclass="math inline">\(log \sigma_2 ^2\)</span>；第二项实际上就是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>；第三项实际是异正态分布的二阶矩，熟悉正态分布的朋友应该都清楚异正态分布的二阶矩为$ $ ；而根据定义，第四项实际上就是“-方差除以方差=-1”。所以总结果就是:<spanclass="math display">\[KL(\mathcal{N}(\mu_1,\sigma_1^2)\|\mathcal{N}(\mu_2,\sigma_2^2))=\frac{1}{2}(\log\sigma_2^2-\log\sigma_1^2+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-1)\]</span></p></details><h2 id="两类kl散度">2) 两类KL散度</h2><p>​  考虑到需要用人工设计的近似分布 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 来拟合真实分布 <spanclass="math inline">\(P(X)\)</span> ，这里下标 <spanclass="math inline">\(\theta\)</span> 强调 <spanclass="math inline">\(Q\)</span> 是一个受到参数 <spanclass="math inline">\(\theta\)</span> 控制的分布。<br/>例如： <spanclass="math inline">\(Q\)</span> 是正态分布<spanclass="math inline">\(N(\mu,\sigma^{2})\)</span>， <spanclass="math inline">\(P\)</span> 是正态分布 <spanclass="math inline">\(N(\mu_0,\sigma_0^{2})\)</span> ，现在希望用 <spanclass="math inline">\(Q\)</span> 来拟合 <spanclass="math inline">\(P\)</span> ，其中<spanclass="math inline">\(Q\)</span> 的均值和方差 <spanclass="math inline">\(\{\mu,\sigma^{2}\}\)</span>就是拟合过程中可以调整的参数<span class="math inline">\(\theta\)</span>。于是基于前向KL和反向KL代价的分布拟合问题分别转化为以下两个优化问题：</p><ul><li>命题1. 极小化前向KL：<span class="math inline">\(\arg min_\thetaKL(P||Q_\theta)\)</span> 等价于对参数 <spanclass="math inline">\(\theta\)</span> 的极大似然估计。</li><li>命题2. 极小化反向KL： <span class="math inline">\(\argmin_{\theta}KL(Q_{\theta}||P)\)</span> 相当于在要求 <spanclass="math inline">\(Q_{\theta}\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</li></ul><p>​  首先，证明命题一，过程如下： <spanclass="math display">\[\begin{aligned}&amp;arg min_{\theta} KL (P||Q)\\&amp;=arg min_{\theta}(E_{X\sim P}[-log Q_{\theta}(X)])+H(P(X))\\&amp;=arg min_{\theta} E_{X\sim P}[-log Q_{\theta}(X)] \\&amp;=argmax_{\theta} E_{X\sim P} [log Q_{\theta} (X)] \\&amp;\approx argmax_{\theta} E_{X\sim P_{data}} [log Q_{\theta}(X)]\end{aligned}\]</span> ​  其中 <spanclass="math inline">\(H(P(X))=-\sum_{x}[P(x)logP(x)]\)</span>，代表信息熵（Entropy）。上述推导的最终结果正好就是极大似然代价的定义式。<br/>​  推导过程分析：上面的推导过程中，第2行到第3行利用了<span class="math inline">\(H(P(X))\)</span> 是与优化自变量 <spanclass="math inline">\(\theta\)</span>无关的，故删除该项不会改变最优化问题的解，因此可以直接省略。第3行到第4行则是通过来将求最小值问题转化为求最大值问题消去负号。第4行到第5行利用了机器学习训练中一般假设特征在样本集上的分布可以被近似看作真实分布，即：<span class="math inline">\(P_{data} (X)\approx P(X)\)</span> 。</p><p>　　综上命题1成立。<br/>  其次，证明命题2，推导如下： <spanclass="math display">\[arg min_{\theta} KL(Q||P)=argmin_{\theta}(E_{X\sim Q_{\theta}}[-log P(X)]+H(Q_{\theta}(X)))\]</span>​  观察上面的等式右侧 <span class="math inline">\(argmin_{\theta}\)</span> 中的两项： <span class="math display">\[E_{X\simQ_{\theta}}[-log P(X)]+H(Q_{\theta}(X))\]</span>​  要想令上面两项之和最小，就意味着要找到参数<spanclass="math inline">\(\theta\)</span>的一个合适的取值，使得上面两项中的每一项 <spanclass="math inline">\(E_{X\sim Q_{\theta}}[-log P(X)])\)</span> 和 <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>都尽可能小。根据熵的性质可知，当 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 越接近于均匀分布，第二项<span class="math inline">\(H(Q_{\theta}(X)\)</span>的值越大，反之当<span class="math inline">\(Q_{\theta}(X)\)</span>越去向于单一模态分布（可以通俗理解为单峰分布） <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>的值越小。因此反向KL散度相当于在要求<span class="math inline">\(Q_{\theta}(X)\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</p><h2 id="js散度jensen-shannon-divergence-1">3) JS散度(Jensen-Shannondivergence)</h2><p>​  <strong>JS 散度</strong>（Jensen-Shannon Divergence，缩写JSD）是基于 KL散度（相对熵）的一种统计学度量，能够衡量两个概率分布之间的差异程度。<br/>​  设概率空间上有两个概率分布<span class="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>，<spanclass="math inline">\(M=\frac12(P+Q)\)</span>，为<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>的平均，则，<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span> 的JS散度定义为 <spanclass="math display">\[JSD(P||Q)=\frac12D_{KL}(P||M)+\frac12D_{KL}(Q||M),\]</span> ​  其中，<span class="math inline">\(D_{KL}\)</span>表示KL散度。</p><p>​  KL散度的缺点：不是距离、不对称。因此引进JS散度的概念，其取值是0到1之间。由定义可以看出，JS散度是对称的，可以用于衡量两种不同分布之间的差异。JS散度用于生成对抗网络的数学推导上。</p><h1 id="kmeans">3. kmeans++</h1><p>​  K-means++算法是对传统的K-means聚类算法的一种改进，它解决了K-means算法的一个主要缺点——对初始聚类中心选择的敏感性。K-means++通过一种更合理的方式来选择初始聚类中心，使得算法更有可能找到全局最优解，而不是陷入局部最优。</p><p>K-means++算法流程：</p><ol type="1"><li><p>随机选择第一个聚类中心：从数据集中随机选择一个样本点作为第一个聚类中心K-means++算法<span class="math inline">\(c_1\)</span>。</p></li><li><p>计算距离并选择剩余的聚类中心：<br/> - 对于数据集中的每个点<spanclass="math inline">\(x_2\)</span>，计算它到已选聚类中心的最短距离<spanclass="math inline">\(D(x_i)\)</span>。<br/> - 选择下一个聚类中心<spanclass="math inline">\(c_j\)</span>的概率与 <spanclass="math inline">\(D(x_i)^2\)</span>成正比。这意味着那些距离现有聚类中心较远的点有更大的机会被选为新的聚类中心。</p></li><li><p>重复上述过程：</p><ul><li>直到选择了所有k。</li></ul></li></ol><p>公式：</p><p>​  在K-means++中，对于每一个未被选作聚类中心的点<spanclass="math inline">\(x\)</span>，计算其到最近的聚类中心的距离<spanclass="math inline">\(D(x)\)</span>，定义为： <spanclass="math display">\[D(x)=\min_{j\in[1,k]}||x-c_{j}||\]</span>​  这里<spanclass="math inline">\(||\cdot||\)</span>表示向量的范数，通常是欧几里得距离：</p><p><spanclass="math display">\[||x-c_j||=\sqrt{\sum_{d=1}^D(x_d-c_{jd})^2}\]</span>​  其中 <span class="math inline">\(x_d\)</span>和<spanclass="math inline">\(c_{jd}\)</span>分别是点 <spanclass="math inline">\(x\)</span> 和聚类中心<spanclass="math inline">\(c_j\)</span>在第<spanclass="math inline">\(d\)</span>维的坐标值，<spanclass="math inline">\(D\)</span>是数据的维度。<br/>​  概率选择新聚类中心：<br/>​  选择下一个聚类中心的概率与距离平方成正比：<span class="math display">\[P(c_{j}=x)\proptoD(x)^{2}\\P=\frac{D(x)^{2}}{\sum_{x\in X}D(x)^{2}}\]</span> ​  其中<spanclass="math inline">\(D(x)^2\)</span>代表单个点到<spanclass="math inline">\(x\)</span>其最近的已选质心（centroid）的距离的平方。这里的<spanclass="math inline">\(D(x)\)</span>是点<spanclass="math inline">\(x\)</span>到最近的聚类中心的距离。<spanclass="math inline">\(\sum_{x\in X}D(x)^{2}\)</span>是指对数据集中<spanclass="math inline">\(X\)</span>的所有点<spanclass="math inline">\(x\)</span>的距离平方<spanclass="math inline">\(D(x)^2\)</span>。<br/>​  这意味着，如果一个点<spanclass="math inline">\(x\)</span>到最近的聚类中心的距离较大，那么它被选为下一个聚类中心的概率也较高。<br/>​  通过这种方式，K-means++算法倾向于在数据集的不同区域选择聚类中心，从而避免了K-means算法可能产生的偏斜聚类中心的问题，提高了算法的稳定性和聚类质量。</p><h1 id="rpn">4. RPN</h1><p>​  区域建议网络（RPN，Region ProposalNetwork）是Faster-RCNN网络用于提取预选框。<br/>在 Faster R-CNN中，区域候选网络 (RPN)每一步的特征大小会随着网络的不同层逐渐减小，下面用具体的特征大小显示 RPN的各个步骤。假设输入图像大小为 <span class="math inline">\(H \times W\times 3\)</span>)，以常见的 VGG16 作为 Backbone 为例：</p><h2 id="输入图像-image">1. 输入图像 (Image)</h2><ul><li><strong>大小</strong>: <span class="math inline">\(H \times W \times3\)</span><br/> - H和 W是输入图像的高度和宽度，3 表示 RGB 通道。</li></ul><h2 id="特征提取网络-backbone-e.g.-vgg16">2. 特征提取网络 (Backbone,e.g., VGG16)</h2><ul><li>VGG16 的卷积层会将输入图像下采样 16 倍(stride=16)，输出的特征图大小为：</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 512\)</span><br/> - 输出的特征图具有 512个通道。</li></ul><h2 id="滑动窗口生成-anchors">3. 滑动窗口生成 Anchors</h2><ul><li>RPN 在特征图的每个像素上生成多个锚框 (anchors)，典型的 anchor 尺寸有3 种尺度和 3 种长宽比，所以每个像素点会生成 9 个 anchor。</li><li><strong>大小</strong>: $ $<br/> - 9 是每个像素位置生成的 anchor个数。</li></ul><h2 id="rpn-分类-前景背景">4. RPN 分类 (前景/背景)</h2><ul><li>RPN 对每个 anchor 进行二分类（目标/非目标），输出一个分类分数（2个通道）。</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 18\)</span><br/> - 18 是 9个 anchor的前景和背景类别概率。</li></ul><h2 id="边界框回归-修正-anchor-坐标">5. 边界框回归 (修正 Anchor坐标)</h2><ul><li>RPN 还会预测每个 anchor 的修正参数，输出的是 4 个参数(中心点坐标和宽高)。</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 36\)</span><br/> - 36 是 9 个 anchor 每个anchor 对应的 4 个回归参数 (<span class="math inline">\(9 \times 4 =36\)</span>)。</li></ul><h2 id="候选区域-rois">6. 候选区域 (RoIs)</h2><ul><li>根据 RPN 分类的输出，对正样本 anchor 进行边界框回归后得到候选区域(Region of Interest, RoIs)。</li><li>候选区域数目通常为预定数目（如 2000 个），在 Non-Maximum Suppression(NMS) 之后保留最佳的 RoIs。</li><li><strong>大小</strong>: 约 <span class="math inline">\(2000 \times4\)</span><br/> - 每个候选区域用 4 个值表示（x, y, w, h)）。</li></ul><h2 id="非极大值抑制-nms">7. 非极大值抑制 (NMS)</h2><ul><li>对生成的候选区域进行NMS，去除冗余的候选框，只保留高置信度的框。非极大值抑制 (NMS,Non-Maximum Suppression)是在区域候选网络（RPN）以及物体检测任务中常用的一种后处理算法，目的是去除冗余的候选框，只保留置信度较高且位置相对不重叠的框。以下是NMS 的主要步骤及其原理：</li></ul><p>NMS 处理步骤：</p><ol type="1"><li><p><strong>输入候选框 (RoIs)</strong>:<br/> - 经过 RPN或目标检测器输出的一系列候选框 (boundingboxes)，每个框都有一个置信度分数（表示是否包含物体的概率）。</p></li><li><p><strong>按置信度排序</strong>:<br/> -将所有候选框按其置信度分数从高到低排序，优先处理高置信度的候选框。</p></li><li><p><strong>选择最高置信度的框</strong>:<br/> -选择置信度最高的框作为基准框，将其加入最终保留的框列表。</p></li><li><p><strong>计算重叠区域（IoU）</strong>:<br/> -对其他候选框，计算它们与当前基准框的交并比 (IoU, Intersection overUnion)，即两个框的重叠区域与总区域的比值。IoU 的公式为：<br/> <spanclass="math inline">\(\text{IoU} =\frac{\text{交集区域}}{\text{并集区域}}\)</span><br/> - 如果 IoU大于一个预定的阈值（例如 0.7），则认为两个框重叠过多，视为冗余框。<br/><br/>5. <strong>删除重叠过多的框</strong>:<br/> - 删除与基准框 IoU大于阈值的框，因为这些框基本上在表示同一个物体。</p></li><li><p><strong>重复步骤</strong>:<br/> -从剩下的框中选择置信度次高的框作为新的基准框，并重复上述步骤，直到所有的候选框都处理完毕。</p></li><li><p><strong>输出结果</strong>:<br/> - 最后保留下来的框是通过 NMS过滤掉重叠候选框后的结果。它们代表了高置信度且位置不重叠的候选区域。</p></li></ol><p>关键点：</p><ul><li><strong>IoU 阈值</strong>：决定了候选框之间的重叠程度。如果 IoU大于这个阈值，候选框就会被认为过于相似，冗余框会被删除。</li><li><strong>置信度分数</strong>：用来排序候选框，确保优先保留最有可能包含物体的框。</li><li><strong>NMS的输出</strong>：是去除了重叠候选框的最终区域，通常用于进一步的分类和边界框精修(refinement)。</li></ul><p>图示化流程</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[输入候选区域 RoIs] </span><br><span class="line">    ↓</span><br><span class="line">[按置信度排序] </span><br><span class="line">    ↓</span><br><span class="line">[选择置信度最高的框] </span><br><span class="line">    ↓</span><br><span class="line">[计算 IoU] </span><br><span class="line">    ↓</span><br><span class="line">[删除重叠大的候选框] </span><br><span class="line">    ↓</span><br><span class="line">[重复以上步骤直到处理完所有框] </span><br><span class="line">    ↓</span><br><span class="line">[输出最终候选框]</span><br></pre></td></tr></table></figure><p>NMS 是 RPN中的重要步骤之一，保证了从大量冗余候选区域中提取高质量的、没有过多重叠的区域用于后续物体检测和分类。</p><h2 id="输出候选区域-用于后续-r-cnn">8. 输出候选区域 (用于后续R-CNN)</h2><ul><li><strong>大小</strong>: 约 <span class="math inline">\(N \times4\)</span><br/> - N是通过 NMS 保留下来的 RoI 数量（例如 300 个），4是每个候选框的坐标。</li></ul><p>流程图带上特征大小：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[输入图像 HxWx3] </span><br><span class="line">    ↓</span><br><span class="line">[特征提取网络 VGG16]</span><br><span class="line">    ↓</span><br><span class="line">[特征图 H/16 x W/16 x 512] </span><br><span class="line">    ↓</span><br><span class="line">[生成 Anchor H/16 x W/16 x 9] </span><br><span class="line">    ↓</span><br><span class="line">[RPN 分类 H/16 x W/16 x 18]</span><br><span class="line">    ↓</span><br><span class="line">[边界框回归 H/16 x W/16 x 36] </span><br><span class="line">    ↓</span><br><span class="line">[候选区域 (RoIs) 2000x4] </span><br><span class="line">    ↓</span><br><span class="line">[非极大值抑制 (NMS)]</span><br><span class="line">    ↓</span><br><span class="line">[输出 RoIs Nx4]</span><br></pre></td></tr></table></figure><p>这个流程描述了 RPN的特征大小变化，Anchors、分类和回归分别对应不同大小的特征。</p><h2 id="rpn-网络代码示例">9. RPN 网络代码示例</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class RPN(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, num_anchors):</span><br><span class="line">        super(RPN, self).__init__()</span><br><span class="line">        # 3x3 卷积层用于提取特征</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, padding=1)</span><br><span class="line">        </span><br><span class="line">        # 分类分支 (前景/背景)</span><br><span class="line">        self.cls_score = nn.Conv2d(512, num_anchors * 2, kernel_size=1)</span><br><span class="line">        </span><br><span class="line">        # 回归分支 (边界框回归)</span><br><span class="line">        self.bbox_pred = nn.Conv2d(512, num_anchors * 4, kernel_size=1)</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 特征提取</span><br><span class="line">        x = F.relu(self.conv(x))</span><br><span class="line">        </span><br><span class="line">        # 分类分支输出</span><br><span class="line">        cls_score = self.cls_score(x)</span><br><span class="line">        </span><br><span class="line">        # 边界框回归分支输出</span><br><span class="line">        bbox_pred = self.bbox_pred(x)</span><br><span class="line">        </span><br><span class="line">        return cls_score, bbox_pred</span><br><span class="line"></span><br><span class="line"># 测试示例</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    # 假设输入特征图的通道数为 256，锚点数为 9</span><br><span class="line">    rpn = RPN(in_channels=256, num_anchors=9)</span><br><span class="line">    </span><br><span class="line">    # 模拟输入特征图的大小 (batch_size, channels, height, width)</span><br><span class="line">    input_tensor = torch.randn(1, 256, 64, 64)</span><br><span class="line">    </span><br><span class="line">    cls_score, bbox_pred = rpn(input_tensor)</span><br><span class="line">    </span><br><span class="line">    print(&quot;Classification score shape:&quot;, cls_score.shape)</span><br><span class="line">    print(&quot;Bounding box prediction shape:&quot;, bbox_pred.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</title>
      <link href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/"/>
      <url>/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/</url>
      
        <content type="html"><![CDATA[<center>Robust Camera Model Identification Over Online Social Network SharedImages via Multi-Scenario Learning <ahref="https://ieeexplore.ieee.org/abstract/document/10262083"><imgsrc="https://img.shields.io/badge/TIFS-2023-orange" alt="TIFS" /></a></center><center>Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE,Xinyu Zhang ,</center><center>Jinyu Tian , Member, IEEE, and Weiwei Sun</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  相机模型识别（CMI，Camera modelidentification）可广泛应用于图像取证的真实性鉴定、版权保护、伪造检测等领域。同时，随着互联网的蓬勃发展，在线社交网络（OSNs，online socialnetworks）已成为图像共享和传输的主导渠道。然而，在OSNs上不可避免的损耗操作，如压缩和后处理，给现有的CMI方案带来了巨大的挑战，因为它们严重破坏了被调查图像中留下的相机痕迹。在这项工作中，我们提出了一种新的CMI方法，它对各种OSN平台的有损操作具有鲁棒性。具体来说，可以观察到，一个相机跟踪提取器可以很容易地训练在一个单一的退化场景（例如，一个特定的OSN平台）；而在混合退化场景中（例如，多个OSN平台）要困难得多。受此启发，我们设计了一种新的多场景学习（MSL，multi-scenariolearning）策略，使我们能够在不同的osn中提取鲁棒的摄像机痕迹。此外，注意到图像平滑区域由OSN引起的失真更少，而图像信号本身的干扰更小，我们提出了一种平滑感知的痕迹提取器（STATE，SmooThness-AwareTraceExtractor），它可以根据输入图像的平滑度自适应地提取相机痕迹。通过与四种先进方法的比较实验，验证了该方法的优越性，特别是在各种OSN传输场景下。特别是在开放集摄像机模型验证任务中，我们在FODB数据集上的AUC大大超过第二名15.30%；而在闭集相机模型分类任务中，我们在SIHDR数据集的F1中显著领先第二名34.51%。我们所提出的方法的代码可在https://github.com/HighwayWu/CameraTraceOSN上找到。</p><p>​  稿件于2022年11月24日收到；分别于2023年8月4日和2023年9月18日修订；2023年9月18日接受。出版日期为2023年9月25日；当前版本的日期为2023年11月20日。澳门科技发展基金2021-2023、0072/2020/AMJ、0022/2022/2/A1、0014/2022/AFJ；部分由澳门大学研究委员会MYRG2020-00101-FST和MYRG2022-00152-FST；中国自然科学基金61971476；部分由阿里巴巴集团通过阿里巴巴创新研究项目。协调审查这份手稿并批准其出版的副主编是Dr.Benedetta Tondi。（通讯作者：Jiantao Zhou）<br/>​  Haiwei Wu、 JiantaoZhou、XinyuZhang就职于智慧城市物联网国家重点实验室和澳门大学科技部计算机与信息科学系，中国澳门999078（电子邮件：yc07912@umac.mo；jtzhou@umac.mo；mc14958@umac.mo）。<br/>​  JinyuTian就职于澳门科技大学创新工程学院，中国澳门999078（电子邮件：jytian@must.edu.mo）。<br/>​  WeiweiSun在阿里巴巴集团工作，位于中国，杭州311100（电子邮件：sunweiwei.sww@alibaba-inc.com）。<br/>​  数字对象标识符10.1109/TIFS.2023.3318968<br/>​  <strong>索引术语：</strong>相机模型识别，在线社交网络，深度神经网络，鲁棒性。</p><h1 id="引言">1. 引言</h1><p>​  每个相机模型在捕获的图像上产生独特的模式噪声。这种模式噪声，也称为摄像机痕迹，主要是由传感器对光子的不同响应和/或内部图像信号处理（ISP）管道产生[1]、[2]、[3]。传统最常用的两种相机迹线提取方法分别是光响应不均匀性（PRNU，photo response non-uniformity）[1]和固定模式噪声（FPN， the fixedpatternnoise）[4]。最近的一些努力也尝试整合PRNU和卷积神经网络（CNN）来提取摄像机的轨迹，以获得更好的分辨性能[5]，[6]。摄像机跟踪的高可鉴别性使其广泛应用于相机模型识别（CMI，camera modelidentification）[7]、伪造检测[5]、传输分类[8]、[9]等各种取证任务。<br/>​  CMI的最新性能是通过基于学习的方法[6]、[10]、[13]来实现的，这些方法通常由两个子网络组成，即提取器和分类器。在训练阶段，给定一个训练图像及其相关的相机标签，提取器的目标是提取摄像机轨迹（高维特征向量），而分类器通过评估提取的轨迹是否可以分类为正确的标签来监督提取器的训练。在测试阶段，分类器通常被丢弃，训练良好的提取器可以用来提取任何输入图像的摄像机轨迹。关于基于通用学习的CMI的更多讨论将见图4和第三节。<br/>​  在实际情况下，被调查的图像通常不是直接来自相机，而是经过了一系列的处理。例如，现在许多图像从各种在线社交网络（OSNs，online socialnetworks）中获得，这不可避免地应用了许多类型的有损操作，如JPEG压缩、缩放和后处理[14]、[15]、[16]。这些操作可能会干扰现有的相机痕迹提取算法，导致它们的性能严重下降。图1给出了一个说明例子，显示了在FODB[12]数据集上的相机模型验证任务的性能。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919223213469.png"alt="image-20240919223213469" /><figcaption aria-hidden="true">image-20240919223213469</figcaption></figure><p>图1。与Kuzin [10]、NoiPri [5]、ForSim [11]和PCN[6]等最先进的方案相比，我们提出的方法在开放集验证任务上的性能。需要注意的是，测试数据集FODB[12]和所考虑的OSN平台（Twitter、WeChat、QQ、Telegram和Dingding）在训练阶段都是未知的，模拟了实际情况。详情请参见第五节。</p><p>​  可以观察到，最先进的方法[5]，[6]，[10]，[11]在无传输（NoTrans）中可以实现近90%的AUC，但考虑到OSN传输，所有现有算法的性能都会严重下降，尤其是WeChat、QQ、Telegram和Dingding。<br/>​  为了减轻OSN的负面影响，在本工作中，我们提出了一种新的相机轨迹提取方法，该方法有望对各种OSN平台的传输具有鲁棒性。为简单起见，我们主要关注单轮OSN传播病例，之后我们也展示了多轮传播的一些结果。需要注意的是，我们更感兴趣的是设计一个统一的提取器，能够针对不同类型的OSN传输提取强大的相机痕迹；而不是准备一系列提取器，每个提取器对应一种特定类型的OSN传输。我们提出的鲁棒CMI方法主要受到两个重要观察结果的启发。</p><h2 id="观察1">1)观察1</h2><p>​  在图2中，我们展示了在所谓的单一和混合OSN场景的训练过程中，提取器的损失曲线。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919223511229.png"alt="image-20240919223511229" /><figcaption aria-hidden="true">image-20240919223511229</figcaption></figure><p>图2。观察1：在VISION[17]数据集上的摄像机分类任务中的训练损失。紫色的线表示使用NoTrans（原始）图像进行的训练，而红、绿和蓝线分别对应于单一场景（Facebook和Whatsapp）和混合场景。此外，我们提出的MSL被标记为橙色线。</p><p>​  这里，“Single”表示训练数据只包含一个特定的OSN传输场景，如NoTrans、Facebook或Whatsapp，分别对应于图2中的紫色、绿色和红线。相比之下，“Mixed”指的是训练数据包含多个OSN传输场景的情况，如混合上述三种场景，如蓝线所示。可以清楚地看出，混合情景的损失曲线远远高于每个单一情景。这意味着，在混合场景中，提取器比在单一场景中更难以提取鲁棒的轨迹。因此，我们推测不同OSN传输场景下摄像机轨迹的特征空间更有可能不重叠，这使得寻找局部最优的训练过程更加困难。换句话说，一个对一个给定的OSN传输场景很鲁棒的特定摄像机痕迹可能不适用于其他场景。<br/>​  因此，为了使摄像机跟踪提取器能够更好地从混合场景中学习共享表示，我们提出了多场景学习（MSL，multi-scenario learning），通过逐个场景的方式策略性地训练提取器。与传统的基于学习的CMI方法不同，我们使用了N个（N个&gt;1）分类器，对应于训练数据中的N个场景。请注意，这N个分类器具有未共享的权值，从而放宽了每个OSN传输场景的分类器在单个分类器情况下必须相同的约束条件。具体来说，每个N个分类器都监督一个特定的OSN传输场景的训练数据。例如，第一个分类器监督NoTrans的数据场景中，第二个分类器监督Facebook场景的数据，等等。与传统的仅使用一个分类器监督提取器[6]、[10]训练的方法相比，MSL策略可以有效地减轻对不同OSN传输场景数据训练的干扰。<br/>​  我们采用N个分类器的MSL的另一个原因是训练复杂性不同。例如，从NoTrans场景中提取相机痕迹的任务显然比从Facebook场景中更容易。与只有一个分类器的传统方法相比，使用不同的分类器分离这些场景有利于学习不同的OSN传输场景之间的共享表示。然而，不平衡的训练复杂性问题可能导致逆向过程中不同场景的训练方向不一致，从而导致梯度冲突[18]。目前已经提出了一些通过梯度归一化[19]或梯度下降[20]来缓解梯度冲突的算法。然而，这些算法是基于单一的逆向过程设计的，这可能不能充分描述不同场景之间的冲突，特别是当场景相似时。因此，为了彻底描述和过滤不同场景之间的冲突，我们进一步提出在MSL中引入一个动量掩蔽操作，该操作通过积累历史上的逆向过程来生成过滤器掩蔽。<br/>​  我们提出的MSL的效果如图2中的橙色曲线所示，明显优于传统混合场景训练所对应的蓝色曲线。</p><h2 id="观察2">2)观察2：</h2><p>​  第二个观察结果是，图像中的平滑区域通常比纹理区域遭受更少的OSN处理。如图3所示，天空的失真相对于树木的要低，这表明在平滑区域的更多的摄像机轨迹可以在传输中存活下来。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919224312157.png"alt="image-20240919224312157" /><figcaption aria-hidden="true">image-20240919224312157</figcaption></figure><p>​  这种现象是合理的，因为纹理区域包含更多的高频信号，而在OSN[21]中，通过压缩和缩放等操作通常会被丢弃。另一方面，现有的作品[11]，[22]证实了在纹理区域中的相机痕迹被复杂的信号本身所掩盖。这一现象表明，痕迹提取器应该更加关注图像的平滑区域。为此，Guera等人通过训练CNN估计全局可靠性图，选择符合条件的区域，而Mayer和Stamm[11]利用固定的熵阈值来过滤合适的图像补丁。然而，通过像[22]这样的一个单独的CNN来估计可靠性图是无效的，并且被[11]丢弃的补丁仍然可以携带关于相机痕迹的有用信息。在本研究中，我们提出了基于交叉注意机制的平滑度感知轨迹提取器（STATE，SmooThness-Aware TraceExtractor），以便更灵活、更有效地从不同区域提取相机轨迹。<br/>​  正如预期的和将被实验验证，我们提出的基于上述观察设计的方法显示了令人满意的鲁棒性，并显著超过了最先进的算法，特别是在各种OSN传输场景中。如图1所示，我们的方法不仅在NoTrans场景中超越了现有的算法，而且在各种OSN平台上的传输上也取得了很大的性能提高。<br/>​  我们的主要贡献如下：</p><ul><li>据我们所知，是我们首次将MSL策略用于CMI，并证明了该策略对OSN传输具有令人满意的鲁棒性。</li><li>我们提出了STATE根据区域平滑度，灵活有效地学习相机的痕迹。</li><li>与最先进的方法[5]，[6]，[10]，[11]方法相比，我们的方法获得了更好的鲁棒性性能，特别是在OSN传输的场景中。</li><li>我们基于现有的相机数据集FODB [12]和SIHDR[23]，构建了9个流行的osn（Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat）的OSN传输数据集，不仅可以评估CMI算法的鲁棒性，而且有利于不同的取证应用。</li></ul><h1 id="相关工作">2. 相关工作</h1><h2id="a.照相机型号识别cmicamera-model-identification">A.照相机型号识别（CMI，CameraModel Identification）</h2><p>​  已经提出了许多方法[1]，[5]，[6]，[10]，[11]，[13]，[24]，[25]，[26]，[27]，[28]，[29]，[30]来表征数字图像中包含的相机痕迹。这些方法大致可以分为传统的类型和基于学习的类型。具体来说，几种传统的方法通过对在图像采集过程中进行的操作进行建模来提取相机的轨迹。最著名的方法之一是PRNU[1]，它建模由传感器缺陷引入的噪声模式。与具有乘法噪声模型的PRNU不同，Thai等人开发了一个广义噪声模型，涉及更多的图像处理操作，如原始像素之间的线性关系、伽马校正的非线性效应等。同样，通过估计相机内必备过程的参数，如彩色插值和彩色滤镜阵列，斯瓦米纳坦等[25]设计了非逼逼组件取证。博尼蒂尼等人。[26]后来分析了不同的JPEG特征算法，并表明JPEG压缩留下的痕迹可以用于CMI。<br/>​  与手工设计特征的繁琐过程相比，随着cnn的快速发展，许多基于学习的CMI算法最近被提出。CMI的开创性深度学习方案是由Bondi等人[13]设计的，其算法超过了利用手工特征[27]，[28]。通过引入细粒度的标签，[30]等人提出了一种在均匀区域内进行层次分类的相机识别算法。受PRNU的启发，CozzolinoandVerdoliva[5]利用孪生子网络，通过同时增强相机伪影和抑制高频场景内容来提取相机噪声表示。利用PRNU[1]算法预提取摄像机模型噪声，Mandelli等[6]提出了一种快速高效的成对相关网络，以更好地分析大型数据库的实用性。Mayer和Stamm[11]没有明确地描述相机的轨迹，而是引入了一种可学习的算法来测量两个图像斑块的相似性，以确定两幅图像是否来自同一个相机模型。<br/>​  需要注意的是，现有的CMI算法[5]、[6]、[10]、[11]在被调查的图像进行一些损耗操作时，仍然会出现严重的性能下降，特别是在实际的OSN传输场景中。因此，开发鲁棒的CMI方法至关重要，促进其在许多多媒体取证任务中的实际部署。</p><h2 id="b.-在线社交网络osnonline-social-network">B.在线社交网络（OSN，Online Social Network）</h2><p>​  在线社交网络大大简化了多媒体数据的传输。如今，OSNs[31]有近37.8亿日活跃用户，超过32亿张图片在日常[32]上被共享。Facebook、YouTube、Whatsapp、Instagram和WeChat是目前最受欢迎的五大社交平台，[33]的月活跃用户分别为29亿、22亿、20亿、20亿和12亿。然而，osn并不是法医友好的平台，因为它们不可避免地进行的有损操作可能会严重影响许多类型的法医算法[34]。例如，Castiglione等人的[35]证实，OSN引入的噪声会降低PRNU[1]的识别能力。因此，许多法医方案努力提高其对OSN[14]、[15]、[36]、[37]、[38]、[39]的负面影响的鲁棒性。</p><h1 id="照相机模型识别的基线方案">3. 照相机模型识别的基线方案</h1><p>​  在深入研究CMI的鲁棒设计之前，我们首先介绍了基于学习的基线方案的架构，该方案包括两个网络，即提取器和分类器，如图4所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902202942944.png"alt="image-20240902202942944" /><figcaption aria-hidden="true">image-20240902202942944</figcaption></figure><p>​  与现有的[6]、[10]、[13]方案类似，提取器的目的是提取摄像机的痕迹，而分类器则监督提取器的训练。在我们的基线方案中，提取器的特定体系结构采用了EfficientNet-b0。该选择是基于一个初步的实验，比较了不同候选架构的相机痕迹提取性能，包括ResNet[40]、VGG [41]、XceptionNet[42]、EfficientNet[43]、ViT[44]和SwinTransformer[45]等。对于分类器架构，我们简单地将其设计为线性层和SoftMax变换的组合。<br/>​  一旦确定了提取器和分类器网络的体系结构，另一个关键问题是如何在测试阶段训练和使用它们。在图4中，我们说明了基线CMI方案的训练和测试过程。在训练阶段，给定一个由Y个不同相机模型捕获的图像组成的数据集<spanclass="math inline">\(\cal{D}\)</span>，<spanclass="math inline">\((\mathbf{X},y)\)</span>表示一对训练数据，其中<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{H\times W\timesC}\)</span>为输入图像，<spanclass="math inline">\(y\in\{1,2,\cdots,Y\}\)</span>为相机的标签。具有可训练参数<spanclass="math inline">\(\theta\)</span>的提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>旨在提取能够表征相机痕迹的高级特征<spanclass="math inline">\(\mathbf{T}\)</span>，<spanclass="math inline">\(\mathbf{T}=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X})\)</span>。为了监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>将<spanclass="math inline">\(\mathbf{T}\)</span>转换为<spanclass="math inline">\(\mathbf{\hat{y}}=\mathcal{C}_{\phi}(\mathbf{T})\)</span>来计算损失<spanclass="math inline">\(\ell(\mathbf{\hat{y}},y)\)</span>，其中<spanclass="math inline">\(\ell\)</span>是广泛使用的交叉熵损失，即： <spanclass="math display">\[\ell(\hat{\mathbf{y}},y)=-\sum_{i=1}^Y\mathcal{I}[y=i]\cdot\log(\hat{\mathbf{y}}^{&lt;i&gt;}).\]</span>​  这里<spanclass="math inline">\(\mathbf{\hat{y}}^{&lt;i&gt;}\)</span>表示<spanclass="math inline">\(\mathbf{\hat{y}}\)</span>的第<spanclass="math inline">\(i\)</span>个条目，<spanclass="math inline">\(\mathcal{I}[y=i]\)</span>是一个二进制指标函数，如果是y= i，则取1，否则取0。<br/>​  经过训练，训练良好的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>可以根据[6]用于两个测试用例：1)开集验证，目的是推断两个测试图像是否被同一相机模型捕获；2)闭集分类，目的是在有限的相机模型池中识别测试图像的源相机模型。<br/>​  请注意，在这两种情况下，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>都被丢弃了。具体来说，在前一种情况下，给定两个图像<spanclass="math inline">\(\mathbf{X_1}\)</span>和<spanclass="math inline">\(\mathbf{X_2}\)</span>，它们的痕迹<spanclass="math inline">\(\mathbf{T_1}\)</span>和<spanclass="math inline">\(\mathbf{T_2}\)</span>首先由训练过的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取。然后，通过余弦相似度计算这些痕迹被同一相机拍摄的概率：<spanclass="math display">\[S(\mathbf{T}_1,\mathbf{T}_2)=\cos\Big(\frac{\mathbf{T}_1}{||\mathbf{T}_1||},\frac{\mathbf{T}_2}{||\mathbf{T}_2||}\Big).\]</span>​  对于闭集分类案例的测试过程，给出一组已知标签的<spanclass="math inline">\(y_i\in\{1,2,\cdots,Y\}\)</span>的图像，<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>首先提取它们的痕迹<spanclass="math inline">\(\mathbf{T_i}\)</span>。然后，平均相同相机模型的痕迹，可以形成一个痕迹池<spanclass="math inline">\(\{\mathbf{\bar{T}}_{i}\}_{i=1}^{Y}\)</span>。换句话说，池中的每个元素代表一个特定相机模型的平均痕迹。当一个测试图像<spanclass="math inline">\(\mathbf{X_t}\)</span>出现时，其预测的相机类型<spanclass="math inline">\(y_t\)</span>可以从痕迹池中搜索最大的相似性，即：<spanclass="math display">\[y_t=\underset{i}{\mathrm{argmax}}S(\mathbf{T}_t,\bar{\mathbf{T}}_i)\]</span>​  其中，<span class="math inline">\(\mathbf{T_t}\)</span>为通过<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的测试图像的痕迹。<br/>​  虽然我们的基线CMI可以用于提取摄像机痕迹，并最终在上述两个测试用例中被采用，但在有损传输上的性能，如各种OSN传输场景，可能会严重降低。这些场景所引入的扭曲很可能会破坏相机的痕迹，这在本质上是脆弱的。在表I中，我们简要展示了FODB[12]数据集上不同的osn造成的畸变，包括平均分辨率和文件大小的减少，以及平均采用的JPEG质量因子（QFs）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902205601762.png"alt="image-20240902205601762" /><figcaption aria-hidden="true">image-20240902205601762</figcaption></figure><p>由FODB[12]数据集上不同的OSNS造成的失真。在这里，“SCALE”和“SIZE”分别表示分辨率减少和文件大小减少的百分比。另外，“JPEGQF”表示qf值的平均值。</p><p>​  可以看出，最严重的畸变是由Dingding造成的，导致分辨率降采样84.60%，文件大小减少93.93%，从最低的平均QF值69.3也可以观察到。其中考虑的最友好的OSN平台是微博，导致分辨率降采样57.77%，文件大小减少60.19%。可以清晰地得到如下结论，这些OSN失真将严重影响CMI算法，因此设计一个鲁棒的CMI方案至关重要，能够可靠地提取OSN传输中摄像机的痕迹。</p><h1 id="鲁棒相机模型识别">4. 鲁棒相机模型识别</h1><p>​  在有了基线之后，我们提出了一种新的方法来设计一个强大的CMI来对抗各种osn上的传输，其中关键的创新是双重的：MSL和STATE。正如预期的那样，并将通过实验验证，STATE为不同的输入提取了更多的自适应痕迹，而MSL策略更好地监督了状态的训练，共同有助于鲁棒提取摄像机痕迹的目标。<br/>​  所提出的鲁棒CMI的训练过程如图5所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png"alt="image-20240902210022345" /><figcaption aria-hidden="true">image-20240902210022345</figcaption></figure><p>图5。我们提出的鲁棒CMI的训练过程。STATE从给定的图像中提取摄像机的痕迹，而MSL策略利用一组分类器，在逐个场景的基础上监督STATE的训练。</p><p>​  具体来说，给定一个训练图像<spanclass="math inline">\(\mathbf{X}\)</span>，我们首先收集其在N个场景下的传输变体。为了达到令人满意的鲁棒性，在本工作中，我们定义了由NoTrans组成的场景。（原始），两种类型的OSN传输：Facebook和Whatsapp，这也被考虑在VISION数据集[17]。此外，考虑到训练和测试场景之间的差异，我们手工制作了一个增强场景来改进对未知（新的）osn的泛化，其中增强包括常用的后处理操作，如缩放、压缩、模糊和噪声添加。更多关于场景影响的分析，例如，不同数量的场景及其组合被推迟到第五章中的消融研究G.4。对于每个变体，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取相应的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>，其中嵌入的平滑注意模块引导<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更多地关注<spanclass="math inline">\(\mathbf{X}\)</span>中的平滑区域。根据观察I，使用N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>来监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，其中每个分类器处理每个单独场景的训练。例如，<spanclass="math inline">\(\mathcal{C}_{\phi_1}\)</span>处理NoTrans场景，<spanclass="math inline">\(\mathcal{C}_{\phi_2}\)</span>处理Facebook场景等。接下来，将在<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>上生成的总损失<spanclass="math inline">\(\sum_{n=1}^{N}\mathcal{L}_{n}\)</span>进行反向传播，以更新与STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>相关的可学习参数<spanclass="math inline">\(\theta\)</span>和与N个分类器相关的<spanclass="math inline">\(\{\boldsymbol{\phi}_n\}_{n=1}^N\)</span>。</p><p>​  在测试阶段，该过程类似于基线CMI，其中只需要训练好的状态<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>，而N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>被丢弃。在下面，我们将提供更多关于我们提出的MSL策略和STATE的更多细节。</p><h2 id="a.-多场景学习mslmulti-scenario-learning">A.多场景学习（MSL，Multi-Scenario Learning）</h2><p>​  如前所述，受观察1启发，我们的MSL策略使用了多个具有非共享权重的分类器来监督多个场景的训练。多分类器产生的一个隐式问题是训练过程可能不稳定。这是因为训练的复杂性因不同的场景不同，导致梯度方向上的冲突。为了弥补这一缺陷，我们建议在MSL的逆向过程中集成一个动量掩膜操作来减轻梯度冲突。我们现在详细介绍我们提出的MSL的前向和反向过程的细节。</p><h3 id="前向过程">1)前向过程</h3><p>​  设<span class="math inline">\(\mathbf{X}\)</span>为输入图像，<spanclass="math inline">\(\{\mathbf{X}_{n}\}_{n=1}^{N}\)</span>是其在N个场景下的N个变体，<spanclass="math inline">\(\{\mathbf{T}_{n}\}_{n=1}^{N}\)</span>是它们由STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的痕迹。有关<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的细节将推迟到下一小节。同时，利用每个分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>，根据第n个场景中的训练数据来监督训练过程，损失如下：<spanclass="math display">\[\mathcal{L}_n=\ell(\mathcal{C}_{\boldsymbol{\phi}_n}(\mathbf{T}_n),y),\]</span>​  其中，<spanclass="math inline">\(\ell\)</span>为(1)中给出的交叉熵损失。<br/>​  值得注意的是，损失函数(4)和损失函数(1)之间的关键区别在于前者涉及多个非共享分类器，而后者只使用一个。另外，请注意，<spanclass="math inline">\(\mathbf{T}_{n}\)</span>和<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>应该有相同的下标n，以便逐场景实现MSL训练场景。</p><h3 id="反向过程">2)反向过程</h3><p>​  在反向过程中，分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>的参数更新为： <spanclass="math display">\[\phi_n=\phi_n-r\nabla_{\phi_n}\mathcal{L}_n,\]</span>​  其中，r是学习率。同样，提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的反向更新可以表示为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\nabla_{\theta}\mathcal{L}_n.\]</span>​  然而，这里的<spanclass="math inline">\(\sum_{n=1}^N\nabla_{\boldsymbol{\theta}}\mathcal{L}_n\)</span>由N项组成，对应于N个场景，其中不一致的梯度方向可能导致冲突，导致<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>[18]的次优训练。此外，通过<spanclass="math inline">\(\mathbf{T}_n=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X}_n)\)</span>和使用链规则，(6)可以重写为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\partial\mathbf{T}_n/\partial\boldsymbol{\theta}\)</span>为<spanclass="math inline">\(\mathbf{T}_n\)</span>的雅可比矩阵。为了减轻梯度冲突对(6)或(7)中<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更新的影响，一种解决方案是设计非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>作为<spanclass="math inline">\(\nabla_{\mathbf{T}_n}\mathcal{L}_n\)</span>的替代品。<br/>​  根据[20]，可以通过基于一致性水平逐元素掩膜<spanclass="math inline">\(\nabla_{\mathbf{T}_n}\mathcal{L}_n\)</span>来形成非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。具体来说， <spanclass="math display">\[\mathbf{L}_n=\mathbf{M}_n\odot\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。这里的<spanclass="math inline">\(\mathbf{M}_n\)</span>是一个与<spanclass="math inline">\(\nabla{\mathbf{T}_n}\mathcal{L}_n\)</span>具有相同维数的二进制矩阵，定义为：<spanclass="math display">\[\begin{aligned}\mathbf{M}_{n}=\mathcal{I}[\mathbf{P}\succcurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\succcurlyeq\mathbf{0}]+\mathcal{I}[\mathbf{P}\preccurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\preccurlyeq\mathbf{0}],\end{aligned}\]</span>​  其中<spanclass="math inline">\(\mathcal{I}\)</span>为标准指标函数，适当尺寸的<spanclass="math inline">\(\mathbf{U}\)</span>表示一个从均匀分布<spanclass="math inline">\(U(0,1)\)</span>中抽样的随机矩阵，<spanclass="math inline">\(\succcurlyeq(\preccurlyeq)\)</span>为元素不等式。此外，<spanclass="math inline">\(\mathbf{P}\)</span>测量了给定梯度中包含的正符号的纯度（一致性），其表述为：<spanclass="math display">\[\mathbf{P}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{G}_n}{\sum_n|\mathbf{G}_n|}\big),\]</span>​  其中<spanclass="math inline">\(\mathbf{G}_{n}=\mathrm{sign}(\mathbf{T}_{n})\odot\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>在批处理维度上合并了梯度贡献，所有的计算包括除值和绝对值操作都是逐元素进行。<br/>​  然而，由（10）计算的<spanclass="math inline">\(\mathbf{P}\)</span>依赖于单个反向的特定梯度<spanclass="math inline">\(\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>（或输入<spanclass="math inline">\(\mathbf{X}\)</span>），限制了其在局部描述梯度一致性的能力。这可能导致不稳定的训练或在某些情况下较差的局部最小值，例如当批中的冲突相互抵消时。因此，我们建议通过动量平均[46]来考虑历史梯度，而不是只涉及当前反向的梯度。这样，<spanclass="math inline">\(\mathbf{P}\)</span>就可以全局计算不同场景的一致性，稳定了随机梯度下降（SGD，stochasticgradient descent）中的训练更新。<br/>​  为了将动量的概念应用于<spanclass="math inline">\(\mathbf{P}\)</span>的生成，我们重新定义了第t个反向过程中的纯度为：<spanclass="math display">\[\mathbf{P}^{(t)}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{g}_n^{(t)}}{\sum_n|\mathbf{g}_n^{(t)}|}\big),\]</span>​  其中 <spanclass="math display">\[\begin{aligned}\mathbf{g}_n^{(t)}&amp;=\mu\cdot\mathbf{g}_{n}^{(t-1)}+(1-\mu)\mathbf{G}_{n}^{(t)}\\&amp;=\mu^{t-1}\mathbf{g}_{n}^{(1)}+\sum_{i=1}^{t-2}\mu^{i}(1-\mu)\mathbf{G}^{(t-i)}+(1-\mu)\mathbf{G}_{n}^{(t)}\end{aligned}\]</span>​  在之前的t−1个历史反向过程上累积梯度，<spanclass="math inline">\(\mathbf{g}_n^{(1)}\)</span>被初始化为<spanclass="math inline">\(\mathbf{G}_{n}^{(1)}\)</span>。这里的<spanclass="math inline">\(\mu\)</span>是控制最近梯度的权重的衰减因子。在实践中，我们根据经验设置<spanclass="math inline">\(\mu=0.95\)</span>。显然，当<spanclass="math inline">\(\mu=0\)</span>时，动量<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>退化到原来的<spanclass="math inline">\(\mathbf{P}\)</span>。<br/>​  在得到动量纯度<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>后，可以通过用<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>代替<spanclass="math inline">\(\mathbf{P}\)</span>来相应地计算出(9)中的掩模<spanclass="math inline">\(\mathbf{M}_{n}\)</span>和(8)中的非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。最后，使用<spanclass="math inline">\(\mathbf{L}_{n}\)</span>通过以下方式更新<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的参数：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\mathbf{L}_n.\]</span></p><h3 id="msl训练算法">3) MSL训练算法</h3><p>​  我们总结了算法1中的整个MSL训练过程。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902220503880.png"alt="image-20240902220503880" /><figcaption aria-hidden="true">image-20240902220503880</figcaption></figure><p>​  更具体地说，前向过程在行5∼7中描述，而其余行专门用于反向过程。在第5行中，我们收集了N个场景中的输入变体，这也可以提前离线进行。然后，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取第6行中的摄像机痕迹，并利用分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>来计算第7行中的损失。为了减轻损失中的梯度冲突，第8行∼20主要用于产生动量掩模<spanclass="math inline">\(\mathbf{M}_n\)</span>，然后在第21行中用于更新<spanclass="math inline">\(\theta\)</span>。最终，经过训练的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>在第24行产生。<br/>​  备注：一种简单的替代训练策略是在不同的场景下重新标记数据，并通过一个单一的分类器来计算预测。例如，一个包含29个摄像机和4个场景的数据集可以被表示为一个包含29个×4=116个类别的单一分类任务。一个潜在的关键问题是，这种替代方案假定不同场景之间有足够的可变性；否则，某些类别在某种程度上是无法区分的。然而，这个假设并不总是正确的，例如，对于NoTrans，在场景和裁剪场景中，摄像机的痕迹或多或少是相同的。换句话说，一个痕迹实际上可能对应于多个标签，这可能会导致训练过程中的不稳定。在我们提出的MSL策略中，这种困境可以通过N个分类器很自然地避免。<br/>​  我们现在将介绍STATE提取器的架构的细节。</p><h2 id="b.-平滑引导的痕迹提取器statesmooth-aware-trace-extractor">B.平滑引导的痕迹提取器（STATE，SmooTh-Aware Trace Extractor）</h2><p>​  STATE的目的是根据给定图像的局部平滑度进行专注的相机跟踪提取。在这项工作中，我们使用著名的香农熵[47]来表示一个图像块的平滑性。显然，越小的熵值表示越光滑的区域，活动越少，反之亦然。在我们提出的STATE下，我们利用交叉注意[48]层来实现注意提取，其中平滑矩阵被转换为一个注意映射并作为指导。我们想强调的是，我们提出的STATE明确地应用了根据观察2之前的平滑性，因此和交叉注意层的简单采用有显著的不同。<br/>​  STATE的过程如图5所示。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902221142472.png"alt="image-20240902221142472" /></p><p>​  具体地说，我们首先将大小为<span class="math inline">\(H\timesW\)</span>的输入图像分割为不重叠的块<spanclass="math inline">\(\{\mathbf{R}_{i}\}\)</span>，每一个块行长<spanclass="math inline">\(\frac{H}{\hat{H}}\)</span>、列长<spanclass="math inline">\(\frac{W}{\hat{W}}\)</span>，总共得到<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>个块。对于第<spanclass="math inline">\(i\)</span>个块<spanclass="math inline">\(\mathbf{R}_{i}\)</span>，我们计算相关的平滑度指标，即香农熵<spanclass="math inline">\(E_i\)</span>： <spanclass="math display">\[E_i=-\sum_{v=0}^{255}p_v\mathrm{log}(p_v),\]</span>​  其中 <spanclass="math display">\[p_{v}=\frac{\hat{H}\hat{W}}{HW}\sum_{h=0}^{H/\hat{H}}\sum_{w=0}^{W/\hat{W}}\mathcal{I}[\mathbf{R}_{i}^{&lt;h,w&gt;}=v]\]</span>​  为<span class="math inline">\(\mathbf{R}_{i}\)</span>内的像素值为<spanclass="math inline">\(v\)</span>的概率。然后，通过将<spanclass="math inline">\(\{E_i\}\)</span>分组和重塑为维数<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>，可以得到平滑矩阵<spanclass="math inline">\(\textbf{S}\in\mathbb{R}^{\hat{H}\times\hat{W}}\)</span>。在实际实现中，我们对输入图像进行灰度处理以降低复杂性，并使用单热编码来进行高效的批处理。<br/>​  得到平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>后，我们首先通过基线提取器提取内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>，然后根据<spanclass="math inline">\(\textbf{S}\)</span>在<spanclass="math inline">\(\textbf{X}^f\)</span>上执行交叉注意力，特别注意的是，<spanclass="math inline">\(\textbf{X}^f\)</span>和<spanclass="math inline">\(\textbf{S}\)</span>分别通过压扁化得到<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>，其大小为<spanclass="math inline">\(\hat{H}\hat{W}\times\hat{C}\)</span>，其中<spanclass="math inline">\(\hat{C}\)</span>是标记化后的通道数。然后通过计算每个<spanclass="math inline">\(\hat{C}/K\)</span>通道的注意力，对展平的<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>计算K头注意力，结果是k头特征<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>，其中：<spanclass="math display">\[\hat{\mathbf{X}}_k^a=\text{Attention}(\hat{\mathbf{S}}\mathbf{Q}_k,\hat{\mathbf{X}}^f\mathbf{K}_k,\hat{\mathbf{X}}^f\mathbf{V}_k),k=1,\ldots,K,\]</span>​  <span class="math inline">\(\mathbf{Q}_k\)</span>、<spanclass="math inline">\(\mathbf{K}_k\)</span>、<spanclass="math inline">\(\mathbf{V}_k\in\mathbb{R}^{\hat{C}^{2}/K}\)</span>是交叉注意函数[48]的第k个投影的查询、键和值矩阵。这里，交叉注意力的执行是：<spanclass="math display">\[\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{SoftMax}(\frac{\mathbf{QK}^{T}}{\sqrt{\hat{C}/K}})\mathbf{V}.\]</span>​  接下来，所有头<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>的拼接输出用来线性预测，得到展平的注意力<spanclass="math inline">\(\mathbf{\hat{X}}^{a}\)</span>： <spanclass="math display">\[\mathbf{X}^a=\mathrm{MLP}(\mathrm{Concat}(\hat{\mathbf{X}}_1^a,\hat{\mathbf{X}}_2^a,\ldots,\hat{\mathbf{X}}_K^a)),\]</span>​  其中<spanclass="math inline">\(\mathrm{MLP}(\cdot)\)</span>代表一个具有GELU激活[49]的MLP。最后，通过将其重塑为<spanclass="math inline">\(\hat{H}\times\hat{W}\times\hatC\)</span>分辨率，获得细化的注意力特征<spanclass="math inline">\(\mathbf{X}^{a}\)</span>。<br/>​  考虑到<spanclass="math inline">\(\mathbf{X}^{a}\)</span>中的维数冗余性，我们遵循传统的[6]，[10]，采用全局平均池化和线性映射将<spanclass="math inline">\(\mathbf{X}^{a}\)</span>编码为低维空间<spanclass="math inline">\(\mathbb{R}^d\)</span>，从而得到最终的相机痕迹<spanclass="math inline">\(\mathbf{T}\)</span>。</p><h1 id="实验结果">5. 实验结果</h1><p>​  在本节中，我们将从开放集验证、闭集分类、对OSN传输的鲁棒性、后处理操作和重传输等方面全面评估所提出的CMI方法的性能。进一步，给出了系统的消融研究和分析。在介绍详细的结果之前，让我们先介绍一下实验设置。</p><h2 id="a.实验设置">A.实验设置</h2><h3 id="训练数据集">1)训练数据集：</h3><p>​  为了训练所提出的方法，类似于[5]，[6]，我们使用了包括29个不同的相机模型的VISION[17]数据集。在[5]、[6]、[11]和[10]之后，我们合并了来自不同设备但具有相同模型的图像，以避免歧义。需要注意的是，该数据集包含Facebook和Whatsapp传输的变体，可以方便地用作N个场景的训练数据（见图5）。</p><h3 id="测试数据集">2)测试数据集</h3><p>​  为了更好地模拟实际情况并评估泛化，我们采用FODB [12]和SIHDR[23]作为交叉测试数据集，与训练数据没有重叠。FODB数据集由25个模型和27个设备组成，而SIHDR数据集分别由21个模型和23个设备组成。在这两个数据集中，每个模型都有一个额外的设备。除非另有说明，我们将根据FODB（和SIHDR）数据集中对具有相同模型的不同设备的图像进行标记，类似于在VISION中进行的过程。</p><h3 id="在线社交网络">3)在线社交网络</h3><p>​  虽然FODB数据集本身包含5个OSN传输版本（Twitter、Telegram、Whatsapp、Instagram和Facebook），但我们进一步将FODB和SIHDR数据集扩展到9个流行的OSN传输场景，包括Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat。这使我们能够更广泛地评估CMI算法对当今实际OSN传输的鲁棒性。扩展的数据集和有关操作系统和OSN平台版本的详细信息可在https://github.com/HighwayWu/CameraTraceOSN上获得。</p><h3 id="比较方法">4)比较方法</h3><p>​  为了展示我们提出的CMI方法的优越性能，我们采用了四种最先进的算法作为竞争对手，即Kuzin[10]，ForSim [11]，NoiPri [5]和PCN [6]。</p><h3 id="实现细节">5)实现细节</h3><p>​  在训练期间，MSL策略中采用的场景数量设置为4，包括NoTrans、Facebook和Whatsapp（由VISION数据集本身提供），以及一个额外的手工增强场景。引入增强场景的原因是为了进一步提高网络对看不见的场景的泛化能力。更具体地说，增强是通过随机混合0∼50%的降采样，与QFs70∼100的JPEG压缩，与核3∼5的高斯模糊，与方差3∼10的高斯噪声相加形成的。<br/>​  我们使用PyTorch深度学习框架实现我们的方法，其中采用默认参数的Adam[50]作为优化器。学习速率初始化为1e-4，如果验证损失在5个时期内没有减少，则学习速率减半。在训练过程中，所有输入的图像被随机裁剪成512个×512补丁。内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>、平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>和交叉注意特征<spanclass="math inline">\(\textbf{X}^a\)</span>具有32×32×320相同的特征维度。提取的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>的尺寸d（参见第IV-B节）设置为256。基于观察到提高测试图像的分辨率将提高性能[6]，[12]，我们将测试大小设置为1536×1536。为了便于我们的结果，我们的代码可以在https://github.com/HighwayWu/CameraTraceOSN上找到。</p><h2 id="b.开集验证任务的评估">B.开集验证任务的评估</h2><p>​  开放集验证任务的目的是推断两个给定的图像是否被同一相机模型捕获。为此，我们从每个相机模型中随机选择25张图像，然后对每个测试数据集形成5000对正对和5000对负对。由于验证任务本质上是一个二元分类问题，我们采用广泛使用的接收机工作特征曲线下面积（AUC）作为评价性能的标准（越高越好）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903093240759.png"alt="image-20240903093240759" /><figcaption aria-hidden="true">image-20240903093240759</figcaption></figure><p>​  从表二可以看出，当图像不通过OSN传输时，所有现有方法都表现良好，AUC从87.32%到89.38%，而我们的方法略好，AUC提高了1.50%。然而，当图像通过osn传输时，所有现有方法的验证性能都会显著下降。以Twitter为例，与NoTrans场景相比，Kuzin[10]、ForSim [11]、NoiPri [5]和PCN[6]的AUC值分别下降了10.83%、26.97%、18.24%和12.23%。这种严重的性能下降可能是因为osn执行的有损操作极大地消除了原有的相机痕迹，特别是那些不鲁棒的痕迹。相反，通过利用我们提出的MSL策略和平滑注意，我们提出的方法可以探索高度鲁棒的摄像机痕迹，在Twitter场景中仅减少1.89%的AUC。对于表二中的其他OSN传输场景，我们的方法仍然表现出令人满意的鲁棒性，在AUC上平均优于第二优的方法15.30%。<br/>​  对于表二下半部分的SIHDR数据集上的结果，我们可以观察到与FODB中的情况类似的现象。例如，对于ForSim，QQ平台的性能下降最为严重，AUC下降了33.62%，最友好的平台是Weibo，导致AUC下降了12.18%。相比之下，我们的方法具有非常理想的鲁棒性，平均超过第二名的AUC11.48%。还需要注意的是，这里考虑的所有方法在SIHDR上的表现都比FODB更好，这主要是因为SIHDR有更少的相机类别，而且类别之间的可变性更大。需要注意的是，在表二的每一列中，所有被选择的对都有两个图像通过相同的OSN。一个更具挑战性的实验，一对内的图像通过不同的osn传输，推迟到V-H节进一步阐述。</p><h2 id="c.-闭集分类任务的评价">C. 闭集分类任务的评价</h2><p>​  闭集分类任务的目标是预测有限的相机模型池中给定查询图像的来源，其中代表每个相机模型的“ground-truth”痕迹是从一组预定义的图像（即锚定集）[6]中提取的。通常，“ground-truth”的痕迹可以通过利用PRNU[1]算法，或者简单地通过平均深层特征来获得。与开放集评估类似，我们从每个相机类别中随机抽取45张图像，其中25张图像作为锚定集，其余的图像作为查询图像。对于每个查询图像，预测的相机标签将被授予池中与提取的痕迹相似性最高的一个。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094227527.png"alt="image-20240903094227527" /><figcaption aria-hidden="true">image-20240903094227527</figcaption></figure><p>图6。在SIHDR数据集上的闭集分类任务的混淆矩阵。对于每个矩阵，纵轴和横轴分别代表实际的标签和预测的标签。在这里，每一行（列）代表一个独特的相机模型，例如，第一行、第二行和最后一行分别对应于GioneeS55、Huaiwei-P8和iPhone5S。有关相机型号的具体信息，请参考我们的代码网站。</p><p>​  为了评估分类任务，我们在图6中显示了混淆矩阵，并在表3中显示了相应的精度（PRC）、召回率（RCL）和F1分数。在形式上，PRC和RCL的定义为：<spanclass="math display">\[\mathrm{PRC}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FP}_{y}},\]</span></p><p><spanclass="math display">\[\mathrm{RCL}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FN}_{y}},\]</span></p><p>​  其中，<span class="math inline">\(\mathrm{TP}_{y}\)</span>、<spanclass="math inline">\(\mathrm{FP}_{y}\)</span>和<spanclass="math inline">\(\mathrm{FN}_{y}\)</span>分别代表给定类y的真阳性、假阳性和假阴性。那么宏观平均的F1得分可以计算如下：</p><p><spanclass="math display">\[\mathrm{F1}=\frac{1}{y}\sum_{y=1}^{Y}\frac{2\times\mathrm{TP}_{y}}{2\times\mathrm{TP}_{y}+\mathrm{FP}_{y}+\mathrm{FN}_{y}}.\]</span><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094839265.png"alt="image-20240903094839265" /></p><p>​  从表三可以看出，在NoTrans的情况下，我们的方法取得了相当令人满意的结果（96.38%F1），明显优于两个强竞争对手PCN [6]和NoiPri[5]算法（F1的差距分别为14.22%和20.51%）。当图像通过osn传输时，所有现有的方法都会受到严重的影响。例如，在QQ和WeChat上，PCN[6]的F1的降幅分别为32.83%和62.30%，这被认为是巨大的。这意味着在OSN干扰下，PCN很难提取有区别的摄像机痕迹。从图6中给出的混淆矩阵也很容易观察到这种现象（见第三行最后一列）。相比之下，我们的方法通常对所有的OSN传输都具有鲁棒性，导致平均F1得分为87.15%。与竞争算法Kuzin[10]、NoiPri [5]和PCN[6]相比，我们在F1方面分别获得了46.68%、37.18%和34.51%的压倒性优势。在这里，表三每列中的查询和锚图像都通过了相同的OSN。<br/>​  需要注意的是，我们在这里省略了ForSim[11]的结果。这是因为在ForSim中使用的相似性网络只能提取测量给定输入对的相似性的特征，而不能捕获特定于一种相机跟踪类型的特征。因此，ForSim无法为锚点集提取相应的特征。</p><h2 id="d.-开放式分类任务的评价">D. 开放式分类任务的评价</h2><p>​  除了之前的开集验证和闭集分类任务外，我们现在还考虑开集分类任务，它不仅涉及检测给定图像是否已知，而且还包括对其特定标签的进一步分类。具体来说，假设SIHDR数据集中的摄像机已知，每个摄像机使用25张图像作为锚定集。我们在SIHDR中为每个相机模型选择20张新图像，并确定它们是否可以被正确地分类为已知的和正确的相机模型。此外，我们在FODB中的每个相机模型中选择了20张图像，并评估它们是否可以被归类为未知的，即在SIHDR中没有一个已知的（可疑的）模型。为了避免歧义，我们排除了SIHDR和FODB数据集所共有的相机模型。显然，确定过程需要建立一个接受或拒绝一个测试示例的阈值。特别是，我们直接拒绝一个与已知相机模型的最大相似度Smax低于给定阈值δ，只有当Smax&gt;δ和正确预测时，才会接受。<br/>​  以精度和f1为标准，图7所示了阈值范围从0到1的实验结果。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903095334042.png"alt="image-20240903095334042" /></p><p>​  由于提取摄像机痕迹的方法不同，竞争的方法在不同的阈值下达到了各自的最佳性能。其中，Kuzin[10]的最佳准确率为79.26%，F1评分为68.87%，而NoiPri[5]的最佳准确率为72.30%，F1评分为65.58%。由于PCN倾向于对正对和负对产生相对较高的相似性得分，因此它对阈值变化变得不那么敏感，导致图7中没有峰值。其中，PCN获得的最高准确率和F1得分分别仅为50.07%和64.15%。这表明PCN并不很适合用于开放集分类任务。相比之下，我们提出的算法的准确率为91.63%，F1为91.40%，以准确率+12.37%、F1+22.53%的表现远远超过了第二好的算法。</p><h2 id="e.-后处理评价">E. 后处理评价</h2><p>​  虽然本文的重点是针对各种CMI方案的OSN传输场景的鲁棒设计，但我们的设计也自然地为常用的后处理带来了令人满意的鲁棒性。具体来说，考虑的后处理操作包括QFs范围为70到95的JPEG压缩，因子从10%到50%的线性调整，核大小为[3,5,7]的高斯模糊，以及方差为[3,5,7,9]的高斯噪声相加。总共有18种不同的后处理操作，JPEG压缩、调整大小、模糊和噪声添加分别有6、5、3和4个变体。这些操作被应用于SIHDR数据集，其中包含929张图像。因此，总共生成了18×929=16,722张图像。对于每个图像，只应用了一种后处理攻击类型。鉴于OSN已经包含了复合攻击场景，我们这里的重点主要是对抗单一的后处理攻击。比较结果如图8所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100039566.png"alt="image-20240903100039566" /><figcaption aria-hidden="true">image-20240903100039566</figcaption></figure><p>​  可以看出，尽管ForSim [11]和NoiPri[5]在NoTrans场景中表现良好，但当遇到高斯模糊和高斯噪声添加等强后处理时，他们正在努力保持稳定的性能。Kuzin[10]在JPEG压缩和调整大小下相对稳定；但在高斯模糊和噪声增加的情况下，性能迅速下降。相比之下，PCN[6]和我们的方法对这些后处理操作具有更好的鲁棒性；对于所有考虑的情况，我们的方法仍然优于PCN，特别是在大核大小的高斯模糊中。</p><h2 id="f.-再传输和交叉传输的评估">F. 再传输和交叉传输的评估</h2><p>​  在实践中，通过多个OSN平台进行再传输和/或交叉传输（即，图像被下载并重新上传到相同或不同的osn上）是非常常见的。现在我们简要地讨论了不同CMI算法在再传输和交叉传输情况下的鲁棒性评估问题。具体来说，我们考虑的是通过Facebook的传输，其次是Facebook/QQ，其次是Whatsapp/QQ。结果列于表四之中。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100523776.png"alt="image-20240903100523776" /></p><p>​  可以观察到，第二轮传输，无论是再传播还是交叉传输，其影响远小于第一轮传播。我们的方法对两轮OSN传输都表现出相当强的鲁棒性，而竞争算法的性能要差得多。例如，例如，在通过Whatsapp场景之后再通过Whatsapp/QQ场景进行传输时，我们的方法只有2.21%/1.99%AUC下降，而PCN的性能下降更严重，达到10.26%/9.36% AUC损失。</p><h2 id="g.-消融研究">G. 消融研究</h2><p>​  在本小节中，我们通过分析每个组件如何有助于提取鲁棒的相机痕迹，对我们提出的方法进行消融研究。具体来说，我们首先禁止使用平滑注意和MSL策略，从而产生基线性能。然后，我们将注意力模块和MSL策略的不同变体纳入基线中，评估它们带来的额外性能收益。比较结果如表V所示，由于页面限制，它只包括在SIHDR[23]数据集下关于NoTrans、Facebook和Whatsapp场景的开放集验证结果。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101054119.png"alt="image-20240903101054119" /><figcaption aria-hidden="true">image-20240903101054119</figcaption></figure><h3 id="平滑注意力的采用">1)平滑注意力的采用：</h3><p>​  在表V的第二行中，我们给出了具有注意机制的结果。在这里，除了我们对平滑注意力的关注，我们还评估了变量注意力的性能，包括熵滤波器[11]或可靠性映射[22]。可以看出，通过引导基线提取器更加注意图像中的平滑区域，所有这三个注意模块都确实提高了性能。具体来说，可靠性映射[22]所带来的改进是有限的（仅有0.14%的收益），主要是因为两个因素：1)它估计给定的图像（局部）的可靠性，从全局角度缺乏交互性；2)它需要预先训练，因此很难端到端训练，这限制了提取器的优化。对于熵滤波器[11]方法，需要手动调整阈值来过滤高熵区域，这不可避免地丢弃了一些有价值的信息，在实践中很麻烦。因此，熵滤波器所带来的增量很小，只有0.82%。相比之下，我们的基于交叉注意的平滑注意模块不仅能够为不同的输入自动定制其平滑度，而且还可以很容易地纳入端到端训练中，从而获得2.79%的性能提高。在图9中，为了更直观的理解，我们还更直观地可视化了提取器注意。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101452445.png"alt="image-20240903101452445" /><figcaption aria-hidden="true">image-20240903101452445</figcaption></figure><p>​  显然，我们的平滑注意模块可以有效地引导特征提取更加注意平滑区域。</p><h3 id="msl模块的采用">2)MSL模块的采用：</h3><p>​  为了分析不同的学习策略的贡献，我们在表V的第三行中给出了相应的消融结果。注意到，第一行的基线提取器使用单个分类器进行优化，而第三行的所有结果都采用多个分类器，属于MSL的类别。除了我们的MSL策略外，我们还包括了通过使用GradNorm[19]或GradDrop[20]来获得非冲突梯度的变体。由此可见，采用多分类器的MSL确实大大提高了整体性能；即使是一个简单的非加权策略也能带来7.63%的收益。GradNorm[19]或GradDrop[20]的加入可以实现更大的改进（分别为10.04%和10.77%）。由于动量掩蔽操作明确地利用了历史逆向过程产生的梯度，我们提出的MSL与基线相比获得了更好的性能增益，达到12.38%。最后，通过联合使用平滑注意和MSL策略，我们的鲁棒CMI可以大大优于基线，导致总性能提高了15.02%。</p><h3 id="基线提取器的选择">3)基线提取器的选择：</h3><p>​  正如第三节中提到的，我们提出的鲁棒CMI是通用的，其中基线提取器（EfficientNet-b0）可以灵活地被其他网络取代。为此，我们采用了另一种最先进的网络，MobileFormer[51]，作为基线，以证明通过应用我们的鲁棒设计，也可以大大提高提取器的鲁棒性。如表V的最后一行所示，MobileFormer基线的鲁棒性得到了很好的加强，例如，平均AUC增加了13.85%。</p><h3 id="msl中n个场景的影响">4)MSL中，N个场景的影响：</h3><p>​  在MSL策略的设计中，可能会出现一个有趣的问题，即需要多少个场景来实现所需的鲁棒性。因此，我们进行了额外的实验来分析在MSL中对N个场景的影响，结果见表6。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102058441.png"alt="image-20240903102058441" /><figcaption aria-hidden="true">image-20240903102058441</figcaption></figure><p>​  可以看出，当N=1时，提取器的鲁棒性远不能令人满意。其潜在原因有两方面：1)当N =1时，MSL只包含一个分类器，从而退化为正常的训练过程；2)场景的奇异性可能导致对特定场景的严重过拟合，导致泛化性能较差。当考虑的场景数量增加时，上述困境可以很好地缓解。即使我们只同时使用原始场景和Facebook场景（N=2），我们也观察到了显著的性能提高，平均实现了7.72%的AUC改进。此外，考虑到训练和测试过程之间的差异，我们引入了一个数据增强场景，以进一步增强提取器对未知（新的）osn的泛化。根据组合场景的结果，“NoTrans+Aug”，可以得出结论，这种增强场景是有效的，可能是因为这些增强操作在一定程度上与其他osn所使用的操作重叠。最后，最后两行的结果表明，进一步增加场景的数量到N= 3和N =4可以不断提高性能。我们也尝试了用更大的N进行更多的组合；但是额外的性能提高是非常边际的，代价是显著增加的复杂性。因此，在我们的方案中，我们采用了四种场景的组合，即“NoTrans+FB+WA+Aug”。</p><h2 id="h.-评估具有挑战性的交叉osn场景"><em>H.</em>评估具有挑战性的交叉OSN场景</h2><p>​  回想一下，在上述实验中，一对内的图像通过相同的OSN传输（一致的OSN场景）。在本小节中，我们进行了更具挑战性的实验，其中图像来自不同的osn（不一致的OSN场景），比如其中一张图像来自NoTrans，另一张来自Facebook（下面标记为“NT，FB”）。在这些场景中，每个算法的性能结果如表7所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102551041.png"alt="image-20240903102551041" /><figcaption aria-hidden="true">image-20240903102551041</figcaption></figure><p>​  对于包含两个以上osn的列，一对中的图像来自两个随机选择的osn。很明显，我们提出的算法仍然大大优于现有的方法，平均AUC比排名第二的Kuzin[10]提高了8.24%。然而，不出所料，与一致的OSN场景相比，所有算法的性能都表现出了严重的下降。由于相机痕迹和OSN退化的混合，将相机痕迹分离出来，然后进行相机模型识别是相当具有挑战性的。事实上，在某些极端情况下，受OSN-A干扰影响的相机a和受OSN-B干扰影响的相机b可能具有相同的累积效应，从而误导了相机模型识别算法。一个可能的解决方案是进行盲分离，以区分相机的痕迹和OSN干扰。然而，盲分离本身就是一项极具挑战性的任务。在这项工作中，我们专注于在存在一致的OSN干扰下的相机痕迹的鲁棒提取。在未来，我们将继续探索和研究在更具挑战性的不一致OSN场景中的鲁棒摄像机模型识别算法。</p><h2 id="i.设备标识的评估">I.设备标识的评估</h2><p>​  虽然本文的主要重点是相机模型识别，但我们也尝试进行一些关于相机设备识别的实验，即评估算法识别同一模型但不同设备的图像的能力。实验结果如表8所示，其中“Same”指定的列表示AUC结果，所有正对的图像都来自同一设备。类似地，带有“Diff”的列对应于正对图像来自同一模型但不同设备的情况。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903110000029.png"alt="image-20240903110000029" /><figcaption aria-hidden="true">image-20240903110000029</figcaption></figure><p>​  可以观察到，我们提出的方法仍然能够准确地分类来自不同设备的图像，在AUC度量上超过排名第二的ForSim[11]5.04%。此外，与“Diff”的情况下相比，在“Same”的情况下，算法的总体性能往往稍好一些。这些结果表明，同一相机型号的不同设备仍然具有一定的独特的相机痕迹。</p><h1 id="结论">6. 结论</h1><p>​  在本文中，我们研究了在OSN共享图像上设计鲁棒CMI的问题。基于两个关键的观察结果，我们提出了一个鲁棒的CMI方案，明确地利用基于平滑的交叉注意和MSL策略。大量的比较实验与几种最先进的方法证明了我们的方法的优越性，特别是在各种OSN传输的场景中。我们的鲁棒设计也可以揭示一些法医取证问题，如耐OSN水印，鲁棒伪造检测等。</p>]]></content>
      
      
      <categories>
          
          <category> 相机模式识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>experimet</title>
      <link href="/experimet/"/>
      <url>/experimet/</url>
      
        <content type="html"><![CDATA[<figure><img src="../postimages/experimet/image-20240902094509834.png"alt="image-20240902094509834" /><figcaption aria-hidden="true">image-20240902094509834</figcaption></figure><figure><img src="../postimages/experimet/image-20240902094320599.png"alt="image-20240902094320599" /><figcaption aria-hidden="true">image-20240902094320599</figcaption></figure><p>第一行是FOCAL论文效果<br/>第二行是复刻的最好效果<br/>第三四行是冻结encoder，训练decoder<br/>第五行是一起训练<br/>表格如下：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;"></td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="odd"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr></tbody></table><p>复刻与FOCAL论文效果相比：</p><table style="width:100%;"><colgroup><col style="width: 15%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 4%" /><col style="width: 5%" /><col style="width: 5%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">+161</td><td style="text-align: center;">+292</td><td style="text-align: center;">9779</td><td style="text-align: center;">+309</td><td style="text-align: center;">+380</td><td style="text-align: center;">8425</td><td style="text-align: center;">-43</td><td style="text-align: center;">-102</td><td style="text-align: center;">8696</td><td style="text-align: center;">+166</td><td style="text-align: center;">+255</td><td style="text-align: center;">8426</td><td style="text-align: center;">-298</td><td style="text-align: center;">-555</td><td style="text-align: center;">7822</td></tr></tbody></table><p>decoder性能相比：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="even"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr><tr class="odd"><td>Log_v09061430/latest/</td><td style="text-align: center;">Log_v09061430/latest/</td><td style="text-align: center;">9718</td><td style="text-align: center;">9545</td><td style="text-align: center;">9734</td><td style="text-align: center;">8036</td><td style="text-align: center;">5693</td><td style="text-align: center;">8604</td><td style="text-align: center;">8284</td><td style="text-align: center;">6411</td><td style="text-align: center;">8547</td><td style="text-align: center;">8826</td><td style="text-align: center;">6851</td><td style="text-align: center;">8582</td><td style="text-align: center;">6767</td><td style="text-align: center;">3495</td><td style="text-align: center;">7930</td></tr></tbody></table><p>计算差值。以下是计算结果：</p><table style="width:100%;"><colgroup><col style="width: 20%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 5%" /><col style="width: 5%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>复刻</td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>冻结encoder的情况下，单独训练decoder</td><td style="text-align: center;">-27</td><td style="text-align: center;">-66</td><td style="text-align: center;">-16</td><td style="text-align: center;">-85</td><td style="text-align: center;">-172</td><td style="text-align: center;">-61</td><td style="text-align: center;">-111</td><td style="text-align: center;">-116</td><td style="text-align: center;">+1</td><td style="text-align: center;">+27</td><td style="text-align: center;">+62</td><td style="text-align: center;">+52</td><td style="text-align: center;">+44</td><td style="text-align: center;">+56</td><td style="text-align: center;">+63</td></tr><tr class="odd"><td>冻结encoder的情况下，单独训练decoder</td><td style="text-align: center;">-15</td><td style="text-align: center;">-49</td><td style="text-align: center;">-6</td><td style="text-align: center;">-99</td><td style="text-align: center;">-160</td><td style="text-align: center;">-90</td><td style="text-align: center;">-92</td><td style="text-align: center;">-125</td><td style="text-align: center;">-9</td><td style="text-align: center;">+33</td><td style="text-align: center;">+75</td><td style="text-align: center;">+59</td><td style="text-align: center;">+46</td><td style="text-align: center;">+49</td><td style="text-align: center;">+58</td></tr><tr class="even"><td>联合训练</td><td style="text-align: center;">-62</td><td style="text-align: center;">-51</td><td style="text-align: center;">-47</td><td style="text-align: center;">-38</td><td style="text-align: center;">-104</td><td style="text-align: center;">+148</td><td style="text-align: center;">-141</td><td style="text-align: center;">-255</td><td style="text-align: center;">+33</td><td style="text-align: center;">+120</td><td style="text-align: center;">+269</td><td style="text-align: center;">+185</td><td style="text-align: center;">-10</td><td style="text-align: center;">+23</td><td style="text-align: center;">+112</td></tr><tr class="odd"><td>联合训练</td><td style="text-align: center;">-63</td><td style="text-align: center;">-37</td><td style="text-align: center;">-45</td><td style="text-align: center;">+37</td><td style="text-align: center;">+73</td><td style="text-align: center;">+179</td><td style="text-align: center;">-313</td><td style="text-align: center;">-547</td><td style="text-align: center;">-149</td><td style="text-align: center;">+90</td><td style="text-align: center;">+206</td><td style="text-align: center;">+156</td><td style="text-align: center;">-35</td><td style="text-align: center;">+20</td><td style="text-align: center;">+108</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</title>
      <link href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/"/>
      <url>/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/</url>
      
        <content type="html"><![CDATA[<center>Towards Modern Image Manipulation Localization:A Large-Scale Dataset andNovel Methods <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a> <ahref="https://github.com/qcf-568/MIML"><imgsrc="https://img.shields.io/github/stars/qcf-568/MIML?style=flat"alt="GitHub" /></a></center><center><span class="math inline">\(\text{Chenfan Qu}^1,\text{YiwuZhong}^{2,*},\text{Chongyu Liu}^1,\text{Guitao Xu}^1,\text{DezhiPeng}^1,\text{Fengjun Guo}^3,\text{Lianwen Jin}^{1,4,*}\)</span></center><center>1华南理工大学，2威斯康星大学，3 INTSIG信息有限公司，4INTSIG-SCUT文件分析与识别联合实验室</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>  近年来，图像篡改定位因其在保障社交媒体安全方面的关键作用而越来越受到人们的关注。然而，如何准确地识别伪造的区域仍然是一个开放的挑战。其中一个主要的瓶颈是，由于其昂贵的创建过程，严重缺乏高质量的数据。为了解决这一限制，我们提出了一种新的范式，称为CAAA，可以在像素级自动和精确地注释大量的人工伪造的图像。我们进一步提出了一种新的度量QES，以方便不可靠注释的自动过滤。利用CAAA和QES，我们构建了一个大规模、多样化、高质量的数据集，其中包括123,150张带有掩码注释的人工伪造图像。此外，我们开发了一种新的模型APSCNet，用于精确的图像篡改定位。根据大量的实验，我们的数据集显著地提高了在广泛使用的基准测试上的各种模型的性能，这些改进归因于我们提出的有效方法。这些数据集和代码可以在https://github.com/qcf-568/MIML上公开获得。</p><h1 id="引言">1. 引言</h1><p>  我们提出了一种新的思想，利用训练有素的约束图像处理定位模型，自动获取这些未标记的伪造图像的掩模标注，从而大大缓解了图像处理定位的数据稀缺问题，如图1所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826151824078.png"alt="image-20240826151824078" /><figcaption aria-hidden="true">image-20240826151824078</figcaption></figure><p>图1.我们提出了一种新的约束图像处理定位范例，它分别处理SPG和SDG中的图像。我们还建议将它用于自动注释，并构建一个大规模、高质量的数据集，显著提高了图像篡改定位模型的泛化性。</p><p>  由于约束图像篡改定位方法利用相应的真实图像对伪造区域进行定位，可以大大降低任务的复杂性。<br/><br/>  然而，尽管在挑战性较小的数据方面取得了进展，但由于三个严重的障碍，以前的约束图像篡改定位方法不足以作为复杂的现代图像的合格自动注释器。首先，他们大多使用一个单一的基于相关性的模型来处理所有的输入数据[19,28]，我们认为这是一个次优的范式。一般情况下，根据操纵图像之间的共同部分是伪造区域还是真实区域，将伪造图像对及其原始图像可分为共享供体组（SDG，SharedDonor Group）和共享探针组（SPG， Shared ProbeGroup）两组，如图2所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826152216897.png"alt="image-20240826152216897" /><figcaption aria-hidden="true">image-20240826152216897</figcaption></figure><p>  虽然以往的基于相关的方法对于可持续发展目标是合理的，但它们还不够适合SPG，因为SPG中数据的实际公共部分大多是背景，而可持续发展目标中的实际公共部分大多是前景。与可持续发展目标中的共享前景相比，SPG中的共享背景具有更大的面积和更少的显著特征。同时对可持续发展目标和SPG数据上训练基于相关的模型会导致混淆，削弱其泛化能力。其次，从真实图像中减去伪造图像得到的差异图总是可以突出伪造区域，但这一重要线索被以往的约束图像篡改定位方法完全忽略了。最后，以往的工作对操作过程中大量重新缩放操作导致的语义错位关注不够重视，这混淆了模型并对其产生负面影响。<br/><br/>  为了解决这些问题，我们提出了一种新的范例，称为类别感知自动注释（CAAA，Category-AwareAuto-Annotation），它分别处理SDG和SPG中的图像对。所提出的CAAA范式由三个组成部分组成。首先，使用分类器来确定输入图像对是属于SDG还是SPG。该分类器可以通过使用无标记图像的自监督学习来有效地进行训练。其次，利用差异感知语义分割模型，利用图像对及其差异映射在SPG中进行精确的约束操作定位。此外，一个语义对齐相关匹配模型，通过更好的语义对齐提高了SDG的性能。实验表明，我们的方法在复杂场景下显著优于以往的约束图像篡改定位方法，并且足以进行自动标注。<br/><br/>  随后，我们从互联网上收集了大量手工伪造的图像，然后用提出的CAAA对其伪造的区域进行注释。该方法可以显著缓解图像处理定位中非合成数据的稀缺性，如图1所示。为了确保所有的注释都足够可靠，我们进一步提出了一个新的度量标准，称为质量评估评分（QES）。QES可以自动评估注释的质量并排除坏的注释，而不需要ground-truths来计算。实验表明，我们的数据集可以在广泛使用的基准测试上显著改进各种图像篡改定位模型。<br/><br/>  此外，为了更好地利用我们的MIML数据集，我们提出了一个新的模型，称为APSC-Net，它在各种基准测试上都优于以前的方法。<br/><br/>  综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一个新想法：从网络规模的图像中促进图像篡改定位的任务，以及从较少挑战性的任务，约束图像篡改定位中提取的自动注释。</li><li>我们提出了一种新的约束图像篡改定位范式，称为CAAA，它分别处理SPG和SDG。对于SPG，我们建议使用用语义信息去噪的图像差分。对于可持续发展目标，我们建议将语义与一个跨级别的特征相关框架对齐。</li><li>我们提出了一种新的有效度量QES，在数据集构建时自动过滤出不可靠的掩码注释。</li><li>基于上述技术，我们构建了一个大规模的、多样化的、高质量的数据集，称为MIML。它显著地解决了图像篡改定位的手工伪造数据的问题，从而大大提高了模型的泛化能力。</li></ul><p>  与图像处理定位相比，约束图像处理定位（CIML， constrained imagemanipulationlocalization）[32]在给定的真实图像的额外帮助下对伪造的图像区域进行定位。以往的工作大多是基于相关匹配，并对SDG和SPG中的图像对进行统一处理。Wu等人[32]提出了第一个深度相关模型DMVN，该模型计算相关映射来定位图像中的相似对象。Liu等人提出去除池化层，采用无卷积获得更丰富的空间信息。Liu等人的[18]采用了注意感知机制来获得更好的表现。Tan等人[28]提出在编码器和解码器中都执行相关性，以提取更好的特征。这些方法在挑战性较小的数据集上取得了重大进展（例如，合成COCO[32]）。然而，它们的性能在具有高分辨率、大变化度和大复杂度的现代图像中受到限制。</p><h1 id="分类感知自动注释模块">2. 分类感知自动注释模块</h1><p>  对于受限的图像篡改定位，以往的工作没有考虑SPG和SDG图像对之间的差异，而是使用单一的相关性模型对其进行统一处理。我们认为这种范式是次优的，原因如下：</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826160604894.png"alt="image-20240826160604894" /><figcaption aria-hidden="true">image-20240826160604894</figcaption></figure><p>  首先，SDG 图像的相似区域是前景区域（例如，图3中SDG分支中的猫）。它们有特定的、相似的形状和独特的特征。相比之下，SPG图像的相似区域是背景。它们通常没有足够独特的特征来进行精确的相关匹配（例如，在图3的SPG分支图像中，一块背景中的雪与所有其他块背景中雪有很高的相似性）。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。<br/><br/>  第二，配对的SPG图像之间的差异是一个重要的提示。SPG图像对中的大部分区域几乎相同，并在空间上对齐（例如，图3中SPG分支的图像对）。简单地在它们之间减去最终的差异图就可以突出被操纵的区域。然而，这些信息在以前的基于相关匹配的模型中难以利用，因此在以前的CIML工作中没有考虑到。<br/><br/>  基于这些观察结果，我们提出了一个新的CIML任务范式，类别感知自动注释CAAA。其关键思想是独立处理SPG和SDG图像，如图3所示。首先，利用第3.1节中提出的分类器，将输入的图像对分为SPG或SDG。对于SPG，图像对采用第3.2节中提出的差分感知语义分割进行处理。对于SDG，图像对通过第3.3节中提出的语义对齐相关匹配进行处理。更重要的是，用我们提出的范式训练的模型被进一步用于执行在大量手工伪造的图像上的自动注释。作为回报，收集的数据解决了用于图像篡改定位的非合成数据的严重短缺。</p><h2 id="自监督分类器">2.1 自监督分类器</h2><p>  为了实现SPG和SDG的分类，我们提出了通过对无标记图像的自监督学习来训练分类器。给定一个图像，我们对其进行随机的增强和操作，然后将伪造的图像和原始图像形成一个SPG图像对。为了构建一个SDG图像对，我们从原始图像中复制随机对象，调整它们的大小，并将它们粘贴到另一个图像中。利用所得到的图像对，我们可以有效地训练我们的分类器。每个输入对中的两个图像在被输入到分类器之前被连接在通道维数中。分类器只需要识别一个图像对中的两个图像是几乎相同（SPG）还是明显不同（SDG），而不考虑哪一个或哪里是假的。因此，这个分类任务非常简单，我们可以准确地将图像对分成两组。</p><h2 id="具有差异感知能力的语义分割">2.2 具有差异感知能力的语义分割</h2><p>  理想情况下，对于SPG中的图像对，真实图像和伪造图像之间的绝对差异实际上是伪造区域。然而，被操纵的图像在传输[31]过程中通常会发生退化，这使得无法利用绝对差异作为精确的注释。如图4所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826162503002.png"alt="image-20240826162503002" /><figcaption aria-hidden="true">image-20240826162503002</figcaption></figure><p>图4.由于篡改后的图像在传输过程中通常会经历一系列的退化，因此它们与真实图像之间的绝对差异不能准确地表示伪造区域。我们的方法通过使用语义信息来实现了充分的去噪，从而解决了这一问题。</p><p>  由于传输退化，图像差分图中几乎所有的区域都是非零的。即使是OTSU[24]算法二值化的差异图也在真实区域上突出，特别是在高频区域，如边缘。为了解决这个问题，我们建议利用图像中的语义信息来去噪差异映射。为了实现这一点，我们提出将真实图像、伪造图像及其差异映射图的信道维数连接输入到一个语义分割模型中。</p><h2 id="语义对齐的关联匹配">2.3 语义对齐的关联匹配</h2><p>  由于广泛的重缩放操作，语义失调成为对基于相关性的方法的有效性产生不利影响的关键因素。例如，在图3的SDG分支中，原始图像中的猫占据了一个很大的区域，而在伪造的图像中，同一只猫被限制在一个小得多的区域内。原始图像的猫特征大多在最高水平，而伪造图像的猫特征大多在最低水平。因此，两幅图像之间在同一编码水平上的视觉特征存在语义错位。然而，以往的工作只是迫使模型在相同的特征层次之间进行特征匹配，这混淆了模型，并对其泛化产生了负面影响。为此，我们提出通过实现更好的语义对齐来提高相关模型的性能。<br/><br/>  具体来说，给定从主干模型中提取的一组不同分辨率的特征映射，我们首先用平均池化的最高特征计算全局表示，然后用卷积层将它们与最高特征融合。随后，我们以一种类似于在FPN[15,34]中的自上而下的方式融合了这些特性映射。这样，低级特性就具有更多的语义，并准备与高级特性相匹配。然后，我们以跨层次的方式计算输入图像对特征之间的相关特征$ F_{corr} $，如式(1)，这不同于以前的方法[18,19,32]，它只计算与方程(2)相同水平的特征图之间的相关特征。<span class="math display">\[[Corr(F_{o,i},F_{m,j}) for i in (0{-}3) andfor j in (0{-}3)]\]</span></p><p><span class="math display">\[[Corr(F_{o,i},F_{m,i}) for i in (0-3)]\]</span></p><p>  在这些方程中，Corr表示之前工作中广泛使用的相关函数，[18,19,32]， $F_{o,i} $ 表示原始图像的第i层特征图， $ F_{m,j} $表示伪造图像的第j层特征图。我们的模型能够自适应地选择最优匹配路由，从而增强语义对齐。$ F_{corr} $ 随后被连接，信道减少并输入卷积解码器进行最终预测。</p><h1 id="miml数据集">3. MIML数据集</h1><p>  在本节中，我们提出了一个大规模的、多样化的、高质量的数据集，称为MIML。其关键思想是利用在现有数据集上训练的约束图像篡改定位模型，自动从网络中人工伪造的图像获得准确的掩码注释。为了确保数据集的高质量，我们还提出了一个新的度量标准来过滤掉不充分的注释。</p><h2 id="数据集构成">3.1 数据集构成</h2><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826163432173.png"alt="image-20240826163432173" /><figcaption aria-hidden="true">image-20240826163432173</figcaption></figure><p>如图6所示，我们构建MIML的步骤如下：</p><p>  <strong>图像收集。</strong>我们从imgur.com中收集图像对。在这个网站上，这些图片是由数百万人手工伪造的，因此有高质量、多样化的伪造区域。<br/><br/>  <strong>数据清理。</strong>我们清理收集到的数据，并排除与第6块中的评估数据集重叠的图像。<br/><br/>  <strong>分类。</strong>我们使用第3.1节中提出的分类器将清理后的图像对分类为SPG或SDG。实际的分类器是三个模型[8,20,21]的集合。<br/>  <strong>自动注释。</strong>我们利用第3.2节和第3.3节中提出的DASS和SACM，分别自动获取SPG和SDG中图像的掩码注释。<br/><br/>  <strong>质量评价。</strong>经过自动注释，SPG的注释已经有了高质量，而SDG的注释仍然不令人满意。为了保证整体质量，我们提出了一种新的度量标准，质量评估评分（QES），以进一步过滤掉不可靠的注释。QES的关键思想是，大多数高质量的预测都有非常高的置信度和尖锐的边缘，因此我们可以评估预测的质量，并通过检查预测的置信度和清晰度来排除不好的预测。具体来说，给定一个具有形状（H，W）和归一化概率的预测掩模，我们计算QES如下：<spanclass="math display">\[\textbf{QES=}\frac{\sum_{i,j}^{H,W}p_{i,j}&gt;(1-T_{h})}{\sum_{i,j}^{H,W}p_{i,j}&gt;T_{l}}\]</span>  其中， $~ \sum_{i,j}^{H,W}p_{i,j} &gt; (1 - T_{h}) $ 表示高置信度大于$ (1 - T_{h}) $ 的预测区域， $~ \sum_{i,j}^{H,W}p_{i,j}&gt;T_l $表示预测的总潜在操纵面积。我们将 $ T_{h} $ 和 $ T_{l} $ 设置为 $~\frac{1}{16} $，只保留QES&gt;0.5的样品。实验表明，我们的QES与IoU度量有很强的相关性，可以有效地帮助过滤出不可靠的掩码注释。</p><h2 id="数据集亮点">3.2 数据集亮点</h2><p>  我们在图5中给出了所建议的数据集的几个例子。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826164936525.png"alt="image-20240826164936525" /><figcaption aria-hidden="true">image-20240826164936525</figcaption></figure><p>  我们的数据集的主要亮点如下：</p><ul><li><strong>高质量。</strong>该数据集中的图像操作是由人类精心制作的。这些数据可以教模型在现实世界中发现伪造，而不仅仅是在合成数据中过度拟合几个简单的模式。</li><li><strong>大规模。</strong>如表1所示，所提出的数据集共有123,150张人工伪造的图像，比之前的手工IML数据集多几十倍（例如，≈比IMD20的60倍）。</li><li><strong>多样性。</strong>我们的数据集包括各种大小、各种样式和各种类型的操作（例如，复制-移动、拼接、删除）的图像。它们是由成千上万的人利用各种软件创建的。这些不同的数据可以大大提高深度IML模型的泛化能力。</li><li><strong>现代风格。</strong>我们的数据集有大量的现代图像，最近被捕获和伪造，跟上了现代数码摄影技术的步伐。相比之下，CASIA数据集[3]是在十多年前提出的，其中大多数图像的尺寸都很小，而且都很模糊。因此，我们的数据集可以更好地满足现代图像操作定位的要求。</li><li><strong>强大的可扩展性。</strong>网络上有许多越来越受欢迎的图像处理比赛，不断吸引数百万人来参加（例如1900万pas-Battles[9,25]的900万人），产生了大量新的手工伪造图像。我们的数据集构建方法已经准备好利用这些不断增长的廉价web数据。因此，我们的数据集可以很容易地进行扩展，显示出强大的可扩展性。</li></ul><h1 id="apsc-net">4. APSC-Net</h1><p>  在本节中，我们提出了一个新的模型，称为APSCNet，以实现精确的图像操作定位。如图7所示，它由特征提取器、自适应感知模块和自校准模块组成。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png"alt="image-20240826165320985" /><figcaption aria-hidden="true">image-20240826165320985</figcaption></figure><h2 id="自适应感知模块">4.1 自适应感知模块</h2><p>  在细致的图像取证分析过程中，人类经常会反复放大和缩小图像，选择一组最佳的观察结果来帮助他们的最终预测。为了模拟人类的感知方式，我们设计了一个自适应感知模块，以帮助模型比较不同的视图，并自适应地选择每个输入图像的最优组合。其关键思想是使用从全局表示计算出的自适应权重对当前和所有高级特征图进行加权。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170701354.png"alt="image-20240826170701354" /><figcaption aria-hidden="true">image-20240826170701354</figcaption></figure><p>  具体来说，给定从主干模型中提取的4个特征图，我们首先将它们映射到1×1转换层的通道上，得到4个特征图$ F_{i,0},F_{i,1},F_{i,2},F_{i,3} $ 。然后，我们通过全局平均池化从 $F_{i,3} $ 中得到全局图像表示，并使用1×1卷积层将它们与 $ F_{i,3} $融合，得到 $ F_{o,3} $。最后，对于（2，1，0）中的a和范围（a+1，3）中的b，我们遵循下面的公式(3)和(4)依次计算$ F_{o,a} $ ： <spanclass="math display">\[[w_{a,a},w_{a,b}]=\sigma(f_{a}(Cat([Avg(F_{i,a}),Avg(F_{o,b})])))\]</span></p><p><spanclass="math display">\[F_{o,a}=Conv(w_{a,a}*F_{i,a}+\sum_{b=a+1}^3w_{a,b}*F_{o,b})\]</span></p><p>  其中Avg表示全局平均池，Cat表示通道维连接， $ f_a $表示具有ReLU层的两个线性层， $ $表示Sigmoid激活函数，Conv表示3×3卷积层。</p><h2 id="自校准模块">4.2 自校准模块</h2><p>  当对操纵图像进行细致的定位时，人类倾向于通过比较预测的伪造区域周围的特征来确认他们的初始预测。此外，他们可能会根据他们对图像真实性的全局评估来修改他们的局部预测。为了模拟人类的感知方式，我们设计了一个自校准模块，以获得更好的性能。<br/><br/>  如图7所示，提出的自校准模块包括基于分割的自校准（SSC，Segmentation-basedSelf Calibration）和基于分类的自校准（CSC，Classification-based SelfCalibration）。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170727546.png"alt="image-20240826170727546" /><figcaption aria-hidden="true">image-20240826170727546</figcaption></figure><p>  对于SSC，从自适应感知模块的末端获得初始预测，并将其输入一个由几个卷积层组成的小校准核映射模块。随后，我们得到一个校准核，并对其初始预测进行卷积运算。然后使用Min-Max方法对结果值进行归一化。我们将标准化结果乘以$ F_o $ ，即 $ F_{o,0},F_{o,1},F_{o,2},F_{o,3} $ 的串联，得到 $ F_{ref1}$ 。接下来，我们使用多个卷积层来细化 $ F_{ref1} $ ，并得到细化的特征 $F_{ref2} $ 。在此之后，我们将 $ F_{ref2} $ 与 $ F_o $连接起来，执行信道注意和信道减少的方法，用结果替换 $ F_o $ ，并重复利用$ F_o $ 和校准预测的过程，再次获得 $ F_{ref2} $ 两次，以获得 $ F_{ref2}$的改进版本。利用SSC，我们的模型可以根据其初始掩模预测大致自适应地关注最优区域，从而通过深入分析获得更高的性能。<br/><br/>  对于CSC，我们首先将改进后的特征$ F_{ref2} $输入到一个小分类头中，用于预测输入图像是否被篡改。如果图像被预测为真实，掩模预测很可能会有很多的假阳性（FP），所以我们增加二值化阈值来减少FP。另一方面，如果图像被预测为篡改，我们会降低二值化阈值以减少假阴性。给定输入图像被预测为篡改的概率P，CSC将预测掩模的二值化阈值从0.5调整到$~ min(max(1-P,\lambda),1-\lambda) $ ， $~ \lambda $ 设置为0.3。</p><h1 id="实验">5. 实验</h1><h2 id="受约束图像篡改定位ciml任务的实验">5.1受约束图像篡改定位CIML任务的实验</h2><p>  图像操作自动标注的任务可以作为一个CIML任务的评估。考虑到IMD20数据集[23]中的图像与我们要标注的目标图像非常相似，我们使用其中一部分使用IoU和F1-score来评估模型的性能。</p><h3 id="实施细节">5.1.2 实施细节</h3><p>  我们将IMD20中伪造的伪造图像分为SPG或SDG，并将它们以大约3：1的比例随机分成训练集和测试集。CASIAv2[3]和大约100万张通过使用COCO数据集合成的图像[14]也被用于训练。输入图像的大小调整为512x512，并在所有方法中应用一致的训练配置以进行公平比较。</p><h3 id="消融实验">5.1.3 消融实验</h3><p>  对于SPG，伪造图像与其真实图像之间的图像差异可以粗略地表示伪造区域，图像本身可以提供语义信息，帮助模型去噪差异映射。我们在IMD20SPG的测试集上对所提出的差异感知语义分割进行了消融实验，如表4的右侧所示，这两种方法都可以提高模型的性能。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172424444.png"alt="image-20240826172424444" /><figcaption aria-hidden="true">image-20240826172424444</figcaption></figure><p>表4。在IMD20SPG上进行的CIML实验。左：我们的差异感知语义分割的比较研究。右：对应的消融实验。“DMVN*”表示同时使用SDG和SPG数据进行训练的DMVN，类似于“DMAC*”。“Nonzero”表示使用一对图像之间的差值的非零区域，“OTSU”表示用OTSU二值化的差值。'w.o.Difference‘表示语义分割模型的输入只包含图像对，’w.o.Images‘表示只使用图像对的差异映射作为输入。“Ours(VGG)”表示我们的模型与DMAC具有相同的VGG主干。“Ours(VAN)”表示我们的模型使用VAN主干。</p><p>  对于SDG，语义对齐可以减少训练过程中的混淆，帮助我们的模型实现更好的泛化。我们在IMD20SDG的测试集上对所提出的语义对齐相关匹配进行了消融实验，结果如表5的右侧所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172752628.png"alt="image-20240826172752628" /><figcaption aria-hidden="true">image-20240826172752628</figcaption></figure><p>  显然，这两个提议的组件都有助于提高模型的更高性能。此外，所提出的质量评价评分（QES）允许自动过滤最令人满意的预测。由于IMD20的groundtruth中存在一些错误，我们的方法足以获得准确的自动注释。</p><h3 id="对qes的消融实验">5.1.4 对QES的消融实验</h3><p>  所提出的QES度量的目标是在数据集创建期间自动过滤掉糟糕的预测，其中groundtruth是不可用的。如表3所示，越高的QES阈值，精度就会越高。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173009052.png"alt="image-20240826173009052" /><figcaption aria-hidden="true">image-20240826173009052</figcaption></figure><p>  这是因为具有更大的高置信度和更清晰的边缘比率的预测大多更接近实际的groundtruth，而清晰度和置信度可以通过我们的QES很好地评估。因此，我们的QES显示了与IoU度量有很强的相关性。</p><h3 id="比较实验">5.1.5 比较实验</h3><p>  我们在与我们相同的数据上，使用它们的公共代码对DMVN [32]和DMAC[19]进行了重新训练，结果如表4和表5的左侧所示。显然，我们的方法明显优于这些以前的方法。值得注意的是，同时使用SPG和SDG数据训练的DMVN和DMAC在这两个任务上的表现都比只使用SPG或SDG数据训练的任务更差。</p><h2 id="图像篡改定位iml任务的实验">5.2 图像篡改定位IML任务的实验</h2><h3 id="实施细节-1">5.2.1 实施细节</h3><p>  我们采用ConvNeXt-Base[21]作为特征提取器，对模型进行160k次迭代的训练，批量大小为20，在按照之前的工作[6,12]进行训练时，输入大小设置为512x512。我们使用交叉熵损失和AdamW优化器[22]，学习速率从1e-4到1e-6。CASIAv2[3]和CAT-Net [12]中的合成数据集用于按照之前的工作[6,12]进行训练。</p><h3 id="对miml数据集的消融实验">5.2.2对MIML数据集的消融实验</h3><p>  除了我们的APSCNet外，我们分别用PSCC-Net [17]和CAT-Net[12]的公共代码重新训练了提出的MIML数据集。当使用MIML数据集进行训练时，我们对原始合成数据和MIML采用近似1：1的采样比，所有实验中的总训练量都是固定的，以便进行公平比较。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173603657.png"alt="image-20240826173603657" /><figcaption aria-hidden="true">image-20240826173603657</figcaption></figure><p>  如表2所示，MIML可以显著提高所有这些模型的性能，而在训练或测试过程中没有任何额外的负担。这是因为MIML可以大大缓解深度IML模型中人工伪造数据的严重短缺。为了进一步确认我们的MIML数据集的有效性，我们将IMD20数据集随机划分为10个12个样本的IMDP1和988个样本的IMDP2，用相同大小规模的IMDP1替换MIML数据集，用它们训练APSC-Net。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174020848.png"alt="image-20240826174020848" /><figcaption aria-hidden="true">image-20240826174020848</figcaption></figure><p>  如表6所示，尽管在训练中加入IMDP1减轻了领域差距，提高了模型在NIST16和IMDP2上的性能，但仍明显低于使用MIML训练的模型。显然，MIML可以通过其大量不同的手工伪造数据，显著提高深度模型的泛化能力。</p><h3 id="apsc-net的比较实验">5.3.3 APSC-Net的比较实验</h3><p>  我们在广泛使用的基准测试上比较了我们的APSC-Net与最先进的（SOTA）方法的性能。考虑到以前的方法执行不同的后处理，导致不公平（例如EVP[16]使用最佳阈值计算GT执行二值化），我们忽略了与他们所提出的方法无关的后处理和用一个固定的阈值0.5均匀地二值化预测，然后评估性能与普通IoU和F1-score指标。定量结果如表7所示，我们的APSC-Net在所有这些基准测试上都优于以前的最先进的方法。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174243317.png"alt="image-20240826174243317" /><figcaption aria-hidden="true">image-20240826174243317</figcaption></figure><p>  视觉比较的定性结果如图8所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174324363.png"alt="image-20240826174324363" /><figcaption aria-hidden="true">image-20240826174324363</figcaption></figure><h1 id="结论">6. 结论</h1><p>  在本文中，我们提出了一种新的约束图像处理定位（CIML）范例，称为CAAA，它分别处理共享探针组SPG和共享供体组SDG图像对。实验表明，该范式明显优于以往的CIML方法。在此范例下，训练后的模型被用于自动标注未标记的伪造图像，以进行图像操作定位。我们还提出了一种新的度量QES来自动排除错误的预测。因此，我们提出了一个大规模、多样化、高质量的数据集MIML，包括123,150张人工伪造的图像和像素级注释，这可以通过解决它们的数据稀缺问题来激发深度取证模型的潜力。此外，我们提出了一种新的有效模型APSC-Net用于图像操作定位。我们希望我们提出的CAAA范式、QES度量、MIML数据集和APSCNet能够为社区带来见解，并促进图像操作定位的现实应用。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</title>
      <link href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/"/>
      <url>/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>Multi-view Feature Extraction via Tunable Prompts is Enough for ImageManipulation Localization <ahref="https://openreview.net/forum?id=Ci5g2dnrMK"><imgsrc="https://img.shields.io/badge/ACMMM-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\text{Xuntao Liu},\text{YuzhouYang},\text{Haoyue Wang},\text{Qichao Ying},\\\text{ZhenxingQian}^*,\text{Xinpeng Zhang},\text{Sheng Li},\)</span></center><center>复旦大学计算机科学学院，NVIDIA上海</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/302_Multi_view_Feature_Extract.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  虚假图像可以通过社交网络服务迅速传播，构成重大风险。图像篡改定位（IML）的快速发展试图解决这个问题。然而，IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p>​  为了应对这一挑战，我们提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。<br/>​  具体来说，一组可调提示使冻结的预训练模型能够提取多视图特征，包括空间和高频特征。这种方法最大限度地减少了跨不同视图进行特征提取的冗余架构，从而降低了培训成本。<br/>​  此外，我们开发了一个即插即用的特征对齐和融合模块，该模块无缝集成到预训练的模型中，无需进行额外的结构修改。所提出的模块通过交互式处理降低了特征中的噪声和不确定性。</p><p>​  实验结果表明，我们提出的方法在6个测试数据集上取得了优异的性能，表现出卓越的鲁棒性。</p><h1 id="引言">1. 引言</h1><p>​  我们观察到，分类、目标检测和语义分割等任务具有许多具有丰富的先验知识的预训练模型，如双变压器[20]。考虑利用这些预先训练过的模型来处理IML任务中的挑战是很自然的。然而，直接将它们应用到IML任务中被证明是低效的[22]。这种低效源于IML任务的独特性质，该任务侧重于从图像中提取非语义的视觉线索和低层次的不连续性。有两个关键方面说明了这种特殊性：</p><p>​  1)高频信息：由不同的摄像机捕获的图像显示出不同的噪声模式[16]。这给伪造的图像带来不一致的噪声，而真实的区域来自不同的图像。此外，由不同网络生成的图像可能在频域[27]上存在差异。<br/>​  2)边缘信息：图像编辑的级别可能会发生变化，导致在锻造区域的边界上的锯齿状和不光滑的边缘或颜色不一致的[37]。这些细节对于精确的操作定位化至关重要，但在许多任务中经常被忽略。</p><p>​  IML-ViT[22]是在IML任务中使用基于普通ViT[6]体系结构的预训练模型的开创性尝试。它们还结合了边缘监督，将网络的注意力引向微妙的篡改伪影。然而，IML-ViT忽略了在以前的许多工作[4,14,15]中已经验证过的有效的高频信息。在IML任务中，处理多视图特征通常需要并行的主干架构[4,14]，这在参数增加的紧急情况下变得具有挑战性。此外，IML-ViT尽管利用了预先训练过的模型，但仍需要从头开始使用数据集来训练模型。这无疑对计算资源产生了巨大的需求，特别是在调优大型预训练模型方面。此外，之前的一些工作表明，在下游任务上调整大型预训练模型可能会损害模型[30]的性能，这在我们的比较实验中也可以观察到。</p><p>​  在本文中，我们提出了Prompt-IML，如图1所示，旨在通过利用预训练模型的丰富的先验知识来解决IML任务中数据集的稀缺问题。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824185716412.png" alt="image-20240824185716412" style="zoom:67%;" /></p><p>图1：Prompt-IML利用一个具有冻结参数的单一预训练主干，通过可调谐提示来处理多视图特性。特性对齐和融合模块被设计为特性交互和增强的即插即用组件。</p><p>​  具体来说，Prompt-IML遵循一个编码器-解码器架构。基于预训练模型的编码器负责特征提取，然后解码器对这些特征进行处理，以准确定位被操纵的区域。为了在不诉诸于复杂的并行架构的情况下处理有利于IML任务的多视图特性，我们建议使用可调提示集来利用预先训练好的模型作为编码器。在训练这些提示时，我们冻结了预先训练好的模型。它有三个主要的优势。首先，它允许预先训练好的模型用于处理每个视图中的特征。其次，处理后的特征保留了来自预训练模型的鲁棒性。最后，它有助于减少训练所需的计算资源。</p><p>​  此外，考虑到多视图特征之间的变化，我们提出了一个特征对齐和融合（FAF）模块。该模块被设计为即插即用组件，可以无缝集成到编码器，而不需要额外的结构修改。在FAF模块中，针对不同的优点采用了多种注意机制。FAF模块减少了特征中的噪声和不确定性，同时也抑制了零星的正响应，以确保输出一致。</p><p>​  为了公平地评估模型的能力，我们遵循了IML-ViT中概述的评估方案。它只涉及使用CASIA2数据集进行训练，然后对其他6个数据集进行测试。重要的是，我们确保了训练数据集和测试数据集之间的零数据重叠，使其成为一个跨数据集的评估。实验结果表明，所提出的Prompt-IML有效地利用了预训练模型中的先验知识，优于以往的先进方法，并表现出更强的鲁棒性。我们的贡献可以概括为三个方面：</p><p>​  我们的贡献可以总结为三个方面：</p><ul><li>我们引入了Prompt-IML来应对IML数据集的稀缺所带来的挑战。我们的方法通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性。</li><li>我们精心设计了一个即插即用的特性对齐和融合（FAF）模块，它可以无缝地集成到主干网中。它有效地减少了特征中的噪声和不确定性，同时减轻了零星的积极响应的影响。</li><li>Prompt-IML在6个测试数据集上都优于最先进的方法。我们的广泛的实验证实了我们的方法的普遍性和鲁棒性，也验证了所提出的FAF模块的有效性。</li></ul><h1 id="方法">2. 方法</h1><h2 id="方法概述">2.1 方法概述</h2><p>​  图2展示了所提出的Prompt-IML的管道设计，它遵循了通用的编码器-解码器框架。完整的传递途径包括特征提取和操作定位两个阶段。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><p>​  在特征提取阶段，我们采用预训练好的双子变压器作为骨干，并在训练过程中保持其参数冻结。同时，我们利用多组可调提示来分别调整图像的空间特征和高频特征。因此，这种方法避免了使用冗余的模型体系结构来从其他视图中提取特征。考虑到多视图特征的差异，我们提出了一个特征对齐和融合（FAF）模块进行处理。FAF模块集成在主干层之间，有效地降低了每层提取特征内的噪声和不确定性。同时，它们有助于抑制零星的积极反应，导致更一致的输出。这些模块都是即插即用的，不需要对主干本身进行任何修改。在操作定位阶段，我们使用掩模2前作为解码器，其中包括一个像素解码器和一个变压器解码器。解码器处理从前一阶段获得的多尺度特征，并产生最终的预测。</p><h2 id="特征提取阶段">2.2 特征提取阶段</h2><p>​  我们将输入图像表示为 $~\mathbf{X}\in\mathbb{R}^{\boldsymbol{h}\times\boldsymbol{w}\times3} $。为了获取空间特征的输入，我们将图像划分为指定大小的斑块：</p><p><spanclass="math display">\[\mathrm{F}_{0}^{RGB}=\mathrm{Norm}(\mathrm{Conv}(\mathrm{X}))+\mathrm{F}_{PE},\]</span>​  其中 $~ \mathbf{F}_0^{RGB}\in\mathbb{R}^{H\times W\times C} $，Conv表示分区操作， $~ \mathrm{F}_{PE} $是一个可学习的位置嵌入。接下来，我们使用一组具有不同大小的内核的BayarConv来提取高频特征：<spanclass="math display">\[\mathbf{F}_{0}^{HFQ}=\mathrm{Concat}(\{\mathrm{BayarConv}_{\mathrm{i\timesi}}(\mathbf{X})\}), i\in\{3,5,7\},\]</span> ​  其中 $~\mathbf{F}_0^{HFQ}\in\mathbb{R}^{H\times W\times C} $，i表示内核大小。所获得的特征将被发送到骨干网中以进行进一步的处理。</p><h3 id="具有可调调提示的多视角特征处理">2.2.1具有可调调提示的多视角特征处理</h3><p>​  我们采用预先训练的语义分割（SS, semanticsegmentation）任务中常用的Swin-Transformer作为主干，原因如下：<br/>​  1)Swin-Transformer包括一个与图像大小相比具有线性时间复杂度的窗口注意设计；<br/>​  2)补丁合并操作可以生成多尺度特征图，这在IML任务[4,11]中被证明是重要的。<br/>​  3)SS任务和IML任务有一些相似之处，因为它们本质上是像素级的分类任务。<br/>​  我们认为，用于SS任务的预训练模型，经过微调后，更有利于实现精确的像素级操作定位。<br/>​  Swin-Transformer包括4层，并具有特定分辨率的输出特征。我们将第i层的输出特征表示为$ F_i $ ：</p><p><spanclass="math display">\[\mathbf{F}_{i}=\mathrm{Layer}_{\mathrm{i}}\left(\mathbf{F}_{i-1}\right)\in\mathbb{R}^{(H_{i}\timesW_{i})\times C_{i}},i\in\{1,2,3,4\},\]</span></p><p>​  其中， $~H_{i}=\frac{H}{2^{i-1}},W_{i}=\frac{W}{2^{i-1}},C_{i}=C*2^{i-1} $ ， ${Layer}_i $ 象征着 Swin-Transformer的第i层。</p><p>​  我们采用了一种提示调优方法[12]，使一个单一的预训练模型能够同时处理空间和高频特征。具体来说，在训练过程中，我们在每一层利用两组提示分别处理空间特征和高频特征，同时冻结主干的参数。我们将第i层的输入特征表示为$~ F^{RGB}_{i-1} $ 和 $~ F^{HFQ}_{i-1} $ 。它们先重塑为 $~\mathbb{R}^{(H_{i-1}\times W_{i-1})\times C_{i-1}} $ ，然后分别加入提示$~ P^{RGB}_{i-1} $ 和 $~ P^{HFQ}_{i-1}\in\mathbb{R}^{n_{p}\timesC_{i-1}} $ 。因此，公式3的每一层过程已被变更为： <spanclass="math display">\[\mathbf{F}_{i}^{RGB}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{RGB},\mathbf{F}_{i-1}^{RGB}\right]\right),\\\mathbf{F}_{i}^{HFQ}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{HFQ},\mathbf{F}_{i-1}^{HFQ}\right]\right),\]</span>​  其中，[·]表示Concat操作。</p><h3 id="特征对齐和融合模块">2.2.2 特征对齐和融合模块</h3><p>​  针对骨干处理的空间和高频特征，我们提出了一个特征对齐和融合的FAF模块。FAF模块集成在主干网的一些相邻层之间，如图2所示。FAF模块由对齐阶段[36]和融合阶段组成，详细的组成和过程如图3所示。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826110222896.png"alt="image-20240826110222896" /><figcaption aria-hidden="true">image-20240826110222896</figcaption></figure><p>图3：所提出的特性对齐和融合（FAF）模块的设计和可调提示的使用。通道，空间，可变形分别表示等式5，等式6和等式9的过程。</p><h4 id="特征对齐阶段">特征对齐阶段</h4><p>​  在特征对齐阶段，我们同时利用通道注意和空间注意来研究特征的通道间和空间间的相关性，从而利用相应的信息增强特征。未经处理的特征从增强的特征中收集信息，减少了潜在的不确定性和噪声。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824221130411.png" alt="image-20240824221130411"  /></p><p>​  具体来说，我们首先使用平均池化操作（用上划线表示）来聚合信息。然后，将它们连接到$ C_i $ 的维数上，即[·]表示，并输入到MLP层以生成信道注意向量 $~\mathbf{W}_{i}^{C_{RGB}} $ ， $~\mathbf{W}_{i}^{C_{HFQ}}\in\mathbb{R}^{1\times1\times C_{i}} $。上述过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{C_{RGB}},\mathbf{W}_{i}^{C_{HFQ}}&amp;=\mathrm{ChannelAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}(\mathrm{MLP}([\overline{\mathbf{F}_{i}^{RGB}},\overline{\mathbf{F}_{i}^{HFQ}}])),\end{aligned}\]</span></p><p>​  其中，Split是Concat的反向操作。为了获得空间注意向量，我们利用两个1×1卷积与一个中间的ReLU层，用$ g() $ 表示，来聚合空间信息。获取空间注意向量 $~\mathbf{W}_{i}^{S_{RGB}} $ ， $~\mathbf{W}_{i}^{S_{HFQ}}\in\mathbb{R}^{H_i\times W_i\times1} $的过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{S_{RGB}},\mathbf{W}_{i}^{S_{HFQ}}&amp;=\mathrm{SpatialAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}\left(\mathrm{Conv}\left(g\left(\mathrm{Conv}\left(\left[\mathrm{F}_{i}^{RGB},\mathrm{F}_{i}^{HFQ}\right]\right)\right)\right)\right).\end{aligned}\]</span>​  最后，我们通过应用交叉注意向量对来自不同分支的特征进行对齐，通过元素级添加为下一个主干层产生输入：<spanclass="math display">\[\begin{aligned}&amp;\mathbf{F}_{i}^{C_{RGB}}=\mathbf{W}_{i}^{C_{RGB}}\odot\mathbf{F}_{i}^{RGB},\quad\mathbf{F}_{i}^{S_{RGB}}=\mathbf{W}_{i}^{S_{RGB}}\odot\mathbf{F}_{i}^{RGB},\\&amp;\mathbf{F}_{i}^{C_{HFQ}}=\mathbf{W}_{i}^{C_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\quad\mathbf{F}_{i}^{S_{HFQ}}=\mathbf{W}_{i}^{S_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\\&amp;\mathbf{F}_{i}^{RGB}:=\mathbf{F}_{i}^{RGB}+\mathbf{F}_{i}^{C_{HFQ}}+\mathbf{F}_{i}^{S_{HFQ}},\\&amp;\mathbf{F}_{i}^{HFQ}:=\mathbf{F}_{i}^{HFQ}+\mathbf{F}_{i}^{C_{RGB}}+\mathbf{F}_{i}^{S_{RGB}}.\end{aligned}\]</span>#### 特征融合阶段</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112404970.png"alt="image-20240826112404970" /><figcaption aria-hidden="true">image-20240826112404970</figcaption></figure><p>​  在特征融合阶段，我们首先利用不同膨胀率的扩张卷积DConv来处理特征图，增强斑块内的相互作用。具体来说，我们使用膨胀速率k∈{1,3,5}进行处理，然后在维度of$ C_i $上连接输出。对这些连接起来的特征进行处理，以整合信息和未经处理的特征：<spanclass="math display">\[\tilde{\mathrm{F}}_{i}=\mathrm{Conv}\left(\left[\mathrm{Conv}\left(\left[\mathrm{DConv}_{\mathrm{k\timesk}}\left(\mathbf{F}_{i}\right)\right]\right),\mathbf{F}_{i}\right]\right),k\in\{1,3,5\}.\]</span></p><p>​  然后，我们应用可变形注意力机制来促进多视图块间的信息交互。可变形注意机制不仅通过可学习偏移量采样降低了计算复杂度，还有助于抑制特征图中的零星积极反应，这有助于定位，因为篡改操作通常影响像素的特定区域，而不是孤立的特定区域[4]。从上一步的$ ~\mathbf{\tilde{F}}_{i}^{RGB} $ 和 $~ \mathbf{\tilde{F}}_{i}^{HFQ} $处理得到特征：</p><p><spanclass="math display">\[\mathbf{attn}^{RGB}=\mathrm{DeformAttn}_{1}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{RGB},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{HFQ}\right),\\\mathbf{attn}^{HFQ}=\mathrm{DeformAttn}_{2}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{HFQ},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{RGB}\right),\\\mathbf{F}_{i}^{d}=\gamma_{1}\cdot\left(\tilde{\mathbf{F}}_{i}^{RGB}+\mathbf{attn}^{RGB}\right)+\gamma_{2}\cdot\left(\tilde{\mathbf{F}}_{i}^{HFQ}+\mathbf{attn}^{HFQ}\right),\]</span></p><p>​  其中， $~ \gamma_{1} $ ， $~ \gamma_{2} $ 是可学习的参数。输出的 $~F^d_i $ 用于解码器。</p><h2 id="篡改定位阶段">2.3 篡改定位阶段</h2><p>​  为了细化上一阶段获得的多尺度特征，我们使用Mask2Former[3]作为解码器，它包括两个关键组件：像素解码器和Transformer解码器。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112712079.png"alt="image-20240826112712079" /><figcaption aria-hidden="true">image-20240826112712079</figcaption></figure><p>​  像素解码器负责逐步向上采样特征从低分辨率到高分辨率。Transformer解码器利用查询嵌入和多尺度特性进行定位。这种方法有几个优点。首先，利用多尺度特征有利于定位小的篡改区域。此外，掩膜注意力的查询嵌入的整合有助于限制交叉注意对被篡改区域的单独关注，从而增强了与篡改相关的特征提取。</p><h2 id="损失函数">2.4 损失函数</h2><p>​  考虑到被篡改区域的边界可能表现出锯齿状、非平滑的边缘和颜色不一致，我们从IML-ViT[22]中汲取灵感，并引入了边缘监督。具体来说，我们使用形态学操作，如侵蚀和膨胀操作，来处理掩模M并生成相应的边缘掩模$ M^★ $。与利用网络生成边缘预测[4]的方法相比，该策略不仅包含了边缘信息，同时还消除了调整骨干的需要，增强了其灵活性。损失函数包括两个分量，每个分量对应于对预测结果和预测边缘的监督：<spanclass="math display">\[\mathcal{L}=\mathcal{L}_{seg}(M_{gt},M_{pred})+\lambda\mathcal{L}_{edge}(M_{gt}^{\star},M_{pred}^{\star})\]</span></p><h1 id="实验">3 实验</h1><h2 id="实验设置">3.1 实验设置</h2><h3 id="数据集">数据集</h3><p>​  我们采用了一个关于IML任务的通用训练协议[2,22,37]，以促进模型性能的公平比较，并避免了私有合成数据集的影响。我们仅使用CASIA2[5]来训练Prompt-IML。6个公共测试数据集用于评估，包括CASIA1 [5]、NIST16[7]、COVERAGE[32]、Columbia[25]、IMD2020[26]和DEFACTO[23]。在MVSS-Net[2]之后，我们对来DEFACTO的抽样子数据集进行了测试，其中包含6000张真实图像和6000张经过处理的图像。评估构成了跨数据集分析，因为我们的训练集和测试数据集之间没有重叠。</p><h3 id="评估标准">评估标准</h3><p>​  我们使用像素级的F1分数来评估我们的模型在测试数据集上的性能。以往的一些方法采用了以最优阈值优化F1分数的策略，为每幅图像选择不同的阈值。然而，最优阈值的决定需要地面真实数据，这在现实场景中是不可行的。因此，我们以固定的阈值报告f1分数，它独立于模型本身，并提供了一个公平的模型性能评估。</p><h3 id="实施细节">实施细节</h3><p>​  我们在RTX 3090GPU上训练我们的模型 Prompt-IML80轮次，每个GPU的批处理大小为2。编码器和解码器都在COCO[17]上使用预先训练的权值进行初始化。除非另有说明，所有图像的大小都会被调整为1024×1024。在IML-ViT[22]之后，我们使用了简单和公共的数据增强技术，包括翻转、模糊、旋转、JPEG压缩、随机复制移动和在单个图像中矩形区域进行图像修复。我们使用基本学习速率为1×10−4的AdamW[21]优化器，并利用余弦衰减策略来调度学习速率。</p><h2 id="性能比较">3.2 性能比较</h2><p>​  我们将我们的方法与其他8种最先进的方法进行了比较，以全面评估我们的方法，并在表1中报告了F1评分。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224337339.png"alt="image-20240824224337339" /><figcaption aria-hidden="true">image-20240824224337339</figcaption></figure><p>​  我们可以观察到，我们的方法对每个改进的数据集的最佳基线分别为2.8%、4.6%、7.6%、8.1%和4.9%。与次优基线IML-ViT[22]相比，它平均提高了4.3%。这些都充分证明了我们的模型的优越性。然而，在COVER[32]数据集上，基于MVSS-net的方法[2,4]的性能优于所有其他方法。COVER是一个仅通过复制-移动技术创建的小型伪造图像数据集，大多数检测线索位于伪造区域的边界周围。因此，我们将这种现象归因于他们精心设计的边缘信息提取结构和数据增强技术。</p><p>​  此外，图4显示了每个模型的预测定位结果，每个图像来自一个不同的数据集，在操纵区域有很大的变化。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224529342.png"alt="image-20240824224529342" /><figcaption aria-hidden="true">image-20240824224529342</figcaption></figure><p>​  研究结果强调了该方法具有显著的泛化能力，表明该方法可以有效地利用嵌入在预训练模型中的先验知识来检测篡改痕迹。</p><h2 id="鲁棒性">3.3 鲁棒性</h2><p>​  在本节中，我们利用6个测试数据集来全面评估Prompt-IML的鲁棒性。在IML-ViT[22]之后，我们应用两种常见的攻击方法，即JPEG压缩和高斯模糊，在不同的扰动级别上，来创建被攻击的图像。计算结果如图5所示。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224656206.png"alt="image-20240824224656206" /><figcaption aria-hidden="true">image-20240824224656206</figcaption></figure><p>​  在JPEG压缩测试中，提出的pt-IML在4个数据集上保持明显的优势。在COVER和NIST16上，我们的方法与领先的方法非常接近。在高斯模糊测试中，Prompt-IML在所有数据集上都显著优于其他方法。</p><p>​  总的来说，与其他方法相比，Prompt-IML表现出了承受JPEG压缩和高斯模糊的显著能力，特别是针对后者。我们还注意到，IML-ViT比其他方法表现出更好的平均鲁棒性，因此我们将我们的方法的鲁棒性归因于更有效地利用大规模的预训练模型，因为这些模型可以学习更鲁棒的特征，因为它们存在于广泛的训练数据集。</p><p>​  值得注意的是，与IML-ViT相比，该方法在抵抗高斯模糊攻击方面有了显著的性能提高。我们认为，这些优势源于高频特性和促进调优的使用，从而导致了以下推测。首先，IML-ViT对预先训练好的网络进行了完全的微调，这可能会由于灾难性遗忘[30]而损害其鲁棒性。此外，不同特性对各种攻击的抵抗力也各不相同，因此充分利用多视图特性可能有助于提高该方法的鲁棒性。</p><h2 id="消融研究">3.4 消融研究</h2><p>​  我们按照表2中概述的设置进行了几个实验，以彻底评估我们的方法中各模块的有效性。我们报告了每个模型在COVER[32]、NIST16[7]和IMD20 [26]上的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225020324.png"alt="image-20240824225020324" /><figcaption aria-hidden="true">image-20240824225020324</figcaption></figure><h3 id="多视图特征的影响">多视图特征的影响</h3><p>​  在设置2中，我们使用一个具有冻结参数的单一主干，同时从图像中提取空间和高频特征。与仅使用空间特征的设置1相比，我们观察到高频特征的利用率分别增加了2.5%、1.9%和2.7%的F1分数，有效地证明了利用预先训练好的模型来处理多视图特征的可行性。</p><h3 id="faf模块的影响">FAF模块的影响</h3><p>​  所提出的FAF模块包括两个独立的阶段：对齐和融合。因此，我们使用设置3和设置4分别来验证每个阶段的有效性。在设置4中，我们跳过特征对齐阶段，并直接将特征传递到下一层。与设置5相比，我们注意到，当没有特征对齐阶段缺失时，所有三个数据集的F1得分都下降，分别下降了3.6%、2.9%和3.0%。在设置3中，我们跳过特征融合阶段，直接添加多视图特征作为融合特征。与设置5相比，特征融合阶段的缺失导致F1评分分别下降了3.0%、3.1%和7.1%。这些结果有效地证明了FAF模块通过特征之间的信息交互成功地增强了特征。</p><h2 id="预训练主干网络的选择">3.5 预训练主干网络的选择</h2><p>​  我们研究了选择不同的预训练模型作为骨干的影响。我们使用CLIP[28]，MAE[9]，SAM[13]和Swin-Transformer[20]。CLIP和MAE都采用了普通ViT的架构，而SAM则类似于Swin-Transformer。考虑到全局自注意机制的计算需求，特别是对大图像，我们将所有图像的大小调整为512×512来进行比较。此外，由于普通ViT输出的特征图大小固定，我们在FAF模块的每个融合阶段结束时合并了几个卷积，以与解码器的输入需求对齐。我们在表3中报告了不同主干网络的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225456518.png"alt="image-20240824225456518" /><figcaption aria-hidden="true">image-20240824225456518</figcaption></figure><p>​  在COCO[17]数据集上训练的语义分割任务的Swin-Transformer模型，提供了优越的结果。我们将这种成功主要归因于它在不同层的不同接受域，使它能够发现微妙的篡改痕迹。虽然CLIP是在一个大的数据集上进行预先训练的，但它强调文本和图像特征之间的对齐，因此单独使用图像编码器可能不是最佳的选择。此外，我们假设在SAM中的窗口注意机制的实现可能会限制其在低分辨率图像上的性能。因此，我们选择预先训练过的Swin-Transformer作为我们的主干网络。</p><h2 id="提示调优v.s.完全调优">3.6 提示调优v.s.完全调优</h2><p>​  我们比较了两种方法，提示调优和完全调优，用于将预先训练好的模型适应IML任务，并评估它们对模型性能的影响。当使用完全调优方法时，作为单一主干网络在同时处理空间和高频特征方面面临限制，我们按照[2,14]的指导方针将主干调整为双分支架构。表4显示了不同调优方法的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225735253.png"alt="image-20240824225735253" /><figcaption aria-hidden="true">image-20240824225735253</figcaption></figure><p>​  完全调优并不会在大多数数据集上带来显著的性能提高，但在COVER数据集上显示出了19.1%的显著改进。我们将这种异常现象归因于COVER数据集的小规模及其使用篡改技术的单一。尽管完全调优显示出一定程度的性能改进，但提示调优相比，双分支结构引入了更多的可训练参数。为了进行更直接的比较，我们不计算FAF模块和解码器的可学习参数，因为它们都包含在这两种方法中。对于提示调优的可学习参数的大小为0.09M，而对于完全调优的双分支主干的大小为93.14M。因此，快速调优更有利于适应大型模型的开发和处理多视图特性。</p><h1 id="结论">4 结论</h1><p>​  在本文中，我们探讨了利用现有的预训练模型来解决IML任务中公共可用数据集的稀缺性的潜力。我们提出了Prompt-IML，它利用单一的预先训练的网络通过可调提示提取多视图特征。采用专门设计的特征对齐融合（FAF）模块集成多视图特征，有效降低了特征的噪声和不确定性，抑制了零星的积极响应。在6个测试数据集上进行的大量实验表明，Prompt-IML具有优异的性能、更好的泛化能力和更高的鲁棒性。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DH-GAN:Image manipulation localization via a dual homology-aware generative adversarial network</title>
      <link href="/DH-GAN/"/>
      <url>/DH-GAN/</url>
      
        <content type="html"><![CDATA[<p>DH-GAN: Image manipulation localization via a dual homology-awaregenerative adversarial network<br/><spanclass="math inline">\(\text{Weihuang Liu}^1,\text{XiaodongCun}^1,\text{Chi-Man Pun}^*\)</span><br/>澳门大学科技学院计算机与信息科学系，澳门凼仔</p><h1 id="摘要">摘要</h1><p>图像操作定位是一种二值分割任务，对被篡改的伪影敏感，而不是物体的对象。因此，传统的方法和基于学习的方法都高度依赖于手工制作的特征。然而，这些特定定义的特征限制了网络对一般场景的能力。</p><p>为了解决这一问题，我们提出了一个双同源感知生成对抗网络（DH-GAN, dualhomology-aware generative adversarialnetwork），这是一种新的基于gan的框架来定位被操纵的区域。首先，我们通过使用选择性金字塔生成器重新校准多尺度编码特征来定位伪造区域。然后，我们在鉴别器中进行同源性识别。所提出的同源识别鉴别器包含一堆掩码卷积（MConv,maskedconvolution）层，并学习以硬门控的方式识别预测/目标掩蔽图像上的分割像素的真实/虚假。总的来说，这些网络是在一个标准的GAN下进行优化的。实验表明，该方法在四种流行的图像处理数据集上都优于其他最先进的算法。</p><p>我们的主要贡献总结如下：</p><ul><li>我们提出了一种双同源感知生成对抗网络（DH-GAN），这是一种新的基于gan的框架，用于由两个同源感知的鉴别器进行图像操作定位。</li><li>在生成器中，我们遵循编码器-解码器结构，设计了选择性金字塔（SAP），它使用融合的多阶段动作机制来选择和重新校准多尺度特征。</li><li>在每个同源感知鉴别器中，我们提出了一个基于mconv的网络，利用预测掩模与地面真实掩模来识别分割像素的同源性。</li><li>我们的方法在几个图像伪造检测基准上取得了最先进的性能。</li></ul><figure><img src="../postimages/DH-GAN/image-20240824153543322.png"alt="image-20240824153543322" /><figcaption aria-hidden="true">image-20240824153543322</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153751891.png"alt="image-20240824153751891" /><figcaption aria-hidden="true">image-20240824153751891</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153820459.png"alt="image-20240824153820459" /><figcaption aria-hidden="true">image-20240824153820459</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>IAD</title>
      <link href="/IAD/"/>
      <url>/IAD/</url>
      
        <content type="html"><![CDATA[<h1 id="cv-surveys">2024-CV-Surveys</h1><h2 id="industrial-anomaly-detection工业缺陷检测">Industrial AnomalyDetection(工业缺陷检测)</h2><ul><li><a href="https://arxiv.org/abs/2401.15448">A Systematic Review ofAvailable Datasets in Additive Manufacturing</a><br/> [2024-01-30]</li><li><a href="https://arxiv.org/abs/2406.07880">A Comprehensive Survey onMachine Learning Driven Material Defect Detection: Challenges,Solutions, and Future Prospects</a><br/> [2024-06-13]</li><li><a href="https://arxiv.org/abs/2406.07694">A PRISMA DrivenSystematic Review of Publicly Available Datasets for Benchmark and ModelDevelopments for Industrial Defect Detection</a><br/> [2024-06-13]</li><li>VAD<br/> - <a href="https://arxiv.org/abs/2401.16402">A Survey onVisual Anomaly Detection: Challenge, Approach, and Prospect</a><br/>[2024-01-30]</li><li>点云的工业系统 3D 缺陷检测和分类<br/> - <ahref="https://arxiv.org/abs/2402.12923">Advancements in PointCloud-Based 3D Defect Detection and Classification for IndustrialSystems: A Comprehensive Survey</a><br/> [2024-02-21]</li></ul><h1 id="cv-surveys-1">2023-CV-Surveys</h1><h2 id="anomaly-detection">Anomaly Detection</h2><ul><li><a href="https://arxiv.org/abs/2301.11514">Deep Industrial ImageAnomaly Detection: A Survey</a><br/> [2023-01-30]<br/> ⭐<ahref="https://github.com/M-3LAB/awesome-industrial-anomaly-detection">code</a></li></ul><h1 id="cv-surveys-2">2022-CV-Surveys</h1><h2 id="工业异常检测">工业异常检测</h2><ul><li><a href="https://arxiv.org/abs/2204.11161">A Survey on UnsupervisedIndustrial Anomaly Detection Algorithms</a><br/> [2022-04-26]</li></ul><h1id="promptad-learning-prompts-with-only-normal-samples-for-few-shot-anomaly-detection">PromptAD:Learning Prompts with only Normal Samples for Few-Shot AnomalyDetection</h1><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h2 id="摘要">摘要</h2><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h2 id="引言">引言</h2><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/IAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h2 id="前期准备工作">前期准备工作</h2><h3 id="clip和提示学习">CLIP和提示学习</h3><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h3 id="clip-surgery">CLIP Surgery</h3><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h2 id="方法论">方法论</h2><h3 id="概观">概观</h3><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><img src="./../postimages/IAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h3 id="语义连接">语义连接</h3><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h2 id="显式异常边缘">显式异常边缘</h2><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h3 id="异常检测">异常检测</h3><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h2 id="实验">实验</h2><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h3 id="数据集">数据集</h3><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h3 id="评估指标">评估指标</h3><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h3 id="实施细节">实施细节</h3><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p><h1 id="open-set-supervised-anomaly-detection">Open-set SupervisedAnomaly Detection</h1><p>Anomaly Heterogeneity Learning for Open-set Supervised AnomalyDetection</p><figure><img src="./../postimages/IAD/640.png" alt="图片" /><figcaption aria-hidden="true">图片</figcaption></figure><p>-这篇文章探讨了一种新兴的异常检测领域——开放集监督异常检测（Open-setSupervised Anomaly Detection,OSAD）。OSAD的目标是利用训练过程中所见异常类别的少量样本，来检测未见过的异常（即来自开放集异常类别的样本），同时有效地识别已见过的异常。现有的OSAD方法虽然能够通过所见异常的先验知识大幅减少误报错误，但它们通常在封闭集设置下训练，并且将异常样本视为来自同质分布，这限制了它们对来自任何分布的未见过异常的泛化能力。</p><p>-为了解决这一问题，文章提出了一种名为异常异质性学习（AnomalyHeterogeneity Learning,AHL）的新方法。AHL通过模拟多样化的异常分布，并利用这些分布来学习统一的异常模型，以在替代的开放集环境中进行学习。AHL是一个通用框架，现有的OSAD模型可以轻松地插入并使用，以增强它们的异常建模能力。通过在九个真实世界的异常检测数据集上的广泛实验，AHL不仅在检测已见和未见过的异常方面显著增强了不同的最新OSAD模型，而且能够有效地泛化到新领域中的未见过异常。</p><p>-文章首先介绍了异常检测的背景和挑战，然后详细阐述了AHL的框架和方法。AHL包括两个主要组件：异质异常分布生成（HeterogeneousAnomaly Distribution Generation,HADG）和异常异质性的协同可微学习（Collaborative Differentiable Learning,CDL）。HADG组件通过将正常样本的不同簇与随机选择的异常样本相结合，模拟并生成了多样化的异常分布数据集。CDL组件则设计为使用T个基础模型学习这些异常分布，并通过迭代验证和调整模型来优化统一的异常检测模型。</p><p>-此外，文章还提出了一种自监督的泛化性估计方法，以适应性地调整模型训练过程中每个学习到的异常分布的重要性。通过这种方式，AHL能够动态地评估基础模型的泛化能力，并据此调整它们在统一模型更新中的权重。</p><p>-在实验部分，作者展示了AHL在多个数据集上的性能，并与其他最新技术进行了比较。结果表明，AHL在检测同一领域和跨领域设置中的未见过异常方面，都取得了显著的性能提升。文章最后对AHL进行了深入的分析，包括对AHL组件的效用、少数样本的实用性以及超参数的敏感性进行了研究，并得出了有价值的结论。</p><p>-总的来说，这篇文章为开放集监督异常检测领域提供了一种新的视角和强大的工具，通过学习异常的异质性，显著提高了模型对未知异常的检测能力和泛化性。</p><h3 id="摘要-1">摘要</h3><p>开放集监督异常检测（OSAD）是一个最近出现的异常检测领域，其目的是利用训练过程中看到的一些异常类的样本来检测不可见的异常（即来自开放集异常类的样本），同时有效地识别可见的异常。得益于所见异常所说明的先验知识，目前的OSAD方法往往可以大大减少假阳性误差。然而，这些方法是在封闭集设置中训练的，并将异常例子视为齐次分布，使得它们在推广到可以从任何分布中得出的看不见的异常时效果较差。本文提出利用有限异常实例学习异质异常分布来解决这一问题。为此，我们引入了一种新的方法，即异常异质性学习（AHL），它模拟了一组不同的异构异常分布，然后利用它们在替代开放集环境中学习一个统一的异构异常模型。此外，AHL是一个通用的框架，现有的OSAD模型可以即插即用，以增强其异常建模。在9个真实世界异常检测数据集上的广泛实验表明，AHL可以1)显著增强不同的最先进的OSAD模型来检测可见和不可见的异常，2)有效地推广到新领域的不可见异常。</p><h3 id="引言-1">引言</h3><p>开放集监督AD（OSAD）是一个新兴的领域，旨在利用这些有限的训练异常数据学习广义模型来检测看不见的异常（即来自开放集异常类的样本），同时有效地识别那些可见的异常（即类似于训练异常例子的异常）。针对这个OSAD问题[1,15,24,32,68]，已经引入了许多方法。得益于由所看到的异常情况所说明的先验知识，当前的OSAD通常可以极大地减少假阳性误差。</p><figure><img src="./../postimages/IAD/QQ_1721787242975.png"alt="QQ_1721787242975" /><figcaption aria-hidden="true">QQ_1721787242975</figcaption></figure><p>目前的OSAD方法的一个问题是，它们将异常例子视为均匀分布，如图1(a)所示，这在很大程度上限制了它们在检测看不见异常方面的性能。这是因为异常可以由广泛的条件产生，并且天生是无界的，从而导致非均匀的异常分布（即，异常可以从非常不同的分布中得出)。例如，肿瘤图像可以根据肿瘤的性质，在外观、形状、大小、位置等方面显示出不同的特征。目前的OSAD方法忽略了这些异常的异质性，如果它们来自于与所看到的异常不同的数据分布，则往往无法检测到异常。</p><p>为了解决这个问题，我们建议用有限的训练异常例子来学习异构异常分布。这些异常只是可见异常类的例子，它们并不能说明所有可能的异常类的分布，例如，那些看不见的异常类，这使得在有限的异常信息下学习潜在的异构异常分布具有挑战性。这项工作引入了一个新的框架，即异常异质性学习（AHL），来解决这一挑战。如图1(b)所示，它首先通过将正态样本的细粒度分布与随机选择的异常样本关联起来，来模拟各种非均匀异常分布。然后AHL执行协作可微学习，综合所有这些异常分布，以学习异构异常模型。进一步，生成的异常数据使我们的模型的训练代理开放环境中，其中异常分布的一部分用于模型训练而其他作为看不见的数据来验证和调整模型，导致更好的广义模型比当前方法训练在一个封闭的设置。此外，模拟的异常分布通常具有不同的质量。因此，在AHL中设计了一种自监督泛化估计，以自适应地调整模型训练过程中每个学习到的异常分布的重要性。</p><p>AHL的另一种简单的替代方法是，在模拟的异构数据分布上，基于同构/异构OSAD模型的简单集成来建立一个集成模型。然而，这样的集合没有考虑到在基础模型中捕获的异常异质性的共性和差异，导致了对异质性的次优学习(Sec。4.5.2).</p><p>因此，本文做出了四个主要贡献。</p><p><strong>框架。</strong>我们提出了异常异质性学习（AHL，AnomalyHeterogeneityLearning），一个新的OSAD框架。与目前将训练异常例子视为均匀分布的方法不同，AHL通过这些有限的例子来学习异构异常分布，从而能够对不可见的异常进行更广义的检测。</p><p><strong>新的模型。</strong>我们进一步将AHL框架实例化为一个新的OSAD模型。该模型使用一组不同的模拟异构异常分布对异常异质性进行协同可微学习，促进了在替代开放集环境中对模型的迭代验证和调优。这使得比简单的集成方法更最优的异常异质性学习。</p><p><strong>通用的。</strong>我们的模型是通用的，其中来自不同OSAD模型的特性和损失函数可以即插即用，并获得显著提高的检测性能。</p><p><strong>具有较强的泛化能力。</strong>在9个真实世界的AD数据集上进行的实验表明，AHL在检测同域和跨域设置中看不见的异常方面大大优于最先进的模型。</p><h3 id="异常异质性学习">异常异质性学习</h3><h4 id="问题陈述">问题陈述：</h4><p>我们假设有一组训练图像和注释<spanclass="math inline">\(\left\{\omega_i,y_i\right\}_{i=1}\)</span>，其中$<em>i^{HWC}<span class="math inline">\(表示图像RGB通道和\)</span>y</em>{i}{0,1}<span class="math inline">\(表示一个图像级类标签，\)</span>y_{i}=1<spanclass="math inline">\(时\)</span><em>{i}<spanclass="math inline">\(为异常，\)</span>y</em>{i}=0<spanclass="math inline">\(时相反。由于异常的粗糙性，标记数据通常主要由正常数据表示。给定现有的AD模型f（·），可以用来提取低维图像特征来构造训练特征集\)</span>={_i,y_i}<spanclass="math inline">\(，其中\)</span>_i<sub>=</sub>f(_i)<sub></sub><spanclass="math inline">\(表示对应的第i个图像特征，\)</span>_n ={_1,_2,...,_N}<span class="math inline">\(和\)</span>_a = {_1,_2,...,_M}(N M)<spanclass="math inline">\(分别表示正常和异常图像的特征集，那么我们提出的AHL框架的目标是学习一个异常检测函数\)</span>g:<spanclass="math inline">\(，它能够为来自不同分布的异常图像分配更高的异常分数。请注意，在OSAD中，训练异常Xa来自于可见的异常类S，它只是C的一个子集，在推理过程中可以包含一个更大的异常类集，例如，\)</span>$。</p><h4 id="我们的方法概述">我们的方法概述</h4><p>我们的AHL框架的关键思想是通过对嵌入在不同模拟异常分布中的异常的协作可微学习，来学习一个统一的异常异质性模型。</p><figure><img src="./../postimages/IAD/QQ_1721788581713.png"alt="QQ_1721788581713" /><figcaption aria-hidden="true">QQ_1721788581713</figcaption></figure><p>图2。我们的方法AHL的概述。它的HADG组件首先从训练集<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中生成T个异构异常分布数据集，每个训练集都包含一个支持集和开放集查询集，即<spanclass="math inline">\(\mathcal{D}_i=\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>。然后利用它们在模拟的开放集环境中学习T个异构AD模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，并通过协作微分学习（CDL）将这些异构异常模型合成为一个统一的AD模型g（·）。不同的ϕi学习不同质量的异常分布，因此我们还设计了一个模型ψ（·），为每个ϕi分配一个重要性评分，以增强CDL成分。</p><p>如图2所示，AHL由两个主要组成部分组成：非均匀异常分布产生（HADG,Heterogeneous Anomaly DistributionGeneration）和异常异质性的协同可微分学习（CDL, CollaborativeDifferentiable Learning）。</p><p>具体来说，HADG组件从训练集<span class="math inline">\(\mathcal{T} =\{\mathcal{D}_i\}_{i=1}^T\)</span>模拟并生成T个异构分布数据集，每个Di包含正态数据子集和随机采样异常例子的混合。每个Di都是以一种代表不同于其他异常分布的方式生成的。然后设计CDL学习一个统一的异构异常检测模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>，该模型合成了一组T基模型，记为<spanclass="math inline">\(\left\{\phi_i\big(\mathcal{D}_i;\theta_i\big)\right\}_{i=1}^T\)</span>，其中<spanclass="math inline">\(\theta_{g}\)</span>和<spanclass="math inline">\(\theta_{i}\)</span>分别表示统一模型g和基模型<spanclass="math inline">\(\phi_{i}\)</span>的可学习权值参数，每个<spanclass="math inline">\(\phi_i:\mathcal{D}_i\to\mathbb{R}\)</span>从一个异常分布中学习进行异常评分。权重参数<spanclass="math inline">\(\theta_{g}\)</span>基于基础模型权重<spanclass="math inline">\(\{\theta_i\}_{i=1}^T\)</span>协同更新。此外，单个基模型的有效性差异很大，因此如果估计相应的基模型ϕi具有较小的泛化误差，则在CDL中添加一个模块ψ，以增加θi在协同权重更新中的重要性。在推理过程中，仅使用统一的异构异常模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>进行异常检测。</p><p>AHL是一个通用的框架，其中可以轻松地插入现成的OSAD模型来实例化ϕi，并获得显著提高的性能。</p><h4 id="非均匀异常分布产生hadg">非均匀异常分布产生HADG</h4><p>学习潜在的复杂异常的一个主要挑战是缺乏说明不同可能的异常分布的训练数据。我们的HADG组件是为了解决这一挑战，我们将正常范例划分为不同的簇，并将每个正常范例与随机抽样的异常示例关联起来，以创建不同的异常分布。由此产生的分布在正常模式和/或异常模式方面彼此不同。具体来说，HADG生成T个训练异常分布数据集，<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>，每个<spanclass="math inline">\(\mathcal{D}_i = \mathcal{X}_{n,i} \cup\mathcal{X}_{a,i}\)</span>，其中<spanclass="math inline">\(\mathcal{X}_{n,i}\subset\mathcal{X}_n\)</span>和<spanclass="math inline">\(\mathcal{X}_{a,i}\subset\mathcal{X}_a\)</span>。为了模拟高质量的异常分布，我应该代表一个主要的正态模式。为此，HADG采用聚类方法将Xn划分为C聚类，然后随机抽取这些C正常聚类中的一个为<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>。另一方面，为了保证每个Di、Xa中异常的多样性，我们从Xa和常用的异常生成方法[22,60,63]生成的伪异常中随机提取了<spanclass="math inline">\(\mathcal{X}_{a,i}\)</span>。</p><p>此外，HADG利用这些训练数据来创建开放集的检测和验证数据集，以便在代理OSAD环境中对我们的模型进行训练。特别是，对于每个Di，HADG将它分成两个不相交的子集，即<spanclass="math inline">\(\mathcal{D}_i =\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>，分别对应支持集和查询集，支持集<spanclass="math inline">\(\mathcal{D}_i^s=\mathcal{X}_{n,i}^s\cup\mathcal{X}_{a,i}^s\)</span>用来训练我们的基本模型ϕi，查询集<spanclass="math inline">\(\mathcal{D}_i^q=\mathcal{X}_{n,i}^q\cup\mathcal{X}_{a,i}^q\)</span>用于验证其开放集性能。保证开放的验证/查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>，我们执行抽样的方式，以确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>是两个不同的正常集群，同时确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>不相互重叠，例如，$<em>{a,i}^s</em>{a,i}^q=$。</p><h4id="异常异质性的协同可微分学习cdl">异常异质性的协同可微分学习CDL</h4><p>我们的CDL组件的目标是首先使用T个基模型ϕi学习隐藏在<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异构异常分布，然后利用这些模型以端到的方式协同优化统一检测模型g。CDL的详细介绍如下。</p><h5 id="学习t个异构异常分布">学习T个异构异常分布。</h5><p>我们首先训练T个基模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，分别捕获<spanclass="math inline">\(\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异质异常分布，每个ϕi使用以下损失进行优化：<spanclass="math display">\[\mathcal{L}_{\phi_i}=\sum_{j=1}^{|\mathcal{D}_i^s|}\ell_{dev}\left(\phi_i(\mathbf{x}_j;\theta_i),y_j\right),\]</span>其中<spanclass="math inline">\(\ell_{dev}\)</span>由偏差损失[32]指定，遵循之前的OSAD方法DRA[15]和DevNet[32]，<spanclass="math inline">\(\mathcal{D}_i^s\)</span>是<spanclass="math inline">\(\mathcal{D}_i\)</span>中的支持集。虽然在训练阶段只有有限的可见异常，但每个Di中的正常样本和异常样本的混合差异很大，使得每个ϕi可以学习不同的异常评分的异常分布。</p><h5 id="协作性可微分学习">协作性可微分学习。</h5><p>每个ϕi只捕获了潜在的异常异质性的全貌的一部分。因此，我们然后执行一个协作的可微学习，利用来自T个基模型的损失来学习统一的AD模型g，以捕获更丰富的异常异质性。关键的见解是，g经过了优化，可以很好地处理各种可能的异常分布，减轻对特定异常分布的潜在过拟合。此外，g的优化是基于在等式1中训练基本模型时没有看到的查询集上的损失，即在一个代理开放环境下进行优化，这有助于训练一个更广义的OSAD模型g。具体来说，g被指定为与基于模型ϕi具有完全相同的网络架构，其在t+ 1阶段的权重参数θg根据所有基础模型在t阶段的损失进行优化： <spanclass="math display">\[\theta_g^t\longleftarrow\theta_g^{t\boldsymbol{-}1}-\alpha\nabla\mathcal{L}_{cdl},\]</span>其中，α是一个学习速率，<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>是对查询集上的T个基模型的聚合损失：<spanclass="math display">\[\mathcal{L}_{cdl}=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right).\]</span>在下一个训练阶段，所有基础模型的<spanclass="math inline">\(\theta_i^{t+1}\)</span>设置为<spanclass="math inline">\(\theta_g^t\)</span>作为新的权重参数。然后，我们使用等式1优化基本模型ϕi，然后使用等式2在查询集上优化统一模型g。这种替代基础模型和统一模型学习用于获得日益捕获更丰富的异常异质性。</p><h5id="学习个体异常分布的重要性得分">学习个体异常分布的重要性得分。</h5><p>模拟异常分布数据Di的质量变化很大，导致基本模型的有效性存在较大差异。此外，在一个轮次效率较低的基础模型可以在另一个轮次变得更有效。因此，在整个优化动态过程中，平均考虑每一个基本模型可能会因为性能不佳的基础模型会影响统一模型g的整体性能从而导致劣等优化。为了解决这个问题，我们提出了一个自监督顺序建模模块来动态估计每个基模型在每个轮次的重要性。这就细化了<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>的损失如下： <spanclass="math display">\[\mathcal{L}_{cdl}^+=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}w_i^t\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right),\]</span>其中，<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>表示其基模型ϕi在t轮次的重要性得分。下面我们将介绍我们是如何通过ψ来学习<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>的。</p><p>我们顺序建模的基于动态重要性分数的估计是建立在直觉，如果一个基础模型ϕi有良好的泛化能力，其预测异常分数为不同的输入数据应该一致和准确的在不同的训练阶段，各种异常异质性逐渐出现随着训练的展开。为此，我们训练了一个序列模型ψ来捕获所有基本模型产生的异常分数的一致性和准确性。这是通过训练ψ使用基础模型之前的输出异常分数来预测它们的下一个轮次的异常分数来实现的。具体来说，给定一个训练样本xj和利用基础模型<spanclass="math inline">\(\left\{\phi_i\right\}_{i=1}^T\)</span>得到的一组异常评分预测<spanclass="math inline">\(\mathbf{s}_j=\begin{Bmatrix}s_{ji}\end{Bmatrix}_{i=1}^T\)</span>，结果在轮次t之前产生了一系列的分数预测，<spanclass="math inline">\(\mathbf{S}_j^t =[\mathbf{s}_j^{t-K},\cdots,\mathbf{s}_j^{t-2},\mathbf{s}_j^{t-1}]\)</span>记录到K个之前的步骤，然后<spanclass="math inline">\(\psi:\mathbf{S}\to\mathbb{R}^T\)</span>旨在预测所有T个基础模型在轮次t的预测得分。在我们的实现中，ψ由一个由θψ参数化的序列神经网络指定，并使用以下下一个序列预测损失进行优化：<spanclass="math display">\[\mathcal{L}_{seq}=\sum_{\mathbf{x}_j\in\mathcal{D}}\mathcal{L}_{mse}(\hat{\mathbf{s}}_j^t,\mathbf{s}_j^t),\]</span>其中，<spanclass="math inline">\(\hat{\mathbf{s}}_j^t=\psi(\mathbf{S}_j^t;\theta_\psi)\)</span>和<spanclass="math inline">\(\mathbf{s}_j^t\)</span>分别为在轮次t的基模型中xj的预测和实际异常得分，<spanclass="math inline">\(\mathcal{L}_{seq}\)</span>为均方误差函数。模型ψ不是使用监督损失，而是使用等式5中的自监督损失函数进行训练，以保留groundtruth标签，避免对标记数据的过拟合，有效地评价基础模型的泛化能力。</p><p>然后利用预测的异常得分<span class="math inline">\(\hat{s}_{ji}^t\)</span>与真实标签<spanclass="math inline">\(y_{j}\)</span>之间的差值来定义基本模型ϕi的泛化误差<spanclass="math inline">\(r_i^t\)</span>，如下： <spanclass="math display">\[r_i^t=\frac{1}{|\mathcal{D}^{\prime}|}\sum_{\mathbf{x}_j\in\mathcal{D}^{\prime}}c_j\mathcal{L}_{mse}(\hat{s}_{ji}^t,y_j),\]</span>其中，<spanclass="math inline">\(\mathcal{D}^{\prime}=\mathcal{D}\setminus\mathcal{X}_{n,i}\)</span>和<spanclass="math inline">\(c_{j}\)</span>是与每个范例<spanclass="math inline">\(x_{j}\)</span>关联的预定义的类别权重。换句话说，<spanclass="math inline">\(r_i^t\)</span>测量ϕi来预测<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}\)</span>和所有其他未看到的正常和异常训练例子中可见异常的异常得分时的检测误差，不包括已看到的正常例子<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>（与ϕi相关）。如果xj是一个看不见的异常，则分配一个较大的cj，以突出检测看不见的异常的重要性；否则，将为其他例子分配相同的值。</p><p>由于较大的<spanclass="math inline">\(r_i^t\)</span>意味着基模型ϕi在轮次t时的泛化能力较差，因此在更新统一模型g时应较少注意它。因此，将ϕi的重要性得分定义为其泛化误差的倒数如下：<spanclass="math display">\[w_i^t=\frac{\exp(-r_i^t)}{\sum_i^T\exp(-r_i^t)}.\]</span></p><h3 id="实验-1">实验</h3><h4 id="实验设置">实验设置</h4><h5 id="数据集-1">数据集</h5><p>在之前的OSAD研究[15,32]之后，我们对9个真实世界的异常检测数据集进行了广泛的实验，包括5个工业缺陷检测数据集MVTecAD [5]，AITEX [42]，SDD [44]，ELPV[13]和光学[50]，一个行星探测数据集（Mastcam [20]）和3个医疗数据集HeadCT[40]，BrainMRI [40]和Hyper-Kvasir[7]。根据我们如何对所看到的异常示例进行采样，我们使用两种协议来评估检测性能，一般设置和硬设置[15]。一般设置假设异常例子是从异常类中随机抽样的，而硬设置提出了一个更具挑战性的情况，即异常例子只从一个类中抽样，以评估对新的或看不见的异常类的泛化能力。与[15]一样，我们还将异常例子的数量分别设置为M= 10和M = 1来评估性能。关于这些数据集的更多细节请见附录A。</p><h5 id="比较的方法和评价指标">比较的方法和评价指标</h5><p>将AHL与五种密切相关的最先进的（SOTA）方法进行了比较，包括MLEP[24]、SAOE [22,30,45]、FLOS [23]、DevNet [32]和DRA[15]。MLEP、DevNet和DRA都是专门为OSAD而设计的。SAOE是一种增强了合成异常和异常值曝光的监督检测器，而FLOS是一种基于焦点损失的不平衡分类器。对于评价指标，我们采用广泛使用的ROC曲线下面积（AUC）来衡量所有方法和设置的性能。所有报告的结果都是三次独立运行的平均结果，另有说明。</p><h5 id="实施细节-1">实施细节</h5><p>为了生成一组不同的异常分布，我们提出的方法使用了随机选择的正常簇和标记的异常簇来创建每个单独的异常分布数据Di。具体来说，首先使用k-means聚类将正常样本划分为三个正常聚类（即使用k=3）。然后选择两个随机选择的聚类，结合可见异常，构造Di，选择一个正常的集群和50%的异常集作为支持集<spanclass="math inline">\(\mathcal{D}_i^s\)</span>，而其余的样本用作查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>（根据只有一个可见的异常例子的协议，该示例都包含在这两个集合中）。这有助于有效地模拟具有部分观察到的异常分布的开放集环境。为了进一步增加异常分布数据集内部和之间的异质性，我们随机选择三种流行的异常生成技术中的一种，包括CutMix[60]、CutPaste [22]和DRAEM Mask[63]，来生成伪异常并注入Di的支持和查询集。以保证开放集相关的伪异常检测、<spanclass="math inline">\(\mathcal{D}_i^s\)</span>和<spanclass="math inline">\(\mathcal{D}_i^q\)</span>中的伪异常都是由两种不同的异常生成方法生成的。对于每个数据集，都使用T=6来生成单个的异常分布数据。当xj表示看不见的异常样本时，Cj设置为1.0，当xj表示可见的异常或看不见的正常样本时，Cj设置为0.5。</p><p>AHL是一个通用框架，在该框架下，现有OSAD模型的特性和损耗函数可以很容易地作为基本特性和基本损耗插入。特别是，从其中一个OSAD模型（如DRA）中提取图像特征，然后使用我们提出的基于基础损失的损失函数来训练AHL(见等式4).DRA [15]、DevNet [32]和BGAD[58]是目前OSAD使用的SOTA模型，但BGAD使用的与其他两个数据集非常不同的基准数据集。我们的实验严格遵循DRA[15]和DevNet [32]中使用的开创性的OSAD评估协议和基准，并选择DRA[15]和DevNet[32]分别插入AHL，表示为AHL（DRA）和AHL（DevNet）。Adam被用作优化器。学习异构T基模型的初始学习率设置为0.0002，而统一AD模型g的初始学习率设置为0.002。在自监督重要性评分估计器中，采用两层双向LSTM[67]作为骨干，隐藏维数设置为6。在预测层之前，后面是一个有12个隐藏节点的全连接层。该组件的初始学习率被设置为0.002。</p><p>上述设置默认用于所有数据集的AHL报告结果。MLEP、SAOE和FLOS的结果取自[15]。DevNet和DRA的结果使用他们的官方代码进行复制，以获得AHL中使用的特性，这意味着DevNet和AHL（DevNet）使用相同的特性集，这也适用于DRA和AHL（DRA）（更多的实现细节请参见附录B）。</p><h5 id="在一般设置下的性能">在一般设置下的性能</h5><p>表1显示了在一般设置下的比较结果，其中模型使用一个或10个随机抽样的异常例子进行训练。</p><figure><img src="./../postimages/IAD/QQ_1721792278221.png"alt="QQ_1721792278221" /><figcaption aria-hidden="true">QQ_1721792278221</figcaption></figure><p>MVTecAD上的结果在其16个数据子集上取平均值（关于这些子集的详细结果见附录C）。总的来说，我们的方法AHL在三个应用场景的所有数据集的10个镜头和一次性设置协议中都为各自的DRA和DevNet带来了持续的实质性改进。由于DRA是一个比DevNet更强的基础模型，因此AHL（DRA）通常比AHL（DevNet）获得更好的性能。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PromptAD</title>
      <link href="/PromptAD/"/>
      <url>/PromptAD/</url>
      
        <content type="html"><![CDATA[<p>PromptAD: Learning Prompts with only Normal Samples for Few-ShotAnomaly Detection</p><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h1 id="摘要">摘要</h1><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h1 id="引言">引言</h1><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/PromptAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h1 id="前期准备工作">前期准备工作</h1><h2 id="clip和提示学习">CLIP和提示学习</h2><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h2 id="clip-surgery">CLIP Surgery</h2><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h1 id="方法论">方法论</h1><h2 id="概观">概观</h2><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><imgsrc="./../postimages/PromptAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h2 id="语义连接">语义连接</h2><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h1 id="显式异常边缘">显式异常边缘</h1><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h2 id="异常检测">异常检测</h2><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h1 id="实验">实验</h1><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h2 id="数据集">数据集</h2><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h2 id="评估指标">评估指标</h2><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h2 id="实施细节">实施细节</h2><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DeepClustering</title>
      <link href="/DeepClustering/"/>
      <url>/DeepClustering/</url>
      
        <content type="html"><![CDATA[<h1 id="聚类算法">聚类算法</h1><p>​聚类算法是一类无监督学习算法，用于将数据分组成具有相似性的簇或群体。</p><h2 id="传统算法">传统算法</h2><h3 id="k均值聚类k-means-clustering">K均值聚类（K-MeansClustering）</h3><ul><li>优点：<br/> - 简单易懂，容易实现。<br/> - 适用于大规模数据。<br/> -速度较快，适用于许多应用。</li><li>缺点：<br/> - 需要预先指定簇的数量K。<br/> -对初始簇中心的选择敏感。<br/> - 对异常值和噪声敏感。<br/> -适用于凸形簇。</li></ul><h3 id="层次聚类hierarchical-clustering">层次聚类（HierarchicalClustering）</h3><ul><li>优点：<br/> - 不需要预先指定簇的数量。<br/> -可以生成层次化的簇结构。<br/> - 适用于不规则形状的簇。</li><li>缺点：<br/> - 计算复杂性较高，不适用于大规模数据。<br/> -结果的可解释性较差。</li></ul><h3 id="密度聚类density-based-clustering">密度聚类（Density-BasedClustering）</h3><ul><li>优点：<br/> - 能够发现任意形状的簇。<br/> -对噪声和异常值相对稳健。<br/> - 不需要预先指定簇的数量。</li><li>缺点：<br/> - 对参数的选择敏感。<br/> -不适用于数据密度差异很大的情况。</li></ul><h3 id="谱聚类spectral-clustering">谱聚类（Spectral Clustering）</h3><ul><li>优点：<br/> - 能够发现任意形状的簇。<br/> -适用于不规则形状的簇。<br/> - 不受初始簇中心的选择影响。</li><li>缺点：<br/> - 计算复杂性较高，对于大规模数据不适用。<br/> -需要谨慎选择相似度矩阵和簇数。</li></ul><h3id="dbscandensity-based-spatial-clustering-of-applications-with-noise">DBSCAN（Density-BasedSpatial Clustering of Applications with Noise）</h3><ul><li>优点：<br/> - 能够自动发现任意形状的簇。<br/> -对噪声和异常值相对稳健。<br/> - 不需要预先指定簇的数量。</li><li>缺点：<br/> - 对于高维数据，需要特别注意参数的选择。<br/> -可能在数据密度差异较大时效果不佳。</li></ul><h3id="em聚类expectation-maximization-clustering">EM聚类（Expectation-MaximizationClustering）</h3><ul><li><p>优点：<br/> - 适用于混合模型，可以发现概率分布簇。<br/> -适用于数据有缺失值的情况。</p></li><li><p>缺点：<br/> - 对初始参数的选择敏感。<br/> -对于高维数据，需要特别注意参数的选择。</p></li><li><p>模糊聚类（Fuzzy Clustering）</p></li><li><p>优点：<br/> -能够为每个数据点分配到多个簇，考虑数据的不确定性。<br/> -适用于模糊分类问题。</p></li><li><p>缺点：<br/> - 计算复杂性较高。<br/> -结果的可解释性较差。</p></li></ul><p>选择适当的聚类方法通常取决于数据的性质、问题的要求以及计算资源的可用性。聚类算法可以用于数据探索、模式发现、异常检测等多种应用，但需要根据具体情况进行选择和调整。</p><h2 id="基于网络的算法">基于网络的算法</h2><h3id="自编码聚类算法--dec-deep-embedded-clustering">自编码聚类算法--DEC(Deep Embedded Clustering)</h3><h3 id="软分配">1.软分配</h3><p>​  使用t-SNE算法的t-分布作为核来衡量嵌入点<spanclass="math inline">\(z_i\)</span>和质心<span class="math inline">\(\mu_j\)</span>之间的相似度： <spanclass="math display">\[q_{ij}=\frac{(1+\|z_i-\mu_j\|^2/\alpha)^{-\frac{\alpha+1}2}}{\sum_{j&#39;}(1+\|z_i-\mu_{j&#39;}\|^2/\alpha)^{-\frac{\alpha+1}2}}\]</span>​  其中<span class="math inline">\(z_i =f_{\theta}(x_i)\inZ\)</span>对应于嵌入后的<span class="math inline">\(x_i\inX\)</span>，其中<spanclass="math inline">\(\alpha\)</span>是t-SNE算法t-分布的自由度，而<spanclass="math inline">\(q_{ij}\)</span>可解释为将样本<spanclass="math inline">\(i\)</span>分配给聚类<spanclass="math inline">\(j\)</span>的概率（即软分配）。由于我们无法在无监督的设置中对验证集上的<spanclass="math inline">\(\alpha\)</span>进行交叉验证，并且得知它是多余的，因此对于所有实验，我们让<spanclass="math inline">\(\alpha=1\)</span>。</p><h3 id="kl分流最小化">2.KL分流最小化</h3><p>​  使用辅助分布用来衡量样本属于某个聚类的分布，在辅助目标分布的帮助下，通过从集群的高可信度分配中学习来迭代地优化集群。具体来说，通过将软分配与目标分布匹配来训练我们的模型。为此，我们将目标定义为软分配qi和辅助分布pi之间的KL散度损失，如下所示：<spanclass="math display">\[L=\mathrm{KL}(P\|Q)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}.\]</span></p><p>​  目标分布P的选择对于DEC的性能至关重要。一种方法是将每个pi设置为高于置信度阈值的数据点的delta分布（至最接近的质心），并忽略其余部分。但是，由于qi是软分配，因此使用较软的概率目标更为自然和灵活。<br/>​  具体来说，我们希望我们的目标分布具有以下属性：<br/>​  （1）加强预测（即提高簇纯度），<br/>​  （2）更加注重以高置信度分配的数据点<br/>​  （3）归一化每个质心的损耗贡献，以防止大型聚类扭曲隐藏的特征空间。<br/>​  在我们的实验中，我们通过先将qi升至第二次幂，然后通过每个群集的频率进行归一化来计算pi：<spanclass="math display">\[p_{ij}=\frac{q_{ij}^2/f_j}{\sum_{j&#39;}q_{ij&#39;}^2/f_{j&#39;}}\\f_{j}= \sum_{i}q_{ij}\]</span> ​  <spanclass="math inline">\(f_j\)</span>为软簇频率。培训策略可以看作是一种自我培训的形式。与自训练中一样，我们采用初始分类器和未标记的数据集，然后使用分类器标记数据集，以便对其自身的高置信度预测进行训练。实际上，在实验中，我们观察到DEC通过学习高置信度预测来提高每次迭代中的初始估计，从而有助于改善低置信度预测。</p><h3 id="损失的优化过程">3.损失的优化过程</h3><p>​  我们使用带有动量的随机梯度下降（SGD）联合优化聚类中心<spanclass="math inline">\(\mu_j\)</span>和DNN参数<spanclass="math inline">\(\theta\)</span>。关于每个数据点zi和每个聚类质心<spanclass="math inline">\(\mu_j\)</span>的特征空间嵌入的L梯度计算如下：<span class="math display">\[\begin{aligned}\frac{\partial L}{\partialz_{i}}&amp;=\quad\frac{\alpha+1}{\alpha}\sum_{j}(1+\frac{\|z_{i}-\mu_{j}\|^{2}}{\alpha})^{-1}\times(p_{ij}-q_{ij})(z_{i}-\mu_{j}),\\\frac{\partialL}{\partial\mu_{j}}&amp;=\quad-\frac{\alpha+1}{\alpha}\sum_{i}(1+\frac{\|z_{i}-\mu_{j}\|^{2}}{\alpha})^{-1}\times(p_{ij}-q_{ij})(z_{i}-\mu_{j}).\end{aligned}\]</span>​  然后将梯度∂L/∂zi向下传递到DNN，并在标准反向传播中用于计算DNN的参数梯度∂L/∂θ。在两次连续的迭代中，当有少于tol%的点会更改聚类簇的时候，停止执行该过程。<br/>​  第一个公式是优化AE中的Encoder参数，第二个公式是优化聚类中心。也就是说作者同时优化了聚类和DNN的相关参数。</p><p>作者设计的网络概念图如下:</p><figure><imgsrc="../postimages/DeepClustering/377271-20181021223727573-1521567551.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  DEC算法由两部分组成，第一部分会预训练一个AE模型；第二部分选取AE模型中的Encoder部分，加入聚类层，使用KL散度进行训练聚类。</p><details open><br/><summary>深度聚类</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/Deep-Clustering-Survey/">Deep Clustering: A ComprehensiveSurvey</a> <ahref="https://ieeexplore.ieee.org/abstract/document/10585323"><imgsrc="https://img.shields.io/badge/TNNLS-2024-yellow"alt="TNNLS" /></a></label></li><li><label><input type="checkbox" />A Survey on Deep Clustering: Fromthe Prior Perspective <a href="https://arxiv.org/abs/2406.19602"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Image Clustering with ExternalGuidance <a href="https://arxiv.org/abs/2310.11989"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Scaling Up Deep Clustering MethodsBeyond ImageNet-1K <a href="https://arxiv.org/abs/2406.01203"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Unsupervised Learning of VisualFeatures by Contrasting Cluster Assignments <ahref="https://arxiv.org/abs/2006.09882"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li></ul></details><h1 id="deep-clustering">Deep Clustering</h1><p>(来自于<strong>Twin Contrastive Learning for OnlineClustering</strong>)</p><p>​  有效的聚类策略和判别特征都是实现良好聚类的关键。由于深度神经网络强大的代表性性，深度聚类方法最近引起越来越多的关注（Asano等，2019；卡隆等，2018；郭等，2017；李等，2020,2021a；彭等人，2016；谢等人，2016；Yang等人，2016）。例如，JULE（Jangetal.，2016）迭代地学习数据表示并执行分层聚类。深度聚类（Caronetal.，2018）使用先验表示对数据进行聚类，并使用每个样本的聚类分配作为分类目标来学习新的表示。虽然表示学习和聚类可以在一定程度上相互引导，但这种两阶段的方法可能会出现在交替过程中积累的错误。这些方法的另一个缺点是，它们不能应用于在线场景，即数据以流的形式呈现，并且一次只能访问一批样本。具体来说，JULE需要全局相似性来决定应该合并哪些子集群，而Deep集群和SL（Asanoetal.，2019）需要执行离线k-means或解决全局最优传输问题来获得集群分配。为了克服离线的局限性，提出了一些在线深度聚类方法（Dang等，2021；黄等，2020；吉等，2019；李等，2021b；钟等，2020）。例如，IIC（Ji等人，2019）通过最大化数据对的集群分配之间的互信息来发现集群。PICA（Huangetal.，2020）通过最大化聚类解决方案的分区置信度来学习语义上最可信的数据分离。最近，一些研究（Niu&amp; Wang，202120；Park等人，2020；VanGansbeke等人，2020）使用初步聚类生成的伪标签，即自标记，以多阶段方式进一步提高聚类性能。<br/>​  与上述大多数在多个阶段执行表示学习和聚类的工作不同，我们的方法将这两个任务统一到双对比学习框架中。与之前只进行即时对比学习的研究相比，这种单阶段学习范式有助于模型学习更多有利于聚类的表征（Niu&amp; Wang，2021；Van Gansbeke etal.，2020）。在推进阶段，尽管基于早期提取的特征修正了聚类分配（Niu &amp;Wang，2021），但我们也可以通过单阶段学习范式，微调实例级对比学习，以减轻假阴性对的影响。</p><h1 id="deep-robust-clustering-by-contrastive-learning">Deep RobustClustering by Contrastive Learning</h1><h2 id="摘要">摘要</h2><p>​  最近，许多无监督的深度学习方法被提出来学习与无标记数据的聚类。通过引入数据增强，大多数最新的方法从原始图像及其转换应该共享相似的语义聚类分配的角度来进行深度聚类。然而，由于softmax函数只对最大值敏感，因此，即使分配给同一集群的表示特征也可能完全不同。这可能导致表示特征空间的高类内多样性，从而导致局部最优不稳定，从而损害聚类性能。为了解决这个缺点，我们提出了深度鲁棒聚类（DRC，<strong>D</strong>eep<strong>R</strong>obust<strong>C</strong>lustering）。与现有的方法不同，DRC从语义聚类分配和表示特征两个角度进行深度聚类，可以同时增加类间多样性，减少类内多样性。与现有的方法不同，DRC从语义聚类分配和表示特征两个角度进行深度聚类，可以同时增加类间多样性，减少类内多样性。此外，我们总结了一个一般的框架，通过研究互信息和对比学习之间的内部关系，可以将任何最大化的互信息转化为最小化对比损失。我们成功地将其应用于DRC，学习不变特征和鲁棒聚类。在6个广泛采用的深度聚类基准上的广泛实验表明，DRC在稳定性和准确性方面具有优越性。例如，在CIFAR-10上达到了71.6%的平均准确率，比最先进的结果高出7.1%。</p><h2 id="方法">方法</h2><h3 id="问题定义">问题定义</h3><p>​  给定一组来自K个不同语义类的未标记图像I = {I1，...，IN}。深度聚类的目的是通过卷积神经网络（CNN）模型将图像分离为K个不同的聚类，从而将具有相同语义标签的图像简化为相同的聚类。在这里，我们旨在学习一个基于参数为θ的映射函数Φ的深度CNN网络，然后每个图像Ii都可以映射到一个k维分配特征<spanclass="math inline">\(z_{i}=\Phi_{\theta}(I_{i})\)</span>。在此基础上，可以通过softmax函数得到分配概率向量pi，该函数可由<span class="math display">\[p_{i j}=\frac{e^{z_{ij}}}{\sum_{t=1}^{K}e^{z_{i t}}},j=1,...,K\]</span>​  然后可以用最大似然法预测聚类分配： <spanclass="math display">\[\ell_{i}=\arg\operatorname*{max}_{j}(p_{ij}),j=1,\dots,K,i=1,\dots,N\]</span></p><h3 id="网络架构">网络架构</h3><p>​  为了解决上述问题，我们引入了一种新的端到端深度聚类框架，同时利用分配概率和分配特征。</p><figure><img src="../postimages/DeepClustering/image-20241218212302013.png"alt="image-20241218212302013" /><figcaption aria-hidden="true">image-20241218212302013</figcaption></figure><p>​  如图2所示，我们首先采用深度卷积神经网络（CNN）来生成K维的分配特征和分配概率。然后，利用基于分配概率的对比损失来保持原始图像及其增强图像的分配一致性，这有助于增加类间的方差，形成分离良好的聚类。利用基于分配特征的对比损失来捕获原始图像及其增强图像之间的表示一致性，有助于减少类内方差，实现更鲁棒的聚类。</p><h3 id="互信息与对比学习">互信息与对比学习</h3><p>​  对比学习已被证明在无监督学习和自我监督学习中是强大的，这有助于在许多任务中实现最先进的结果。而对比损失也与互信息密切相关。设X= {x1，x2，...，xN }是一个给定空间中的N个样本。X的变换由X 0 = {x 01，x02，...，x 0N}定义。因为我们不知道X的ground-truth，我们所知道的是，xi’都可以被视为xi的正样本，对于任何的i=1,2，...，N。换句话说，<spanclass="math inline">\(p({x_{i}^{\prime}}|x_{i})\)</span>应该比<spanclass="math inline">\(p({x_{j}^{\prime}}|x_{i}),j \neqi\)</span>。一个非常自然的想法是最大限度地保留X和X'之间的互信息，定义为<span class="math display">\[M I({\bf X},{\bfX}^{\prime})=\sum_{i=1}^{N}\sum_{j=1}^{N}p(x_{i},x_{j}^{&#39;})l og{\frac{p(x_{j}^{&#39;}|x_{i})}{p(x_{j}^{&#39;})}}\]</span>​  如果我们假设 <spanclass="math display">\[\frac{p(x_{j}^{&#39;}|x_{i})}{p(x_{j}^{&#39;})}\proptof(x_{i},x_{j}^{&#39;})\]</span>​  其中f是一个在不同的情况下可能会有不同的函数，那么我们有以下定理。<br/><strong>定理1</strong>假设存在一个常数c0，使得<spanclass="math inline">\(p({x_{i}^{\prime}}|x_{i})&gt;0\)</span>对所有的i =1,2，...，N都成立，那么 <span class="math display">\[M I({\bf x},{\bfx}^{&#39;})\geq\logN+\frac{c_{0}}{N}\sum_{i=1}^{N}\log\frac{f(x_{i},x_{i}^{&#39;})}{\sum_{t=1}^{N}f(x_{i},x_{t}^{&#39;})}\]</span>​  定义 <spanclass="math display">\[\mathcal{L}_{c}=\sum_{i=1}^{N}\log\frac{f(x_{i},x_{i}^{\prime})}{\sum_{t=1}^{N}f(x_{i},x_{t}^{\prime})},\]</span>​  因此，最小化对比损失Lc等于最大化互信息MI（X，X 0）的下界。</p><h3 id="损失函数">损失函数</h3><p>​  我们的损失函数由三个部分组成：1。一种基于分配特征的对比损失，在特征水平上保留互信息。2.一种基于分配概率的对比损失，使原始图像的预测标签与转换图像的预测标签之间的互信息最大化。3. 聚类正则化损失是为了避免平凡的解。</p><h1 id="deep-clustering-1">Deep Clustering</h1><p>(来自于<strong>Twin Contrastive Learning for OnlineClustering</strong>)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸识别与深度鉴伪研究进展</title>
      <link href="/20240627%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240627%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<p><ahref="https://www.bilibili.com/video/BV1F4421D7DL/">【在线课程】人脸识别与深度鉴伪研究进展（CSIG图像视频通信专委会青年学者沙龙第七期）_哔哩哔哩_bilibili</a></p><p>深度伪造反制技术需求迫切</p><figure><img src="./../postimages/0627/image-20240627201621844.png"alt="image-20240627201621844" /><figcaption aria-hidden="true">image-20240627201621844</figcaption></figure><figure><img src="./../postimages/0627/image-20240627201913441.png"alt="image-20240627201913441" /><figcaption aria-hidden="true">image-20240627201913441</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202139680.png"alt="image-20240627202139680" /><figcaption aria-hidden="true">image-20240627202139680</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202304130.png"alt="image-20240627202304130" /><figcaption aria-hidden="true">image-20240627202304130</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202334148.png"alt="image-20240627202334148" /><figcaption aria-hidden="true">image-20240627202334148</figcaption></figure><p>总结<br/>遮挡人脸识别<br/>√渐进式学习-----兼顾非口罩人脸识别性能<br/>√无标签样本助力-----适应真实口罩遮挡<br/>√遮挡预测与身份特征耦合学习-----应对多样性遮挡</p><p>伪造人脸检测及溯源<br/>√隐身份驱动-----解释伪造人脸检测<br/>√身份解耦溯源-----追溯目标人脸</p>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAA+</title>
      <link href="/SAA/"/>
      <url>/SAA/</url>
      
        <content type="html"><![CDATA[<p>Segment Any Anomaly without Training via Hybrid PromptRegularization</p><p>中国华中科技大学数字制造设备与技术国家重点实验室</p><h1 id="摘要">摘要</h1><p>​  我们提出了一个新的框架，即分段任意异常+（SAA+, Segment Any Anomaly+），用于混合快速正则化的零快速异常分割，以提高现代基础模型的适应性。现有的异常分割模型通常依赖于特定领域的微调，限制了它们在无数异常模式上的泛化。在这项工作中，受到基础模型强大的零镜头泛化能力的启发，我们首先探索它们的组装，以利用不同的多模态先验知识进行异常定位。对于非参数基础模型对异常分割的自适应，我们进一步引入了来自领域专家知识和目标图像上下文的混合提示作为正则化。我们提出的SAA+模型在几个异常分割基准测试上取得了最先进的性能，包括VisA、MVTec-AD、MTD和KSDD2。code:https://github.com/caoyunkang/Segment-Any-Anomaly</p><h1 id="介绍">介绍</h1><p>​  异常分割模型[1,2,3]在工业质量控制[4,5]和医学诊断[6]等各个领域都引起了人们极大的研究兴趣。可靠异常分割的关键是区分异常数据的分布。具体来说，本文考虑了图像上的零样本异常分割（ZSAS,zero-shot anomalysegmentation），这是一种很有前途但未探索的设置，在训练过程中没有为目标类别提供正常和异常图像。</p><p>​  由于缺乏用于训练的异常样本，许多工作都致力于无监督或自监督的异常分割，其目标是在训练过程中学习正常样本的表示。然后，通过计算测试样本与学习到的正态分布之间的差异，可以对异常进行分割。具体来说，这些模型，包括基于自动编码器的重建[7,8,9,10,11,12]、一类分类[13,14,15]和基于记忆的正态分布[3,2,16,17,18]方法，通常需要对特定的有限的类别训练单独的模型。然而，在现实场景中，有数以百万计的工业产品，为单个对象收集大型训练集没有成本效益，这阻碍了在需要高效部署的情况下，例如生产的初始阶段，它们的部署。</p><p>​  最近，基础模型，如SAM [19]和CLIP[20]，通过提示[21,22]检索存储在这些模型中的先验知识，显示出了巨大的零镜头视觉感知能力。在这项工作中，我们想探索如何适应基础模型来实现异常分割下的零样本迁移能力。</p><p><img src="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA）。然而，SAA显示了严重的误警报问题，它错误地检测到所有的“灯芯”，而不是ground-truth异常区域（“过长的灯芯”）。因此，我们在改进的模型（SAA+)中进一步加强了混合提示的正则化，成功地帮助识别了异常区域。</p><p>​  为此，如图1所示，我们首先通过级联提示引导的目标检测[23]和分割基础模型[19]，构造一个普通的基础模型，即分别作为异常区域生成器和异常区域细化器（SAA）。根据解锁基础模型知识[24,25]的实践，使用朴素语言提示，如“缺陷”或“异常”，来分割目标图像所期望的异常。具体来说，语言提示用于提示异常区域生成器为所需的异常区域生成提示条件的框级区域。然后在异常区域细化器中对这些区域进行细化，以产生最终的预测，即用于异常分割的掩模。</p><p>​  然而，如图1所示，普通的基础模型装配（SAA）往往会导致严重的误报，例如，SAA错误地将所有的灯芯称为异常，而只有超长的灯芯是真正的异常，我们将其归因于幼稚的语言提示带来的歧义。首先，当面对基础模型的预训练数据分布与下游数据集之间的域转换时，传统的语言提示可能会变得无效。其次，目标的“异常”程度取决于对象上下文，这对于朴素的粗粒度语言提示，例如“异常区域”，很难准确地表达。</p><p>​  因此，超越幼稚的语言提示，我们将领域专家知识和目标图像纳入我们改进的框架上下文，即分段任何异常+（SAA+）。一方面，专家知识提供了在开放世界场景中与目标相关的异常情况的详细描述。我们利用更具体的描述作为上下文提示，有效地对齐预训练和目标数据集中的图像内容。另一方面，我们利用目标图像上下文来可靠地识别和自适应地校准异常分割预测[26,27]。通过利用目标图像中丰富的上下文信息，我们可以准确地将对象上下文与最终的异常预测联系起来。</p><p>​  从技术上讲，除了单纯的类不可知的提示外，我们还利用领域专家知识来构建面向目标的异常语言提示，即特定于类的语言表达式。此外，由于语言不能准确地检索具有特定对象特征的区域，如数量、大小和位置，精确地[28,29]，我们以阈值过滤器的形式引入对象属性提示。这些提示有助于识别和删除不满足所需属性的候选区域。此外，为了充分利用目标图像的上下文，我们建议利用图像显著性和区域置信度排序作为提示，通过考虑一个区域与图像内其他区域之间的相似性，如欧氏距离，来建模一个区域的异常程度。最后，我们进行了彻底的实验，以确认我们的混合提示在适应基础模型的零镜头异常分割的有效性。具体来说，我们最终的模型（SAA+）在零镜头设置下，在各种异常分割数据集上获得了新的最新性能。总之，我们的主要贡献是：</p><p>​  我们提出了异常分割的SAA框架，允许在不需要训练的情况下协同组装不同的基础模型。</p><p>​  我们引入混合提示作为一种正则化技术，利用领域专家知识和目标图像上下文来适应基础模型进行异常分割。这导致了SAA+的开发，这是我们框架的一个增强版本。</p><p>​  我们的方法在几个基准数据集上实现了最先进的零镜头异常分割，包括VisA、MVTec-AD、KSDD2、MTD的性能。值得注意的是，SAA/SAA+在不需要任何注释的情况下检测与纹理相关的异常方面显示出了显著的能力。</p><h1 id="saa针对zsas的基础模型组装">SAA：针对ZSAS的基础模型组装</h1><h2 id="问题定义zsas">问题定义：ZSAS</h2><p>零样本异常分割（ZSAS， Zero-shot Anomaly Segmentation）</p><p>​  ZSAS的目标是对新对象进行异常分割，而不需要任何相应的对象训练数据。ZSAS试图创建一个基于空训练集∅的异常映射<spanclass="math inline">\(\mathbf{A}\in[0,1]^{h\timesw\times1}\)</span>，以识别包含新对象的图像<spanclass="math inline">\(\mathbf{I}\in\mathbb{R}^{h\timesw\times3}\)</span>中单个像素的异常程度。ZSAS任务有可能显著减少对培训数据的需求，并降低与实际检查部署相关的成本。</p><h2 id="基线模型saa">基线模型：SAA</h2><p>分段任何异常（SAA，Segment Any Anomaly）</p><p>​  对于ZSAS，我们首先构建一个普通的基础模型组件，即分段任何异常（SAA），如图1所示。<br/><imgsrc="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA)。</p><p>具体来说，给定一个特定的异常分割查询图像，我们首先使用语言作为初始提示，通过一个异常区域生成器粗略地检索粗糙的异常区域建议，即GroundingDINO[23]。然后，使用异常区域细化器将异常区域建议细化为像素级高质量的分割掩模，其中使用提示驱动的分割基础模型，即SAM[19]。</p><h3 id="异常区域发生">异常区域发生</h3><p>​  随着语言视觉模型的蓬勃发展，一些基础模型[24,23,46]逐渐获得了通过语言提示检索图像中对象的能力。给定描述要检测区域的语言提示T，例如“异常”，基础模型可以为查询图像i生成所需区域i。在那里，我们将区域检测器的结构基于文本引导的开集对象检测结构，用于视觉grounding。具体来说，我们采用了一个已经在[41]上预先训练过的大规模语言视觉数据集的GroundingDINO[23]架构。该网络首先通过文本编码器和视觉编码器分别提取语言提示符和查询图像的特征。然后用交叉模态解码器以边界框的形式生成粗糙的对象区域。给定边界盒级区域集RB及其对应的置信度评分集S，异常区域生成器（生成器）的模块可以表示为：<spanclass="math display">\[\mathcal{R}^{B},\mathcal{S}:=\mathrm{Generator}(\mathbf{I},\mathcal{T})\]</span></p><h3 id="异常区域细化">异常区域细化</h3><p>​  为了生成像素级的异常分割结果，我们提出了异常区域细化器，将边界盒级的异常区域候选区域细化为异常分割掩模集。为此，我们使用了一个复杂的基础模型来进行开放世界的视觉分割，即SAM[19]。该模型主要包括一个基于vit的[56]主干和一个提示条件掩码解码器。具体来说，该模型是在一个具有10亿个细粒度掩模的大规模图像分割数据集[19]上进行训练的，这使得在开放集分割下能够具有高质量的掩模生成能力。有提示条件的掩码解码器接受各种类型的提示作为输入。我们将边界框候选RB视为提示，得到像素级分割掩模r。异常区域细化器（Refiner）的模块可以表述如下：<spanclass="math display">\[\mathcal{R}:=\operatorname{Refiner}(\mathbf{I},\mathcal{R}^B)\]</span>​  在此之前，我们以具有相应置信度分数s的高质量分割掩模R的形式获得了区域集。综上所述，我们总结了框架（SAA）如下：<spanclass="math display">\[\mathcal{R},\mathcal{S}:=\text{SAA}(\mathbf{I},\mathcal{T}_n)\]</span>​  其中Tn是一个朴素的类不可知的语言提示，例如”异常“，在SAA中使用。</p><h3 id="基线模型组件的zsas性能分析">基线模型组件的ZSAS性能分析</h3><p>​  我们提出了一些初步的实验来评估基础模型组装对ZSAS的有效性。尽管解决方案的简单和直观，我们观察到一个语言歧义的问题。具体来说，某些语言提示，如“异常”，可能无法检测到所需的异常区域。例如，如图1所示，所有的“灯芯”都被SAA用“异常”提示符错误地识别为异常。</p><p>​  我们将这种语言歧义归因于训练前的语言-视觉数据集和目标ZSAS数据集之间的领域差距，这意味着一些语言提示可能具有不同的含义，并在不同的数据集中与不同的图像内容相关联。此外，在这些大规模的数据集中几乎没有像“异常”这样的形容词表达，这使得这种快速的设计很难理解什么是异常区域。此外，确切的“异常”是特定于对象的，并且会因对象而变化。例如，它表示皮革上的划痕或榛子上的裂缝。语言歧义问题导致ZSAS数据集中严重的误警报。我们建议引入由领域专家知识和目标图像上下文生成的混合提示，以减少语言歧义，从而实现更好的ZSAS性能。</p><h1id="saa通过混合提示正则化的自适应基础模型">SAA+：通过混合提示正则化的自适应基础模型</h1><p>​  为了解决SAA中的语言歧义并提高其在ZSAS上的能力，我们提出了一个名为SAA+的升级版本，它包含了混合提示，如图2所示。除了利用从预先训练过的基础模型中获得的知识外，SAA+还利用领域专家知识和目标图像上下文来生成更准确的异常区域掩模。我们将在下面提供关于这些混合提示的进一步细节。</p><h2 id="从领域专家知识中生成的提示">从领域专家知识中生成的提示</h2><p>​  根据提示学习[48,54]的趋势，我们以语言的形式初始化提示，以解锁基础模型的知识。然而，当只使用朴素的语言提示“异常”时，由领域差距引起的语言歧义问题尤为严重。为了解决这个问题，我们利用了包含关于目标异常区域的有用的先验信息的领域专家知识。具体来说，尽管专家可能没有为新产品提供潜在开放世界异常的全面列表，但他们可以根据他们过去使用类似产品的经验来确定一些候选产品。领域专家知识使我们能够将朴素的“异常”提示细化为更具体的提示，以更详细地描述异常状态。除了语言提示之外，我们还引入了属性提示，以补充现有基础模型[28]中对“count”和“area”[28]等特定属性的认识不足。</p><h3 id="异常的语言表达式作为提示">异常的语言表达式作为提示</h3><p>​  为了描述潜在的开放世界异常情况，我们建议设计更精确的语言提示。这些提示可分为两种类型：类无关的提示和类特定的提示。</p><p>​  <strong>类别不可知论提示（Ta）</strong>是描述非特定于任何特定类别的异常情况的通用提示，例如，“异常”和“缺陷”。尽管预先训练的数据集和目标ZSAS数据集之间存在领域差距，但我们的实证分析（5.3）表明，这些通用提示提供了令人鼓舞的初始性能。</p><p>​  <strong>类别特定提示（Ts）</strong>是基于对类似产品的异常模式的专家知识而设计的，以补充更具体的异常细节。我们使用预先训练的视觉语言数据集中已经使用的提示，例如“黑洞”和“白色气泡”，来查询所需的区域。这种方法重新定义了寻找异常区域的任务，以定位具有特定异常状态表达式的对象，这比利用基础模型在对象上下文中识别“异常”更简单。</p><p>​  通过使用来自领域专家知识的异常语言提示<spanclass="math inline">\(\mathcal{P}^L=\{\mathcal{T}_\mathrm{a},\mathcal{T}_\mathrm{s}\}\)</span>提示SAA，我们生成了更精细的异常区域候选项R和相应的置信分数S。</p><h3 id="异常对象属性作为提示">异常对象属性作为提示</h3><p>​  目前的基础模型[23,57]在查询具有特定属性描述的对象时存在局限性，比如大小或位置，这些对于描述异常很重要，比如“电缆左边的小黑洞”。为了整合这一关键的专家知识，我们建议使用作为规则而不是语言来表述的异常属性提示。具体来说，我们考虑了异常的位置和面积。</p><p>​  <strong>异常定位。</strong>异常的准确定位在区分真实异常和假阳性中起着关键作用。通常，在推理过程中，异常被期望位于感兴趣的对象内。然而，由于背景上下文的影响，异常可能偶尔会出现在被检查的物体之外。为了解决这一挑战，我们利用基础模型的开放世界检测能力来确定被检查对象的位置。随后，我们计算了潜在异常区域和被检查对象之间的并集的交集（IoU）。通过应用expert-derived的IoU阈值，表示为<spanclass="math inline">\(θ_{IoU}\)</span>，我们过滤出了IoU值低于该阈值的异常候选值。此过程确保保留的异常候选项更有可能表示位于被检查对象内的真实异常。</p><p>​  <strong>异常区域。</strong>由其面积所反映的异常现象的大小，也是一种可以提供有用信息的特性。一般来说，异常应小于被检查物体的大小。专家可以为所考虑的特定类型的异常提供一个合适的阈值<spanclass="math inline">\(θ_{area}\)</span>。与<spanclass="math inline">\(θ_{area}\)</span>目标区域不匹配的候选区域可以被过滤掉。</p><p>​  通过结合两个属性提示<spanclass="math inline">\(\mathcal{P}^P=\left\{\theta_{area},\theta_{IoU}\right\}\)</span>，我们可以通过过滤候选区域的过滤函数（Filter）R，得到具有相应置信分数<spanclass="math inline">\(S^P\)</span>的候选<spanclass="math inline">\(R^P\)</span>的子集， <spanclass="math display">\[\mathcal{R}^P,\mathcal{S}^P:=\mathrm{Filter}(\mathcal{R},\mathcal{P}^P)\]</span></p><h2 id="来自目标图像上下文的提示">来自目标图像上下文的提示</h2><p>​  除了结合领域专家知识外，我们还可以利用输入图像本身提供的信息来提高异常区域检测的准确性。在这方面，我们提出了两个由图像上下文引起的提示。</p><h3 id="异常显著为提示">异常显著为提示</h3><p>​  由于预先训练的语言视觉数据集[41]和目标异常分割数据集[4,58]之间的领域差距，由[23]等基础模型生成的预测可能是不可靠的。为了校准个体预测的置信度得分，我们提出了模拟人类直觉的异常显著性提示法。具体来说，人类可以通过与周围区域[40]的差异来识别异常区域，即视觉显著性包含了指示异常程度的有价值的信息。因此，我们通过计算相应的像素特征(f)与其N个最近邻之间的平均距离，来计算输入图像的显著性映射(s)，<span class="math display">\[\mathbf{s}_{ij}:=\frac1N\sum_{\mathbf{f}\inN_p(\mathbf{f}_{ij})}(1-\langle\mathbf{f}_{ij},\mathbf{f}\rangle)\]</span>​  式中，<span class="math inline">\((i,j)\)</span>表示像素位置，<spanclass="math inline">\(N_p(\mathbf{f}_{ij})\)</span>表示对应像素的N个最近邻，<spanclass="math inline">\(\langle\cdot,\cdot\rangle\)</span>表示余弦相似度。我们使用来自大规模图像数据集[59]的预先训练好的cnn来提取图像特征，以确保特征的描述性。显著性地图表示一个区域与其他区域的不同程度。显著性提示PS定义为相应区域掩模内的指数显著性平均值，<spanclass="math display">\[\mathcal{P}^S:=\left\{\exp(\frac{\sum_{ij}\mathbf{r}_{ij}\mathbf{s}_{ij}}{\sum_{ij}\mathbf{r}_{ij}})\quad|\quad\mathbf{r}\in\mathcal{R}^P\right\}\]</span>​  显著性提示提供了异常区域置信度的可靠指示。这些提示是用来重新校准基础模型生成的信心分数，产生新的调整分数<spanclass="math inline">\(S^S\)</span>基于异常显著性提示<spanclass="math inline">\(P^S\)</span>。这些调整分数提供一个综合措施，考虑到信心来自基础模型和地区候选人的显著性。该流程的表述如下：<span class="math display">\[\mathcal{S}^S:=\begin{Bmatrix}p\cdots&amp;|&amp;p\in\mathcal{P}^S,s\in\mathcal{S}^P\end{Bmatrix}\]</span></p><h3 id="异常置信为提示">异常置信为提示</h3><p>​  通常，一个被检查对象中的异常区域的数量是有限的。因此，我们提出异常置信度提示<spanclass="math inline">\(P^C\)</span>根据图像内容识别出置信度得分最高的K个候选对象，并使用它们的平均值进行最终的异常区域检测。这是通过根据其对应的置信度得分选择前K个候选区域来实现的，如下所示，<spanclass="math display">\[\mathcal{R}^C,\mathcal{S}^C:=\mathrm{Top}_K(\mathcal{R}^P,\mathcal{S}^S)\]</span>​  将单个区域及其对应的得分表示为<spanclass="math inline">\(r^C\)</span>和<spanclass="math inline">\(s^C\)</span>，然后我们使用这些K个候选区域来估计最终的异常图，<spanclass="math display">\[\mathbf{A}_{ij}:=\frac{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C\cdots^C}{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C}\]</span>​  通过提出的混合提示<spanclass="math inline">\((\mathcal{P}^L,\mathcal{P}^P,\mathcal{P}^S,\text{and}\mathcal{P}^C)\)</span>，SAA在我们最终的框架中进行了正则化，即分段任何异常+（SAA+），从而做出了更可靠的异常预测。</p><h1 id="实验">实验</h1><p>​  在本节中，我们首先评估SAA/SAA+在几个异常分割基准上的性能。然后，我们广泛地研究了个体混合提示的有效性。</p><h2 id="实验设置">实验设置</h2><p>​  <strong>数据集。</strong>我们利用了四个带有像素级注释的数据集。：VisA [58]、MVTec-AD [4]、KSDD2 [60]和MTD[61]。VisA和MVTec-AD由多种对象子集组成，如电路板，而KSDD2和MTD则由纹理异常组成。总之，我们将所有这些数据集的子集分类为通常在单个图像中显示相似模式的纹理（如地毯），以及包括更多样化分布的对象（如蜡烛）。</p><p>​  <strong>评估指标。</strong>ZSAS性能的评估基于两个指标： (I)<strong>max-F1-pixel</strong>（Fp）[25]，它测量在最优阈值下的像素分割的F1分数；（II）<strong>max-F1-region</strong>（Fr），本文提出，以减轻最大f1像素[4]观察到的大缺陷的偏差。具体来说，我们在最优阈值下计算区域分割的f1分数，如果重叠值超过0.6，考虑预测为正。</p><p>​  <strong>实施细节。</strong>我们采用了GroundingDINO和分段任何模型2的官方实现来构建基线（SAA）。关于来自领域专家知识的提示的细节在补充材料中有解释。对于由图像内容诱导的显著性提示，我们使用WideResNet50[62]网络，在ImageNet [59]上进行预训练，并根据之前的研究[40]设置N =400。对于异常置信度提示，我们将超参数K默认设置为5。输入图像的分辨率固定为400×400进行评估。</p><h2 id="主要结果">主要结果</h2><p>​  <strong>比较方法。</strong>我们比较了我们最终的模型，即分段任何异常+（SAA+）与几种并发的最先进的方法，包括WinClip[25]，UTAD [40]，ClipSeg[24]，和我们的香草基线（SAA）。对于WinClip，我们报告其在VisA和MVTec-AD上的官方结果。对于其他三种方法，我们使用官方实现，并使它们适应于ZSAS任务。值得注意的是，由于所有的方法都不需要训练过程，它们的性能是稳定的，方差为±0.00。</p><p>​  <strong>定量结果：</strong>如表1所示，SAA+方法在Fp和Fr方面均显著优于其他方法。虽然WinClip[25]、ClipSeg[24]和SAA也使用基础模型，但SAA+更好地释放了基础模型的能力，并调整它们来解决ZSAS问题。SAA+的显著性能满足了不经训练就能分割任何异常现象的期望。</p><figure><img src="./../postimages/SAA/image-20240621164328885.png"alt="image-20240621164328885" /><figcaption aria-hidden="true">image-20240621164328885</figcaption></figure><p>​  <strong>定性结果：</strong>图3为SAA+与以往竞争方法的定性比较，其中SAA+取得了更好的性能。此外，可视化显示SAA+能够检测纹理异常，如皮革上的小划痕。</p><figure><img src="./../postimages/SAA/image-20240621164520983.png"alt="image-20240621164520983" /><figcaption aria-hidden="true">image-20240621164520983</figcaption></figure><h1 id="消融研究">消融研究</h1><p>​  在表2中，我们执行组件级分析，以消除框架中特定的提示设计。</p><figure><img src="./../postimages/SAA/image-20240621164657482.png"alt="image-20240621164657482" /><figcaption aria-hidden="true">image-20240621164657482</figcaption></figure><p>​  <strong>语言提示符<spanclass="math inline">\((\mathcal{P}^L)\)</span>。</strong>表2验证了来自领域专家知识的语言提示的有效性（Fp中+3.90%，Fr+4.90%）。然后，我们深入研究了Ta和Ts的有效性，这清楚地表明，一般描述和专门设计的异常描述都可以达到合理的性能。此外，它们的组合可以产生协同作用，提高异常分割性能。<spanclass="math inline">\(\mathcal{P}^L\)</span>的改进有助于解锁当前基础模型[23,19]的语言驱动区域检测能力。</p><p>​  <strong>属性提示符<spanclass="math inline">\((\mathcal{P}^P)\)</span>。</strong>除了改善整体性能，属性提示带来显著的改善（从21.83%到53.79%）纹理类别，由于过滤机制过滤掉大量的错误检测异常区域候选人通过高级特征，例如，目标图像的位置和面积。</p><p>​  <strong>显著性提示符<spanclass="math inline">\((\mathcal{P}^S)\)</span>。</strong>表2提供了<spanclass="math inline">\(\mathcal{P}^S\)</span>在异常分割的有效性的明确证据。这是因为区域显著性可以准确地描述一个区域与周围环境的偏离程度。</p><figure><img src="./../postimages/SAA/image-20240621165328601.png"alt="image-20240621165328601" /><figcaption aria-hidden="true">image-20240621165328601</figcaption></figure><p>​  在图4中，我们展示了<spanclass="math inline">\(\mathcal{P}^S\)</span>对异常分割的定性影响，说明了视觉显著性图可以帮助突出异常区域，即与其他区域相比更高的显著性值。通过结合<spanclass="math inline">\(\mathcal{P}^S\)</span>来校准置信度分数，可以获得更精确的分割结果。例如，<spanclass="math inline">\(\mathcal{P}^S\)</span>的使用可以有效地定位榛子的裂缝区域和蜡烛上的过长的灯芯。</p><p>​  <strong>置信度提示符<spanclass="math inline">\((\mathcal{P}^C)\)</span>。</strong>通过加入异常置信度提示，我们限制了异常区域的数量，这有效地减少了假阳性，导致所有类别的Fp平均提高0.72%，如表2所示。</p><figure><img src="./../postimages/SAA/image-20240621165812635.png"alt="image-20240621165812635" /><figcaption aria-hidden="true">image-20240621165812635</figcaption></figure><p>​  超参数K在PC中的影响如图5所示。从图中可以看出，随着K的提高，异常区域检测准确。然而，当K超过一定的阈值（约为K= 5）时，随着更多的区域被错误地识别为异常，性能略有下降。在K =5左右时获得最佳结果，所有类别的平均Fp为34.85%。</p><h1 id="结论">结论</h1><p>​  在这项工作中，我们探索如何在没有任何进一步训练的情况下，通过释放现代基础模型的全部力量来分割任何异常现象。基础模型装配的调整归功于快速设计，这是控制非基础模型功能的关键。因此，我们提出了一个新的框架，即分段任何异常+，利用来自专家知识和目标图像上下文的混合提示来规范无需训练的基础模型。最后，我们成功地采用了多个基础模型来解决零镜头异常分割问题，并在几个基准上获得了新的SoTA结果。我们希望我们的工作能够阐明对异常分割的无标签模型自适应的设计。</p><p>​  <strong>限制。</strong>由于计算的限制，我们目前没有在更大尺度的基础模型上测试我们的方法。我们已经用具有代表性的基础模型完成了对我们的方法的探索，并将在未来探讨这些模型的尺度效应。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning for Image Super-Resolution</title>
      <link href="/Image_Super-Resolution/"/>
      <url>/Image_Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p><h1 id="摘要">摘要：</h1><p>​  图像超分辨率（SR）是在计算机视觉中提高图像和视频分辨率的一类重要的图像处理技术。近年来，利用深度学习技术实现的图像超分辨率取得了显著进展。本文旨在对利用深度学习方法的图像超分辨率的最新进展进行一个全面的调查。一般的来说，我们可以将现有的SR技术的研究大致分为三大类：有监督SR、无监督SR和领域特异性SR。此外，我们还讨论了一些重要的问题，如公开可用的基准数据集和性能评估指标。最后，我们通过强调几个未来的方向和社区应该进一步解决的问题来结束这项调查。</p><h1 id="介绍">介绍</h1><p>​  图像超分辨率（SR）是指从低分辨率（LR）图像中恢复高分辨率（HR）图像的过程，是计算机视觉和图像处理中的一类重要的图像处理技术。它享有广泛的现实世界的应用，如医学成像[1]，[2]，[3]，监视和安全[4]，[5])等。除了提高图像感知质量外，它还有助于改善其他计算机视觉任务[6]、[7]、[8]、[9]。一般来说，这个问题是非常具有挑战性的，并且具有固有的不适定性，因为总是有多个HR图像对应于单个LR图像。在文献中，已经提出了各种经典的SR方法，包括基于预测的方法[10]，[11]，[12]，基于边缘的方法[13]，[14]，统计方法[15]，[16]，基于补丁的方法[13]，[17]，[18]，[19]和稀疏表示方法[20]，[21]，等。</p><p>​  近年来深度学习技术的快速发展，基于深度学习的SR模型已经积极探索，经常实现最先进的性能的各种基准的各种深度学习方法被应用于解决SR任务，从早期基于卷积神经网络（CNN）的方法（例如，SRCNN[22][23]）最近有前途的SR方法使用生成对抗网（GAN）[24]（如SRGAN[25]）。一般来说，使用深度学习技术的SR算法家族在以下主要方面有所不同：不同类型的网络架构[26]、[27]、[28]、不同类型的损失函数[8]、[29]、[30]、不同类型的学习原则和策略[8]、[31]、[32]等。</p><p>​  在本文中，我们全面概述了图像超分辨率的最新进展。虽然有一些现有的SR调查文献，我们的工作不同，我们专注在深度学习SR技术，而大多数早期作品[33]，[34]，[35]，[36]旨在调查传统SR算法或一些研究主要集中在提供定量评估基于全参考指标或人类视觉感知[37]，[38]。与现有的调查不同，本调查采用了一个独特的基于深度学习的视角，以系统和全面的方式回顾了SR技术的最新进展。</p><p>​  这次调查的主要贡献有三方面：<br/>&gt; 1.我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。<br/>&gt;1.我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。<br/>&gt;1.我们讨论挑战和开放的问题，并确定新的趋势和未来的方向，为社区提供深刻的指导。</p><p>​  在下面的章节中，我们将介绍在深度学习中图像超分辨率的最新进展的各个方面。图1显示了本次调查中将以层次结构的方式覆盖的图像SR的分类。第2节给出了问题的定义，并回顾了主流数据集和评估指标。第3节模块化地分析了监督SR的主要成分。第4节简要介绍无监督的SR方法。第5节介绍了一些流行的特定于领域的SR应用程序，第6节还讨论了未来的发展方向和开放的问题。</p><h1 id="问题设置和术语">问题设置和术语</h1><h2 id="问题定义">问题定义</h2><p>​  图像超分辨率的目的是从LR图像中恢复相应的HR图像。通常，LR图像Ix被建模为以下退化的输出：<span class="math display">\[I_x=\mathcal D(I_y;\delta),\]</span>​  式中，D为退化映射函数，Iy为对应的HR图像， $ $为退化过程的参数（如缩放因子或噪声）。</p><p>​  一般来说，退化过程（即D和 $ $）是未知的，只提供LR图像。在这种情况下，也称为盲SR，需要研究人员通过从LR图像Ix中恢复地面真实HR图像^的HR近似，如下：<span class="math display">\[\hat{I}_y=\mathcal{F}(I_x;\theta),\]</span>​  其中，F为超分辨率模型， $ $ 为F的参数。</p><p>​  虽然退化过程是未知的，并且可能受到各种因素的影响（如压缩伪影、各向异性退化、传感器噪声和散斑噪声），但研究人员正试图对退化映射进行建模。大多数工作都直接将退化建模为一个单一的降采样操作，如下所示：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y)\downarrow_s,\{s\}\subset\delta,\]</span>​  其中， $ <em>s $是一个具有缩放因子s的降采样操作。事实上，大多数通用SR的数据集都是基于这种模式构建的，而最常用的降采样操作是带有抗锯齿的双边插值。然而，[39]还有其他一些工作，将退化建模为几种操作的组合：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y\otimes\kappa)\downarrow_s+n_\varsigma,\{\kappa,s,\varsigma\}\subset\delta,\]</span>​  其中 $ I_y$ 表示模糊核与HR图像Iy之间的卷积， $ n</em>$ 是带有标准差 $$的加性高斯白噪声。与等式的朴素定义相比3、等式的组合降解模式4更接近真实世界的情况，并已被证明对SR[39]更有利。为此目的，SR的目标如下： <spanclass="math display">\[\hat{\theta}=\arg\min_\theta\mathcal{L}(\hat{I}_y,I_y)+\lambda\Phi(\theta),\]</span>​  其中， $ (_y,I_y) $ 表示生成的HR图像 $ _y $与地面真实图像Iy之间的损失函数，$ ()$为正则化项，为权衡参数。虽然SR最流行的损失函数是像素级均方误差（即像素损失），但更强大的模型倾向于使用多个损失函数的组合，这将在第3.4.1节中介绍。</p><h2 id="图像质量评估">图像质量评估</h2><p>​  图像质量是指图像的视觉属性，侧重于对观众的感知评估。一般来说，图像质量评估（IQA）方法包括基于人类感知的主观方法（即图像看起来的真实程度）和客观的计算方法。前者更符合我们的需求，但往往是耗时和昂贵的，因此后者是目前的主流。然而，这些方法之间不一定一致，因为客观方法往往不能非常准确地捕捉人类的视觉感知，这可能导致IQA结果[25]，[58]的很大差异。</p><p>​  此外，客观IQA方法进一步分为三种类型的[58]：使用参考图像进行评估的全参考方法，基于提取特征比较的简化参考方法，以及无任何参考图像的无参考方法（即盲IQA）。接下来，我们将介绍几种最常用的IQA方法，包括主观方法和客观方法。</p><h3 id="峰值信噪比">峰值信噪比</h3><p>​  峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）是有损变换（如图像压缩、图像嵌入绘制）中最常用的重建质量测量方法之一。对于图像的超分辨率，PSNR是通过图像之间的最大像素值（记为L）和均方误差（MSE）来定义的。给定N个像素的groundtruth图像I和重建I，PSNR定义如下：<spanclass="math display">\[\mathrm{PSNR}=10\cdot\log_{10}\left(\frac{L^2}{\frac{1}{N}\sum_{i=1}^N\left(I(i)-\hat{I}(i)\right)^2}\right),\]</span>​  其中，L等于255，在一般情况下使用8位表示。由于PSNR只与像素级MSE相关，只关注对应像素之间的差异而不是视觉感知，这往往导致在真实场景中表示重建质量的表现不佳，而我们通常更关注人类的感知。然而，由于需要与文献作品进行比较，且缺乏完全准确的感知指标，PSNR仍然是目前SR模型中使用最广泛的评价标准。</p><h2 id="操作通道">操作通道</h2><p>​  除了常用的RGB颜色空间外，YCbCr颜色空间也被广泛用于SR。在这个空间中，图像分别用Y、Cb、Cr通道表示，分别表示亮度、蓝差和红差的色度分量。虽然目前还没有公认的最佳实践来执行或评估超分辨率，但早期的模型倾向于在YCbCr空间[26]、[43]、[78]、[79]的Y通道上运行，而最近的模型倾向于在RGB通道[28]、[31]、[57]、[70]上运行。值得注意的是，在不同颜色的空间或通道上进行操作（培训或评估）可以使评估结果差异很大（高达4dB）[23]。</p><h2 id="超分辨率挑战">超分辨率挑战</h2><p>​  在本节中，我们将简要介绍图像SR的两个最流行的挑战，NTIRE [80]和PIRM[47]，[81]。</p><h3 id="ntire的挑战">NTIRE的挑战</h3><p>​  图像恢复和增强（NTIRE, The New Trends in Image RestorationandEnhancement）的新趋势挑战[80]与CVPR相结合，包括多个任务，如SR，去噪和着色。对于图像SR，NTIRE挑战是建立在DIV2K[42]数据集上，由双边降缩放轨迹和具有现实未知退化的盲轨迹组成。这些轨道在降解和比例因子上有所不同，旨在促进在理想条件和现实世界的不利情况下的SR研究。</p><h3 id="pirm挑战">PIRM挑战</h3><p>​  感知图像恢复和操作（PIRM, The Perceptual Image Restoration andManipulation）挑战与ECCV相结合，还包括多个任务。与NTIRE相比，PIRM的一个子挑战[47]侧重于生成准确性和感知质量之间的权衡，而另一个[81]侧重于智能手机上的SR。正如众所周知的[77]一样，针对失真的模型经常产生视觉上不愉快的结果，而针对感知质量的模型在信息上表现较差保真度。具体来说，PIRM根据均方根误差（RMSE）的阈值将感知扭曲平面划分为三个区域。在每个区域，获胜的算法是获得最佳感知质量的[77]，由NIQE[76]和Ma[66]评估。而在另一个子挑战[81]，智能手机上的SR，参与者被要求使用有限的智能手机硬件（包括CPU、GPU、RAM等）执行SR，评价指标包括PSNR、MS-SSIM和MOS测试。通过这种方式，PIRM鼓励对感知-失真的权衡进行高级研究，并在智能手机上驱动轻量级和高效的图像增强。</p><h1 id="监督超分辨率">监督超分辨率</h1><p>​  目前，研究人员已经提出了各种具有深度学习的超分辨率模型。这些模型侧重于有监督的SR，即同时用LR图像和相应的HR图像进行训练。虽然这些模型之间的差异非常大，但它们本质上是一组组件的一些组合，如模型框架、上采样方法、网络设计和学习策略。从这个角度来看，研究人员结合这些组件来建立一个集成的SR模型，以拟合特定的目的。在本节中，我们将集中精力模块化地分析基本组件（如图1所示），而不是孤立地介绍每个模型，并总结它们的优点和局限性。</p><h2 id="超分辨率框架">超分辨率框架</h2><p>​  由于图像超分辨率是一个不适定问题，如何进行上采样（即从LR输入生成HR输出）是关键问题。尽管现有模型的架构差异很大，但基于所采用的上采样操作及其在模型中的位置，它们可以归因于四个模型框架（如图2所示）。</p><h3 id="预上采样超分辨率">预上采样超分辨率</h3><p>​  由于直接学习从低维空间到高维空间的映射的困难，利用传统的上采样算法获得高分辨率的图像，然后利用深度神经网络进行细化是一个简单的解决方案。因此，Dong等人[22]，[23]首先采用预上采样SR框架（如图2a所示），并提出SRCNN来学习从插值的LR图像到HR图像的端到端映射。具体来说，使用传统方法（如双边插值）将LR图像上采样到具有所需大小的粗糙HR图像，然后在这些图像上应用深度cnn来重建高质量的细节。由于最困难的上采样操作已经完成，cnn只需要对粗糙的图像进行细化，这大大降低了学习难度。此外，这些模型可以以任意大小和缩放因子的插值图像作为输入，并给出与单尺度SR模型[26]性能相当的细化结果。因此，它逐渐成为[55]、[56]、[82]、[83]中最流行的框架之一，这些模型之间的主要区别是后验模型设计（第3.3节）和学习策略（第3.4节）。然而，预定义的上采样往往会引入副作用（如噪声放大和模糊），由于大多数操作是在高维空间进行的，时间和空间的成本比其他框架[43]，[84]高得多。</p><h3 id="后上采样超分辨率">后上采样超分辨率</h3><p>​  为了提高计算效率，充分利用深度学习技术自动提高分辨率，研究人员提出在低维空间中用端到端可学习层替换预定义的计算。在该框架的先驱作品[43]，[84]中，即如图2b所示的上采样后SR，LR输入图像在不提高分辨率的情况下输入深度cnn，在网络末端应用端到端可学习的上采样层。</p><p>​  由于计算成本较大的特征提取过程只发生在低维空间中，而分辨率最终只会提高，因此大大降低了计算复杂度和空间复杂度。因此，这种框架也已成为最主流的框架之一，[25]，[31]，[79]，[85]。这些模型的不同主要在于可学习的上采样层（第3.2节）、前CNN结构（第3.3节）和学习策略（第3.4节）等。</p><h3 id="逐步上采样超分辨率">逐步上采样超分辨率</h3><p>​  虽然上采样后的SR框架极大地降低了计算成本，但它仍存在一些缺点。一方面，上采样只进行了一步，这大大增加了对大尺度因子（如4,8）的学习差异。另一方面，每个比例因子都需要训练一个单独的SR模型，这无法应对多尺度SR的需要。为了解决这些缺点，拉普拉斯金字塔SR网络（LapSRN）[27]采用了渐进式上采样框架，如图2c所示。具体来说，该框架下的模型是基于cnn的级联，并逐步重建更高分辨率的图像。在每个阶段，图像被上采样到更高的分辨率，并通过cnn进行细化。</p><p>​  其他的工作，如MS-LapSRN[65]和渐进式SR（ProSR）[32]也采用了这个框架，并实现了相对较高的性能。与LapSRN和MSLapSRN使用中间重建图像作为后续模块的“基础图像”相比，ProSR保留主要信息流，并通过单个头部重建中间分辨率图像。</p><p>​  该框架下的模型将困难任务分解为简单任务，大大降低了学习难度，特别是在因素较大的情况下，并在不引入过多空间和时间成本的情况下应对多尺度SR。此外，一些具体的学习策略，如课程学习（第3.4.3节）和多监督（第3.4.4节），进一步降低学习难度，提高最终成绩。然而，这些模型也遇到了一些问题，如多阶段模型设计复杂和训练稳定性高，需要更多的建模指导和更先进的训练策略。</p><h3 id="迭代上下采样超分辨率">迭代上下采样超分辨率</h3><p>​  为了更好地捕捉LR-HR图像对的相互依赖关系，在SR[44]中加入了一种有效的反投影[12]迭代过程。该SR框架，即迭代上下采样SR（如图2d所示），尝试迭代应用反投影细化，即计算重建误差，然后将其重新融合，调整HR图像强度。具体来说，Haris等人[57]利用迭代上下采样层提出DBPN，将上采样和下采样层交替连接，并使用所有中间重建重建最终的HR结果。类似地，SRFBN[86]采用了一个迭代的上下采样反馈块，具有更密集的跳跃连接，并学习更好的表示。而用于视频超分辨率的RBPN[87]从连续的视频帧中提取上下文，并将这些上下文结合起来，通过一个反向投影模块产生循环输出帧。</p><p>​  该框架下的模型可以更好地挖掘LR-HR图像对之间的深层关系，从而提供更高质量的重建结果。然而，反投影模块的设计标准仍然不清楚。</p><p>​  由于该机制刚刚被引入到基于深度学习的SR中，因此该框架具有巨大的潜力，需要进一步的探索。</p><h2 id="上采样方法">上采样方法</h2><p>​  除了模型中的上采样位置外，如何进行上采样也非常重要。虽然有各种传统的上采样方法[20]、[21]、[88]、[89]，但利用cnn学习端到端上采样已逐渐成为一种趋势。在本节中，我们将介绍一些传统的基于插值的算法和基于深度学习的上采样层。</p><h3 id="基于插值的上采样">基于插值的上采样</h3><p>​  图像插值，a.k.a.图像缩放，是指调整数字图像的大小，并被广泛应用于与图像相关的应用程序中。传统的插值方法包括最近邻插值、双线性和双边插值、Sinc和兰氏重采样等。由于这些方法易于解释和易于实现，其中一些方法仍被广泛应用于基于cnn的SR模型中。</p><h4 id="最近邻插值">最近邻插值</h4><p>最近邻插值是一种简单、直观的算法。它为每个要被插值的位置选择最近的像素的值，而不考虑任何其他像素。因此，这种方法速度非常快，但通常会产生低质量的块状结果。</p><h4 id="双线性插值">双线性插值</h4><p>双线性插值（BLI）首先在图像的一个轴上进行线性插值，然后在另一个轴上进行，如图3所示。由于它导致了一个接受场大小为22的二次插值，因此在保持相对较快的速度的同时，它显示出了比近邻域插值更好的性能。</p><h4 id="二进制插值">二进制插值</h4><p>同样，双边插值（BCI）[10]在两个轴上分别进行三次插值，如图3所示。与BLI相比，BCI考虑了44个像素，并导致更流畅的结果，更少的伪影，但速度更低。事实上，具有抗锯齿的BCI是构建SR数据集的主流方法（即将HR图像降解为LR图像），也广泛应用于预上采样SR框架（第3.1.1节）。</p><p>​  事实上，基于插值的上采样方法仅基于其自身的图像信号来提高图像的分辨率，而没有带来更多的信息。相反，它们经常会引入一些副作用，如计算复杂性、噪声放大、模糊的结果。因此，目前的趋势是用可学习的上采样层取代基于插值的方法。</p><h3 id="基于学习的上采样">基于学习的上采样</h3><p>​  为了克服基于插值的方法的不足，以端到端学习上采样，在SR场中引入了转置卷积层和亚像素层。</p><h4 id="转置卷积层">转置卷积层</h4><p>转置卷积层，a.k.a.反卷积层[90]，[91]，试图执行与正常卷积相反的变换，即，基于类似于卷积输出大小的特征图来预测可能的输入。具体来说，它通过插入零和进行卷积来展开图像来提高图像的分辨率。以33 核的2SR为例（如图4所示），首先将输入的大小扩展原来的两倍，其中添加的像素值设置为0（图4b）。然后对核大小为33、步幅1和填充1进行卷积（图4c）。通过这种方式，输入被上采样了2倍，在这种情况下，接受域最多为22倍。由于转置卷积在保持与普通卷积兼容的连接模式的同时，使图像大小以端到端方式放大，因此在SR模型[57]、[78]、[79]、[85]中被广泛用作上采样层。然而，这一层很容易在每个轴[92]上造成“不均匀的重叠”，并且在两个轴上相乘的结果进一步创建了一个不同大小的棋盘状模式，从而损害了SR性能。</p><h4 id="亚像素层">亚像素层</h4><p>亚像素层[84]是另一个端到可学习的上采样层，通过卷积生成多个信道，然后进行上采样，如图5所示。在这一层中，首先应用卷积来产生具有s2倍通道的输出，其中s是比例因子（图5b）。假设输入大小为hw c，输出大小将为h w s2c。之后，进行整形操作(a.k.a.执行shuffle[84])来产生大小为sh sw c的输出（图5c）。在这种情况下，接受野最高可达33。由于端到端上采样方式，该层也被广泛应用于SR模型[25]、[28]、[39]、[93]。与转置卷积层相比，亚像素层具有更大的接受域，提供了更多的上下文信息，帮助生成更真实的细节。然而，由于感受野的分布是不均匀的，块状区域实际上共享相同的感受野，它可能会在不同块的边界附近产生一些伪影。另一方面，独立预测块状区域中的相邻像素可能会导致输出不平滑输出。因此，Gao等人[94]提出了PixelTCL，它将独立预测替换为相互依赖的序列预测，并产生更平滑、更一致的结果。</p><h4 id="meta上采样模块">Meta上采样模块</h4><p>以往的方法需要对缩放因子进行预细化，即针对不同的因子训练不同的上采样模块，但效率低，不符合实际需求。因此，Hu等人[95]提出了Meta上采样模块（如图6所示），首先基于元学习解决了任意比例因子的SR。具体来说，对于HR图像上的每个目标位置，该模块将其投影到LR特征图上的一个小补丁（即kk cin），根据投影偏移和比例因子预测卷积权值（即k k cincout），并进行卷积。这样，Meta上采样模块就可以通过单一模型的任意因素连续放大。由于大量的训练数据（同时训练多个因素），该模块在固定因素上可以表现出类似甚至更好的性能。虽然该模块在推理过程中需要预测权重，但上采样模块的执行时间只占特征提取[95]时间的1%左右。然而，该方法基于独立于图像内容的几个值来预测每个目标像素的大量卷积权值，因此在面对较大的放大倍数下，预测结果可能不稳定，效率较低。</p><p>​  目前，这些基于学习的层已经成为应用最广泛的上采样方法。特别是在上采样后框架（第3.1.2节）中，这些层通常在最终上采样阶段使用，基于低维空间提取的高级表示重建HR图像，从而在避免高维空间中压倒性操作的同时实现端到端SR。</p><h2 id="网络设计">网络设计</h2><p>​  网络设计是深度学习的重要组成部分之一。在超分辨率领域，研究人员在四种SR框架之上（第3.1节）应用各种网络设计策略来构建最终的网络。在本节中，我们将这些网络分解为网络设计的基本原则或策略，介绍它们，并逐一分析其优点和局限性。</p><h3 id="残差学习">残差学习</h3><p>​  在He等人[96]提出ResNet来学习残差而不是进行彻底的映射之前，残差学习已被SR模型[48]、[88]、[97]广泛使用，如图7a所示。其中，剩余学习策略大致可分为全局残差学习和局部残差学习。</p><h4 id="全局残差余学习">全局残差余学习</h4><p>由于图像SR是一种图像-图像转换任务，输入图像与目标图像高度相关，研究者尝试只学习它们之间的残差，即全局残差学习。在这种情况下，它避免了学习从一个完整图像到另一个完整图像的复杂转换，而是只需要学习一个残差映射来恢复缺失的高频细节。由于大多数区域的残差接近于零，模型的复杂性和学习差异大大降低。因此，它被广泛应用于SR模型[26]、[55]、[56]、[98]。</p><h4 id="局部残差学习">局部残差学习</h4><p>局部残差学习类似于ResNet[96]中的残差学习，用于缓解由于网络深度不断增加而导致的[96]退化问题，降低训练难度，提高学习能力。它也被广泛用于SR[70]、[78]、[85]、[99]。</p><p>​  在实际应用中，上述方法都是通过快捷连接（通常按一个小常量缩放）和元素加法实现的，不同之处在于前者直接连接输入和输出图像，而后者通常在网络内部不同深度的层之间添加多个快捷方式。</p><h3 id="递归学习">递归学习</h3><p>​  为了在不引入压倒性参数的情况下学习更高层次的特征，我们在SR字段中引入了递归学习，即以递归的方式多次应用相同的模块，如图7所示。</p><p>​  其中，16递归DRCN [82]采用单一卷积层作为递归单元，达到4141，远远大于SRCNN [22]的13 13，没有过多参数。DRRN [56]使用一个ResBlock[96]作为25次递归的递归单元，并且获得了比17-ResBlock基线更好的性能。后来Tai等人[55]提出了基于内存块的MemNet，该块由6个递归的重新块组成，每个递归的输出被连接起来，并经过额外的11个卷积进行记忆和遗忘。级联剩余网络（CARN）[28]也采用了类似的递归单元，包括几个重新块。最近，Li等人[86]采用了迭代上下采样SR框架，提出了一种基于递归学习的反馈网络，其中整个网络的权值在所有递归中共享。</p><p>​  此外，研究人员还在不同的部分使用了不同的递归模块。具体来说，Han等人[85]提出了双状态递归网络（DSRN）来在LR和HR状态之间交换信号。在每个时间步长（即递归），每个分支的表示都被更新和交换，以更好地探索LR-HR关系。</p><p>​  类似地，Laiet al.[65]使用嵌入和上采样模块作为递归单元，因此以性能损失很小为代价，大大减少了模型的大小。</p><p>​  一般来说，递归学习确实可以学习更高级的表示，而不引入过多的参数，但仍然不能避免高昂的计算成本。它本质上带来了消失或爆炸的梯度问题，因此一些技术，如残差学习（第3.3.1节）和多监督（第3.4.4节）经常与递归学习集成，以缓解这些问题[55]，[56]，[82]，[85]。</p><h3 id="多路径学习">多路径学习</h3><p>​  多路径学习是指将特征通过多条路径，这些路径执行不同的操作，并将它们融合回来以提供更好的建模能力。具体来说，它可以分为全局、局部和特定规模的多路径学习，如下所述。</p><h4 id="全局多路径学习">全局多路径学习</h4><p>全局多路径学习是指利用多条路径来提取图像的不同方面的特征。这些路径在传播过程中可以相互交叉，从而大大提高了学习能力。具体来说，LapSRN[27]包括一个以粗到细的方式预测子带残差的特征提取路径和另一个基于来自两条路径的信号重建HR图像的路径。同样，DSRN[85]利用两条路径分别在低维和高维空间中提取信息，并不断交换信息以进一步改进学习能力。像素递归超分辨率[64]采用条件反射路径来捕获图像的全局结构，并采用先验路径来捕获生成的像素的串行依赖性。相比之下，Ren等人[100]在模型的末端采用多条具有不平衡结构的路径进行上采样和融合。</p><h4 id="局部多路径学习">局部多路径学习</h4><p>在初始模块[101]的激励下，MSRN[99]采用了一个新的块来进行多尺度特征提取，如图7e所示。在这个块中，采用两个核大小为33 和5 5的卷积层同时提取特征，然后将输出连接并再次进行相同的操作，最后应用额外的11个卷积。快捷方式通过元素添加连接输入和输出。通过这种局部多路径学习，SR模型可以更好地从多个尺度中提取图像特征，进一步提高性能。</p><h4 id="特定尺寸的多路径学习">特定尺寸的多路径学习</h4><p>考虑到不同尺度的SR模型需要进行相似的特征提取，Lim等人[31]提出了特定尺度的多路径学习来应对单一网络的多尺度SR。具体地说，它们共享模型的主成分（即特征提取的中间层），并分别在网络的开始和结束时附加了特定尺度的预处理路径和上采样路径（如图7f所示）。在训练期间，只启用和更新与所选比例对应的路径。通过这种方式，所提出的MDSR[31]通过共享不同尺度的大部分参数，大大减少了模型的大小，并表现出与单尺度模型相当的性能。CARN[28]和ProSR [32]也采用了类似的尺度特异性多路径学习。</p><h3 id="密集连接">密集连接</h3><p>​  由于Huang等人[102]提出了基于密集块的DenseNet，密集连接在视觉任务中越来越流行。对于密集块中的每一层，前面所有层的特征图都被用作输入，其自己的特征图被用作所有后续层的输入，从而导致l层密集块中的（l*(l-1)/2）的连接。密集连接不仅有助于缓解梯度消失，增强信号传播，鼓励特征重用，而且还通过使用小增长率（即密集块中的通道数量）和连接所有输入特征图后压缩通道，大大减少模型大小。</p><p>​  为了融合低层次和高层次的特征，为重构高质量的细节提供更丰富的信息，在SR域中引入了密集的连接，如图7d所示。唐等[79]不仅采用密集块构造一个69层SRDenseNet，还插入密集连接不同密集块，也就是说，对于每一个密集块，所有之前的特征映射块被用作输入，和自己的特性映射被用作输入到所有后续块。MemNet[55]、CARN [28]、RDN [93]和ESRGAN[103]也采用了这些层级和块级的密集连接。DBPN[57]也广泛地采用了密集连接，但它们的密集连接位于所有的上采样单元之间，下采样单元也是如此。</p><h3 id="注意力机制">注意力机制</h3><h4 id="通道注意力">通道注意力</h4><p>考虑到不同通道之间特征表示的相互依赖和相互作用，Hu等人[104]提出了一个“挤压和激励”块，通过明确建模通道相互依赖来提高学习能力，如图7c所示。在这个块中，使用全局平均池化（GAP）将每个输入信道压缩到一个信道描述符（即一个常数）中，然后将这些描述符输入到两个密集的层中，为输入信道生成信道缩放因子。最近，Zhang等人[70]将通道注意机制与SR结合起来，提出了RCAN，显著提高了模型的表示能力和SR性能。为了更好地学习特征相关性，Dai等人的[105]进一步提出了一个二阶信道注意（SOCA）模块。SOCA通过使用二阶特征统计而不是GAP自适应地调整信道特征，并能够提取更多信息性和区别性的表示。</p><h4 id="非本地注意力">非本地注意力</h4><p>大多数现有的SR模型的局部接受域非常有限。然而，一些遥远的对象或纹理可能对局部补丁的生成非常重要。因此，Zhang等人[106]提出了局部和非局部注意块来提取捕获像素之间的长期依赖关系的特征。具体地说，他们提出了一个用于提取特征的主干分支，以及一个（非）局部掩码分支，用于自适应地重新调整主干分支的特征。其中，局部分支采用编码器-解码器结构来学习局部注意，而非局部分支采用嵌入式高斯函数来评估特征图中每两个位置指标之间的成对关系，以预测尺度权值。通过这种机制，该方法很好地捕捉了空间注意力，并进一步提高了表示能力。同样，Dai等人[105]也采用了非局部注意机制来捕获长距离空间背景信息。</p><h3 id="先进的卷积">先进的卷积</h3><p>​  由于卷积操作是深度神经网络的基础，研究人员也试图改进卷积操作，以提高性能或提高效率。</p><h4 id="膨胀卷积">膨胀卷积</h4><p>众所周知，上下文信息有助于生成SR生成现实细节。因此，Zhang等人[107]在SR模型中用扩张卷积来取代常见的卷积，增加了两次以上，获得了更好的性能。</p><h4 id="集团卷积">集团卷积</h4><p>受轻量级CNNs的最新进展的推动，[108]，[109]，Hui等人[98]和Ahn等人[28]分别提出了IDN和CARN-M，用组卷积代替香草卷积。正如之前的一些工作所证明的那样，组卷积大大减少了参数和操作的数量，而牺牲了一点性能损失[28]，[98]。</p><h4 id="深度可分离卷积">深度可分离卷积</h4><p>自从Howard等人[110]提出深度可分离卷积以实现有效的卷积以来，它已经被扩展到各个领域。具体地说，它由一个因子分解的深度卷积和一个点态卷积（即11个卷积）组成，因此只在很小的情况下减少了大量的参数和操作降低精度的[110]。最近，Nie等人的[81]采用了深度可分离卷积，并大大加速了SR体系结构。</p><h3 id="区域递归学习">区域递归学习</h3><h3 id="金字塔池化">金字塔池化</h3><h3 id="小波变换">小波变换</h3><h3 id="desubpixel">Desubpixel</h3><h3 id="xunit">xUnit</h3><h2 id="学习策略">学习策略</h2><h3 id="损失函数">损失函数</h3><h4 id="pixel-loss">Pixel Loss</h4><p>像素损失测量两个图像之间的像素级差异，主要包括L1损失（即平均绝对误差）和L2损失（即均方误差）：<spanclass="math display">\[\mathcal{L}_{\mathrm{pixel}\perp1}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}|\hat{I}_{i,j,k}-I_{i,j,k}|\\\mathcal{L}_{\mathrm{pixel}\perp2}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}(\hat{I}_{i,j,k}-I_{i,j,k})^{2},\]</span></p><h4 id="content-loss">Content Loss</h4><p>为了评价图像的感知质量，将内容损失引入SR[29]，[127]。具体来说，它使用预先训练好的图像分类网络来测量图像之间的语义差异。将该网络表示为f，提取的第1层高级表示表示为fðlÞðIÞ，内容损失表示为两幅图像的高级表示之间的欧氏距离，如下：<spanclass="math display">\[\mathcal{L}_{\mathrm{content}}(\hat{I},I;\phi,l)=\frac{1}{h_lw_lc_l}\sqrt{\sum_{i,j,k}(\phi_{i,j,k}^{(l)}(\hat{I})-\phi_{i,j,k}^{(l)}(I))^2},\]</span></p><h3 id="批量规范化">批量规范化</h3><h3 id="课程学习">课程学习</h3><h3 id="多重监督">多重监督</h3><h3 id="其他改进">其他改进</h3><h4 id="上下文网络融合">上下文网络融合</h4><h4 id="数据增强">数据增强</h4><h4 id="多任务学习">多任务学习</h4><h4 id="网络插值">网络插值</h4><h4 id="自我整合">自我整合</h4><h2 id="最先进的超分辨率模型">最先进的超分辨率模型</h2><p>​  近年来，基于深度学习的图像超分辨率模型受到了越来越多的关注，并取得了最先进的性能。在前面的章节中，我们将SR模型分解为特定的组件，包括模型框架（第3.1节）、上采样方法（第3.2节）、网络设计（第3.3节）和学习策略（第3.4节），对这些组件进行分层分析，并确定它们的优点和局限性。事实上，今天大多数最先进的SR模型基本上都可以归因于我们在上面总结的多种策略的组合。例如，RCAN[70]最大的贡献来自于通道注意机制（第3.3.5节），它还采用了其他策略，如亚像素上采样（第3.3.2.2节）、残差学习（第3.3.1节）、像素L1损失（第3.4.1节）和自集成（第3.5.5节）。以类似的方式，我们总结了一些具有代表性的模型及其关键策略，如表2所示。</p><figure><imgsrc="./../postimages/Image_Super-Resolution/image-20240619184059043.png"alt="image-20240619184059043" /><figcaption aria-hidden="true">image-20240619184059043</figcaption></figure><p>在上面，“Fw", "Rec.", "Res"，"Dense","Att."分别表示SR框架、上采样方法、递归学习、残差学习、密集连接、注意机制</p><p>​  除了SR精度外，效率是另一个非常重要的方面，不同的策略对效率有或多或少的影响。因此，在前面几节中，我们不仅分析了所提出策略的准确性，而且还指出了对效率影响较大的策略的具体影响，如后上采样（3.1.2节）、递归学习（3.3.3.2节）、密集连接（3.3.3.4节）、xUnit（3.3.11节）。我们还对一些具有代表性的SR模型的SR精度（即PSNR）、模型大小（即参数数）和计算成本（即多加数）等方面的SR模型进行了基准测试，如图8所示。精度是通过在4个基准数据集（即Set5[48]，Set14 [49]，B100 [40]和Urban100[50]）上的PSNR的平均值来测量的。模型大小和计算成本用PyTorch-光学传感器[157]计算，其中输出分辨率为720p（即1080720）。所有的统计数据都是根据原始论文或根据官方模型计算得出的，比例因子为2。为了更好地查看和比较，我们还提供了一个交互式的在线版本1。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UnionFormer:Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</title>
      <link href="/UnionFormer/"/>
      <url>/UnionFormer/</url>
      
        <content type="html"><![CDATA[<center>UnionFormer: Unified-Learning Transformer with Multi-View Representationfor Image Manipulation Detection and Localization <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a></center><center><span class="math inline">\(\text{Shuaibo Li}^{1,2}\quad\text{WeiMa}^{1\dagger}\quad\text{Jianwei Guo}^2\quad\text{ShibiaoXu}^3\quad\text{Benchong Li}^1\quad\text{Xiaopeng Zhang}^2\)</span></center><center>北京理工大学1、MAIS(中国科学院自动化研究所)2、北京邮电大学3</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/UnionFormer/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  我们提出了一个新的框架，通过统一学习集成了三个视图上的篡改线索，用于图像操作检测和定位。特别地，我们构建了一个BSFI-Net，从RGB和噪声视图中提取篡改特征，在调节不同尺度上的空间一致性的同时，增强了对边界伪影的响应性。此外，为了探索对象之间的不一致性作为一种新的线索视角，我们将对象一致性建模与篡改检测和定位结合成一个三任务统一的学习过程，使它们能够相互促进和改进。</p><p>​  因此，我们在多尺度监督下获得了一个统一的操作鉴别表示，从三个角度整合信息。这种集成便于高效的并行检测和定位篡改。我们在不同的数据集上进行了大量的实验，结果表明，该方法在篡改检测和定位方面优于最先进的方法。</p><h1 id="引言">1. 引言</h1><p>​  数字图像篡改可分为三大类[19]：拼接，即将区域从一幅图像复制到另一幅图像；复制-移动，包括复制或移动同一图像中的元素；移除，删除图像部分和创建视觉一致的内容以掩盖改变的过程。这些操作在被篡改区域和周围环境之间留下痕迹，造成真实区域和伪造区域之间的不一致。与传统的强调高级语义信息的传统检测或分割任务不同，图像篡改检测优先考虑局部语义无关的线索，以区分真实性，而不是语义内容。因此，篡改检测的关键挑战是学习结合不同层次信息并捕获真实和篡改区域之间多尺度不一致的通用特征。以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层[23,27,40,71]的特征，不能充分表示篡改痕迹。受[9,12,67]的启发，我们设计了一个专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork），并将其作为特征编码器集成到我们的框架中。BSFI-Net是一个并行的cnn-Transformer结构，它可以加强边缘响应，同时有效地在局部特征和全局表示之间进行交互，以探索不同尺度上图像内部的一致性。<br/>​  另一方面，许多在RGB视图中难以察觉的篡改伪影在噪声视图中变得明显明显。使用固定的[18]或可学习的高通滤波器[6,35,66]将RGB图像转换为噪声图，可以抑制内容，并突出显示低级的伪造线索。因此，开发一种同时建模RGB和噪声维度的多视图策略对于检测细微的篡改痕迹至关重要。我们的框架采用了一个双流架构来独立地构建RGB和噪声视图的表示，随后合并它们以提高鉴别能力和泛化性。此外，我们还结合了对比监督，以改善这两种观点之间的协作。<br/>​  此外，为了创建空间相干和语义一致的图像，篡改操作总是改变整个对象来隐藏证据，即执行对象级操作。目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息。相反，我们认为图像操作检测应该不仅仅是识别分布外的像素或补丁，以捕获由操作导致的对象一致性和分布的异常。由于扩散模型[4,5,20,30,44,65,69]生成的超真实的篡改图像，利用对象视图信息变得特别重要。基于扩散的模型[4,30,44]反复更新了整个图像的初始噪声，增强了空间连续性，留下了更少的RGB和噪声痕迹。此外，与真实的图像源不同，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。最近的扩散模型[20,29,55,64]试图通过采用以对象为中心的方法来解决这个问题，强调了使用对象视图线索进行篡改检测的必要性和可行性。然而，创建和集成这样的新视图与其他视图，以篡改伪影表示是一个重大的挑战，需要新的架构和学习策略。<br/>​  考虑到上述要点，我们引入了UnionFormer，一个用于图像操作检测和定位的多视图表示的统一学习transformer框架，如图1所示。</p><p><img src="../postimages/UnionFormer/image-20240617100605323.png"alt="image-20240617100605323" /><br/>图1.UnionFormer的组成概述。我们通过整合来自三个视图表示的篡改线索来实现同时的篡改检测和定位，每个视图由不同的颜色背景表示。我们通过BSFI-Net获得了RGB和噪声视图下的表示，并在统一学习中构建了基于两者的对象视图表示。同时，将三个视图的信息交互融合成统一的操作判别表示（UMDR,unified manipulation discriminative representation）进行检测和定位</p><p>​  首先，我们使用BSFI-Net作为特征编码器，获得在RGB和噪声视图下的通用化特征，并将其进行组合。然后，我们利用融合的特征进行一个单一化的学习过程，其中包括三个子任务：对象一致性建模、伪造检测和伪造定位。在统一学习中，我们的模型建立了对象视图表示，并将三个视图信息集成到一个统一的操作鉴别表示（UMDR,unified manipulation discriminativerepresentation）中，同时完成伪造检测和定位。综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一种新的图像取证transformer框架，UnionFormer。通过多尺度监督的统一学习，整合三个视角的信息，同时执行图像操作检测和定位。</li><li>我们引入了BSFI-Net，一种用于高级人工表示学习的混合网络结构，它增强了边界响应，同时揭示了不同层次的局部不一致性。</li><li>通过对UMDR的统一学习，我们构建了一种创新的对象视图表示方法，能够从三个视图中捕获对象之间的不一致性和聚合信息，用于伪造检测。</li><li>我们通过各种基准进行了全面的实验，证明了我们的方法在检测和定位任务中都获得了最先进的结果。</li></ul><h1 id="方法">2. 方法</h1><p>​  在本节中，我们首先提供对工会成员的概述和对每个组件的详细介绍。我们的目标是充分利用来自三个视图的丰富工件来同时进行篡改检测和定位。我们通过在多尺度监督下的统一学习过程来实现这一目标。<br/>​  如图1所示，首先使用受约束的CNN[7]将输入的RGB图像X转换为噪声视图表示N = C(X)，可以显示低级的篡改。<br/>​  然后，将X和N分别输入边界敏感特征交互网络（BSFI-Net）进行特征编码。高频边缘特征(H)与X或N一起作为BSFI-Net的输入，以提高边缘响应性。这使得我们能够在RGB和噪声视图下获得可推广的和可鉴别的特征，构造两个特征金字塔$ f_r = _1(X,H), f_n = _2(N,H) $。<br/>​  随后，我们使用区域建议网络（RPN）[51]从特征fr中获得一组感兴趣的区域（RoIs），用pi表示。从fr和fn中提取RoI信息，然后扁平得到建议的嵌入表示，记为ri，ni。将每个方案的RGB特征ri和噪声特征ni连接起来，生成融合的方案特征di，并将其输入到I变压器编码器层。<br/>​  在统一学习阶段，我们处理了三个子任务：建模对象的一致性、真实性的二进制分类和篡改区域定位。在转换器编码器之后，将伪造-判别查询嵌入DI输入到统一操作判别表示部分，对三个子任务生成三个预测。如图1所示，我们对三个子任务采用了具有统一形式的多尺度监督，包括Lcls、Locm和Lloc。</p><figure><img src="../postimages/UnionFormer/image-20240618124653610.png"alt="image-20240618124653610" /><figcaption aria-hidden="true">image-20240618124653610</figcaption></figure><h2 id="特征交互编码">2.1 特征交互编码</h2><h3 id="rgb和噪声视图表示">2.1.1 RGB和噪声视图表示。</h3><p>​  在特征编码阶段，我们利用一个双流结构来利用来自RGB和噪声视图的线索。RGB流被设计为捕获视觉上明显的篡改伪影，而噪声流旨在探索被篡改区域和真实区域之间的分布不一致性。我们利用[7]中提出的可学习约束卷积层将RGB图像转换为噪声视图。如第2节所述，被篡改区域及其周围环境的边缘表现出更明显的篡改线索。因此，我们增强了两个流中的高频边缘信息，将网络的响应集中在被篡改的区域。具体来说，我们利用离散余弦变换（DCT）将图像数据X转换为频域，然后应用高通滤波器得到高频分量。然后，我们将高频分量转换回空间域，以促进特征交互和保持局部一致性。因此，我们得到的边缘增强信息H如下：<spanclass="math display">\[H=\mathcal{T}_d^{-1}\left(\mathcal{F}_h\left(\mathcal{T}_d(X),\beta\right)\right)\]</span>​  其中Td表示DCT，Fh表示高通滤波器，β为阈值。我们将X和N分别输入到BSFI-Net中，以及H来进行特征编码，如图2所示。</p><h3 id="边界敏感特征交互网络">2.1.2 边界敏感特征交互网络。</h3><p>​  除了增强边界响应外，集成局部特征和全局表示对图像伪造检测也至关重要。这就要求进行全面分析在不同尺度上的图像内部的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-Transformer并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><br/>图2.BSFI-Net的概述。FCU表示特征耦合单元，BOB表示边界向块。</p><p>​  如图2所示，CNN分支作为主分支，以一个RGB或噪声图像作为输入，对局部信息进行编码。变压器分支以输入作为边缘增强信息H，引导CNN分支聚焦于被篡改的区域，并将图像补丁之间的长距离不一致传输给它。我们使用[48]提出的特征耦合单元（FCU）来消除来自CNN分支的特征映射和来自transformer分支的补丁嵌入之间的错位。此外，我们还设计了一个面向边界的块（BOB），以方便将高级补丁一致性和边界信息从变压器分支传输到CNN分支，从而指导CNN分支。<br/>​  CNN分支由5个卷积块组成，类似于ResNet构造[24]。与[16,48]一样，transformer分支由5个重复的transformer块组成，由一个多头自注意模块和一个MLP块组成。采用与ViT[16]相同的令牌化操作。在FCU中，在添加补丁嵌入和CNN特征之前，使用1×1的卷积和重新采样来对齐通道和空间维度。在BOB中，CNN分支的特征映射被输入1×1卷积层、批归一化层、s型层，并通过双线性插值上采样到高分辨率。然后，将来自CNN分支的特征与长距离判别权值进行元素级乘法。我们将BSFI-Net作为特征编码器进行预训练，生成RGB和噪声视图表示，特征金字塔网络[38]基于中间特征映射{C2、C3、C4、C5}生成两个特征金字塔fr，fn。培训细节详见第4.1节。</p><h2 id="特征对比性协作">2.2 特征对比性协作</h2><p>​  在特征协作阶段，受[51,56]的启发，我们首先使用一个基于RGB特征金字塔fr的区域建议网络（RPN）来生成一组感兴趣的区域（RoIs）。然后，我们利用RoIAlign[25]从两个流的特征金字塔fr和fn中提取RoIs的信息。除了特征连接之外，我们还采用对比监督来促进两个视图之间的协作。我们将来自不同流的被篡改的建议视为积极建议，被篡改的建议和真实建议被指定为负对。在InfoNCE损失[47,67]之后，对比度损失被定义为：<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{con}}=-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{1})}-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{2})}\end{aligned}\]</span>​  式中，s0表示正对之间的相似性，s1表示RGB篡改嵌入与噪声真实嵌入之间的相似性，s2表示RGB真实嵌入与噪声篡改嵌入之间的相似性。对比损失Lcon引入统一学习监督，将在第3.3节进行讨论。</p><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><h2 id="具有多尺度监督下的统一学习">2.3 具有多尺度监督下的统一学习</h2><p>​  <strong>Transformer编码器。</strong>我们的统一学习模块是一个仅限编码器的transformer架构，它处理融合的提议嵌入二，以及它们的特定位置编码作为输入。在转换器编码器的每一层中，自我注意机制通过不同的建议嵌入来聚合信息，并捕获它们的长距离依赖关系，这意味着对象的一致性。详细地说，我们使用了一个变压器解码器，具有六层，宽度为512，和8个注意头。变压器内的前馈网络（FFN）的隐藏大小为2048。在转换器编码器之后，我们生成判别查询嵌入DI，并输入统一操作判别表示（UMDR）部分，以生成三个子任务的预测，即。对象一致性建模、图像操作检测和定位。</p><p>​  <strong>统一伪造判别表示</strong>。在转换器编码器之后，DI中的每个篡改判别查询都表示对应建议的三个视图中的篡改线索。图3显示了三个子任务的学习过程。</p><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure><p>图3。多尺度监督下的UMDR学习。图像内部在不同尺度上的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-变压器并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p>​  UMDR是在真实性分类、对象一致性建模和操作定位分支的监督下学习的。与DETR[9]和SOLQ[12]一样，分类分支是一个完全连接的（FC）层，用来预测真实性可信度Pˆc。目标一致性建模分支是一个多层感知（MLP），隐藏大小为256，用于预测目标空间信息Pˆo。操作定位分支也是一个隐藏大小为1024的多层感知来预测定位掩码向量Pˆm。对前两个分支机构的监管类似于DETR[9]。在第三个分支中，我们利用对地面真实掩码进行编码得到的掩模向量作为监督信息。在推理过程中，将压缩后的编码过程应用于Pˆm来重构定位掩码。在压缩编码中，我们利用主成分分析（PCA）将二维空间二值掩模转换为一维掩模向量。</p><p>​  <strong>损失函数。</strong>UnionFormer监督的总体损失职能可表示为：<spanclass="math display">\[\mathcal{L}_{union}=\lambda_{cls}\cdot\mathcal{L}_{cls}+\mathcal{L}_{ocm}+\lambda_{loc}\cdot\mathcal{L}_{loc}+\beta\cdot\mathcal{L}_{con},\]</span>​  其中Lcls表示分类的focal损失[39]。Lloc表示定位掩码向量监督的L1损失。Lcon是在第3.2节中引入的对比性学习损失。λcls、λloc、β是相应的调制系数。Locm是对象一致性建模的损失，其定义为：<spanclass="math display">\[\mathcal{L}_{\mathrm{ocm}}=\lambda_{L_1}\cdot\mathcal{L}_{L_1}+\lambda_{gious}\cdot\mathcal{L}_{gious}\]</span>​  其中LL1和Lleam为L1损失和广义IoU损失[52]，与DETR相同。λL1和λgious是对应的系数。在[12]之后，Lloc不包括在二部匹配过程中。</p><h1 id="实验">3. 实验</h1><h2 id="实验设置">3.1 实验设置</h2><p>​  <strong>训练</strong>。我们使用了一个大规模的训练数据集，包括各种类型的篡改和真实的图像。它分为五个部分：1) CASIA v2 [14]，2)Fantastic Reality[32]，3)Tampered COCO,，来自COCO2017数据集[37]，4)Tampered RAISE，基于RAISE数据集[11]构建，5)从COCO2017和RAISE数据集中选择的原始图像。我们在合成数据中随机添加高斯噪声或应用JPEG压缩来模拟现实场景中的视觉质量和篡改轨迹。在训练过程中，我们依次分三个阶段对BSFI-Net、RPN和UnionFormer进行训练。</p><p>​  <strong>测试。</strong>为了全面评估和比较我们的模型与各种最先进的方法，我们使用了6个公开可用的测试数据集和另一个由混合扩散模型[4]创建的超真实篡改图像数据集。具体来说，我们使用了CASIAv1 [14]、Columbia[26]、Coverage[61]、NIST16 [22]、IMD20 [46]和CocoGlide[23]。然后，我们构建了BDNIE，包括512张由先进的混合扩散模型生成的超真实的假图像，用于文本驱动的自然图像编辑。训练和测试数据的细节载于补充资料。</p><p>​  <strong>评价指标。</strong>我们评估了该方法在图像篡改检测和定位任务中的性能。对于定位图像操作的任务，我们报告了像素级的曲线下面积（AUC）和F1分数，同时使用最佳的和固定的0.5阈值。对于[23]之后的检测任务，我们采用图像级AUC和平衡精度，同时考虑假报警和遗漏检测，在这种情况下，阈值设置为0.5。为了保证比较的公平性和准确性，我们从文献[23,59]中取出了其他方法的一些结果值。</p><p>​  <strong>实施细节。</strong>BSFI-Net采用AdamW优化器[41]进行了100个周期的交叉熵损失训练，批处理大小为512，权重衰减为0.05。初始学习速率被设置为0.001，并在余弦时间表中衰减。</p><p>​  在与Lunion一起训练完整的UnionFormer时，受[56,63]的启发，我们采用36周期（3×）计划来训练UnionFormer进行2.7×105次迭代，批大小为16。在这个阶段还使用了一个AdamW优化器。学习速率在开始时被设置为10−4，并在1.8×105和2.4×105迭代时乘以0.1。</p><h2 id="与最先进的技术相比较">3.2 与最先进的技术相比较</h2><p>​  <strong>Baseline。</strong>为了确保公平和准确的比较，我们只选择了最先进的方法，其中作者提供了预训练的模型，发布的源代码，或在通用标准[27,40,59]下进行评估。为了减少偏差，我们只考虑了在不与测试数据集重叠的数据集上训练的方法或版本。详细地说，我们包括了7种最先进的方法：MantraNet[62]，SPAN[27]，PSCC-Net[40]，MVSS-Net[13]，CAT-Netv2[34]，ObjectFormer[59]，和TruFor[23]。</p><p>​  <strong>定位结果。</strong>表2和表1分别显示了基于像素级AUC和F1评分指标的图像篡改定位结果。排名最高的方法用粗体表示，一条水平线表示排名第二的方法，在表4和表3中也采用了相同的注释。</p><figure><img src="../postimages/UnionFormer/image-20240617115640307.png"alt="image-20240617115640307" /><figcaption aria-hidden="true">image-20240617115640307</figcaption></figure><p>​  我们的方法在所有数据集上展示了像素级AUC评估的最佳性能。</p><figure><img src="../postimages/UnionFormer/image-20240617115601335.png"alt="image-20240617115601335" /><figcaption aria-hidden="true">image-20240617115601335</figcaption></figure><p>​  对于f1评估，我们的方法在所有数据集上排名最好或第二。平均而言，无论是否使用最优或固定的阈值，我们都获得了显著的优势。事实上，在包含基于扩散的局部操作的相对新颖的CocoGlide数据集上，我们在两个阈值上分别比排名第二的TruOfor高出2.2%和1.3%。这是由于联合前体构建的对象视图伪影表达式，它可以揭示由扩散模型生成的区域和真实区域之间的不一致性。这些比较表明，我们的方法具有较强的泛化和捕获篡改伪的能力。</p><p>​  <strong>检测结果。</strong>表4为篡改检测的比较结果。</p><figure><img src="../postimages/UnionFormer/image-20240617115836976.png"alt="image-20240617115836976" /><figcaption aria-hidden="true">image-20240617115836976</figcaption></figure><p>​  在[23]之后，我们使用定位映射的最大值作为未明确为检测任务设计的方法的检测统计量。UnionFormer在除Columbia外的所有数据集上都取得了最佳的性能，并在平均结果上显示了显著的优势，无论是通过AUC还是平衡精度测量。正如[13,23]中提到的，精度对阈值选择很敏感，如果没有良好校准的数据集，很难确定。然而，我们的方法和次要的TruFor在这个要求很高的场景中取得了值得称赞的结果。我们在平均AUC和精度上分别保持了2.5%和2%的领先优势。这一优势主要归因于我们的框架的统一学习过程。统一学习通常会促进对定位和检测任务的相互增强。通过统一的操作鉴别表示，掌握了两个子任务，进一步提高了模型的性能。</p><p>​  <strong>鲁棒性评估。</strong>我们通过对NIST16数据集图像应用图像失真，验证了UnionFormer的鲁棒性。在[40,59]之后，我们包括了四种类型的畸变：1)将图像的大小改变到不同的尺度；2)应用核大小为k的高斯模糊；3)添加以标准偏差σ为特征的高斯噪声；4)对图像进行JPEG压缩，使用质量因子q。我们比较了像素级AUC与其他方法的性能。表3显示，我们的方法对各种失真操作表现出鲁棒性，优于其他方法。</p><figure><img src="../postimages/UnionFormer/image-20240617120221607.png"alt="image-20240617120221607" /><figcaption aria-hidden="true">image-20240617120221607</figcaption></figure><h1 id="可视化结果">4. 可视化结果</h1><h2 id="定性比较">4.1 定性比较</h2><p>​  <img src="../postimages/UnionFormer/image-20240618103731575.png"alt="image-20240618103731575" /></p><p>​  图4显示了跨不同数据集的定位结果。我们的方法可以准确地定位被篡改的区域，预测更详细和清晰的边界。这是由于我们的多视图特征捕获和BSFI-Net，其中频率信息增强了边缘响应，而分支之间的交互作用增强了特征的泛化和识别。由于对对象视图线索的建模和统一的学习框架，我们的方法在具有挑战性的BDNIE数据集上取得了令人满意的结果，而其他方法都失败了。</p><h2 id="不同视图表示法的可视化">4.2 不同视图表示法的可视化</h2><p>​  在图5中，我们可视化了BSFI-Net中变压器分支的噪声特征和边缘引导特征。</p><figure><img src="../postimages/UnionFormer/image-20240618103938959.png"alt="image-20240618103938959" /><figcaption aria-hidden="true">image-20240618103938959</figcaption></figure><p>如列1到4所示，一些图像在RGB视图中可能看起来很自然，但它们被篡改/真实的部分很容易在频域或噪声视图中被容易区分出来。第5列和第6列显示了由一个CNN分支和BSFI-Net的双分支生成的RGB特性。与只使用CNN分支相比，BSFI-Net更准确地激活了被篡改的区域，这得益于变压器分支提供的边缘引导和长距离线索。</p><p>​  此外，我们还定量地分析了对象视图，如图6所示。</p><figure><img src="../postimages/UnionFormer/image-20240618104100130.png"alt="image-20240618104100130" /><figcaption aria-hidden="true">image-20240618104100130</figcaption></figure><p>​  在统一学习阶段，我们从transformer编码器中推导出亲和矩阵Ai。基于Ai，我们随机选择提案嵌入的一个子集，计算它们与其他建议的平均亲和力，记为ei。然后将ei归一化到范围[0,1]，并作为一个颜色系数来可视化建议，较浅的颜色表示较低的亲和力。结果表明，使用伪造物体的提案与其他区域的平均亲和力较低，这表明UMDR能够捕捉真实物体和虚假物体之间的不一致性。</p><h1 id="消融研究">5. 消融研究</h1><p>​  我们进行了消融研究，以评估我们的方法中关键成分的影响。定量结果见表5。</p><figure><img src="../postimages/UnionFormer/image-20240618104307449.png"alt="image-20240618104307449" /><figcaption aria-hidden="true">image-20240618104307449</figcaption></figure><p>​  我们可以观察到，通过在第一个基线模型上添加噪声流，CASIAv1的AUC得分增加8.7%，NIST 16增加8.3%，同时进一步增加对象视图表示，CASIAv1继续增加10.7%，NIST16继续增加7.4%。这证明了噪声和对象视图表示的有效性。此外，当缺乏对比监督，或BSFI-Net被ResNet-50[24]取代时，模型的性能会显著下降。这突出了两个流之间的交互的有效性和BSFI-Net在描述伪造制品方面的特殊能力。</p><p>​  BSFI-Net中的BOB和FCU模块改善了其两个分支之间的交互作用，并有效地消除了它们之间的特征失调。当单独去除BOB或FCU时，整体模型在NIST16数据集上的定位AUC得分分别下降了4.8%和6.3%。</p><p>​  我们进一步进行了实验，研究了UMDR中几个关键因素的影响。λloc，Locm，掩码向量维度nv，以及压缩编码的类型。</p><figure><img src="../postimages/UnionFormer/image-20240618104459423.png"alt="image-20240618104459423" /><figcaption aria-hidden="true">image-20240618104459423</figcaption></figure><p>​  我们比较了三种压缩编码方法：稀疏编码[15]、离散余弦变换（DCT）[2]和主成分分析（PCA）[1]。如表6所示，当设置对比损失时，以PCA为编码类型，并将λloc和Locm分别设置为1和256时，该模型在NIST16数据集上表现最好。</p><h1 id="结论">6. 结论</h1><p>​  在本文中，<br/>​  我们介绍了UnionFormer，一个联合学习transformer框架，它利用来自三个不同视图的线索来进行图像操作检测和定位。UnionFormer使用BSFI-Net作为特  征编码器，在RGB和噪声视图下提取具有高度区分性的特征。然后，通过三个任务的统一学习过程，UnionFormer建模了对象之间的不连续性，即对象视图表示，并学习统一的判别表示。从三种观点整合信息的统一表示具有较强的通用性和区分性。它可以准确地识别各种图像操作，无论是传统的手动编辑还是基于扩散模型的自然语言驱动的篡改。此外，统一的学习框架使子任务的相互增强，实现了高精度的检测和定位。在不同的数据集上进行的综合实验证明了该方法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 边缘增强 </tag>
            
            <tag> 多尺度 </tag>
            
            <tag> 区域建议网络RPN </tag>
            
            <tag> 双流（噪声图） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告1</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/</url>
      
        <content type="html"><![CDATA[<h1 id="图像超分辨率的深度学习综述">图像超分辨率的深度学习：综述</h1><p><ahref="%5BDeep%20Learning%20for%20Image%20Super-Resolution:%20A%20Survey%20%7C%20IEEE%20Journals%20&amp;%20Magazine%20%7C%20IEEE%20Xplore%5D(https://ieeexplore.ieee.org/abstract/document/9044873)">TPAMI2020</a></p><p>本综述的主要贡献有三个方面：</p><p>​  1)我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。</p><p>​  2)我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。</p><p>​  3)我们讨论了这些挑战和开放的问题，并确定了新的趋势和未来的发展方向，为社区提供了一个深刻的指导。</p><p><strong><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240606164746361.png"alt="image-20240606164746361" /></strong></p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094212179.png"alt="image-20240607094212179" /><figcaption aria-hidden="true">image-20240607094212179</figcaption></figure><h1 id="选取网络">选取网络</h1><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/SR.drawio.png"alt="SR.drawio" /><figcaption aria-hidden="true">SR.drawio</figcaption></figure><p>​  SRCNN</p><p>​  SRResNet</p><p>​  VDSR</p><p>​  CARN</p><p>​  MemNet</p><h1 id="最新图像超分辨率">最新图像超分辨率</h1><p><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28194">AAAI2024</a></p><p>AdaFormer: Efficient Transformer with Adaptive Token Sparsificationfor Image Super-resolution</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094111731.png"alt="image-20240607094111731" /><figcaption aria-hidden="true">image-20240607094111731</figcaption></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<p>《基于深度学习的图像增强算法》——图像超分辨</p><p>基本要求：</p><ul><li><p>训练集T91-train,测试集Set5-test，不能更改。</p></li><li><p>需要分析网络架构不同所引起的性能变化，并在提交报告中对比分析。（可以是模型不同，也可以只是层数不同）</p></li><li><p>报告内容必须包含数据处理部分、模型部分、训练部分和测试部分。</p></li><li><p>超分辨任务b可以只选择放大倍数为4倍（横纵各4倍，图像大16倍）。</p></li><li><p>老师使用的模型：</p><ul><li><p>SRCNN:</p><ul><li><p>结构：</p><ul><li><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529181952910.png"alt="image-20240529181952910" /><figcaption aria-hidden="true">image-20240529181952910</figcaption></figure></li></ul></li><li><p>损失：nn.MSELoss</p></li><li><p>优化器：optim.Adam</p></li><li><p>训练批次：200轮<br/> <br/> - 评价指标：Peak Signal-to-Noise Ratio(PSNR)<br/> <br/> -<code>&lt;br/&gt;                def calc_psnr(img1, img2):&lt;br/&gt;                    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))&lt;br/&gt;</code></p></li></ul></li></ul></li><li><p>可以比较的模型：（完成意味着代码写完了，可以跑起来，但是效果需要针对训练集进行微调，延续老师代码的损失、优化器和学习率）</p><ul><li><p>初始模型SRCNN<br/> - 输入为16*1*76*76<br/> -输出为16*1*76*76<br/> - 来源于ResNet的模型SRResNet(完成，训练中)<br/> -输入为16*3*19*19<br/> - 输出为16*3*76*76<br/> - best epoch: 166, psnr:24.29<br/> - 来源于DenseNet的模型SRDenseNet <br/> - 基于递归的方法<br/>- DRCN<br/> - DRRN<br/> - CARN<br/> -VDSR(完成，但是在第三个epoch，损失就不在降低了，效果也不如SRCNN，所以需要针对训练集进行微调)</p></li><li><p>基于像素的方法：<br/> - 生成式对抗性网络（GANs）：<br/> -SRGAN<br/> - ESRGAN<br/> - 基于光流的方法</p></li></ul></li><li><p>可以使用的对比评价指标</p><ul><li><p>峰值信噪比（PSNR）：峰值信噪比（PSNR）是评价SISR重建质量最广泛使用的技术之一。它表示SR图像ˆy与实际图像y之间的最大像素值L与均方误差（MSE）之比。</p></li><li><p>结构相似度指数（SSIM）：SSIM和PSNR一样，是一种流行的评价方法，侧重于图像之间结构特征的差异。它通过比较亮度、对比度和结构来独立地捕获结构上的相似性。SSIM估计一个图像y的亮度µy为强度的平均值，而它估计对比度σy为其标准差。</p></li><li><p>平均意见评分（MOS）：MOS是一种主观的测量方法，利用人类的感知质量来评估生成的SR图像。人类观众会看到SR图像，并要求他们进行质量评分，然后映射到数值，然后取平均值。通常，这些范围从1（坏）到5（好），但可能有不同的[15]。虽然这种方法是对人类感知的直接评估，但与客观指标相比，进行它更耗时和麻烦。此外，由于这个度量标准的高度主观性，它很容易受到偏见的影响。</p></li></ul></li></ul><p>后面是学习一些综述</p><h1 id="超分辨率评价指标">超分辨率评价指标：</h1><p>​  图像质量评估（IQA）</p><p>​  许多特性与优秀的图像质量有关，如锐度，对比度，或没有噪声。因此，对SR模型的公平评价具有挑战性。本节展示了属于图像质量评估（IQA）范畴的不同评估方法。广义上说，IQA指的是任何基于对人类观众的感知评估的度量，即应用SR方法后图像的真实程度。IQA可以是主观的（例如，人类评分者）或客观的（例如，正式的指标）。</p><p>​  1)平均意见得分（MOS, Mean OpinionScore）：数字图像最终是为人类观看的。因此，评估图像的最合适的方法是主观评价[12]，[13]。一种常用的主观IQA方法是平均意见评分（MOS）。人类观众给有质量分数的图像打分，通常是1（差）到5（好）。MOS是所有评分的算术平均值。尽管具有可靠性，但调动人力资源是耗时和麻烦的，特别是对于大型数据集。</p><p>​  2)峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）：由于近年来产生的大量图像和主观测量的弱点，客观评估质量具有无可争辩的重要性。一种流行的目标质量测量方法是峰值信噪比（PSNR）。它是可能的最大像素值L（8位表示为255）与参考图像的均方误差（MSE）之间的比率。给定近似值$ y $ 和地面真实值y，PSNR是一个使用分贝尺度[dB]的对数量： <spanclass="math display">\[\mathrm{PSNR}\left(\mathbf{y},\mathbf{\hat{y}}\right)=10\cdot\log_{10}\frac{L^2}{\frac{1}{N_{\mathbf{y}}}\sum_{p\in\Omega_{\mathbf{y}}}\left[\mathbf{y}_p-\mathbf{\hat{y}}_p\right]^2}\]</span>​  虽然它被广泛用作SR模型的评价标准，但在真实场景中往往导致平庸的结果。它关注像素水平的差异，而不是哺乳动物的视觉感知，后者更吸引结构[14]。随后，它与主观感知质量的相关性较差。像素的轻微变化（例如，移动）可能会导致一个显著的PSNR降低，而人类几乎不知道这种差异。因此，新的指标关注于图像中更多的结构性特征。</p><h1 id="srnet">SRnet</h1><h2 id="简单的网络">简单的网络</h2><p>​  简单的网络是一种主要应用卷积链的体系结构。它们很容易理解，并且由于它们的大小，通常只使用最少的计算资源。大多数这些体系结构都可以在基于dl的SR的早期找到，因为它们的性能低于最先进的水平。此外，DL的“越深越好”的范式并不能很好地适用于简单的网络，因为正在消失/爆炸的梯度[98]。</p><p>​  图6显示了不同的简单网络设计。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182208483.png"alt="image-20240529182208483" /><figcaption aria-hidden="true">image-20240529182208483</figcaption></figure><p>​  第一个引入SR数据集的CNN是由Dong等人[39]提出的SRCNN（2014）。它使用双边缘预上采样来匹配地面真实空间大小（见第7.1节）。随后，它由三个卷积层组成，这遵循了图像恢复中流行的策略：补丁提取、非线性映射和重建。SRCNN的作者声称，应用更多的图层会损害性能，这与DL范式“越深越好”的[99]相矛盾。</p><p>​  如下所示，这个观察结果是错误的，需要更高级的构建块才能正确工作，例如，像VDSR[98]中那样的残差连接。</p><p>​  在他们的后续论文中，作者探索了各种加速SRCNN的方法，导致FSRCNN（2016）[33]利用了三个主要技巧：</p><p>​    首先，他们减少了卷积层内核的大小。</p><p>​    其次，他们使用了一个1x1的卷积层来增强和减少在使用3x3卷积的特征处理之前和之后的通道维度。</p><p>​    第三，他们采用了带有换位卷积的后上采样，这是提高速度的主要原因（见第7.1节）。</p><p>​  令人惊讶的是，它们在获得更快的同时优于SRCNN。</p><p>​  一年后，LapSRN（2017）[44]被提出，其关键贡献是一个拉普拉斯金字塔结构[100]，可以实现逐步上采样（见第7.1节）。它以粗分辨率的特征图作为输入，并预测高频残差，逐步细化每个金字塔层的SR重建。为此，在一个前馈通道中预测多尺度图像是可行的，从而促进了资源感知的应用程序。</p><p>​  简单的网络架构设计主要出现在基于dl的SR的早期，因为由于它们的大小，它们学习复杂结构的能力有限。最近，研究人员关注的是更有深度的网络，无论是残差网络，还是基于循环网络的合成深度。下面的部分将介绍这两种可能性。</p><h2 id="残差网络">残差网络</h2><p>​  残差的网络使用跳过连接来跳过图层。增加跳过连接的主要原因有两个：为了避免梯度的消失和降低精度饱和问题[99]。对于SR来说，引入跳过连接开启了深度构建模型的世界。其主要优点是深度架构用大的接受域替代卷积，这对于捕获重要特征至关重要。SRCNN的作者指出，“越深越好”的范式并不支持SR。相比之下，Kim等人用VDSR（2015）[98]驳斥了这一说法，并表明非常深的网络可以显著改善SR，如图7所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182809721.png"alt="image-20240529182809721" /><figcaption aria-hidden="true">image-20240529182809721</figcaption></figure><p>​  他们使用了来自其他DL方法的两种见解：首先，他们应用了一个著名的架构VGG-19[24]作为特征提取块。其次，他们使用了从插值层到最后一层的剩余连接。因此，VGG-19特征提取块在插值中添加了高频细节，导致目标分布呈正态分布，极大地降低了学习难度，如图8所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182849749.png"alt="image-20240529182849749" /><figcaption aria-hidden="true">image-20240529182849749</figcaption></figure><p>​  此外，由于插值中高频的稀疏表示，它减少了消失/爆炸梯度。这一优点产生了跟踪残余网络的趋势，从而增加了使用的残差的数量。</p><p>​  一个例子是RED-Net（2016）[101]，它将U-Net[102]架构适应SR。它结合了一个下采样编码器和一个上采样解码器网络，它对给定的输出进行下采样以提取特征，然后将特征映射上采样到目标空间大小。在此过程中，RED-Net利用在整个下采样过程中获得的剩余信息扩展了上采样操作，从而减少了消失的梯度。因此，它在几个缩放因子上都优于SRCNN。</p><p>​  另一个例子采用了SRGAN（2016）[13]的出版物中，即由多个残差单元组成的RenseNet[104]，它将残差信息发送给所有后来出现的卷积操作。此外，作者还比较了这些架构应用于像素和对抗性损失（见第3节）。SRResNet由多个堆叠的剩余单元组成，允许高级特征提取通过大量的求和操作访问低级特征信息。因此，它通过提供一个简单的反向传播路径来简化优化。与SRResNet一样，SRDenseNet[40]应用密集的残余块，它利用更多的残余连接来允许直接路径到更早的层。相比之下，SRResNet的性能大大优于SRCNN、DRCN和ESPCN。对SRDenseNet的一个扩展是在2018年提出的剩余密集网络[42]，它在密集块上包含了一个额外的剩余连接。</p><p>​  密集残差拉普拉斯网络（DRLN）[6]是SRDenseNet的扩展，是一种基于后上采样、通道注意的残差网络，并取得了最先进的竞争结果。每个密集块后面都有一个基于拉普拉斯金字塔注意的模块，它学习特征映射之间的层间和层内依赖关系。它在每个DRLM中逐步加权子频带特征，类似于HAN（连接不同深度的各种特征图）。</p><p>​  残差块的另一种变体是信息蒸馏网络（IDN）[43]。它使用剩余连接将特征映射的一部分积累到以后的层。给定六个卷积层，它将特征映射在中间分成两个部分。然后，其中一部分被最后三层进一步处理，并添加到输入部分和另一部分的连接中。简而言之，利用剩余连接的网络是最先进的。它们有效地传播信息的能力有助于对抗消失/爆炸的梯度，从而产生出色的性能。有时，剩余块的使用会与其他体系结构相结合，例如基于循环的网络。</p><h2 id="基于递归的网络">基于递归的网络</h2><p>​  人工深度可以通过重复来完成，其中接受野对于获取重要信息至关重要，通过重复相同的操作而扩大。此外，递归性减少了参数的数量，这有助于对抗小型设备的过拟合和内存消耗。它是通过不引入新的参数而多次应用卷积层来实现的。</p><p>​  Kim等人[105]通过DRCN（2015）引入了第一个基于循环的SR网络。它使用相同的卷积层多达16次，随后的重建层考虑所有递归输出进行最终估计。然而，他们观察到，他们的深度递归网络很难训练，但通过跳过连接和递归监督来缓解它，本质上是辅助训练。图9显示了DRCN。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529190806493.png"alt="image-20240529190806493" /><figcaption aria-hidden="true">image-20240529190806493</figcaption></figure><p>​  结合了DRCN [105]和VDSR[98]的核心思想，DRRN（2017）[65]在一个递归块结构中使用了几个堆叠的残差单元。此外，它使用的参数分别比VDSR和DRCN少6倍和14倍，同时获得了更好的结果。与DRCN相反，DRRN在剩余单元之间共享权值集，而不是在所有递归应用的卷积层中共享一个权值。通过强调多路径，DRNN比DRCN（总共52个）训练得更稳定，递归程度更深。</p><p>​  受DRCN的启发，Tai等人介绍了MemNet（2017）[92]。主要贡献是由递归单元和门单元组成的内存块，以挖掘持久内存。该递归单元被应用了多次，类似于DRCN。输出被连接并发送到一个栅极单元，这是一个简单的1x1卷积层。自适应门单元控制先前信息的量和保留的当前状态。图9显示MemNet。引入门对序列到序列任务（如LSTM[106]）的影响是开创性的，但深入了解其对SR任务的影响标志着一个开放的研究问题。</p><p>​  受DRRN的启发，DSRN（2018）[97]的作者探索了一种具有多路径网络的双态设计。它介绍了两种状态，一种在HR操作，另一种在LR空间，它们共同利用LR和HR信号。通过延迟反馈[107]，信号在lr到hr和hr到lr这两个空间之间反复交换。从lr到hr，它使用了一个转置的卷积层来进行上采样。HR-to-LR是通过分层卷积来执行的。最终的近似值使用了在人力资源空间中所做的所有估计值的平均值。因此，它应用了迭代上下上采样的扩展公式（见第7.1节）。这两种状态使用的参数多于DRRN，但小于DRCN。然而，适当地开发双态设计在未来需要进行更多的探索。</p><p>​  超分辨率反馈网络（SRFBN，2019）[64]也在使用反馈[108]。最基本的贡献是反馈块（FB）作为一个实际的循环细胞。FB使用多个具有密集跳跃连接的迭代上下采样来产生高水平的判别特征。SRFBN为每次迭代生成一个SR图像，并且FB块接收前一次迭代的输出。它尝试在每次迭代中为单个退化任务生成相同的SR图像。对于更复杂的案例，它通过课程学习通过每次迭代返回更好、更好质量的图像（见第6.1节）。与其他框架相比，SRFBN已经显示出了显著的改进，但在未来还需要更多的研究。</p><p>​  Liu等人提出了NLRN（2018）[109]，它提供了一个非局部模块来产生自相似性的特征相关性。图像中的每个位置测量其邻近区域的每个位置的特征相关性。NRLN利用特征相关消息之间的相邻循环阶段。事实上，NLRN的表现也略好于DRCN、DRCN和MemNet。</p><p>​  然而，最近对SR中rnn的主要研究是针对MISR进行的，如视频SR[110]或元学习[111]相关任务。一般来说，基于循环的网络在保存参数方面很有趣，但其主要缺点是通过重复应用相同的操作来实现其计算开销。此外，由于时间依赖性，它们不能并行化。替代方案是轻量级架构，接下来将介绍它们。</p><h2 id="轻量级网络">轻量级网络</h2><p>​  到目前为止，我们已经引入了能够提高SR图像质量的模型，以及一些尝试去做同样的事情，但计算量较少的模型。例如，FSRCNN[33]利用更小的内核大小、后采样和1x1卷积层来增强/减少通道维度，从而比SRCNN[39]更快（见7.2节）。本工作中的另一个例子是基于循环的网络，它减少了第7.4节中所述的冗余参数。这些精益递归网络的缺点是，参数的减少是以增加操作和推理时间为代价的，这是现实世界场景的一个基本方面。例如，移动设备上的SR受到电池容量的限制，这取决于所需的计算功率。因此，轻量级体系结构明确地同时关注执行速度和内存使用情况。补充材料，在线提供，包括参数比较，和执行速度的公平比较是受欢迎的需求。</p><p>​  MDSR（2017）[38]使用多路径方法来学习具有共享参数的多个缩放因子。它有三个不相同的路径作为预处理步骤和三个路径作为上采样。对于给定的比例因子s∈{2,3,4}，MDSR在三条路径之间选择确定性的。大尺度的路径比低尺度因子的路径建立得更深。在预处理和上采样步骤之间是一个由多个剩余块组成的共享模块。该特征提取块经过训练，常用于所有的缩放因子，如图10所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191245073.png"alt="image-20240529191245073" /><figcaption aria-hidden="true">image-20240529191245073</figcaption></figure><p>​  其主要优点是，一个模型就足以在多个尺度上进行训练，从而节省了参数和内存。相比之下，其他SR模型必须在不同的尺度上独立训练，并独立保存用于多尺度应用。然而，添加一个新的缩放因子需要从头开始进行训练。其他轻量级体系结构也采用了这一想法，以实现参数高效的多尺度训练，如CARN/CARNM（2018）[112]。</p><p>​  此外，它还在残差网络[103]上实现了一种级联机制。CARN由多个级联块（见图11）和它们之间的1x1卷积组成。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191409578.png"alt="image-20240529191409578" /><figcaption aria-hidden="true">image-20240529191409578</figcaption></figure><p>​  级联块的输出将被发送到所有后续的1x1卷积中，就像在级联块本身中一样。因此，局部级联几乎与全局级联完全相同。它允许多层次的表示和稳定的训练，如残余网络。最终，它在三条路径中进行选择，通过类似于MDSR的高效亚像素层，将特征映射上采样到2倍、3倍或4倍的缩放因子。受MobileNet[113]的启发，CARN还在每个残差块组件中使用分组卷积。这允许配置模型的效率，因为选择不同的组大小和由此产生的性能是在一种权衡关系中。具有组卷积的残差块根据组卷积的大小，最多可减少计算14倍。他们测试了CARN的一种变体，它设置了组的大小，从而使计算减少最大化，并将其称为CARN-Mobile（CARN-M）。此外，他们通过允许在每个级联块中的残余块的权重共享（与非共享块相比减少了3倍），进一步减少了CARN-M的参数。</p><p>​  受IDN和IMDB[115]的启发，RFDN（2020）[114]通过使用RFDB块重新考虑了IMDB体系结构，如图12所示。RFDB块由特征蒸馏连接组成，它们将1x1的卷积级联到最后一层。此外，它使用浅层残差块（SRBs），其中只有一个3x3的卷积，来进一步处理给定的输入。最后一层是一个1x1的卷积层，它结合了所有的中间结果。最后，它应用了专门为轻量级模型设计的增强型空间注意力[114]。RFDN架构包括后续的RFDB块，并使用具有最终亚像素层的后上采样框架。</p><p>​  XLSR（2021）[116]是一个非常具有硬件感知能力和量化友好性的网络。它应用多路径来减轻卷积操作的负担，并使用1x1卷积来按像素级进行组合。每个卷积层都有一个较小的滤波器尺寸（8、16、27)。在组合之后，它将分割特征贴图，并再次应用多条路径。XLSR的一个核心方面是末端激活层，它利用了量化的好处。量化是有用的，因为它可以通过使用更多的微型位表示[117]来保存参数。不幸的是，许多移动设备都支持8位数据。因此，对在浮32或浮16中表现良好的SR模型应用uint8量化不起作用。裁剪的ReLU（限制为最大值1）作为最后一个激活层而不是典型的ReLU可以消除这个问题。然而，作者建议通过进一步的实验来寻找其他的最大值。</p><p>​  一般来说，有很多想法可以让SR模型轻量级有待发现。它们包括对现有架构的简化、量化和修剪。此外，利用SR的资源有限的设备和应用是一个日益感兴趣的领域。</p><h2 id="小波变换网络">小波变换网络</h2><p>​  不同的图像表示可以带来一些好处，比如提高计算速度。小波理论为表示和存储多分辨率图像提供了稳定的数学基础，描述了上下文和纹理信息[119]。离散小波变换（DWT）将一幅图像分解为一系列小波系数。在SR中最常见的小波是Haar小波，通过二维快速小波变换计算出来。通过对每个输出系数进行迭代重复分解，计算出小波系数。它捕获四个子波段的图像细节：平均（LL）、垂直（HL）、水平（LH）和对角线（HH）信息。DWSR（2017）[120]是第一个使用小波预测的网络之一。它使用了一个简单的网络架构来细化在一个预上采样框架中的LR和HR图像小波分解之间的差异。首先，计算放大（采用双边插值）LR图像的小波系数。然后对小波系数进行卷积层处理。然后，加入初始计算的小波系数，并采用残差连接进行波面处理。因此，卷积层学习了系数的额外细节。最后，采用二维-DWT的反过程得到SR图像，如图13所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191746837.png"alt="image-20240529191746837" /><figcaption aria-hidden="true">image-20240529191746837</figcaption></figure><p>​  利用WIDN（2019）[121]提出了另一种方法，它使用平稳小波变换代替DWT来实现性能更好。与DWSR使用小波-srnet（2017）[122]的同时，我们提出了一个更复杂的模型。为了从LR图像中生成特征映射，它提供了一个由残余块组成的嵌入网络。然后，它多次应用小波变换，并利用多个小波预测网络。最后，它应用反向过程，并使用转置卷积进行上采样。该系数用于小波损失函数，而SR图像用于传统的纹理和MSE损失函数。因此，他们的网络适用于不同放大倍数的不同输入分辨率，并对MS-SR的未知高斯模糊、姿态和遮挡显示出鲁棒性。多级小波CNNS的思想也可以在以后的出版物中找到，即MWCNN（2018）[123]。</p><p>​  下面的工作应用了一种混合的方法，通过混合小波变换与其他著名的SR方法。即Zhang等人提出了一个基于小波的SRGAN（2019）框架[124]，它融合了SRGAN和小波分解的优点。生成器使用嵌入网络将输入处理到特征映射中，类似于小波-srnet。接下来，它使用一个小波预测网络来细化系数，类似于DWSR。2020年，Xue等人[125]将小波与包含通道注意和空间注意模块的残差注意块（混合注意，见下文第5节）相结合，并将其称为网络称为WRAN。在过去的几年里，小波的应用也应用于视频SR[126]。</p><p>​  一般来说，小波变换可以有效地表示图像。因此，使用这种策略的SR模型通常会降低总体模型的大小和计算成本，同时达到与最先进的架构相似的性能。然而，这一研究领域还需要更多的探索。例如，由于高频子带和低高频子带的分布，合适的归一化技术存在显著差异，或者由于高频子带的稀疏表示可能不合适，因此可以替代卷积操作。</p><h1 id="无监督超分辨率">无监督超分辨率</h1><p>​  监督SR的惊人性能归因于它们主要从许多LR-HR图像对学习自然图像的能力，大多是已知的退化映射，这在实践中通常是未知的。因此，经过监督训练的SR模型对于切实可行的用例有时是不可靠的。例如，当训练数据集生成LR图像（保留高频），然后SR模型在该数据集上进行的训练不太适合用于使用抗锯齿生成的真实LR图像（平滑图像）。使用抗混叠方法生成的LR图像（平滑图像）。此外，一些专门的应用领域缺乏LR-HR图像对数据集。因此，人们对无监督SR越来越感兴趣。我们简要地研究了这个领域，为了进一步阅读基于流的方法（退化核的密度估计），我们参考Liu等人[127]的调查。</p><h2 id="弱监督方法">弱监督方法</h2><p>​  弱监督方法使用未配对的LR和HR图像，如WESPE（2018）[128]。WESPE由两个发生器和两个鉴别器组成。第一个生成器获取一个LR图像并对其进行超级解析。第一发生器的输出构成一个SR图像，但也与电视损失[59]正则化。第二个生成器接受对第一个生成器的预测，并执行逆映射。第二个生成器的结果通过内容丢失[12]与原始输入的LR图像进行优化。这两个鉴别器取第一个生成器的SR图像，并被训练来区分预测和原始HR图像。第一鉴别器根据图像颜色将输入分类为SR或HR图像。第二个鉴别器使用图像纹理[61]来进行分类。</p><p>​  一个类似的方法是一个被称为CinCGAN（2018）[52]的周期中循环SR框架，基于CycleGAN[129]。它总共使用了四个发生器和两个鉴别器。第一个生成器取一个有噪声的LR图像，并将其映射到干净的版本。第一个鉴别器被训练来区分来自数据集的干净LR图像和预测的干净图像。第二台发电机训练逆函数。因此，它从预测的干净版本中生成有噪声图像，从而关闭了一个半周期周期的第一个周期。第三个生成器特别有趣，因为它是实际的SR模型，它将LR图像上采样到HR。第二个鉴别器被训练来区分预测的和数据集的HR图像。最后一个生成器将预测的HR图像映射到有噪声的LR图像，从而关闭CycleGAN的第二个周期。除了其有希望的结果和类似的方法[130]外，它还需要进一步的研究来降低学习难度和计算成本。</p><h2 id="零次学习">零次学习</h2><p>​  零次学习或一次学习与对物体的训练和对从未观察到的完全不同的物体的测试有关。理想情况下，如果将“斑马看起来像条纹马”转换为[132]马，那么用马训练的分类器应该识别斑马。关于SR的零次学习的第一个出版物是ZSSR（2017）[87]。我们的目标是只训练手头的一张图像，一张独一无二的图像。ZSSR对LR图像进行下采样，并训练CNN以反转退化映射。训练后的CNN最终直接用于LR图像。令人惊讶的是，该方法的效果优于SRCNN，与VDSR比较接近。</p><p>​  在此基础上，提出了一种基于深度信息[134]的退化仿真网络（DSN，2020）[133]，以避免预定义的退化内核。它使用双循环训练来同时学习未知的退化核和SR图像的重建。MZSR[135]将ZSSR设置与元学习合并，并使用一个外部数据集来学习不同的模糊内核，这被称为元学习领域中的任务分布。然后将SR模型在类似于ZSSR的降采样图像上进行训练，并从元测试阶段返回模糊核。这种方法的好处是，它使SR模型更快地学习特定信息，比纯ZSSR性能更好。SR的零镜头学习标志着进一步研究的一个令人兴奋的领域，因为它非常实用，特别是对于特定于应用程序的数据集很少或不存在的应用程序。</p><h2 id="深度图像先验">深度图像先验</h2><p>​  Ulyanov等人[131]提出了深度图像先验（DIP），这与在大数据集上训练CNN的传统范式相矛盾。它使用一个CNN来预测降采样时的LR图像，给定一些随机的噪声，而不是一个实际的图像。因此，它遵循了ZSSR的策略，只使用LR图像。然而，它将输入固定为随机噪声，并对预测采用固定的降采样方法进行修正。此外，它还优化了降采样预测与LR图像之间的差异。然后，CNN在不使用固定降采样方法的情况下生成SR图像。因此，它利用噪声生成一个SR图像，而不是转换一个原始图像。ZSSR和DIP之间的差异见图14。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529192743577.png"alt="image-20240529192743577" /><figcaption aria-hidden="true">image-20240529192743577</figcaption></figure><p>图14。零镜头超分辨率（ZSSR）[87]和深度图像先验（DIP）[131]。ZSSR使用LR图像进行降采样，SR模型学习反向降采样。对于LR图像的SR图像的最终预测，它直接应用于LR图像。DIP使用固定噪声作为输入，预测SR图像，并进行降采样，以优化降采样图像与给定LR图像之间的差异。最终的预测使用SR模型来预测SR图像，但跳过了退化映射。</p><p>​  令人惊讶的是，研究结果与LapSRN[136]很接近。不幸的是，它是一篇关于图像先验的理论出版物，而且正如作者自己所说的那样，这种方法太慢了，对大多数实际应用都不太有用。然而，它并不排除未来可以提高DIP关于更好的图像重建质量，特别是运行时的实用性的想法。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM1</title>
      <link href="/SAM1/"/>
      <url>/SAM1/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything Model for Medical Images?</p><p>发表于MICCAI 2024</p><p>Testing pipeline of SAM</p><figure><img src="./../postimages/SAM1/image-20240528220811746.png"alt="image-20240528220811746" /><figcaption aria-hidden="true">image-20240528220811746</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/yuhoo0302/Segment-Anything-Model-for-Medical-Images</span><br><span class="line">任务是医学图片的分割</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">优化器和损失函数设计：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"># Set up the optimizer, hyperparameter tuning will improve performance here</span><br><span class="line">optimizer = torch.optim.AdamW(sam_model.mask_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction=&#x27;mean&#x27;)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">为一个训练过程中的代码：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">outputs = []</span><br><span class="line"># do not compute gradients for image encoder and prompt encoder</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    none_grad_features = &#123;&quot;sparse&quot;: &#123;&#125;, &quot;dense&quot;: &#123;&#125;&#125;</span><br><span class="line">    for idx, image_record in enumerate(batched_input):</span><br><span class="line">        sparse_embeddings, dense_embeddings = model.prompt_encoder(</span><br><span class="line">                    points=None,</span><br><span class="line">                    boxes=image_record[&quot;box&quot;].to(device),</span><br><span class="line">                    masks=None,</span><br><span class="line">                )</span><br><span class="line">        none_grad_features[&quot;sparse&quot;][idx] = sparse_embeddings</span><br><span class="line">        none_grad_features[&quot;dense&quot;][idx] = dense_embeddings </span><br><span class="line"></span><br><span class="line">batched_loss = 0</span><br><span class="line">for id, im_record in enumerate(batched_input):</span><br><span class="line">    # low_res_masks.shape == (B, M, 256, 256) M is set to 1</span><br><span class="line">    low_res_masks, iou_predictions = model.mask_decoder(</span><br><span class="line">        image_embeddings=im_record[&quot;img_embed&quot;].unsqueeze(0).to(device), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        image_pe=model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        sparse_prompt_embeddings=none_grad_features[&quot;sparse&quot;][id], # (B, 2, 256) !!B = target num instead of batch size</span><br><span class="line">        dense_prompt_embeddings=none_grad_features[&quot;dense&quot;][id], # (B, 256, 64, 64) !!B = target num instead of batch size</span><br><span class="line">        multimask_output=False,</span><br><span class="line">    )</span><br><span class="line">    # upscale + eliminate padding + restore to ori size</span><br><span class="line">    masks = model.postprocess_masks(</span><br><span class="line">        low_res_masks,</span><br><span class="line">        input_size=tuple(im_record[&quot;size_before_pad&quot;]),</span><br><span class="line">        original_size=tuple(im_record[&quot;image_ori_size&quot;]),</span><br><span class="line">    )</span><br><span class="line">    outputs.append(&#123;</span><br><span class="line">        &quot;masks&quot;: masks,</span><br><span class="line">        &quot;iou_predictions&quot;: iou_predictions,</span><br><span class="line">        &quot;low_res_logits&quot;: low_res_masks,</span><br><span class="line">        &quot;gt2D&quot;: im_record[&quot;gt2D&quot;].to(device)</span><br><span class="line">    &#125;)</span><br><span class="line">    # first ele: 1, B, ori_H, ori_W</span><br><span class="line">    # second ele: 1, B, ori_H, ori_W</span><br><span class="line">    # considering the multi-object situation</span><br><span class="line">    batched_loss += criterion(masks.squeeze(1).unsqueeze(0), im_record[&quot;gt2D&quot;].to(device).unsqueeze(0)) </span><br><span class="line">loss = batched_loss / len(batched_input)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">epoch_loss += loss.item()</span><br></pre></td></tr></table></figure><p>Segment Anything in High Quality</p><p>发表于NeurIPS 2023</p><figure><img src="./../postimages/SAM1/image-20240528221000632.png"alt="image-20240528221000632" /><figcaption aria-hidden="true">image-20240528221000632</figcaption></figure><p>图3:HQ-SAM将HQ输出令牌和全局局部特征融合引入SAM，用于高质量掩模预测。为了保持SAM的零样本能力，轻量级HQ-Output-Token重用SAM的掩码解码器，并生成新的MLP层，用于执行具有融合HQ-Features的点向产品。在训练过程中，当我们固定预先训练的SAM的模型参数时，HQ-SAM中只有少数可学习的参数是可训练的。为了清晰起见，此处省略了提示编码器。误差校正简单地用作推理期间SAM的输出令牌和HQ输出令牌的预测logits之间的直接元素和。</p><p>损失函数设置</p><p>We supervise mask prediction of the new HQ-Output token with acombination of both BCE Loss and Dice Loss.</p><p>我们用BCE损失和Dice损失组合的联合损失监督新HQ输出token的掩码预测。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box、point、noise_mask</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/SysCV/SAM-HQ</span><br><span class="line">任务是SAM的高质量掩模预测问题</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">from utils.loss_mask import loss_masks</span><br><span class="line"></span><br><span class="line">net = MaskDecoderHQ(args.model_type)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">if input_type == &#x27;box&#x27;:</span><br><span class="line">dict_input[&#x27;boxes&#x27;] = labels_box[b_i:b_i+1]</span><br><span class="line">elif input_type == &#x27;point&#x27;:</span><br><span class="line">point_coords = labels_points[b_i:b_i+1]</span><br><span class="line">    dict_input[&#x27;point_coords&#x27;] = point_coords</span><br><span class="line">    dict_input[&#x27;point_labels&#x27;] = torch.ones(point_coords.shape[1], device=point_coords.device)[None,:]</span><br><span class="line">elif input_type == &#x27;noise_mask&#x27;:</span><br><span class="line">    dict_input[&#x27;mask_inputs&#x27;] = labels_noisemask[b_i:b_i+1]</span><br><span class="line">else:</span><br><span class="line">    raise NotImplementedError</span><br><span class="line">dict_input[&#x27;original_size&#x27;] = imgs[b_i].shape[:2]</span><br><span class="line">batched_input.append(dict_input)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">batched_output, interm_embeddings = sam(batched_input, multimask_output=False)</span><br><span class="line"></span><br><span class="line">batch_len = len(batched_output)</span><br><span class="line">encoder_embedding = torch.cat([batched_output[i_l][&#x27;encoder_embedding&#x27;] for i_l in range(batch_len)], dim=0)</span><br><span class="line">image_pe = [batched_output[i_l][&#x27;image_pe&#x27;] for i_l in range(batch_len)]</span><br><span class="line">sparse_embeddings = [batched_output[i_l][&#x27;sparse_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">dense_embeddings = [batched_output[i_l][&#x27;dense_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">masks_hq = net(</span><br><span class="line">    image_embeddings=encoder_embedding,</span><br><span class="line">    image_pe=image_pe,</span><br><span class="line">    sparse_prompt_embeddings=sparse_embeddings,</span><br><span class="line">    dense_prompt_embeddings=dense_embeddings,</span><br><span class="line">    multimask_output=False,</span><br><span class="line">    hq_token_only=True,</span><br><span class="line">    interm_embeddings=interm_embeddings,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_mask, loss_dice = loss_masks(masks_hq, labels/255.0, len(masks_hq))</span><br><span class="line">loss = loss_mask + loss_dice</span><br><span class="line"></span><br><span class="line">loss_dict = &#123;&quot;loss_mask&quot;: loss_mask, &quot;loss_dice&quot;:loss_dice&#125;</span><br><span class="line"></span><br><span class="line"># reduce losses over all GPUs for logging purposes</span><br><span class="line">loss_dict_reduced = misc.reduce_dict(loss_dict)</span><br><span class="line">losses_reduced_scaled = sum(loss_dict_reduced.values())</span><br><span class="line">loss_value = losses_reduced_scaled.item()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line">metric_logger.update(training_loss=loss_value, **loss_dict_reduced)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">loss_masks：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">def loss_masks(src_masks, target_masks, num_masks, oversample_ratio=3.0):</span><br><span class="line">    &quot;&quot;&quot;Compute the losses related to the masks: the focal loss and the dice loss.</span><br><span class="line">    targets dicts must contain the key &quot;masks&quot; containing a tensor of dim [nb_target_boxes, h, w]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # No need to upsample predictions as we are using normalized coordinates :)</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # sample point_coords</span><br><span class="line">        point_coords = get_uncertain_point_coords_with_randomness(</span><br><span class="line">            src_masks,</span><br><span class="line">            lambda logits: calculate_uncertainty(logits),</span><br><span class="line">            112 * 112,</span><br><span class="line">            oversample_ratio,</span><br><span class="line">            0.75,</span><br><span class="line">        )</span><br><span class="line">        # get gt labels</span><br><span class="line">        point_labels = point_sample(</span><br><span class="line">            target_masks,</span><br><span class="line">            point_coords,</span><br><span class="line">            align_corners=False,</span><br><span class="line">        ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    point_logits = point_sample(</span><br><span class="line">        src_masks,</span><br><span class="line">        point_coords,</span><br><span class="line">        align_corners=False,</span><br><span class="line">    ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    loss_mask = sigmoid_ce_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line">    loss_dice = dice_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line"></span><br><span class="line">    del src_masks</span><br><span class="line">    del target_masks</span><br><span class="line">    return loss_mask, loss_dice</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">get_uncertain_point_coords_with_randomness：</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">    Sample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The unceratinties</span><br><span class="line">        are calculated for each point using &#x27;uncertainty_func&#x27; function that takes point&#x27;s logit</span><br><span class="line">        prediction as input.</span><br><span class="line">    See PointRend paper for details.</span><br><span class="line">    Args:</span><br><span class="line">        coarse_logits (Tensor): A tensor of shape (N, C, Hmask, Wmask) or (N, 1, Hmask, Wmask) for</span><br><span class="line">            class-specific or class-agnostic prediction.</span><br><span class="line">        uncertainty_func: A function that takes a Tensor of shape (N, C, P) or (N, 1, P) that</span><br><span class="line">            contains logit predictions for P points and returns their uncertainties as a Tensor of</span><br><span class="line">            shape (N, 1, P).</span><br><span class="line">        num_points (int): The number of points P to sample.</span><br><span class="line">        oversample_ratio (int): Oversampling parameter.</span><br><span class="line">        importance_sample_ratio (float): Ratio of points that are sampled via importnace sampling.</span><br><span class="line">    Returns:</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) that contains the coordinates of P</span><br><span class="line">            sampled points.</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">point_sample：</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.</span><br><span class="line">    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside</span><br><span class="line">    [0, 1] x [0, 1] square.</span><br><span class="line">    Args:</span><br><span class="line">        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains</span><br><span class="line">        [0, 1] x [0, 1] normalized point coordinates.</span><br><span class="line">    Returns:</span><br><span class="line">        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains</span><br><span class="line">            features for points in `point_coords`. The features are obtained via bilinear</span><br><span class="line">            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">sigmoid_ce_loss_jit = torch.jit.script(</span><br><span class="line">    sigmoid_ce_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def sigmoid_ce_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    Returns:</span><br><span class="line">        Loss tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&quot;none&quot;)</span><br><span class="line"></span><br><span class="line">    return loss.mean(1).sum() / num_masks</span><br><span class="line">    </span><br><span class="line">dice_loss_jit = torch.jit.script(</span><br><span class="line">    dice_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def dice_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the DICE loss, similar to generalized IOU for masks</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    inputs = inputs.sigmoid()</span><br><span class="line">    inputs = inputs.flatten(1)</span><br><span class="line">    numerator = 2 * (inputs * targets).sum(-1)</span><br><span class="line">    denominator = inputs.sum(-1) + targets.sum(-1)</span><br><span class="line">    loss = 1 - (numerator + 1) / (denominator + 1)</span><br><span class="line">    return loss.sum() / num_masks</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCIG大会—1</title>
      <link href="/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/"/>
      <url>/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/</url>
      
        <content type="html"><![CDATA[<h1 id="csig-年度学科发展报告论坛">CSIG 年度学科发展报告论坛</h1><h2 id="报告题目分割一切模型综述">报告题目：分割一切模型综述</h2><p><strong>报告嘉宾：</strong>张军平，复旦大学计算机科学技术学院教授、博士生导师，中国自动化学会普及工作委员会主任。主要研究方向包括人工智能、机器学习、图像处理、生物认证、智能交通及气象预测。至今发表论文100 余篇，连续两年（2022、2023）入选全球前2%顶尖科学家榜单终身科学影响力排行榜。著有《人工智能极简史》《爱犯错的智能体》《高质量读研》，主编《人机混合增强智能》，译著《统计学习要素》（第二版）。</p><p><strong>报告摘要：</strong>Meta 公司提出的“分割一切模型”(SegmentAnything Model，简称SAM)于 2023 年在图像分割领域获得了优异的性能。在 SAM开源后不久，科研</p><p>人员提出了一系列改进的方法和应用。为了能全面深入了解分割一切模型的发展脉络，优势与不足，本报告将对SAM 的研究进展进行综述。我将先介绍分割一</p><p>切模型的背景和核心框架。在此基础上，综述相关改进方法，并探讨 SAM在图像处理、视频处理以及其他领域的应用。最后，对 SAM未来的发展方向和潜在</p><p>应用前景进行分析和讨论。</p><h3 id="sam背景">SAM背景</h3><ul><li>分割一切项目<br/> - 任务:提出新的提示分割任务范式。<br/> -模型:图像编码器、提示编码器、轻量级掩码解码器。<br/> -数据:提出新的数据引擎构建了SA-1B数据集。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528163704285.png"alt="image-20240528163704285" /><figcaption aria-hidden="true">image-20240528163704285</figcaption></figure><h3 id="sam的应用">SAM的应用</h3><h4 id="视频超分辨率">视频超分辨率</h4><ul><li>SEEM模块可以利用语义信息增强模型的特征对齐和融合能力。<br/> -具体来说，通过利用注意力机制和特征映射操作实现将SAM的表示与当前输入帧的特征相结合，然后生成语义感知的特征。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164011993.png"alt="image-20240528164011993" /><figcaption aria-hidden="true">image-20240528164011993</figcaption></figure><p>基于滑动窗口的超分辨率方法，引入SEEM改进了三个步骤：即对齐、融合和重建</p><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164206753.png"alt="image-20240528164206753" /><figcaption aria-hidden="true">image-20240528164206753</figcaption></figure><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164239992.png"alt="image-20240528164239992" /><figcaption aria-hidden="true">image-20240528164239992</figcaption></figure><p>在基于循环结构的超分辨率方法，我们将SEEM应用于基于双向递归结构的方法。“F”和“B”是向前和向后传播表示串联操作。</p><h4 id="视频目标追踪">视频目标追踪</h4><ul><li>TAM结合了分割模型SAM和高级视频对象分割模型多重记忆模型，这两个模型以交互的方式集成在一起。</li></ul><h4 id="总结">总结</h4><p>SAM的应用形式主要大致分为四类:</p><ol type="1"><li>在<strong>特定领域</strong>对SAM进行<strong>微调</strong><br/>2.使用<strong>SAM辅助其他领域</strong>原有的模型<br/>3.利用SAM构建其他特定领域的数据集<br/>4.使用生成提示模型自动生成提示来辅助SAM</li></ol><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/SAM(CCIG).drawio.png"alt="SAM(CCIG).drawio" /><figcaption aria-hidden="true">SAM(CCIG).drawio</figcaption></figure><h3 id="未来研究方法">未来研究方法</h3><ul><li>模块化</li><li>弱监督语义分割</li><li>多模态融合图像分割</li><li>对SAM进行高效率微调</li><li>格式塔心理学的整体认知观加强SAM的对抗鲁棒性</li></ul><p>综述论文</p><p><ahref="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=202311030000002">分割一切模型SAM的潜力与展望：综述-Thepotential and prospects of segement anything model: a survey(cjig.cn)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="第三章-经典参数估计">第三章—-经典参数估计</h1><p>如何评价估计器的好坏？</p><h2 id="两个重要的无偏一致估计">两个重要的无偏一致估计：</h2><h3 id="均值">均值：</h3><p><span class="math display">\[\hat \mu = \frac 1 N \sum _{i=1} ^Nx_i\]</span></p><h3 id="协方差矩阵">协方差矩阵：</h3><p>已知均值 <span class="math display">\[\hat R = \frac 1 N \sum _{i=1}^N (x_i -  \mu _x)(x_i -  \mu _x)^H\]</span> 未知均值 <spanclass="math display">\[\hat R = \frac 1 {N-1} \sum _{i=1} ^N (x_i-  \hat \mu _x)(x_i -  \hat \mu _x)^H\]</span></p><h2 id="均方误差mean-squared-error-mse">均方误差(mean squared error, MSE)</h2><p><span class="math display">\[E[||\hat A -A||^2]=E[e^He]=trE[e^He]=trM\]</span></p><p>$ M=E[ee^H] $ 称为均方误差矩阵，可对其进行进一步分解</p><p>令 $ b = _e $ : <span class="math display">\[R_e = E[(e - \mu _e)(e -\mu _e)^H] = M - bb^H\]</span> 因此 $ M = R_e + bb^H $，可视为协方差和偏差的加权，当估计器为无偏估计时，</p><p>最小方差估计器不一定是无偏的，因此存在<strong>最小均方误差估计</strong>和<strong>最小方差无偏估计</strong>两种不同的准则</p><h2 id="最小方差无偏估计mvdr">最小方差无偏估计MVDR</h2><p>对于$ y=Hx+n $ <em>现有问题</em>：</p><p><spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\\\\\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\\\\\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span></p><p>以下是例题：</p><p>例： $ y=Hx+n $，对于上述线性模型，在满足线性运算的条件下寻找最小方差无偏估计，结果称为minimumvariance distortionless response (MVDR) ，也称作best linear unbiasedestimator (BLUE)：</p><p>解：</p><p>假设线性条件： $ x = W^H y $</p><p>则无偏性条件： <span class="math display">\[E[\hat x] = E[W^H y]=E[W^H(Hx+n)]= E[W^HHx+W^Hn]= W^HHx+W^HE(n)=W^HHx=x\\W^HH=I\]</span>最小化方差： <span class="math display">\[E[||\hat x - x||^2]=trE[(\hatx - x)(\hat x - x)^H]\\=trR_{\hat x \hatx}=tr\{W^HR_{yy}W\}=tr\{W^HR_{nn}W\}\]</span>则原问题变成了MVDR优化<em>现有问题</em>： <spanclass="math display">\[\underset{W}{min} \ tr\{ W^HR_{nn}W \}\\s.t.W^HH=I\]</span> 首先构造拉格朗日函数： <spanclass="math display">\[\mathcal{L}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\sum_{i,j}\lambda_{ij}\left[\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]_{ji}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\mathrm{tr}\left[\mathbf{\Lambda}\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]\]</span>对W求偏导 <spanclass="math display">\[\begin{aligned}&amp;\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{R}_{mn}\mathbf{W})}{\partial\mathbf{W}}=2\mathbf{R}_m\mathbf{W}\\&amp;\frac{\partial(\mathbf{\Lambda}\mathbf{W}^T\mathbf{H})}{\partial\mathbf{W}}=\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{H}\mathbf{\Lambda})}{\partial\mathbf{W}}=\mathbf{H}\mathbf{\Lambda}\end{aligned}\quad\Rightarrow\quad\frac{\partial\mathcal{L}}{\partial\mathbf{W}}=2\mathbf{R}_{mn}\mathbf{W}-\mathbf{H}\mathbf{\Lambda}=0\quad\Rightarrow\quad\mathbf{W}=\frac12\mathbf{R}_{mn}^{-1}\mathbf{H}\mathbf{\Lambda}\]</span>由约束条件得： <spanclass="math display">\[\mathbf{W}^{T}\mathbf{H}=\frac{1}{2}\mathbf{\Lambda}^{T}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H}=\mathbf{I}\Rightarrow\mathbf{\Lambda}^{T}=2(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\]</span>解得 <spanclass="math display">\[\mathbf{W}^{T}=(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\]</span>对于复数情形 <spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\]</span>因此，MVDR估计为： <spanclass="math display">\[\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\]</span>MVDR估计的协方差矩阵 <spanclass="math display">\[\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span>例： $ y=sA+n $ ，其中s为已知实信号，A为待估计幅度，n为噪声，其协方差矩阵为 $ R_{nn} $，试设计MVDR估计并优化s,在满足总功率限制的条件下使得的估计误差最小化</p><p>解：首先将信号模型写成矩阵的形式 <spanclass="math display">\[y=sA+n\quad\Rightarrow\quady=Hx+n\\H=s,A=x\]</span>计算权值 <spanclass="math display">\[\mathbf{W}^T=(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{R}_{nn}^{-1}\\=(\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s})^{-1}\mathbf{s}^T\mathbf{R}_{nn}^{-1}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}\]</span>由此可得MVDR估计 <spanclass="math display">\[\hat{\mathbf{A}}=\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}y\]</span>要使得估计误差最小化，即最小化方差，即最小化MSE <spanclass="math display">\[\mathrm{tr}(\mathbf{R}_{\mathrm{xx}})=\mathrm{tr}\left[\left(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H}\right)^{-1}\right]=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\]</span>最小化方差等同于最大化分母 ，考虑 $ R_{nn} $ 的EVD分解 <spanclass="math display">\[\mathbf{R}_{\mathrm{nn}}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^T\]</span>同时将s表示为 $ R_{nn} $ 特征向量的加权 <spanclass="math display">\[s=V\alpha\]</span> 所以： <spanclass="math display">\[\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\Lambda}^{-1}\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\Lambda}^{-1}\mathbf{\alpha}=\sum_{i\operatorname{=}1}^N\frac{|\alpha_i|^2}{\lambda_i}\]</span>考虑功率约束 <spanclass="math display">\[\mathcal{E}=\sum_{k=1}^Ns_k^2=\parallel\mathbf{s}\parallel^2=\mathbf{s}^T\mathbf{s}\]</span>由于 $ s=V$ : <spanclass="math display">\[\mathbf{s}^T\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\alpha}=\parallel\mathbf{\alpha}\parallel^2=\sum_{i=1}^N\alpha_i^2\]</span>为最大化 $ <sup>T<em>{nn}^{-1} = </em>{i1}</sup>N $，应将能量分配到较小的 $ <em>i $ 上，因此最佳方案为<br /><span class="math display">\[\alpha_i^2=\begin{cases} 0,&amp;i=1,...,N-1 ,\\ \mathcal{E},&amp;i=N,\end{cases}\]</span> 此时 $ =</em>{}$，对应于 $ R_{nn} $ 最小特征根对应的特征向量</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理作业</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<h1id="在超密集网络中基于连接的定位crlb理论方差和mle">在超密集网络中基于连接的定位：CRLB，理论方差，和MLE</h1><h1 id="摘要">摘要</h1><p>​  超密集网络（UDNs, ultra-densenetworks）中基于连接的地理定位的性能分析是一项非常重要的任务。虽然已经对无距离定位进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。在本文中，我们首先推导了对无范围定位的性能评估的Cramer-Rao下界（CRLB）。文献中关于无距离定位的所有当前性能分析都用于评估给定算法的实际性能，而所提出的CRLB提供了评估任何无偏无距离定位算法性能的基准，并确定了无偏估计器的方差小于界的物理不可能性。</p><p>​  据我们所知，这是文献中第一次推导出用于无距离定位的CRLB。其次，推导了任意节点分布下基于质心定位的理论方差。与均匀节点分布下CL的现有理论方差相比，所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还给出了所提出的CRLB的特性和理论方差。最后，为了提高定位精度，提出了一种基于最大似然估计器的最优估计器。由于我们的算法有效地利用了空间节点分布的先验信息和连通性，因此所提出的方法比CL方法性能更好，并且可以渐近地获得CRLB。</p><h1 id="引言">引言</h1><p>​  随着网络设备数量的增加，超密集网络（UDN）系统中盲节点（BN）的位置估计近年来引起了[1]的广泛关注。无线位置作为一个重要的公共安全功能，在未来的无线通信系统中创造了许多潜在的应用，如位置敏感的计费、欺诈保护、人员/资产跟踪、车队管理、移动黄页、无线网络设计、无线电资源管理和智能交通系统[2]、[3]。</p><p>​  虽然全球导航卫星系统（GNSSs），如全球定位系统（GPSs），可以提供较高定位精度的定位服务，但其局限性，包括高功率消耗，在室外丰富散射场景和城市峡谷的性能下降，阻止GNSSs应用于复杂的城市和室内环境。</p><p>从广义上讲，无线通信系统中的定位技术可以分为两类：</p><p>​  (1)基于范围的定位方法</p><p>​  (2)无范围的定位方法（也称为基于连接的定位）。</p><p>​  几种基于距离的定位技术，包括到达时间（TOA）、到达时差（TDOA）、到达角度（AOA）、基于接收信号强度（RSS）的方法和混合定位方法，被用于无线定位。基于范围的定位首先利用承载角和绝对或相对距离建立BN和参考节点（RNs）之间的显式几何关系，这些距离是由AOA、TOA、RSS和TDOA测量值估计的。然后，根据几何模型可以得到BN的位置。由于其定位精度高，基于范围的定位方法已经在文献[4]-[25]中得到了广泛的研究。研究了视线（LOS）环境[4]-[9]下的封闭解和迭代算法。对于非视线（NLOS）传播，开发了几何约束条件[10]-[12]和机器学习理论[13]-[16]来减轻NLOS误差。这些研究大多是基于单一的测量路径。为了进一步提高定位精度，在[17]，[18]中提出了基于多天线阵列的多路径传播环境的AOA定位算法。此外，在文献[19]-[25]中，还推导出了许多精度的几何稀释度和Cramer-Rao下界（CRLBs）。</p><p>​  本文从CRLB和理论方差的角度分析了无范围定位的性能。本文还提出了基于最大似然估计量（MLE）的最优估计量。本文的主要贡献如下：</p><p>​  (1)本文推导了一个具有随机分布RNs的UDN中无范围定位的CRLB。虽然已经对无距离定位[26]、[27]、[31]进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。所有当前的性能分析[26]，[27]，[31]无距离定位用于评估给定算法的实际性能，而提出CRLB提供了一个基准来评估任何无偏的位置算法的性能和确定物理不可能的方差无偏的估计器小于绑定。据我们所知，这是文献中首次推导出无范围定位的CRLB。</p><p>​  (2)推导了具有任意节点分布的CL(centroid-basedlocalization)方法的理论方差。需要注意的是，[26]中CL方法的理论方差是针对均匀节点分布推导出的。所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还提供了所提出的CRLB的特征和理论方差。</p><p>​  (3)提出了一种基于MLE的最优估计器来提高定位精度。由于该算法有效地利用了空间节点分布的先验信息和连通性，因此该方法优于CL方法，并能渐近得到CRLB。</p><p>​  本文的组织结构如下。第二节给出了信号模型和一些基本的符号。在第三节中，本文首先推导了一个在随机分布的UDN中无距离定位的CRLB。然后，导出了任意节点分布下CL方法的理论方差。在本节的最后给出了所提出的CRLB和理论方差的一些特征。第四节提出了一种基于MLE的基于连通性信息和RN分布的迭代方法。第五节给出了所提出的CRLB的性能评价、理论方差和位置方法。本文的结论见第六节。</p><h1 id="系统模型">系统模型</h1><p>​  无距离定位的目的是利用BN和RNs之间的连接信息来定位BN。通常，CL算法包括两个阶段：监听和定位。在监听阶段，BN尝试监听并与RNs沟通。当接收到的信号功率超过检测阈值时，建立通信链路。在定位阶段，BN的位置近似为在其传输范围内所有RN的位置（RN的质心）位置的平均值。显然，CL的性能受到许多因素的影响，如节点密度和随机性、无线信道环境和位置方案。</p><p>​  无线信道环境对定位系统的性能起着非常重要的作用。这个通道环境决定了可以检测到多少和哪些RNs用于定位。传播模型通常是用来描述无线信道的情况，并预测在距离发射机的给定距离下的平均接收信号强度。虽然有几种传播模型[32]，[33]，本文选择了路径损失法向阴影模型，因为它被广泛应用于通信和定位应用，并已通过现场测量[32]得到证实。</p><p>​  假设（x，y）是待估计的BN的位置，并且N个RNs系统中第i个RN的已知坐标为（xi，yi），如果不失一般性，可以将BN的位置设为（0,0）。第i个RN和BN之间的真实距离可以建模为：<span class="math display">\[r_i = \sqrt{(x_i - x)^2 + (y_i -y)^2}\]</span>​  基于路径损失正态阴影模型，测量的RNi（dBm）的接收功率Pi可以视为对数正态变量[32]。因此，Pi和ri之间的关系变为：<span class="math display">\[P_i=P_0-10 \beta log_{10}(\frac {r_i}{r_0}) + n_i\]</span>​  式中，β为路径损失指数，表示路径损失随距离增加的速率；ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）；P0为参考距离r0处的参考功率，它取决于传输功率。一般来说，r0=1米。为简单起见，本文将r0设为1m。本文利用路径损失法向阴影模型推导出了无距离定位的CRLB和MLE。</p><p>​  节点的随机性是影响该定位方法性能的另一个主要因素。在文献中，基于不同的假设，提出了不同的节点分布。文献中首先提出了均匀分布，建立了一个节点分布模型，假设传感器节点均匀分布在半径为R[26]，[27]，[34]的圆盘中。然而，最近，人们已经认识到，均匀分布节点的假设对于实际部署的无线网络[35]，[36]是相当不可信的。事实上，节点的空间分布依赖于许多因素，如部署方法、节点的周围环境、节点的运动，甚至是通信协议。根据中心极限定理，实际节点位置将遵循高斯分布[35]，[36]。在这个模型中，根据二维高斯空间分布，协方差矩阵σp2i。一个RN位于（xi，yi）的概率可以用概率密度函数（PDF）[36]来描述：<span class="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span>​  需要注意的是，(3)是基于笛卡尔坐标系的。对于极坐标，PDF (3)可以写成：<span class="math display">\[f(r_i) = \frac {1} {\sigma _p ^2}exp(-\frac {r _i ^2} {2 \sigma _p ^2}) r_i\\f(\phi _i)= \frac {1} {2 \pi}\]</span> ​  其中ri是第i个RN和BN之间的范围。 $ _i = acos((x−x_i)/r_i) $是RNi相对于BN的方位角。式(4)表明，RN将在不同方向上以相同的概率出现，而f（xi，yi）只取决于RN与BN之间的距离。这也意味着靠近BN的RN可能比在更大距离的RN有更高的概率。</p><p>​  CL(centroid-based localization,基于质心的定位)是最简单的无范围定位方法，它只需要BN和相邻rN之间的二进制连接信息。CL算法基于以下假设[28]：</p><p>​  (1)有完美的球形无线电传播</p><p>​  (2)所有无线电具有相同的传输范围（功率）</p><p>​  (3)RN对称地分布在一个BN周围。</p><p>​  (4) CL仅基于从相邻RNs收集的连通性信息（单跳假设）。</p><p>​  假设（1-3）保证了CL算法是一个无偏估计量，第四个假设简化了定位过程。由于仿真和实验结果都证明了该模型在整洁环境[28]下非常符合户外无线电传播，因此本文也遵循了这些假设。BN定位于与RNs集合的连通性区域相交重合的区域，该区域由RNs[28]的质心定义： <span class="math display">\[(\hat x , \hat y)=(\frac 1M \sum _{i=1} ^ M x_i , \frac 1 M \sum _{i=1} ^ M y_i )\]</span>​  其中M≤N是实际参与定位过程的RNs的数量。从(5)可以看出，CL算法只对RN的坐标进行平均，用等权值估计BN的位置。为了进一步提高定位精度，文献中提出了几种加权CL（WCL）算法[37]-[48]。[37]中早期的WCL算法使用链路质量指示作为权值，并应用于基于zigbee的传感器网络中<span class="math display">\[(\hat x , \hat y)=(\frac {\sum _{i=1} ^ Mw_i x_i} {\sum _{i=1} ^ M w_i}  , \frac {\sum _{i=1} ^ M w_i y_i} {\sum_{i=1} ^ M w_i} )\]</span> ​  其中，权重wi是RSSPi的函数。由于现有的WCL算法需要RSS测量，这些都是基于范围的定位技术，超出了我们的研究范围。本文主要研究了仅使用单跳连接信息的无范围定位技术。</p><h1id="基于连接性的定位技术的理论分析">基于连接性的定位技术的理论分析</h1><p>​  本节推导出CRLB和理论方差来研究无距离定位技术的性能。CRLB对于参数估计非常重要，因为它提供了一个基准来评估任何无偏估计器的性能，而理论方差被用来评估给定算法的真实性能。</p><h2 id="crlb">CRLB</h2><p>​  对于无距离定位技术，BN的位置使用被检测的位置估计RN的位置。因此，测量向量为s= [x1，y1，···，xM，yM ] T，待估计的参数向量θ为[x，y] T。</p><p>​  假设PDF满足“规律性”条件： <spanclass="math display">\[E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right]=0\text{for all }\boldsymbol{\theta}\]</span>​  然后，将CRLB矩阵定义为Fisher信息矩阵（FIM）Jθ的逆： <spanclass="math display">\[E((\hat{\theta}-\theta)(\hat{\theta}-\theta)^T)\geq\mathbf{J}_\theta^{-1}\]</span>​  Fisher信息矩阵的确定为[49]： <spanclass="math display">\[\mathbf{J}_{\theta}=\begin{bmatrix}J_{xx}&amp;&amp;J_{xy}\\J_{xy}&amp;&amp;J_{yy}\end{bmatrix}=E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\left(\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right)^T\right]\]</span>​  其中 <spanclass="math display">\[f\left(\mathbf{s};\theta\right)=\prod_{i=1}^{M}f\left(x_{i},y_{i};\theta\right)\\f\left(x_{i},y_{i};\theta\right)=\frac{\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)}{\gamma}\]</span>​  f（xi，yi）描述了节点的空间分布概率，定义为(3)。 <spanclass="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span> $ (r_i) $ 为检测概率，表示在RNi处接收到的信号功率超过检测阈值Pth的概率。</p><p>γ是一个归一化常数： <spanclass="math display">\[\gamma=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)dx_{i}dy_{i}=\int\limits_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\]</span></p><p>因为 $ P_i=P_0-10 log_{10}( {r_0}) + n_i $,其中接收功率Pi、β为路径损失指数，表示路径损失随距离增加的速率、ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）、P0为参考距离r0处的参考功率，它取决于传输功率。<spanclass="math display">\[f\left(P_i\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_i-P_0+10\beta\log_{10}\left(r_i\right)\right)^2}{2\sigma^2}\right)\]</span>所以开始， $ (r_i) $ 可计算为： <spanclass="math display">\[\begin{aligned}&amp;\Phi\left(r_{i}\right)\\&amp;=p\left(P_{i}\geqP_{th}\right)=\int_{P_{th}}^{+\infty}f\left(P_{i}\right)dP_{i}\\&amp;=\int_{P_{th}}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_{i}-P_{0}+10\beta\log_{10}\left(r_{i}\right)\right)^{2}}{2\sigma^{2}}\right)dP_{i}\end{aligned}\]</span>使用替代方法，（14）可简化为： <spanclass="math display">\[\Phi\left(r_i\right)=\int_{\varphi\left(r_i\right)}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\nu^2}{2}\right)d\nu\]</span> 其中 <spanclass="math display">\[\varphi\left(r_i\right)=\begin{pmatrix}P_{th}-P_0+10\beta\log_{10}\left(r_i\right)\end{pmatrix}/\sigma\]</span> 和 $ (r_i) $ 可以使用MATLAB函数“erfc”来计算： <spanclass="math display">\[\Phi\left(r_i\right)=0.5erfc\left(\frac{\varphi\left(r_i\right)}{\sqrt{2}}\right)\]</span>方程（17）表明， $ (r_i) $只依赖于一个给定系统的距离ri。将（10）-（17）替换为(9)，如附录所示，基于连接的定位技术的CRLB为：<spanclass="math display">\[CRLB=tr\left\{\mathbf{J}_\theta^{-1}\right\}=\frac{4}{\bar{\psi}^2M}\]</span>其中 $ {}^2 $ ： <spanclass="math display">\[\begin{aligned}\bar{\psi}^{2}&amp; =\frac{1}{\gamma}\int_{0}^{+\infty}\left(\frac{1}{\Phi\left(r\right)}\frac{b}{r}\exp\left(-\frac{\varphi\left(r\right)^{2}}{2}\right)+\frac{r}{\sigma_{p}^{2}}\right)^{2}\times\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\\&amp; b = 10\beta/\left(\ln10\sqrt{2\pi}\sigma\right)\end{aligned}\]</span> 由于M是检测BN的RNs的数量，并由RNsN的总数和信道传输模型(2)决定，因此在每个定位过程中可能会发生变化。为了评估平均性能，将平均CRLB定义为<span class="math display">\[CRLB_{average}=E[CRLB]=E\left[\frac{4}{\bar{\psi}^{2}M}\right]=\frac{4}{\bar{\psi}^{2}}E\left[\frac{1}{M}\right]\]</span>显然，数值计算可以直接用于计算E[1/M]。为了进行进一步的性能分析，本文提出了一种分析方法。当M足够大时，期望均值可以替换为样本均值[34]，平均CRLB近似为：<spanclass="math display">\[CRLB_{average}\approx\frac{4}{\bar{\psi}^{2}}\frac{1}{\bar{M}}\]</span>其中 <span class="math display">\[\begin{aligned}\bar{M}&amp; =E [M]=E[N\Phi (r)]\\&amp;=NE\left[\Phi\left(r\right)\right]=N\int_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\\&amp;=N\int_{0}^{+\infty}\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\end{aligned}\]</span></p><h2id="具有任意节点分布的质心定位的理论方差">具有任意节点分布的质心定位的理论方差</h2><p>​  类似于CRLB，它提供了一个基准来评估任何无偏定位算法的性能，理论方差对于性能分析非常重要，因为它被用来评估给定算法的真实性能。虽然对于均匀节点分布[26]已经提出了CL算法的理论方差，但任意节点分布的情况仍然是一个有待解决的问题。本小节推导了具有任意节点分布的CL算法的理论方差。理论方差的定义为：<spanclass="math display">\[\begin{aligned}\operatorname{cov}(\theta)&amp;=tr\left\{E\left(\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)^{T}\right)\right\}\\&amp;=E\left(\left(\widehat{x}-x\right)^{2}+\left(\widehat{y}-y\right)^{2}\right)\end{aligned}\]</span>​  将CL估计值(5)替换入（24），得到： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}+\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)\]</span>​  上式中的第一项可以写成： <spanclass="math display">\[\begin{aligned}&amp;E\left(\left({\frac{1}{M}}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)\\&amp;=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}\left(x_{i}-x\right)\right)^{2}\right)\\&amp;\left.=E\left(\frac{1}{M^{2}}\left(\sum_{i=1}^{M}(x_{i}-x)^{2}+\sum_{i=1}^{M}\sum_{i=1}^{M}(x_{i}-x)\left(x_{j}-x\right)\right)\right)\right)\end{aligned}\]</span>​  注意，在i 不等于j的情况下，xi和xj是独立的，它可以从RN节点分布(3)中得到E（xi−x）=0。因此，（26）可以简化为： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(x_{i}-x\right)^{2}\right)\]</span>​  同理： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(y_{i}-y\right)^{2}\right)\]</span>​  将（27）和（28）代入（25），CL算法的理论方差可计算为： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}r_{i}^{2}\right)=E\left(\frac{1}{M}\right)\bar{r}^{2}\approx\frac{\bar{r}^{2}}{\bar{M}}\]</span>​  类似于（22），E [1/M]可以使用数值方法或近似方法（23）来求解。</p><p>​  ¯r2可计算为： <spanclass="math display">\[\bar{r}^{2}=\frac{1}{\gamma}\int_{0}^{+\infty}r^{2}\Phi\left(r\right)f\left(r\right)dr\]</span>​  其中f (r)为rN和BN之间范围的PDF。使用不同的f(r)，可以使用（29）和（30）计算具有任意节点分布的CL的理论方差。</p><p>​  对于高斯节点分布，f (r)可以从(4)得到： <spanclass="math display">\[f\left(r\right)=\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)r\quad0&lt;r\]</span>​  对于均匀的节点分布，f (r)为： <spanclass="math display">\[f\left(r\right)=\frac{2r}{R^{2}}\quad0&lt;r&lt;R\]</span>​  其中，R为均匀节点分布的分布半径。</p><h2 id="所提出的crlb的特征和理论方差">所提出的CRLB的特征和理论方差</h2><p>所提出的CRLB的特征和理论方差。</p><p>​  命题1：对于高接收功率，所提出的CRLB可以近似为： <spanclass="math display">\[CRLB_H\approx\frac{2\sigma_p^2}N\]</span>​  备注1：命题1表明，在高信噪比（SNR）情况下，无距离方法的性能主要取决于信N的节点分布，而不是信道环境。这种现象的发生是因为所有的rn都可以连接到一个具有良好信道环境的BN。</p><p>​  此外，下面还提出了密度λ对所提出的CRLB的影响</p><p>​  命题2：在高接收功率下，所提出的CRLB可以用密度λ近似为： <spanclass="math display">\[CRLB_H\approx\frac{1}{5.95\pi\lambda}\]</span>​  命题2表明，CRLB与λ呈负相关。这意味着一个更密集的网络将导致更高的定位精度。因此，无范围的方法是UDN的首选解决方案。以下命题提供了CL的实际性能与所提出的CRLB之间的关系。</p><p>​  命题3：在高斯节点分布和高接收功率的情况下，基于质心的定位的理论方差等于CRLB：<spanclass="math display">\[\operatorname{cov}(\theta)_H=CRLB_H=\frac{2\sigma_p^2}{N}\]</span>​  备注2：由于理论方差代表了CL方法的实际性能，因此从命题3中可以确定，在高信噪比的情况下，CL方法可以达到CRLB。这可以解释为这样一个事实，即在高信噪比的情况下，几乎所有的rN都可以与BN通信。因此，对于8(r)→1和γ→1，联合PDF（11）简化为高斯节点分布(3)。对于高斯PDF(3)，质心估计(5)是一个MLE。众所周知，MLE是渐近无偏的，并且可以渐近地获得具有明显的大测量值的CRLB。它是渐近有效的和最优的[49]。因此，CL在高信噪比的情况下具有最佳的性能。命题3也证明了所提出的CRLB的有效性。对于一个更实用的信噪比变化的信道，下一节提出了一种基于MLE的新的定位方法来提高性能。</p><p>基于</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>面对AIGC的多功能数字水印与版权保护研究(5月22日讲座)</title>
      <link href="/20240522%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240522%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<h1id="题目面对aigc的多功能数字水印与版权保护研究">题目：面对AIGC的多功能数字水印与版权保护研究</h1><p>讲师：张建</p><h2 id="图像重建">图像重建</h2><ol type="1"><li>Zero-Shot Image Restoration Using Denoising Diffusion Null-SpaceModel</li></ol><p>​  大多数现有的图像恢复（IR）模型都是特定于任务的，不能推广到不同的退化算子。在这项工作中，我们提出了去噪扩散零空间模型（DDNM,Denoising Diffusion Null-SpaceModel），这是一种新的零样本框架，用于解决任意线性IR问题，包括但不限于图像超分辨率、着色、修复、压缩感知和去模糊。DDNM只需要一个预先训练的离架扩散模型作为生成先验，而不需要任何额外的训练或网络修改。通过在反向扩散过程中仅细化零空间内容，我们可以产生满足数据一致性和真实性的不同结果。我们进一步提出了一个增强和稳健的版本，称为DDNM+，以支持噪声恢复并提高硬任务的恢复质量。我们在几个红外任务上的实验表明，DDNM优于其他最先进的零样本红外方法。我们还证明了DDNM+可以解决复杂的现实世界应用，例如旧照片恢复。</p><h2 id="图像条件生成">图像条件生成</h2><ol start="2" type="1"><li>FreeDoM: Training-Free Energy-Guided Conditional DiffusionModel</li></ol><p>​  最近，条件扩散模型由于其卓越的生成能力而在许多应用中受到欢迎。然而，许多现有的方法都是需要培训的。他们需要训练一个与时间相关的分类器或与条件相关的分数估计器，这增加了构建条件扩散模型的成本，并且不方便在不同条件下转移。目前的一些工作旨在通过提出无训练的解决方案来克服这一限制，但大多数只能应用于特定类别的任务，而不能应用于更一般的条件。在这项工作中，我们提出了一种用于各种条件的训练自由条件扩散模型（FreeDoM）。具体来说，我们利用现成的预训练网络，如人脸检测模型，来构建与时间无关的能量函数，该函数在不需要训练的情况下指导生成过程。此外，由于能量函数的构造非常灵活，能够适应各种条件，因此我们提出的FreeDoM比现有的无训练方法具有更广泛的应用范围。FreeDoM的优势在于其简单、有效和低成本。实验表明，FreeDoM在各种条件下都是有效的，适用于不同数据域的扩散模型，包括图像域和潜在代码域。</p><h2 id="图像精准控制生成">图像精准控制生成</h2><ol start="3" type="1"><li>T2I-Adapter: Learning Adapters to Dig Out More Controllable Abilityfor Text-to-Image Diffusion Models</li></ol><p>​  大规模文本到图像（T2I,text-to-image）模型令人难以置信的生成能力已经证明了学习复杂结构和有意义语义的强大能力。然而，仅仅依靠文本提示并不能充分利用模型所学到的知识，尤其是在需要灵活准确的控制（如结构和颜色）时。在本文中，我们的目标是“挖掘”T2I模型隐式学习的能力，然后显式地使用它们来更细粒度地控制生成。具体而言，我们建议学习低成本的T2I适配器，以使T2I模型中的内部知识与外部控制信号相一致，同时冻结原始的大型T2I模型。这样，我们可以根据不同的条件训练各种适配器，从而在生成结果的颜色和结构上实现丰富的控制和编辑效果。此外，所提出的T2I适配器具有可组合性和泛化能力等有吸引力的实用价值。大量实验表明，我们的T2I转换器具有良好的生成质量和广泛的应用。我们的代码可在https://github.com/TencentARC/T2I-Adapter.</p><ol start="4" type="1"><li>DragonDiffusion: Enabling Drag-style Manipulation on DiffusionModels</li></ol><p>​  尽管现有的大规模文本到图像（T2I）模型能够从详细的文本描述中生成高质量的图像，但它们往往缺乏精确编辑生成的或真实图像的能力。在本文中，我们提出了一种新的图像编辑方法，DragonDiffusion，可以在Diffusion模型上进行Drag风格的操作。具体来说，我们基于扩散模型中中间特征的强对应性来构建分类器引导。它可以通过特征对应损失将编辑信号转换为梯度，以修改扩散模型的中间表示。基于这种制导策略，我们还构建了一个多尺度制导，同时考虑语义和几何对齐。此外，增加了跨分支的自关注，以保持原始图像和编辑结果之间的一致性。我们的方法通过高效的设计，实现了对生成或真实图像的各种编辑模式，如对象移动、对象大小调整、对象外观替换和内容拖动。值得注意的是，所有编辑和内容保存信号都来自图像本身，并且该模型不需要微调或附加模块。我们的源代码将在这个httpsURL上提供。</p><ol start="5" type="1"><li>DiffEditor: Boosting Accuracy and Flexibility on Diffusion-basedImage Editing</li></ol><p>图像视频隐写</p><ol type="1"><li>Robust Invertible Image Steganography</li></ol><p>​  图像隐写术旨在将秘密图像隐藏到容器图像中，在容器图像中隐藏秘密，并在必要时进行恢复。以前的图像隐写方法在隐藏能力和鲁棒性方面受到限制，通常容易受到容器图像失真的影响，如高斯噪声、泊松噪声和有损压缩。本文提出了一种新的基于流的鲁棒可逆图像隐写框架，称为RIIS。我们引入了条件归一化流，以容器图像为条件对冗余高频分量的分布进行建模。此外，精心设计的容器增强模块（CEM）也有助于稳健的重建。为了调节不同失真水平的网络参数，我们提出了一种基于流的块上的失真引导调制（DGM），使其成为一个一刀切的模型。在干净和失真图像隐写方面，大量实验表明，所提出的RIIS有效地提高了鲁棒性，同时保持了不可见性和容量。据我们所知，我们是文献中第一个增强图像隐写术鲁棒性的基于学习的方案。隐写术鲁棒性的保证大大拓宽了隐写术在现实应用中的应用。</p><ol start="2" type="1"><li>Large-Capacity and Flexible Video Steganography via InvertibleNeural Network</li></ol><p>​  视频隐写术是一种在封面视频中不引人注目地隐藏秘密数据，然后在接收器端通过解码协议恢复秘密数据的技术。尽管已经进行了几次尝试，但大多数都局限于低容量和固定的隐写术。为了弥补这些不足，本文提出了一种大容量、灵活的视频隐写网络（LF-VSN）。对于大容量，我们提出了一种可逆管道，通过单个可逆神经网络（INN）来执行多个视频的隐藏和恢复。我们的方法可以在1个封面视频中隐藏/恢复7个秘密视频，性能良好。为了灵活性，我们提出了一种密钥可控方案，使不同的接收器能够通过特定的密钥从同一封面视频中恢复特定的秘密视频。此外，我们通过提出一种可扩展的多视频隐藏策略，进一步提高了灵活性，该策略可以用单个模型和单个训练会话在封面视频中隐藏可变数量的秘密视频。大量实验表明，随着视频隐写性能的显著提高，我们提出的LF-VSN具有高安全性、大隐藏容量和灵活性。源代码位于https://github.com/MC-E/LF-VSN.</p><h2 id="定制化溯源水印">定制化溯源水印</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM</title>
      <link href="/SAM/"/>
      <url>/SAM/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything <ahref="https://github.com/facebookresearch/segment-anything"><imgsrc="https://img.shields.io/github/stars/facebookresearch/segment-anything?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><div class="row">    <embed src="/postpdfs/SAM/2304.02643v1.pdf" width="100%" height="550" type="application/pdf"></div></details><h1 id="摘要">摘要</h1><p>​  我们介绍了分段任意事物（SA, SegmentAnything）项目：一个新的图像分割任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们建立了迄今为止（迄今为止）最大的分割数据集，在11M许可和尊重隐私的图像上有超过10亿个面具。该模型的设计和训练是及时的，因此它可以转移零镜头到新的图像分布和任务。我们评估了它在许多任务上的能力，发现它的零样本性能令人印象深刻——通常与之前的完全监督结果竞争，甚至更好。</p><h1 id="segment-anything-model">Segment Anything Model</h1><p>​  接下来，我们将描述用于快速分割的分段任何东西模型（SAM, SegmentAnything Model）。</p><figure><img src="./../postimages/SAM/image-20240519201116761.png"alt="image-20240519201116761" /><figcaption aria-hidden="true">image-20240519201116761</figcaption></figure><p><img src="./../postimages/SAM/image-20240519194006526.png"alt="image-20240519194006526" />图4：分段任何东西模型（SAM)概述。重量级图像编码器输出图像嵌入，然后可以通过各种输入提示有效地查询，以平摊的实时速度产生对象掩模。对于对应于多个对象的模糊提示，SAM可以输出多个有效的掩码和相关的置信度分数。</p><p>​  SAM有三个组件，如图4所示：图像编码器、灵活的提示编码器和快速掩码解码器。我们建立在转换视觉模型[14,33,20,62]上，对（摊销）实时性能进行特定的权衡。我们在这里高级描述这些组件，在a中详细说明。</p><h2 id="图像编码器">图像编码器</h2><p>​  一般来说，图像编码器可以是任何输出C×H×W图像嵌入的网络。基于可伸缩性和强大的预训练，我们使用MAE[47]预训练视觉transformer（ViT）[33]，具有最小的适应来处理高分辨率输入，特别是ViT-H/16，有14×14窗口注意和4个等间隔的[62]块。图像编码器的输出是输入图像的16×缩小嵌入。由于我们的运行时目标是实时处理每个提示，因此我们可以提供大量的图像编码器片段，因为它们每幅图像只计算一次，而不是每个提示计算一次。根据标准的实践（例如，[40]），我们使用了1024×1024的输入分辨率，这是通过重新缩放图像和填充较短的边而获得的。因此，图像嵌入值为64×64。为了减少信道维度，在[62]之后，我们使用1×1卷积得到256个通道，然后使用3×3卷积得到256个通道。每个卷积之后都是一个层的归一化[4]。</p><h2 id="提示编码器">提示编码器</h2><p>稀疏提示被映射到256维的向量嵌入如下。</p><p>​  一个点被表示为该点的位置的位置编码[95]和两个学习嵌入之一的总和，这表明该点是在前景中还是在背景中。</p><p>​  盒子由嵌入对表示：(1)其左上角的位置编码与表示“左上角”的学习嵌入求和，(2)相同的结构，但使用学习嵌入表示“右下角”。最后，为了表示自由形式的文本，我们使用CLIP[82]的文本编码器（任何文本编码器都是可能的）。我们将在本部分的其余部分中关注几何提示，并在D.5中深入讨论文本提示。</p><p>​  密集的提示（即掩码）与图像具有空间对应关系。我们以比输入图像低4×的分辨率输入掩模，然后使用两个2×2，步幅-2卷积分别与输出通道4和16缩小额外的4×。最后的1×1卷积将通道维度映射到256。每一层通过GELU激活[50]和层归一化分开。然后，将按元素的方式添加图像嵌入和掩码。如果没有掩码提示，则在每个图像嵌入位置添加一个表示“无掩码”的学习嵌入。</p><h2 id="轻量级掩码器">轻量级掩码器</h2><p>​  该模块有效地将图像嵌入和一组提示嵌入映射到一个输出掩码。为了结合这些输入，我们从transformer分割模型[14,20]中获得灵感，并修改了一个标准的transformer解码器[103]。在应用我们的解码器之前，我们首先在提示嵌入集中嵌入一个学习到的输出tokens嵌入，该嵌入将用于解码器的输出，类似于[33]中的[类]tokens。为简单起见，我们将这些嵌入（不包括图像嵌入）统称为“标记”。</p><p>​  我们的解码器设计如图14所示。</p><figure><img src="./../postimages/SAM/image-20240519195617684.png"alt="image-20240519195617684" /><figcaption aria-hidden="true">image-20240519195617684</figcaption></figure><p>图14：轻量级掩码解码器的细节。一个两层解码器通过交叉注意来更新图像嵌入和提示标记。然后对图像嵌入进行升级，利用更新后的输出标记来动态预测掩模。（为了图形清晰度，没有说明：在每个注意层，位置编码被添加到图像嵌入中，整个原始提示tokens（包括位置编码）被重新添加到tokens查询和键中。）</p><p>​  每个解码器层执行4个步骤：(1)对标记的自我注意，(2)从标记（作为查询）交叉注意到图像嵌入，(3)点级MLP更新每个标记，以及(4)从图像嵌入（作为查询）交叉注意到标记。这最后一步将使用提示信息更新图像嵌入。在交叉注意过程中，将图像嵌入视为一组64225个6维向量。每个自我/交叉注意和MLP都有一个残差连接[49]、层归一化和退出[93]为0.1。下一个解码器层从上一层中获取更新的tokens和更新的图像嵌入。我们使用了一个两层解码器。</p><p>​  为了确保解码器能够访问关键的几何信息，当位置编码参与注意层时，它们将被添加到图像嵌入中。此外，整个原始提示标记（包括它们的位置编码）都会被重新添加到更新后的标记中。这允许强烈地依赖于提示tokens的几何位置和类型。</p><p>​  在运行解码器后，我们用两个转置卷积对更新后的图像嵌入上采样，即4×（现在它相对于输入图像缩小了4×）。然后，tokens再次关注图像嵌入，我们将更新后的输出tokens嵌入传递给一个小的3层MLP，该MLP输出一个与升级图像嵌入的通道维数相匹配的向量。最后，我们预测了一个在升级的图像嵌入和MLP的输出之间具有空间点级乘积的掩模。</p><p>​  该transformer使用的嵌入尺寸为256。TransformerMLP块有一个很大的内部尺寸为2048，但MLP只应用于有相对较少（很少大于20）的提示tokens。在交叉注意层中，我们有一个64×64的图像嵌入，为了提高计算效率，我们将查询、键和值的通道维数降低两倍到128。所有的注意力层都使用8个头。用于升级输出图像嵌入的转置卷积为2×2，步幅为2，输出通道尺寸分别为64和32，并具有GELU激活。它们被层归一化分开。</p><h1 id="sam-adapter">SAM-Adapter</h1><p>​  《SAM Fails to Segment Anything? – SAM-Adapter: Adapting SAM inUnderperformed Scenes: Camouflage, Shadow, Medical ImageSegmentation,and More》<ahref="https://tianrun-chen.github.io/SAM-Adaptor/"><imgsrc="https://img.shields.io/badge/project-blue" alt="project" /></a> <ahref="https://arxiv.org/abs/2304.09148"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/tianrun-chen/SAM-Adapter-PyTorch"><imgsrc="https://img.shields.io/github/stars/tianrun-chen/SAM-Adapter-PyTorch?style=flat"alt="GitHub" /></a></p><p>​  该文指出号称可以“分割一切”的SAM模型虽然在自然图像的通用分割任务中取得了优异的效果，但在许多特殊图像的特定分割任务上表现差强人意，如水下目标分割、阴影分割、伪装对象分割等。作者认为这是由于SAM主要在常见的自然图像中进行训练，其特征提取器不能很好的适应特殊图像。因此作者提出一种轻量化的适配器模块（Adaptor），对SAM的编码器得到的特征图进行适应性调整。编码器的输入为特定任务信息，该文采用了图块嵌入特征和高频成分特征，将两种特征相加后经过两个MLP层得到适配器模块的输出，并将该输出与对应SAM编码器的Transformer层输出相加，并传递至下一层。训练过程中SAM编码器的参数保持不变，解码器部分使用SAM的参数进行初始化，然后利用特定数据集进行微调。</p><figure><img src="./../postimages/SAM/image-20240519202804593.png"alt="image-20240519202804593" /><figcaption aria-hidden="true">image-20240519202804593</figcaption></figure><p>​  如上图所示，该模型使用了SAM的图像编码器和掩模解码器，其中图像编码器冻结了参数，解码器是参与梯度回传的。这样可以有效利用SAM已经预训练好的分割能力，同时解码器更新参数以改装下游任务。另外引入了Adaptor模块，用于引入特殊任务的知识，辅助适配器模型。Adaptor的网络结构由两层MLP层构成，其输入的知识可以是微处理器的，对于文中的任务，其输入可以是纹理信息或者是频率信息等。各种信息用下面的权重来均衡。</p><blockquote><p>该文提出的Adaptor模块包括所使用的两个特定任务信息——图块嵌入特征和高频成分特征，都是来源于另一篇论文《ExplicitVisual Prompting for Low-Level StructureSegmentations》（EVP）。图块嵌入特征就是将图片划分成若干个图块，利用ViT将其映射为一个$ C_{seg} $维的特征；高频成分特征，则是将图片进行快速傅里叶变换，并保留其中的高频成分，再进行反变换得到高频成分对应的时域图，最后经过一个线性映射层得到一个特征向量。</p></blockquote><p>​  实验表明，在多个任务中SAM-Adapter均取得了远超SAM的表现，甚至由于各自领域的其他优秀算法，作为SAM的一种改进思路还是有值得借鉴和学习的地方。然而，整篇论文的思路几乎完全照搬了EVP，只是将模型从SegFormer换成了SAM，其他并没有明显改变。但在实验章节的算法效果对比中却回避了EVP，尤其是有些结果还不如EVP，这就很让人质疑其原创性和先进性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CMX:_Cross-Modal_Fusion_for_RGB-X_Semantic_Segmentation_With_Transformers</title>
      <link href="/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/"/>
      <url>/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/</url>
      
        <content type="html"><![CDATA[<p>CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation WithTransformers(TruFor使用了这个方法)</p><figure><imgsrc="../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170446816.png"alt="image-20240516170446816" /><figcaption aria-hidden="true">image-20240516170446816</figcaption></figure><p>1、原任务是分割任务，论文提出了一种将RGB图与其他图特征充分融合的方法RGB-X，可以从RGB图与X图提取特征。</p><p>2、RGB-X主要由两个部分组成：CM-FRM、FFM。</p><p>CM-FRM用于提取图片特征，其可以纠正关于另一个特性的一个特性，反之亦然，将属于同一层次的特征融合成一个单一的特征图。</p><p>FFM参考自注意力机制，设计了一种将特征融合的方法最后通过融合特征，完成分割任务。</p><p>TruFor使用了cmx来提取融合特征</p><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170520387.png"alt="image-20240516170520387" /><figcaption aria-hidden="true">image-20240516170520387</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170527564.png"alt="image-20240516170527564" /><figcaption aria-hidden="true">image-20240516170527564</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170535748.png"alt="image-20240516170535748" /><figcaption aria-hidden="true">image-20240516170535748</figcaption></figure><p>两阶段的特征融合模块（FFM）来增强信息的交互和组合。</p><p>在信息交换阶段（阶段1），两个分支仍然保持不变，并设计了一种交叉注意机制，在两个分支之间进行全局信息交换。</p><p>在融合阶段（阶段2），通过混合嵌入通道将连接的特征转换为原始大小。</p>]]></content>
      
      
      <categories>
          
          <category> 损失函数 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Uncertainty-guided Learning for Improving Image Manipulation Detection</title>
      <link href="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/"/>
      <url>/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<p>Uncertainty-guided Learning for Improving Image ManipulationDetection</p><h1 id="摘要">摘要</h1><p>​图像操纵检测（IMD）至关重要，因为伪造图像和传播错误信息可能是恶意的，会危害我们的日常生活。IMD是解决这些问题的核心技术，并在两个主要方面提出了挑战：（1）数据不确定性，即被操纵的工件通常很难被人类辨别，并导致噪声标签，这可能会干扰模型训练；（2）模型不确定性，即由于操纵操作，同一对象可能包含不同的类别（篡改或未篡改），这可能会混淆模型训练并导致不可靠的结果。以往的工作主要集中在通过设计细致的特征和网络来解决模型的不确定性问题，但很少考虑数据的不确定性。在本文中，我们通过引入一个不确定性引导的学习框架来解决这两个问题，该框架通过一个新的不确定性估计网络（UEN）来测量数据和模型的不确定性。UEN在动态监督下进行训练，并输出估计的不确定性图来细化操纵检测结果，这显著缓解了学习困难。据我们所知，这是第一项将不确定性建模嵌入IMD的工作。在各种数据集上进行的大量实验证明了最先进的性能，验证了我们方法的有效性和可推广性。</p><p>​  将不确定性引入图片篡改检测：Model不确定性（同一对象因不同的模型标记不同）与 Data不确定性（误标签与漏标签）</p><p>​  Model不确定性由U^p测定，p_t是第 t次采样中的估计操纵得分图；U^GT是真实值 y 和 μ̂之间的差异，可以测定Model不确定性和Data不确定性，可以用动态不确定性监督Lu，让U^GT专注于Data不确定性</p><p>​  其主干网络是HRNetV2 （特征提取网络）</p><p>​  模型基于数据不确定性可以增强复杂边缘的篡改检测，精细化粗略输出的边缘，得到更好的结果</p><p>​  该模型是在7张A100上训练，可见训练难度大，不易收敛</p><figure><imgsrc="../postimages/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/image-20240516170144843.png"alt="image-20240516170144843" /><figcaption aria-hidden="true">image-20240516170144843</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning</title>
      <link href="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/"/>
      <url>/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<center>Pre-training-free Image Manipulation Localization through Non-MutuallyExclusive Contrastive Learning <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-orange" alt="ICCV" /></a></center><center><span class="math inline">\(\text{Jizhe Zhou}^{1,4},\text{XiaochenMa}^{1,4},\text{Xia Du}^{2,4},\text{AhmedY}.\text{Alhammadi}^{3,4},\text{Wentao Feng}^{1,4*}\)</span></center><center>1四川大学计算机科学学院</center><center>2厦门理工大学计算机与信息工程学院</center><center>3 Mohamed Bin Zayed大学战略事务办公室</center><center>4中国教育部机器学习与产业智能工程研究中心</center><h1 id="摘要">摘要</h1><p>​  深度图像操作定位（IML）模型存在训练数据不足，严重依赖于预训练。我们认为对比学习更适合于解决IML的数据不足问题。形成相互排斥的正性与负性是对比学习的先决条件。然而，当在IML中采用对比学习时，我们遇到了三类图像补丁：篡改、真实和轮廓补丁。篡改和真实的补丁自然是相互排斥的，但是包含篡改和真实像素的轮廓补丁对它们不是相互排斥的。<br/>​  简单地取消这些轮廓补丁会导致巨大的性能损失，因为轮廓补丁对学习结果是决定性的。因此，我们提出了非互斥对比学习（NCL，Nonmutually exclusive ContrastiveLearning）框架来从上述困境中拯救传统的对比学习。在NCL中，为了应对非互斥性，我们首先建立一个具有双分支的枢轴结构，在训练时不断地在正和负之间切换轮廓补丁的作用。然后，我们设计了一个枢轴一致的损失，以避免由角色转换过程造成的空间损坏。<br/>​  通过这种方式，NCL既继承了自监督的优点来解决数据不足，又保留了较高的操作定位精度。大量的实验证明，我们的NCL在没有任何预训练的情况下，在所有五个基准测试上都达到了最先进的性能，并且在看不见的真实样本上更鲁棒。</p><h1 id="引言">1. 引言</h1><p>​  媒体技术的惊人进步使我们更容易地获得操作图像。图像处理定位（IML）是防御性信息取证中必不可少的一部分，并得到了信息安全行业的大量投资。今天，数据不足是构建深度IML模型中最突出的问题。由于用于篡改识别的密集注释和专业知识过高，IML的公共数据集都很小（有几百到几千张图像），严重不足以训练深度cnn。因此，主要的深度IML方法在额外的大规模数据集上进行预训练。<br/>​  一般来说，IML模型的预训练依赖于综合数据集。一方面，合成数据集消除了较高的标记成本，对合成数据集的预训练避免了过拟合。另一方面，使用综合数据集进行预训练会阻碍模型之间的公平比较，甚至危及模型的通用性。预训练对模型的性能至关重要，为了公平比较，同一任务的模型通常在同一数据集上进行预训练。然而，IML模型的合成预训练数据集在注释数量和质量上存在显著差异。例如，ManTra-Net[34]基于一个自收集的、像素标记的数据集102,028张图像和385种操作类型进行预训练；RGBN[38]采用了超过42000张图像的随机合成数据集；星网[33]包含100,000张复制移动图像的合成数据集用于预训练；MVSS[9]采用84000张图像的合成数据集。对在不同的合成数据集上预先训练的模型进行忠实的评估变得不可能了。此外，与真实的篡改图像不同，这些天真合成的图像严重缺乏复杂的后处理来覆盖它们的操作痕迹或伪影[5,29,9]。换句话说，合成数据集的采样过程偏向于人工构建数据集[36,37]的采样过程。在这样一个具有抽样偏差的数据集上学习的模型在通用性上很短，并且在很小的、非同源的基准上测量这种模式不能完全揭示其在真实情况下的糟糕性能。</p><p>​  总之，我们的主要贡献是四重的：</p><ul><li><strong>没有额外的数据。</strong>据我们所知，我们是第一个在IML中引入对比学习来解决训练数据的不足和训练前造成的缺陷的工作。</li><li><strong>非相互排斥的对比。</strong>据我们所知，我们也是第一个通过对比学习处理非互排他的三边关系。我们的非互斥对比学习（NCL，Non-mutuallyexclusive ContrastiveLearning）框架可以服务于其他任务，如语义分割或目标细粒度检测。</li><li><strong>最高的基准性能。</strong>我们的方法使用较少和较差的训练数据，但在所有五个公共基准上取得了最先进的性能以及最高的模型泛化能力。</li><li><strong>插件价值。</strong>我们的方法功能可以基于CNN风格和Transformer风格的骨干网络。主干选择不会破坏NCL的完整性。</li></ul><h1 id="方法">3. 方法</h1><h2 id="基本的编码器-解码器结构">3.1 基本的编码器-解码器结构</h2><p>​  我们采用DeepLabV3+[4]作为我们的IML模型的基本编码-解码器结构，因为它已经被许多其他IML模型作为基线[13,9]。请注意，基本模式选择或主干模式选择会影响我们的NCL的有效性。因此，图2中的编码器主干是ResNet101[15]块，在最后几个块中存在空洞卷积。同样应用了空间空间金字塔池（ASPP）块。然后，所编码的大小特征（64×64）被传递给解码器。该解码器采用了两个上采样模块。编码器的输出被两次上采样到4倍。简而言之，我们的基本编码器-解码器应用了与DeepLabV3+模型相同的网络结构和训练设置。</p><h2 id="非互斥对比学习">3.2 非互斥对比学习</h2><h3 id="问题公式">3.2.1 问题公式</h3><p>​  对于传统的对比学习，将问题域定义为通用集<spanclass="math inline">\(\mathbb{U}\)</span>。如图1中传统的对比学习部分所示，我们有集合的正类<spanclass="math inline">\(\mathbb{P}\)</span>，集合的负类<spanclass="math inline">\(\mathbb{N}\)</span>，其中： <spanclass="math display">\[\begin{aligned}\mathbb{P}\cup\mathbb{N}&amp;=\mathbb{U}\\\mathbb{P}\cap\mathbb{N}&amp;=\emptyset\end{aligned}\]</span>​  <spanclass="math inline">\(\emptyset\)</span>表示正类与负类的互斥性。将<spanclass="math inline">\(p\)</span>标记为一个被篡改的图像补丁，是<spanclass="math inline">\(\mathbb{P}\)</span>的一个元素。对于<spanclass="math inline">\(\forallp\in\mathbb{P}\)</span>，我们进一步表示<spanclass="math inline">\(p_{j}\in\mathbb{P},p_{j}\neq p\)</span>；和<spanclass="math inline">\(n_i\in\mathbb{N}\)</span>。那么，传统的对比学习目标是：<span class="math display">\[\arg\max_{f}\{\sum_{ii}\phi(f(p),f(n_{i}))-\phi(f(p),f(p_{j}))\}\]</span> ​  <spanclass="math inline">\(f(\cdot)\)</span>是一个图像patch的学习特征表示。<spanclass="math inline">\(f(p_{j})\)</span>和<spanclass="math inline">\(f(n_{j})\)</span>为图2中IML特征图中的红色和蓝色立方体，<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>表示两个特征向量之间的测量距离，即相似度。本文的符号是统一的，其中图像块集用大写字母表示，图像块用小写字母表示，<spanclass="math inline">\(f(\cdot)\)</span>函数是图像块的学习特征表示。</p><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240919233419057.png"alt="image-20240919233419057" /><figcaption aria-hidden="true">image-20240919233419057</figcaption></figure><p>​  然而，对于图1所示的NCL，我们有： <spanclass="math display">\[\begin{aligned}&amp;\mathbb{N}\cup\mathbb{P}\cup\mathbb{C}=\mathbb{U}\\&amp;\mathbb{P}\cap\mathbb{N}=\emptyset;\mathbb{C}\cap\mathbb{N}=\mathbb{C}^-;\mathbb{C}\cap\mathbb{P}=\mathbb{C}^+\end{aligned}\]</span>​  <spanclass="math inline">\(\mathbb{C}\)</span>是所有轮廓补丁的集合。<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>表示为<spanclass="math inline">\(\mathbb{C}\)</span>与正集和负集的交点。这意味着正像素和负像素混合在轮廓补丁中。对于对比学习，通过在同一集合中找到另一个元素，可以很容易地形成正对。根据(1)和(2)，空的交点意味着如何形成重要的负对。因此，我们首先将(3)与(1)修改为完全相同的格式。用一些小技巧，我们就可以有：<spanclass="math display">\[\begin{aligned}&amp;\mathbb{N}\cup\mathbb{P}\cup\mathbb{C}=\mathbb{U}\\&amp;\mathbb{P}\cap\mathbb{N}=\mathbb{C}^+\cap\mathbb{N}=\mathbb{C}^-\cap\mathbb{P}=\emptyset\end{aligned}\]</span>​  然后，根据(1)，我们现在可以将(3)中所写的非互斥对比转换为（<spanclass="math inline">\(\mathbb{P}\cap\mathbb{N}\)</span>）、（<spanclass="math inline">\(\mathbb{C}^+\cap\mathbb{N}\)</span>）和（<spanclass="math inline">\(\mathbb{C}^-\cap\mathbb{P}\)</span>）之间的三个二进制对比。为了进行三对比较，我们首先需要找出(3)中定义的<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>。<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>也是补丁片段或像素。基本的编码器网络不能产生补丁片段的特征。因此，我们设计了枢轴网络，直接使用轮廓块作为输入，并生成C+和C−的特征表示。也就是说，枢轴网络通过学习（<spanclass="math inline">\(\mathbb{C}\cap\mathbb{C}^+\)</span>）和（<spanclass="math inline">\(\mathbb{C}\cap\mathbb{C}^-\)</span>）之间的两个映射函数来切换轮廓斑块的作用。自然地，枢轴网络应该拥有两个具有相同输入的相似分支。</p><h3 id="枢轴网络">3.2.2 枢轴网络</h3><p>​  在构建支点网络的详细布局之前，我们需要进一步考虑支点网络的输入。训练枢轴网络也需要足够的轮廓补丁。但是，如果我们选择一个较小的斑块大小来生成更多的轮廓斑块。小的补丁大小导致一个图像补丁中的少量像素。那么，<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>中的一些元素可能包含少量的像素，不适合训练枢轴网络。因此，在单个图像中，我们将所有的轮廓块特征连接到一个完整的嵌入<spanclass="math inline">\(\mathfrak{p}\)</span>中，并发送<spanclass="math inline">\(\mathfrak{p}\)</span>作为枢轴网络的输入，以确保学习结果足够显著地进行比较。</p><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920091847744.png"alt="image-20240920091847744" /><figcaption aria-hidden="true">image-20240920091847744</figcaption></figure><p>图2。 (a): 我们的NCL框架的一般网络结构。 (b):详细的透视图网络结构。绿色箭头表示通过枢轴网络进行非互斥对比，然后产生非互斥对比学习（NCL）损失。赭色的箭头表示产生枢轴一致（PC）损失的流量。第一个编码器块输出的特征图根据地面真相，按点方向分为篡改（红色）、真实（蓝色）和等高线（紫色）特征。黄色矩形的伪造面具是不同大小的地面真相。功能尺寸用方括号表示。</p><p>​  在图2中，这个连接将紫色的立方体组装成一条大小的条带（<spanclass="math inline">\(k\times C\times W\times H\)</span>）。<spanclass="math inline">\(k=card(\mathbb{C})\)</span>。C、W、H是一个轮廓特征的通道、高度和宽度。<spanclass="math inline">\(card()\)</span>表示基数或集<spanclass="math inline">\(\mathbb{C}\)</span>中的元素数。一方面，利用<spanclass="math inline">\(k=card(\mathbb{C})\)</span>，我们将轮廓块特征连接成一个向量（<spanclass="math inline">\(k\times C\times W\timesH\)</span>）。该向量将整个图像中的轮廓块特征聚集在整个图像中，以解决当存在少数轮廓块时的模型效率低下的问题。另一方面，枢轴网络将这个（<spanclass="math inline">\(k\times C\times W\timesH\)</span>）向量平化为一个固定大小的（<spanclass="math inline">\(1\times C\times W\timesH\)</span>）向量。这进一步有助于处理特征处理中k的不同大小。<br/>​  枢轴网络的详细结构如图2(b)所示，通过粉红色的矩形和绿色的箭头表示。<br/>​  然后，我们为我们的枢轴网络设计了两个对称的分支。这些分支共享相同的输入和具有相同的结构。<spanclass="math inline">\(\mathfrak{p}\)</span>是由（1×1）卷积产生的第一个过程。这个（1×1）卷积核使<spanclass="math inline">\(\mathfrak{p}\)</span>扁平化为（<spanclass="math inline">\(1\times C\times W\timesH\)</span>）的形状。此外，这个（1×1）核将p投射到一个潜在的Hilbert空间<spanclass="math inline">\(\mathcal{H}:\mathbb{R}^{C\times W\timesH}\)</span>，其中<span class="math inline">\(f(p_{j})\)</span>和<spanclass="math inline">\(f(n_{j})\)</span>确定，特征之间的相似性可以用<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>均匀地度量。BN和ReLU是批处理规范化层和ReLU激活层。<br/>​  枢轴网络在输入集<spanclass="math inline">\(\mathbb{C}\)</span>（<spanclass="math inline">\(c\in\mathbb{C}\)</span>）和输出集<spanclass="math inline">\(\mathbb{C}^+\)</span>（<spanclass="math inline">\(c\in\mathbb{C}^+\)</span>）和<spanclass="math inline">\(\mathbb{C}^-\)</span>（<spanclass="math inline">\(c\in\mathbb{C}^-\)</span>）之间构造反射f（·）。因此，<spanclass="math inline">\(f(\cdot)\)</span>应满足：<br/>​   (1). <spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>有利于IML的精度；<br/>​   (2).<span class="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>是平滑的流形，以保证NCL损失的反向传播。由于<spanclass="math inline">\(\mathbb{C}\)</span>是一个光滑的流形（受限于Euclidean空间)，所以<spanclass="math inline">\(f(\cdot)\)</span>应该是一个双射体；<br/>​   (3).反射后无信息丢失。这意味着我们可以通过一些二进制操作<spanclass="math inline">\((\cdot)\)</span>将<spanclass="math inline">\({c}^+\)</span>和<spanclass="math inline">\({c}^-\)</span>组合回<spanclass="math inline">\({c}\)</span>；<span class="math inline">\(c^+\cdotc^-=c,c^+\cdot c=c,c^-\cdotc=c.\)</span>。<br/>​  因此，我们可以有一个组<spanclass="math inline">\((G,\cdot)\)</span>，其中<spanclass="math inline">\(G=\mathbb{C}^+\cup\mathbb{C}^-\)</span>。G是一个李群，因为：<br/>​  根据(2)，组逆<span class="math inline">\(G\toG\)</span>是平滑的。<br/>​   根据(3)，组乘<spanclass="math inline">\(G\times G\toG\)</span>是平滑的。<br/>​  因此，枢轴网络（<spanclass="math inline">\({c}^+\)</span>和<spanclass="math inline">\({c}^-\)</span>）的输出是李群元素。然后，我们将枢轴网络作为一个光滑的映射函数，并从李群中借用<spanclass="math inline">\(\mathfrak{se}\)</span>符号。我们将这两个分支的输出写成<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>。<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>只是表示由枢轴网络学习到的特征变换函数；我们不能保证它们是微分流形。<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>是在图2(b)中得到的浅红色和浅蓝色的立方体。<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>的集合是所期望的<spanclass="math inline">\(\mathbb{PI}^{+}\)</span>和<spanclass="math inline">\(\mathbb{PI}^{-}\)</span>。对<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>的直观解释是，它们是由枢轴网络生成的特征<spanclass="math inline">\(\mathfrak{p}\)</span>压缩出来的特殊的正负特征；而常见的正负特征是由主干网络根据物理上存在的图像补丁产生的。从这个角度来看，枢轴网络就像钟摆一样在正与负之间摆动枢轴的作用。<br/>​  基于H中的<spanclass="math inline">\(f(\cdot)\)</span>和<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>、<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>，我们将NCL的学习目标表述为：<span class="math display">\[&amp;\arg\max_{f,s\mathbf{c}^{+},\mathbf{s}^{-}}\{\sum_{i,j}\phi(f(p),f(n_{i}))-\phi(f(p),f(p_{j}))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}))-\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),f(p_{j}))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}))-\phi(\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}),f(n_{i}))\}\]</span></p><h3 id="非互斥的对比度损失">3.2.3 非互斥的对比度损失</h3><p>​  我们确实可以根据(5)构造NCL损失函数。但是，由于轴网络为每个被操纵的图像产生一个<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>，<spanclass="math inline">\(\phi(\mathfrak{se}^+(\mathfrak{p}),\mathfrak{se}^-(\mathfrak{p}))\)</span>独立于和参数<spanclass="math inline">\(i,j\)</span>，并在损失积累过程中成为一个常数。这样的常数破坏了对比对的多样性。因此，我们在正对的构建中做了一些小的替换，并进一步细化(5)为：<spanclass="math display">\[&amp;\arg\max_{f,\mathfrak{sc}^+,\mathfrak{sc}^-}\{\sum_{i,j}\phi(f(p),f(n_i))-\phi(f(p),f(p_j))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{sc}^+(\mathfrak{p}),f(n_i))-\phi(\mathfrak{sc}^+(\mathfrak{p}),f(p_j))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{sc}^-(\mathfrak{p}),f(p_j))-\phi(\mathfrak{sc}^-(\mathfrak{p}),f(n_i))\}\]</span>​  通过我们的枢轴网络，在(6)中，NCL将三边图像补丁之间的非互斥关系改革为三个由“+”连接的互斥、成对的二值比较。这是由图2中的NCL监督绘制的。为了简化，我们让<spanclass="math inline">\(p = p_m\)</span>分配一个下标；标记$e_{x}^{y} =(f(x),f(y))/<span class="math inline">\(，\)</span>e_{x}^{-} =(<sup>{-}(),f(x))/<span class="math inline">\(和\)</span>e_{x}</sup>{+}= (^{+}(),f(x))/<span class="math inline">\(，其中\)</span><spanclass="math inline">\(是温度参数。参考(6)，NCL损失函数为：\)</span><spanclass="math inline">\(L_{NCL}=\frac1{m\timesj}\sum_m\sum_j\log\frac{e_{p_m}^{p_j}}{e_{p_m}^{p_j}+\sum_ie_{p_m}^{n_i}}+\frac1j\sum_j\log\frac{e_{p_j}^+}{e_{p_j}^++\sum_ie_{n_i}^+}+\frac1i\sum_i\log\frac{e_{n_i}^-}{e_{n_i}^-+\sum_je_{p_j}^-}\)</span>$​  最后但并非最不重要的是，我们探索了实施支点网络的确切位置。之前的一些工作[3]截断了不同层的深度cnn，揭示了早期截断的网络为伪造检测提供了更好的特征。此外，早期截断的网络布局浅，接收场小，大特征图，理想地满足NCL小块尺寸的要求。然后，我们将ResNet101划分为卷积块，如在他们的论文[15]中所述，并探索由每个ResNet101块产生的特征图。如预期的那样，实验结果验证了第一个块后的特征图是最合适的。在实验部分，我们提供了更多关于NCL图像补丁大小选择的详细信息。</p><h2 id="枢轴一致性损失">3.3 枢轴一致性损失</h2><p>​  枢轴网络对连接的轮廓块进行卷积，破坏了轮廓块内部和之间的空间相关性。[16]已经表明，空间信息在IML中是至关重要的。因此，我们在解码器侧开发了一个枢轴一致性（PC，PivotConsistent）损失，以确保轮廓斑块的空间相关性在枢轴网络后仍然存在。PC损失在基本的像素级BCE损失中为轮廓像素分配额外的权重<spanclass="math inline">\(\mu\)</span>，以加强轮廓像素之间的空间连接。然而，轮廓像素的数量远远少于操纵或真实的像素。为了避免过拟合，如图2(a)中解码器侧的赭色箭头所示，我们使用辅助分类器[7]，在每个上采样过程中逐步累积PC损失。每次上采样后，我们将groundtruth缩小到与特征图相同的大小；然后，可以通过缩小的伪造掩模进行像素级的IML监督。我们在这里略滥用小写字母的符号。<spanclass="math inline">\(t\)</span>表示图像中的像素，<spanclass="math inline">\(\hat t\)</span>表示轮廓像素，<spanclass="math inline">\(\mu\)</span>表示额外的权重。<spanclass="math inline">\(\gamma(\cdot)\)</span>是一个像素的地面真实标签，<spanclass="math inline">\(\theta(\cdot)\)</span>是我们的网络对一个像素的预测标签。<spanclass="math inline">\(\gamma(\cdot)\)</span>和<spanclass="math inline">\(\theta(\cdot)\)</span>提供二进制值作为输出。那么，我们的PC损失是：<spanclass="math display">\[L_{PC}=\frac{\mu}{\hat{t}}\sum_{\hat{t}}(\gamma(\hat{t})\log(\theta(\hat{t}))+(1-\gamma(\hat{t}))\log(1-\theta(\hat{t})))+\frac{(1-\mu)}t\sum_t(\gamma(t)\log(\theta(t))+(1-\gamma(t))\log(1-\theta(t)))\]</span>​  我们发现较大的<spanclass="math inline">\(\mu\)</span>有利于最终的IML精度。对<spanclass="math inline">\(\mu\)</span>的评估详见实验部分。</p><h2 id="总损失函数">3.4 总损失函数</h2><p>​  综上所述，IML的NCL的混合总损失为： <spanclass="math display">\[L_{total}=\omega\times L_{NCL}+L_{PC}\]</span>​  <spanclass="math inline">\(\omega\)</span>是在浅层编码器层上进行的非互斥对比学习的权值参数。更多的<spanclass="math inline">\(\omega\)</span>可以在实验部分找到。</p><h1 id="实验和讨论">4. 实验和讨论</h1><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095639419.png"alt="image-20240920095639419" /><figcaption aria-hidden="true">image-20240920095639419</figcaption></figure><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095705963.png"alt="image-20240920095705963" /><figcaption aria-hidden="true">image-20240920095705963</figcaption></figure><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095718686.png"alt="image-20240920095718686" /><figcaption aria-hidden="true">image-20240920095718686</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 边界引导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ANOMALYCLIP</title>
      <link href="/ANOMALYCLIP/"/>
      <url>/ANOMALYCLIP/</url>
      
        <content type="html"><![CDATA[<p>ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING FOR ZERO-SHOT ANOMALYDETECTION <a href="https://arxiv.org/abs/2310.18961"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/zqhang/AnomalyCLIP"><imgsrc="https://img.shields.io/github/stars/zqhang/AnomalyCLIP?style=flat"alt="GitHub" /></a></p><p><strong>Qihang Zhou</strong>1<em>∗</em> <strong>, GuansongPang</strong>2<em>∗</em> <strong>, Yu Tian</strong>3 <strong>, ShiboHe</strong>1<em>†</em> <strong>, Jiming Chen</strong>1<em>†</em></p><p>1浙江大学2新加坡管理大学3哈佛大学</p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/ANOMALYCLIP/2310.18961v8.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  零样本异常检测（ZSAD, Zero-shot anomaly detection）需要使用辅助数据进行训练的检测模型，以便在目标数据集中没有任何训练样本的情况下检测异常。这是一个至关重要的任务时，当训练数据无法访问由于各种问题，例如，数据隐私，但它是具有挑战性的，因为模型需要推广异常在不同领域前景对象的外观，异常区域和背景特性，如缺陷/肿瘤在不同的产品/器官，可以显著不同。最近，大型的预先训练的视觉语言模型（VLMs），如CLIP，在包括异常检测在内的各种视觉任务中显示出了很强的零样本识别能力。然而，它们的ZSAD性能较弱，因为vlm更关注于前景对象的类语义建模，而不是图像中的异常/正常性。</p><p>​  在本文中，我们介绍了一种新的方法，即AnomalyCLIP，使CLIP在不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习与对象无关的文本提示，这些提示捕获图像中的一般正常性和异常性，而不管其前景对象如何。这使得我们的模型能够关注异常的图像区域，而不是对象的语义，从而能够对不同类型的对象概括归纳正常和异常识别。在17个真实异常检测数据集上进行的大规模实验表明，AnomalyCLIP在不同缺陷检测和医学成像领域的高度多样性类语义数据集的异常检测和分割方面具有优越的零镜头性能。</p><h1 id="引用">引用</h1><p>本文的主要贡献如下。</p><p>  1.我们首次揭示了学习对象不可知的文本提示的正常和异常是一种简单而有效的准确的ZSAD方法。与目前主要为对象语义对齐而设计的文本提示方法(Jeong et al., 2023; Zhou et al.,2022b)相比，我们的文本提示嵌入模型语义的一般异常和正常，允许对象无关，广义ZSAD性能。<br/>  1.然后，我们引入了一种新的ZSAD方法，称为AnomalyCLIP，其中我们利用一个对象不可知的提示模板和一个g局部异常损失函数（即全局和局部损失函数的组合）来学习通用异常和正常提示。在此过程中，AnomalyCLIP在很大程度上简化了提示设计，并可以有效地应用于不同的领域，而不需要更改其学习到的两个提示，这与WinCLIP等现有的方法不同，后者的有效性很大程度上依赖于对数百个手动定义提示的广泛工程。<br/>  1.对来自不同工业和医学领域的17个数据集进行的综合实验表明，AnomalyCLIP在检测和分割来自缺陷检查和医学成像领域的高度多样性类语义数据集的异常方面具有优越的ZSAD性能。</p><h1 id="方法">方法</h1><h2 id="与对象无关的提示学习">与对象无关的提示学习</h2><h3 id="方法概述">方法概述</h3><p>​  在本文中，我们提出了一种催化CLIP通过对象不可知的提示学习使CLIP适应ZSAD。</p><figure><img src="./../postimages/anomalyClip/image-20240519164440456.png"alt="image-20240519164440456" /><figcaption aria-hidden="true">image-20240519164440456</figcaption></figure><p>图2：AnomalyCLIP的概述。为了使CLIP适应于ZSAD，AnomalyCLIP引入了与对象无关的文本提示模板来捕获一般的正常性和异常性，而不管对象的语义如何。然后，我们引入了全局上下文优化，将全局和细粒度的异常语义纳入到对象不可知的文本提示学习中。最后，使用文本提示调优和DPAM，在CLIP的文本和局部视觉空间中实现提示学习。</p><p>​  如图2所示，AnomalyCLIP首先引入了对象不可知的文本提示模板，其中我们设计了$ g_n $ 和 $ g_a $的两个通用的对象不可知的文本提示模板，分别学习正态类和异常类的广义嵌入(见Sec.3.2)。为了学习这种通用的文本提示模板，我们引入了全局和局部上下文优化，将全局和细粒度的异常语义合并到与对象无关的文本嵌入学习中。此外，文本提示调优和DPAM还被用于支持在CLIP的文本和局部视觉空间中的学习。最后，我们整合了多个中间层，以提供更多的局部视觉细节。在训练过程中，所有模块通过全局和局部上下文优化进行联合优化。在推理过程中，我们量化了文本和全局/局部视觉嵌入的错位，以分别获得异常得分和异常得分图(见Sec.3.3)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dual_Defense</title>
      <link href="/Dual-Defense/"/>
      <url>/Dual-Defense/</url>
      
        <content type="html"><![CDATA[<p>Dual Defense: Adversarial, Traceable, and Invisible RobustWatermarking Against Face Swapping</p><h1 id="摘要">摘要</h1><p>​深度人脸交换技术的恶意应用构成了虚假信息传播和身份欺诈等安全威胁。一些研究提出了利用鲁棒水印方法来跟踪人脸图像的版权，促进伪造后的身份归属。然而，这些方法并不能从根本上防止或消除换脸的不利影响。为了解决这个问题，我们提出了双重防御，这是一种基于<strong>鲁棒对抗性水印</strong>的创新框架。它通过一次嵌入鲁棒对抗性水印，同时跟踪图像版权并破坏人脸交换模型。</p><p>​ 具体而言，我们提出了一种原域特征冒充攻击（OFEA, Original-domainFeature EmulationAttack）方法，该方法通过专门设计的原始域对抗性损失，使可跟踪水印更具攻击性。此外，我们将小波域图像结构信息补偿损失与通道注意力机制相结合，以联合平衡水印的不可见性、对抗性和可追溯性。此外，我们设计了一种更全面、更合理的评估方法来全面评估对抗性攻击对人脸交换模型的有效性。大量实验表明，双重防御表现出非凡的跨任务通用性和数据集泛化能力。它在原始和稳健的环境中都保持了令人印象深刻的对抗性和可追溯性，超过了目前仅拥有其中一种功能的伪造防御方法。</p><figure><img src="./../postimages/Dual-Defense/image-20240513111747304.png"alt="image-20240513111747304" /><figcaption aria-hidden="true">image-20240513111747304</figcaption></figure><p>图1。深度伪造主动防御场景的说明。基于水印的(a)主动防御。能够追踪伪造图像的来源，但不能防止伪造，并消除其在来源上的不利影响。基于对抗性例子的(b)积极防御。它可以破坏伪造文件的生成，但不支持可追溯性，在攻击失败时不提供可追溯性基础。(c)，我们的双重防御，主动防御。在跟踪面部图像版权的同时，它可以在确保水印完整性的同时破坏FaceSwap模型。此外，它还提供了在攻击失败时的辅助可跟踪性。</p><h1 id="引言">引言</h1><p>​ 我们的贡献可以总结为：</p><ol type="1"><li><p>我们提出了一种新型的可追溯性对抗性水印网络，这是第一种结合了对抗性和可追溯性的针对人脸交换模型的双效应主动防御方法。它具有优异的鲁棒性、跨任务通用性和数据集泛化能力。</p></li><li><p>我们创新性地提出了OFEA方法，通过将可追溯的水印嵌入到载体的鲁棒对抗性特征中，使其具有对抗性。同时，我们通过结合一个专门设计的小波域结构信息补偿损失来解决水印多目标学习中的优化冲突。</p></li><li><p>我们专门设计了一种更合理、更全面的评估方法来充分评估对脸交换的逆性。结合传统的评估指标，我们已经证明了双重防御在三个大数据集上的源跟踪和对抗性攻击中的双重有效性。</p></li></ol><h1 id="网络">网络</h1><p>Dual Defense整体算法流程如图所示：</p><figure><img src="./../postimages/Dual-Defense/image-20240513113840540.png"alt="image-20240513113840540" /><figcaption aria-hidden="true">image-20240513113840540</figcaption></figure><p>图3。双重防御的整个管道。双防御系统通过端到端训练来优化水印模型。该过程首先将目标图像$X_t <span class="math inline">\(和用户定义的水印\)</span> W_{ID}$输入到编码器中，以生成水印图像。随后，水印图像进行FaceSwap进行原始域特征冒充攻击（OFEA），计算原始域对抗损失。受干扰的图像和水印图像都通过水印解码器进行解码器优化。</p><p>​ 。</p><h1 id="实验">实验</h1><h2 id="定量结果">定量结果</h2><p>​本文首先在CASIA-WebFace、VGGFace2和LFW三个大型人脸数据集上，从不可见性、对抗性以及可溯源性三个方面的多个指标全面评估DualDefense的性能。实验表明DualDefense在保证不可见性的同时实现了出色的对抗性以及水印恢复精度。此外，通过跨图像身份及跨数据集测试表明，DualDefense具有显著的身份通用性和数据集泛化性</p><p>Dual Defense在原始设置下的定量结果。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114244119.png"alt="image-20240513114244119" /></p><p>​此外，本文与典型的基于对抗攻击的和基于深度水印的深度伪造主动防御方法分别在原始场景和各种不同的鲁棒场景下进行了对比。实验表明，DualDefense在对抗性及可溯源性方面都几乎保持最优的性能。尤其在对图像进行后处理后，对抗攻击方法的对抗性显著降低，而DualDefense依旧保持显著的对抗性能。</p><p>Dual Defense与其他主动防御方法的比较。N/A表示该方法无对应性能。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114456325.png"alt="image-20240513114456325" /></p><p>​在真实社交网络传输信道中，图像通常经历各种后处理操作。因此，本文评估了DualDefense在四种常见图像后处理操作下对 FaceSwap的对抗性和可溯源性，实验表明当水印图像经过各种处理操作时，DualDefense始终保持出色的性能，从而验证了本文方法在实际场景中的可行性。</p><figure><img src="./../postimages/Dual-Defense/image-20240513114605619.png"alt="image-20240513114605619" /><figcaption aria-hidden="true">image-20240513114605619</figcaption></figure><p><img src="./../postimages/Dual-Defense/image-20240513114617035.png"alt="image-20240513114617035" /><imgsrc="./../postimages/Dual-Defense/image-20240513114636009.png"alt="image-20240513114636009" /></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning</title>
      <link href="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/"/>
      <url>/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Generic Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</p><p>University at Buffalo</p><p><ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-blue" alt="ICCV" /></a><ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a><ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></p><h1 id="摘要">摘要</h1><p>​随着先进的图像处理技术的出现，检测操作变得越来越重要。尽管最近基于学习的图像操作检测方法取得了成功，但它们通常需要昂贵的像素级注释来进行训练，同时在与训练图像相比被不同操作的图像上进行测试时表现出较差的性能。为了解决这些局限性，我们提出了<strong>弱监督图像篡改检测</strong>，使得训练目的只需要二进制图像级别的标签（真实或篡改）。这种弱监督设置可以利用更多的训练图像，并有可能快速适应新的操作技术。为了提高泛化能力，我们提出了弱监督自一致性学习（WSCL）来利用弱注释图像。具体来说，学习了两个一致性属性：多源一致性（MSC,multi-source consistency）和补丁间一致性（IPC, inter-patch consistency）。MSC利用不同的内容无关信息，并通过在线伪标签生成和细化过程实现跨源学习。IPC执行全局成对补丁关系推理，以发现完整的操作区域。大量实验验证了我们的WSCL，即使是弱监督的，在分布内和分布外评估下，与完全监督的WSCL相比，也表现出竞争性能，以及合理的操纵定位能力。</p><p>单流概述：</p><figure><imgsrc="../postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171025018.png"alt="image-20240510171025018" /><figcaption aria-hidden="true">image-20240510171025018</figcaption></figure><p>图2:单流概述。给定输入图像，基线方法（上图）预测操作图。预测图由基于自适应池的分类损失$ L_{A-CLS} $ 和多源一致性学习损失 $ L_{MSC} $监督。同时，补丁间一致性分支（底部）学习测量补丁相似性的一致性体积。一致性卷由补丁间一致性丢失$ L_{IPC} $ 来监督。</p><p>多流概述：</p><figure><imgsrc="../postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171253974.png"alt="image-20240510171253974" /><figcaption aria-hidden="true">image-20240510171253974</figcaption></figure><p>图3。多源一致性学习的概述。分别在RGB图像、SRM噪声图和Bayar噪声图上训练三个并行流。它们的加权平均预测被用作伪地面实况，以监督每个流。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>损失函数合集</title>
      <link href="/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉熵损失crossentropyloss">1. 交叉熵损失CrossEntropyLoss</h1><p>交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight = imbalance_weight)</span><br><span class="line">loss = criterion(pred, tar.long().detach()) </span><br></pre></td></tr></table></figure><p>最优化的目标是找到一组模型参数 θ，使得交叉熵损失在训练数据上最小化：<span class="math display">\[\operatorname*{min}_{\theta}L_{CE}(\theta)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{i=1}^{C}y_{i}^{(n)}\log(p_{i}^{(n)}(\theta))\]</span>其中 N是训练样本数。</p><p>来源于AAAI2020的F3Net的加权二值交叉熵损失(the weighted binarycross-entropy loss)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def structure_loss(pred, mask):</span><br><span class="line">    weit  = 1+5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15)-mask)</span><br><span class="line">    wbce  = F.binary_cross_entropy_with_logits(pred, mask, reduce=&#x27;none&#x27;)</span><br><span class="line">    wbce  = (weit*wbce).sum(dim=(2,3))/weit.sum(dim=(2,3))</span><br><span class="line"></span><br><span class="line">    pred  = torch.sigmoid(pred)</span><br><span class="line">    inter = ((pred*mask)*weit).sum(dim=(2,3))</span><br><span class="line">    union = ((pred+mask)*weit).sum(dim=(2,3))</span><br><span class="line">    wiou  = 1-(inter+1)/(union-inter+1)</span><br><span class="line">    return (wbce+wiou).mean()</span><br></pre></td></tr></table></figure><h1 id="块对比损失patchcontrastloss">2. 块对比损失PatchContrastLoss</h1><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><p>  我们首先将 $ F∈R^{256×H×W} $ 在空间上划分为k×k个块，从而得到 $f_iR^{256hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个 $ f_i $都变成了 $ R^{256} $的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到$ m_iR^{hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $ 。为了得到每个$ m_i $的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为$ m_i $ 的值。</p><p>  然后，我们有了像素嵌入 $ f_i $ 和每个嵌入 $ m_i $的相应标签。我们现在得到监督对比损失： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中， $ A_i $表示与 $ f_i $ 具有相同标签的所有其他像素嵌入 $ k^+ $ 的集合。类似地， $k^− $ 是所有与 $ f_i $ 有不同标签的负像素嵌入。损失函数中的所有嵌入都是$ L_2 $归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span></p><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/CFL-Net-contrast-loss.drawio.png"alt="CFL-Net-contrast-loss.drawio" /><figcaption aria-hidden="true">CFL-Net-contrast-loss.drawio</figcaption></figure><h1 id="infonce对比损失">3. InfoNCE对比损失</h1><p>​  FOCAL 的训练过程如图3 (b)所示：</p><figure><img src="../postimages/FOCAL/image-20240518144850862.png"alt="image-20240518144850862" /><figcaption aria-hidden="true">image-20240518144850862</figcaption></figure><p>​  一旦我们从给定的输入X中提取高级特征F，我们就通过像素级对比学习直接监督F。地面真实伪造掩模Y自然为我们提供了正和消极类别的索引，使有效的像素级对比学习。正如很快就会更清晰的那样，焦点的对比学习以逐图像的方式进行监督，这与现有的对整个正向小批执行监督的算法[19,6,15,54,56]有很大的不同。</p><p>​  具体来说，我们采用了一种改进的InfoNCE损失[16,35]来实现焦点中的对比学习。</p><p>​  我们首先通过执行一个扁平化操作来构造一个字典 $ f( ) : { }^{ } ^{ }$ <span class="math display">\[f(F) \rightarrow \{ q , k^+_1 , k^+_2 ,..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K \}\]</span> ​  其中， $ { q ,k^+_1 , k^+_2 , ..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K } $被定义为字典，q是一个编码查询。我们让 $ { q , k^+_1 , k^+_2 , ..., k^+_J} $ 表示属于原始区域的特征（以Y中的0为索引），而 $ { k^-_1 , k^-_2 ,..., k^-_K } $ 表示伪造区域的特征（以Y中的1索引）。</p><p>​  在图像伪造检测任务中，伪造或原始区域通常覆盖超过1像素（特征）的区域，这意味着字典中正键J的数量也远远大于1。然后，根据图像伪造任务而定制的改进的信息损失可以计算为<span class="math display">\[{\cal {L}}_{InfoNCE++}=-log \frac { \frac 1J \sum _{ j \in [1,J] } exp(q \cdot k^+_J / \tau ) } { \sum _{ i \in[1,K] } exp(q \cdot k^+_i / \tau ) }\]</span> ​  其中， $ $是一个温度超参数[51]。注意，在原始的InfoNCE loss[16,35]中，字典中只有一个q匹配的正键。在我们改进的InfoNCE损失(2)中，我们通过取q的$ { k^+_j } $的期望，在每个损失计算中涉及所有的正键。这将促进优化过程。</p><p>​  需要强调的是，训练阶段的监督是直接在地面真实伪造掩模Y和提取的特征F之间进行的，而没有生成预测的伪造掩模。</p><p>​  此外，对于前向小批量中的每一幅图像， $ { \cal {L}} _ { InfoNCE++ }$以逐图像的方式（one-by-one）计算，而不是对整个批量进行计算，然后求和计算总体损失。更具体地说，给定一个小批特征$ {F_1、F_2、···、F_B} $ ，总体对比损失 $ { \cal {L}} _ { ct } $ : <spanclass="math display">\[{\cal {L}}_{ct}=\frac {1} {B} \sum _{b=1} ^{B}({\cal {L}}_{InfoNCE++}(F_b))\]</span>​  请注意，在上述（3）式中，没有合并小批特征来计算整体的 $ { \cal{L}}_{InfoNCE++} $，避免了训练数据的交叉图像影响。在伪造/原始像素的相对定义的指导下设计的总损失与[5,15,16,32]中的损失有很大的不同，[5,15,16,32]中的损失计算是在批处理级别进行的。</p><p>​  为了进一步证明(3)的合理性，我们在图4中绘制了传统的基于批处理和我们的逐图像图像的对比损失曲线。<br/><imgsrc="../postimages/FOCAL/image-20240518153103103.png"alt="image-20240518153103103" /></p><p>​  可以清楚地看到，损失函数（橙色线）的逐图像设计不仅使收敛速度更快，而且使优化更加稳定。特别是，在蓝线中检测到的高振幅脉冲表明相关的图像中可能存在严重的冲突，例如，类似于图2(a)和(b)的情况，其中出现冲突的标签。</p><figure><imgsrc="../postimages/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20240519102823673.png"alt="image-20240519102823673" /><figcaption aria-hidden="true">image-20240519102823673</figcaption></figure><p>让我们简化一下问题，假设我们只有一个查询样本，并且有3个负样本。</p><h3 id="简化场景">简化场景</h3><ul><li><strong>查询样本（query）</strong>：q</li><li><strong>正样本（positive key）</strong>：p</li><li><strong>负样本（negative keys）</strong>：n1, n2, n3</li></ul><p>在InfoNCE损失中，我们的目标是让模型学会区分 <code>q</code>和它的正样本 <code>p</code>，同时推开所有负样本 <code>n1</code>,<code>n2</code>, <code>n3</code>。</p><h3 id="构建相似度得分">构建相似度得分</h3><p>对于这个例子，我们会计算4个相似度得分：</p><ol type="1"><li><code>q</code> 和 <code>p</code> 之间的相似度得分，记作<code>s_qp</code><br/>2. <code>q</code> 和 <code>n1</code>之间的相似度得分，记作 <code>s_qn1</code><br/>3. <code>q</code> 和<code>n2</code> 之间的相似度得分，记作 <code>s_qn2</code><br/>4.<code>q</code> 和 <code>n3</code> 之间的相似度得分，记作<code>s_qn3</code></li></ol><p>这些相似度得分会组成一个向量<code>[s_qp, s_qn1, s_qn2, s_qn3]</code>。在这个向量中，<code>s_qp</code>是我们希望模型最大化得分的位置，因为它代表了查询样本和其对应的正样本之间的相似度。</p><h3 id="构建-logits-和-labels">构建 <code>logits</code> 和<code>labels</code></h3><p>在实际的代码实现中，这个相似度得分向量会被组织成 <code>logits</code>张量的一部分。对于每个查询样本，<code>logits</code> 的结构如下：</p><ul><li>第一个元素是与正样本的相似度得分（即 <code>s_qp</code>）</li><li>接下来的元素是与各个负样本的相似度得分（即 <code>s_qn1</code>,<code>s_qn2</code>, <code>s_qn3</code>）</li></ul><p>因此，对于上述例子中的单个查询样本，<code>logits</code>将是一个形状为 <code>[1, 4]</code>的张量，其中第一个元素对应于正样本的相似度得分。</p><p>现在，<code>labels</code>应该指示出哪个位置是我们期望模型正确分类为目标类别的索引。因为我们希望模型识别出正样本（即<code>logits</code> 中的第一个元素），所以我们将 <code>labels</code>设置为0，表示正样本的索引是0。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[s_qp, s_qn1, s_qn2, s_qn3]。在这个向量中，s_qp 是我们希望模型最大化得分的位置。s_qn1、s_qn2、s_qn3 是我们希望模型最小化得分的位置。对应的不是[最大化，最小化，最小化，最小化] ，所有label并不应该是[1，0，0，0]吗?</span><br></pre></td></tr></table></figure><p>你的直觉是合理的，但 InfoNCE 损失函数的工作方式稍微不同。InfoNCE并不是直接最小化负样本的得分，而是通过最大化正样本相对于所有负样本的相对得分来实现对比学习的目标。这可以通过使用softmax 函数和交叉熵损失来完成。</p><h3 id="为什么-labels-是全0">为什么 <code>labels</code> 是全0</h3><p>在构建 <code>logits</code>张量时，我们把正样本的相似度得分放在每个批次元素的第一个位置（索引为0）。因此，对于每一个查询样本而言，<code>logits</code>的结构如下：</p><ul><li>第一个元素是查询样本与它的正样本之间的相似度得分。</li><li>后续的元素是查询样本与各个负样本之间的相似度得分。</li></ul><p>当我们设置 <code>labels</code>为全0时，我们实际上是在告诉交叉熵损失函数：“对于每一个查询样本，正样本的索引是0。”也就是说，我们希望模型能够正确地选择第一个位置上的得分作为最高分。</p><h3 id="如何理解-labels">如何理解 <code>labels</code></h3><p><code>labels</code>中的0并不意味着最小化或最大化，而是指定了正确的类别索引。在我们的例子中，正确的类别就是正样本，它总是位于<code>logits</code> 的第一个位置。因此，<code>labels</code>设置为全0表示我们希望模型将正样本识别为最可能的类别。</p><h3 id="交叉熵损失的作用">交叉熵损失的作用</h3><p>交叉熵损失会根据 <code>logits</code> 和 <code>labels</code>来计算实际的损失值。具体来说，它会尝试让 <code>logits</code> 中对应于<code>labels</code>指定位置（即正样本的位置）的得分尽可能高，同时其他位置（即负样本的位置）的得分尽可能低。这并不是直接最小化负样本得分，而是在所有候选得分中，使得正样本的得分相对更高。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAIG课代表</title>
      <link href="/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/"/>
      <url>/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>b站up主 <strong>AAIG课代表</strong></p><table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 10%" /><col style="width: 39%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th style="text-align: left;">关键词</th><th>title</th><th>description</th></tr></thead><tbody><tr class="odd"><td>1</td><td>BV1qr421u7vJ</td><td style="text-align: left;">安全大模型</td><td>揭秘阿里安全审核领域的“专家”｜集内容安全/舆情分析/代码漏洞修复为一身的AI安全大模型长什么样子？</td><td></td></tr><tr class="even"><td>2</td><td>BV1aF4m1w7RB</td><td style="text-align: left;">机器语言大模型ML</td><td>《追AI的人》第38期直播清华大学网络研究院副院长张超教授分享《机器语言大模型MLM：面向软件安全分析》</td><td></td></tr><tr class="odd"><td>3</td><td>BV1dt421j7Fy</td><td style="text-align: left;"></td><td>揭秘阿里AI的全生命期风险治理实践！</td><td></td></tr><tr class="even"><td>4</td><td>BV1DM4m1Q7Kt</td><td style="text-align: left;">内容安全</td><td>防范看不见的直播“陷阱”！解密内容安全背后的四大挑战！电商平台五大要素是？分别有哪些风险？</td><td></td></tr><tr class="odd"><td>5</td><td>BV1KJ4m157Sn</td><td style="text-align: left;">阿里安全大模型</td><td>《追AI的人》第37期阿里巴巴安全部御风大模型算法能力负责人秦鹏达分享《如何用安全大模型锻造企业“盾牌”？浅谈阿里安全大模型的实践应用！》</td><td></td></tr><tr class="even"><td>6</td><td>BV1YH4y1H7zZ</td><td style="text-align: left;">生成式AI</td><td>文字变电影?!揭秘生成式AI的五大构成要素！</td><td></td></tr><tr class="odd"><td>7</td><td>BV11x4y1m7Aa</td><td style="text-align: left;">Sora</td><td>6分钟Get生成式内容检测技术,让你拥有辨别Sora的"火眼金睛"！</td><td></td></tr><tr class="even"><td>8</td><td>BV18i421Z7vH</td><td style="text-align: left;">Sora</td><td>2分钟解密全网爆火的Sora！Sora是什么？它的主要特点是？</td><td></td></tr><tr class="odd"><td>9</td><td>BV1Wz421X7GM</td><td style="text-align: left;">Sora</td><td>《追AI的人》第36期阿里安全刘佳睿分享《解密全网爆火的Sora：如何区分真实与AI生成内容？》</td><td></td></tr><tr class="even"><td>10</td><td>BV1gU421o7e4</td><td style="text-align: left;">生成式AI</td><td>AI生成图与电影画面傻傻分不清？揭秘AI绘画成功背后的技术支持</td><td></td></tr><tr class="odd"><td>11</td><td>BV1i44y1F7o5</td><td style="text-align: left;"></td><td>《追AI的人》第35期中国社会科学院大学互联网法治研究中心执行主任刘晓春分享《人工智能发展的数据流通制度基础》</td><td></td></tr><tr class="even"><td>12</td><td>BV1sC4y1C7wk</td><td style="text-align: left;"></td><td>人工智能"上位"成功？揭秘ChatGPT成功背后的技术突破！</td><td></td></tr><tr class="odd"><td>13</td><td>BV1sw411g7Ui</td><td style="text-align: left;"></td><td>《追AI的人》第34期直播回放复旦大学张谧教授分享《当“巨兽”成为“宠物”：复旦白泽带你领略大模型安全伦理风险与治理》》</td><td></td></tr><tr class="even"><td>14</td><td>BV1Tb4y1j7iU</td><td style="text-align: left;"></td><td>生成式AI能减小人与人之间的差异？生成式AI互动机制如何能够更有效地产生结果？</td><td></td></tr><tr class="odd"><td>15</td><td>BV1Tg4y1f7gf</td><td style="text-align: left;"></td><td>《追AI的人》第33期直播回放中山大学网络空间安全学院院长操晓春教授分享《“病态的”的计算机视觉算法》</td><td></td></tr><tr class="even"><td>16</td><td>BV1yu411F7E5</td><td style="text-align: left;"></td><td>为什么生成式AI更容易取代白领员工,对体力工作者的影响却微乎其微?</td><td></td></tr><tr class="odd"><td>17</td><td>BV1fN41137zv</td><td style="text-align: left;">数字水印</td><td>《追AI的人》第32期直播回放中国科学技术大学张卫明教授分享《人工智能背景下的数字水印》</td><td></td></tr><tr class="even"><td>18</td><td>BV1Wy4y1c7Dm</td><td style="text-align: left;"></td><td>《追AI的人》第31期直播回放清华大学经济管理学院领导力与组织管理系主任李宁教授分享《人机协同、效率与创新：AI时代的组织模式探索》</td><td></td></tr><tr class="odd"><td>19</td><td>BV1Rh4y1i75x</td><td style="text-align: left;"></td><td>《追AI的人》第30期直播上海交通大学张拳石教授分享《较真地追问，神经网络是否可以被严谨地彻底地解释清楚？》</td><td></td></tr><tr class="even"><td>20</td><td>BV1XH4y1o7Nh</td><td style="text-align: left;">多模态信号融合</td><td>从生物心理学角度看多模态大模型发展史！多模态信号如何融合？</td><td></td></tr><tr class="odd"><td>21</td><td>BV1nm4y1N7Q7</td><td style="text-align: left;">安全伦理</td><td>《追AI的人》第29期直播回放复旦大学桂韬老师分享《大模型有何安全伦理风险问题？看MOSS-RLHF如何实现人类与AI的价值观对齐》</td><td></td></tr><tr class="even"><td>22</td><td>BV1M94y1s76K</td><td style="text-align: left;">深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td><td></td></tr><tr class="odd"><td>23</td><td>BV178411v7xy</td><td style="text-align: left;"></td><td>《追AI的人》第13期直播回放IEEE亚太区执委、人道主义科技活动委员会主席董晶分享《AI前沿技术对抗中的”天使”与“恶魔”》</td><td></td></tr><tr class="even"><td>24</td><td>BV1m34y1P7oY</td><td style="text-align: left;"></td><td>《追AI的人》第14期直播回放中国政法大学兼职教授、盈理律师事务所资深顾问赵军分享《《对人工智能产业发展四大要素的保护——数据与知识产权的挑战与实践》》</td><td></td></tr><tr class="odd"><td>25</td><td>BV1QF411k7LA</td><td style="text-align: left;"></td><td>3分钟Get多模态是什么？不许在脑海中想象一头粉红色的大象，你想的是什么？</td><td></td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td style="text-align: left;">图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td><td></td></tr><tr class="odd"><td>27</td><td>BV1dG411o7fn</td><td style="text-align: left;"></td><td>《追AI的人》第27期直播回放AAIG图片视觉大模型与视觉AIGC安全算法负责人洪海文分享《多模态大模型的发展与攻防</td><td></td></tr><tr class="even"><td>28</td><td>BV1DV4y1v7VS</td><td style="text-align: left;"></td><td>以ChatGPT为例3分钟解密生成式语言模型的训练过程！</td><td></td></tr><tr class="odd"><td>29</td><td>BV1nu4y1S7eX</td><td style="text-align: left;"></td><td>教你分清生成式AI、大模型、AIGC！Bert、T5、ChatGPT三者有什么区别？</td><td></td></tr><tr class="even"><td>30</td><td>BV1qP411C7KT</td><td style="text-align: left;"></td><td>“吃“书狂魔是怎么炼成的？解密ChatGPT数据集之谜！</td><td></td></tr><tr class="odd"><td>31</td><td>BV14j411S7ys</td><td style="text-align: left;"></td><td>从感知机到大模型,3分钟读懂AIGC的前世今生！</td><td></td></tr><tr class="even"><td>32</td><td>BV1na4y1A7LK</td><td style="text-align: left;"></td><td>《追AI的人》第26期直播回放阿里巴巴人工智能治理与可持续发展研究中心AAIG人工智能安全实验室主任张荣分享《我们给生成式模型做一个体检》》</td><td></td></tr><tr class="odd"><td>33</td><td>BV1Vo4y1N7Lg</td><td style="text-align: left;"></td><td>《追AI的人》第25期直播回放橙盾科技-塔玑虚拟模特产品算法负责人郎一宁分享《从灵魂画师到光速写手，挖一挖大模型背后的知识结构》</td><td></td></tr><tr class="even"><td>34</td><td>BV1oc411G7qb</td><td style="text-align: left;"></td><td>震惊!白领职业将被替代?如何和AI合作共赢?工作的毁灭和创造的时代来了!</td><td></td></tr><tr class="odd"><td>35</td><td>BV1Da4y1u7bg</td><td style="text-align: left;"></td><td>ChatGPT来抢饭碗了!盘点20个最有可能被取代的职业!</td><td></td></tr><tr class="even"><td>36</td><td>BV1cz4y1a7or</td><td style="text-align: left;"></td><td>《追AI的人》第24期直播回放北京大学国家发展研究院助理教授胡佳胤分享《生成式AI的时代，我们的工作会变成什么样子？》</td><td></td></tr><tr class="odd"><td>37</td><td>BV1bz4y1Y7ss</td><td style="text-align: left;"></td><td>用户、网店、骑手、监管部门如何看待算法?算法是否能完全透明?</td><td></td></tr><tr class="even"><td>38</td><td>BV12c411n7Q1</td><td style="text-align: left;"></td><td>大数据杀熟现象:商业惯例还是不道德行为？如何定义？</td><td></td></tr><tr class="odd"><td>39</td><td>BV15g4y1T7ey</td><td style="text-align: left;"></td><td>什么是三近一反原则?流量为王的时代,教你如何“破圈”创作!</td><td></td></tr><tr class="even"><td>40</td><td>BV1PN411A7FF</td><td style="text-align: left;"></td><td>《追AI的人》第23期直播回放中国广告协会法律与道德工作委员会常务委员杜东为分享《算法治理拉开帷幕，知识需不断碰撞融合》</td><td></td></tr><tr class="odd"><td>41</td><td>BV1sv4y1V7zN</td><td style="text-align: left;"></td><td>《追AI的人》第22期直播回放清华大学副教授眭亚楠老师分享《AI助力瘫痪患者恢复站立和行走》</td><td></td></tr><tr class="even"><td>42</td><td>BV1qg4y1t7xR</td><td style="text-align: left;"></td><td>《追AI的人》第21期直播回放阿里巴巴人工智能治理与可持续发展研究中心算法专家许皓天分享《ChatGPT的前世今生与风险治理》</td><td></td></tr><tr class="odd"><td>43</td><td>BV1yY4y1y77L</td><td style="text-align: left;"></td><td>这份安全收快递指南请收好！这份安全快递指南请收好！如何减少隐私泄漏的风险？如何自我保护？</td><td></td></tr><tr class="even"><td>44</td><td>BV1g54y1c7i3</td><td style="text-align: left;"></td><td>《追AI的人》第20期直播回放南开大学陈兵老师分享《以友好型算法治理为中心，推进可信人工智能健康发展》</td><td></td></tr><tr class="odd"><td>45</td><td>BV1mP4y1k7aM</td><td style="text-align: left;"></td><td>《追AI的人》直播第18期对外经贸许可老师分享《中国民众如何看待算法：经验与规范》</td><td>我们已然生活在“算法社会”中，无处不在的算法无时不刻不在改变着我们的生活和工作。为了更好地了解我国民众对算法应用真实的感受和评价，完善我国算法治理，我们开展了“算法应用用户感知”大规模实证调查，并且由此得出了富含意蕴的启示。</td></tr><tr class="even"><td>46</td><td>BV1p3411d7eC</td><td style="text-align: left;"></td><td>《追AI的人》直播第19期清华梁正老师分享《人工智能治理亟待标准落地》</td><td>过去两年来，世界范围内人工智能治理加速推进，欧盟、美国、英国相继出台了相关法律、政策与战略文件，有专家总结人工智能领域的“治理竞争”已经开始。国内来看，随着数安法、个保法、以及跨境数据流动、数据治理、算法治理、伦理治理等领域重要法规和文件的颁布实施，我国在数字经济乃至人工智能领域的治理架构已基本成型。当前人工智能治理面临的一个突出矛盾是如何将相关合规要求落实为可以具体操作实施的举措，而在这方面，作为治理基本指引和行业最佳实践的标准恰恰可以发挥重要的作用。标准作为“软法”在人工智能治理中如何发挥作用？国</td></tr><tr class="odd"><td>47</td><td>BV1Bd4y1572B</td><td style="text-align: left;"></td><td>大数据是如何精准猜透你的心（下）如何减少推荐用户不喜欢的内容？如何增加推荐系统的多样性？</td><td></td></tr><tr class="even"><td>48</td><td>BV1EG4y1L7eg</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（下）</td><td></td></tr><tr class="odd"><td>49</td><td>BV1qe4y1V7NV</td><td style="text-align: left;"></td><td>信息茧房和马太效应是什么?大数据如何精准猜透你的心?</td><td>在日常生活中，你有没有遇到过这种情况：只要你点赞或收藏了某个短视频，就会持续收到同类型的内容？不仅仅是短视频，还有听音乐的每日推荐，是不是也都是你经常听的类型或者歌手？其实，这些都属于个性化推荐！</td></tr><tr class="even"><td>50</td><td>BV1g44y1R7bK</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（中）</td><td></td></tr><tr class="odd"><td>51</td><td>BV1uD4y1V7hF</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享(上)</td><td></td></tr><tr class="even"><td>52</td><td>BV16P411u7ea</td><td style="text-align: left;"></td><td>《追AI的人》第16期直播重庆邮电大学校长高新波老师分享《人工智能的未来发展趋势分析》</td><td>近年来，人工智能获得了迅猛的发展，面向特定领域取得了诸多单点突破。但是，从本质上看今天人工智能的成功更多来自“勤能补拙”，距离真正的智慧还有较大的距离。为了使人工智能聪明更可靠。《追AI的人》第16期直播首先介绍了目前人工智能发展所遇到的瓶颈问题，然后提出了未来人工智能发展的六个方向，即绿色低碳更灵巧的人工智能、知识数据双驱动的人工智能、人机物融合的混合人工智能、可信可靠可解释的人工智能、非深度神经网络的人工智能、开放环境自适应的人工智能。最后，一起展望未来人工智能的发展前景。</td></tr><tr class="odd"><td>53</td><td>BV1wW4y1W764</td><td style="text-align: left;"></td><td>什么是大数据杀熟？揭秘“杀熟”误区！为什么老用户看到的价格比新用户更高？</td><td>什么是大数据杀熟？揭秘“杀熟”误区！《如何获取消费者对电商平台价格和用户权益的信任》为什么老用户看到的价格比新用户更高？为什么不少用户会产生“被杀熟”的想法？阿里针对不同用户，可能在获得的优惠结果上不一致的情况下，如何实施遵循原则以保障用户的权益公平性</td></tr><tr class="even"><td>54</td><td>BV1Pv4y1D7Za</td><td style="text-align: left;"></td><td>《追AI的人》直播第15期AAIG自然语言理解实验室EMNLP专场《机器=冰冷？看机器如何捕捉你的小情绪》《文本如药，如何精确提炼“有效成分”？》》</td><td>Part 1: 《机器=冰冷?看机器如何捕捉你的小情绪》人工智能领域日新月异，人们已经不满足于让机器完成指定的任务，从如今大火的各类语音助手和数字人可以看出，“拟人化”是人工智能应用的重大需求。为了让人机对话过程更加丝滑，理解用户的情绪是关键一步。计算机该如何理解我们的情绪呢？构建有人情味的人工智能应用有哪些困难？第一部分将简要对此进行介绍。1、对话中的情感分析问题的定义、应用和当前的困难2、基于有监督原型对比学习的对话情感分析方法3、对话情感分析的未来发展方向 Part 2: 《文本如药,如何</td></tr><tr class="odd"><td>55</td><td>BV1WP411E7Xk</td><td style="text-align: left;"></td><td>如何识别美颜照片的真实性？眼见一定为实吗？虚拟idol也是深度合成吗？什么是深度合成呢？阿里针对深度合成做了哪</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>56</td><td>BV1ae411u7cY</td><td style="text-align: left;"></td><td>算法透明=公开源代码？源代码开放后会带来什么影响和后果呢？为什么算法透明是算法治理的核心要求？</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="odd"><td>57</td><td>BV1Zd4y1d7aa</td><td style="text-align: left;"></td><td>科幻小说中的人类为什么害怕人工智能？如何构建人与人工智能的伦理关系?</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>58</td><td>BV1SU4y1v7VL</td><td style="text-align: left;"></td><td>人工智能的生成物能否获得版权的保护？获得著作权保护需要满足哪些条件呢?|《追AI的人》1分钟AI科普短视频</td><td>《追AI的人》系列短视频是一档由阿里巴巴人工智能治理与可持续发展研究中心(AAIG)联合高校和产业界发起的AI治理交互栏目《追AI的人》的衍生短视频。用浅显易懂的词汇把人工智能新技术、AI治理新观点、可持续发展新风向在短短1分钟左右传达给大家，让AI与我们的生活离得更近。</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位视频合集</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 13%" /><col style="width: 37%" /><col style="width: 37%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>题目</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>30</td><td>BV1xN411Z7Tt</td><td>智能信息伪装</td><td>【图图Seminar30】张卫明：智能信息伪装</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”前沿论坛报告人：中国科学技术大学 张卫明</td></tr><tr class="even"><td>30</td><td>BV1j64y1y7XS</td><td>神经网络水印</td><td>【图图Seminar30】张新鹏：神经网络水印</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”论坛报告人：复旦大学 张新鹏</td></tr><tr class="odd"><td>76</td><td>BV18Y4y1r7tb</td><td>视频身份识别技术</td><td>【图图Seminar76】“视频身份识别技术”主编论坛——图图名师讲堂</td><td>论坛致辞：中山大学 赖剑煌 报告专家：厦门大学 纪荣嵘 清华大学 王生进南方科技大学 于仕琪 国防科技大学 蓝龙 武汉科技大学 王晓论坛主持：山东科技大学 张鹏</td></tr><tr class="even"><td>77</td><td>BV1Kr4y147rU</td><td>智能图像安全</td><td>【图图Seminar77】“智能图像安全”主编论坛——图图名师讲堂</td><td>论坛致辞：中国科学技术大学 张卫明 报告专家：华南理工大学 胡永健上海理工大学 秦川 中国科学技术大学 储琪 论坛主持：复旦大学 钱振兴</td></tr><tr class="odd"><td>125</td><td>BV1ry4y1F7vX</td><td>视频身份识别</td><td>【图图Seminar125】视频身份识别前沿论坛——图图专刊优秀成果分享会</td><td>主持人：于仕琪 副教授（南方科技大学） 报告人：王生进教授（清华大学）；马丙鹏 教授（中国科学院大学）；张权博士生（中山大学）；许文正 硕士生（山东大学）</td></tr><tr class="even"><td>131</td><td>BV14w41187rS</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar131】数字媒体深度伪造与对抗论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共11位报告人，论坛主席：卢伟 教授、钱振兴 教授，评审专家：李晓龙 教授、张卫明 教授、王员根 教授、秦川 教授。</td></tr><tr class="odd"><td>148</td><td>BV1Bt421A7gP</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar148】“数字媒体深度伪造与对抗”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：生成式人工智能与可证明安全隐写 报告人：陈可江中国科学技术大学特任副研究员报告二：联合多重对抗与通道注意力的高安全性图像隐写 报告人：马宾齐鲁工业大学教授 报告三：基于语义解耦的AI生成图像取证技术 报告人：丁峰南昌大学校聘教授</td></tr></tbody></table><table><colgroup><col style="width: 4%" /><col style="width: 13%" /><col style="width: 17%" /><col style="width: 65%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>title</th></tr></thead><tbody><tr class="odd"><td>22</td><td>BV1M94y1s76K</td><td>深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td>图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards Effective Image Manipulation Detection with Proposal Contrastive Learning</title>
      <link href="/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/"/>
      <url>/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Effective Image Manipulation Detection with ProposalContrastive Learning</p><h1 id="摘要">摘要：</h1><p>​深度模型在图像操作检测中得到了广泛而成功的应用，旨在对篡改图像进行分类和定位篡改区域。现有的方法大多集中于从被篡改图像中提取全局特征，而忽略了单个被篡改图像中被篡改区域与真实区域之间的局部特征的关系。为了利用这种空间关系，我们提出了建议对比学习（PCL）来进行有效的图像操作检测。我们的PCL由一个双流架构组成，通过分别从RGB和噪声视图中提取两种类型的全局特征。为了进一步提高鉴别能力，我们通过吸引/排斥基于命题的代理命题对比学习任务来利用局部特征的关系。此外，我们还证明了我们的PCL在实践中可以很容易地适应未标记的数据，这可以降低人工标记的成本，并促进更一般化的特性。在几个标准数据集中进行的大量实验表明，我们的PCL可以作为一个通用的模块来获得一致的改进。</p><figure><imgsrc="../postimages/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/image-20250107105329585.png"alt="image-20250107105329585" /><figcaption aria-hidden="true">image-20250107105329585</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CatmullRom Splines-Based Regression for Image Forgery Localization</title>
      <link href="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/"/>
      <url>/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/</url>
      
        <content type="html"><![CDATA[<center>CatmullRom Splines-Based Regression for Image Forgery Localization<ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\textbf{Li Zhang}^{1,2},\textbf{MingliangXu}^{2},\textbf{Dong Li}^{2},\textbf{JianmingDu}^{1,\dagger},\textbf{Rujing Wang}^{1,2\dagger}\)</span></center><center>中国科学院合肥物理科学研究所、中国科技大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/28548.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>  IFL（Image ForgeryLocation）有助于确保数字媒体取证的安全。然而，许多方法存在错误的检测（即FPs）和不准确的边界。在本文中，我们提出了基于CatmullRom样条的回归网络（CSR-Net,CatmullRom Splines-based RegressionNetwork），它从回归的角度重新考虑IFL任务来处理这个问题。</p><p>  具体来说，我们提出了一种自适应的CutmullRom样条变换方案，用于被篡改区域的粗定位。然后，对于假阳性例子，我们开发了一种新的重新评分机制，旨在筛选出不能在分类分支和实例分支上都有响应的样本。随后，为了进一步限制边界，我们设计了一个可学习的纹理提取模块，该模块通过解耦水平和垂直伪造特征来参考和增强轮廓表示，从而提取出更鲁棒的轮廓表示，从而抑制FPs。与基于分割的方法相比，由于不需要后处理，我们的方法简单而有效。大量的实验表明，CSR-Net优于现有的先进方法，不仅在标准的自然图像数据集上，而且在社交媒体数据集上。</p><h1 id="引言">引言</h1><p>  第一个问题是<strong>假阳性（FPs）</strong>。假阳性是指测试结果表明存在一个令人满意的目标区域，而实际上它并不令人信服。传统的基于分割的方法往往会出现这种情况（如图2所示）。</p><p><img src=" ../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503164842397.png" alt="image-20240503164842397 " style="zoom:40%;" /></p><p>  二值化是这些方法中不可或缺的决定性策略，它是一种直接决定前景区域块数量的阈值敏感任务。在传统的分割方法中，一个不合理的阈值往往会导致出现意外的区域（即假阳性的情况）。然而，许多方法在关注潜在的被篡改区域时，通常会忽略误报率。这对数字内容的传播有负面影响，影响了相关新闻来源的可获得性，这限制了分析结果向更有令人信服的方向发展。</p><p>  第二个问题是<strong>不准确的边界</strong>。传统的基于分割的方法存在连续解码器层之间的掩模预测不一致，导致优化目标不一致和特征空间的弱耦合（如图1a所示）。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503165143372.png" alt="image-20240503165143372" style="zoom:67%;" /></p><p>  另一方面，当直接引入一般回归方法来处理任务时，定位效果也不令人满意，因为所使用的边界盒只能以四边形的方式定位目标区域，而且通常目标区域大多出现在不规则曲线中（图1(b)显示了旋转检测的定位效果（Li et al.2022），其中我们使用掩蔽区域的最小边界四边形作为GroundTruth）。越来越复杂的被篡改图像提出了更大的挑战，因为大多数方法不能很好地约束或明确地建模伪造的区域边界，这很容易导致在检测结果中混合其他目标或不兼容的背景。</p><p>  最近，一些基于回归的策略在目标检测领域的假阳性判定方面取得了显著进展(You et al. 2022; Li and Kosecka 2022; Chen et al.2023)。与目标检测任务不同，IFL是一个像素级的任务，这意味着方法迁移的直接性将带来性能下降。为此，需要引入一些定制的方法和处理方法，可以有效地连接这两个任务。</p><p>  首先，对于标记为GT的掩模，我们引入了CatmullRom样条曲线来将其转换为多边形帧，从而使回归策略能够应用于像素级的任务，如IFL。同时，在训练和推理过程中，为了使多边形标记更接近真实标记，提出了自适应参数CatmullRom样条方法，该方法可以最小化预测区域与地面真实值之间的相似性差距和目标区域的曲率。其次，为了进一步、明确地抑制定位结果中的假阳性，我们提出了一种有效的重评分机制：我们通过两个独立的预测分支，直接拒绝在两个分支中都没有接收到响应的假阳性，每个预测分支都有一个区域分类得分和一个实例得分。此外，为了得到更准确的边界，我们通过解耦水平纹理特征和垂直纹理特征，进一步细化预测区域的轮廓，以建模锻造区域边界，减少它们与其他掩模之间的重叠。</p><p>  我们的贡献可以概括为三类：</p><p>  1)我们制作了一个基于CatmullRom样条的回归网络（CSR-Net,<strong>C</strong>atmullRom <strong>S</strong>plines-based<strong>R</strong>egressionNetwork），首次尝试将回归方法引入像素级任务。</p><p>  2)为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><p>  3)在多个公共数据集（包括自然图像数据集和社交媒体数据集）上进行的大量实验表明，我们的方法与IFL中最先进的方法相比具有优越性。</p><h1 id="方法">方法</h1><h2 id="概括">概括</h2><p>  图3是对我们的框架的概述。输入图像表示为 $ X R^{H×W×3} $。首先，我们使用嵌入ResNet-50的FPN作为骨干网络进行Catmull样条检测。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>  更具体的是，我们的方法利用一种基于CatmullRom样条的参数化方法来自适应地识别目标分割区域（橙色部分）。跟随（Chenet al. 2017），我们采用空间空间金字塔池（ASPP, atrous spatial pyramidpooling）和ResNet-50来捕获长期上下文信息和多尺度特征。<br/><br/>  这种无锚定的卷积神经网络大大简化了我们的任务的检测，也允许我们获得一个粗糙的特征图。稍后，我们使用一个重新评分机制（CRA,re-scoringmechanism）来筛选出在粗特征图（蓝色部分）上突出显示的可疑区域的假阳性样本。最后，我们同时从水平和垂直方向的区域进行纹理提取（byVTP），以预期获得更准确的边界（绿色部分）。请注意，保留的每个篡改区域将由VTP独立处理。</p><h2 id="catmullrom样条检测">CatmullRom样条检测</h2><p>  大多数传统方法在IFL中使用基于分割的思想。<br/><br/>  然而，在处理此类面具或基于多边形的数据集时，基于回归的方法往往是一种更有效的方法，例如（Liu等2019；Pranav，正刚等2020；Zhang等2022）。<br/><br/>  一般来说，主流的基于回归的方法需要复杂的处理来适应实例的边界，这导致了实践中的不可靠和不稳定性。近年来，样条曲线被用于计算机图形应用中生成各种形状的曲线。例如，自动驾驶车道线(Ma等2019年；余和陈2017年）、文本检测（刘等2020年；唐等2022年；阮等2021年）、故障检测(Park等2011年；郭和王2005年）等。其中，CatmullRom样条函数是一种经典的插值样条，由于其变换效应和推理代价，适用于被篡改区域的参数化（钱德拉2020；Li2022）。<br/><br/>  具体地说，CatmullRom样条是一组三次插值样条，因此每个点上的切线使用样条上的上一个点和下一个点计算。在给定的控制点下，卡特穆尔罗姆样条函数可以适应任何形状（李和陈2016；李，刘，刘2022）。此外，构造三次小矩阵样条函数只涉及整数系数，与其他样条函数相比，它降低了实现成本。上面提到的所有这些属性都有助于提高更快的推理速度和更低的计算消耗（Flops）。<br/><br/>  数学上，数学样条被定义为等式1: <spanclass="math display">\[c_i(t)=\sum_{j=0}^3b_j(t)\boldsymbol{p}_{i+j},\quadi=0,1,\ldots,n-3\]</span></p><p>  其中，0≤t≤1，<spanclass="math inline">\(p_{i}(i=0,1,\ldots,n-3;n\geq3)\)</span>为控制点，<spanclass="math inline">\(b_{j}(t)\)</span>为基。例如，它可以用等式2来表示当函数<spanclass="math inline">\(b_{j}(t)\)</span>中t的最大幂为3时：</p><p><span class="math display">\[c_i(t)=\frac{1}{2}\cdot[1\quad t\quadt^2\quadt^3]\cdot\begin{bmatrix}0&amp;2&amp;0&amp;0\\-\tau&amp;0&amp;\tau&amp;0\\2\tau&amp;\tau-6&amp;-2(\tau-3)&amp;-\tau\\-\tau&amp;4-\tau&amp;\tau-4&amp;\tau\end{bmatrix}\cdot\begin{bmatrix}p_i\\p_{i+1}\\p_{i+2}\\p_{i+3}\end{bmatrix}\]</span></p><p>  为了协调被篡改区域的任意形状与CatmullRom样条曲线，我们从现有的数据集和真实的图像中深入研究了定向或弯曲的被篡改区域。在卡氏样条曲线中，<spanclass="math inline">\(\tau\)</span>（张力因子）是控制样条线紧性的一个重要参数。张力因子的值越高，曲线在控制点之间弯曲越紧密，从而在移动过程中更接近给定的数据点。相反，较低的张力系数值会使控制点之间的曲线更平滑。直观地看，传统的卡莫样条（参数<spanclass="math inline">\(\tau=1\)</span>）对IFL任务很差，因此我们试图通过调整τ在匹配精度和曲线平滑度之间找到正确的平衡。消融实验（在消融分析部分）表明，当<spanclass="math inline">\(\tau\)</span>设置为16时，CatmullRom样条曲线对该任务是可靠的。它还允许学习到的控制点更接近前景（篡改）区域。</p><h2 id="catmullrom-ground-truth-生成"><strong>CatmullRom Ground Truth生成</strong></h2><p>  在IFL中，许多基准测试使用基于掩码或多边形的数据集作为公共数据集(Dong, Wang, and Tan 2013; Hsu and Chang 2006; Alibaba2021/2022)。给定曲线边界的注释点 $ { p_i } _{i=1} ^n $ ，其中 $ p_i $表示第 $ i $ 个注释点，主要目标是根据等式一获得CatmullRom样条 $ c(t) $的最优参数。为了实现这一点，我们可以简单地应用标准最小二乘法，如等式3所示 :</p><p><spanclass="math display">\[\begin{bmatrix}p_{03t_0}&amp;\cdots&amp;p_{33t_0}\\p_{03t_1}&amp;\cdots&amp;p_{33t_1}\\\vdots&amp;\ddots&amp;\vdots\\p_{03t_m}&amp;\cdots&amp;p_{33t_m}\end{bmatrix}\begin{bmatrix}c_{x_0}&amp;c_{y_0}\\c_{x_1}&amp;c_{y_1}\\c_{x_2}&amp;c_{y_2}\\c_{x_3}&amp;c_{y_3}\end{bmatrix}=\begin{bmatrix}\mathscr{P_{x_0}}&amp;\mathscr{P_{y_0}}\\\mathscr{P_{x_1}}&amp;\mathscr{P_{y_1}}\\\mathscr{P_{x_2}}&amp;\mathscr{P_{y_2}}\\\mathscr{P_{x_3}}&amp;\mathscr{P_{y_3}}\end{bmatrix}\]</span></p><p>  其中，m表示一个曲线边界的标注点的数量。而t是通过使用累积长度与多段线的周长的比值来计算的。$ p_{ij} $ 可以从等式中引用1，我们使用 $ _{i} $表示变换后的新坐标点。根据等式1和等式3，我们将原始的掩码注释转换为一个参数化的CatmullRom样条曲线。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240505171016728.png" alt="image-20240505171016728" style="zoom:50%;" /></p><p>图4：立方CatmullRom样条曲线的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。</p><h2 id="综合再评分算法cra">综合再评分算法CRA</h2><p>  MaskR-CNN背后的基本机制是将所得到的边界框的分类一致性视为分数，然后使用一个预先确定的阈值来筛选出背景框。然而，尽管有了一些进展，当边界框包含一个明显不兼容的区域实例时，它伴随着大量的背景信息，并且MaskRCNN经常显示出如此低分数的真阳性，而它保留了一些相对较高可信度的FPs。因此，我们为每个区域实例重新分配分数。具体来说，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。数学上，给定预测的n类分数$~ \mathrm{CLS}=\{s_{ij}^{cls}\mid j\in[0,\cdots,n-1]\} $ 和 $~\mathrm{INS}=\{s_{ij}^{ins}\mid j\in[0,\cdots,n-1]\} $通过等式4计算第i个提出建议的综合得分： <spanclass="math display">\[s_{ij}=\frac{e^{s_{ij}^{cls}+s_{ij}^{ins}}}{\sum_{l=0}^{n-1}e^{s_{il}^{cls}+s_{il}^{ins}}}\]</span>  在我们的工作中，我们采用了n =2，其中这两个类再现了图4：三次组合样条的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。发送篡改（前景）和真实（背景）区域。因此，我们只需要计算前景类的分数。<br/><br/>  CLS由一个类似于MaskR-CNN的分类分支直接获得，INS是区域实例在全局区域分割图上的激活值。具体来说，它被投影到每个区域实例的篡改区域分割地图上，包含<spanclass="math inline">\(P_{i} = \{p_{i}^{1},p_{i}^{2}\ldots p_{i}^{n}\}\)</span>，并且区域实例区域中Pi的平均值可以表示为： <spanclass="math display">\[s_{i1}^{ins}=\frac{\sum_jp_i^j}N\]</span>  其中，Pi是区域分割地图上第i个区域实例的像素值的集合。将分类得分与实例得分有机结合，得到综合得分，在实践中可以降低FP的可信度。这是因为FPs往往比分割图上的区域有更弱的响应。<br/><br/>  下面的实验结果表明，我们的设计对图片剪切情况更友好，因为剪接情况通常在分割图上享有更强的响应，较高的实例分数将弥补较低的分类分数。</p><h2 id="垂直纹理-交互式感知vtp">垂直纹理-交互式感知VTP</h2><p>  传统的边缘检测操作符（例如，索贝尔、罗伯茨、普雷威特等）有助于提取自然图像处理任务中手工制作的特征，而最大的缺点是它们不能根据任务的特殊性进行动态学习。受（Holla和Lee2022）的启发，我们在一个被称为Sobel层的可学习模块中采用了一个边缘检测算子，见图5。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>图5：Sobel层图，用于VTP，用于增强边缘相关模式和操作边缘检测。来自第i个块首先通过Sobel单元（SU），然后通过一个边缘剩余单元（ERU）。为了进行训练和优化的原因，引入了一种残差学习策略。</p><p>  此外，为了更好地建模被篡改的区域边界，我们在我们的网络中引入了垂直纹理交互式感知（VTP,Vertical Texture interactivePerception）。在VTP中，被篡改的区域用一组等高线点表示，这些包含强纹理特征的点可以精确地定位具有任意形状的被篡改区域。<br/><br/>  看到它们：有两个核心并行分支在VTP分支，在顶部，我们引入一个卷积内核大小1×k滑动功能地图模型局部纹理信息在水平方向，只关注k-range地区的纹理特征。这个巧妙的技巧通过我们的预实验证明是简单且有效的。此外，它几乎是免资源开销的同时保持竞争效率。通过类似的范例，将底部分支通过大小为k×1的卷积核在垂直方向上对纹理特征进行建模。k是控制纹理特征接受场大小的超参数。在实际实验中，我们取k=3。最后，涉及两个独立的s型层，将两个方向上的热图归一化为[0,1]。这样，就可以在两个正交方向上检测到篡改区域，并在两个不同的热图中用等高线点表示，其中任何一个热图都只响应特定方向上的纹理特征。<br/><br/>  由于在两个正交方向上考虑响应值可以有效地抑制假阳性预测，因此通过点重评分算法进一步处理来自VTP的两个热图。具体地说，通过NMS对不同热图中的点进行直接处理，以实现紧密的表示。然后，为了抑制具有强单向或弱正交响应的预测，我们只选择在两个热图中具有不同响应的点作为候选点。最后，篡改区域可以用由这些高质量轮廓点组成的多边形表示。</p><h2 id="最优化">最优化</h2><p>  如上所述，我们的网络包括多任务任务。因此，我们计算了以下组件的损失函数：<span class="math display">\[L=L_{rpn}+\lambda_{1}\cdotL_{cls}+\lambda_{2}\cdot L_{mask}+\lambda_{3}\cdotL_{qts}+\lambda_{4}\cdot L_{CR}\]</span>  其中，Lrpn、Lcls和Lmask是来自MaskR-CNN的标准损失。Lgts用于优化篡改区域检测，定义为： <spanclass="math display">\[L_{gts}=\frac{1}{N}\sum_i-\log\left(\frac{e^{p_i}}{\sum_je^{p_j}}\right)\]</span>  Lgts是Softmax损失，其中p是网络的输出预测。<br/><br/>  LCR用于优化CatmullRom样条检测的ft，定义为：<span class="math display">\[L_{CR}=L_{ctr}+L_{bias}\]</span>  Lctr和Lbias均为FCOS损失（Tian等人，2019年）。前者用于优化从CatmullRom控制点中心的距离损失，而这些控制点到中心的偏移距离受后者的限制。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><h3 id="预训练数据"><strong>预训练数据</strong></h3><p>我们创建了一个规模可观的图像篡改数据集，并使用它来预训练我们的模型。<br/>该数据集包括三类：1）拼接，2）复制移动，和3）删除。</p><h3 id="测试数据集">测试数据集</h3><p>如下（Wang等2022；胡等2020），我们在 CASIA (Dong, Wang, and Tan2013), Columbia (Hsu and Chang 2006), NIST16(Guan et al. 2019), COVER(Wen et al. 2016)上评估我们的模型。</p><h3 id="评价指标">评价指标</h3><p>为了量化定位性能，根据之前的工作（Hu et al.2020），我们在操作掩模上使用像素级的曲线下面积（AUC）和F1分数。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h3 id="实施细节">实施细节</h3><p>输入图像的大小被调整为512×512。在这项工作中，骨干网络是ResNet-50，在ImageNet上进行了预训练。由PyTorch实现，我们的模型使用GeForce GTX3090进行训练，使用Adam作为优化器。</p><h2 id="与sota方法的比较">与SOTA方法的比较</h2><p>遵循经典方法（Hu et al. 2020；Wang et al.2022），我们的模型与其他最先进的篡改定位方法在两种设置下进行了比较：1)训练合成数据集和评估完整的测试数据集，2)调整预训练模型对测试数据集的训练分割和评估它们的测试分割。预先训练的模型将演示每种方法的通用性，fne调整模型将演示一旦域差异显著减少，每种方法在局部的表现如何。</p><h3 id="预训练模型">预训练模型</h3><p>表1显示了不同SOTA方法的预训练模型在像素级AUC下的fve数据集上的定位性能。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823163003537.png"alt="image-20240823163003537" /><figcaption aria-hidden="true">image-20240823163003537</figcaption></figure><p>我们的CSR-Net在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia排名第二。特别是在复制-移动数据集(COVER)上达到了94.4%，其图像伪造区域与背景难以区分。这验证了我们的模型具有抑制FPs和生成更准确的边缘的优越能力。然而，我们未能在Columbia上取得最好的表现，AUC得分比PSCCNet低1.4%。我们推测，原因可能是他们（PSCCNet）合成的训练数据的分布与Columbia数据集非常相似。表2中的结果进一步支持了这一点，这说明CSR-Net在AUC和F1得分上都优于PSCCNet。此外，值得指出的是，我们在较少的预训练数据下取得了不错的结果。</p><h3 id="微调模型">微调模型</h3><p>预训练模型的网络权值用于启动调优模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练部分上进行训练。我们在表2中评估了不同方法的微调模型。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210155557.png"alt="image-20240823210155557" /><figcaption aria-hidden="true">image-20240823210155557</figcaption></figure><p>对于AUC和F1，我们的模型取得了显著的性能提高。这表明CRA模块有效地抑制了假阳性错误实例，提高了VTP预测区域位置和边界的准确性。</p><p>在综合表1和表2中的数据后，我们的方法证明了为像素级任务引入回归方法是有效的，这是在引言中提到的。</p><h3 id="消融分析">消融分析</h3><p>  在本节中，我们将进行实验来证明我们所提出的CSR-Net方法的有效性。与传统的回归方法相比，正式引入了基于CatmullRom样条的回归（CSR）来更好地描述被篡改的区域。综合评分算法（CRA）的目标是选择分类得分高、实例得分高的期望区域，而垂直纹理交互感知（VTP）则采用水平和垂直两种方式对纹理特征进行建模，以参考目标区域。为了进一步评估CSR、CRA和VTP的有效性，我们分别删除它们，并验证它们在CASIA和NIST16数据集上的伪造定位性能。表3显示定量结果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210814928.png"alt="image-20240823210814928" /><figcaption aria-hidden="true">image-20240823210814928</figcaption></figure><p>  基线(I)表示我们只使用传统的回归方法（Li et al.2022）。在接下来的消融实验中，我们可以推断，当不涉及VTP时，CASIA的F1评分下降了1.9%，NIST16的评分下降了1.7%。而在没有CRA时，AUC评分比（IV）下降更多。然而，当CRA不可用时，在（II）中可以观察到明显的性能下降，即AUC为12.3%，F1为11.2%。<br/><br/>  在图7中，我们展示了CatmullRomGround Truth生成中参数<spanclass="math inline">\(\tau\)</span>的不同值，以验证对自然图像数据集（即CASIA）和社交媒体数据集（即RIFL21）的各自预测效果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211034561.png"alt="image-20240823211034561" /><figcaption aria-hidden="true">image-20240823211034561</figcaption></figure><p>  直观地看，随着<spanclass="math inline">\(\tau\)</span>的逐渐增加，不在不同数据集中，拟合的CatmullRom控制点与具有掩模水平的地面真值之间的欧几里德距离逐渐减小，显示出更好的拟合。然而，当<spanclass="math inline">\(\tau\)</span>超过16时，欧氏距离反而呈现出扩张的趋势，这意味着拟合效果可能会减少。显然，<spanclass="math inline">\(\tau =16\)</span>是生成基于CatmullRom的最佳GroundTruth的最好选择。</p><h2 id="可视化结果">可视化结果</h2><h3 id="定性结果">定性结果</h3><p>  我们在图6中提供了不同方法的预测伪造掩模。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211647860.png"alt="image-20240823211647860" /><figcaption aria-hidden="true">image-20240823211647860</figcaption></figure><p>图6：通过不同的方法可视化预测的操作掩模。从左到右，我们展示了伪造的图像，对ManTra-Net、SPAN、PSCCNet、TruFor、我们的预测和GT掩模。</p><p>  由于 ObjectFormer（Wang et al.2022）的源代码不可用，因此他们的预测不可用。与最先进的方法相比，我们的CSR-Net在抑制假阳性方面取得了更好的性能，无论是在抑制还是在更准确的篡改区域边界方面。我们有理由相信，改善受益于CRA和VTP。CRA能够更全面地考虑每个可能的区域，确定被篡改区域和真实区域之间的细微差别，而VTP同时通过两种正交方法建模纹理边界，以准确描述目标区域。</p><h3 id="不同的基于样条曲线的回归">不同的基于样条曲线的回归</h3><p>  有许多类型的插值函数，经典的CatmullRom样条和贝塞尔曲线，前者是一个插值样条函数，精确插值一组已知数据点通过使用一系列的节点，而后者是一个近似样条函数，近似一组数据点使用节点。IFL的数据集来自自然图像和社交媒体，被篡改的区域共享不同的形状。通过比较实验，我们发现CutmullRom样条更适合于具有不同曲率的数据集（如IFL），而基于贝塞尔曲线的方法有时容易受到其他目标的干扰。详情请参见图8。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211913581.png"alt="image-20240823211913581" /><figcaption aria-hidden="true">image-20240823211913581</figcaption></figure><p>图8：通过不同的回归可视化结果。从左到右，我们展示了伪造的图像，不同的基于样条曲线的回归结果（左边显示了信任分数，而右边是预测的操作掩模），GT掩模。由于空间限制，请放大以更好地可视化。</p><h1 id="结论">结论</h1><p>  在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），以分类评分和实例评分来区分精确的篡改区域。此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（优秀代码鉴赏）(施工中)</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>2024华为软件精英挑战赛决赛代码（初赛粤港澳赛区冠军，复赛粤港澳赛区冠军，全球总决赛第4 名）)<ahref="https://github.com/OrangeQi-CQ/HuaweiCodeCraft2024-Final"><imgsrc="https://img.shields.io/github/stars/OrangeQi-CQ/HuaweiCodeCraft2024-Final?style=flat"alt="GitHub" /></a></p><p>这是 2024 华为软件精英挑战赛<strong>“适可而止矣，涓埃之事，亦央原神”</strong> 队的决赛代码。</p><p>我们属于<strong>粤港澳赛区</strong>，三名队员（<ahref="https://github.com/OrangeQi-CQ">cq</a>、<ahref="https://github.com/yhf4aspe">yhf</a>、<ahref="https://github.com/zzwtx">xsf</a>）都是来自<strong>华南理工大学</strong>的本科生。在2024华为软件精英挑战赛中成绩如下：</p><ul><li><p>初赛：<strong>粤港澳赛区冠军</strong></p></li><li><p>复赛：<strong>粤港澳赛区冠军、全国第 2 名</strong></p></li><li><p>决赛：<strong>全球总决赛第 4 名（季军/三等奖）</strong></p></li></ul><hr /><p>成功之处：</p><ul><li>我们的代码实现能力比较强，能够高效准确地将想法落地并测试效果。有很多想法预期效果很好但实际徒劳无功甚至负作用，而个别想法看似普通却会有很惊喜的效果。我认为将idea 快速落地并测试是在华为软挑取得好成绩的关键。</li><li>有一点点算法基本功（三人都有 icpc/ccpc银），但相比其他一些队伍并不亮眼。</li><li>虽然没有单元测试，但编写了很多集成测试，帮助我们迅速定位没有正常达到目标的模块。</li><li>临时抱佛脚学习了 git（之前只会用 zip压缩+微信传代码）、cmake、clang-format 等工具，并写了一些 python 和shell 脚本。利用工具提升效率。</li></ul><p>不足之处：</p><ul><li>三人都没有大厂实习经验，缺乏项目合作开发的流程。例如没有需求和交付的流程和文档，git分支混乱，git 流程不规范，缺乏设计模式的使用等。</li><li>在决赛中，策略过于保守（意图避免出大错，也不算是一件坏事）。</li></ul><hr /><p>main.cpp</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifdef USE_MFMC</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE USE_MFMC&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef AVOID_SWING</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE AVOID_SWING&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef RAND</span><br><span class="line">    unsigned rand_seed = static_cast&lt;unsigned&gt;(time(nullptr));</span><br><span class="line">    srand(rand_seed);</span><br><span class="line">    std::cerr &lt;&lt; &quot;srand = &quot; &lt;&lt; rand_seed &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef LOCAL</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE LOCAL&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef DEBUG</span><br><span class="line">    fprintf(stderr, &quot;DEFIND DEBUG\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#if !defined(_WIN32) &amp;&amp; !defined(_WIN64)</span><br><span class="line">    // 将 main 线程绑定到 cpu 0 上</span><br><span class="line">    pthread_t main_thread_id = pthread_self();</span><br><span class="line">    cpu_set_t cpu_set;</span><br><span class="line">    CPU_ZERO(&amp;cpu_set);</span><br><span class="line">    CPU_SET(0, &amp;cpu_set);</span><br><span class="line">    pthread_setaffinity_np(main_thread_id, sizeof(cpu_set), &amp;cpu_set);</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;Set CPU!\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    Init();     // 初始化</span><br><span class="line">    Control();  // 控制所有帧</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFL-Net:Image Forgery Localization Using Contrastive Learning</title>
      <link href="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/"/>
      <url>/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>CFL-Net: Image Forgery Localization Using Contrastive Learning</p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/2210.02182v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/><a href="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></p><h2 id="摘要">摘要</h2><p>  传统的伪造定位方法的缺点是过度拟合和只关注少数特定的伪造痕迹。我们需要一种更通用的图像伪造定位方法，能够很好地适应各种伪造条件。底层伪造区域定位的一个关键假设是，无论伪造类型如何，每个伪造图像样本中未被篡改和被篡改区域的特征分布都存在差异。在本文中，我们的目标是利用这种差异特征分布来帮助图像伪造定位。</p><p>  具体来说，我们使用对比损耗来学习映射到一个特征空间，在该空间中，每个图像的未篡改区域和被篡改区域之间的特征被很好地分离。此外，该方法不需要对伪造类型进行任何先验知识或假设，就可以对伪造区域进行局部定位。我们证明，我们的工作优于几个现有的方法在三个基准的图像处理数据集。</p><h2 id="引言">引言</h2><p>  交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><p>  考虑到这些局限性，我们在最近提出的对比损失的基础上，提出了一种新的伪造定位方法，称为对比伪造定位网络CFL-Net。我们的方法依赖于底层伪造区域定位的一般假设，即无论伪造类型如何，未被篡改区域和被篡改区域之间的特征统计量仍存在差异，即颜色、强度、噪声等。在本文中，我们着重于利用特征空间中的这种差异，通过对比损失来帮助图像伪造定位。具体来说，我们的模型学习映射到一个特征空间，在这个空间中，每个图像中未被篡改和被篡改区域之间的特征被很好地分离和分散。因此，我们的方法并不专注于特定的伪造线索。此外，我们还计算了每个样品的对比损失。因此，我们的方法对每个样本的伪造线索进行了不同的处理，这有助于归纳。</p><p>  因此，我们的方法对每个样本的伪造线索处理不同，有助于泛化。我们的主要贡献总结如下：</p><p>  1.提出了一种新的图像伪造定位方法，称为CFL-Net。利用了每个图像样本中未被篡改和被篡改区域之间特征分布的差异，而不关注特定的伪造足迹。因此，我们的方法更适合于检测真实生活中的伪造。</p><p>  2.解决了在无任何约束的情况下使用交叉熵损失进行通用图像伪造定位的问题。我们将对比损失纳入其中，并针对这个问题进行调整。</p><p>  3.我们在基准操作数据集上进行了大量的实验，表明我们的方法优于现有的几种图像伪造定位方法。</p><h2 id="网络框架">网络框架</h2><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/archnew.png"alt="CFL-Net" /><figcaption aria-hidden="true">CFL-Net</figcaption></figure><p>图3：对比学习模块：为了便于可视化，图中的投影头输出了一个形状为256×8×8的特征图F。然后将特征图分成4个×4个补丁。然后，对每个补丁中的4个空间向量进行平均，得到大小为4×4的嵌入（图中表示为‘k×k带标签的嵌入’）。groundtruth掩码也被划分为4×4个补丁，每个补丁中计数出出现的最大像素标签，得到输出的4×4掩码（图中表示为“k×k掩码”）。</p><p>  我们使用了两个流编码器，一个用于RGB输入图像，另一个用于SRM滤波图像。由编码器产生的特性被融合并传递到ASPP模块中。来自ASPP块的输出特征然后通过分割头和Projection头，其中第一个产生最终的预测掩模，后者产生进入对比学习模块的特征。</p><p>  我们使用SRM过滤器，并使用它作为其他流的输入。SRM滤波器是一种高通滤波器，它增强了输入图像的高频信息，从而更突出边缘信息，有利于篡改的定位。我们使用ResNet作为这两个流的Backbone。然后，我们通过将特性通道连接起来，融合两个流中的特性。融合特征图采用ASPP模块，提取多尺度信息。全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过在不同尺度上提取信息来帮助实现这一点，比如全局上下文以及更细粒度的像素级上下文信息变得可用。</p><p>  然后我们使用分割头和Projection头，将ASPP模块提取的上采样多尺度特征作为输入。我们选择一个DeepLab风格的分割头，输出大小为H×W的最终分割图。投影图由Conv-BatchNorm-Conv层构成，该层将特征图投影到<spanclass="math inline">\(F∈R^{256×H×W}\)</span>​​,256为嵌入维数。将嵌入的特征图F传递给对比学习模块。评估时不使用投影头。</p><h3 id="aspp模块">ASPP模块</h3><p>  在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><h3 id="对比学习模块">对比学习模块</h3><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410174105358.png"alt="image-20240410174105358" /><figcaption aria-hidden="true">image-20240410174105358</figcaption></figure><p>  由于我们的嵌入式特征图在空间上的大小是H×W，并且我们有相应的、大小相似的真实掩模M，所以我们知道每个像素嵌入的标签。因此，我们可以使用有监督的对比学习。对于每个查询像素嵌入<spanclass="math inline">\(z_i\)</span>​，该嵌入的对比损失变为： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(z_i\cdot k^+/ \tau)}{exp(z_i\cdot k^+/\tau)+\sum_{k^-}exp(z_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(k^+\)</span>是一个嵌入与查询<spanclass="math inline">\(z_i\)</span>具有相同标签的像素。<spanclass="math inline">\(A_i\)</span>表示投影头输出特征图<spanclass="math inline">\(F\)</span>中所有<spanclass="math inline">\(k_+\)</span>的集合。同样，<spanclass="math inline">\(k_-\)</span>是<spanclass="math inline">\(F\)</span>中与<spanclass="math inline">\(z_i\)</span>不同标签的像素嵌入。</p><p>  然而，用这种方式计算<spanclass="math inline">\(L_i\)</span>有一些主要的局限性。首先，基于单像素嵌入的对比损失没有考虑相邻嵌入的上下文信息。此外，为了计算损失，需要存储大小为HW×HW的点积矩阵，这很消耗内存。因此，为了在上下文和细粒度轨迹之间找到平衡，我们选择将F划分为局部区域。</p><p>  我们首先将<spanclass="math inline">\(F∈R^{256×H×W}\)</span>在空间上划分为k×k个块，从而得到<spanclass="math inline">\(f_i\in R^{256\times h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个<spanclass="math inline">\(f_i\)</span>都变成了<spanclass="math inline">\(R^{256}\)</span>的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到<spanclass="math inline">\(m_i\in R^{h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。为了得到每个<spanclass="math inline">\(m_i\)</span>的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为<spanclass="math inline">\(m_i\)</span>的值。</p><p>  然后，我们有了像素嵌入<spanclass="math inline">\(f_i\)</span>和每个嵌入<spanclass="math inline">\(m_i\)</span>​​​的相应标签。我们现在得到监督对比损失：<span class="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(A_i\)</span>表示与<spanclass="math inline">\(f_i\)</span>具有相同标签的所有其他像素嵌入<spanclass="math inline">\(k^+\)</span>的集合。类似地，<spanclass="math inline">\(k^−\)</span>是所有与<spanclass="math inline">\(f_i\)</span>有不同标签的负像素嵌入。损失函数中的所有嵌入都是<spanclass="math inline">\(L_2\)</span>归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span>   最后要优化的损失是： <spanclass="math display">\[L=L_{CE}+L_{CON}\]</span>   其中<spanclass="math inline">\(L_{CE}\)</span>是交叉熵损失。</p><h2 id="实验">实验</h2><p>  在本节中，我们将描述在三个不同的操作数据集上进行的实验，以探索CFL-Net的有效性。这些数据集是包含几种操作类型的通用操作数据集，并不只特定于一种操作类型。我们使用的评估度量是像素级的曲线下面积（AUC）评分。</p><p>  我们使用ResNet-50作为这两个流的编码器。我们用Adam优化器训练CFL-Net，学习率为1e-4。每过了20个epochs，我们就会将学习率降低20%。我们将输入图像的大小调整为256×256。我们将F划分为总共64个×64个块。温度系数<spanclass="math inline">\(\tau\)</span>​设置为0.1。对交叉熵损失进行加权，使被篡改类的权重增加十倍。我们将批处理大小设置为4，并在NVIDIARTX Titan GPU上训练模型超过100个epochs。</p><h3 id="与各种baseline模型的比较">与各种baseline模型的比较：</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410194714118.png" alt="image-20240410194714118 " style="zoom:50%;" /></p><p>  我们在表1中报告了我们的方法和基线模型的AUC评分（%）。需要注意的是，这里陈述的RGB-N和SPAN的结果是它们在各自的论文中报道的精细结果。JLSTM和Transforensics不进行任何预训练。虽然ManTraNet在合成操作数据集上对它们的模型进行了预训练，但它们并没有对特定的数据集进行微调。从表格中可以看出，CFLNet在基线模型中的所有数据集上都取得了最好的定位性能。特别是，CFLNet在IMD-20数据集上的性能大大优于所有的基线模型，而IMD-20数据集是一个具有各种伪造类型的真实操作数据集。具体来说，CFL-Net在IMD-20数据集上获得了89.9%的AUC分数，比性能第二良好的模型Transforensics提高了5.1%。</p><p>  因此，它验证了我们的主张，即cfll-net非常适合于本地化真实生活中的伪造品。我们的模型在其他数据集上也优于基线模型——Casia和Nist。此外，值得指出的是，CFLNet在没有对合成操作数据进行预训练的情况下就实现了这些结果。</p><h3 id="实验有无对比损失">实验（有无对比损失）</h3><p><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195126470.png"alt="image-20240410195126470" /><br/>表2：最左边的一列显示了所训练的数据集模型。后面的列是评估模型的数据集。“w/o”训练没有对比损失，“w”训练有对比损失。结果以%AUC表示。</p><p>  表2显示了结果。很明显，用对比损失训练的CFL-Net在跨数据集的推广方面表现得非常好。在所有情况下，该模型比没有对比损失的模型表现得更好。当在IMD-20上进行训练并在NIST的测试集上进行评估时，我们提出的模型甚至优于ManTraNet的AUC分数。当在IMD-20数据集上进行训练时，可以看到的性能提高最高。IMD-20是真实的图像操作数据集，因此在这个数据集上进行训练有助于模型学习最一般化的特征。因此，我们提出的在IMD-20上训练并在其他数据集上进行评估的模型，与在没有对比损失的情况下训练的模型相比，产生了最大的性能改进。<br/>  还需要注意的是，在NIST上训练和在其他数据集上评估的两种模型的表现都很差，因为NIST拥有的图像很少，即数据集中有584张图像。因此，很难使用NIST来推广到其他数据集。尽管如此，我们提出的模型还是比训练后的没有对比损失的模型表现得更好。</p><h3 id="定性分析">定性分析</h3><p>  为了证明我们的对比损失通过避免同一类特征的聚类来保持特征的变化，我们通过t-SNE将从图5中分割头获得的类特征可视化。</p><p><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195906151.png"alt="image-20240410195906151" /><br/>图5：左栏显示了当仅使用交叉熵损失训练CFL-Net时，IMD-20和CASIA测试集上的平均特征的t-SNE图。右列对应于使用交叉熵损失和对比损失训练的CFL-Net。绿色=未篡改特征，红色=篡改特征。</p><p>  左列显示了当仅使用CFL-Net训练交叉熵损失时，IMD-20和CASIA测试集上每个图像样本的平均特征向量。很明显，未被篡改（图中绿色）和被篡改（图中红色）区域对应的特征在这里是堵塞的。与此同时，右栏显示了同时使用交叉熵和对比损失进行CFL-Net训练时的平均特征。这两个区域对应的特征更加分散。<br/>  因此，不同的操作足迹更容易可分离。实验表明，传统的交叉熵损失由于类别内不变性而减少了图像伪造定位的泛化，而我们提出的方法可以通过分散特征分布来提高泛化效果。</p><h3 id="消融实验损失函数变化">消融实验（损失函数变化）</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410200342409.png" alt="image-20240410200342409 " style="zoom:50%;" /></p><p>  在表3中，我们报告了结果。从表中可以清楚地看出，添加对比损失确实有助于定位。这种改进在真实的图像处理数据集IMD-20上更为突出。对比损失有助于提高AUC评分4.7%。需要注意的是，在没有对比损失的情况下，我们的方法已经取得了很好的结果。</p><h2 id="结论">结论</h2><p>  在本文中，我们从一个新的角度来研究通用的图像伪造定位问题。我们发现了现有方法的一个主要缺点，该方法关注特定的伪造足迹，并使用没有任何约束的交叉熵损失来定位伪造。为了解决这一缺点，我们补充了交叉熵损失和对比损失，并提出了一种新的图像伪造定位方法，即对比伪造定位网络CFL-Net。我们在三个基准图像处理数据集上进行了实验，并将实验结果与近年来的主要伪造定位方法进行了比较。CFL-Net在AUC度量方面优于所有方法。此外，在现实生活中的图像处理数据集IMD-2020上的改进更为突出。在未来的工作中，可以考虑一个更复杂的融合机制来融合来自RGB和SRM流的特征映射。例如，注意模块或最近提出的视觉变压器可以被用作一种融合机制。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文合集</title>
      <link href="/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/"/>
      <url>/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<hr /><p><ahref="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/">Attentiveand Contrastive Image Manipulation Localization With BoundaryGuidance</a>(TIFS24)</p><p><em>现有问题</em>：在被操纵的图像中，被篡改区域的边界是分离被操纵和未被操纵像素的关键位置，在定位被操纵区域时应特别注意并明确利用这一点。然而，如何利用这些边界信息来提高检测被操纵图像区域的性能仍有待探索。。</p><p><em>解决方案</em>：提出了一种新的边界引导图像操纵定位模型，该模型通过精心设计的注意力和对比学习机制充分利用被篡改区域的边界信息，在框架的解码器中引入了一个边界感知注意模块，旨在指导模型通过提取被操纵区域的边界来强调图像操作的非自然混合，我们提出了一种边界引导的篡改对比损失，鼓励模型将样本的边缘从篡改和非篡改区域扩大到最大的程度。</p><details close><br/><summary>具体情况</summary><p>其网路架构如下：</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png"alt="image-20240910222829501" /><figcaption aria-hidden="true">image-20240910222829501</figcaption></figure><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure></details><hr /><p><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image ManipulationDetection</a>(AAAI24)<br/><em>现有问题</em>：</p><ul><li>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。</li><li>基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</li></ul><p><em>解决方案</em>：包含RGB和频率特征的双分支架构，能够检测双压缩伪影的压缩伪影学习模型。</p><details close><br/><summary>具体情况</summary><blockquote><p>RGB和频率特征的双分支架构</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><blockquote><p>双压缩伪影的压缩伪影学习模型</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /><figcaption aria-hidden="true">image-20240326170313123</figcaption></figure></details><hr /><p><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -所有现有的IMD主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p><em>解决方案</em>：一种基于掩码引导查询的转换器框架（MGQFormer），该框架使用基本事实掩码来引导可学习查询令牌（LQT）识别伪造区域。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png"alt="image-20240401164940206" /><figcaption aria-hidden="true">image-20240401164940206</figcaption></figure><p>  利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征，过空间和通道注意模块（SCAM,spatialand channel attentionmodule）对多模态特征进行融合。其特征提取器如下:</p><p><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822164324909.png"alt="image-20240822164324909" /><br/>  我们设计了两个可学习的查询token来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。其解码器如下:</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><p>  具体来说，我们将噪声的GT掩模输入MGQFrorer，以获得引导查询token（GQT)和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。</p><p><em>解决方案</em>：通过关注噪声域内的操纵痕迹来检测和定位图像伪造，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><p>一阶段：</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><br/>为了明确地分离出这两个区域（真实的和伪造的)的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $ 的噪声。</p><figure><imgsrc="./../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822220206200.png"alt="image-20240822220206200" /><figcaption aria-hidden="true">image-20240822220206200</figcaption></figure><p>式中， $ _a $ 、 $ _f $ 为 $ N_a $ 和 $ N_f $ 的标准差， $ _a $ 、 $_f $ 为 $ N_a $ 和 $ N_f $ 的平均值。</p><p><spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>二阶段：</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>利用两个分支来处理RGB和噪声信息，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。</p></details><hr /><p><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a>(AAAI24)</p><p><em>现有问题</em>： 假阳性（FPs）和不准确的边界。</p><p><em>解决方案</em>：基于CatmullRom样条的回归网络（CSR-Net, CatmullRomSplines-based RegressionNetwork），首次尝试将回归方法引入像素级任务。为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。</p><p>详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），我们为每个区域实例重新分配分数，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。</p><p>此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p></details><hr /><p><ahref="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/">Multi-viewFeature Extraction via Tunable Prompts is Enough for Image ManipulationLocalization</a>(ACMMM24)</p><p><em>现有问题</em>：IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p><em>解决方案</em>：提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。</p><details close><br/><summary>具体情况</summary><blockquote><p>通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><blockquote><p>特征对齐和融合的FAF模块</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824220942200.png"alt="image-20240824220942200" /><figcaption aria-hidden="true">image-20240824220942200</figcaption></figure></details><hr /><p><a href="/UnionFormer/">UnionFormer: Unified-Learning Transformerwith Multi-View Representation for Image Manipulation Detection andLocalization</a>(CVPR24)</p><p><em>现有问题</em>：以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层的特征，不能充分表示篡改痕迹；目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。</p><p><em>解决方案</em>：设计了专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork）设计了用于图像操作检测和定位的多视图表示的统一学习transformer框架</p><details close><br/><summary>具体情况</summary><blockquote><p>cnn-Transformer并发网络BSFI-Net，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><figcaption aria-hidden="true">image-20240617110632461</figcaption></figure><blockquote><p>采用对比监督来促进两个视图之间的协作</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><blockquote><p>统一伪造判别表示，每个篡改判别查询都表示对应建议的三个视图中的篡改线索</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>：篡改痕迹主要来源于真实区域和伪造区域的噪声分布则不一致性。</p><p><em>解决方案</em>：使用两阶段训练方法：第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><blockquote><p>第一阶段，训练一个特征提取器，使用的是DNE，一个盲去噪网络，使用JS散度来约束Gd，将Gd划分为真实区域Na和伪造区域Nf，将其视为两个高斯分布</p></blockquote><p><span class="math display">\[JSD=\\log\\frac{\\sqrt{\\sigma_{a}^{2}+\\sigma_{f}^{2}}}{2}-\\frac{\\log\\sigma_{a}+\\log\\sigma_{f}}{2}+\\frac{(\\mu_{a}-\\mu_{f})^{2}}{\\sigma_{a}^{2}+\\sigma_{f}^{2}}+\\frac{1}{2}\]</span></p><p><span class="math display">\[\\mathbf{L_{n}}=\\lambda\\left(1-JSD\\right)+\\left(1-\\lambda\\right)\\mathcal{L}\\left(Y,G_{c}\\right)\]</span></p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><blockquote><p>第二阶段，使用一阶段训练好的特征提取器DNE，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。</p></blockquote><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure></details><hr /><p>CVPR '23:</p><p><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a></p><p>ICCV '23:</p><p><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>WACV' 23:</p><p><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> (<em>WACV'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2210.02182"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/niloy193/CFLNet"><strong>Code</strong></a><strong>]</strong></p><p>TPAMI '22:</p><p><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a></p><details close><br/><summary>CLIP</summary><p><br/>原论文：<br/><ahref="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/">LearningTransferable Visual Models From Natural Language Supervision</a></p>Coop：<br/><a href="/Prompt-Learning-for-Vision-Language-Models/">PromptLearning for Vision Language Models</a><br/></details><details close><br/><summary>HRnet</summary><p><br/>原论文：<br/><a href="/HRnet/">Deep High-ResolutionRepresentation Learning for Human Pose Estimation</a></p></details><details close><br/><summary>自监督学习——对比学习</summary><p><a href="">SimCLR</a></p><p>https://proceedings.mlr.press/v119/chen20j.html</p><p>https://github.com/google-research/simclr</p><p>https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning</p><p>本文介绍了SimCLR：一个用于视觉表征对比学习的简单框架。我们简化了最近提出的对比自监督学习算法，而不需要专门的架构或内存库。为了理解是什么使对比预测任务能够学习有用的表征，我们系统地研究了我们框架的主要组成部分。我们表明：（1）数据增强的组成在定义有效的预测任务中起着关键作用；（2）在表征和对比损失之间引入可学习的非线性变换，大大提高了学习表征的质量；（3）与监督学习相比，对比学习受益于更大的批量和更多的训练步骤。通过结合这些发现，我们能够在ImageNet上大大优于以前的自监督和半监督学习方法。在SimCLR学习的自监督表示上训练的线性分类器实现了76.5%的top-1准确率，这比以前的最先进技术提高了7%，与监督ResNet-50的性能相匹配。当只对1%的标签进行微调时，我们实现了85.8%的前五名准确率，比AlexNet少了100倍的标签。</p><p><a href="">Matrix Information Theory for Self-SupervisedLearning</a></p><p>https://paperswithcode.com/paper/kernel-ssl-kernel-kl-divergence-for-self</p></details><details close><summary>监督对比学习</summary><blockquote><p><ahref="https://paperswithcode.com/paper/supervised-contrastive-learning">SupervisedContrastive Learning | Papers With Code</a><br/>&gt; <br/>&gt; <ahref="https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html">SupervisedContrastive Learning NeurIPS 2020</a></p></blockquote></details><details close><summary>arXiv</summary><p><br/><a href="/FOCAL/">Rethinking Image Forgery Detection viaContrastive Learning and Unsupervised Clustering</a></p><p>图像伪造检测旨在检测和定位图像中的伪造区域。大多数现有的伪造检测算法都提出了将像素分类为伪造或原始的分类问题。然而，伪造像素和原始像素的定义仅在单个图像内是相对的，例如，图像a中的伪造区域实际上是其源图像B中的原始区域（拼接伪造）。现有的方法严重忽视了这种相对定义，不必要地将不同图像中的伪造（原始）区域混合到同一类别中。为了解决这一困境，我们提出了模糊控制聚类（FOCAL）方法，这是一种新的、简单但非常有效的基于对比学习和无监督聚类的图像伪造检测方法。</p>具体而言，FOCAL1）利用像素级对比学习，以逐图像的方式监督高级取证特征提取，明确地反映了上述相对定义；2）采用动态无监督聚类算法（而不是经过训练的算法）将学习到的特征聚类为伪造/原始类别，进一步抑制了训练数据对跨图像的影响；以及3）允许在不需要重新训练的情况下通过简单的特征级级联来进一步提高检测性能。<br/></details><details close><summary>论文（c）</summary><p><br/>MMM '24：<br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection andLocalization</a></p><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p></details>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HRnet</title>
      <link href="/HRnet/"/>
      <url>/HRnet/</url>
      
        <content type="html"><![CDATA[<p>Deep High-Resolution Representation Learning for Human PoseEstimation<a href="https://arxiv.org/abs/1902.09212"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><ahref="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"><imgsrc="https://img.shields.io/github/stars/leoxiaobin/deep-high-resolution-net.pytorch?style=flat"alt="GitHub" /></a></p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/hrnet.png"alt="Illustrating the architecture of the proposed HRNet" /><figcaption aria-hidden="true">Illustrating the architecture of theproposed HRNet</figcaption></figure><p>算法流程如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205309290.png"alt="architecture" /><figcaption aria-hidden="true">architecture</figcaption></figure><p>其中Bottleneck的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205400578.png"alt="Bottleneck" /><figcaption aria-hidden="true">Bottleneck</figcaption></figure><p>stage：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205432768.png"alt="stage2" /><figcaption aria-hidden="true">stage2</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205452973.png"alt="stage3" /><figcaption aria-hidden="true">stage3</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205529301.png"alt="stage4" /><figcaption aria-hidden="true">stage4</figcaption></figure><p>其中BasicBlock的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205604131.png"alt="BasicBlock" /><figcaption aria-hidden="true">BasicBlock</figcaption></figure><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details>]]></content>
      
      
      
        <tags>
            
            <tag> hrnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征融合、特征采样方法合集</title>
      <link href="/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/"/>
      <url>/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="swim-transformer">Swim Transformer:</h1><p>PatchMerging类 PatchMerging 类是 Swin Transformer架构中用于降低特征图分辨率的层。这个过程通过合并相邻的patch来减少序列长度，同时增加通道数，以保持信息的密度。</p><p>每执行一个stage后，都会执行一个一个下采样操作，也就是PatchMerging类的前向传播。所谓的下采样操作，主要是把x切片成4个，<spanclass="math inline">\(x_0\)</span>、<spanclass="math inline">\(x_1\)</span>、<spanclass="math inline">\(x_2\)</span>、<spanclass="math inline">\(x_3\)</span>这四个是按照长宽间隔去选的：</p><figure><imgsrc="../postimages/特征融合、特征采样方法合集/f34d8f6a311047b18d5e32e49a827f8b.png"alt="Swim Transformer" /><figcaption aria-hidden="true">Swim Transformer</figcaption></figure><p>x原来是8 ∗ 8，取完后变成了4个4 ∗4的，再把4个做一个拼接，拼接完成后再连接一个全连接，使用全连接进行降维。</p><p>构造函数：</p><ul><li>input_resolution ，dim ：输入特征的分辨率和通道数</li><li>reduction，初始化一个线性变换，用于将相邻四个patch的特征合并成一个patch</li></ul><p>前向传播：</p><p>原始输入：</p><ol type="1"><li>torch.Size([4, 3136, 96])<br/>2. H, W =56<em>56，输入特征长宽<br/>3. B, L, C =4</em>3136*96，batch，序列长度，特征维度<br/>4. x： torch.Size([4, 56,56, 96])，将输入特征重塑为四维张量，准备进行patch合并操作<br/>5. x0：torch.Size([4, 28, 28, 96])、x1： torch.Size([4, 28, 28, 96])、x2：torch.Size([4, 28, 28, 96])、x3： torch.Size([4, 28, 28,96])，提取四个相邻patch的特征，每个patch分别来自原始特征图的不同子区域<br/>6.x： torch.Size([4, 28, 28,384])，将四个patch的特征在通道维度上合并<br/>7. x： torch.Size([4, 784,384])，将合并后的特征图重塑，准备进行线性变换<br/>8. x： torch.Size([4,784, 384])，层归一化，维度不变<br/>9. x： torch.Size([4, 784,192])，通过线性变换降低合并后特征的维度，减少通道数</li></ol><details close><br/><summary>Swim Transformer code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.reduction(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br/></details><h1 id="hrnet">hrnet：</h1><p>详情<ahref="/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/">DeepHigh-Resolution Representation Learning for Human PoseEstimation</a></p><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details><h1 id="aspp">ASPP:</h1><p><a href="https://arxiv.org/abs/1706.05587">Rethinking atrousconvolution for semantic image segmentation</a></p><p>​在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><p>其中nn.Conv2d中的dilation参数含义如下：<br/>dilation = 1：</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>dilation=2:</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16-1712826246932-3.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><details close><br/><summary>ASPP code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, kernel_size, padding, dilation, BatchNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(_ASPPModule, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,</span><br><span class="line">                                            stride=<span class="number">1</span>, padding=padding, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn = BatchNorm(planes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.atrous_conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_stride == <span class="number">16</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>]</span><br><span class="line">        <span class="keyword">elif</span> output_stride == <span class="number">8</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">36</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.aspp1 = _ASPPModule(inplanes, outplanes, <span class="number">1</span>, padding=<span class="number">0</span>, dilation=dilations[<span class="number">0</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp2 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">1</span>], dilation=dilations[<span class="number">1</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp3 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">2</span>], dilation=dilations[<span class="number">2</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp4 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">3</span>], dilation=dilations[<span class="number">3</span>], BatchNorm=BatchNorm)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                             nn.Conv2d(inplanes, outplanes, <span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                             BatchNorm(outplanes),</span><br><span class="line">                                             nn.ReLU())</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(outplanes*<span class="number">5</span>, outplanes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = BatchNorm(outplanes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = <span class="variable language_">self</span>.aspp1(x)</span><br><span class="line">        x2 = <span class="variable language_">self</span>.aspp2(x)</span><br><span class="line">        x3 = <span class="variable language_">self</span>.aspp3(x)</span><br><span class="line">        x4 = <span class="variable language_">self</span>.aspp4(x)</span><br><span class="line">        x5 = <span class="variable language_">self</span>.global_avg_pool(x)</span><br><span class="line">        x5 = F.interpolate(x5, size=x4.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat((x1, x2, x3, x4, x5), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                <span class="comment"># n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels</span></span><br><span class="line">                <span class="comment"># m.weight.data.normal_(0, math.sqrt(2. / n))</span></span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_aspp</span>(<span class="params">inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">    <span class="keyword">return</span> ASPP(inplanes, outplanes, output_stride, BatchNorm)</span><br></pre></td></tr></table></figure></details><h1 id="srm-滤波器">SRM 滤波器:</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/6197267">RichModels for Steganalysis of Digital Images</a></p><p>SRM滤波器可以捕捉高频篡改痕迹，使用SRM过滤器从RGB图像中提取局部噪声特征</p><p>我们的设置中，噪声是通过像素值与仅通过内插相邻像素的值而产生的该像素值的估计之间的残差来建模的。从30个基本滤波器开始，再加上非线性运算（例如，滤波后附近输出的最大值和最小值），SRM功能将收集基本噪声特征。SRM量化并截断这些滤波器的输出，并提取附近的共现信息作为最终特征。从该过程获得的特征可以被视为局部噪声描述符。我们选择3个内核，其权重如下所示，并将其直接输入经过3通道输入训练的预训练网络中。我们将噪声流中SRM滤波器层的内核大小定义为5×5×3。SRM层的输出通道大小为3。</p><p><span class="math display">\[\frac {1} {4} \begin{bmatrix}0 &amp; 0&amp; 0 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 2&amp; -4 &amp; 2 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 0&amp; 0 &amp; 0 &amp; 0\end{bmatrix}\\\frac {1} {12} \begin{bmatrix}-1&amp; 2 &amp; -2 &amp; 2 &amp; -1\\2 &amp; -6 &amp; 8 &amp; -6 &amp;2\\-2 &amp; 8 &amp; -12 &amp; 8 &amp; -2\\2 &amp; -6 &amp; 8 &amp; -6&amp; 2\\-1 &amp; 2 &amp; -2 &amp; 2 &amp; -1\end{bmatrix}\\\frac {1}{2} \begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp;0\end{bmatrix}\]</span></p><details close><br/><summary>SRM 滤波器 code 1 from CFL-Net</summary><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def setup_srm_weights(input_channels: int = 3, output_channel=1) -&gt; torch.Tensor:</span><br><span class="line">    &quot;&quot;&quot;Creates the SRM kernels for noise analysis.</span><br><span class="line">    note: values taken from Zhou et al., &quot;Learning Rich Features for Image Manipulation Detection&quot;, CVPR2018</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional):  Defaults to 3.</span><br><span class="line">        output_channel (int, optional): Defaults to 1.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.Tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    srm_kernel = torch.from_numpy(</span><br><span class="line">        np.array([</span><br><span class="line">            [  # srm 1/2 horiz</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 1., -2., 1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/4</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 2., -4., 2., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/12</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-2., 8., -12., 8., -2.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">            ]</span><br><span class="line">        ])).float()</span><br><span class="line">    srm_kernel[0] /= 2</span><br><span class="line">    srm_kernel[1] /= 4</span><br><span class="line">    srm_kernel[2] /= 12</span><br><span class="line">    return srm_kernel.view(3, 1, 5, 5).repeat(output_channel, input_channels, 1, 1)</span><br><span class="line">    </span><br><span class="line">def setup_srm_layer(input_channels: int = 3, output_channel=None) -&gt; torch.nn.Module:</span><br><span class="line">    &quot;&quot;&quot;Creates a SRM convolution layer for noise analysis.</span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional): [description]. Defaults to 3.</span><br><span class="line">        output_channel ([type], optional): [description]. Defaults to None.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.nn.Module: [description]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if output_channel == None:</span><br><span class="line">        weights = setup_srm_weights(input_channels)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)</span><br><span class="line">    else:</span><br><span class="line">        weights = setup_srm_weights(input_channels, output_channel)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels,</span><br><span class="line">                               out_channels=output_channel,</span><br><span class="line">                               kernel_size=5,</span><br><span class="line">                               stride=1,</span><br><span class="line">                               padding=2,</span><br><span class="line">                               bias=False)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        conv.weight = torch.nn.Parameter(weights, requires_grad=False)</span><br><span class="line">    return conv</span><br><span class="line">    </span><br></pre></td></tr></table></figure></details><details close><br/><summary>SRM 滤波器 code 2 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>SRM滤波器[14，66]使用预定义的核来学习中心像素的相邻像素之间不同类型的噪声残差，然后进行线性或非线性的最大/最小运算。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SRMConv2d(nn.Module):</span><br><span class="line">    def __init__(self, stride: int = 1, padding: int = 2, clip: float = 2):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.clip = clip</span><br><span class="line">        self.conv = self._get_srm_filter()</span><br><span class="line"></span><br><span class="line">    def _get_srm_filter(self):</span><br><span class="line">        filter1 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 2, -4, 2, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        filter2 = [</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-2, 8, -12, 8, -2],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">        ]</span><br><span class="line">        filter3 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 1, -2, 1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        q = [4.0, 12.0, 2.0]</span><br><span class="line">        filter1 = np.asarray(filter1, dtype=float) / q[0]</span><br><span class="line">        filter2 = np.asarray(filter2, dtype=float) / q[1]</span><br><span class="line">        filter3 = np.asarray(filter3, dtype=float) / q[2]</span><br><span class="line">        filters = [</span><br><span class="line">            [filter1, filter1, filter1],</span><br><span class="line">            [filter2, filter2, filter2],</span><br><span class="line">            [filter3, filter3, filter3],</span><br><span class="line">        ]</span><br><span class="line">        filters = torch.tensor(filters).float()</span><br><span class="line">        conv2d = nn.Conv2d(</span><br><span class="line">            3,</span><br><span class="line">            3,</span><br><span class="line">            kernel_size=5,</span><br><span class="line">            stride=self.stride,</span><br><span class="line">            padding=self.padding,</span><br><span class="line">            padding_mode=&quot;zeros&quot;,</span><br><span class="line">        )</span><br><span class="line">        conv2d.weight = nn.Parameter(filters, requires_grad=False)</span><br><span class="line">        conv2d.bias = nn.Parameter(torch.zeros_like(conv2d.bias), requires_grad=False)</span><br><span class="line">        return conv2d</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        if self.clip != 0.0:</span><br><span class="line">            x = x.clamp(-self.clip, self.clip)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    srm = SRMConv2d()</span><br><span class="line">    x = torch.rand((63, 3, 64, 64))</span><br><span class="line">    x = srm(x)</span><br></pre></td></tr></table></figure></details><h1 id="bayarconv">BayarConv:</h1><p><ahref="https://ieeexplore.ieee.org/abstract/document/8335799">BelhassenBayar and Matthew C Stamm. Constrained convolutional neural networks: Anew approach towards general purpose image manipulationdetection</a></p><p>Bayar卷积滤波器[2]通过使用可学习权重来改进SRM滤波器，约束条件是相邻像素的加权和等于中心像素的权重的负。</p><details close><br/><summary>BayarConv code 1 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from einops import rearrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BayarConv2d(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channles: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        kernel_size: int = 5,</span><br><span class="line">        stride: int = 1,</span><br><span class="line">        padding: int = 0,</span><br><span class="line">        magnitude: float = 1.0,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line">        assert kernel_size &gt; 1, &quot;Bayar conv kernel size must be greater than 1&quot;</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channles</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.magnitude = magnitude</span><br><span class="line"></span><br><span class="line">        self.center_weight = nn.Parameter(</span><br><span class="line">            torch.ones(self.in_channels, self.out_channels, 1) * -1.0 * magnitude,</span><br><span class="line">            requires_grad=False,</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight = nn.Parameter(</span><br><span class="line">            torch.rand((self.in_channels, self.out_channels, kernel_size**2 - 1)),</span><br><span class="line">            requires_grad=True,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def _constraint_weight(self):</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(2, 0, 1)</span><br><span class="line">        self.kernel_weight.data = torch.div(</span><br><span class="line">            self.kernel_weight.data, self.kernel_weight.data.sum(0)</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(1, 2, 0) * self.magnitude</span><br><span class="line">        center_idx = self.kernel_size**2 // 2</span><br><span class="line">        full_kernel = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                self.kernel_weight[:, :, :center_idx],</span><br><span class="line">                self.center_weight,</span><br><span class="line">                self.kernel_weight[:, :, center_idx:],</span><br><span class="line">            ],</span><br><span class="line">            dim=2,</span><br><span class="line">        )</span><br><span class="line">        full_kernel = rearrange(</span><br><span class="line">            full_kernel, &quot;ci co (kw kh) -&gt; ci co kw kh&quot;, kw=self.kernel_size</span><br><span class="line">        )</span><br><span class="line">        return full_kernel</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = nn.functional.conv2d(</span><br><span class="line">            x, self._constraint_weight(), stride=self.stride, padding=self.padding</span><br><span class="line">        )</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    device = &quot;cuda&quot;</span><br><span class="line">    bayer_conv2d = BayarConv2d(3, 3, 3, magnitude=1).to(device)</span><br><span class="line">    bayer_conv2d._constraint_weight()</span><br><span class="line">    i = torch.rand(16, 3, 16, 16).to(device)</span><br><span class="line">    o = bayer_conv2d(i)</span><br></pre></td></tr></table></figure></details><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% 读取图像</span><br><span class="line">grayImage = imread(&#x27;image.jpg&#x27;);</span><br><span class="line"></span><br><span class="line">% 检查图像是否为灰度图像，如果不是，转换为灰度图像</span><br><span class="line">if size(grayImage, 3) == 3</span><br><span class="line">    grayImage = rgb2gray(grayImage);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% 显示灰度图像</span><br><span class="line">imshow(grayImage);</span><br><span class="line">title(&#x27;显示灰度图像&#x27;);</span><br><span class="line"></span><br><span class="line">% 保存灰度图像</span><br><span class="line">imwrite(grayImage, &#x27;grayImage.png&#x27;);</span><br></pre></td></tr></table></figure><h1 id="cbam-convolutional-block-attention-module">CBAM: ConvolutionalBlock Attention Module:</h1><p>本文出自2018ECCV会议</p><p>论文地址：<ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.06521">https://arxiv.org/abs/1807.06521</a></p><h2 id="一计算机视觉中的注意力机制">一、计算机视觉中的注意力机制</h2><p>​  在计算机视觉中能够能够把注意力聚集在图像重要区域而丢弃掉不相关的方法被称作是<strong>注意力机制</strong>(<strong>AttentionMechanisms</strong>)。在人类视觉大脑皮层中，使用注意力机制能够更快捷和高效地分析复杂场景信息。这种机制后来被研究人员引入到计算机视觉中来提高性能。  注意力机制可以看做是对图像输入重要信息的动态选择过程，这个过程是由对于特征自适应权重实现的。注意力机制极大提升了很多计算机视觉任务性能水平，比如在分类，目标检测，语义分割，人脸识别，动作识别，小样本检测，医疗影像处理，图像生成，姿态估计，超分辨率，3D视觉以及多模态中等任务中发挥着重要作用。<br/>​  一般来说，注意力机制通常被分为以下基本四大类：</p><ul><li><strong>通道注意力 Channel Attention</strong>，告诉网络 what to payattention to</li><li><strong>空间注意力机制 Spatial Attention</strong>，告诉网络 where topay attention to</li><li><strong>时间注意力机制 Temporal Attention</strong>，告诉网络 when topay attention</li><li><strong>分支注意力机制 Branch Attention</strong>，告诉网络 which topay attention to</li></ul><p>​  最后还有两种混合注意力机制：</p><p>​    <strong>通道&amp;空间注意力机制</strong> 和<strong>空间&amp;时间注意力机制</strong>。</p><p>​  不同种类的注意力机制在不同的视觉任务中的效果是不同的。</p><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-021feae50e271854cedcfd078189bed7_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-14bb2947cc066e31a3ab653b2a8ab99e_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-fb04ed7d7293bafda456ae921af6deeb_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="二cbam-convolutional-block-attention-module">二、CBAM:Convolutional Block Attention Module</h2><p>​  本文提出的<strong>Convolutional Block AttentionModule(CBAM)</strong>就是上面所提到的两种混合注意力中<strong>通道&amp;空间注意力</strong>的一种。在给定一张特征图，CBAM模块能够序列化地在<strong>通道</strong>和<strong>空间</strong>两个维度上产生注意力特征图信息，然后两种特征图信息在与之前原输入特征图进行相乘进行自适应特征修正，产生最后的特征图。CBAM是一种轻量级的模块，可以嵌入到任何主干网络中以提高性能。</p><h3 id="设计启发">1.设计启发</h3><p>​  CNN网络由于其强大的特征表示能力极大地提高了计算机视觉任务的水平，为了进一步增强这种特征表达能力，研究者在三个主要方面对其进行了更深入地研究，分别是<strong>网络的深度</strong>，<strong>网络的宽度</strong>，<strong>网络的维度</strong>。<br/>​  在深度研究层面，像是我们熟知的最早先的网络<strong>LeNet</strong>，到深度逐渐加深的采用1x1卷积和3x3卷积不断堆叠的<strong>VGGNet</strong>，再到采用了残差设计的<strong>ResNet</strong>，证实了网络可以堆叠到几百层甚至上千层。<strong>GoogLeNet</strong>设计了<strong>Inception</strong>模块可以对网络的宽度进行研究并证明了其效果。<strong>Xception</strong>和<strong>ResNeXt</strong>提出了网络另一个维度Cardinality，证实了这个维度不仅可以节约大量参数而且展示出了比深度和宽度有更强特征表达能力的效果。<br/>​  所以在除了在以上维度对CNN的特征表达能力进行研究之外，作者对网络结构的另外一个层面进行了探索，那就是注意力机制。早前的注意力机制研究已经进行的如火如荼了。注意力机制的主要目的就是：聚焦图像的重要特征，抑制不必要的区域响应，通过在对<strong>通道维度</strong>和<strong>空间维度</strong>组合分析研究，提出了CBAM模块，并证实了网络的性能的提升来自于精确的注意力机制和对无相关噪声信息的抑制。</p><h3 id="cbam总体流程">2.CBAM总体流程</h3><p>对于网络主干生成的特征图： <span class="math display">\[F\inR^{C\times H\times W}\]</span>CBAM分别产生<strong>1D通道注意力特征图</strong>、<strong>2D空间注意力特征图</strong>：<span class="math display">\[M_c\in R^{C\times 1\times 1}\\M_s\inR^{1\times H\times W}\]</span> 这个过程我们可以描述为以下公式： <spanclass="math display">\[F^{&#39;}=M_{c}(F)\otimesF\\F^{&#39;&#39;}=M_{s}(F^{&#39;})\otimes F^{&#39;}\]</span> <spanclass="math inline">\(\otimes\)</span>表示元素级相乘，中间采用广播机制进行维度变换和匹配。</p><h3 id="channel-attention-module">3.Channel Attention Module</h3><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-4d30e7347da8e92cab00fcd45aa65a5d_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  先看通道注意力机制，通过特征内部之间的关系来通道注意力机制。特征图的每个通道都用来被视作一个特征检测器，所以通道特征聚焦的是图像中有用的信息是"什么"（what）。<br/>​  为了更高效地计算通道注意力特征，要做的就是压缩特征图的空间维度，之前采用的是平均池化方法，这个方法可以学习到目标物体的程度信息，作者这里研究了最大池化也能够学习到物体的判别性特征。所以在通道注意力模块同时采用了这两种方法，在后面的实验中也证实了，同时使用两种方法要比单独使用一种方法效果要好。<br/>​  在经过这种方法之后产生了两种不同的空间上下文信息：<spanclass="math inline">\(F_{avg}^c\)</span> 和 <spanclass="math inline">\(F_{max}^c\)</span>分别代表平均池化特征和最大池化特征。<br/>​  然后再将该特征送入到一个共享的多层感知机(MLP)网络中产生最终的通道注意力特征图<span class="math inline">\(M_c\in R^{C\times 1\times1}\)</span><br/>​  为了降低计算参数，在MLP中采用了一个降维系数r， <spanclass="math inline">\(M_c\in R^{C/r\times 1\times1}\)</span><br/>​  综上通道注意力计算公式总结为： <spanclass="math display">\[\begin{gathered}M_{c}(F)=\sigma(MLP(AvgPool(F))+MLP(MaxPool(F)))\\=\sigma(W_{1}(W_{0}(F_{avg}^{c}))+W_{1}(W_{0}(F_{max}^{c})))\end{gathered}\]</span></p><h3 id="spatial-attention-module">4.Spatial Attention Module</h3><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-3ffbd68a6064eaf2fae6de939c92c584_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  通过对特征图空间内部的关系来产生空间注意力特征图。不同于通道注意力，空间注意力聚焦于特征图上的有效信息在"哪里"（where）。为了计算空间注意力，首先在通道维度平均池化和最大池化，然后将他们产生的特征图进行拼接起来（concat）。然后在拼接后的特征图上，使用卷积操作来产生最终的空间注意力特征图：<span class="math display">\[M_s(F)\in R^{1\times H\times W}\]</span>​  同上，在通道维度使用两种池化方法产生2D特征图：</p><p><span class="math display">\[F_{avg}^c \in R^{1\times H\timesW}\\F_{max}^c \in R^{1\times H\times W}\]</span>​  最终这个过程的公式如下： <spanclass="math display">\[\begin{aligned}M_{s}(F)&amp;=\sigma(f^{7\times7}([AvgPool(F);MaxPool(F)]))\\&amp;=\sigma(f^{7\times7}(F_{avg}^{s};F_{max}^{s}))\end{aligned}\]</span></p><h2 id="三.代码实现">三.代码实现</h2><p>​  经过上面的分析，我们可以看到CBAM主要包括两个部分，Channel AttentionModule 和 Spatial AttentionModule，其实际的代码也是非常通俗易懂。<br/>​  下面是pytorch版本的代码。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#通道注意力</span><br><span class="line">class ChannelAttention(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, ratio=16):</span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(1)</span><br><span class="line">        </span><br><span class="line">        #MLP  除以16是降维系数</span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) #kernel_size=1</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        #结果相加</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        return self.sigmoid(out)</span><br><span class="line"></span><br><span class="line">#空间注意力</span><br><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, kernel_size=7):</span><br><span class="line">        super(SpatialAttention, self).__init__()</span><br><span class="line">        #声明卷积核为 3 或 7</span><br><span class="line">        assert kernel_size in (3, 7), &#x27;kernel size must be 3 or 7&#x27;</span><br><span class="line">        #进行相应的same padding填充</span><br><span class="line">        padding = 3 if kernel_size == 7 else 1</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = torch.mean(x, dim=1, keepdim=True)  #平均池化</span><br><span class="line">        max_out, _ = torch.max(x, dim=1, keepdim=True) #最大池化</span><br><span class="line">        #拼接操作</span><br><span class="line">        x = torch.cat([avg_out, max_out], dim=1)</span><br><span class="line">        x = self.conv1(x) #7x7卷积填充为3，输入通道为2，输出通道为1</span><br><span class="line">        return self.sigmoid(x)</span><br></pre></td></tr></table></figure><p>CBAM的整理逻辑还是很简单很清晰的，如果向更深入的了解一些细节信息，可以去下载论文仔细研究研究。注意力机制系统比较庞大，最近大火的Tranformer系列又是SelfAttention的一种设计。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</title>
      <link href="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/"/>
      <url>/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<center>Learning Discriminative Noise Guidance for Image Forgery Detection andLocalization</center><p><span class="math inline">\(\text{Jiaying Zhu}^*,\text{DongLi}^*,\text{Xueyang Fu}^\dagger,\text{Gang Yang, Jie Huang, Aiping Liu,Zheng-Jun Zha}\)</span></p><center>中国科学技术大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/28608.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange"alt="AAAI" /></a></p><h1 id="摘要">1. 摘要</h1><p>​  随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。<br/>​  本研究介绍了一种新的图像伪造检测和定位方法，该方法通过关注<strong>噪声域内的操纵痕迹</strong>来检测和定位图像伪造。我们假设RGB图像中几乎看不见带有篡改痕迹的噪声，有助于识别和定位伪造品。然而，篡改技术的进步使噪声用于伪造检测的直接应用变得复杂，因为伪造区域和真实区域之间的噪声不一致性没有得到充分利用。为了解决这一问题，我们开发了一种两步判别噪声引导的方法，以明确地增强一致性中噪声的表示和使用，从而充分利用噪声信息来提高伪造检测的准确性和鲁棒性。<br/>​  具体而言，我们使用去噪网络和基于统计量的约束，增强了伪造区域与真实区域之间的噪声可识别性。然后，我们将模型驱动的引导学习机制与数据驱动的注意力机制相结合，创建了一个可学习且可微分的噪声引导学习机制。这种复杂的滤波器使我们能够从噪音中学习到的伪造区域的边缘。在多个数据集上的综合实验表明，我们的方法可以可靠地检测和定位伪造品，超过了现有的最先进的方法。</p><h1 id="引言">2. 引言</h1><p>​  真实区域和伪造区域的噪声分布则不一致，导致噪声域中的操纵痕迹。许多研究人员已经利用这种噪音信息来图像伪造信息的检测与定位，实现了显著的结果。这些方法直接构建噪声特征到掩模的端到端映射，并采用融合策略集成RGB和噪声信息，以提高伪造检测精度。然而，随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不再是隐含的关系。<br/>​  鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。因此，我们引入了一种新的两步噪声引导方案。<br/>​  第一阶段，训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。我们使用去噪网络，然后使用Bayar卷积来构建噪声提取器，并使用基于统计的约束进行优化。使用去噪器的基本原理源于一个合适的去噪网络可以放大两个区域间的噪声分布差异。<br/>​  如图1所示，标准差为25的去噪网络在噪声域内最大化了真实区域和伪造区域之间的差异，而直接提取的噪声不能达到相同的效果。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502202145227.png" alt="image-20240502202145227 " style="zoom:70%;" /></p><p>图1：我们使用不同标准差的去噪网络（CBDNet (Guoet al.2019），用标准差为15、25、50）对伪造图像进行处理，然后分别提取噪声。对于该图像，25个标准差的去噪器可以帮助获得鉴别噪声。<br/>  为了自适应地调整去噪网络，我们基于高斯分布的Jensen-Shannon（JS）散度对处理后的图像噪声施加自定义约束。</p><p>​  第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。与之前的融合策略不同，我们明确地利用噪声来指导RGB分支，显著提高了有效性。我们将手工制作的引导变换（He，Sun，和Tang2012）和数据驱动的注意力机制（Wang et al.2018）合并，创建了基于交叉注意力的引导滤波器（CAGF，theCross-Attention-Based GuidedFilter）。由于引导变换的局部线性和边缘保存，CAGF不仅充分集成了RGB和噪声域的互补信息，而且确保了结构信息从噪声域到RGB域的传输。<br/>​  本质上，我们的方法明确地学习了第一阶段的噪声不一致，并在第二阶段利用了这种表示。</p><p>​  我们的贡献如下：<br/>​  1.我们提出了一种新的判别噪声引导方案，该方案明确增强了噪声不一致性的表示和利用。<br/>​  2.我们开发了一种方法来突出伪造区域中的噪声不一致性，使用去噪网络来处理图像，并使用基于统计的约束来优化噪声提取。<br/>​  3.我们设计了一种基于交叉注意力的引导滤波器，它结合了模型驱动和数据驱动技术，充分利用伪造信息的噪声表示，显著增强了噪声不一致对RGB分支的引导效果。</p><p>​  在几个有代表性的基准上进行的广泛实验表明，我们的方法优于最先进的方法，特别是在真实数据集IMD20(Novozamsky,Mahdian, and Saic 2020)上。</p><h1 id="方法">3. 方法</h1><p>​  源图像和目标图像之间的噪声特征不太可能匹配（Zhou et al.2018），篡改操作破坏了自然噪声分布（Wang et al.2022a）。此外，使用噪声可以抑制内容信息，这有利于提取IFDL的语义不可知的特征（Chenet al.2021）。然而，随着篡改和后处理技术的发展，噪声的不一致性并不明显，不再是隐含的关系。我们认为，明确地挖掘和利用噪声不一致性可以进一步提高精度和鲁棒性。<br/>​  因此，我们提出了一种两步策略，来充分利用IFDL的噪声不一致性，包括噪声表示学习和噪声引导网络。<br/>​  首先，我们提出了一种使用去噪网络和自定义约束的学习方案。输入图像用$ X R^{H×W×3} $表示，其中H和W表示图像的高度和宽度。将X输入去噪网络得到图像 $ X^{'} $，然后用BayarConv (Wu, AbdAlmageed, and Natarajan 2019)提取噪声 $ G_dR^{H×W×3} $。去噪网络的选择不是本工作的重点，所以我们使用广泛使用的CBDNet（Guo etal. 2019）来权衡性能和计算量。我们对 $ G_d $施加了基于统计的约束，以确保这两个区域的噪声分布被分开。<br/>​  其次，我们训练噪声引导网络（NGNet,noise-guidednetwork）显式地应用噪声不一致性。NGNet是一个双分支网络，它包含多个基于交叉注意力引导的信号，逐步引导具有噪声不一致的RGB信息，以获得更准确的IFDL。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png"alt="image-20240822204529012" /><figcaption aria-hidden="true">image-20240822204529012</figcaption></figure><h2 id="噪声表示学习">3.1 噪声表示学习</h2><p>​  为了明确地表示噪声的不一致性，我们设计了一个噪声表示学习网络（NRLNet,noise representation learningnetwork），如图2a所示，它使用一个去噪网络来处理图像和统计损失来优化网络。考虑到伪造图像的噪声分布未知，我们使用盲去噪网络CBDNet（Guoet al. 2019)，并加载盲去噪的权值作为初始化。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><p>​  具体的体系结构和培训策略如下所述。<br/>​  我们使用一个基于CBDNet的网络来处理输入的图像X。具体地说，我们预测了一个噪声水平图$~ \hat{l} \in R^{H×W×3} $ ，它可以看作是与噪声分布相关的权值。然后将 $$ 和输入X一起输入编码解码器结构，得到图像 $~ X^{'} \in R^{H×W×3} $，表示为： <spanclass="math display">\[\hat{l}=\mathrm{NE}\left(X\right)\\X^{&#39;}=\mathrm{D}\left(\mathrm{Concat}\left(X,\hat{l}\right)\right)\]</span></p><p>​  其中，Concat表示连接操作。NE是由5层全卷积网络实现的噪声估计模块，卷积核大小为3×3。D是一种U-Net架构，它可以获得具有鉴别噪声的图像。<br/>​  然后，跟随（Chenet al. 2021），我们采用BayarConv从 $ X^{'} $ 中提取噪声 $~ G_d \inR^{H×W×3} $。此外，为了使学习到的噪声更有利于IFDL，我们将噪声输入Res-CNN，以预测粗定位结果$~ G_c \in R^{H×W×1} $。Res-CNN包含10个res-块，其中一个块由两个3×3卷积和ReLU函数组成。<br/>​  <strong>最优化</strong>。为了明确地分离出这两个区域（真实的和伪造的）的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $的噪声。图像中的平稳扰动可以建模为高斯分布（Guo et al. 2019）， $ N_a $和 $ N_f $都可以视为噪声的采样值。因此，我们利用连续高斯分布的JS散度来测量两个区域的噪声分布之间的距离：<span class="math display">\[JSD(P_a||P_f)= \frac {1}{2} KL(P_a||M) +\frac {1}{2} KL(P_f||M)\]</span> ​  式中， $ P_a $ 和 $ P_f $ 分别为 $N_a $ 和 $ N_f $ 的分布，M为 $ 2 $ 。两个高斯分布的KL散度计算如下：<span class="math display">\[KL(P_1||P_2)=log \sigma _2 − log \sigma _1+ \frac { \sigma _1 ^2 + {( \mu _1 - \mu _2 )}^2} {2 \sigma _2 ^2} -\frac {1} {2}\]</span> ​  式中， $~ \sigma_1 $ 、 $~ \sigma_2 $ 为 $ P_1$ 和 $ P_2 $ 的标准差， $~ \mu_1 $ 、 $~ \mu_2 $ 为 $ P_1 $ 和 $ P_2 $的平均值。然后，JSD的计算方法如下：</p><p><spanclass="math display">\[JSD=\log\frac{\sqrt{\sigma_{a}^{2}+\sigma_{f}^{2}}}{2}-\frac{\log\sigma_{a}+\log\sigma_{f}}{2}+\frac{(\mu_{a}-\mu_{f})^{2}}{\sigma_{a}^{2}+\sigma_{f}^{2}}+\frac{1}{2}\]</span></p><p>​  式中， $~ \sigma_a $ 、 $~ \sigma_f $ 为 $ N_a $ 和 $ N_f $的标准差， $~ \mu_a $ 、 $~ \mu_f $ 为 $ N_a $ 和 $ N_f $的平均值。此外，如果仅使用JS散度作为损失函数，网络的优化过程就会发生振荡。因此，我们采用伪造损失定位来辅助网络的优化，这也更有利于神经图像的伪造定位。<br/>​  我们结合了辅助损失和JS散度组成噪声表示学习的损失函数，可以写成：<spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>​  其中L表示Dice损失（Chen et al. 2021）， $~ Y \in R^{H×W×1} $为groundtruth， $ $ 为平衡两项的超参数，设置为0.80。</p><h2 id="噪声引导网络">3.2 噪声引导网络</h2><p>​  如图2b所示，在NRLNet收敛后，我们将训练好的判别噪声提取器（去噪器和BayarConv）嵌入到噪声引导网络（NGNet）中。与以前的工作不同，我们以显式指导的形式应用噪声。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>​  NGNet的网络架构、基于交叉注意的引导滤波（CAGF）和优化具体如下。<br/>​  <strong>网络架构。</strong>我们利用两个分支来处理RGB和噪声信息。利用训练好的判别噪声提取器获取噪声分支的输入，以更好地提取伪造痕迹。我们使用在ImageNet上预训练的ResNet-50(Deng et al.2009)作为骨干网络。然后，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。最后，我们将提取的具有平卷积层和双线性上采样的特征转换为最后预测掩模$~ G_{out} \in R^{H×W×1} $。  <strong>基于交叉注意力引导的滤波。</strong>现有的IFDL方法直接使用融合策略，不能明确保证噪声域内的篡改伪影得到充分利用。引导滤波器可以保证结构信息在从引导图像到目标图像的传输过程中具有保持边缘的特性(He, Sun, and Tang2012)。受此启发，我们从引导的角度探讨了噪声和RGB信息的融合，从而提出了CAGF，如算法1所示。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502212532274.png"alt="image-20240502212532274" /><figcaption aria-hidden="true">image-20240502212532274</figcaption></figure><p>​  我们在实践中使用了三个CAGF块。传统的引导滤波是由局部线性模型导出的。它通过考虑引导来产生滤波输出结果。  然而，传统的引导滤波是一种不可训练的算法，没有考虑引导与目标之间的相互依赖性，这不适用于IFDL。由于噪声和RGB之间的信息差距很大，简单地将结构信息从噪声传输到RGB就会产生各种伪影。因此，在传统算法的基础上，我们利用注意力机制来计算方差和协方差，并使用卷积层代替平均频率。具体而言，来自噪声流和RGB流的输入特征分别为$~ G_n \in R^{H_s×W_s×C_s} $ 和 $~ G_r \in R^{H_s×W_s×C_s} $ ，我们以 $G_n $ 作为引导图像，以 $ G_r $作为输入图像。首先，我们设计了新的跨模态注意（CMA, cross-modalattention)来获得协方差和方差。以协方差的计算为例，CMA的输入量分别为 $G_n $ 和 $ G_r $ 。CMA利用图3中描述的计算块将它们转换为 $ cov_{nr}R^{H_s×W_s×C_s} $</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171039698.png"alt="image-20240401171039698" /><figcaption aria-hidden="true">image-20240401171039698</figcaption></figure><p>​  具体计算公式如下：</p><p><spanclass="math display">\[\begin{aligned}cov_{nr}&amp;=CMA(G_n,G_r)\\&amp;=Res(C(C(G_n)^T  \otimes C(G_r)) \otimes C(C(G_n) \odotC(G_r)))\end{aligned}\]</span> ​  式中， $ $ 为矩阵乘法， $ $为Hadamard乘积，Res表示包含两个3×3卷积和ReLU函数的Res-块，C表示1×1卷积。我们执行$ G_n $ 和 $ G_r $ 的矩阵乘法获得 $ C R^{N×N} $ （ $ N=H_s×W_s $）而不是传统的引导滤波，并计算两者的Hadamard乘积来代替传统算法的 $mean_n ∗ mean_r $ (He, Sun, and Tang 2012)。在 $ G_n $ 和 $ G_r $的矩阵乘法之前，将两者转换为 $ C_s/r×N $，其中r是一个标量，它降低信道维数以提高计算效率。同样，当CMA的两个输入都是噪声特征$ G_n $ 时，我们可以得到方差 $ var_n R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[var_n = CMA(G_n, G_n)\]</span> ​  res块用于获得系数 $ a R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[a = Res (Concat (cov_{nr}, var_n))\]</span>​  接下来，我们按照算法1中的公式来计算 $ b R^{H_s×W_s×C_s} $ 。受（Wu etal. 2018）的启发，我们使用卷积运算（MFConv）代替平均滤波： <spanclass="math display">\[mean_M = MFConv (M) = \frac {Conv (M)} {Conv (J)}\]</span>​  其中M为MFConv的输入，J为与M大小相同的全一矩阵，Conv为3×3卷积。</p><p>​  最后，我们根据局部线性关系得到CAGF的输出 $ q R^{H_s×W_s×C_s} $ ：<span class="math display">\[q = mean_a \odot G_r + mean_b\]</span>​  其中 $ mean_a $ 和 $ mean_b $ 是a和b经过MFConv后的结果，大小为 $H_s×W_s×C_s $ 。</p><p>​  <strong>检测器。</strong>对于检测器，我们采用了MVSS-Net++ (Dong etal. 2023)提出的ConvGeM，它可以将定位结果 $ G_{out} $ 转换为检测预测 $D_{out} $。ConvGeM通过一个衰减的跳过连接，在检测和定位之间取得了很好的平衡。因此，我们使用ConvGem来获得一个更准确的检测结果：<span class="math display">\[D_{out} = ConvGeM(G_{out})\]</span>​  <strong>最优化。</strong>根据大多数研究 (Chen et al. 2021;Wang et al.2022c)，我们也利用了边缘监督。但是，这并不是这项工作的重点，所以我们使用了一些常见的方  法。跟随（Chenet al. 2021），我们使用Sobel层和残差块，以浅到深的方式获得边缘预测 $ G_eR^{H_e×W_e×1} $ 。对于边缘损失，ground-truth边缘 $ E R^{H×W×1} $被降采样到一个更小的尺寸 $ E^{'} R^{H×W×1} $ 以匹配 $ G_e $。该策略在计算成本和性能方面优于上采样 $ G_e $ 。NGNet的损失可以写成：<span class="math display">\[L_N = \alpha L_1 (Y, G_{out}) + \beta L_2(y, D_{out})+(1 − \alpha − \beta ) L_3 (E^{&#39;},G_e)\]</span> ​  式中 $L_1 $ 和 $ L_3 $ 表示Dice损失（Chen et al. 2021）， $ L_2 $为BCE损失，y表示图像真实性的标签， $ $ ， $ $是平衡损失函数的超参数。实际上， $ $ 设置为0.60， $ $设置为0.2。请注意，真实的图像只用于计算 $ L_2 $ 。</p><h1 id="实验">4. 实验</h1><h2 id="实验设置">4.1 实验设置</h2><p>​  <strong>预训练数据。</strong>我们构建了一个大量的图像篡改数据集，并将其用于预训练我们的模型。该数据集包括三个类别：1）拼接、2）复制移动和3）删除。<br/>​  <strong>测试数据集。</strong>在CASIA (Dong, Wang, and Tan 2013)、Coverage (Wen et al. 2016)、Columbia(Hsu and Chang 2006)、Nist Nimble 2016 (NIST16) (Guan et al.2019)和IMD20(Novozamsky, Mahdian, and Saic2020)上评估我们的模型。我们使用 (Hu et al. 2020; Wang et al.2022b)一样的方法来划分训练，以便进行公平比较。<br/>​  <strong>评估指标。</strong>为了量化定位性能，根据之前的工作（Huet al. 2020；Wang et al.2022b），我们使用像素级曲线下面积（AUC）和F1分数。为了评估检测性能，我们使用了图像级的AUC和F1评分。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h2 id="图像伪造定位">4.2 图像伪造定位</h2><p>​  在两种情况下，我们的模型与其他最先进的篡改定位方法进行了比较：1)在合成数据集上进行训练和对完整的测试数据集进行评估；2)对测试数据集的训练分割和对它们的测试分割进行评估。<br/>​  <strong>预训练模型。</strong>表1a显示了在像素级AUC下的fve数据集上，不同方法的预训练模型在五个数据集上的定位性能。我们将我们的模型NGNet与MantraNet(Wu, AbdAlmageed, and Natarajan 2019), SPAN (Hu et al. 2020), PSCCNet(Liu et al. 2022),ObjectFormer (Wang et al. 2022b), TANet (Shi, Chen,and Zhang 2023) 和HiFi-Net (Guo et al.2023)进行比较。预训练的NGNet在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia上排名第二。特别是NGNet在复制-移动数据集Coverage上达到了94.1%，该数据集图像伪造区域与背景难以区分。这验证了我们的模型具有在噪声域中捕获篡改痕迹的优越能力。我们未能在Columbia上取得最好的表现，在AUC下落后TANet0.2%。我们认为，可能的解释是他们合成的训练数据的分布与Columbia数据集非常相似。表1b的结果进一步支持了这一点，结果显示NGNet在AUC和F1分数方面都优于TANet。此外，值得指出的是，NGNet在较少的训练前数据下取得了不错的结果。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171128950.png"alt="image-20240401171128950" /><figcaption aria-hidden="true">image-20240401171128950</figcaption></figure><p>表1：图像伪造检测和定位结果。(a)训练前模型的定位性能。(b)调整模型的(b)定位性能。(c)对CASIA-D数据集的检测性能。（粗体表示最好，下划线表示第二好）。<br/>  <strong>微调模型。</strong>预训练模型的网络权值用于启动调整模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练分割上进行训练。我们在表1b中评估了不同方法的神经调优模型。对于AUC和F1，我们的模型取得了显著的性能提高。这验证了我们的方法可以通过识别噪声表示学习和基于交叉注意的引导转换来精确捕获细微的篡改痕迹。</p><h2 id="图像伪造检测">4.3 图像伪造检测</h2><p>​  为了避免误警报，我们还考虑了伪造检测任务。根据ObjectFormer（Wang etal. 2022b），我们对（Liu et al.2022）引入的CASIA-D数据集进行了实验比较。如表1c所示，我们的方法具有良好的检测性能，即AUC为99.81%，F1为98.72%。我们的方法明确地建模和利用噪声的不一致，从而准确地区分伪造的图像和真实的图像。</p><h2 id="鲁棒性评估">4.4 鲁棒性评估</h2><p>​  为了分析我们的定位模型的鲁棒性，我们遵循（Wang et al.2022b）中的失真设置来降解NIST16中的伪造图像。这些失真类型包括将图像调整到不同的尺度，应用核大小k的高斯模糊，添加标准偏差σ的高斯噪声，以及使用质量因子q执行JPEG压缩。我们将我们预训练模型的伪造定位性能（AUC分数）与SPAN和ObjectFormer做比较，并报告的结果在表2中。我们的模型对各种失真技术证明了更好的鲁棒性。值得注意的是，JPEG压缩通常是在向社交媒体上载图片时执行的。我们的模型在压缩图像上表现得明显更好。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171143021.png"alt="image-20240401171143021" /><figcaption aria-hidden="true">image-20240401171143021</figcaption></figure><p>表2：NIST16数据集在各种畸变情况下的性能。报告AUC分数（%），（模糊：高斯模糊：噪声：高斯噪声，压缩：JPEG压缩。）</p><h2 id="消融实验">4.5 消融实验</h2><p>​  在本节中，我们进行了实验来证明我们的方法的有效性。噪声表示学习（NRL,noise representationlearning）的设计是为了明确地扩大两个区域之间的噪声分布的差异（真实和伪造）。基于交叉注意力的引导滤波包含跨模态注意力（CMA,cross-modal attention）和引导滤波机制（GF, guided flteringmechanism）。CMA充分集成了RGB和噪声分支中所包含的互补信息，而GF则保证了结构信息从噪声到RGB的传输，并具有保持边缘的特性。为了评估NRL、CMA和GF的有效性，我们将它们从我们的方法中分离出来，并评估它们在CASIA和NIST16上的伪造定位性能。<br/>​  表3显示了定量结果。基线表示我们只使用ResNet-50。可以看出，没有GF，CASIA和NIST16的AUC评分下降了4.5%，而没有CMA，CASIA和NIST16的AUC评分下降了10.9%。此外，当丢弃NRL时，表3的性能严重下降，即AUC为9.5%，F1为20.9%。</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171152148.png"alt="image-20240401171152148" /><br/>  由于不同的去噪器可能产生不同的性能，我们对去噪器的选择进行了消融研究。我们选择DnCNN(Zhanget al. 2017)、FFDNet (Zhang, Zuo, and Zhang 2018)、RIDNet(Anwar andBarnes 2019) 和DRUNet (Zhang et al. 2021)等盲去噪网络进行比较。如表4所示，CBDNet权衡了性能和计算复杂度。<br/><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171207701.png"alt="image-20240401171207701" /></p><h2 id="可视化结果">4.6 可视化结果</h2><p>​  <strong>定性的结果。</strong>如图4所示，我们提供了各种方法的预测掩模。结果表明，该方法不仅能准确地定位篡改区域，而且能开发出清晰的边界。它受益于我们的模型能够明确地扩大两个区域之间的噪声差异，并保持边缘。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222847414.png"alt="image-20240502222847414" /><figcaption aria-hidden="true">image-20240502222847414</figcaption></figure><p>图4：通过不同的方法可视化预测的操作掩码。从上到下，我们展示了伪造的图像、GT面具、ManTraNet、SPAN、PSCC-Net、HiFi-Net和我们的预测。<br/>  <strong>噪声表示学习的可视化。</strong>我们在图5中展示了有和没有NRL的特征的变化。可见，NRL有助于对伪造特征的学习，并获得更准确的伪造区域轮廓。这是因为NRL帮助网络捕获噪声域中的篡改痕迹。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222929983.png"alt="image-20240502222929983" /><figcaption aria-hidden="true">image-20240502222929983</figcaption></figure><p>图5：噪声表示学习和基于交叉注意的引导飞行器的可视化。从左到右，我们显示了没有（w/o）NRL和没有CAGF以及两者都有（NGNet）的特征地图的伪造图像、掩模（Selvaraju等人2017），以及预测。<br/>  <strong>基于交叉注意力的引导滤波器的可视化。</strong>为了验证CAGF的效果，我们在图5中展示了闪烁前后特征的变化。可见，CAGF可以提高伪造定位的精度。没有CAGF的网络将会对类似于伪造文件的物体做出错误的判断。<br/>  <strong>判别噪声表示的可视化。</strong>为了进一步验证我们的方法的动机和有效性，我们分别在图6中展示了提取的没有和有噪声表示学习（NRL）的噪声。可以看出，NRL获得了一个更有鉴别性的噪声表示，这是伪造信息的。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502223135944.png" alt="image-20240502223135944 " style="zoom:67%;" /></p><h1 id="结论">5. 结论</h1><p>​  本文提出了一种包含噪声表示学习和噪声引导网络的两步噪声引导方案。第一步是明确地强调在真实区域和伪造区域之间的噪声分布的可辨别性。在第二步中，设计了一个定制的基于交叉注意力的引导滤波架构，结合了模型驱动和数据分割技术，以增强噪声不一致对RGB分支的引导效果，充分利用伪造噪声表示。我们的工作为解决模糊提取微妙的伪造痕迹的问题提供了一个新的研究策略。在几个基准上的大量实验结果证明了该方案的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BayarConv </tag>
            
            <tag> JS散度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技巧合集</title>
      <link href="/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p><a href="/MarkDown（一）/">MarkDown（一）：基本语法</a></p><p><a href="/MarkDown（二）/">MarkDown（二）：图表流程图</a></p><p><a href="/MarkDown（三）/">MarkDown（三）：公式语法</a></p><p><a href="/Hexo技巧/">Hexo技巧</a></p><p><a href="/typora技巧/">typora技巧</a></p><figure><imgsrc="../postimages/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/image-20250625103013869.png"alt="image-20250625103013869" /><figcaption aria-hidden="true">image-20250625103013869</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization</title>
      <link href="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/"/>
      <url>/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>MGQFormer: Mask-Guided Query-Based Transformer for Image ManipulationLocalization <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://dml.fudan.edu.cn/d1/65/c35285a643429/page.htm"><imgsrc="https://img.shields.io/badge/News-4096ff.svg" alt="new" /></a></center><center><spanclass="math inline">\(\text{KunlunZeng}^*,\text{RiCheng}^*,\text{WeiminTan}^\dagger,\text{BoYan}^\dagger\)</span></center><center>复旦大学智能信息处理上海关键实验室计算机科学学院</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/MGQFormer.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  基于深度学习的模型在图像篡改定位方面取得了巨大的进展，其目标是区分被篡改和真实区域。然而，这些模型在训练效率上存在问题。这是因为它们主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p>  为了解决这个问题，我们提出了一个基于<strong>掩码引导查询</strong>的Transformer框架（MGQProtrer），它使用GroundTruth掩码来指导<strong>可学习查询token</strong>（LQT,learnable query token）识别伪造区域。</p><p>  具体地说，提取GroundTruth掩码的特征嵌入作为<strong>指导查询token</strong>（GQT,guiding querytoken），并将GQT和LQT分别输入到MGQFrorter中来估计篡改区域。然后，我们提出了一种掩模引导损失的位置和形状信息，以减少掩模在GQT和LQT之间的特征距离。我们还观察到，这种掩模引导的训练策略对MGQprort训练的收敛速度有显著影响。在多个基准测试上的大量实验表明，我们的方法显著地改进了最先进的方法。</p><h1 id="引言">引言</h1><p>  在训练过程中，我们使用多分支特征提取器从RGB输入图像中提取空间信道感知特征。它使用两个不同的转换器编码器分别从RGB输入图像及其噪声图中提取特征。然后，利用空间注意和通道注意来融合不同分布和域的RGB图像和噪声图特征。最后，将融合的特征输入到我们提出的基于查询的transformer解码器中，以输出伪造区域在图像中的位置。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164911763.png" alt="image-20240401164911763 " style="zoom:80%;" /></p><p>图1：以前的方法和我们的方法之间的区别。我们的方法使用了一个有效的和可解释的基于查询的Transformer。Token相似度是指图像特征与查询Token之间的标量乘积的softmax结果。此外，我们使用GroundTruth掩码来指导可学习的查询token（LQT）来识别真实的和伪造的区域。</p><p>  为了迫使LQT集中于伪造区域，提取GroundTruth掩模特征作为真实的伪造引导查询token（GQT），并将它们输入解码器，以估计伪造区域的位置。由于GQT来自于地面真实掩模，这是预测掩模的目标，因此GQT将包含伪造区域的空间位置和形状细节。因此，提出了一种掩模引导损耗来减小GQT和LQT之间的特征距离。模型经过训练后，LQT也使网络关注锻造区域的位置和形状。因此，我们只在推理过程中使用LQT来定位基于查询的transformer解码器中被篡改的区域。</p><p>  我们介绍了基于掩模引导的基于查询的Transformer，它包含一个基于查询的Transformer解码器，利用可学习的查询token（LQT）来定位被篡改的区域。</p><p>  我们提出了一种掩模引导训练方法，将从GT掩模中提取的引导查询token（GQT）作为参考LQT。此外，我们设计了掩模引导的损失，以迫使GQT引导LQT集中于被篡改区域的空间位置和形状细节。</p><p>  我们在多个基准测试上进行了广泛的实验，并证明了我们的方法在多个数据集上达到了最先进的性能。</p><h1 id="方法">方法</h1><p>  我们的方法旨在使用基于掩码引导的基于查询的Transformer（MGQprorter）来识别可疑图像中被篡改的区域。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png" alt="image-20240401164940206" style="zoom:50%;" /></p><p>图2：提出的框架MGQprorter的概述，由双分支transformer编码器、融合模块和掩模引导transformer解码器组成。在训练过程中，输入是一个可疑图像（H×W×3）和一个地面真实掩模，输出包括一个预测掩模和一个辅助掩模（H×W×1），这两者都涉及损失计算（Lloc和Laux）。请注意，红线部分是由掩模引导的训练，在推理过程中不需要进行训练。</p><p>  我们将输入图像表示为 $~ X \in R^{H×W×3} $，其中H和W分别为图像的高度和宽度。我们利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征。然后，通过空间和通道注意模块（SCAM,spatialand channel attention module）对多模态特征进行融合。</p><p>  我们设计了两个可学习的查询token（LQT）来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。</p><p>  具体来说，我们将噪声的GT掩模输入MGQFormer，以获得引导查询token（GQT）和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p><h2 id="多分支特征提取器">多分支特征提取器</h2><p>  图像处理定位通常包含复杂的后处理，使得检测微小的差异和伪造痕迹对RGB域具有挑战性。因此，我们采用了一个双分支transformer编码器来完全利用来自两个域的信息。</p><p>  BayarConv 提取噪声特征 $~ X_n \in R^{H×W×3} $。然后将输入的图像和噪声图发送到Transformer编码器。具体地说，我们将X和 $X_n $ 划分为大小为P的补丁，并将补丁重塑为嵌入 $~ X_p \in R^{N×D} $，其中 $ N=HW/P^2 $ 为补丁的数量，D是嵌入的维数。将可学习的位置嵌入 $~pos \in R^{N×D} $ 添加到图像嵌入中，生成序列token $ Z = X_p + pos $，然后通过Transformer层L进行处理。对噪声分支也进行了上述相同的结算。在Transformer编码器之后，两个分支的输出被连接起来，我们得到$~ Z_c \in R^{N×2D} $，用于后续的融合。<br/><br/>  来自不同分支transformer编码器的token具有不同的域和不同的分布。因此，我们使用空间和通道注意模块（SCAM）来完成融合任务。<br/><br/>  我们首先重塑标记$ Z_c $ ，并使用一个卷积层得到 $~ Z_m \in R^{h×w×c} $ ，其中 $ h = H/P,w= W/P,c = D $ 。<br/><br/>  接下来，我们将 $ Z_m $ 和 $ Z_m $的转置分别定义为： <span class="math display">\[V=proj(Z_m) \inR^{hw×c}, K=proj(Z_m) \in R^{hw×c}, Q=transpose(proj(Z_m)) \inR^{c×hw}\]</span>   其中 $ proj $是一个独特的投影层，包括1×1卷积和重塑操作。然后，通道注意模块如下：<span class="math display">\[CAM(Z_m)=proj(V(softmax(QK)))\]</span>  同时，我们继续计算空间注意力，除了转置的Q和K，几乎与通道注意相同。随后，我们可以得到的token如下：<spanclass="math display">\[SAM(Z_m)=proj(softmax(Q^TK^T)V)\\Z_f=CAM(Z_m)+SAM(Z_m)+Z_m\]</span>  然后将图像特征令牌 $~ Z_f \in R^{N×D} $​发送到基于查询的Transformer解码器。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822164324909.png"alt="image-20240822164324909" /><figcaption aria-hidden="true">image-20240822164324909</figcaption></figure><h2 id="掩模transformer解码器">掩模Transformer解码器</h2><p>  我们首先介绍在引用阶段的解码器。</p><p>  对于所提出的基于查询的transformer解码器，我们使用了真实的和伪造的可学习查询令牌$~ LQT \in R^{2×D} $。这些查询被随机初始化，并表示伪造的和真实的特性。具体地说，图像特征token$ Z_f $和LQT由由n个基于Transformer层组成的解码器同时处理。在注意机制过程中，LQT与特征token$ Z_f $ 相互作用，提取丰富的伪造信息。然后，我们得到了图像特征 $ Z_f^∗ $和 $ LQT^∗ $ 。掩码的计算方法如下： <spanclass="math display">\[M^∗=norm(proj(Z_f^∗))∗(norm(proj(LQT^∗))^T\]</span>  其中proj是一个线性层，norm表示 $ L_2 $归一化，我们通过在参考图像特征和可学习查询标记之间执行标量乘积得到 $~M^∗ \in R^{N×2} $ 。<br/><br/>  为了得到最终的掩模，我们将序列重塑为掩模$~ M^{∗∗} \in R^{h×w×2} $ ，并以类的维度应用一个softmax层： <spanclass="math display">\[M=upsample(norm(softmax(M^{∗∗})))\]</span> 其中，$~ M \in R^{H×W} $为预测的掩模，upsample为双线性上采样操作，将掩模的大小调整为与输入图像相同的大小。</p><p>综上所述，我们基于查询的方法利用真实和伪造的LQT来选择与自身高度相似的区域，这使得预测伪造区域的过程更容易解释和有效。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><h2 id="掩码引导的训练过程">掩码引导的训练过程</h2><p>  基于查询的模型在相应的任务中取得了很大的成功。然而，这些模型已经被证明存在查询回复的效率较低的问题。先前的方法已经提出了诸如去噪（Li等人2022a）和掩蔽注意（Cheng等人2022a）等方法。<br/><br/>  我们指出，以往的方法缺乏对伪造区域的位置和形状细节的LQT的直接监督，导致训练无效。这些方法主要利用交叉熵损失的地面真实掩模，优先考虑每像素的精度。<br/><br/>  为了解决这个问题，我们提出了一种掩模引导的训练策略，该策略使用引导查询令牌（GQT）来迫使LQT关注伪造区域的位置和形状。GQT通过提取噪声地面真掩模的特征得到，并利用辅助损失使GQT包含伪造区域的空间和形状信息。从而提高了MGQ前体训练的收敛速度。<br/><br/>  特别地，我们首先会向GroundTruth掩模添加噪声。这一步是因为从原始GroundTruth掩码预测辅助掩码对于transformer解码器和延迟训练来说可能太简单了。我们将点噪声应用于掩模中，类似于DN-DETR（Liet al.2022a）用于盒子去噪训练，以获得更鲁棒的模型。我们随机选择掩模内的点，并倒置原始值来表示不同的区域。此外，我们使用一个调优的参数µ来表示面积的噪声百分比，因此噪声点的数量为µ·HW。<br/><br/>  在噪声掩模的情况下，我们通过卷积网络将掩模转换为GQT，以保持掩模中的空间信息，并将GroundTruth$~ G \in R^{H×W} $ 转换为 $~ GQT \in R^{2×N} $ 。然后，GQT连同图像特征 $Z_f $和LQT一起被发送到Transformer解码器。在解码器中，GroundTruth信息GQT作为与其他查询交互的引导，并帮助解码器重构LQT。<br/><br/>  在Transformer解码器之后，我们得到了由GroundTruthtoken GQT引导的图像特征 $ Z_f^∗ $ 和查询令牌 $ LQT^∗ $ 和 $ GQT^∗ $。通过使用与掩模Transformer解码器部分相同的过程，对 $ Z_f^∗ $ 和 $ GQT^∗$ 进行标量乘积，进一步计算了辅助掩模 $~ M_{aux} \in R^{H×W} $。然后我们让 $ M_{aux} $ 参与到损失的计算中来。</p><h2 id="辅助损失">辅助损失</h2><p>  由于我们使用卷积网络将GroundTruth掩码转换为查询token，并且对掩模加噪声以保持鲁棒性，为了使辅助掩码更加精确，需要对卷积网络进行监督。因此，为了使GQT包含锻造区域的空间和形状信息，我们使用像素级交叉熵损失如下：</p><p><span class="math display">\[L_{aux}=−\sum_{i=1}^{HW}G_i\cdotlog(M_{auc},i)\]</span></p><p>  其中 $~ G \in R^{H×W} $​是GroundTruth掩模。注意，为了得到模型预测的精确掩模，我们使用没有噪声的原始GT掩模G来计算辅助损失。</p><h2 id="掩模引导的损失">掩模引导的损失</h2><p>  GQT的目的是引导LQT，并对两者进行相同的处理，生成预测的掩模M和辅助掩模<spanclass="math inline">\(M_{aux}\)</span>​。因此，我们期望LQT与GQT相似，从而使预测更加精确。我们采用余弦相似度损失来减少两个查询的距离，可以表述为：<span class="math display">\[L_{guide}=1-cos(LQT^*,GQT^*)\]</span>  其中cos为余弦相似度。</p><h2 id="损失函数">损失函数</h2><p>  总损失函数L包括三个部分：使<spanclass="math inline">\(M_{aux}\)</span>精确的辅助损失，使$ LQT^∗ $ 和 $GQT^∗ $ 更接近的掩模引导损失，以及预测掩模M的定位损失<spanclass="math inline">\(L_{loc}\)</span>，其中<spanclass="math inline">\(L_{loc}\)</span>​采用了和辅助损失相同的交叉熵损失：<span class="math display">\[L=L_{loc}+L_{aux}+ \lambdaL_{guide}\]</span></p><p>  其中，$ $是一个权重参数，在训练期间设置为0.5。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>  <strong>测试数据集。</strong>我们首先使用PSCC-Net合成的数据集对我们的模型进行预训练（Liu等人，2022年）。然后，我们在CASIA数据集（Dong、Wang和Tan2013）、Columbia数据集（Hsu和Chang 2006）、NIST16数据集（Guan et al.2019）和IMD20数据集（诺沃扎姆斯基、马赫迪安和Saic2020）上评估我们的模型。特别地，CASIA提供拼接和复制移动图像，这广泛出现在图像伪造场中。Columbia数据集由180张拼接图像组成，它们未压缩，没有经过后处理。NIST16是一个具有挑战性的数据集，它有564张眼睛很难识别的高分辨率图像。IMD20收集了由不同的相机模型捕获的35,000张真实图像，并由不同的内部绘制方法生成的不同类型的操作组成。</p><p>  <strong>评估指标。</strong>为了评估所提出的MGQFrotar的定位性能，在PSCCNet（Liuet al.2022）之后，我们报告了图像级F1评分和曲线下面积（AUC）作为评价度量。我们采用fxed阈值对预测的掩模进行二值化，这是计算f1分数所必需的。</p><p>  <strong>实施细节。</strong>MGQFormer在一个NVIDIA GTX 1080 TiGPU上实现的。所有输入图像的大小都被调整为384×384。我们使用Adam作为优化器，学习率从2.5e-7衰减到1.5e-8，批处理大小为2。特征提取器使用ImageNet预训练的ViT模型权值（Steineret al.2021）初始化，共12层，补丁大小为16，而解码器使用来自6层的截断正态分布的随机权值初始化。</p><h2 id="与最先进的方法的比较">与最先进的方法的比较</h2><p>  我们在两种设置下，将我们的模型与其他最先进的方法进行了比较：</p><p>  1)在合成数据集上进行训练和在完整的测试数据集上进行评估。</p><p>  2)对测试数据集的训练分割和评估其训练分割的预训练模型进行微调。</p><p>  对于预先训练的模型，我们评估的方法有：ManTraNet (Wu, AbdAlmageed,and Natarajan 2019), SPAN (Hu et al. 2020), ObjectFormer (Wang et al.2022), 和ERMPC (Li et al. 2023)，同时进一步比较的方法有： RGB-N (Zhou etal. 2018) 和PSCCNet (Liu et al. 2022)。</p><p>  <strong>预先训练的模型。</strong>表1报告了使用预先训练过的模型获得的最佳定位AUC（%）分数。我们可以观察到，MGQFormer在Columbia、CASIA、IMD20和所有数据集的平均AUC（%）上取得了最高的性能，并在NIST16上获得了具有竞争力的效果。特别是，MGQFormer在现实世界的IMD20数据集上达到88.3%，比ERMPC高2.7%。这验证了我们的方法具有捕获篡改痕迹和泛化到高质量数据集的突出能力。在NIST16数据集上，我们未能达到最佳的性能。我们认为，Transformer网络的性能受到了训练决策的影响。如果测试时的分辨率接近训练，就可以完全实现高性能。然而，NIST16是一个高分辨率的数据集，它大大超过了我们的训练数据集。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185721944.png" alt="image-20240502185721944 " style="zoom:50%;" /></p><p>  <strong>微调模型。</strong>为了补偿合成数据集与标准数据集之间的视觉质量差异，使用预训练模型的网络权值来启动调整模型，该模型将在CASIA数据集的训练分割上进行训练。如表2所示，我们将AUC和F1结果（%）与其他方法进行了比较，我们的模型获得了最好的性能，表明MGQFormer可以通过查询有效地捕获细微的篡改伪。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185818907.png" alt="image-20240502185818907 " style="zoom:40%;" /></p><h2 id="鲁棒性评估">鲁棒性评估</h2><p>  我们对Columbia数据集的原始图像应用不同的图像失真方法，并评估我们的MGQFormer的鲁棒性。失真类型包括：1)用不同的尺度调整图像的大小，2)用核大小k的高斯模糊，3)用质量因子q的JPEG压缩。我们比较了预训练模型在原始数据集和损坏数据上的操作定位性能（AUC分数），并报告了表3中的结果。与以往的方法相比，MGQFormer对所有失真具有最好的鲁棒性。特别是当面对调整大小和JPEG压缩时，我们的方法的性能略有下降，表示补丁MGQFormer对低质量图像具有鲁棒性。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165650581.png"alt="image-20240401165650581" /><figcaption aria-hidden="true">image-20240401165650581</figcaption></figure><h2 id="消融实验">消融实验</h2><p>  MGQFormer的设计包含多分支特征提取器和掩模引导训练。该多分支特征提取器采用了一个额外的BayarConv分支来利用噪声信息，并利用SCAM融合这两个域。利用掩模引导的训练来添加基础GroundTruth信息，引导LQT专注于目标区域，提高查询回复的效率。</p><p>  <strong>噪声分支的消融研究。</strong>定量结果见表4。基线表示我们只使用单个编码器和基于查询的转换器解码器。为了评估噪声分支的有效性，我们使用了一个RGB分支并去除SCAM。我们可以观察到，如果没有噪声分支，哥伦比亚大学的AUC评分下降了1.1%，CASIA大学的AUC评分下降了2.3%。性能提升验证了多分支特征提取器的使用有效地提高了模型的性能。</p><p>  <strong>掩模引导训练的消融研究。</strong>为了证明掩模引导训练的影响，我们在带有图像特征的transformer解码器中只留下LQT，并在训练过程中取出GroundTruth掩模的输入。如表4所示，在没有面具引导训练的情况下，哥伦比亚大学组的AUC评分下降了2.8%，CASIA组下降了3.6%。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165731530.png"alt="image-20240401165731530" /><figcaption aria-hidden="true">image-20240401165731530</figcaption></figure><p>  除了促进定位外，掩模引导训练进一步提高了收敛速度。为了评估这种效果，我们比较了不同时期训练策略的存在和不存在的结果。如图3所示，我们在训练期间显示了合成数据集的验证分割上的AUC（%）分数。事实证明，MGQFrorer在开始时显著促进了训练，在frst时期比没有面具引导训练的模型多了12.7%，并显著加快了收敛速度。这表明，GQT当然有助于transformer解码器提高重构LQT的效率。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165639502.png"alt="image-20240401165639502" /><figcaption aria-hidden="true">image-20240401165639502</figcaption></figure><p>  <strong>GT引导掩膜对应用噪声与否的消融研究。</strong>在图4中，我们展示了参数µ的不同值，表示噪声点的百分比，以验证其对哥伦比亚和IMD20的影响。随着地面真实掩模的增加，有更多的噪声点，得到更鲁棒和广义的模型；然而，较大的值可能会对空间信息造成损害，误导网络。相比之下，较小的µ值提供了一个更准确的地面真实掩模，但模型可能太容易预测辅助掩模和延迟训练。从比较中可以看出，设置为0.01是最优解。点噪声的使用达到了0.9%/1.2%的AUC增益，如表4所示。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165703374.png"alt="image-20240401165703374" /><figcaption aria-hidden="true">image-20240401165703374</figcaption></figure><h1 id="可视化结果">可视化结果</h1><p>  <strong>定性的结果。</strong>如图5所示，我们提供了各种方法的预测伪造掩模。可以观察到PSCC-Net和ManTraNet要么输出错误的区域，要么做出不明确的预测。对可视化结果的比较表明，该方法不仅可以更准确地定位篡改区域，而且还可以输出清晰的区域。它受益于多模态信息和基于查询的transformer解码器，它使用全局注意来生成掩码。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191008224.png"alt="image-20240502191008224" /><figcaption aria-hidden="true">image-20240502191008224</figcaption></figure><p>  <strong>掩膜引导训练的可视化。</strong>为了验证掩模引导训练的有效性，我们展示了MGQFormer预测的掩模，未掩模引导训练生成的掩模，以及图6中的辅助掩模。很明显，MGQFrorer利用地面真实掩模关注伪造区域，从预测掩模与辅助掩模之间的相似性可以看出。具体来说，没有掩模引导训练的网络会对相对较小的物体做出错误的判断。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191142614.png" alt="image-20240502191142614 " style="zoom:70%;" /></p><p>  在图7中，我们进一步展示了表示MGQFormertransformer解码器伪造的LQT注意图与未经掩模引导训练的注意图之间的差异。很明显，在掩膜引导训练中，由于GQT的引导，LQT可以准确地聚焦于目标区域。相比之下，没有掩模引导训练的LQT不能很好地检测伪造，甚至被分配到代表真实位置的完全相反的区域。这一比较表明，所提出的包含来自GT掩模的空间和形状信息的GQT可以迫使LQT集中于我们分配给LQT的正确区域类型。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191302621.png" alt="image-20240502191302621 " style="zoom:67%;" /></p><h1 id="结论">结论</h1><p>  在本文中，我们提出了一种新的基于掩模引导的Transformer框架（MGQFormer）。具体来说，第一步，提取RGB和噪声特征，并进一步融合它们。第二步，将噪声GroundTruth掩码转换为引导查询token（GQT），并将GQT和LQT输入MGQFormer分别估计篡改区域。我们进一步提出了辅助损失和掩模引导损失来指导LQT的重建。可视化结果表明，所提出的掩模引导训练策略对MGQ训练的收敛速度和定位性能有显著影响。在几个基准上的大量实验结果证明了我们的算法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BayarConv </tag>
            
            <tag> Mask-Guided </tag>
            
            <tag> Query-Based </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/"/>
      <url>/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/</url>
      
        <content type="html"><![CDATA[<p>Learning Transferable Visual Models From Natural LanguageSupervision<a href="https://arxiv.org/abs//2103.00020"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/OpenAI/CLIP"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></p><p><strong>官方解读博客：</strong></p><p><a href="https://openai.com/research/clip">CLIP: Connecting text andimages (openai.com)</a></p><h2 id="clip-论文解读"><strong>1 CLIP 论文解读：</strong></h2><h3 id="背景和动机"><strong>1.1 背景和动机</strong></h3><p><strong>[借助文本的监督方法属于有监督和无监督的一个中间地带]</strong>借助文本的监督方法属于：”借助有限的标注数据进行有监督训练” 和“借助几乎无限量的原始文本进行无监督训练”二者之间的中间地带。相同的是，这两种方式都使用静态的 Softmax分类器来执行预测，缺乏动态输出的机制。这严重限制了它们的灵活性和“Zero-Shot” 能力。</p><p><strong>[CLIP 方法及其结果]</strong>在本文中作者研究了借助大规模自然语言监督训练图像分类器。互联网上存在大量公开可用的无标注文本数据集，作者创建了一个包含4亿对(图像，文本) 的新数据集，并通过对比语言-图像预训练的方式训练了 CLIP模型，是一种从自然语言监督中学习视觉模型的有效新方法。作者发现 CLIP类似于 GPT家族，在预训练期间学习执行一系列任务，包括动作识别，OCR，地理定位，ImageNet-1K图像分类，细粒度图像分类任务等。作者通过在30多个现有数据集上对 CLIP 的“Zero-Shot” 迁移学习性能进行测试，并发现 CLIP可以与有监督训练得到的模型性能相当。比如，CLIP 在 ImageNet-1K上的性能与专门有监督训练的 ResNet-50 相当，但是却没有使用 1.28M 的ImageNet-1K 训练数据集。</p><h3 id="自然语言的监督"><strong>1.2 自然语言的监督</strong></h3><p>本文方法的核心是从自然语言的监督中获得感知能力。只要是你的方法具备这一特点，都可以称之为“接受了自然语言的监督”。那这种方法有哪些优势呢？其一就是可扩展性。因为它不需要经典机器学习方法中大量的有标签数据。</p><h3 id="clip-的数据集"><strong>1.3 CLIP 的数据集</strong></h3><p>本文的一个主要特点是想利用互联网上大量公开可用的数据。由于现有的数据集(MS-COCO 约100,000张，YFCC100M 高质量的仅仅约 15M 张，和 ImageNet-1K大小相似) 不够大，可能会低估这一研究领域的潜力。</p><p>为了解决这个问题，作者构建了一个新的数据集，其中包含4亿对(图像，文本)对，这些数据来自互联网上各种公开可用的资源。而且这个数据清理得非常好，质量是非常高的，这也可能是CLIP 这么强大的主要原因之一。结果数据集的总字数与用于训练 GPT-2 的WebText 数据集相似，因此作者将此数据集称为 WebImageText (WIT)。</p><h3 id="clip-的预训练方法"><strong>1.4 CLIP 的预训练方法</strong></h3><p>本文采取基于对比学习的高效预训练方法。作者的思路是这样的：一开始的方法是联合训练了一个处理图像的CNN 和一个处理文本的 Transformer 模型，来预测图像的caption。这个实验结果如下图1的蓝色曲线所示，可以看到其 Scalability是很差的。橘红色曲线是预测文本的词袋，其效率是蓝色曲线的3倍。这两种方法都有一个关键的相似性，即试图去预测每幅图片对应的文字的确切单词是什么。但我们知道这可不是一件容易的事，因为与同一幅图像对应的描述、注释和相关文本种类繁多。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-25eb6d56491be12552c88613e2a0a3d1_720w.png"alt="img" /></p><p>图1：不同方法的 Zero-Shot ImageNet-1K 精度</p><p>基于最近的图像对比表征学习方面的研究，可以仅预测整个文本与哪个图像配对，而不是该文本的确切单词，实验结果如下图1的绿色曲线所示，其效率是橘红色曲线的4倍。具体的做法是：</p><p><strong>对比学习阶段：</strong><br/>如下图2所示，给定一个 Batch 的N个(图片，文本) 对，图片输入给 Image Encoder 得到表征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，文本输入给 Text Encoder得到表征<span class="math inline">\(I_1\)</span>,<spanclass="math inline">\(I_2\)</span>,…,<spanclass="math inline">\(I_N\)</span>，作者认为<spanclass="math inline">\((I_j,T_j)\)</span>属于是正样本，<spanclass="math inline">\((I_i,T_j)\)</span>属于负样本。最大化N个正样本的Cosine 相似度，最小化<spanclass="math inline">\(N^2-N\)</span>个负样本的 Cosine 相似度。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-65e1dbb935aa7804189dc100783a4940_720w.png"alt="img" />]</p><p>图2：CLIP 的对比学习阶段</p><p>作者从头开始训练 CLIP，不使用 ImageNet-1K 权重初始化 ImageEncoder，也不使用预先训练的权重初始化 Text Encoder。同时使用线性投影(权重为<span class="math inline">\(W_i,W_t\)</span>)将每个编码器的表征映射到多模态的嵌入空间。数据增强只使用随机裁剪，温度系数<spanclass="math inline">\(\tau\)</span>的对数形式随整个模型一起训练。</p><p><strong>Zero-Shot Transfer：</strong>如下图4所示，这个阶段是使用 CLIP的预训练好的 Image Encoder 和 Text Encoder 来做 Zero-ShotTransfer。比如来一张 ImageNet-1K 验证集的图片，我们希望 CLIP预训练好的模型能完成这个分类的任务。但是你想想看，这个 Image Encoder是没有分类头 (最后的 Classifier)的，也就是说它没法直接去做分类任务，所以说呢 CLIP 采用了下面的 PromptTemplate 模式：</p><p>比如来一张 ImageNet-1K 验证集的图片，作者把它喂入 CLIP 预训练好的Image Encoder，得到特征 <spanclass="math inline">\(I_1\)</span>，接下来把所有类别的词汇 “cat”, “dog”等，做成一个 prompt：”A photo of a {object}”，并将这个 prompt 喂入 CLIP预训练好的 Text Encoder，依次得到特征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，最后看<strong>哪个的余弦相似度和<spanclass="math inline">\(I_1\)</span>最高</strong>，就代表<strong>该图片是哪个类别的</strong>。</p><p>那我们就可以注意到貌似这个 prompt 的加入很关键，正好弥补了 ImageEncoder 没有分类头的问题，又正好用上了 CLIP 训练好的 Text Encoder。</p><p>而且重要的是，CLIP 的这种推理的方法摆脱了类别的限制，比如一张“三轮车” 的图片，假设 ImageNet 里面没有 “三轮车” 这个类，那么基于ImageNet 所训练的任何模型都无法正确地讲这个图片分类为 “三轮车” ，但是CLIP 的范式是可以做到的，只需要去做成一个 prompt：”A photo of a{tricycle}”。</p><p>那么我们不禁要问：<strong>其他任务可以像这样使用 prompt吗？或者什么样的 prompt 可以带来 Zero-Shot的性能提升？</strong>作者做了实验发现：</p><ul><li><p>对于细粒度图像分类任务，比如 Oxford-IIIT Pets 数据集，prompt就可以设置为：”A photo of a {label}, a type of pet.”。比如 Food101数据集，prompt 就可以设置为：”A photo of a {label}, a type offood.”。比如 FGVC Aircraft 数据集，prompt 就可以设置为：”A photo of a{label}, a type of aircraft.”</p></li><li><p>对于 OCR 任务，加上一些文本或者数字的引号可以提升性能。</p></li><li><p>对于卫星图像分类数据集，prompt 就可以设置为：”a satellite photoof a {label}.”。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2e0561dbbcf5ae00c5053824d2148c25_720w.png"alt="img" />]</p><p>图4：CLIP 的 Zero-Shot Transfer</p></li></ul><p>作者还开脑洞尝试了通过使用多个上下文的 prompt 来 Ensemble 多个Zero-Shot 分类器，比如一个 prompt 是 ‘A photo of a big {label}”，另一个prompt 是 ‘A photo of a small{label}”。作者观察到这样可以可靠地提高性能。在 ImageNet 上，作者集成了80 个不同的上下文提示，这比上面讨论的单个默认提示提高了 3.5%的性能。当一起考虑时，如下图5所示是 Prompt 工程和 Ensemble策略如何改变一组 CLIP 模型的性能，可以看到 Prompt 工程和 Ensemble 策略将ImageNet 精度提高了近 5%，其中蓝色的线代表直接嵌入类名的结果。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-12b0f33645fcb5ffb3fdcea97a2bc0b6_720w.png"alt="img" />]</p><p>图5：prompt 工程和 Ensemble 对 Zero-Shot 性能的影响</p><h3 id="clip-的模型选择"><strong>1.5 CLIP 的模型选择</strong></h3><p>对于 Image Encoder，作者尝试了改进版的 ResNet-50 和 ViT，对于 TextEncoder，作者使用改进版的 Transformer，作者使用了一个带有8个注意头的 63M参数的12层512宽 Transformer 模型，其输入是一个大小为49152的词汇表的BPE[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_2">2]</a>小写表征。为了计算效率，最大序列长度为76。文本序列用[SOS] 和 [EOS] 令牌括起来，[EOS] 处 Transformer末层的输出被视为文本的特征，然后通过 LN，后接 Linear层投影到多模态空间中。</p><p>至于模型缩放的问题，作者发现对于图像编码器ResNet，同时缩放其深度，宽度，和输入分辨率的效果是最优的。而对于文本编码器Transformer，作者只缩放模型的宽度，使其与 ResNet宽度的计算增量成正比，而无需缩放深度，因为作者发现 CLIP的性能对文本编码器的容量不太敏感。</p><h3 id="零样本迁移-zero-shot-transfer-实验结果"><strong>1.6 零样本迁移(Zero-Shot Transfer) 实验结果</strong></h3><p>本节中的 Zero-Shot是指研究对未见过的数据集的泛化性能，也就是说一个模型训练号以后，在它从未见到过的新数据集上的性能如何。</p><p>作者进一步探索 CLIP 的 Zero-Shot 性能。为了说明这一点，作者比较了CLIP 与基于 ResNet-50完全监督的、正则化的逻辑回归分类器的性能。实验结果如下图7所示，在一共对比的27个数据集中，Zero-ShotCLIP 在16个数据集上面战胜了全监督的 ResNet-50 模型。</p><p>在细粒度分类任务上，可以观察到性能上的广泛差异。在其中两个数据集(Stanford Cars 和 Food101) 上，Zero-Shot CLIP 在 ResNet-50特征上的表现比逻辑回归好 20% 以上，而在另外两个数据集 (Flowers102 和FGVCAircraft) 上，Zero-Shot CLIP 的表现比逻辑回归差 10% 以上。在OxfordPets 和 Birdsnap 上，二者的表现更为接近。</p><p>在 ImageNet、CIFAR10/100、STL10 和 PascalVOC2007 等 “更广义的”分类数据集上，二者的性能相对相似，在所有情况下，Zero-Shot CLIP都有轻微的优势。在 STL10 上，CLIP 在不使用任何训练样本的情况下达到了99.3% 的精度。在 Kinetics70 上，CLIP的表现比ResNet-50高出14.5%，在UCF101 上，Zero-Shot CLIP 的性能也比 ResNet-50 的性能高出7.7%。作者推测这估计是因为与 ImageNet中以名词为中心的对象监督相比，自然语言对涉及动词的视觉概念提供了更广泛的监督。</p><p>也可以看到，Zero-Shot CLIP在一些专业、复杂或抽象的任务上相当弱，如卫星图像分类 (EuroSAT和RESISC45)、淋巴结肿瘤检测 (PatchCamelyon)、合成场景中的物体计数(CLEVRCounts)、与自动驾驶相关的任务，如德国交通标志识别(GTSRB)、识别到最近汽车的距离 (KITTI distance)。这些结果突出了 Zero-ShotCLIP 在更复杂任务中的较差能力。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-9f59b5dfeae10e475d35e3a2b715b0ba_720w.png"alt="img" />]</p><p>图7：Zero-Shot CLIP 与完全监督的基线相比具有竞争力</p><blockquote><p><strong>CLIP 零样本迁移的 Data Efficiency</strong></p></blockquote><p>除此之外，作者还进行了一个有趣的实验，即探究 CLIP的零样本迁移的性能与其他模型的少样本学习性能的比较。这里的其他模型，作者使用的是ImageNet-21K 数据集上面预训练的 BiT-MResNet-152x2。如下图8所示的结果是零样本迁移 (Zero-Shot Transfer) 的 dataefficiency，即少样本学习 (Few-Shot Learning)在样本量为多少时的性能能够跟上 CLIP零样本迁移的性能。可以发现每个数据集的效率差异很大，从有的数据集不到一个标记到有的数据集需要184个标记。比如，Flowers102数据集可以在 1-shot 的情况下就能够跟上 CLIP 零样本迁移的性能，但是像FER2013 数据集在 184-shot 的情况下才能够做得到。平均估计数据效率为每个类20.8 个示例。对于 ImageNet 数据集，CLIP零样本迁移的结果与在相同特征空间上训练的 16-shot线性分类器的结果相当。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-f0b49427382cb9fbf87dbc6ae3eec46d_720w.png"alt="img" />]</p><p>图8：CLIP Zero-Shot Transfer 的 data efficiency</p><h3 id="表征学习-representation-learning-实验结果"><strong>1.7 表征学习(Representation Learning) 实验结果</strong></h3><p>为了更全面地评估 CLIP模型的效果，作者进一步评估了它的表征学习能力。关于表征学习的评估方法，有很多方法来评估某个表征的质量，以及一个“理想”的表征应该具有哪些属性。一种比较常见的方法是冻住模型的骨干部分，只训练最后的分类器，通过在某个数据集上的精度来衡量提取到的特性的好坏。</p><p>如下图9所示是本文关于表征学习研究结果。作者首先研究了[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_3">3]</a>论文的12个数据集，虽然像ResNet-50 和 ResNet-101 这样的小型 CLIP 模型比在 ImageNet-1K上训练的其他 ResNet 表现更好，但它们比在 ImageNet-21K (BiT-M) 上训练的ResNet 表现更差。这些小型 CLIP 模型在具有类似计算需求的情况下，也不如EfficientNet系列的模型。作者继续在27个更多的数据集上做了相关研究，在这个更广泛的评估套件上，CLIP的优势更加明显。所有 CLIP模型，无论规模如何，在计算效率方面都优于其他模型。最佳模型的平均分数的提高从2.6% 增加到 5%。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-04eacca100b96d628b95e2f7538bb332_720w.png"alt="img" />]</p><p>图9：CLIP 模型与最先进的计算机视觉模型 Linear Probe 性能的比较</p><p>作者还研究了 CLIP 的特征在各种各样的数据集上与最佳 ImageNet模型的特征的比较。最佳 ImageNet 模型的特征用的是 Noisy StudentEfficientNet-L2 的最佳模型的特征。结果发现在27个数据集上，CLIP取得了21个数据集的优势。CLIP 在需要 OCR (SST2，HatefulMemes)，地理定位和场景识别 (Country211, SUN397) 的任务上改进最多。此外，CLIP在细粒度的汽车和交通标志识别方面也做得更好 (Stanford Cars 和GTSRB)。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2faa222d0c791900c6e1551b568a3d6f_720w.webp"alt="img" />]</p><p>图10：CLIP 的特征在各种各样的数据集上优于最佳 ImageNet 模型的特征</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo技巧合集</title>
      <link href="/Hexo%E6%8A%80%E5%B7%A7/"/>
      <url>/Hexo%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="hexo-butterfly支持mermaid">hexo butterfly支持mermaid</h4><p><a href="https://mermaid.js.org/intro/">mermaid官方文档</a></p><p>如果主题本身自带了mermaid，只需要在config里改mermaid:true即可，以下方法针对没有mermaid的主题。</p><h5 id="安装hexo插件">安装hexo插件</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm i hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure><h5 id="配置">配置</h5><p>在config文件里加入以下代码</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># mermaid chart</span><br><span class="line">mermaid: ## mermaid url https://github.com/knsv/mermaid</span><br><span class="line">  enable: true  # default true</span><br><span class="line">  version: &quot;8.13.8&quot; # default v7.1.2</span><br><span class="line">  options:  # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js</span><br><span class="line">    #startOnload: true  // default true</span><br></pre></td></tr></table></figure><h5 id="主题配置">主题配置</h5><p>找到themes_partials.pug文件，加入这一行代码即可<br/>butterfly的路径为_modules-theme-butterfly.pug</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">script(src=&quot;https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js&quot;)</span><br></pre></td></tr></table></figure><h4 id="hexo-部署-github-错误解决方案">hexo 部署 github错误解决方案</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Failed to connect to github.com port 443 after 21074 ms: Couldn&#x27;t connect to server</span><br></pre></td></tr></table></figure><p>解决方案：</p><p>1.通过git配置文件查看是否使用代理：git config --globalhttp.proxy；</p><p>2.通过git取消代理：</p><ul><li>git config --global --unset http.proxy</li><li>git config --global --unset https.proxy</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>typora技巧</title>
      <link href="/typora%E6%8A%80%E5%B7%A7/"/>
      <url>/typora%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h5 id="图片保存路径">图片保存路径：</h5><p>文件</p><ul><li>自动保存</li></ul><p>图像<br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">../../source/postimages/$&#123;filename&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown合集（三）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>转载自https://blog.csdn.net/jzj_c_love/article/details/122279703</p><p>代码都可以在<code>typora</code>中运行，给出的图片链接语法是<code>Ketax</code>，可能有少数的不适用，但基本可以。</p><h1 id="一基本公式">一、基本公式</h1><h2 id="上下标">1. 上下标</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">A_1^2</span><br><span class="line">\\</span><br><span class="line">B_&#123;12&#125;</span><br><span class="line">\\</span><br><span class="line">2^&#123;x^2+y&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A_1^2\\B_{12}\\2^{x^2+y}\]</span></p><h2 id="分数">2. 分数</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;x&#125;&#123;1+x^2&#125;</span><br><span class="line">\\</span><br><span class="line">\frac&#123;\frac&#123;1&#125;&#123;2&#125;+x&#125;&#123;y&#125;</span><br><span class="line">\\</span><br><span class="line">\tfrac&#123;a&#125;&#123;b&#125;</span><br><span class="line">\frac&#123;a&#125;&#123;b&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\frac{x}{1+x^2}\\\frac{\frac{1}{2}+x}{y}\\\tfrac{a}{b}\frac{a}{b}\]</span></p><h2 id="开根号">3. 开根号</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sqrt&#123;x&#125;</span><br><span class="line">\sqrt[3]&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\sqrt{x}\sqrt[3]{x}\]</span></p><h2 id="组合数">4. 组合数</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\binom&#123;n&#125;&#123;k&#125;</span><br><span class="line">\tbinom&#123;n&#125;&#123;k&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\binom{n}{k}\tbinom{n}{k}\]</span></p><h2 id="导数">5. 导数</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">a&#x27;</span><br><span class="line">a&#x27;&#x27;</span><br><span class="line">a^&#123;\prime&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[a&#39;a&#39;&#39;a^{\prime}\]</span></p><h2 id="取模">6. 取模</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">x \pmod a</span><br><span class="line">\\</span><br><span class="line">2\mod&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[x \pmod a\\2\mod{x}\]</span></p><h2 id="积分">7. 积分</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\int_&#123;1&#125;^&#123;2&#125;</span><br><span class="line">\intop_&#123;2&#125;^&#123;1&#125;</span><br><span class="line">\oint</span><br><span class="line">\smallint</span><br><span class="line">\\</span><br><span class="line">\iint</span><br><span class="line">\oiint</span><br><span class="line">\iiint</span><br><span class="line">\oiiint</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\int_{1}^{2}\intop_{2}^{1}\oint\smallint\\\iint\oiint\iiint\oiiint\]</span></p><h2 id="微分">8.微分</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\nabla&emsp;&emsp;</span><br><span class="line">\partial x&emsp;&emsp;</span><br><span class="line">\mathrm&#123;d&#125;x</span><br><span class="line">\dot x&emsp;&emsp;</span><br><span class="line">\ddot y     </span><br><span class="line">\Delta</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\nabla&amp;emsp;&amp;emsp;\partialx&amp;emsp;&amp;emsp;  \mathrm{d}x \dot x&amp;emsp;&amp;emsp;\ddoty     \Delta\]</span></p><h2 id="累积累乘极限">9.累积/累乘/极限</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sum_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\prod_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\lim_&#123;k \to \infty&#125;</span><br><span class="line">\lim\limits_&#123;k \to \infty&#125;</span><br><span class="line">\lim\nolimits_&#123;k \to \infty&#125;]</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\sum_{i=1}^{k}\displaystyle\sum_{i=1}^n\textstyle\sum_{i=1}^n\\\prod_{i=1}^{k}\displaystyle\prod_{i=1}^n\textstyle\prod_{i=1}^n\\\lim_{k\to \infty}\lim\limits_{k \to \infty}\lim\nolimits_{k \to\infty}]\]</span></p><h1 id="二修饰符号">二、修饰符号</h1><h2 id="简单的帽子">1. 简单的帽子</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\hat&#123;\theta&#125;</span><br><span class="line">\widehat&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;y&#125;</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\tilde&#123;a&#125;</span><br><span class="line">\widetilde&#123;ac&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;a&#125;</span><br><span class="line">\acute&#123;a&#125;</span><br><span class="line">\check&#123;a&#125;</span><br><span class="line">\grave&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\dot&#123;a&#125;</span><br><span class="line">\ddot&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\vec&#123;a&#125;</span><br><span class="line">\overline&#123;a&#125;</span><br><span class="line">\underline&#123;a&#125;</span><br><span class="line">\underset&#123;min&#125;&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\hat&#123;a&#125;</span><br><span class="line">\widehat&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\mathring&#123;a&#125;\dddot&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\ddddot&#123;a&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\hat{\theta}\widehat{AB}\\\bar{y}\overline{AB}\\\tilde{a}\widetilde{ac}\\\bar{a}\acute{a}\check{a}\grave{a}\\\dot{a}\ddot{a}\\\vec{a}\overline{a}\underline{a}\underset{min}{a}\\\hat{a}\widehat{a}\\\mathring{a}\dddot{a}\\\ddddot{a}\]</span></p><h2 id="帽子和袜子">2. 帽子和袜子</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overleftarrow&#123;AB&#125;</span><br><span class="line">\overrightarrow&#123;AB&#125;</span><br><span class="line">\overleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\underleftarrow&#123;AB&#125;</span><br><span class="line">\underrightarrow&#123;AB&#125;</span><br><span class="line">\underleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overbrace&#123;AB&#125;</span><br><span class="line">\underbrace&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\underline&#123;AB&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overleftarrow{AB}\overrightarrow{AB}\overleftrightarrow{AB}\\\underleftarrow{AB}\underrightarrow{AB}\underleftrightarrow{AB}\\\overbrace{AB}\underbrace{AB}\\\overline{AB}\underline{AB}\]</span></p><h2 id="盒子和帽子">3. 盒子和帽子</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overbrace&#123;a+b+c&#125;^&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\underbrace&#123;a+b+c&#125;_&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\boxed&#123;\pi=3.14&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overbrace{a+b+c}^{\text{note}}\\\underbrace{a+b+c}_{\text{note}}\\\boxed{\pi=3.14}\]</span></p><h2 id="各种括号">4. 各种括号</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">(</span><br><span class="line">\big(</span><br><span class="line">\Big(</span><br><span class="line">\bigg(</span><br><span class="line">\Bigg(</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[(\big(\Big(\bigg(\Bigg(\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">[]</span><br><span class="line">&lt;&gt;</span><br><span class="line">|-2|</span><br><span class="line">\&#123;\&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[[]&lt;&gt;|-2|\{\}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\lgroup x \rgroup</span><br><span class="line">\lVert a \rVert</span><br><span class="line">\lceil 2.6 \rceil</span><br><span class="line">\lfloor 1.2 \rfloor</span><br><span class="line">\ulcorner</span><br><span class="line">\urcorner</span><br><span class="line">\llcorner</span><br><span class="line">\lrcorner</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\lgroup x \rgroup\lVert a \rVert\lceil2.6 \rceil\lfloor 1.2\rfloor\ulcorner\urcorner\llcorner\lrcorner\]</span></p><h1 id="三字母">三、字母</h1><h2 id="数学环境默认字体">1、数学环境默认字体</h2><p><spanclass="math display">\[mathnormal\\\mathnormal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathnormal{abcdefghijklmnopqrstuvwxyz}\\\mathnormal{1234567890}\]</span></p><h2 id="意大利体">2、意大利体</h2><p><spanclass="math display">\[mathit\\\mathit{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathit{abcdefghijklmnopqrstuvwxyz}\\\mathit{1234567890}\]</span></p><p>##3、罗马体</p><p><spanclass="math display">\[mathrm\\\mathrm{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathrm{abcdefghijklmnopqrstuvwxyz}\\\mathrm{1234567890}\]</span></p><h2 id="粗体">4、粗体</h2><p><spanclass="math display">\[mathbf\\\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathbf{abcdefghijklmnopqrstuvwxyz}\\\mathbf{1234567890}\]</span></p><h2 id="无衬线体">5、无衬线体</h2><p><spanclass="math display">\[mathsf\\\mathsf{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathsf{abcdefghijklmnopqrstuvwxyz}\\\mathsf{1234567890}\]</span></p><h2 id="打印机体">6、打印机体</h2><p><spanclass="math display">\[mathtt\\\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathtt{abcdefghijklmnopqrstuvwxyz}\\\mathtt{1234567890}\]</span></p><h2 id="手写体">7、手写体</h2><p><spanclass="math display">\[mathcal\\\mathcal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathcal{abcdefghijklmnopqrstuvwxyz}\\\mathcal{1234567890}\]</span></p><h2 id="黑板粗体">8、黑板粗体</h2><p><spanclass="math display">\[mathbb\\\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathbb{abcdefghijklmnopqrstuvwxyz}\\\mathbb{1234567890}\]</span></p><h2 id="花体">9、花体</h2><p><spanclass="math display">\[mathscr\\\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathscr{abcdefghijklmnopqrstuvwxyz}\\\mathscr{1234567890}\]</span></p><h2 id="哥特体">10、哥特体</h2><p><spanclass="math display">\[mathfrak\\\mathfrak{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\\mathfrak{abcdefghijklmnopqrstuvwxyz}\\\mathfrak{1234567890}\]</span></p><h2 id="其他字母">11、其他字母</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\text&#123;字母&#125;</span><br><span class="line">\bf&#123;字母&#125;</span><br><span class="line">\mathit&#123;字母&#125;</span><br><span class="line">\pmb&#123;字母&#125;</span><br><span class="line">\cal&#123;字母&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\text{R}\text{A}\text{C}\text{L}\\\bf{R}\bf{A}\bf{C}\bf{L}\\\mathit{R}\mathit{A}\mathit{C}\mathit{L}\\\pmb{R}\pmb{A}\pmb{C}\pmb{L}\\\cal{R}\cal{A}\cal{C}\cal{L}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\tiny ABCabc</span><br><span class="line">\small ABCabc</span><br><span class="line">\normalsize ABCabc</span><br><span class="line">\large ABCabc</span><br><span class="line">\Large ABCabc</span><br><span class="line">\huge ABCabc</span><br><span class="line">\Huge ABCabc</span><br><span class="line">&#123;\tiny ABC&#125; &#123;\large ABC&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\tiny ABCabc\\\small ABCabc\\\normalsizeABCabc\\\large ABCabc\\\Large ABCabc\\\huge ABCabc\\\Huge ABCabc\\{\tinyABC} {\large ABC}\]</span></p><h2 id="希腊字母">12、希腊字母</h2><p><span class="math display">\[\alpha \beta \gamma \delta \epsilon\zeta \eta \theta \iota \kappa \lambda \mu \nu \xi \pi \rho \sigma \tau\upsilon \phi \chi \psi \omega \\\Gamma \varGamma \gamma \digamma\\\Delta \varDelta \delta \\\epsilon \varepsilon \\\Theta \varTheta\theta \vartheta \\\kappa \varkappa\\\Xi \varXi \xi\\\Pi \varPi \pi\varpi\\\rho \varrho\\\Sigma \varSigma \sigma \varsigma\\\Upsilon\varUpsilon \upsilon\\\Phi \varPhi \phi \varphi\\\Psi \varPsi\psi\\\Omega \varOmega \omega\\\]</span></p><figure><imgsrc="../postimages/MarkDown（三）/e6a627023fa735a129f1725b85da1fa3.png"alt="image" /><figcaption aria-hidden="true">image</figcaption></figure><figure><imgsrc="../postimages/MarkDown（三）/99c3a73a6e705c382b6b2acc99920392.png"alt="image" /><figcaption aria-hidden="true">image</figcaption></figure><h2 id="希伯来字母">13、希伯来字母</h2><p><span class="math display">\[\aleph \beth \daleth \gimel\]</span></p><p>$$</p><p>$$</p><h1 id="四算术运算符号">四、算术运算符号</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\times</span><br><span class="line">\div</span><br><span class="line">\cdot</span><br><span class="line">\%</span><br><span class="line">\circ</span><br><span class="line">\ast</span><br><span class="line">\star</span><br><span class="line">\otimes</span><br><span class="line">\oplus</span><br><span class="line">\odot</span><br><span class="line">\oslash</span><br><span class="line">\pm</span><br><span class="line">\mp</span><br><span class="line">\dotplus</span><br><span class="line">\divideontimes</span><br><span class="line">\textbackslash</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\times\div\cdot\%\circ\ast\star\otimes\oplus\odot\oslash\pm\mp\dotplus\divideontimes\textbackslash\]</span></p><h1 id="五比较运算符">五、比较运算符</h1><p><span class="math display">\[= \ne \neq\\&lt; \nless &gt; \ngtr\\\leq\le \nleq \leqq \nleqq \lneqq \lvertneqq \leqslant \nleqslant\lneq\\\geq \ge \ngeq \geqq \ngeqq \gneqq \gvertneqq \geqslant\ngeqslant \gneq\\\lesssim \lnsim \lessapprox \lnapprox\\\gtrsim \gnsim\gtrapprox \gnapprox\\\prec \nprec \\\succ \nsucc\\\preceq \npreceq\precneqq\\\succeq \nsucceq \succneqq\\\in \notin \ni \owns\\\ll \lll\gg\ggg\\\sim \nsim \simeq \cong \ncong\\\approx\equiv\doteq\\\subset\subseteq \nsubseteq \subsetneq \varsubsetneq \\\subseteqq \nsubseteqq\subsetneqq \varsubsetneqq\\\supset \supseteq \nsupseteq \supsetneq\varsupsetneq\\\supseteqq \nsupseteqq \supsetneqq \varsupsetneqq\\\smile\frown\perp\models\\\mid \nmid\shortmid \nshortmid \shortparallel\nshortparallel\\\vdash \nvdash \dashv\vDash \nvDash \Vvdash\Vdash\nVdash \nVDash\\\propto\bowtie \Join\\\vartriangleleft \ntriangleleft\trianglelefteq \ntrianglelefteq\\\vartriangleright \ntriangleright\trianglerighteq \ntrianglerighteq\\\]</span></p><h1 id="六集合运算符">六、集合运算符</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\in</span><br><span class="line">\owns \not</span><br><span class="line">\subset \not</span><br><span class="line">\supset</span><br><span class="line">\subseteq</span><br><span class="line">\supseteq</span><br><span class="line">\\</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\\</span><br><span class="line">\neg</span><br><span class="line">\emptyset</span><br><span class="line">\varnothing</span><br><span class="line">\\</span><br><span class="line">\because</span><br><span class="line">\forall</span><br><span class="line">\exists</span><br><span class="line">\therefore</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\sqcup</span><br><span class="line">\sqcap</span><br></pre></td></tr></table></figure><p><span class="math display">\[\in\owns \not\subset\not\supset\subseteq\supseteq\\\cap\cup\land\lor\\\neg\emptyset\varnothing\\\because\forall\exists\therefore\cap\cup\land\lor\sqcup\sqcap\]</span></p><h1 id="七各种箭头">七、各种箭头</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\gets</span><br><span class="line">\leftarrow</span><br><span class="line">\to</span><br><span class="line">\rightarrow</span><br><span class="line">\leftrightarrow</span><br><span class="line">\\</span><br><span class="line">\uparrow</span><br><span class="line">\downarrow</span><br><span class="line">\updownarrow</span><br><span class="line">\Leftarrow</span><br><span class="line">\Rightarrow</span><br><span class="line">\Leftrightarrow</span><br><span class="line">\iff</span><br><span class="line">\\</span><br><span class="line">\Uparrow</span><br><span class="line">\Downarrow</span><br><span class="line">\Updownarrow</span><br><span class="line">\nearrow</span><br><span class="line">\searrow</span><br><span class="line">\swarrow</span><br><span class="line">\nwarrow</span><br><span class="line">\longleftarrow</span><br><span class="line">\longrightarrow</span><br><span class="line">\longleftrightarrow</span><br><span class="line">\Longleftarrow</span><br><span class="line">\Longrightarrow</span><br><span class="line">\Longleftrightarrow</span><br><span class="line">\longmapsto</span><br><span class="line">\xrightarrow&#123;over&#125;</span><br><span class="line">\xrightarrow[over]&#123;&#125;</span><br><span class="line">\xrightarrow[under]&#123;over&#125;</span><br><span class="line">\xleftarrow[]&#123;over&#125;</span><br><span class="line">\xleftarrow[under]&#123;&#125;</span><br><span class="line">\xleftarrow[under]&#123;over&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\gets\leftarrow\to\rightarrow\leftrightarrow\\\uparrow\downarrow\updownarrow\Leftarrow\Rightarrow\Leftrightarrow\iff\\\Uparrow\Downarrow\Updownarrow\nearrow\searrow\swarrow\nwarrow\longleftarrow\longrightarrow\longleftrightarrow\Longleftarrow\Longrightarrow\Longleftrightarrow\longmapsto\xrightarrow{over}\xrightarrow[over]{}\xrightarrow[under]{over}\xleftarrow[]{over}\xleftarrow[under]{}\xleftarrow[under]{over}\]</span></p><p><span class="math display">\[\rightarrow \nrightarrow \longrightarrow\Rightarrow \nRightarrow \Longrightarrow \\\leftarrow \nleftarrow\longleftarrow \Leftarrow \nLeftarrow \Longleftarrow \\\leftrightarrow\nleftrightarrow \Leftrightarrow \nLeftrightarrow \longleftrightarrow\iff \Longleftrightarrow\\\uparrow \downarrow \updownarrow \Uparrow\Downarrow \\\nearrow \swarrow \nwarrow \searrow \\\rightharpoonup\rightharpoondown \leftharpoonup \leftharpoondown \\\upharpoonleft\downharpoonleft \upharpoonright \downharpoonright \\\rightleftharpoons\leftrightharpoons \\\curvearrowleft \curvearrowright \circlearrowleft\circlearrowright \\\Lsh \Rsh \upuparrows \downdownarrows\leftleftarrows \rightrightarrows \]</span></p><h1 id="七空间间距">七、空间间距</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A\!B</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\\</span><br><span class="line">A\thinspace B</span><br><span class="line">\\</span><br><span class="line">A\:B</span><br><span class="line">\\</span><br><span class="line">A\ B</span><br><span class="line">\\</span><br><span class="line">A \enspace B</span><br><span class="line">\\</span><br><span class="line">A\quad B</span><br><span class="line">\\</span><br><span class="line">A\qquad B</span><br></pre></td></tr></table></figure><p><span class="math display">\[A\!B\\AB\\A\thinspace B\\A\:B\\A\ B\\A\enspace B\\A\quad B\\A\qquad B\]</span></p><h1 id="八矩阵">八、矩阵</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A=</span><br><span class="line">\begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b &amp; \cdots &amp; c  \\</span><br><span class="line">d &amp; e &amp; \cdots &amp; f  \\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\</span><br><span class="line">g &amp; h &amp; \cdots &amp; j</span><br><span class="line">\end&#123;pmatrix&#125;</span><br><span class="line">\tag&#123;5.1&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A=\begin{pmatrix}a &amp; b &amp; \cdots&amp; c  \\\\d &amp; e &amp; \cdots &amp; f  \\\\\vdots &amp; \vdots&amp; \ddots &amp; \vdots  \\\\g &amp; h &amp; \cdots &amp;j\end{pmatrix}\tag{5.1}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A = \begin&#123;matrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A = \begin{matrix}a &amp; b\\c &amp;d\end{matrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">B = \begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;pmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[B = \begin{pmatrix}a &amp; b\\\\c &amp;d\end{pmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C = \begin&#123;vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[C = \begin{vmatrix}a &amp; b\\\\c &amp;d\end{vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">D = \begin&#123;bmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[D = \begin{bmatrix}a &amp; b\\\\c &amp;d\end{bmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E = \begin&#123;Vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;Vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[E = \begin{Vmatrix}a &amp; b\\\\c &amp;d\end{Vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">f(x) &amp;= (x+1)^2\\</span><br><span class="line">&amp;= x^2 + 2x + 1</span><br><span class="line">\end&#123;aligned&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{aligned}f(x) &amp;=(x+1)^2\\\\&amp;= x^2 + 2x + 1\end{aligned}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">f(x) = \begin&#123;cases&#125;</span><br><span class="line">a &amp;\text&#123;if b&#125;\\</span><br><span class="line">b &amp;\text&#123;if a&#125;\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[f(x) = \begin{cases}a &amp;\text{ifb}\\\\b &amp;\text{if a}\\\\\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x + 2y &amp;= 1\\</span><br><span class="line">3x - y &amp;= 5</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{cases}\begin{aligned}x + 2y&amp;= 1\\\\3x - y &amp;= 5\end{aligned}\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">g(x,y)=\left\&#123;</span><br><span class="line">\begin&#123;array&#125;&#123;rcl&#125;</span><br><span class="line">\frac&#123;M_g - d&#125;&#123;M_f-b&#125;[f(x,y)-b]+d       &amp;      &amp; &#123;b      \leq  f(x,y)  \leq M_f&#125;\\</span><br><span class="line">F^*_L     &amp;      &amp; &#123;S_L \leq 0 &lt; S_M&#125;\\</span><br><span class="line">F^*_R     &amp;      &amp; &#123;S_M \leq 0 &lt; S_R&#125;\\</span><br><span class="line">F_R       &amp;      &amp; &#123;S_R \leq 0&#125;</span><br><span class="line">\end&#123;array&#125; \right.</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[g(x,y)=\begin{cases}\begin{array}{rcl}\frac{M_g -d}{M_f-b}[f(x,y)-b]+d       &amp;      &amp; {b      \leq  f(x,y)  \leqM_f}\\\\F^*_L     &amp;      &amp; {S_L \leq 0 &lt;S_M}\\\\F^*_R     &amp;      &amp; {S_M \leq 0 &lt;S_R}\\\\F_R       &amp;      &amp; {S_R \leq 0}\end{array}\end{cases}\]</span></p><p>九、修改字体大小</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AB</span><br><span class="line">\Huge AB</span><br><span class="line">\huge AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\LARGE AB</span><br><span class="line">\Large AB</span><br><span class="line">\large AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\small AB</span><br><span class="line">\tiny AB</span><br></pre></td></tr></table></figure><p><span class="math display">\[AB\Huge AB\huge AB\\AB\LARGE AB\LargeAB\large AB\\AB\small AB\tiny AB\]</span></p><h1 id="十划掉">十、划掉</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\cancel&#123;5&#125;</span><br><span class="line">\bcancel&#123;5&#125;</span><br><span class="line">\xcancel&#123;ABC&#125;</span><br><span class="line">\not =</span><br></pre></td></tr></table></figure><p><span class="math display">\[\cancel{5}\bcancel{5}\xcancel{ABC}\not=\]</span></p><h1 id="十一常见图形">十一、常见图形</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\Box</span><br><span class="line">\square</span><br><span class="line">\blacksquare</span><br><span class="line">\triangle</span><br><span class="line">\triangledown</span><br><span class="line">\blacktriangle</span><br><span class="line">\diamond</span><br><span class="line">\Diamond</span><br><span class="line">\star</span><br><span class="line">\bigstar</span><br><span class="line">\circ</span><br><span class="line">\bullet</span><br><span class="line">\bigcirc</span><br><span class="line">\bigodot</span><br><span class="line">\diamondsuit</span><br><span class="line">\clubsuit</span><br><span class="line">\heartsuit</span><br><span class="line">\spadesuit</span><br><span class="line">\angle</span><br><span class="line">\measuredangle</span><br><span class="line">\top</span><br><span class="line">\bot</span><br><span class="line">\infty</span><br><span class="line">\checkmark</span><br><span class="line">\dagger</span><br><span class="line">\ddagger</span><br><span class="line">\yen</span><br><span class="line">\$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\Box\square\blacksquare\triangle\triangledown\blacktriangle\diamond\Diamond\star\bigstar\circ\bullet\bigcirc\bigodot\diamondsuit\clubsuit\heartsuit\spadesuit\angle\measuredangle\top\bot\infty\checkmark\dagger\ddagger\yen\\]</span>$</p><h1 id="十二声明宏">十二、声明宏</h1><p>对于一些复杂但是只有少许不同的表达式，可以声明一个函数来调用，提高源码的可读性，减少出错</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\macroname#1#2&#123;</span><br><span class="line">your command</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>宏允许带任意数量的参数（也可以不带参），必须是#1,#2,……这样的命名格式，同时注意再定义宏的时候注意让#1与，否则会解析成#。再调用的时候格式为，可以参考一下的例子</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\Normal#1#2#3&#123;</span><br><span class="line">\frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;\ #3&#125;\exp&#123;[-\frac&#123;(#1 - #2)^2&#125;&#123;2\ #3^2&#125;]&#125;</span><br><span class="line">&#125;</span><br><span class="line">f(x)=\Normal&#123;x&#125;&#123;u_1&#125;&#123;\sigma_1&#125;\\</span><br><span class="line">f(y)=\Normal&#123;y&#125;&#123;u_2&#125;&#123;\sigma_2&#125;\\</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\Normal#1#2#3{\frac{1}{\sqrt{2\pi}\#3}\exp{[-\frac{(#1 - #2)^2}{2\ #3^2}]}}f(x)=\Normal{x}{u_1}{\sigma_1}\\f(y)=\Normal{y}{u_2}{\sigma_2}\\\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\EXP&#123;</span><br><span class="line">e^x = 1 + x + \frac&#123;1&#125;&#123;2!&#125;x^2 + \frac&#123;1&#125;&#123;3!&#125;x^3  + \cdots</span><br><span class="line">&#125;</span><br><span class="line">\EXP</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\EXP{e^x = 1 + x + \frac{1}{2!}x^2 +\frac{1}{3!}x^3  + \cdots}\EXP\]</span></p><h1 id="十三其他">十三、其他</h1><p>1、排版</p><p>公式居中：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E=mc^2\\</span><br></pre></td></tr></table></figure><p><span class="math display">\[E=mc^2\\\]</span></p><p>添加标签：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cos\theta+isin\theta=e^&#123;i\theta&#125;\tag&#123;1.1&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[cos\theta+isin\theta=e^{i\theta}\tag{1.1}\]</span></p><p><span class="math display">\[E=mc^2\]</span></p><p>等号对齐：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;align&#125;f(x)=&amp;x-1\\=&amp;(x-1)(x^2+x+1)\end&#123;align&#125;\\</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\begin{align}f(x)=&amp;x-1\\=&amp;(x-1)(x^2+x+1)\end{align}\\\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;equation&#125; \begin&#123;split&#125;</span><br><span class="line">a &amp;= b + c - d \\</span><br><span class="line">  &amp;= e - f \\</span><br><span class="line">  &amp;= g + h \\</span><br><span class="line">  &amp;= i</span><br><span class="line">\end&#123;split&#125; \end&#123;equation&#125;\\</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{equation} \begin{split}a &amp;= b+ c - d \\  &amp;= e - f \\  &amp;= g + h \\  &amp;= i\end{split}\end{equation}\\\]</span></p><p>2、公式加方框行号 <spanclass="math display">\[\begin{equation}\boxed{a^{2}=b^{2}+c^{2}-2bc\cosA }\\\end{equation}\]</span></p><p><span class="math display">\[{ \bbox[#EFF]{\boxed{\text{求导数:}y=\sin^2\left(\frac1x\right)-2^x.}}}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\ldots \quad \cdots \quad \vdots \quad \ddots \quad \dotsc</span><br></pre></td></tr></table></figure><p><span class="math display">\[\ldots \quad \cdots \quad \vdots \quad\ddots \quad \dotsc\]</span></p><p>3、 求和符号下多行限制条件</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\prod_&#123;k_0,k_1,\ldots&gt;0\atop </span><br><span class="line">&emsp;&emsp; k_0+k_1+\cdots=n&#125;</span><br><span class="line">&emsp;&emsp;&#123;A_&#123;k_0&#125;A_&#123;k_0&#125;\cdots&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\prod_{k_0,k_1,\ldots&gt;0\atop&amp;emsp;&amp;emsp;k_0+k_1+\cdots=n}&amp;emsp;&amp;emsp;{A_{k_0}A_{k_0}\cdots}\]</span></p><p>4、上下标记</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\overline&#123;x+y&#125; \qquad \underline&#123;a+b&#125; \qquad \overbrace&#123;1+2+\cdots+n&#125;^&#123;n个&#125; \qquad \underbrace&#123;a+b+\cdots+z&#125;_&#123;共有26个&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\overline{x+y} \qquad \underline{a+b}\qquad \overbrace{1+2+\cdots+n}^{n个} \qquad\underbrace{a+b+\cdots+z}_{共有26个}\]</span></p><p>5、显示长方程</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;multline&#125;</span><br><span class="line">p(x) = 3x^6 + 14x^5y + 590x^4y^2 + 19x^3y^3\\ </span><br><span class="line">- 12x^2y^4 - 12xy^5 + 2y^6 - a^3b^3</span><br><span class="line">\end&#123;multline&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{multline}p(x) = 3x^6 + 14x^5y +590x^4y^2 + 19x^3y^3\\ - 12x^2y^4 - 12xy^5 + 2y^6 -a^3b^3\end{multline}\]</span></p><p>6、普通数学符号 $$</p><p></p><p><span class="math display">\[7、其他符号\]</span><span class="math display">\[8、可带上下限的数学算子巨型符号\]</span></p><p>\</p><p>\</p><p>\$$</p><p>9、LaTex二元运算符</p><p><span class="math display">\[\triangleleft \triangleright\bigtriangleup \bigtriangledown\\\blacktriangleleft\blacktriangleright\\\wedge \land \vee \lor\\\cap \cup \sqcap\sqcup\\\ddagger \dagger\\\uplus \amalg \diamond \bullet \wr \div\\\odot\oslash \otimes \oplus\\\mp \pm \circ \bigcirc \setminus \\\cdot \ast\times \star\\\]</span></p><p>10、AMS二元运算符 <spanclass="math display">\[\dotplus\smallsetminus\intercal\\\Cap \doublecap\Cup \doublecup\\\barwedge \veebar \doublebarwedge\\\boxminus \boxdot\boxplus\\\ltimes \rtimes\\\divideontimes\\\leftthreetimes\rightthreetimes\\\curlywedge \curlyvee\\\centerdot\\\circleddash\circledast \circledcirc\\\lhd \unlhd \rhd \unrhd\\\]</span></p><p>11、其他 <span class="math display">\[ \begin{CD}  &amp;&amp; @VV\partial V &amp;@VV\partial V&amp;@V V\partial V \\0@&gt;&gt;&gt;S_q(X;G&#39;)@&gt;\phi&gt;&gt;S_q(X;G)@&gt;\psi&gt;&gt;S_q(X;G&#39;&#39;)@&gt;&gt;&gt;0\\ &amp;&amp; @V V\partial V &amp;@VV\partial V&amp;@V V\partial V \\0@&gt;&gt;&gt;S_{q-1}(X;G&#39;)@&gt;\phi&gt;&gt;S_{q-1}(X;G)@&gt;\psi&gt;&gt;S_{q-1}(X;G&#39;&#39;)@&gt;&gt;&gt;0\\ &amp;&amp; @V V\partial V &amp;@VV\partial V&amp;@V V\partial V\end{CD}  \]</span></p><p><span class="math display">\[ \begin{CD}0@&gt;&gt;&gt;G&#39;@&gt;\phi&gt;&gt;G@&gt;\psi&gt;&gt;G&#39;&#39;@&gt;&gt;&gt;0   \end{CD}  \]</span></p><p><spanclass="math display">\[\begin{array}{c|c}      \cap&amp;\color{blue}{BC}\\  \hline    \color{red}A&amp;-3\\          \color{blue}B&amp;-1\\        \color{blue}C&amp;-1\\           \end{array}\]</span></p><p>$$x x \</p><p> $$</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown合集（二）</title>
      <link href="/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="图表绘制"><strong>图表绘制</strong></h5><h6 id="横向流程图"><strong>横向流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><h6 id="竖向流程图">竖向流程图</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><h6 id="标准流程图"><strong>标准流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### 标准流程图（横向）</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### UML时序图：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><p><br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><br/>###### UML时序图源码复杂样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Title: 标题：复杂使用</span><br><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象B-&gt;小三: 你好吗</span><br><span class="line">小三--&gt;&gt;对象A: 对象B找我了</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br><span class="line">Note over 小三,对象B: 我们是朋友</span><br><span class="line">participant C</span><br><span class="line">Note right of C: 没人陪我玩</span><br></pre></td></tr></table></figure><p><br/>###### UML标准时序图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头</span><br><span class="line">  sequenceDiagram</span><br><span class="line">    participant 张三</span><br><span class="line">    participant 李四</span><br><span class="line">    张三-&gt;王五: 王五你好吗？</span><br><span class="line">    loop 健康检查</span><br><span class="line">        王五-&gt;王五: 与疾病战斗</span><br><span class="line">    end</span><br><span class="line">    Note right of 王五: 合理 食物 &lt;br/&gt;看医生...</span><br><span class="line">    李四--&gt;&gt;张三: 很好!</span><br><span class="line">    王五-&gt;李四: 你怎么样?</span><br><span class="line">    李四--&gt;王五: 很好!</span><br></pre></td></tr></table></figure><p><br/>###### 甘特图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 语法示例</span><br><span class="line">        gantt</span><br><span class="line">        dateFormat  YYYY-MM-DD</span><br><span class="line">        title 软件开发甘特图</span><br><span class="line">        section 设计</span><br><span class="line">        需求                      :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">        原型                      :active,  des2, 2014-01-09, 3d</span><br><span class="line">        UI设计                     :         des3, after des2, 5d</span><br><span class="line">    未来任务                     :         des4, after des3, 5d</span><br><span class="line">        section 开发</span><br><span class="line">        学习准备理解需求                      :crit, done, 2014-01-06,24h</span><br><span class="line">        设计框架                             :crit, done, after des2, 2d</span><br><span class="line">        开发                                 :crit, active, 3d</span><br><span class="line">        未来任务                              :crit, 5d</span><br><span class="line">        耍                                   :2d</span><br><span class="line">        section 测试</span><br><span class="line">        功能测试                              :active, a1, after des3, 3d</span><br><span class="line">        压力测试                               :after a1  , 20h</span><br><span class="line">        测试报告                               : 48h</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown合集（一）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一编写标题">一、编写标题</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 一级标题(h1)</span><br><span class="line">## 二级标题(h2)</span><br><span class="line">### 三级标题(h3)</span><br><span class="line">#### 四级标题(h4)</span><br><span class="line">##### 五级标题(h5)</span><br><span class="line">###### 六级标题(h6)</span><br></pre></td></tr></table></figure><h5 id="二字体">二、<strong>字体</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*斜体文本*</span><br><span class="line">_斜体文本_</span><br><span class="line">**粗体文本**</span><br><span class="line">__粗体文本__</span><br><span class="line">***粗斜体文本***</span><br><span class="line">___粗斜体文本___</span><br></pre></td></tr></table></figure><p><em>斜体文本</em><br/><em>斜体文本</em><br/><strong>粗体文本</strong><br/><strong>粗体文本</strong><br/><strong><em>粗斜体文本</em></strong><br/><strong><em>粗斜体文本</em></strong></p><h5 id="三分割线">三、<strong>分割线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">***</span><br><span class="line"></span><br><span class="line">* * *</span><br><span class="line"></span><br><span class="line">*****</span><br><span class="line"></span><br><span class="line">- - -</span><br><span class="line"></span><br><span class="line">----------</span><br></pre></td></tr></table></figure><hr /><hr /><hr /><hr /><hr /><h5 id="四删除线">四、<strong>删除线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RUNOOB.COM</span><br><span class="line">GOOGLE.COM</span><br><span class="line">~~BAIDU.COM~~</span><br></pre></td></tr></table></figure><p>RUNOOB.COM<br/>GOOGLE.COM<br/><del>BAIDU.COM</del></p><h5 id="五下划线">五、<strong>下划线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;带下划线文本&lt;/u&gt;</span><br></pre></td></tr></table></figure><p><u>带下划线文本</u></p><h5 id="六脚注">六、<strong>脚注</strong></h5><p>脚注是对文本的补充说明。</p><p>Markdown 脚注的格式如下:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[^要注明的文本]</span><br></pre></td></tr></table></figure><p>以下实例演示了脚注的用法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建脚注格式类似这样 [^RUNOOB]。</span><br><span class="line"></span><br><span class="line">[^RUNOOB]: 菜鸟教程 -- 学的不仅是技术，更是梦想！！！</span><br></pre></td></tr></table></figure><h5 id="七markdown列表">七、<strong>Markdown列表</strong></h5><p>Markdown 支持有序列表和无序列表。</p><p>无序列表使用星号(*<strong>)、加号(+</strong>)或是减号(-**)作为列表标记：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* 第一项</span><br><span class="line">* 第二项</span><br><span class="line">* 第三项</span><br><span class="line"></span><br><span class="line">+ 第一项</span><br><span class="line">+ 第二项</span><br><span class="line">+ 第三项</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- 第一项</span><br><span class="line">- 第二项</span><br><span class="line">- 第三项</span><br></pre></td></tr></table></figure><ul><li><p>第一项<br/>* 第二项<br/>* 第三项</p></li><li><p>第一项<br/>+ 第二项<br/>+ 第三项</p></li><li><p>第一项</p></li><li><p>第二项</p></li><li><p>第三项</p></li></ul><h5 id="八列表嵌套">八、<strong>列表嵌套</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 第一项：</span><br><span class="line">    - 第一项嵌套的第一个元素</span><br><span class="line">    - 第一项嵌套的第二个元素</span><br><span class="line">2. 第二项：</span><br><span class="line">    - 第二项嵌套的第一个元素</span><br><span class="line">    - 第二项嵌套的第二个元素</span><br></pre></td></tr></table></figure><ol type="1"><li>第一项：<br/> - 第一项嵌套的第一个元素<br/> -第一项嵌套的第二个元素<br/>2. 第二项：<br/> -第二项嵌套的第一个元素<br/> - 第二项嵌套的第二个元素</li></ol><h5 id="九markdown区块">九、<strong>Markdown区块</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 区块引用</span><br><span class="line">&gt; 菜鸟教程</span><br><span class="line">&gt; 学的不仅是技术更是梦想</span><br></pre></td></tr></table></figure><blockquote><p>区块引用<br/>&gt; 菜鸟教程<br/>&gt; 学的不仅是技术更是梦想</p></blockquote><h5 id="十markdown代码">十、<strong>Markdown代码</strong></h5><p>如果是段落上的一个函数或片段的代码可以用反引号把它包起来（<strong>`</strong>），例如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`printf()` 函数</span><br></pre></td></tr></table></figure><p>演示效果如下：</p><p><code>printf()</code> 函数</p><h5 id="十一代码块">十一、<strong>代码块</strong></h5><p>代码区块使用 <strong>```</strong>包裹一段代码，并指定一种语言（也可以不指定）：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```javascript</span><br><span class="line">$(document).ready(function () &#123;</span><br><span class="line">    alert(&#x27;RUNOOB&#x27;);</span><br><span class="line">&#125;);</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">$(document).ready(function () &#123;</span></span><br><span class="line"><span class="string">    alert(&#x27;RUNOOB&#x27;);</span></span><br><span class="line"><span class="string">&#125;);</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br></pre></td></tr></table></figure><h5 id="十二markdown链接"><strong>十二、Markdown链接</strong></h5><p>链接使用方法如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[链接名称](链接地址)</span><br><span class="line">或者</span><br><span class="line">&lt;链接地址&gt;</span><br></pre></td></tr></table></figure><p>例如：</p><p>这是一个链接 <ahref="https://www.cnblogs.com/caoleiCoding/">云中志</a></p><p>https://www.cnblogs.com/caoleiCoding/</p><h5 id="十三高级链接">十三、<strong>高级链接</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">链接也可以用变量来代替，文档末尾附带变量地址：</span><br><span class="line">这个链接用 1 作为网址变量 [Google][1]</span><br><span class="line">这个链接用 runoob 作为网址变量 [Coding][Coding]</span><br><span class="line">然后在文档的结尾为变量赋值（网址）</span><br><span class="line"></span><br><span class="line">  [1]: http://www.google.com/</span><br><span class="line">  [Coding]: https://www.cnblogs.com/caoleiCoding/</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><p>这个链接用 1 作为网址变量<ahref="http://www.google.com/%3Cbr/%3E%5BCoding%5D:%20https://www.cnblogs.com/caoleiCoding/">Google</a></p><p>这个链接用 runoob 作为网址变量 [Coding][Coding]</p><h5 id="十四markdown图片">十四、<strong>Markdown图片</strong></h5><p>Markdown 图片语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![alt 属性文本](图片地址)</span><br><span class="line"></span><br><span class="line">![alt 属性文本](图片地址 &quot;可选标题&quot;)</span><br></pre></td></tr></table></figure><ul><li>开头一个感叹号 !</li><li>接着一个方括号，里面放上图片的替代文字</li><li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上选择性的‘title’ 属性的文字。</li></ul><p>使用示例：</p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><h5 id="十五markdown表格"><strong>十五、Markdown表格</strong></h5><p>Markdown 制作表格使用 <strong>|</strong> 来分隔不同的单元格，使用<strong>-</strong> 来分隔表头和其他行。</p><p>语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|  表头   | 表头  |</span><br><span class="line">|  ----  | ----  |</span><br><span class="line">| 单元格  | 单元格 |</span><br><span class="line">| 单元格  | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th>表头</th><th>表头</th></tr></thead><tbody><tr class="odd"><td>单元格</td><td>单元格</td></tr><tr class="even"><td>单元格</td><td>单元格</td></tr></tbody></table><h5 id="十六对齐方式">十六、<strong>对齐方式</strong></h5><p><strong>我们可以设置表格的对齐方式：</strong></p><ul><li><strong>-:</strong> 设置内容和标题栏居右对齐。</li><li><strong>:-</strong> 设置内容和标题栏居左对齐。</li><li><strong>:-:</strong> 设置内容和标题栏居中对齐。</li></ul><p>示例如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :-----| ----: | :----: |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th style="text-align: left;">左对齐</th><th style="text-align: right;">右对齐</th><th style="text-align: center;">居中对齐</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr><tr class="even"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr></tbody></table><h5 id="十七缩进">十七、<strong>缩进</strong></h5><p>总结一下缩进的方式：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;能缩进一个汉字，可叠加</span><br><span class="line">&amp;ensp;能缩进半个汉字，可叠加</span><br><span class="line">&amp;nbsp;能缩进四分之一，可叠加</span><br></pre></td></tr></table></figure><p>一般需要所缩进两个汉字，以下方式都可以缩进两个汉字</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;&amp;emsp;</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;</span><br><span class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br></pre></td></tr></table></figure><p> 能缩进一个汉字，可叠加</p><p> 能缩进半个汉字，可叠加</p><p> 能缩进四分之一，可叠加</p><p>  能缩进两个汉字，可叠加</p><h5id="十八markdown高级技巧">十八、<strong>Markdown高级技巧</strong></h5><h6 id="支持的-html-元素"><strong>支持的 HTML 元素</strong></h6><p>不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML撰写。</p><p>目前支持的 HTML 元素有：<code>&lt;kbd&gt;</code><code>&lt;b&gt;</code> <code>&lt;i&gt;</code> <code>&lt;em&gt;</code><code>&lt;sup&gt;</code> <code>&lt;sub&gt;</code><code>&lt;br&gt;</code>等 ，如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑</span><br></pre></td></tr></table></figure><p>使用 <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> 重启电脑</p><h6 id="转义"><strong>转义</strong></h6><p>Markdown使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown使用反斜杠转义特殊字符：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">**文本加粗** </span><br><span class="line">\*\* 正常显示星号 \*\*</span><br></pre></td></tr></table></figure><p>显示效果：</p><p><strong>文本加粗</strong> <br/>** 正常显示星号 **</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A New Benchmark and Model for Challenging Image Manipulation Detection</title>
      <link href="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/"/>
      <url>/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<p>A New Benchmark and Model for Challenging Image ManipulationDetection</p><p><spanclass="math inline">\(ZhenfeiZhang^1,MingyangLi^2,Ming-ChingChang^1\)</span></p><p>美国纽约州立大学奥尔巴尼大学计算机科学系</p><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></p><h1 id="摘要">摘要</h1><p>  <strong>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。此外，基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</strong><br/>  为了研究在这些具有挑战性的条件下最先进的（SoTA）IMD方法，我们引入了一个新的具有挑战性的图像操作检测（CIMD）基准数据集，它由两个子集组成，分别用于评估基于编辑和基于压缩的IMD方法。数据集的图像是手工拍摄和篡改高质量的注释。<br/>  此外，我们提出了一种新的基于HRNet的双分支网络模型，该模型可以在这些具有挑战性的条件下更好地检测图像编辑和压缩伪影。在CIMD基准上的大量实验表明，我们的模型在CIMD上显著优于SoTAIMD方法。<br/>本文的贡献包括：</p><ul><li>我们提出了一种新的双分支架构，结合了RGB和频率特征，以实现具有挑战性的图像操纵检测。据我们所知，我们的模型是第一个专注于检测小的被篡改区域的方法。</li><li>我们引入了开创性的压缩伪影学习模型，能够检测双压缩伪影，无论量化因子（QFs）是不同的还是相同的。</li><li>我们引入了一个新的高质量的CIMD基准来评估SoTAIMD方法在具有挑战性的操作中的性能。我们将在书面接受后公开CIMD。</li><li>在CIMD上的大量实验表明，该方法在具有挑战性的图像操作检测方面显著优于SoTA。</li></ul><h1 id="数据集">数据集</h1><p>The Challenging Image Manipulation Detection Dataset(CIMD)</p><p>  在这项工作中，我们的目标是建立一个全面的验证数据集（CIMD），专门用于在压缩和未压缩场景下的小区域伪造（平均小于1.5%）。我们的数据集在数据集大小、图像质量、图像多样性和伪造策略方面都具有优势。<br/>  引入了两个独立的子集来分别评估基于图像编辑和基于压缩的方法。收集我们使用佳能RP相机捕获原始图像，包括未压缩的TIFF和压缩的JPG伪造-原始图像对。这些捕捉是在高度多样的多季节拍摄的，特点是复杂和复杂的照明条件。我们的目的是在现实生活中提供一个公正和全面的模型评估。<br/>  两个CIMD数据集。我们提供了两个子集：</p><blockquote><p>CIMD-Raw子集由成对的原始未压缩的TIFF图像组成，用于评估基于图像编辑的方法。<br/>&gt;CIMD-压缩子集包括拼接伪图像及其对应的原始JPEG图像，其统一量化因子（QFs）范围在50到100之间。这个子集评估了基于压缩的模型在相同的QF条件下检测伪造的能力。</p></blockquote><h3 id="cimd-raw子集cimd-r">CIMD-Raw子集(CIMD-R)</h3><p>  CIMD-R旨在提供一个对基于图像编辑的模型在检测未压缩图像上的小篡改复制移动、对象删除和拼接伪造方面的性能的全面评估。未压缩图像的使用消除了伪造区域上不希望的压缩伪影，否则可以被神经网络感知，使对检测的更真实的性能评估。CIMD-R由600张TIFF图像组成，分辨率为2048×1365。还提供了ground-truth标签。此外，CIMD-R采用了一种面向未来的方法，提供16bit的图像对，可以提供多达<spanclass="math inline">\(2^{48}\)</span>种（以万亿）颜色。<br/>  对于复制-移动操作，将图像的一部分复制和粘贴到同一图像中，然后是五种后处理方法，即缩放、旋转、水平/曲线增加、光照变化和颜色再分配。<br/>  对于删除伪造操作，通过ps中的内容感知填充来从图像中删除选定的区域。内容感知填充被广泛应用于多个数据集（Parketal.2018b；Dong，Wang，和Tan2013b），代表了PS根据周围区域绘制物体的最佳猜测。<br/>  对于拼接伪造操作，将一个图像的区域复制粘贴到另一个图片。然后，采用复制-移动操作中相同的后处理方法，使锻造区域与周围环境相协调。</p><h3 id="cimd-compressed子集cimd-c">CIMD-Compressed子集(CIMD-C)</h3><p>  CIMD-C旨在评估基于压缩的模型在主压缩和二次压缩具有相同的QFs时检测双JEPG压缩伪影的能力。该数据集包含200张JPEG图像，分辨率为2048×1365，其中QF均匀分布为50≤QF&lt;100。<br/>  伪造图像的生成类似于CIMD-R的拼接样本，区别在于伪造图像使用JPEG压缩算法保存，使用与原始图像相同的QF。原始图像由RAW文件生成，确保原始图像第一次被压缩，增强了数据集的可信度。在伪造的图像中，背景是双压缩的，而被篡改的区域是单压缩的。此外，该数据集还包括用于压缩的二进制掩码和QF值，从而增强了其对进一步研究不同QFs的影响的效用。</p><h1 id="提出的imd方法">提出的IMD方法</h1><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><p>  我们提出的双分支架构能够检测异常特征和压缩伪影，其灵感来自于（Kwonet al.2022）。此外，我们的模型可以有效地检测小的操作区域和识别双压缩轨迹，应用相同的量化矩阵（Q-矩阵）。为了实现我们的研究目标，我们采用HR-Net（Wanget al.2020）作为我们模型的支柱，基于其提供三倍收益的能力。首先，HR-Net中没有池化层，这确保了这些特性在整个过程中保持高分辨率。其次，该模型在处理不同尺度的特征的同时，也要处理有效的信息交换，这对于获取不同尺度的信息至关重要。最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。<br/>  为了更精确地定位微小的篡改区域，我们应用面积空间金字塔池（ASPP）（Chen等2017；Yang等2021）和注意机制（Vaswani等2017；胡，沈，和孙2018）仔细设计了模型。<br/>  对于RGB流，输入的图像被输入到一个完整的HR-Net，它从视觉内容中学习图像编辑跟踪。<br/>  对于DCT流，我们向主干提供量化的DCT系数、q矩阵和新的多次重压缩残差DCT系数，以检测双压缩伪影。这种设计工作，不考虑QF是否相同。为了提高所提出的双分支模型的性能，我们在最后引入了一种自适应加权热图聚合设计，使用软选择来融合由两个分支生成的热图。<br/>  其次，该模型在处理不同尺度的特征的同时，还能处理有效的信息交换，这对于获取不同尺度的信息至关重要。<br/>  最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。我们对RGB流应用完整的HR-Net，而对于频率流，我们使用三分辨率的变体HR-Net，用图5中所示的压缩伪影学习模型代替第一阶段。</p><h2id="压缩伪影学习模型compression-artifacts-learning-model">压缩伪影学习模型CompressionArtifacts Learning Model</h2><p>  当使用相同的QF创建拼接图像时，被操纵的区域被单独压缩，而背景区域被双重压缩。因此，当图像被反复压缩时，不稳定的量化DCT系数逐渐集中在被篡改的区域上，而真实的区域则保持相对稳定。在此基础上，我们引入了一种新的残差DCT图来指导DCT特征，以更好地关注IMD的不稳定区域。</p><p>YCbCr 是一种用于压缩彩色图像的色彩空间，其中：</p><ul><li><strong>Y 通道</strong>表示亮度（Luminance），也即图像的灰度级信息。</li><li><strong>Cb 和 Cr 通道</strong>分别表示蓝色差异和红色差异（Chrominance），即色彩信息。</li></ul><p>  我们的方法只关注于y通道DCT图，因为它对人眼更敏感。给定一个JPEG图像，很容易从JPEG文件报头中得到y通道量化的DCT系数<spanclass="math inline">\(Q_0\)</span>及其相应的<spanclass="math inline">\(Q\)</span>矩阵。<br/>首先重复<spanclass="math inline">\(Q\)</span>矩阵具有与<spanclass="math inline">\(Q_0\)</span>相同的大小，我们将重复的<spanclass="math inline">\(Q\)</span>矩阵设为<spanclass="math inline">\(q\)</span>。</p><p>  然后，我们使用以下方程依次计算(k+1)次再压缩量化JPEG系数<spanclass="math inline">\(Q_{k+1}\)</span>：</p><p><spanclass="math display">\[\begin{cases}\begin{array}{l}D_k=Q_k\odotq\\B_k=IDCT(D_k)\\I_{k+1}=RT(B_k)\\Q_{k+1}=[DCT(I_{k+1})\oslashq]\end{array}\end{cases}\]</span>   其中<spanclass="math inline">\(\oslash\)</span>表示元素级划分，D、B、I、Q分别表示去量化的DCT系数、使用逆DCT进行反变换块的DCT系数、使用图像块进行反变换块的DCT系数和使用量化JPEG系数进行反变换块的DCT系数。上述方程中变量的下标表示重压缩的次数，我们实验设置了<spanclass="math inline">\(k=7\)</span>。<spanclass="math inline">\(RT(.)\)</span>是四舍五入和截断操作。<spanclass="math inline">\([.]\)</span>表示该舍入操作。</p><p>  然后，将k次重压缩后的残余去量化DCT系数R定义为：</p><p><spanclass="math display">\[R=\frac{1}{k}\sum_{i=1}^{k}(Q_i-Q_{i-1})\]</span>  对于原始的通道系数<spanclass="math inline">\(Q_0\)</span>​，在把它们转换成一个二进制卷之后，我们使用一个阈值<spanclass="math inline">\(T\)</span>​来执行一个剪切操作，将此二进制值转换表示为：</p><p><span class="math display">\[f:Q_o^{H\times W}\rightarrow\{0,1\}^{(T+1)\times H\times W}\]</span>   DCT系数<spanclass="math inline">\(Q_0\)</span>​被转换为二进制：</p><p><span class="math display">\[f(Q_0(i,j))=\begin{cases}1, &amp;if|clip(Q_0(i,j))|=t,t\in [0,T]\\\\0, &amp;otherwise\end{cases}\]</span></p><p>  利用<span class="math inline">\(clip(.)\)</span>提取<spanclass="math inline">\([−T,T]\)</span>中的直方图特征，这对GPU内存约束是必不可少的。我们实验中将T设为20。此外，我们应用绝对值运算作为DCT直方图显示的对称性。<br/><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /></p><p>  在频率流中，首先将图像输入到图5所示的<strong>压缩伪影学习模型</strong>中，提取各种DCT特征。随后，DCT特性被输入到HR-Net的一个变体中，该变体在三种不同的分辨率（1/8、1/16和1/32）下运行。</p><h2 id="注意力空间金字塔池aspp">注意力空间金字塔池ASPP</h2><p>  为了精确地定位小的篡改区域，我们使用图6(a)所示的注意力空间金字塔池（ASPP）仔细设计了我们的模型。ASPP通过不同的接受域捕获远程距离信息，并处理尺度变化。它由三个具有不同速率的扩张卷积层和一个全局平均池化（GAP）组成。得到的特征被连接并传递到1×1卷积。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095235566.png" alt="image-20240329095235566 " style="zoom:50%;" /></p><h2 id="注意力交互机制">注意力交互机制</h2><p>其中</p><ul><li>左边输入为四个分辨率的分支</li><li>CA：通道注意力ChannelAttention</li><li>UP：双线性上采样BilinearUp-sampling</li><li>SA：空间注意力SpatialAttention</li></ul><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329100333958.png" alt="image-20240329100333958 " style="zoom:45%;" /></p><p>接下来，我们将描述注意力如何在RGB流中交互式地工作，其中的过程实际上与频率流相同，具有不同数量的输出分辨率分支。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329141519802.png" alt="image-20240329141519802 " style="zoom:50%;" /></p><h3 id="通道注意力channelattention">通道注意力ChannelAttention</h3><p>左侧输入为HRnet不同分辨率下的输入：</p><p><span class="math display">\[I\in R^{H \times W \times3}\rightarrow\begin{cases}\begin{aligned}F_1 \in R^{H/4 \times W/4\times C_1} &amp;  &amp; ,C_1=48\\F_2 \in R^{H/8 \times W/8 \times C_2}&amp;  &amp; ,C_2=96\\F_3 \in R^{H/16 \times W/16 \times C_3}&amp;  &amp; ,C_3=192\\F_4 \in R^{H/32 \times W/32 \times C_4}&amp;  &amp; ,C_4=384\\\end{aligned}\end{cases}\]</span>自下而上的通道注意特征的计算使用如下： <span class="math display">\[F_n= C(F_{n+1})\odot F_n, n = 1,2,3\]</span> 其中，<spanclass="math inline">\(C(.)\)</span>表示通道注意块，如下图所示，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。由于<spanclass="math inline">\(F_4\)</span>包含了最高级别的语义信息，因此它在通道级别上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095324781.png" alt="image-20240329095324781 " style="zoom:50%;" /><span class="math display">\[C(F)=\sigma(E(GAP(Conv_{1\times1}(F))))\]</span></p><p>  其中1×1卷积，以减少信道，GAP（·）为全局平均池，激励过程 <spanclass="math inline">\(E(.)=C^{&#39;}\rightarrow C^{&#39;}/r \rightarrowC^{&#39;}，r=4\)</span>，<span class="math inline">\(\sigma(.)\)</span>为Sigmoid激活函数。</p><h3id="双线性上采样bilinearup-sampling">双线性上采样BilinearUp-sampling</h3><p>在应用自底而上的信道注意后，使用双线性上采样方法对特征图<spanclass="math inline">\(F_2\)</span>、<spanclass="math inline">\(F_3\)</span>和<spanclass="math inline">\(F_4\)</span>进行上采样，以匹配<spanclass="math inline">\(F_1\)</span>的分辨率。</p><h2 id="空间注意力spatialattention">空间注意力SpatialAttention</h2><p>应用自上而下路径的空间注意机制，由： <spanclass="math display">\[F_m=S(F_{m-1})\otimes F_m,m = 2, 3, 4,\]</span>其中，S(.)为空间注意力，如下图所示。由于<spanclass="math inline">\(F_1\)</span>包含了丰富的空间信息，因此在空间层面上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329142515201.png" alt="image-20240329142515201 " style="zoom:50%;" /></p><h2 id="热图聚合heatmapaggregation">热图聚合HeatmapAggregation</h2><p>  每个分支的特征图在经过上采样和交互注意后，具有相同的分辨率。然后将这些特征连接在一起，形成最终特征，用于推理阶段的自适应加权热图聚合。</p><p>  我们的模型生成了两个最终的热图，它们通过软选择进行聚合。具体来说，我们采用双线性特征上采样来升级频率流的热图，以匹配RGB流热图的分辨率。然后，我们将Softmax激活函数应用于热图，然后使用全局最大池化（GMP），记为GMP（·），来选择主热图及其相应的权重。这种选择是基于更高的值，这表明与其他热图相比，它具有更强的定位响应。</p><p>  我们使用<span class="math inline">\(h_m\)</span>和<spanclass="math inline">\(h_s\)</span>​​定义主热图和次热图。因此，加权聚合热图h可以表示为：</p><p><span class="math display">\[h = GMP(h_m)\cdot h_m+(1-GMP(h_m))\cdoth_s\]</span></p><p>  最后，我们在预测的二值掩模上应用一个不可训练的GMP来执行图像级检测，因为图像级检测与像素级预测高度相关。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p><strong>数据集。</strong>本研究中使用的训练数据集大多采用了（Kwonetal.2022），其中包括CASIAv2、FantasticReality、IMD2020，以及专门为使用不同QFs检测压缩伪影而设计的数据集。测试阶段需要使用CIMD-R和CIMD-C来分别评估基于图像编辑和基于压缩的方法的有效性。补充材料中提供了有关所使用的数据集的进一步细节，以及与选定的公开可访问的数据集进行的评价指标的比较分析。</p><p><strong>实施细节。</strong>我们的模型是使用PyTorch（Paszke等人，2019年）实现的，并在8个RTX2080GPUs，上进行训练，批处理大小为4。我们将初始学习率设置为0.001，并呈指数衰减。为了减轻不平衡数据集对模型训练的影响，我们从每批的每个数据集中随机选择相同数量的样本。训练过程总共250批。该模型被设计为接受各种图像格式，包括JPEG和非JPEG格式。对于非jpeg图像，该模型采用全1的Q矩阵对样本进行压缩，这相当于使用100的QF进行无损压缩。RGB流的主干使用ImageNet进行预训练（(Krizhevsky,Sutskever,andHinton2017），而DCT流使用（Parketal.2018a）引入的双压缩图像进行预训练的方法。为了提高模型检测小篡改区域的灵敏度，设计了训练目标来<strong>最小化像素级的二值交叉熵损失</strong>。</p><p><strong>比较网络的选择。</strong>为了保证公平的比较和评估之前使用新引入的CIMD的模型，我们选择了使用这两个标准的最先进的方法：(1)预训练模型是公开的，(2)我们使用的评估数据集不在它们的训练集中。根据这些标准，我们选择了RRU-Net、MantraNet、CR-CNN、SPAN、PSCC-Net、MVSS-Net、IF-OSN、CAT-Net、DJPEG和对照品。其中，DJPEG和Comprint被设计用于压缩伪影检测，而CATNet可以联合检测异常特征和压缩伪影。上述所有的研究均在相关的工作部分中被适当地引用。我们使用CIMD-R来评估基于图像编辑的方法，用CIMD-C来评估基于压缩的方法。</p><h2 id="在cimd-r上的评估结果">在CIMD-R上的评估结果</h2><p>  使用CIMD-R子集进行评估。表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213659139.png"alt="image-20240327213659139" /><figcaption aria-hidden="true">image-20240327213659139</figcaption></figure><p>表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。像素级f1评分使用每张图像的最佳f1阈值，并使用固定的f1阈值0.5。最佳分数用粗体突出显示。我们的方法在图像级和像素级的检测任务中都取得了最好的性能。值得注意的是，我们的方法在图像级和像素级评估方面都优于现有的SOTA方法，这表明了它在检测小篡改区域方面的优越性。</p><h2id="在cimd-c上评价基于压缩的方法的评估结果">在CIMD-C上评价基于压缩的方法的评估结果</h2><p>  表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213738541.png"alt="image-20240327213738541" /><figcaption aria-hidden="true">image-20240327213738541</figcaption></figure><p>表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。我们的方法在整体性能方面仍然是表现最好的，突出了我们的方法对于具有相同QF的双压缩图像的有效性。</p><h2 id="消融研究">消融研究</h2><p>  我们提供了一个如表3所示的简单的消融研究。请注意，我们的RGB流在压缩的数据和未压缩的数据中都是有效的。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327214430772.png"alt="image-20240327214430772" /><figcaption aria-hidden="true">image-20240327214430772</figcaption></figure><p>  值得注意的是，由于没有压缩伪影，频率流在CIMD-R中不能产生令人满意的结果。然而，当这两个分支协同工作时，模型的性能在定位和检测评估方面都有所提高。在补充材料中提供了额外的烧蚀研究和实验结果。</p><h1 id="结论">结论</h1><p>  本研究提出了一种新的具有挑战性的图像处理检测（CIMD）数据集，它包括两个子集，分别用于评估基于图像编辑和基于压缩的方法。这些数据集被手动获取和篡改，并提供了高质量的注释。此外，我们提出了一种双分支的方法，在使用CIMD数据集检测图像操作方面优于最先进的模型。我们已经发布了我们的数据集，以促进未来的研究。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RGB </tag>
            
            <tag> HRnet </tag>
            
            <tag> BayarConv </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization</title>
      <link href="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/"/>
      <url>/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<p>Exploring Multi-Modal Fusion for Image Manipulation Detection andLocalization <a href="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></p><p>希腊信息技术研究所，研究和技术研究中心，希腊塞萨洛尼基</p><details close><br/><summary>论文（arxiv）</summary><br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">post1</a><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/">post2</a><br/><div class="row">    <embed src="/postpdfs/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/2312.01790.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h2 id="摘要">摘要</h2><p>最近的图像操作定位和检测技术通常利用由噪声敏感滤波器产生的法医伪影和痕迹，如SRM和Bayar卷积。</p><p>在本文中，我们展示了在这种方法中常用的不同过滤器擅长于揭示不同类型的操作，并提供互补的法医痕迹。因此，我们探索了合并这些滤波器输出的方法，其目的是利用所产生的伪影的互补性来执行图像操作定位和检测（IMLD）。</p><p>我们提出了两种不同的方法：一种是从每个法医过滤器产生独立的特征，然后将它们融合（称为晚期融合），另一种是执行不同模态输出的早期混合并产生早期组合特征（这称为早期融合）。</p><p>我们证明了这两种方法在图像操作定位和检测方面都取得了具有竞争力的性能，在多个数据集上优于最先进的模型1。</p><h2 id="方法">方法</h2><h3 id="编码器解码器框架">编码器解码器框架</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/architecture.png"alt="Architecture" /><figcaption aria-hidden="true">Architecture</figcaption></figure><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p><h3 id="特征融合方法大模型">特征融合方法（大模型）：</h3><p>首先分别从NoisePrint++、SRM和bayar卷积中提取RGB图像x的辅助特征。然后将每个辅助特征与原始RGB一起输入到一个双分支CMX编码器中，生成4尺度的特征图如图所示：</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213343139.png"alt="image-20240328213343139" /><figcaption aria-hidden="true">image-20240328213343139</figcaption></figure><p>在每个尺度上，3个编码器的输出被连接起来，以产生编码器的最终输出f。我们使用与TruFor中相同的解码器架构来处理异常和置信解码器。</p><h3id="提出的另外一种特征融合方法小模型">提出的另外一种特征融合方法（小模型）：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213412073.png"alt="image-20240328213412073" /><figcaption aria-hidden="true">image-20240328213412073</figcaption></figure><p>再次提取了RGB图像x的辅助特征、rbayar。然后每个输入通过卷积块C，生成早期特征fmod。然后将这3组特征映射连接起来，生成完整的早期特征集fef。这些特征然后通过另一个卷积块C，产生混合特征f mf = C（fef）。混合特征fmf和RGB图像x被用作双分支CMX编码器[34]的输入，其方式与TruFor中的相同。</p><p>这是一种特别轻量级的方法来扩展TruFor架构以处理多个辅助模式，因为它不会显著增加参数的数量（与TruFor的68.7M相比，是68.9M参数）。</p><h2 id="实验">实验：</h2><h3 id="与其他方法在f1参数上的比较">与其他方法在F1参数上的比较：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213436800.png"alt="image-20240328213436800" /><figcaption aria-hidden="true">image-20240328213436800</figcaption></figure><p>其提出的两种特征融合方式除了在DSO-1上不如TruFor，剩下的都好于TruFor<br/>特别是对于只包含复制移动伪造的覆盖数据集，我们的最佳方法比之前的最佳方法TruFor高了6.3%。</p><p>DSO-1数据集主要用于检测包含人的拼接图像。比TruFor落后3%。</p><h3 id="专门与trufor比较结果">专门与TruFor比较结果：</h3><p><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213450185.png"alt="image-20240328213450185" />认为只是误差。</p><h3 id="与其他方法在auc参数上的比较">与其他方法在AUC参数上的比较：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213506766.png"alt="image-20240328213506766" /><figcaption aria-hidden="true">image-20240328213506766</figcaption></figure><p>可以发现</p><p>小模型显示出了卓越的性能，超过了最先进的平均水平。与之前的领先方法相比，AUC实现了近7%的显著改进，bAcc面实现了9%的显著改进。</p><p>大模型也表现出具有竞争力的AUC性能，但在bAcc方面略落后于TruFor模型。bAcc性能的这种差异可能归因于大模型的大小，这可能容易发生过拟合。进一步的研究和实验需要探索需要额外的正则化技术的可能性，以优化其性能的检测任务。</p><h2 id="消融实验">消融实验：</h2><p>多模态特征输入的消融：</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213528368.png"alt="image-20240328213528368" /><figcaption aria-hidden="true">image-20240328213528368</figcaption></figure><p>在本节中，为了对比各种过滤器（SRM，Bayarconv，NoisePrint++），采用了一个双分支CMX架构，其中每个过滤器作为RGB图像的辅助输入。</p><p>结果见表6。在这个训练过程中，bayar卷积层是可训练的，而SRM和NoisePrint则保持冻结。</p><p>我们可以看到，NoisePrint++基于编辑历史的训练有助于实现DSO-1的最佳性能，其中使用后处理操作覆盖操作，而SRM和bayar在编码和覆盖方面表现更好。</p><p>覆盖范围只包含复制移动操作，噪音打印的相机模型识别可能无法提供足够强大的法医痕迹，而编码道的操作是基于扩散的内画，可能导致不同于传统编辑历史的独特文物。因此，噪音打印在有效处理此类案件时遇到了困难。</p><h2 id="鲁棒性分析"><strong>鲁棒性分析：</strong></h2><p>观察在后处理程度逐渐加深下，模型性能的变化</p><p>在本节中，我们包括对不同质量下降的图像进行的实验，以证明我们的方法的鲁棒性。我们使用Casiav1+数据集，使用不同的内核大小进行高斯模糊，使用不同的质量因子进行JPEG压缩，并与TruFor进行比较。</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213554181.png"alt="image-20240328213554181" /><figcaption aria-hidden="true">image-20240328213554181</figcaption></figure><p>图4中描述的结果表明，我们的两种融合方法在广泛的降解过程中都表现出良好的鲁棒性，在所使用的所有降解水平上保持了比TruFor的一致优势。</p>]]></content>
      
      
      <categories>
          
          <category> IML </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位合集</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/</url>
      
        <content type="html"><![CDATA[<p><a href="/论文总集/">已读论文汇总</a></p><blockquote><p>橘色为A类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/A类-年份-red"alt="A类" /></a><br/>&gt; 黄色为B类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/B类-年份-yellow"alt="B类" /></a><br/>&gt; 绿色为C类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/C类-年份-green" alt="C类" /></a></p></blockquote><h3 id="image-tampering">Image Tampering</h3><h4 id="image-editing">Image Editing</h4><details open><br/><summary>2024</summary><ul class="task-list"><li><label><input type="checkbox" checked="" /><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection and Localization</a><ahref="https://link.springer.com/chapter/10.1007/978-3-031-53311-2_15"><imgsrc="https://img.shields.io/badge/MMM-2024-green" alt="MMM" /></a> <ahref="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image Manipulation Detection</a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-red" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/UnionFormer/">UnionFormer: Unified-Learning Transformer withMulti-View Representation for Image Manipulation Detection andLocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-red"alt="CVPR" /></a></label></li><li><label><input type="checkbox" /><a href="/DH-GAN/">DH-GAN: Imagemanipulation localization via a dual homology-aware generativeadversarial network</a> <ahref="https://doi.org/10.1016/j.patcog.2024.110658"><imgsrc="https://img.shields.io/badge/PR-2024-yellow"alt="PR" /></a></label></li></ul></details><details open><br/><summary>2023</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2023-red" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2212.10957"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/grip-unina/TruFor"><imgsrc="https://img.shields.io/github/stars/grip-unina/TruFor?style=flat"alt="GitHub" /></a> <a href="https://grip-unina.github.io/TruFor/"><imgsrc="https://img.shields.io/badge/project-blue"alt="project" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/WACV2023/html/Niloy_CFL-Net_Image_Forgery_Localization_Using_Contrastive_Learning_WACV_2023_paper.html"><imgsrc="https://img.shields.io/badge/WACV-2023-yellow" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a> <ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></label></li><li><label><input type="checkbox" /><ahref="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/">Pre-training-freeImage Manipulation Localization through Non-Mutually ExclusiveContrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/Knightzjz/NCL-IML"><imgsrc="https://img.shields.io/github/stars/Knightzjz/NCL-IML?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/">Uncertainty-UncertaintyLearning for Improving Image Manipulation Detection</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Uncertainty-guided_Learning_for_Improving_Image_Manipulation_Detection_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red"alt="ICCV" /></a></label></li></ul></details><details close><br/><summary>2022</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/PSCC-Net-Progressive-Spatio-Channel-Correlation-Network-for-Image-Manipulation-Detection-and-Localization/">PSCC-Net:Progressive Spatio-Channel Correlation Network for Image ManipulationDetection and Localization</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903/"><imgsrc="https://img.shields.io/badge/TCSVT-2022-yellow" alt="TCSVT" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2103.10596"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/proteus1991/PSCC-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" /><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903"><imgsrc="https://img.shields.io/badge/TPAMI-2022-red" alt="TPAMI" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2112.08935"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/dong03/MVSS-Net"><strong>Code</strong></a><strong>]</strong></label></li></ul></details><h3 id="tamper-text-in-detection">Tamper Text in Detection</h3><p>图像中的<strong>文本篡改检测</strong>问题 (parts of)</p><ul class="task-list"><li><label><input type="checkbox" />Image-based Freeform HandwritingAuthentication with Energy-oriented Self-Supervised Learning <ahref="https://arxiv.org/abs/2408.09676"><imgsrc="https://img.shields.io/badge/TMM_&#39;24-dc3545"alt="paper" /></a></label></li><li><label><input type="checkbox" />Generalized Tampered Scene TextDetection in the era of Generative AI <ahref="https://arxiv.org/abs/2407.21422"><imgsrc="https://img.shields.io/badge/arXiv_&#39;24-6c757d"alt="paper" /></a></label></li><li><label><input type="checkbox" />A Two-Stage Dual-Path Framework forText Tampering Detection and Recognition <em>(arXiv '24)</em><strong>[<ahref="https://arxiv.org/abs/2402.13545">Paper</a>]</strong></label></li><li><label><input type="checkbox" />CTP-Net: Character TexturePerception Network for Document Image Forgery Localization (<em>arXiv'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2308.02158v1"><strong>Paper</strong></a><strong>]</strong><a href="https://github.com/FCTMdataset/FCTM"><imgsrc="https://img.shields.io/github/stars/FCTMdataset/FCTM?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" />Toward Real Text ManipulationDetection: New Dataset and New Solution <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2312.06934">Paper</a>]</strong> <strong>[<ahref="https://github.com/DrLuo/RTM">Code</a>]</strong></label></li><li><label><input type="checkbox" />Towards Robust Tampered TextDetection in Document Image: New dataset and New Solution (<em>CVPR'23</em>) <strong>[</strong><ahref="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf"><strong>Paper</strong></a><strong>]</strong><strong>[<ahref="https://github.com/qcf-568/DocTamper">Code</a>]</strong></label></li><li><label><input type="checkbox" />Progressive Supervision forTampering Localization in Document Images (<em>ICONIP '23</em>)<strong>[<ahref="https://link.springer.com/chapter/10.1007/978-981-99-8184-7_11">Paper</a>]</strong></label></li><li><label><input type="checkbox" />SigScatNet: A Siamese + Scatteringbased Deep Learning Approach for Signature Forgery Detection andSimilarity Assessment <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/pdf/2311.05579.pdf">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Image Generation and LearningStrategy for Deep Document Forgery Detection <em>(arXiv '23)</em><strong>[<ahref="https://arxiv.org/abs/2311.03650">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Forgery-free signature verificationwith stroke-aware cycle-consistent generative adversarial network<em>(Neurocomputing '22)</em> <strong>[<ahref="https://doi.org/10.1016/j.neucom.2022.08.017">Paper</a>]</strong><strong>[<ahref="https://github.com/KAKAFEI123/Stroke-cCycleGAN">Code</a>]</strong></label></li><li><label><input type="checkbox" />Document Forgery Detection in theContext of Double JPEG Compression <em>(ICPR '22)</em> <strong>[<ahref="https://link.springer.com/chapter/10.1007/978-3-031-37745-7_5">Paper</a>]</strong></label></li></ul><p>datasets 下载:</p><ul><li><p><ahref="https://github.com/namtpham/casia2groundtruth">Casiav2</a></p></li><li><p><ahref="https://github.com/mjkwon2021/CAT-Net#1-downloading-tampcoco--compraise">tampCOCO</a></p></li><li><p><ahref="http://staff.utia.cas.cz/novozada/db/">IMD2020</a></p></li><li><p><ahref="http://zefirus.org/articles/9f78c1e9-8652-4392-9199-df1b6a6c1a3d/">FantasticReality</a></p></li><li><p><ahref="https://github.com/namtpham/casia1groundtruth">Casiav1</a>(创建Casiav1+数据集需要corel数据集)</p></li><li><p><ahref="https://www.kaggle.com/datasets/elkamel/corel-images">corel</a></p></li><li><p><ahref="https://github.com/grip-unina/TruFor#cocoglide-dataset">CocoGlide</a></p></li><li><p><ahref="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/">Columbia</a></p></li><li><p><a href="https://github.com/wenbihan/coverage">COVER</a></p></li><li><p><ahref="https://recodbr.wordpress.com/code-n-data/#dso1_dsi1">DSO-1</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（二）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一stderr-log打印">一、stderr log打印</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sys.stderr = <span class="built_in">open</span>(<span class="string">&quot;errors.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eprint</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(*args, file=sys.stderr, **kwargs)</span><br></pre></td></tr></table></figure><p>这样代码中使用eprint()方法输出的信息会保存在根目录errors.txt文件中</p><h5 id="二机器人状态控制">二、机器人状态控制</h5><p>机器人的状态由变量self.gooding控制，其中参数与其对应的状态如下：</p><table><colgroup><col style="width: 12%" /><col style="width: 60%" /><col style="width: 17%" /><col style="width: 10%" /></colgroup><thead><tr class="header"><th>self.gooding</th><th>状态</th><th>判断条件</th><th>下一个状态</th></tr></thead><tbody><tr class="odd"><td>0</td><td>机器人处于空闲状态，可以通过self.getTarget()方法，得到要去往的goods目标</td><td>无</td><td>1</td></tr><tr class="even"><td>1</td><td>机器人已经找到目标goods，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>1.5</td></tr><tr class="odd"><td>1.5</td><td>机器人已经得到goods，判断是否get成功</td><td>self.goods == 0</td><td>0</td></tr><tr class="even"><td></td><td></td><td>self.goods == 1</td><td>2</td></tr><tr class="odd"><td>2</td><td>机器人处于取得goods状态，可以通过self.getBerthTarget()方法，得到要去往的berth目标</td><td>无</td><td>3</td></tr><tr class="even"><td>3</td><td>机器人已经找到目标berth，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>0</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（一）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="赛题背景">【赛题背景】</h5><ul><li><p>华为基于自身ICT基础设施能力，通过对港口场景的洞察和理解，以智慧、绿色、高效、安全为目标，数字化、智能化为手段，联合生态伙伴，助力世界一流港口建设。</p></li><li><p>本次赛题抽象自华为云智能港口真实业务难题，选手通过算法完成运输船只智能泊靠、运输机器人智能拣货装货等任务，以最大化提升港口物流效率。</p></li></ul><h5 id="数据集说明"><strong>【数据集说明】</strong></h5><ul><li>初赛练习赛每天使用1张地图进行判题。</li><li>初赛正式赛有3张地图，取3张图的总分作为当次判题成绩。</li><li>初赛练习赛共8张地图，地图数据完全公开给大家下载。练习阶段前8日，每日公开当日判题地图，方便大家本地调试。</li></ul><p><strong>【官方论坛】</strong></p><ul><li><h5id="华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云-huaweicloud.com"><strong><ahref="https://bbs.huaweicloud.com/forum/forum-0168144383617537003-1.html">华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云(huaweicloud.com)</a></strong></h5></li><li><strong><ahref="https://bbs.huaweicloud.com/forum/thread-0239145895671582004-1-1.html">初赛练习阶段赛题相关材料及配套软件（Windows版本）整合版_2024华为软件精英挑战赛_华为软件精英挑战赛_华为云论坛(huaweicloud.com)</a></strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
