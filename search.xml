<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A Survey on Deep Clustering:From the Prior Perspective</title>
      <link href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/"/>
      <url>/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/</url>
      
        <content type="html"><![CDATA[<p>A Survey on Deep Clustering: From the Prior Perspective</p><p>四川大学计算机科学学院，成都，中国四川</p><h1 id="摘要">摘要</h1><p>​  由于神经网络具有强大的特征提取能力，深度聚类在分析高维和复杂的真实世界数据方面取得了巨大的成功。深度聚类方法的性能受到网络结构和学习目标等各种因素的影响。然而，正如本调查中所指出的，深度聚类的本质是对先验知识的整合和利用，这在很大程度上被现有的工作忽略了。从开创性基于数据结构假设的深度聚类方法到最近基于数据增强不变性的对比聚类方法，深度聚类的发展本质上对应于先验知识的演化。在本调查中，我们通过将深度聚类方法分为六种先验知识类型，提供了一个全面的回顾。我们发现，总的来说，先前的创新遵循两个趋势，即，i)从采矿到建设，以及ii)从内部到外部。此外，我们在五个广泛使用的数据集上提供了一个基准，并分析了具有不同先验的方法的性能。通过提供一个新的先验知识视角，我们希望这次调查能够提供一些新的见解，并启发未来在深度聚类社区的研究。</p><h1 id="介绍">1 介绍</h1><h1 id="问题定义">2 问题定义</h1><p>​  在本节中，我们将介绍深度聚类的管道，包括符号和问题定义。除非特别通知，否则在本文中，我们使用粗体大写和小写分别表示矩阵和向量。表1总结了常用的符号。</p><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107222628947.png"alt="image-20250107222628947" /><figcaption aria-hidden="true">image-20250107222628947</figcaption></figure><p>​  深度聚类问题的正式定义如下：给定一组属于C类的实例<spanclass="math inline">\(\mathcal{D}=\{\bfx_{i}\}_{i=1}^{N}\in\mathcal{X}\)</span>，深度聚类的目的是学习鉴别特征，并根据实例的语义将其分为C聚类。具体来说，深度聚类方法首先学习一个深度神经网络<spanclass="math inline">\(f:{\mathcal{X}}\rightarrow{\mathcal{Z}}\)</span>用于特征提取<spanclass="math inline">\(\mathbf{z}_{i}=f(\mathbf{x}_{i})\)</span>。给定潜在空间的实例特征，可以得到聚类结果。最直接的方法是应用经典算法，如K-means（MacQueen等，1967）和DBSCAN（Ester等，1996a）。另一种解决方案是训练一个额外的集群头<spanclass="math inline">\(h\:\mathcal{Z}\to\mathbb{R}^{C}\)</span>生成满足<spanclass="math inline">\(\textstyle{\sum_{i=0}^{K}\mathbf{p}_{ij}}\;=\;1\)</span>的生成软集群分配<spanclass="math inline">\(\mathbf{p}_i=\operatorname{sotrmax}(h(\mathbf{z}_{i}))\)</span>。第<spanclass="math inline">\(i\)</span>个实例的硬簇分配可以通过argmax操作来计算，即： <spanclass="math display">\[\tilde{y}_{i}=\arg\operatorname*{max}_{j}\ {\bfp}_{i j},1\leq j\leq C\]</span>​  聚类分配提供了数据底层的固有语义结构，可以用于各种下游分析。</p><h1 id="深度聚类的先验条件">3 深度聚类的先验条件</h1><p>​  在本节中，我们将从先验知识的角度来回顾现有的深度聚类方法。先验情况如图1所示，方法分类总结在表2中。</p><hr /><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107223309527.png"alt="image-20250107223309527" /><figcaption aria-hidden="true">image-20250107223309527</figcaption></figure><p>图1.深度聚类的6类先验知识。(a)结构先验：数据结构可以反映实例之间的语义关系。(b)分布先验：来自不同集群的实例遵循不同的数据分布。(c)增强不变性：由相同实例增强的样本具有相似的特征。(d)邻域一致性：相邻的样本具有一致的聚类分配。(e)伪标签：具有高可信度的聚类分配很可能是正确的。(f)外部知识：在开放世界的数据和模型中存在大量有利于聚类的知识。</p><hr /><p>表2 从先验知识的角度总结了深度聚类方法。</p><table><colgroup><col style="width: 14%" /><col style="width: 50%" /><col style="width: 20%" /><col style="width: 14%" /></colgroup><thead><tr class="header"><th>先验知识</th><th></th><th>方法</th><th>主要贡献</th></tr></thead><tbody><tr class="odd"><td>结构先验</td><td>固有的数据结构反映了语义关系</td><td>ABDC (2013)</td><td>以EM的方式优化特征和聚类分配</td></tr><tr class="even"><td></td><td></td><td>DEN (2014)，SpectralNet (2018)</td><td>将光谱聚类从浅层扩展到深层</td></tr><tr class="odd"><td></td><td></td><td>PARTY (2016)</td><td>引入了从子空间学习到深度聚类的稀疏性先验</td></tr><tr class="even"><td></td><td></td><td>JULE (2016)</td><td>将聚集层从浅层扩展到深层</td></tr><tr class="odd"><td></td><td></td><td>DCC (2018)</td><td>提出了关系匹配来实现非参数深度聚类</td></tr><tr class="even"><td>分布先验</td><td>不同语义的实例遵循不同的数据分布</td><td>VaDE (2016)</td><td>利用高斯混合模型学习不同的簇分布</td></tr><tr class="odd"><td></td><td></td><td>ClusterGAN (2019)，DCGAN (2015)</td><td>使用GAN隐式地学习集群分布</td></tr><tr class="even"><td>增强不变性</td><td>实例特征对数据增强不变</td><td>IMSAT (2017)</td><td>提出了成对增广样本之间的不变性</td></tr><tr class="odd"><td></td><td></td><td>IIC (2019)，Completer (2021)</td><td>提出了关于增强不变性的互信息框架</td></tr><tr class="even"><td></td><td>集群分配对数据增强是不变的</td><td>PICA (2020)</td><td>探索增强样本的聚类分配之间的不变性</td></tr><tr class="odd"><td></td><td></td><td>CC (2021)，DRC (2020)</td><td>同时探索在实例级别和集群级别上的增强不变性</td></tr><tr class="even"><td></td><td></td><td>TCC (2021)</td><td>利用集群语义和实例组合的统一表示</td></tr><tr class="odd"><td>邻域一致性</td><td>相邻的实例具有相似的语义</td><td>SCAN (2020)</td><td>在相邻实例之间强加一致的集群分配</td></tr><tr class="even"><td></td><td></td><td>NNM (2021)</td><td>在邻居之间执行集群级的对比学习</td></tr><tr class="odd"><td></td><td></td><td>GCC (2021)</td><td>在邻居之间执行实例级和集群级的对比学习</td></tr><tr class="even"><td>伪标签</td><td>具有高置信度的聚类分配是可靠的</td><td>DEC (2016)</td><td>通过锐化构造目标集群分布</td></tr><tr class="odd"><td></td><td></td><td>DeepCluster (2018)</td><td>使用K-means生成伪标签</td></tr><tr class="even"><td></td><td></td><td>SCAN (2020)</td><td>选择高置信度的预测，用强增强样本来微调模型</td></tr><tr class="odd"><td></td><td></td><td>SPICE (2022)</td><td>利用原型选择伪标签，采用半监督学习的方法对模型进行微调</td></tr><tr class="even"><td></td><td></td><td>TCL (2022)</td><td>在对比学习中使用伪标签来减少假负对</td></tr><tr class="odd"><td></td><td></td><td>ProPos (2022)</td><td>使用K-means中的伪标签来增加集群的紧凑性</td></tr><tr class="even"><td>外部知识</td><td>在开放的世界中存在着丰富的集群有利知识</td><td>SIC (2023)</td><td>从预先训练过的视觉语言模型的文本空间中生成图像伪标签</td></tr><tr class="odd"><td></td><td></td><td>TAC (2023b)</td><td>构建更有区别的文本，并进行跨模态蒸馏来改进聚类</td></tr></tbody></table><hr /><h2 id="结构先验">3.1 结构先验</h2><h2 id="分配先验">3.2 分配先验</h2><h2 id="增强不变性">3.3 增强不变性</h2><h2 id="邻域一致性">3.4 邻域一致性</h2><h2 id="伪标签">3.5 伪标签</h2><h2 id="外部知识">3.6 外部知识</h2><h1 id="实验">4 实验</h1><p>​  在本节中，我们将介绍对深度聚类的评估。简单地说，我们首先介绍评估指标和通用基准。然后对现有的深度聚类方法的结果进行了分析。</p><h2 id="评价指标">4.1 评价指标</h2><p>​  对于聚类评估，通常使用三个度量标准来度量预测的聚类分配<spanclass="math inline">\(\tilde{y}\)</span>如何与 ground-truth标签<spanclass="math inline">\(y\)</span>相匹配，包括准确性（ACC）、标准化互信息（NMI）和调整兰德系数（ARI）。指标值越高，对应的聚类性能越好。这三个指标的定义如下：</p><ul><li>ACC（Amig´o et al, 2009）表示聚类预测的正确率：</li></ul><p><spanclass="math display">\[\mathrm{ACC}={\frac{1}{N}}\sum_{i=1}^{N}\mathrm{1}\{y_{i}=\tilde{y}_{i}\},\]</span></p><ul><li>NMI（McDaid et al，2011）量化了预测标签<spanclass="math inline">\(\tilde{Y}\)</span>和 ground-truth标签<spanclass="math inline">\(Y\)</span>之间的互信息：</li></ul><p><span class="math display">\[\mathrm{NMI}={\frac{I(\bar{\bf Y};{\bfY})}{\frac{1}{2}[H(\bar{\bf Y})+H({\bf Y})]}},\]</span></p><p>​  其中H (Y)表示Y的熵，<span class="math inline">\(I(\bar{\bf Y};{\bfY})\)</span>表示<span class="math inline">\(\tilde{Y}\)</span>和<spanclass="math inline">\(Y\)</span>之间的互信息。</p><ul><li>ARI（Hubert and Arabie，1985）是兰德系数(RI，randindex)的归一化，它计算同一集群和不同集群中的实例对的数量：</li></ul><p><spanclass="math display">\[\mathrm{RI}={\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{C}_{N}^{2}}},\]</span></p><p>​  其中，TP和TN为真正对和真负对的个数，<spanclass="math inline">\(C^2_N\)</span>为可能的实例对的个数。ARI是通过添加以下规范化来计算出来的：<spanclass="math display">\[\mathrm{ARI}={\frac{\mathrm{RI}-\mathbb{E}(\mathrm{RI})}{\operatorname*{max}(\mathrm{RI})-\mathbb{R}(\mathrm{RI})}},\]</span></p><h2 id="数据集">4.2 数据集</h2><p>​  在早期阶段，深度聚类方法在相对较小的低维数据集上进行评估（例如COIL-20（Nene等，1996），YaleB（Georghiades等，2001））。近年来，随着深度聚类方法的快速发展，在更复杂和更具有挑战性的数据集上评估聚类性能变得越来越流行。有五个被广泛使用的基准数据集：</p><ul><li>CIFAR-10 (Krizhevsky et al, 2009)由来自10个不同类别的6万张彩色图像组成，包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。</li><li>CIFAR-100 (Krizhevsky et al, 2009)包含100个类，分为20个超类。每个图像都带有一个“细”类标签和一个“粗”超类标签。</li><li>STL-10 (Coates et al, 2011)包含来自10个对象类的13000张标记图像。此外，它还提供了10万张未标记图像用于自监督学习，以提高聚类性能。</li><li>ImageNet-10 (Chang et al, 2017)是ImageNet数据集的一个子集（Deng等人，2009）。它包含10个类，每个类都有1300张高分辨率图像。</li><li>ImageNet-Dog (Chang et al, 2017)是ImageNet的另一个子集。它由属于15个犬种的图像组成，适用于细粒度的聚类任务。</li></ul><p>​  除此之外，最近的一些工作采用了两个更具挑战性的大规模数据集，Tiny-ImageNet(Le and Yang, 2015)和ImageNet-1K (Deng et al,2009)，来评估其有效性和效率。表3总结了对这些数据集的简要描述。</p><h2 id="性能比较">4.3 性能比较</h2><p>​  在5个广泛使用的数据集上的聚类性能如表4所示。</p><figure><imgsrc="../postimages/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/image-20250107225754511.png"alt="image-20250107225754511" /><figcaption aria-hidden="true">image-20250107225754511</figcaption></figure><p>​  由于深度神经网络的特征提取能力，早期基于结构和分布先验的深度聚类方法获得了比经典的K-means方法更好的性能。然后，一系列的对比聚类方法通过数据增强引入额外的先验，显著提高了性能。在此之后，更先进的方法通过进一步考虑邻域一致性（GCC与CC相比），并使用伪标签（SCAN与SCAN∗相比）来提高性能。值得注意的是，不同先验的性能收益是独立的。例如，ProPos通过分别利用增强不变性和伪标记先验，显著优于DEC和CC。最近，基于外部知识的方法取得了最先进的性能，证明了这是一种新的深度聚类范式的广阔前景。此外，当类别数量不断增长（从CIFAR-10到CIFAR-100）或语义变得更加复杂（从CIFAR-10到ImageNet-Dogs）时，聚类就变得更具挑战性。这些结果表明，更具挑战性的数据集，如完整的ImageNet-1K，有望在未来的工作中进行基准测试。</p><h1 id="在vicinagearth中的应用">5 在Vicinagearth中的应用</h1><p>​  在本节中，我们将探讨在Vicinagearth领域内深度聚类的一些典型应用，这是一个由“Vicinage”和“Earth”融合而成的术语。Vicinagearth代表了从海平面以下1000米（阳光停止穿透的深度）到海拔10000米（商用飞机的典型巡航高度）的关键空间高度。这个区域非常重要，因为它包括人类活动的核心区域，包括居住和生产地区。近年来，深度聚类已成为邻近地球内不可或缺的分析工具，有助于揭示邻近空间内数据的复杂模式和结构。深度聚类在该区域的多种应用包括异常检测、环境监测、社区检测、人员再识别等。<br/>​  <strong>异常检测，</strong>也被称为异常值检测（Comaniciu和Meer，2002）或新奇检测（Esteretal.，1996b），试图识别异常实例或模式。在Vicinagearth的背景下，深度聚类被证明是为了分析从不同来源获得的传感器数据，如水下监测系统、空中传感器或地面传感器（Chatterjee和Ahmed，2022）。通过对传感器数据的模式和典型行为的分析，系统能够检测异常，这些异常可能是安全威胁或不规则活动的信号。<br/>​  <strong>环境监测</strong>包括分析从环境传感器收集的数据（Xia和Vlajic，2007），如监测空气质量、水条件和地质因素。其主要目标是确保生态系统的健康（Wuetal，2016），并发现潜在的环境威胁，如污染事件或自然灾害。深度聚类技术在对相似的环境模式进行分组方面起着至关重要的作用，有助于对异常情况的识别。这一应用程序有助于实时环境监测（Kumaretal，2012），提高了及时应对环境挑战的能力。<br/>​  <strong>社区检测</strong>（Fortunato，2010；Jin等人，2021年）涉及到评估节点组是如何聚集或分割的，以及它们在网络中加强或分裂的趋势。在Vicinagearth的背景下，这项技术被用于识别密切相互作用或共享相似生态位的物种群（默多克和耶格尔，2011）。深度聚类在复杂生态网络的分析中起着关键作用（Montoyaetal，2006），有助于更深入地了解生态群落及其动态。<br/>​  <strong>人的再识别</strong>（Wu等人，2019；Ye等人，2021）是一项关键的任务，涉及到识别和匹配不同摄像机视图中的个体（Yangetal，2022a）。这项技术在公共安全和执法行动中发挥着重要作用，因为它有助于监测人口密集的地区，以将潜在的威胁或主题列入观察名单。深度聚类算法的集成显著提高了人的再识别系统的可伸缩性和效率（Yanetal，2023）。深度聚类有效地管理大量和动态变化的人群带来的复杂性。此外，深度聚类技术的适应性扩大了其应用范围，包括监测自然栖息地和跟踪在不同的和不受控制的环境中的野生动物。</p><h1 id="未来的挑战">6 未来的挑战</h1><p>​  虽然现有的工作取得了显著的性能，但一些实际的挑战和新出现的要求尚未得到充分解决。在本节中，我们将深入探讨现代深度集群的一些未来方向。</p><h2 id="细粒度聚类">6.1 细粒度聚类</h2><p>​  细粒度聚类的目的是识别数据中细微和复杂的变化，这在像识别生物亚种这样的研究中特别有利（Lietal，2023c，d）。主要的挑战是，细粒度的类表现出高度的相似性，其区别通常在于颜色、标记、形状或其他微妙的特征。在这种情况下，传统的粗粒度集群先验经常被证明是不够的。例如，在增强不变性之前的颜色和形状增强会变得无效。最近，C3-GAN（Kim和Ha，2021）在对抗性训练中使用对比学习来生成逼真的图像，能够细致入微地捕获细粒度的细节，并确保集群之间的可分离性。</p><h2 id="非参数聚类">6.2 非参数聚类</h2><p>​  许多集群方法通常需要预定义的和固定数量的集群。然而，真实世界的数据集经常对未知的集群数量的挑战，反映了更接近现实的情况。只有少数作品(Chen,2015; Shah and Koltun, 2018; Zhao et al, 2019;Wang et al, 2021)一直致力于解决这个问题。这些方法通常依赖于计算全局相似度，并引入巨大的计算成本，特别是在大规模数据集中。因此，有效地确定簇数C的最优值仍然是一个开放的挑战，通常涉及到人类先验的合并。在现有的工作中，DeepDPM引入了狄利克雷过程高斯混合模型（DPGMM，DirichletProcess Gaussian MixtureModels）（Antoniak，1974），利用狄利克雷过程作为混合组件的先验分布。DeepDPM通过MetropolisHastings框架(Hastings,1970)指导下的拆分和合并操作动态调整集群C的数量。</p><h2 id="公平聚类">6.3 公平聚类</h2><p>​  用不同的获取方法从不同的来源收集真实世界的数据集，可以增强机器学习模型的泛化性。然而，这些数据集经常表现出固有的偏见，特别是在敏感的属性上，如性别、种族和民族。这些偏差将导致个人和少数群体之间的差异，导致聚类划分偏离数据的潜在目标特征。在公正和公平的分析至关重要的应用程序中，如就业、医疗保健和教育，追求公平尤其相关。为了解决这一挑战，公平聚类试图减轻这些偏见的影响，因为每个样本的偏见属性。<br/>​  为了解决这一艰巨的任务，Chierichettietal首先引入了一种称为球流分解的数据预处理方法。最近的研究进展通过对抗性训练（Li等人，2020年）和互信息最大化（Zeng等人，2023年）在大规模数据上解决了这一问题。值得注意的是，Zeng等人设计了一种新的度量方法，从信息论的角度来评估聚类质量和公平性。尽管有这些发展，但仍有改进的空间，建立更好的评价指标是本研究的一个持续领域。</p><h2 id="多视图聚类">6.4 多视图聚类</h2><p>​  多视图数据（Xu etal，2013；Liu等人，2019b）在现实世界中很常见，即信息从各种传感器捕获或从多个角度观察。这些数据本身就很丰富，提供了多样化而又一致的信息。例如，RGB视图将提供颜色细节，而深度视图将显示空间信息，这表示视图的互补方面。同时，存在一种视图一致性级别，因为同一对象在不同的视图之间具有共同的属性。为了处理多视图数据，提出了多视图聚类（Denget al，2015；Liu etal，2019a）来利用互补和一致的特征。其目标是整合来自所有视图的信息，以产生一个统一的和深刻的聚类结果。<br/>​  近年来，几种深度学习方法（Andrew等，2013；Wang等，2016；赵等，2016；Peng等人，2019）旨在应对这一挑战。二进制多视图聚类Zhang等人（2018）同时细化了二进制聚类结构和离散数据表示，确保了内聚聚类。为了追求视图的一致性，Lin等人（2021,2022）最大化了跨视图的互信息，从而对齐了共同的属性。SURE（Yangetal，2022b）旨在通过利用鲁棒对比损失来加强视图之间共享特征的一致性。最近，Li等人（2023a）执行了约束对比损失，以在集群水平上保持视图互补。这些创新的方法表明了在多视图分析领域取得的重大进展，在那里，聚类在加强对多视图数据的协同利用方面继续发挥关键作用。</p><h1 id="结论">7 结论</h1><p>​  深度聚类或无监督学习的关键是寻求有效的监督来指导表示学习。与传统的网络结构或数据类型的分类法不同，本调查从先验知识的角度提供了一个全面的回顾。随着聚类技术的发展，出现了一种明显的趋势，从探索数据本身内部的先验转向诸如自然语言指导等外部知识。探索ChatGPT或GPT-4V（ision）等外部预训练模型可能成为一个很有前途的途径。这项调查可能提供了一些有价值的见解，并激发了对深度聚类的进一步探索和进步。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pixel and region level information fusion in membership regularized fuzzy clustering for image segmentation</title>
      <link href="/Fuzzy-clustering-method-for-image-segmentation/"/>
      <url>/Fuzzy-clustering-method-for-image-segmentation/</url>
      
        <content type="html"><![CDATA[<p>Pixel and region level information fusion in membership regularizedfuzzy clustering for image segmentation</p><p>发表于期刊Information Fusion 2023<br/><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241229221827412.png"alt="image-20241229221827412" /></p><h2 id="摘要">摘要：</h2><p>​  隶属度正则化模糊聚类方法应用了一个重要的先验，即相邻的数据点根据亲和度/相似度矩阵应该具有相似的隶属度。因此，它们在许多数据挖掘任务中都取得了良好的性能。然而，这些聚类方法在正则化过程中并没有充分利用图像空间信息。它们在图像分割问题上的性能仍然不理想。在本文中，我们首先关注于建立一个新的亲和矩阵来存储和呈现图像空间信息，以帮助成员正则化模糊聚类方法获得良好的分割结果。为此，通过融合像素级和区域级信息来计算亲和度值，以表示图像中两点之间的微妙关系。此外，为了减少图像噪声的影响，我们在算法的迭代中使用了固定的聚类中心，因此，隶属度值的更新仅以融合信息的先验为指导。在合成图像数据集和真实图像数据集上的实验结果表明，该方法比现有的聚类方法具有更好的分割效果。</p><h2 id="介绍">1.介绍</h2><p>​  图像分割是图像分析中的关键环节，已广泛应用于目标识别[1]、医学诊断支持系统[2]、工业过程[3]等计算机视觉任务[4,5]。它是一种将图像分割成几个具有独特属性的特定区域的技术，可以看作是一个像素聚类过程。许多关于图像分割的方法已经被提出，如基于超像素的方法[6]、基于神经网络[7,8]的监督方法、聚类[9,10]等。虽然这些技术在一定程度上对图像分割取得了良好的效果，但对于不同的图像都不具有足够的鲁棒性和效率。例如，基于超像素的方法对复杂图像不能很好地保留细节，基于神经网络的监督方法需要大量的训练样本和标记图像，而且分割结果具有粗糙的轮廓。本文主要研究了基于模糊聚类的无监督分割方法。<br/>​  在实践中，聚类由于其有效性，是图像分割中最常用和最重要的方法之一。在图像分割的聚类算法方面也有许多进展。提出了基于K-means聚类算法和一种快进量子优化算法的FFQOAK[11]方法。这是一种硬划分方法，只承认像素属于单个簇/段。此外，Pritpal等人[12]提出了一种基于模糊集理论和相关概念的模糊d-均值融合聚类算法。模糊集的固有模糊模型具有四个固定程度的成员关系，如真、假、真模糊和假模糊。在各种聚类方法中，模糊聚类在处理图像分割[13]的模糊特性中的有效性和鲁棒性得到了广泛的研究和应用。模糊c-均值（FCM）是一种基于模糊集理论的经典聚类算法，最初由Dunn[14]提出，后来由Bezdek[15]进行改进。模糊集理论不同于模糊集理论。每个数据的隶属度始终属于范围[0,1]。此外，模糊聚类是一种软分区方法，它允许数据点在所有聚类中都具有成员资格，在处理图像[16]中的边缘信息时比硬分区方法效果更好。但传统的模糊聚类方法在处理有噪声或其他伪影的图像时往往效果较差，因此如何在复杂图像上获得准确的分割结果仍然是一项具有挑战性的任务[17]。<br/>​  为了克服FCM的缺点，对距离测度的重新设计进行了重视和探索，因此，提出了一些核诱导的FCM变量[18,19]，这种核诱导的距离测度被视为非线性数据挖掘和图像分割的一个强大的形式工具。为了解决普通FCM对模糊指数值变化敏感的问题，宫本等人开发了最大熵FCM[20]，而且，两个[21,22]都引入了相对熵来改进其目标函数。近年来，为了解决不准确的分类信息或图像分割受离群值的影响，一些算法基于模糊聚类和信息融合[23–25]提出，如Musaet al.[26]试图提出一个基于模糊聚类集成聚合器的基础集群和实现成功的性能、速度和鲁棒性。<br/>​  事实上，在模糊聚类过程中涉及到一些利用空间信息的技术，并对处理有噪声的图像分割任务[27]有积极的影响。例如，Guo等人[28]开发了一种基于噪声检测（NDFCM）的自适应图像分割FCM，其中采用两种图像滤波方法来抑制噪声，并使用一个参数，通过测量每个邻域的灰度水平的方差来保持图像细节。为了实现鲁棒的分割结果与低执行时间，Lei等[29]开发了一个快速、鲁棒的模糊c均值聚类算法（FRFCM）基于平方社区空间水平信息，采用形态重建和会员滤波平滑图像详细保存，降低计算成本。Zhang等[30]在考虑空间邻域信息的基础上，考虑了测量值与理论值之间的偏差，提出了偏差-稀疏模糊c-均值（DSFCM_N）来提高图像分割的聚类性能。毕竟，研究人员正专注于使用越来越复杂的基于空间信息的技术来解决复杂的图像分割问题，但他们忽略了图像中不同层次的空间信息的使用，如区域级[31]的空间信息。<br/>​  近年来，在多视图数据集[32]和真实世界数据库[33]上，提出了许多针对不同类型数据挖掘任务的成员正则化模糊聚类方法。RSSFCA[34]涉及高斯度量下的正则化，将其集成到模糊聚类算法的目标函数中，以获得稀疏的隶属度，以减少噪声特征的比例。Chen等人[35]提出了一种基于空间信息构造的图的正则化FCM。[36]等人提出了一种成员变异正则化方法来修改FCM，用于具有噪声和不完整数据的图像分割。这些方法中的正则化是试图充分利用最终反对函数中对数据的先验知识来指导聚类处理[37,38]。然而，这些最近提出的隶属度正则化模糊聚类方法[39,40]大多用于数据挖掘，而不将空间信息作为先验知识，这对图像分割问题很有价值。更广泛地说，成员正则化模糊聚类的亲和性不适用于图像分割任务，存储在亲和矩阵中的先验知识不能充分反映图像像素点之间的相似性。换句话说，这些隶属度正则化模糊聚类方法需要改进，采用一种新的适当的亲和/相似矩阵获取图像分割方法来考虑数据的空间结构，这是图像数据[37,41]的主要特征。<br/>​  一般来说，亲和矩阵是一种基本的统计技术，用于度量一组数据点中的相似性或关系。它存储并呈现了关于数据内在结构的先验知识。该矩阵可以用一些半监督学习方法[42,43]得到，也可以用一些光谱聚类方法[44,45]构造。它在数据挖掘问题上工作得很好。例如，模糊聚类获得的会员可以被视为一个新的表示原始数据，然后数据点彼此应该拥有类似的会员，并基于这个角度，郭etal. [46]探索成员亲和力套索项（MAL）改进模糊聚类方法数据挖掘，命名MALFCMFCM的增强版本。实际上，对于图像分割问题，图像中两个像素点的亲和度值应该同时反映像素级和区域级上的相似性。这意味着在同一集群中分配的附近的两个点同时具有空间一致性和相似的灰度值。因此，在此基础上，本文以MALFCM为例，提出了一种新的隶属度正则化模糊聚类亲和矩阵，以提高其在图像分割上的性能。<br/>​  简而言之，虽然人们为提高图像分割结果付出了大量的努力，但现有的算法主要存在以下限制和挑战。</p><p>（1）传统的模糊聚类方法只采用单一空间信息来提高其对图像分割的性能。基于聚类的图像信息是利用不同层次图像分割模型的关键。<br/>（2）在这些所讨论的模型中，由于隶属度正则化方法的亲和矩阵中存储的先验知识，不能充分反映图像像素点之间的相似性。</p><p>​  在本研究中，为了解决上述挑战，我们提出了一种基于信息融合技术的模糊聚类方法。在该方法中，提出了一种新的亲和矩阵来存储和呈现关于图像的先验知识。此外，为了抑制噪声和其他异常值的影响，我们使用了一个同时考虑像素的空间信息和灰度信息的加权距离。具体来说，在该方法中，像素之间的亲和度受到两层空间约束的影响：像素级近邻域和区域级空间上下文，这是通过简单线性迭代聚类（SLIC）[6]、均值位移[47]等超像素方法获得的。与基于超像素和其他基于模糊的方法相比，一个像素的标记会受到上述两种信息的影响。通过融合这两个层次的信息，提出基于信息融合的方法将缩小同一区域像素之间的差异，减少像素的相似性属于不同地区，即使其中一些被噪声和其他异常值，同时保持图像细节。此外，预计成员关系的更新仅受融合信息的指导，因此我们在合理的迭代处理中使用固定的聚类中心进行初始化。基于此融合信息，该方法利用更多的空间数据先验知识，指导聚类过程，找到更准确和合理的分割结果。</p><p>​  我们的主要贡献可以总结如下：</p><ul><li>新的亲和矩阵：提出了一种亲和矩阵<spanclass="math inline">\(A^{SR}\)</span>，并通过融合像素和区域级信息进行计算，改进了图像分割的隶属度正则化模糊聚类方法。这种更精确的先验知识，存储在亲和矩阵中，反映了图像中两点之间的微妙关系。</li><li>基于信息融合的新模型：在新的亲和矩阵的基础上，利用加权距离融合了像素的空间信息和灰度信息。因此，提出了一种改进的成员正则化模糊聚类方法，并命名为<spanclass="math inline">\(A^{SR}MF\)</span>。</li><li>使用固定聚类中心的策略：仅凭简单的直觉，在亲和矩阵中存储的融合信息的指导下，可以得到合理的分割结果。因此，我们采用了使用固定的聚类中心，利用新的亲和矩阵通过正则化项目更新隶属度值的策略，即先验知识将在分割任务中发挥更大的作用。</li></ul><p>​  本文的其余部分组织如下。在第二节中，我们将描述与我们的研究相关的工作的概述。在第3节中，所提出的方法在这里显示。实验结果报告见第4节。最后，我们在第5节中作出结论。</p><h2 id="前期准备工作">2.前期准备工作</h2><p>​  在本节中，我们将简要回顾一下与我们的研究相关的工作，包括传统的图像分割的模糊聚类、超像素技术和一些隶属度正则化模糊聚类方法。</p><h3 id="图像分割的模糊聚类方法">2.1.图像分割的模糊聚类方法</h3><p>​  对于图像分割，模糊聚类使用模糊隶属关系将数据/像素点分配给每个段/区域。我们以经典的FCM模型为例来说明它。给定一个图像<spanclass="math inline">\(X=\{X_{1},X_{2},\dots,X_{N}\}\)</span>，FCM通过迭代最小化如下定义的函数，将𝑁个像素点划分为𝐶个簇/段：<spanclass="math display">\[J_{\mathrm{FCM}}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}\vert\vert X_{i}-P_{j}\vert\vert^{2}\]</span> ​  限制： <spanclass="math display">\[\sum_{j=1}^{C}u_{i j}=1,i=1,\ldots,N\]</span>​  其中<spanclass="math inline">\(m&gt;1\)</span>是控制会员模糊性的模糊因子，<spanclass="math inline">\(0\,\leq\,u_{ij}\,\leq\,1\)</span>，𝑃𝑗表示𝑗th集群的原型/中心值，𝑢𝑖𝑗是𝑖th数据点对𝑗th集群的会员值，FCM采用拉格朗日乘数法更新会员<spanclass="math inline">\(U(u_{i j})\)</span>和集群中心<spanclass="math inline">\(P(P_{i})\)</span>，如下 <spanclass="math display">\[u_{i j}={\frac{(\lVertX_{i}-P_{j}\rVert)^{-2/(m-1)}}{\sum_{j=1}^{C}(\lVertX_{i}-P_{j}\rVert)^{-2/(m-1)}}}\]</span></p><p><span class="math display">\[P_{j}=\frac{\sum_{i=1}^{N}u_{ij}^{m}X_{i}}{\sum_{i=1}^{N}u_{i j}^{m}}    \]</span></p><p>​  当函数收敛或达到最大迭代次数时，此更新过程就会结束。<br/>​  然而，正如我们从FCM的目标函数中可以看到的，它没有考虑任何关于数据的空间关系的信息，这使得传统的FCM对噪声和其他干扰很敏感。将空间信息纳入目标函数是缓解FCM[48,49]弱点的最常用的尝试之一。Ahmed等人[50]提出了具有空间约束的FCM_S，以便允许一个像素的标记受到其邻域标签的影响。目标函数：<spanclass="math display">\[J_{\mathrm{FCM}_{-}S}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}(\|X_{i}-P_{j}\|^{2}+\frac{\alpha}{N_{R}}\sum_{k\inN_{i}}\|X_{k}-P_{j}\|^{2})\]</span> ​  其中，<spanclass="math inline">\(𝑁_𝑖\)</span>代表落入<spanclass="math inline">\(X_i\)</span>周围窗口的邻居集，<spanclass="math inline">\(N_R\)</span>为其基数，<spanclass="math inline">\(\alpha\)</span>控制空间项的影响。按照同样的思路，在[51]中提出了FCM_S的两个低复杂度变量，FCM_S1和FCM_S2，他们分别采用均值滤波图像和中值滤波图像来获取空间邻域信息。因此，简化的目标函数可以改写为<spanclass="math display">\[J_{\mathrm{FCM}_{S}S1/S2}=\sum_{i=1}^{N}\sum_{j=1}^{C}u_{ij}^{m}(||X_{i}-P_{j}||^{2}+\alpha\|\bar{X}_{i}-P_{j}||^{2})\]</span>​  其中<span class="math inline">\(\bar{X}_{i}\)</span>为<spanclass="math inline">\(X_i\)</span>附近指定窗口内邻居的平均值或中值。遵循这一趋势，研究人员专注于使用越来越复杂的基于空间的技术来解决复杂的图像分割问题，但他们忽略了图像中不同层次的空间信息的使用，如基于超像素技术的区域层次。</p><h3 id="基于超像素的图像分割方法">2.2.基于超像素的图像分割方法</h3><p>​  超像素算法根据像素的位置关系、颜色、纹理等特征将像素划分为有意义的区域。它们在捕获图像冗余方面具有很大的优势，大大降低了后续图像处理任务的复杂性，并广泛应用于图像分割应用。生成超像素的算法有很多，如平均位移算法[47]、多尺度形态梯度重建（MMGR）[52]和简单线性迭代聚类（SLIC）[6]。<br/>​  平均位移是一种基于密度估计的方法，假设不同簇的像素符合不同的概率分布，迭代计算并更新像素的平均位移向量，生成超像素区域。<br/>​  MMGR采用不同的结构元素获得多个重建图像，然后将这些重建的梯度图像融合，获得超像素图像。<br/>​  SLIC采用k-means，利用颜色和空间信息对CIELAB局部颜色空间中的像素进行聚类。在SLIC中的距离测度被表示为<span class="math display">\[d_{l ab}=\sqrt{(l_{j}-l_{i})^{2}+(a_{j}-a_{i})^{2}+(b_{j}-b_{i})^{2}}\]</span></p><p><spanclass="math display">\[d_{s}={\sqrt{(x_{j}-x_{i})^{2}+(y_{j}-y_{i})^{2}}}\]</span></p><p><span class="math display">\[d(i,c)=\sqrt{(d_{l ab})^{2}+(\frac{d_{s}}{S})^{2}m_{s}^{2}}\]</span></p><p>​  其中<spanclass="math inline">\(d_{lab}\)</span>代表两个像素之间的颜色空间差异，<spanclass="math inline">\([l,a,b]^T\)</span>是CIELAB颜色空间中像素颜色的表示，<spanclass="math inline">\(𝑑_𝑠\)</span>是两个像素之间的空间距离，<spanclass="math inline">\([x,y]^T\)</span>指像素的空间位置，<spanclass="math inline">\(d(i,c)\)</span>是一个加权距离，用于测量第<spanclass="math inline">\(i\)</span>个像素和第<spanclass="math inline">\(c\)</span>个聚类中心之间的距离，<spanclass="math inline">\(m_s\)</span>是用于平衡<spanclass="math inline">\(d(i,c)\)</span>中颜色和空间信息的参数，<spanclass="math inline">\(\mathbf{S}\times\mathbf{S}\)</span>为所需超像素的大小，其中<spanclass="math inline">\(S={\sqrt{N/C_{s}}}\)</span>，𝑁为图像像素的数量，<spanclass="math inline">\(𝐶_𝑠\)</span>为超像素的数量。<br/>​  在Liu的论文[27]中，提出了一种基于均值移算法自适应局部信息的模糊聚类方法。目标函数被定义为<span class="math display">\[J_{L iu^{\prime}s}=\sum_{i=1}^{C}\sum_{j=1}^{N}u_{i j}D_{ij}^{L}+\lambda\sum_{i=1}^{C}\sum_{j=1}^{N}u_{i j}\log(\frac{u_{ij}}{\pi_{i j}})\]</span> ​  其中，𝜆是控制簇模糊性的超参数，<spanclass="math inline">\(𝐷_{𝑖𝑗}^𝐿\)</span>是结合像素级和区域级差异的距离函数，如下所示<span class="math display">\[D_{i j}^{L}={\frac{d_{ij}^{L}+d_{R_{i},j}^{L}}{2}}\]</span> ​  这里，<spanclass="math inline">\(𝑑_{𝑖𝑗}^𝐿\)</span>是𝑗th像素与𝑖th聚类中心之间的像素级距离，<spanclass="math inline">\(d_{R_{i},j}^{L}\)</span>是由均值位移算法得到的区域<spanclass="math inline">\(𝑅_𝑗\)</span>值与𝑖th聚类中心之间的区域级距离，<spanclass="math inline">\(𝑅_𝑗\)</span>是𝑗th像素所属的区域。另外，<spanclass="math inline">\(\pi_{i j}\)</span>是先验概率函数。<br/>​  在SFFCM[52]中，提出了另一种基于超像素的模糊c均值聚类，在多尺度形态梯度重建（MMGR）得到的超像素图像上实现直方图参数的FCM。因此，SFFCM利用区域级空间信息，在真实图像分割问题上具有良好的性能。</p><h3 id="隶属度正则化模糊聚类方法">2.3.隶属度正则化模糊聚类方法</h3><p>​  隶属度正则化模糊聚类是试图充分利用最终反对函数中对数据的先验知识来改进聚类处理。先验知识也以不同的形式呈现出来。Pedrycz[53]认为，模糊聚类获得的隶属关系是原始数据的新表示，换句话说，相邻的点应该具有相似的隶属关系。基于这一观点，并受到网络套索的启发，Guo等人[46]探索了一个新的成员亲和套索术语，并将该术语添加到FCM的目标函数中，名为MALFCM。MALFCM对应的目标函数定义如下<spanclass="math display">\[J_{\mathrm{MAL}}=\sum_{i=1}^{C}\sum_{j=1}^{N}u_{ij}^{m}||X_{j}-P_{i}||^{2}+\frac{\lambda}{2}\sum_{j=1}^{N}\sum_{k=1}^{N}\omega_{jk}|u_{i j}-u_{i k}|\]</span>​  其中，等式的第二项（12）是隶属度亲和度套索正则化，𝜆是一个折衷参数，<spanclass="math inline">\(\omega_{j k}\)</span>是点<spanclass="math inline">\(X_{j}\)</span>和点<spanclass="math inline">\(X_{k}\)</span>之间的亲和度。采用乘子交替方向法（ADMM），使上述函数最小化。<br/>​  引入隶属度正则化，隶属度亲和lasso，是为了确保附近数据的隶属度接近，所以MALFCM可以通过从原始数据中建立良好的亲和矩阵得到良好的分类结果。显然，反映数据点之间关系的矩阵在隶属度正则化聚类方法中具有重要意义。然而，在这些聚类方法中构建的亲和矩阵是用于数据挖掘的，并没有充分利用具有空间结构的数据。它们在图像分割问题上的性能仍然不理想。本文在不同层次图像信息融合的基础上，提出了一种新的隶属度正则化模糊聚类亲和矩阵，并给出了一种新的图像分割模型，并给出了一种求解的算法。</p><h1 id="提出的方法mathbfasrmathbfmf">3.提出的方法：<spanclass="math inline">\(\mathbf{A}^{SR}\mathbf{MF}\)</span></h1><p>​  在本研究中，基于上述分析，我们旨在利用像素级邻域和区域级上下文的融合信息来生成亲和矩阵，如图1所示。</p><figure><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241230161302607.png"alt="image-20241230161302607" /><figcaption aria-hidden="true">image-20241230161302607</figcaption></figure><p>​  以MALFCM为例，解释了所提出的亲和力<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>在图像分割任务上，我们还引入了一种新的MALFCM距离测量，考虑了像素的空间和灰度信息。此外，我们采用了在迭代过程中使用固定的簇中心的策略，即使用新的亲和矩阵只通过ADMM更新隶属度值。我们将修改后的方法命名为ASRMF。ASRMF的主要框架如图2所示。</p><figure><imgsrc="../postimages/Fuzzy-clustering-method-for-image-segmentation/image-20241230161435927.png"alt="image-20241230161435927" /><figcaption aria-hidden="true">image-20241230161435927</figcaption></figure><h2id="基于像素和区域级信息的融合构建亲和矩阵">3.1.基于像素和区域级信息的融合构建亲和矩阵</h2><h2 id="隶属度正则化的模糊聚类模型">3.2.隶属度正则化的模糊聚类模型</h2><h2 id="参数估计">3.3.参数估计</h2><hr /><p>算法1 <spanclass="math inline">\(\mathbf{A}^{SR}\mathbf{MF}\)</span></p><hr /><p><strong>输入：</strong>图像<spanclass="math inline">\(\mathbf{X}(x_i,i=1,2,\cdot\cdot\cdot,N)\)</span>和聚类中心<spanclass="math inline">\(\mathbf{V}\)</span>；正则化参数𝜑、终止准则𝜀和最大迭代次数𝑡𝑚𝑎𝑥；<br/><strong>输出：</strong>成员关系矩阵𝑈。<br/>1.通过SLIC算法获得超像素图像，并计算不同区域的平均值；<br/>2.初始化辅助变量𝑐和𝑞，并设置迭代数𝑡=0；<br/>3.通过输入的簇中心得到隶属度划分矩阵𝑈；<br/>4.通过等式13、等式14、等式15推导出亲和矩阵<spanclass="math inline">\(\mathbf A^{S R}(A_{j k}^{SR})\)</span>;<br/>5.通过等式17计算每个像素对每个簇<spanclass="math inline">\(\mathbf D(D_{ij})\)</span>的归属;<br/>6.<strong>repeat</strong><br/>7 Let t = t +1;<br/>8 通过等式26更新成员资格矩阵<span class="math inline">\(\mathbf𝑈\)</span>;<br/>9 通过等式30更新辅助变量𝑐;<br/>10通过等式32更新辅助变量q;<br/>11.直到<span class="math inline">\(t\gtt_{m a x}\)</span>或者<spanclass="math inline">\(\|U^{t+1}-U^{t}\|\leq\varepsilon\)</span>;</p><hr /><h1 id="实验结果">4.实验结果</h1><h2 id="参数设置">4.1.参数设置</h2><h2 id="评价指标">4.2.评价指标</h2><h2 id="使用固定的集群中心的合理性">4.3.使用固定的集群中心的合理性</h2><h2 id="对合成图像的分析结果">4.4.对合成图像的分析结果</h2><h2 id="对医学图像的检测结果">4.5.对医学图像的检测结果</h2><h2 id="对真实图像的结果">4.6.对真实图像的结果</h2><h1 id="运行时间分析">5.运行时间分析</h1><h1 id="结论及未来的工作">6.结论及未来的工作</h1><p>​  本文首先创新地融合了像素级邻域信息和区域级上下文信息两种图像信息，构建了隶属度正则化聚类方法的亲和矩阵<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>，以提高其在图像分割上的性能。然后，在聚类过程中采用固定聚类中心的策略；分割结果仅依赖于亲和矩阵中存储的先验知识，可以减少异常值或噪声像素对隶属度计算的负面影响。以隶属度亲和套索为例，提出了基于信息融合的ASRMF方法，它利用新的亲和矩阵<spanclass="math inline">\(\mathbf{A}^{SR}\)</span>来指导隶属度值的更新。实验结果表明，该方法ASRMF能够在不同类型的图像分割中取得良好的效果。它的性能优于其他最先进的算法。<br/>​  本文只考虑单通道图像，而多通道图像也是一个重要的研究趋势。因此，在我们的未来工作中，我们将研究多通道和单通道融合范式[65]，以创建更好的亲和矩阵。然而，如何自动设置ASRMF的相关参数以及聚类的数量仍是一个有待进一步研究的问题。此外，如何在其他聚类方法中使用该亲和矩阵，如光谱聚类[66]，也是我们需要研究和改进的一个问题。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Forgery-aware Adaptive Learning with Vision Transformer for Generalized Face Forgery Detection</title>
      <link href="/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/"/>
      <url>/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/</url>
      
        <content type="html"><![CDATA[<p>Forgery-aware Adaptive Learning with Vision Transformer forGeneralized Face Forgery Detection</p><p>(发表于TCSVT，IEEE Transactions on Circuits and Systems for VideoTechnology， 1区B刊)</p><h1 id="摘要">摘要</h1><p>​  随着生成模型的快速发展，目前人脸伪造检测面临的挑战是如何有效地检测来自不同未知领域的真实操纵人脸。虽然以往的研究表明，经过预训练的基于视觉变换器（ViT）的模型在对深度伪数据集进行完全微调后，可以获得一些有希望的结果，但其泛化性能仍不令人满意。为此，我们提出了一种在自适应学习范式下的伪造感知自适应视觉Transformer网络（FAViT），其中预先训练的ViT网络中的参数保持不变，而设计的自适应模块进行优化以捕获伪造特征。<br/>​  具体来说，设计了一个全局自适应模块来模拟输入令牌之间的长期交互，利用自注意机制来挖掘全局伪造线索。为了进一步探索必要的局部伪造线索，提出了一种局部自适应模块，通过增强局部上下文关联来暴露局部不一致性。此外，我们还引入了一个细粒度的自适应学习模块，该模块强调通过细粒度对的关系学习对真实面孔的共同紧凑表示，驱动这些提出的自适应模块感知细粒度的伪造感知信息。<br/>​  大量的实验表明，我们的FA-ViT在交叉数据集评估中取得了最先进的结果，并增强了对看不见扰动的鲁棒性。特别是在FA-ViT的跨数据集评估中，Celeb-DF和DFDC数据集的AUC得分分别达到93.83%和78.32%。该代码和训练过的模型已经在：https://github.com/LoveSiameseCat/FAViT上发布了。</p><p>索引术语-人脸伪造检测，视觉转换器，自适应学习，泛化性能。</p><h1 id="i.介绍">I.介绍</h1><p>​  随着深度学习技术的快速发展，人工智能生成内容（AIGC）技术在多种多媒体任务中取得了重大进展。然而，这一进步对人眼辨别这些数字内容提出了巨大的挑战。特别是，攻击者可以很容易地为各种恶意目的生成伪造的面部内容（又名Deepfakes），对社会构成金融欺诈、政治冲突和冒充的紧迫威胁。<br/>​  以往的大多数工作都使用卷积神经网络（CNN）来构建检测器，其中初始[1]或效率网络[2]由于其出色的深度伪检测性能而被广泛应用作基本主干。为了提高其普遍性，一些作品探索了隐藏在被操纵面孔中的常见的伪造线索，如噪声信息[3]-[5]、混合伪影[6]-[8]、频率特征[9]-[11]等。然而，cnn中有限的接受域限制了它们全面学习更广义的特征[12]的能力。作为回应，一些方法寻求使用视觉变压器（ViT）进行人脸伪造检测[13]-[16]。由于自注意机制，这些基于vit的方法可以模拟不同输入标记之间的长程关系。然而，ViT难以捕获局部特征细节，这在深度伪造检测[17]中尤为重要。为了解决这一限制，之前的工作将CNN本地先验纳入到ViT架构[18]，[19]中。<br/>​  与从头开始训练的模型[20]相比，预先训练的模型在下游任务中表现出更好的收敛性和泛化性，并且预先训练的ViT1在取证任务[21]中已被证明是有效的。因此，在以前的工作[13]，[17]-[19]中，通常使用公共可访问的预先训练的权重来初始化基于vit的检测器，然后在深度假数据集上更新这些参数。然而，最近的工作[22]，[23]指出，基于vit的模型对特定的下游任务进行完全微调会破坏预先训练的特征，并可能过度拟合特定的数据模式[21]，可能会阻碍它们在开放集环境中的泛化能力。另一方面，如图1所示，被篡改人脸上的伪造伪影会导致全局或局部的不一致，这表明需要从多个角度对关键表示进行建模。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226110804711.png"alt="image-20241226110804711" /><figcaption aria-hidden="true">image-20241226110804711</figcaption></figure><p>​  基于这些观察结果，我们提出了用于广义人脸伪造检测的伪造感知自适应视觉Transformer网络（FA-ViT），其中预先训练的权值是固定的，只有设计的自适应模块被优化，以从全局和局部的角度捕获丰富的伪造感知信息。<br/>​  FA-ViT的概述如图2所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226110937373.png"alt="image-20241226110937373" /><figcaption aria-hidden="true">image-20241226110937373</figcaption></figure><p>​  它主要由两个模块组成：全局自适应模块（GAM，Global AdaptiveModule）和局部自适应模块（LAM， Local AdaptiveModule），以用于捕获全局和局部伪造感知信息。具体来说，GAM设计来为接收ViT令牌，并与自注意层的原始查询、键和值输出进行交互。<br/>​  具体来说，GAM被设计为接收ViT令牌，并与自注意层的原始查询、键和值输出进行交互。考虑到全局伪影跨越了操纵面部的大区域，GAM利用自我注意机制来增强其从这些伪影中学习全局取证线索的能力。<br/>​  另一方面，LAM采用二次编码[24]来增强每个ViT标记与其相邻空间信息之间的局部关联，指导模型强调局部不一致性之间的异常情况。因此，全局和局部伪造感知信息共同适应固定的预训练特征，形成用于在各种场景中检测深度伪造的广义法医表示。<br/>​  此外，常用的交叉熵损失强调类别水平的差异，但难以捕获细粒度信息，揭示操纵和真实面孔之间的细微差别[10]，[25]，[26]的差异。为了解决这一问题，我们设计了细粒度自适应学习（FAL）。如图2所示，将具有相似视觉语义内容但属于不同类别的细粒度对作为输入对进行分组。FAL利用来自最后一个完全连接（FC）层的权重作为真实面孔的代理原型，并通过圆损失[27]来规范原型和每个细粒度对之间的关系。在FAL的指导下，将所提出的自适应模块挖掘更细粒度的伪造感知信息，在特征空间中压缩真实面，从而进一步提高模型的通用性。<br/>​  我们的主要贡献总结如下：</p><ul><li>我们观察到，当对深度假检测的任务进行完全微调时，基于vit的模型难以推广到看不见的数据集。为了解决这一问题，我们提出了一种新型的伪造感知自适应视觉Transformer网络（FAViT），用于自适应学习范式下的广义人脸伪造检测。</li><li>我们提出了全局自适应模块（GAM）和局部自适应模块（LAM），它们有效地使全局和局部伪造感知信息适应于预先训练的ViT特征。此外，我们设计了一种新的线粒度自适应学习（FAL）来指导这些自适应模型在自适应学习过程中捕获细粒度信息。</li><li>我们在多个数据集和传感器上进行了广泛的实验，结果表明，我们提出的FA-ViT在各种评估中优于最先进的方法。</li></ul><h1 id="ii.-相关工作">II. 相关工作</h1><h3 id="a.面部伪造检测">A.面部伪造检测</h3><h3 id="b.模型适应">B.模型适应</h3><h3 id="c.-细粒度的信息学习">C. 细粒度的信息学习</h3><h1 id="iii.-提出的方法">III. 提出的方法</h1><p>​  在本节中，我们首先在第III-A节中概述了所提出的FA-ViT。三。然后，我们在章节III-B和III-C中描述了全局自适应模块（GAM）和局部自适应模块（LAM）。最后，在第III-D节中提出了细粒度自适应学习（FAL）。</p><h2 id="a.-概述">A. 概述</h2><p>​  我们提出的FA-ViT的框架如图2所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226145840044.png"alt="image-20241226145840044" /><figcaption aria-hidden="true">image-20241226145840044</figcaption></figure><p>​  它采用预先训练好的ViT为基本骨干，由12个ViT块组成。每个块由一个自我注意（SA）层和一个多层感知器（MLP）层组成。在训练过程中，这些块内的参数保持冻结。<br/>​  在FA-ViT中，输入的<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{H\timesW\times3}\)</span>被划分为L个图像补丁，并进一步处理成D维标记，记为<spanclass="math inline">\({\bf X}_{v i t}\ \in\ \mathbb{R}^{L\timesD}\)</span>。在每个ViT块中，我们在SA层中插入一个GAM。当<spanclass="math inline">\({\bf X}_{v it}\)</span>通过SA层时，GAM借助自我注意机制捕获全局伪造感知信息。另一方面，我们从<spanclass="math inline">\({\bfX}\)</span>中提取多尺度空间特征，其中每个尺度特征<spanclass="math inline">\({\bf X}_{c nn}\)</span>都是从一个由三个卷积层组成的CNN块中获得的。LAM为每个ViT标记聚合了来自<spanclass="math inline">\({\bf X}_{c nn}\)</span>的空间伪造感知信息，这有助于以自适应学习的方式捕获丰富的局部细节。除了常用的交叉熵（CE）损失外，我们还引入了FAL来指导GAMs和LAMs来捕获更细粒度的伪造感知信息。</p><h2 id="b.-全局自适应模块gam">B. 全局自适应模块GAM</h2><p>​  自注意层是ViT中的一个关键组件，它使每个输入令牌能够聚合来自所有其他令牌的信息。我们首先简要介绍了自注意层的计算过程。<br/>​  表示<spanclass="math inline">\(\mathbf{X}_{v i t}^{i n}\in\mathbb{R}^{L\timesD}\)</span>是ViT块的输入，首先通过三个可学习矩阵<spanclass="math inline">\(W_{Q}\in\mathbb{R}^{D\times D}\)</span>，<spanclass="math inline">\(W_{K}\in\mathbb{R}^{D\times D}\)</span>和<spanclass="math inline">\(W_{V}\in\mathbb{R}^{D\timesD}\)</span>，预测查询令牌<span class="math inline">\({\bfQ}\in\mathbb{R}^{L\times D}\)</span>，关键令牌<spanclass="math inline">\({\bf K}\in\mathbb{R}^{L\timesD}\)</span>和值令牌<span class="math inline">\({\bfV}\in\mathbb{R}^{L\times D}\)</span>： <span class="math display">\[{\bfQ}={\bf X}_{v i t}^{i n}W_{Q},\ \ {\bf K}={\bf X}_{v i t}^{i n}W_{K},\ \{\bf V}={\bf X}_{v i t}^{i n}W_{V}.\]</span>​  然后将自注意层的计算表示为： <span class="math display">\[{\bf X}_{v it}^{o u t}=\mathrm{Attention}({\bf Q},{\bf K},{\bfV})=\mathrm{softmax}(\mathbf{QK}^{\mathsf{T}}/{\sqrt{D}})\mathbf{V},..\]</span>​  其中，<span class="math inline">\({\bf X}_{v i t}^{o ut}\)</span>是输出。<br/>​  所提出的GAM建立在自注意层之上，利用自注意机制挖掘全局信息。先前的工作[18]，[61]，[62]已经证明了标记嵌入（即块表示）表现出比与遥远标记的邻近标记更强的相关性。因此，采用具有瓶颈结构的卷积层在GAM中建模这种局部空间关系，在全局自适应学习过程中引入与标记相关的局部先验。我们提出的GAM的细节如图3所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226152740847.png"alt="image-20241226152740847" /><figcaption aria-hidden="true">image-20241226152740847</figcaption></figure><p>​  具体来说，首先将<span class="math inline">\(\mathbf{X}_{v i t}^{in}\)</span>根据其原始的空间位置重塑为<spanclass="math inline">\(\sqrt{L}\times\sqrt{L}\timesD\)</span>的形状0。然后应用1×1卷积层来降维，然后使用3×3卷积层来捕获令牌级依赖。最后，GAM通过三种不同的1×1卷积生成Q、K和V的全局自适应信息。此过程的表述如下：<span class="math display">\[\triangle{\bf Q},\triangle{\bfK},\triangle{\bfV}=\mathrm{Conv}_{1\times1}^{Q,K,V}(\mathrm{Conv}_{3\times3}(\mathrm{Conv}_{1\times1}({\bfX}_{w i t}^{i n}))),\]</span></p><p><span class="math display">\[{\bf X}_{v i t}^{o ut}=\mathrm{Atention}(\bf Q+\triangle Q,{\bf K}+\triangle\bf K,{\bfV}+\triangle{\bf V}),\]</span></p><p>​  其中<span class="math inline">\(\triangle{\bf Q},\triangle{\bfK},\triangle{\bfV}\)</span>是原始Q、K和V的自适应信息。由于原始信息和自适应信息融合在一起，并随后通过自我注意操作进行处理，这确保了GAM与所有令牌交互，从而从全局的角度捕获伪造感知特征。值得注意的是，<spanclass="math inline">\(\mathrm{Conv}_{1\times1}^{Q,K,V}\)</span>中的参数被初始化为零，这有助于全局伪造感知知识的稳定学习。</p><h2 id="c.-局部自适应模块lam">C. 局部自适应模块LAM</h2><p>​  纯ViT处理通过堆叠的线性层的输入，这很难捕获对检测被操纵的面孔至关重要的局部细节。如图1所示，被操纵面中的伪影往往会引入局部不一致，说明每个查询令牌必须强调其与周围位置的关系，以检测局部伪造线索。以前的方法使用交叉注意模块[14]、[15]、[63]或加法操作[17]、[64]将局部空间信息注入到类似vit的架构中，但经常忽略了每个查询令牌的上下文重要性。为了缓解这一问题，我们提出的LAM旨在强调自适应学习过程中的上下文信息，从而有效地从局部不一致中捕获关键的局部伪造线索。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226160540754.png"alt="image-20241226160540754" /><figcaption aria-hidden="true">image-20241226160540754</figcaption></figure><p>​  图4提供了我们提出的LAM的细节。空间特征<spanclass="math inline">\({\bf X}_{c n n}\)</span>首先通过MLP层投射到<spanclass="math inline">\(\hat{\mathbf{X}}_{c n n}\)</span>上，其中<spanclass="math inline">\(\hat{\mathbf{X}}_{c n n}\)</span>和<spanclass="math inline">\({\bf X}_{v i t}\)</span>具有相同的形状。对于<spanclass="math inline">\({\bf X}_{v i t}\)</span>中的第a个令牌<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>，LAM通过同时考虑其全局注意<spanclass="math inline">\(\mathrm{A}_{g l o b al}^{a}\)</span>和局部注意<span class="math inline">\(\mathrm{A}_{l o c al}^{a}\)</span>，计算其在<span class="math inline">\(\hat{\mathbf{X}}_{cn n}\)</span>不同部分的注意得分： <spanclass="math display">\[\mathbf{A}_{f i n al}^{a}=(1-\varphi(\sigma))\mathbf{A}_{g l o b al}^{a}+\varphi(\sigma)\mathbf{A}_{l o c a l}^{a},\]</span> ​  其中<spanclass="math inline">\(\mathbf{A}_{f i n a l}^{a}\)</span>是<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>的最终注意图。<spanclass="math inline">\(\varphi(\cdot)\)</span>表示s型函数，<spanclass="math inline">\(\sigma\)</span>是一个初始化值为零的可学习参数。<spanclass="math inline">\(\mathrm{A}_{g l o b al}^{a}\)</span>的计算类似于交叉注意，其表示如下： <spanclass="math display">\[{\bf A}_{g l o b a l}^{a}=s o f t m a x(({\bfX}_{t o k e n}^{a}W_{Q})({\bf\hat{X}_{c n n}}W_{K})^{T}).\]</span>​  利用二次编码[24]，引入<span class="math inline">\(\mathrm{A}_{l o c al}^{a}\)</span>算法来强调<span class="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>周围空间信息的局部性： <span class="math display">\[{\bfA}_{l o c a l}^{a}=s o f t m a x(\Phi\Psi^{T}),\]</span> ​  其中，<spanclass="math inline">\(\Phi \in\mathbb{R}^{\sqrt{L}\times{\sqrt{L}}\times3}\)</span>表示局部强度，<spanclass="math inline">\(\Psi\in \mathbb{R}^{1\times3}\)</span>是决定<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>注意方向的方向向量。注意，为了简化，我们在softmax操作之前省略了平坦操作。假设<spanclass="math inline">\(\mathbf{X}_{t o k en}^{a}\)</span>的空间位置为<spanclass="math inline">\((i_a,j_a)\)</span>，同样，在<spanclass="math inline">\(\hat{\mathbf{X}}_{c nn}\)</span>中第b个标记的空间位置为<spanclass="math inline">\((i_b,j_b)\)</span>。<spanclass="math inline">\(\Phi\)</span>在位置<spanclass="math inline">\((a,b)\)</span>上的相对局域性强度<spanclass="math inline">\(\phi_{a,b}\in\mathbb{R}^{1\times3}\)</span>表示为：<spanclass="math display">\[\phi_{a,b}=(\|(i_{b}-i_{a},j_{b}-j_{a})\|_{2},i_{b}-i_{a},j_{b}-j_{a})\,.\]</span>​  另一方面，方向向量<span class="math inline">\(\Psi\)</span>表示为：<spanclass="math display">\[\Psi=(-1,2\psi_{1},2\psi_{2}),(\psi_{1},\psi_{2})\in\{-1,0,1\}\,.\]</span>​  在实践中，我们在不同的头部为<spanclass="math inline">\(\psi_{1}\)</span>和<spanclass="math inline">\(\psi_{2}\)</span>分配不同的值，以探索不同方向的局部信息，如图5所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226163304337.png"alt="image-20241226163304337" /><figcaption aria-hidden="true">image-20241226163304337</figcaption></figure><p>​  我们为每个查询标记收集最终的注意映射，以形成<spanclass="math inline">\(\mathbf{X}_{v i t}\)</span>的<spanclass="math inline">\(\mathbf{A}_{f\,i n al}\)</span>，并使用它将局部空间信息注入到ViT特征中。此过程的表述如下：<span class="math display">\[\hat{\bf X}_{v i t}={\bf X}_{v it}+\beta{\bf A}_{f i n a l}\hat{\bf X}_{c n n}W_{V},\]</span>​  其中β是一个初始化的可学习缩放因子，<spanclass="math inline">\(\hat{\bf X}_{v it}\)</span>被传递到下一个vit块。根据经验，将多尺度空间信息分别注入第一、第四和第七个ViT块。</p><h2 id="d.-细粒度的自适应学习fal">D. 细粒度的自适应学习FAL</h2><p>​  细粒度信息对于提高泛化性能[17]、[47]、[48]非常重要。因此，我们引入了FAL来促进细粒度伪造感知信息的自适应学习。我们首先将最后一个FC层中与真实人脸对应的权重向量设置为真实人脸的代理原型，因为之前的工作[65]已经证明，分类器中的权重收敛到每个类的中心方向。在每个细粒度对中，FAL拉近了原型和真实人脸之间的相似性，同时通过修改后的circle损失将篡改过的人脸从原型中推开[27]：<span class="math display">\[L_{F AL}=\log\big[1+\sum\exp(\eta(\gamma_{n}(s_{n}-m_{n})-\gamma_{p}(s_{p}-m_{p})))\big],\]</span></p><p><span class="math display">\[s_{p}=\mathrm{CosSim}(\mathrm{F}_{r e al},\mathrm{F}_{p r o}),\ s_{n}=\mathrm{CosSim}(\mathrm{F}_{f a ke},\mathrm{F}_{p r o})\]</span></p><p><span class="math display">\[\gamma_{p}=m ax(1+m-s_{p},0),\;\;\gamma_{n}=m a x(m+s_{n},0),\]</span></p><p><span class="math display">\[m_{p}=1-m,\ m_{n}=m,\]</span></p><p>​  其中CosSim表示余弦相似度，而<spanclass="math inline">\(F_{pro}\)</span>是真实面孔的原型。η是比例因子。M是控制细粒度对中的裕度和加权因子的超参数。<spanclass="math inline">\(F_{real}\)</span>和<spanclass="math inline">\(F_{fake}\)</span>是来自最后一个ViT块的细粒度对的编码特性。<br/>​  在优化过程中，通过<spanclass="math inline">\(\gamma_{n}(s_{n}-m_{n})-\gamma_{p}(s_{p}-m_{p})=0.\)</span>，实现了决策边界。通过使用方程式13和14，将决策边界转换为：<span class="math display">\[s_{n}^{2}+(1-s_{p})^{2}=2m^{2}\]</span>​  方程15表明FAL鼓励细粒度对向以sp = 1和sn = 0为中心、半径为<spanclass="math inline">\({\sqrt{2}}m\)</span>的圆收敛，如图6所示。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226171306453.png"alt="image-20241226171306453" /><figcaption aria-hidden="true">image-20241226171306453</figcaption></figure><p>​  这种优化使每个细粒度对能够提供更灵活的梯度来指导细粒度信息的学习。例如，在A点，当远远大于<spanclass="math inline">\(grad_r\)</span>时，该模型强调捕获细粒度的鉴别信息，以推开被篡改的人脸。相比之下，在B点，当<spanclass="math inline">\(grad_f\)</span>比<spanclass="math inline">\(grad_r\)</span>小得多时，该模型侧重于学习细粒度的一致性，以拉近真实的人脸。另一方面，FAL忽略了非细粒度对的梯度，如图图6中C点的例子。在这种情况下，差异涉及到背景或非必要的操作区域，这可能会导致模型过拟合到平凡的特征。<br/>​  显然，FAL通过探索细微差异区域的细粒度信息，在特征空间中紧凑真实的人脸特征。它没有明确地惩罚被操纵面孔的距离，因为我们希望在不同的伪造技术中保留操纵痕迹的多样性。与单中心损失（SCL，SingleCenterLoss）[10]使用不同类别之间的平均距离来压缩类内方差不同，FAL专注于学习每个细粒度对中的细粒度鉴别信息和一致信息，使挖掘关键信息的过程更加精确。</p><h2 id="e.-总损失">E. 总损失</h2><p>​  我们提出的FA-ViT是端到端训练的，并由预测结果<spanclass="math inline">\(\haty\)</span>和地面真实标签y之间的交叉熵损失进行监督： <spanclass="math display">\[L_{ce}=-y\log\hat{y}-(1-y)\log\left(1-\hat{y}\right),\]</span>​  其中，标签y为0是为真实的面孔，否则y为1。总体目标函数由两个组成部分组成：<span class="math display">\[L_{t o t a l}=L_{c e}+\lambda L_{F AL},\]</span>​  其中，λ是一个加权参数。在第一个训练阶段，我们将λ设置为0，允许模型专注于学习分类信息。随后，我们将λ调整为1，从而促进了细粒度信息的学习。</p><h1 id="iv.-实验">IV. 实验</h1><h2 id="a.实验设置">A.实验设置</h2><h3 id="数据集">1)数据集：</h3><p>​  我们采用了8个广泛使用的公共数据集来评估我们的模型。<br/>​  1)FaceForensics++（FF++）[66]是一个广泛使用的数据集，由四种类型的人脸操作技术组成：DeepFakes(DF) [75], Face2Face (F2F)[76], FaceSwap (FS) [77], and NeuralTextures(NT)[78]。<br/>​  2)Celeb-DF-v2（CDF）[68]是一个高质量的深度假数据集，专门针对名人的人脸。<br/>​  3)WildDeepfake（WDF）[69]从互联网上收集深度伪造视频，其中包括各种场景和伪造方法。在WDF上的评价结果反映了检测器在现实世界场景中的性能。<br/>​  4)DeepfakeDetectionChallenge（DFDC）[71]提供了一个具有挑战性的数据集，包含来自不同场景的各种深度假视频，使用了不同的深度伪造、基于GAN和传统的人脸交换方法。<br/>​  5)DeepfakeDetection ChallengePreview（DFDC-P）[70]提供了一个DFDC数据集的预览版本，并合并了两种面部修改算法。<br/>​  6)DeepFakeDetection（DFD）[72]是另一个全面的深度假数据集。这个数据集包括超过3000个被操纵的视频，包括28个演员和不同的场景。<br/>​  7)DeeperForenics-1.0（DFR）[67]是通过使用FF++的真实视频和创新的端到端面部交换框架。它也可以作为一个流行的数据集来衡量morel的鲁棒性。<br/>​  8)FFIW-10K (FFIW) [73]是一个最近的大规模数据集，它专注于多人的场景。对于DFD数据集，我们使用所有的视频来进行评估。对于其他的数据集，我们按照官方的策略来分割相应的数据集。关于一个全面的概述，请参见表一。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226212602830.png"alt="image-20241226212602830" /><figcaption aria-hidden="true">image-20241226212602830</figcaption></figure><h3 id="实施细节">2)实施细节：</h3><p>​  我们使用MTCNN[79]裁剪面区域，并将其调整为224×224。我们从每个视频中只采样20帧来构建训练数据。在测试过程中，我们从每个视频中抽取50帧的样本进行评估。我们的方法是在一个NVIDIAGTX 3090上使用PyTorch库[80]实现的。我们采用在ImageNet-21K[81]上预训练的ViT-Base模型作为FA-ViT的主要骨干。为了进行优化，我们使用了Adam[82]优化器，其初始学习速率为3×10−5，权重衰减为1×10−5，批量大小为32。学习速率每5个时代衰减0.5个。FAL中m和η的超参数将在第2节中进行讨论。IV-F.CNN的详细结构如表二所示，参数使用随机初始化方法进行初始化。</p><h3 id="评价指标">3)评价指标：</h3><p>​  我们遵循[17]中的评价策略，以准确性（ACC）和受试者工作特征曲线下面积（AUC）作为我们的评价指标。为了进行公平的比较，我们对同一视频中的预测进行平均，得到视频级的预测，并给出了其他工作的结果。</p><h2 id="b.-数据集内评估">B. 数据集内评估</h2><p>​  我们在广泛使用的FF++数据集上进行了数据集内实验，包括高质量（C23）和低质量（C40）数据集。所有的模型都在同一个数据集上进行训练和测试，其中的性能显示了模型在伪造的人脸中捕获操作痕迹的能力。表3说明了数据集内的结果，其中我们分别加粗和下划线显示了最佳和第二优的分数。</p><figure><imgsrc="../postimages/Forgery-aware-Adaptive-Learning-with-Vision-Transformer-for-Generalized-Face-Forgery-Detection/image-20241226213115559.png"alt="image-20241226213115559" /><figcaption aria-hidden="true">image-20241226213115559</figcaption></figure><p>​  一般来说，在C40压缩数据中删除了许多操作痕迹，这使得它们很难被检测到。在这种具有挑战性的环境下，可以观察到，我们提出的FAViT比大多数以前的艺术有相当大的优势。例如，我们的方法在AUC方面比最近基于vit的模型F2Trans多出了2.53%。当对C23数据进行评估时，所有的SOTA方法都能够实现非常高的检测acc。FA-ViT仍然取得了97.86%的ACC评分和99.60%的AUC评分，证明了其对C23数据的有效性。此外，与ViT基线相比，FAViT在C40设置下将ACC从90.00%提高到92.17%，在C23设置下将96.00%提高到97.86%。<br/>​  虽然FA-ViT在数据集内评估中获得了令人满意的检测精度，但其性能仍低于CFM[39]和RECCE[37]。这一结果可能是由于CFM需要像素级标签来进行监督，而侦察采用了像素级重建任务，使得这些方法对数据集内模式更加敏感。我们将研究其他设计的可能性，以进一步提高数据集内设置的性能。</p><h2 id="c.-跨数据集评估">C. 跨数据集评估</h2><p>​  跨数据集评估是一个非常具有挑战性的设置，因为这些数据集中的场景和字符是复杂的，并且与训练数据不同。为了全面评估在不同的不可见数据集上的泛化性能，我们在7个基准数据集上进行了广泛的实验。具体来说，所有的模型都在FF++（C23）上进行训练，并在不可预见的数据集上进行评估：CDF、WDF、DFDC-P、DFDC、DFD、DFR和FFIW。</p><p>​  表四说明了在AUC方面的交叉数据集比较结果。与其他基于ViT的CDF方法相比，包括UIA-ViT、F2Trans、TALL-ViT和ViT，我们提出的FA-ViT方法分别优于4.98%、6.37%、7.25%和10.05%。值得注意的是，CFM获得了最好的数据集内性能，但其泛化性能不如我们提出的FA-ViT。此外，与最近的方法LSDA和DSRL相比，我们的FA-ViT在AUC方面分别提高了4.88%和4.75%，突出了其检测看不见的深度假面孔的最先进的泛化能力。总的来说，FA-ViT在所有7个数据集上都表现出了出色的性能，这些发现验证了自适应学习范式对预训练的ViT的有效性。</p><h3 id="d.-交叉操作评估">D. 交叉操作评估</h3><h3 id="e.-对真实世界扰动的鲁棒性">E. 对真实世界扰动的鲁棒性</h3><h3 id="f.-消融研究">F. 消融研究</h3><p>​  为了分析我们提出的FA-ViT中不同成分的影响，我们通过在FF++（C23）上训练所有变体来进行消融实验。我们给出了FF++的数据集内结果和CDF和WDF数据集的跨数据集结果。为了确保公平的比较，所有的实验都使用相同的随机种子进行。</p><p>1)对不同模型自适应的影响</p><p>2)GAM的消融：</p><p>3)LAM的影响</p><p>4)对提出的FAL进行的实验</p><p>5)不同参数对FAL的影响</p><p>6)零初始化的影响</p><p>7)不同的预训练初始化的影响</p><p>8)不同的预训练初始化的影响</p><h2 id="g.-显著性地图可视化">G. 显著性地图可视化</h2><p>​  为了更好地阐明我们的FA-ViT的有效性，我们使用GradCAM++[92]将模型对深度假脸的注意力可视化，如图10所示。可以观察到，FA-ViT为不同的深度假脸生成了可区分的显著性图，并捕获了方法特有的伪影，如FS的前额区域和F2F的口腔区域。在跨数据集场景中，ViT难以在复杂环境中检测深度造假，如CDF中的大姿态人脸或DFDC中具有挑战性的照明条件。相比之下，FA-ViT在不同的未知数据集上持续地跟踪被操纵的区域，从而从决策的角度验证其有效性。</p><h1 id="v.-结论">V. 结论</h1><p>​  在本文中，我们从模型自适应的角度提出了一种新的伪造感知自适应自适应视觉变压器（FA-ViT），这在以往的研究中被忽略了。具体来说，在训练过程中设计了全局自适应模块（GAM）和局部自适应模块（LAM），将全局和局部伪造感知信息用于广义表示学习时，保留了预训练后的ViT的表达性。此外，我们还引入了细粒度自适应学习（FAL），以促进细粒度伪造感知信息的自适应学习。总之，我们提出的框架为人脸伪造检测的挑战提供了一个广义和稳健的解决方案。我们相信，我们提出的方法可以为研究界提供有价值的见解，并进一步推进人脸伪造检测系统的发展。</p>]]></content>
      
      
      <categories>
          
          <category> 人脸篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Adaptive Fuzzy Clustering for Evolutionary Unsupervised Representation Learning</title>
      <link href="/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/"/>
      <url>/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/</url>
      
        <content type="html"><![CDATA[<p>Deep Adaptive Fuzzy Clustering for Evolutionary UnsupervisedRepresentation Learning</p><h1 id="摘要">摘要</h1><p>​  在模式识别和计算机视觉中，大型和复杂数据集的聚类分配是一项关键但具有挑战性的任务。在本研究中，我们探索了在深度神经网络框架中使用模糊聚类的可能性。因此，我们提出了一种新的具有迭代优化的进化无监督学习表示模型。它实现了深度自适应模糊聚类（DAFC）策略，从只给定的未标记数据样本中学习卷积神经网络分类器。DAFC由深度特征质量验证模型和模糊聚类模型组成，其中实现了深度特征表示学习损失函数和具有加权自适应熵的嵌入式模糊聚类。我们将模糊聚类联合到深度重建模型中，其中利用模糊隶属度来表示一个清晰的深度聚类分配结构，并联合优化深度表示学习和聚类。同时，该联合模型通过检查从估计的瓶颈空间重采样的数据是否具有一致的聚类特性来评估当前的聚类性能，从而逐步改进深度聚类模型。在不同数据集上的实验表明，与其他先进的深度聚类方法相比，该方法在重建和聚类质量方面都有更好的性能，大量实验的深入分析证明了这一点。</p><h1 id="i.介绍">I.介绍</h1><p>​  受深度特征学习框架、深度信念网络（DBN）[18]和稀疏自编码器层次结构[19]、[20]的成功启发，它们试图从输入数据中提取特征，如反进化网络，贪婪地以无监督的方式从图像向上构建层。在这些研究中，每一层都由具有聚类模型的编码器和解码器组成。此外，用于深度聚类的卷积神经网络（CNN）架构，其中变分自编码器（VAE）是用于深度生成学习的非常强大的无监督框架。在深度聚类中广泛使用的cnn是堆叠自动编码器（SAEs）[21]和结构化自动编码器（StructAE），它们结合了基于图的聚类来实现精确的深度表示。它们都采用了多阶段管道，用无监督学习方法对统一的深度网络模型进行预训练，并对大规模图像实现了传统的聚类方法作为后处理。然而，它们在聚类分配中进行深度聚类，且高度依赖于预先训练好的相似度矩阵，因此在大数据集上的聚类性能不够好。<br/>​  此外，Yang等人的[11]使用循环框架迭代地学习深度特征表示和聚类分配。该框架在聚类模型中的连续操作被表示为循环过程中的步骤，并堆叠在通过cnn输出的表示方式之上。他们的模型在小的复杂数据集上表现出了很好的性能，但可能对多凸网竞争所需的大量复杂图像具有挑战性。深度嵌入式聚类（DEC）[22]是一种著名的深度网络结构，它可以同时实现特征表示和聚类模型来训练复杂的数据集。该方法通过采用高度机密的样本作为监督，使每个聚类中样本的分布更密集、更清晰地分组。然而，在使用随机梯度下降（SGD）训练大而复杂的数据集时，并不能保证将样本拉到正确的聚类附近，这也不能保证快速收敛。<br/>​  虽然卷积网方法的联合聚类和特征学习在无监督学习中表现出了显著的性能，但在特征聚类和网络参数更新之间交替的训练计划导致特征表示[23]的学习不稳定。此外，它们也没有联合优化深度表示学习和聚类。具有自进化聚类[24]和大规模多目标决策聚类[25]的深度对流模型由于能够处理大规模数据集和复杂的表示而受到越来越多的关注。由于计算复杂度高，深度卷网络结构的应用通常需要一个具有强大计算能力的平台。一些聚类方法已经用深度共流网络进行了研究，但深度聚类的关键成分仍不清楚。例如，如何有效地将实例为巨大的复杂数据分组到集群中，并提供定义面向集群的损失函数的有效信息？如何在精度和效率之间实现良好的权衡，并提高深度卷网的聚类性能？哪些类型的神经网络结构适合用于聚类的特征表示学习？<br/>​  在本研究中，我们的目标是开发新的进化表示学习解决方案的深度无监督聚类问题。综上所述，本研究的主要贡献如下。</p><ol type="1"><li>我们提出了DAFC来自动分组图像，得到的迭代优化问题可以通过小批量RMSprop和反向传播而不是SGD有效地解决，可以学习一个更聚类友好的瓶颈空间。<br/>2.我们仔细地制定了一个目标，以包含对高聚类性能至关重要的三个关键方面：有效的潜在表示空间、相似度度量和深度聚类的加权自适应熵，它们可以集成到深度学习架构中<br/>3.与单独优化这些目标的情况相比，这种深度进化无监督表示学习对网络损失和具有模糊聚类的重建损失提供了优越的性能。<br/>4.加权自适应熵的深度聚类，我们计算模糊会员和最优权重作为全局变量，可以共同优化深度表示学习，有效地组实例到集群为巨大的复杂数据和实现一个好的权衡精度和效率，以及进一步提高深度双convnet集群的性能。</li></ol><p>​  本研究的其余部分组织如下。我们首先回顾了在第二节中包含一些有效的深度聚类框架的相关工作。我们在第三节提出的深度自适应模糊聚类（DAFC）。在第四节中，我们讨论了最先进的深度聚类方法的结果和分布，并将它们与我们的方法进行了比较。本研究的结论和今后的工作见第五节。</p><h1 id="ii.相关工作">II.相关工作</h1><p>​  VAE的网络由编码器和解码器组成，其中潜在特征层后面是潜在特征层，如图1所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165218699.png"alt="image-20241223165218699" /><figcaption aria-hidden="true">image-20241223165218699</figcaption></figure><p>​  编码器将输入的样本映射到潜在的特征层。潜在特征层从编码器模型中学习输入数据的潜在特征，并将高维特征映射到低维子空间，然后利用聚类模型对所映射的数据进行划分。VAE的解码器恢复并重建特征，使特征数据能够重构为原始数据。期望VAE的输入和输出可以以一致的或无损的方式进行重构，且潜在特征向量的维数远小于输入样本的维数。使用学习到的潜在层来聚类分配和其他任务将更有效。该方法有利于学习更多重要的特征，而忽略了一些冗余的特征。但是，如果目标的大小与训练图像的背景有较大的差异，训练网络很容易忽略学习过程[34]中忽略目标特征。因此，当训练诸如ImageNet数据集等庞大而复杂的数据集时，VAE的精度并不高，甚至不能对其进行分区。<br/>​  虽然该方案利用SAE和VAE将输入数据映射到一个具有代表性的特征空间，然后进行聚类分析，但特征表示空间和聚类方法是两个独立的过程，其目标函数没有联合优化。在第三节中，我们将基于ConvNets结构建立更深层次的EF和重建模型，并结合模糊聚类模型。</p><h1 id="iii.方法">III.方法</h1><p>​  在本节中，我们将描述我们的深度进化无监督表示学习，并表明通过深度聚类框架可以获得有用的通用的巨大和复杂的特征。</p><h2 id="a.网络架构">A.网络架构</h2><p>​  基于统计学习和深度cnn的现代深度网络计算机视觉方法需要良好的图像特征化。在DAFC方法中，我们建立了一个更深入的FE模型和Rec模型，并通过潜在的表示瓶颈层将它们连接起来。瓶颈空间的有效潜在表示是深度进化无监督表示学习的一个关键方面，它可以更好地提取网络的每一层之间的特征。在联合聚类的瓶颈空间中构建了一个局部结构，以实现更好的聚类分配。与最先进的深度聚类相比，我们很容易假设这种优势是由于瓶颈空间可以通过最小化重构和聚类损失来保留输入数据的局部结构。我们设计了一种有效的基于凸面的模糊聚类模型分类器，以广泛地利用瓶颈空间，其中损失函数是重构损失和模糊聚类损失的和。<br/>​  DAFC的网络体系结构的目标是将多层数据点划分为完全没有任何标签的集群，并联合优化深度表示学习，进一步提高了对异常值的鲁棒性。对于同一样本，不同类型的层（深和浅）之间的相互信息应该最大化。为了提高深度聚类的准确性，我们增加了模型中网络的特征训练层的数量。受ResNet[35]和DenseNet学习[36]的启发，我们试图控制网络优化器，以便对SGD、Adam和RMSprop进行比较。在网络的梯度下降问题上，我们采用了一个较慢的学习速率来处理脊间的跳跃问题，这有助于获得更好的聚类性能。<br/>​  所提出的网络模块的关键因素是，在我们的深度ConvNets模型中，我们使用了具有步幅的卷积层和池化层，而不是接在池化层后的卷积层。它不同于VAE的模块。在这项工作中设计的模块导致了更高的转换能力。我们对卷积层、ReLU层、批归一化、池化和退出层进行多个组合，如图2所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223165700880.png"alt="image-20241223165700880" /><figcaption aria-hidden="true">image-20241223165700880</figcaption></figure><p>​  此外，深度ConvNets模型通常通过池层缩小特征映射的大小。所设计模型的每个模块通过前馈方式将每一层连接到每一层，模块1为22层前馈神经网络，具有前2层，模块2为8层。该模型采用多个全连通层作为瓶颈空间的潜在表示。我们还可以根据输入数据的要求，灵活地深化网络模块的建设。当在足够复杂的数据上进行训练时，该方法在标准的竞争分类基准上不断取得最佳性能。<br/>​  在我们的网络模型中，我们使用多个全连接层，加深全连接层的数量，以提高模型的非线性表达式和特征学习能力。为了防止深化模型的学习能力过好，导致过拟合，我们添加了dropout层，然后是全连接层。dropout层只允许为每个反向传播调整部分网络参数。如图3所示，我们根据网络模块建立了DAFC的深度卷积神经网络模型。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223170209837.png"alt="image-20241223170209837" /><figcaption aria-hidden="true">image-20241223170209837</figcaption></figure><p>​  在这些模块中，每个块都有不同的功能层。此外，模块1和模块2分别对应于所设计的网络模型的每个部分。<br/>​  在该网络体系结构中，模块1被设计为FE模型，模块2被设计为重构模型，如图2所示。FE模型提供了一个从输入到潜在表示瓶颈空间的自底向上的映射，而重构（Rec）模型将提取的特征映射回输入空间，希望能给出一个接近原始数据的重构。FE和重构模型，并通过卷积操作的潜在表示瓶颈层将它们连接起来。在联合模糊聚类的瓶颈空间中构建了一个局部结构，以实现更好的聚类分配。在Rec模型中，我们设计了一种有效的基于深度凸网的分类器，具有加权自适应熵模糊聚类模型，以广泛利用瓶颈空间。此外，我们还研究了一种联合策略，其中损失函数是最小化重建损失和模糊聚类损失的和，其中FE模型<spanclass="math inline">\(F = f_w (x)\)</span>和Rec模型<spanclass="math inline">\(G\equivg_{\theta}(F)\)</span>的参数。<br/>​  为此，我们将FE模型的重构损失和模糊聚类损失同时结合为目标函数。深度卷积网络的FE模型保留了复杂数据生成分布的局部结构，避免了特征空间的破坏。然后，随着学习的进行，设计的深度卷积网络可以通过迭代训练来测量更准确的相似性，并且会逐渐选择更多的聚类任务来找到更精细的组。模糊聚类模型的目的是在大量和不同的数据点之间对相似或相同的模式进行分组。在这个深度聚类模型中，我们使用隶属度使聚类结果更具区分性，µ是数据点x属于第j个聚类的分配概率。</p><h2 id="b.深度聚类策略">B.深度聚类策略</h2><p>​  在我们的网络体系结构中，FE和Rec模型重叠了复杂多层次的所有特征，其中重叠的域，这是整个体系结构的层次组成。通过将l层的特征映射f(x)作为层l + 1的输入，可以很容易地堆叠形成层次，其中输入数据x到瓶颈空间Z(x)的初始非线性映射，如图4所示。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223171332756.png"alt="image-20241223171332756" /><figcaption aria-hidden="true">image-20241223171332756</figcaption></figure><p>​  映射F和G通过FE模型和Rec模型实现，它们通过优化的瓶颈空间与卷积层和多个全连接层进行连接。改进了瓶颈空间表示，使具有属于同一集群的高概率的映射点对将被拉得更近在一起。因此，设计瓶颈空间将FE模型和Rec模型连接起来，这对联合模糊聚类更有意义。</p><figure><imgsrc="../postimages/Deep-Adaptive-Fuzzy-Clustering-for-Evolutionary-Unsupervised-Representation-Learning/image-20241223170642966.png"alt="image-20241223170642966" /><figcaption aria-hidden="true">image-20241223170642966</figcaption></figure><p>​  f是训练过程中模糊隶属度为<spanclass="math inline">\(\mu\)</span>的瓶颈空间中输入数据的特征，如图5中的相似性结构所示，z为瓶颈空间表示。它们也代表了在网络超参数下由卷积网分类器进行的特征划分的结果。通过优化重构的损失函数，共同学习ConvNets分类器的参数θ和映射的参数w，公式如下：</p><p><spanclass="math display">\[{L}_{\mathrm{Rec}}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),z_{i}\right]\]</span></p><p>​  首先，我们通过数据增强得到一批原始数据<spanclass="math inline">\(\{x\}\)</span>加上转换后的数据<spanclass="math inline">\(\{x^{\prime}\}\)</span>，然后得到输入数据：<spanclass="math inline">\(x_i = x + x^{\prime}\)</span>。<spanclass="math inline">\(x_i\)</span>表示第i张图像，M为图像数。在输入数据中，每个图像<spanclass="math inline">\(x_i\)</span>都与<spanclass="math inline">\(\{0,1\}^k\)</span>中的一个标签<spanclass="math inline">\(y_i\)</span>相关联。该网络将输入数据映射到紧凑的特征映射<spanclass="math inline">\(F=f_w(x_i)\)</span>中，并对FE模型的输出进行聚类，然后使用后续的聚类分配作为伪标签来优化(1)，并作为反馈信息反向传播到Rec模型中。下一步通过使用Rec模型的网络生成输入数据的特征标签，其中相似度矩阵从ConvNets的样本内存中读取该批数据的伪标签。图5是所提出的使用模糊聚类模型的深度进化无监督表示学习的示意图。我们在定义1中定义了相似矩阵，并对于潜在瓶颈空间表示<spanclass="math inline">\(z_{i}=f_{\mu}(y_{i},x_{i})\)</span>。利用相似度矩阵，利用最优优化器对深度网络进行更新，结合模糊聚类模型如下：</p><p><spanclass="math display">\[{L}_{\mathrm{Rec}}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\]</span>​  其中<spanclass="math inline">\(f_{\mu}(\cdot)\)</span>为相似度矩阵每一列设置的模糊隶属度，这种表示应该更准确。基于(2)，我们进一步得到<spanclass="math display">\[f_{\mu}\left(y_{i},x_{i}\right)=C_{i,j}\left(x_{i}\right)\cdoty_{i}\]</span></p><h3 id="定义1相似矩阵">定义1：相似矩阵。</h3><p>​  我们假设C是生成特征标签的邻接矩阵，<spanclass="math inline">\(a_i\)</span>是C的第i列。标签的节点i与节点j的相似性为<spanclass="math display">\[C_{i,j}(\cdot)=\frac{a_{i}^{T}a_{j}}{\|a_{i}\|}\=\frac{a_{i}^{T}a_{j}}{\sqrt{d_{i}}\sqrt{d_{j}}}\]</span>​  我们将FE模型训练的特征转移到瓶颈空间，在目标中加入模糊聚类，并随着聚类损失进行优化。将模糊聚类模型的输出结果作为伪标签反向传播到ConvNets中，并对<spanclass="math inline">\(L_{Rec}\)</span>进行优化，并对网络参数进行优化。在经过逐层贪婪训练后，通过反向传输将FE模型和Rec模型的所有层连接起来，构建深度联合训练模型。然后对联合模型进行微调，设置一个高阈值和两个权衡参数，以最小化重构信息的损失。Rec模型保留了伪标签生成分布的局部结构，避免了瓶颈空间的破坏。该深度聚类模型可以迭代地学习输入数据的特征，并对其进行划分。<br/>​  此外，我们计算特征之间的相似性，并选择高置信度的生成的标签特征，通过联合框架反馈FE和Rec模型的训练。在Rec模型中，我们设置了一个高阈值来确定是否应该将一些图像伪标签合并到特征映射中。如果两个标签之间的相似性大于高阈值，并且我们将这种类型的标签分组属于同一个聚类。反馈的反向传播FE和Rec旨在学习基于卷积神经网络的深度映射函数g，该函数由θ参数化。通过模糊聚类模型最小化Rec的损失函数来更新联合框架的参数<span class="math display">\[L_{F_{-}\mathrm{clu}}&amp;=&amp;\ell\[||X_{i}-g_{\theta}(f_{\mu}(x_{j}))||_{2}^{2}]+H(W)\\&amp;=&amp;\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\left\|X_{i}-g_{\theta}\bigl(f_{\mu}\bigl(x_{j}\bigr)\bigr)\right\|_{2}^{2}+H(W)\]</span>​  式中，M为数据集中的数据样本数，j为第j个簇，k为簇分配的数量。<br/>​  在模糊聚类的损失函数中，我们考虑了一个庞大而复杂的数据集的权值在瓶颈空间中被划分为聚类，它也代表了该数据特征映射在形成聚类中的贡献概率。我们进一步改进了加权自适应损失函数的模糊聚类，增加加权熵，刺激更多的特征有助于聚类识别。这样，DAFC模型即使使用加权自适应熵，也可以直接进行深度端到端训练，这种学习到的层次表示被证明对深度聚类任务是有效的。</p><h3id="定义2深度聚类的加权自适应熵44">定义2：深度聚类的加权自适应熵[44]。</h3><p>​  假设聚类的权值信息为H (w)，并同时设置最优模糊隶属度和<spanclass="math inline">\(w_{i j}\)</span>的最优权值。H (w)的计算方法如下：<spanclass="math display">\[H(w)=\lambda_{1}\Biggl(1-\sum_{j=1}^{k}\mu_{ij}\Biggr)+\lambda_{2}\sum_{i=1}^{M}\sum_{j=1}^{k}w_{i j}\log w_{ij}\]</span> ​  其中，<spanclass="math inline">\(\lambda_{1}\)</span>是一个权衡参数，控制分配给各种类型异常值的模糊鲁棒性，<spanclass="math inline">\(\lambda_{2}\)</span>也是一个权衡参数，控制模糊隶属度的簇分布。DAFC的目标函数是<span class="math display">\[\mathrm{min}\, L=L_{\mathrm{Rec}}+L_{F_{-}\mathrm{clu}}\]</span> ​  可改写如下： <spanclass="math display">\[\begin{array}{ll}\operatorname*{min}L&amp;=&amp;\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\cdot\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\\&amp;&amp;+\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\cdot\ell\,\left\|X_{i}\,-\,g_{\theta}\big(f_{\mu}\big(x_{j}\big)\big)\right\|^{2}\\&amp;&amp;+H(W)\\&amp;{\mathrm{s.t.}}&amp;~\mu_{ij}\in\{0,1\}\\&amp;&amp;\sum_{j=1}^{k}\mu_{i j}=1,\quadi=1,\dots,M\end{array}\]</span> ​  考虑一组M图像<spanclass="math inline">\(\{x_{1},\cdot\cdot\cdot,x_{M}\}^{k}\)</span>聚类到瓶颈空间的k个聚类，每个图像<spanclass="math inline">\(x_i\)</span>与<spanclass="math inline">\(\{0,1\}^k\)</span>中的标签<spanclass="math inline">\(y_i\)</span>相关联。我们还使用模糊隶属度µ表示图像到k个可能的预定义簇之一的概率。输入数据x的瓶颈空间Z(x)离质心<spanclass="math inline">\(c_{j}=g_{\theta}(f_{\mu}(x_{j}))\)</span>越近，属于聚类j的x的模糊隶属度就越高。µ是模糊隶属度，m是拉格朗日乘子。然后从这个目标函数中推导出µ如下：<span class="math display">\[{\frac{\partialL}{\partial\mu}}=0\Rightarrow\mu=\left({\frac{\lambda_{1}}{m\cdot\eta}}\right)^{\frac{1}{m-1}}\]</span>​  其中<spanclass="math inline">\(\eta=\ell[\|X_{i}-g_{\theta}(f_{\mu}(x_{j}))\|_{2}^{2}]\)</span>。<br/>​  现在我们考虑函数L的导数与在新的步骤中的权重。给定度量µ和cj是固定的，L为(8)中的最优权重w最小化<span class="math display">\[w_{ij}=\exp\left[2\sum_{j=1}^{k}g_{\theta}{\big(}f_{\mu}(x_{j}){\big)}-1\right]\]</span>​  自适应熵的加入允许聚类用一个更快的迭代算法来解决分割问题。<br/>​  总之，我们引入了一个用于进化无监督表示学习的DAFC。在算法1中描述了DAFC的伪代码。在每次迭代过程中，深度模糊聚类通过固定的深度FE模型和Rec模型交替选择相似和不同的样本组，并根据所选的样本组对其进行训练。具体步骤见算法1。</p><hr /><p>算法1：深度自适应模糊聚类（DAFC）</p><hr /><p><strong>输入：</strong>数据集<spanclass="math inline">\(X=\{x_{i}\}_{i=1}^{k}\)</span>，平衡系数<spanclass="math inline">\(\lambda_{1}\)</span>、<spanclass="math inline">\(\lambda_{2}\)</span>，高阈值<spanclass="math inline">\(\epsilon_r\)</span><br/><strong>输出：</strong>深度聚类结果R和Test_Err<br/>1随机初始化θ；<br/>2 用pre_train FE模型和Rec模型初始化C、G和F；<br/>3<span class="math inline">\(\{A c c,A R I,N M I\}=R\)</span>；<br/>4重复；<br/>5 <strong>for</strong> <em>epoch</em> ∈ 0<em>,</em> 1<em>, .. . ,</em> <em>MaxEpochs</em> <strong>do</strong><br/>6 |生成深度ConvNets表示G；<br/>7 | <strong>for</strong> <em>iter</em> ∈0<em>,</em> 1<em>, . . . ,</em> <em>MaxIter</em><strong>do</strong><br/>8 | | float Max <spanclass="math inline">\(\mu_{i j}\)</span> = 1；<br/>9 | | <spanclass="math inline">\(f_{\mu}(x)\neq n u l l\)</span>；<br/>10| |计算相似度矩阵<spanclass="math inline">\(C_{i,j}(\cdot)=\frac{a_{i}^{T}a_{j}}{\|a_{i}\|}\=\frac{a_{i}^{T}a_{j}}{\sqrt{d_{i}}\sqrt{d_{j}}}\)</span>；<br/> | |模糊隶属度=<spanclass="math inline">\(f_{\mu}\left(y_{i},x_{i}\right)=C_{i,j}\left(x_{i}\right)\cdoty_{i}\)</span><br/>11| | 计算<spanclass="math inline">\(L_{Rec}=\operatorname*{min}_{w,\theta}\sum_{i=1}^{M}\frac{1}{M}\ell\left[g_{\theta}\left(f_{w}\left(x_{i}\right)\right),f_{\mu}\left(y_{i},x_{i}\right)\right]\)</span><br/>| | 计算聚类的权值信息<spanclass="math inline">\(H(w)=\lambda_{1}\Biggl(1-\sum_{j=1}^{k}\mu_{ij}\Biggr)+\lambda_{2}\sum_{i=1}^{M}\sum_{j=1}^{k}w_{i j}\log w_{ij}\)</span><br/> | | 计算<spanclass="math inline">\(L_{F_{-}\mathrm{clu}}=\operatorname*{min}_{\theta,\mu,m}\sum_{i=1}^{M}\sum_{j=1}^{k}\mu_{ij}^{m}\left\|X_{i}-g_{\theta}\bigl(f_{\mu}\bigl(x_{j}\bigr)\bigr)\right\|_{2}^{2}+H(W)\)</span><br/>12|| 通过最小化(7)加上L Rec和L F_clu；<br/>13| | 通过最小化来更新<spanclass="math inline">\(\mu_{i j}\)</span>(9)；<br/>14| |<strong>while</strong> <em>not end of R</em> <strong>do</strong><br/>15|| | 计算设计的convnet的精度（1 - test_error）；<br/>16| |根据（10）的要求更新<span class="math inline">\(w_{ij}\)</span>；<br/>17| | 在DAFC中向FE和Rec进行反向传播；<br/>18| |直到<span class="math inline">\(\{A c c,A R I,N MI\}=MaxR\)</span>；<br/>19| | <strong>end</strong>;<br/>20|<strong>end</strong>;<br/>21 return *R**,* <em>Test</em>_<em>Err</em>;</p><hr /><h1 id="iv.实验">IV.实验</h1><h2 id="a.实验设置">A.实验设置</h2><p>​  然后，我们与IV-d中最新的深度聚类模型和传统的聚类方法进行了全面的比较。这些实验都是在cudnnv7和RTX 2080 TiGPU（NVIDIA）中实现的，以测量不同深度聚类方法的性能和效率。我们考虑相同的批处理大小和相同的时代数量，以及以毫秒为单位度量推理时间。当对街景房子号（SVHN）和STL-10数据集进行训练时，我们需要注意计算所需的内存量，如果训练要在所需的时间范围内完成，也需要仔细平衡内核大小和过滤器大小。对于所有的卷积层，我们设置了不同的通道数（64、128、256和512）和滤波器大小（2×2和3×3）、步幅=1和填充=1。对于池化层，我们在最大池中设置内核大小=2和步幅=2，在平均池中设置内核大小=4和步幅=4。每一层都经过了2000次迭代的预训练，退出率为50%。我们在几个基准测试上研究了这些算法。据我们所知，我们为每个轮次设置了最大的batch_size。<br/>​  本文采用了三种优化器作为优化算法。我们已经验证了当使用Adam和RMSprop优化器时的性能。与Adam和SGD相比，我们发现Adam患有通常的平衬里问题，因此给出了一个糟糕的解决方案。SGD被发现需要数千次迭代来收敛，并且也给出了一个很差的解决方案。RMSprop为在选定的复杂数据集上训练深度聚类框架提供了更精确的解决方案。此外，我们还设置了学习率（lr）=1×10−4。为了公平地比较和清楚地说明DAFC的有效性，每种比较方法的训练期数和学习率都保持在相同的数量。我们实现了我们的方法和比较方法，并给出平均结果，以防止随机情况。<br/>​  在相同的实验环境和设置下，我们还使用最优参数训练了深度自适应聚类（DAC）和深度综合相关挖掘（DCCM）。自适应参数λ被初始化为0，并在DAC中使用具有学习率lr= 0.009的RMSprop。DCCM采用带有lr = 1e-4的RMSprop优化器，设置α = 5和β =0.1，采用3个1×1卷积层的网络进行互信息估计鉴别器。</p><h2 id="b.评价指标">B.评价指标</h2><h2 id="c.-数据集">C. 数据集</h2><h2 id="d.-baselines">D. Baselines</h2>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Fuzzy_Machine_Learning</title>
      <link href="/Fuzzy-Machine-Learning/"/>
      <url>/Fuzzy-Machine-Learning/</url>
      
        <content type="html"><![CDATA[<p>Fuzzy Machine Learning: A Comprehensive Framework and SystematicReview</p><h1 id="摘要">摘要</h1><p>​  机器学习的力量来自各种学科，包括计算机科学、认知科学和统计学。虽然机器学习在理论和实践上都取得了很大的进步，但其方法在处理复杂情况和高度不确定的环境时存在一定的局限性。数据的不足、不精确的观察和模糊的信息/关系都会混淆传统的机器学习系统。为了解决这些问题，研究人员从不同的方面整合了机器学习和模糊技术，包括模糊集、模糊系统、模糊逻辑、模糊测度、模糊关系等。本文从模糊机器学习，从理论到应用方法，总体目标是概述模糊机器学习领域的最新进展。为此，将所讨论的概念和框架分为五类：1）模糊经典机器学习；2）模糊迁移学习；3）模糊数据流学习；4）模糊强化学习；和5）模糊推荐系统。所提供的文献将使研究人员深入了解模糊机器学习研究的进展及其应用。</p><h1 id="i.介绍">I.介绍</h1><p>​  在动态的技术领域，机器学习已经深刻地改变了各个领域。它通过解码复杂的数据模式，推动人工智能的进步，并影响我们如何参与信息和理解计算系统的能力，以此来引领创新。然而，在大多数现有的机器学习方法中，具有不确定性的场景的准确性会受到影响，例如唯一可用的观测是不精确的或数据是有噪声或不完整的。此外，许多真实世界的数据集包含不确定的关系，传统的机器学习方法通常发现很难识别或处理这些结构。为了解决这些问题，研究人员已经使用模糊技术集成到机器学习中，称为模糊机器学习（FML）[1]作为一种解决方案，因为模糊技术能够成功地处理不确定性。FML系统融合机器学习算法与模糊技术，如模糊集[2]，模糊系统[3]，模糊聚类[4]，模糊关系[5]，模糊措施[6]，模糊匹配[7]，模糊优化[8]等等，建立新的模型更健壮的许多和各种类型的不确定性在现实世界的问题。<br/>​  FML在复杂和动态（不确定）的环境中是一个宝贵的盟友，展示了提高其功效的实质性优势。与传统的机器学习方法不同，通常基于模糊集[9]和模糊理论[10]的模糊技术擅长捕捉和导航动态场景中固有的细微不确定性。它们建模不确定性的内在能力使其能够优雅地适应动态环境中不断变化的模式。在传统模型可能会动摇或难以跟上步伐的情况下，模糊技术作为鲁棒的问题解决者出现，提供了真实数据[11]中固有模糊性的更准确的表示。</p><ol type="1"><li>模糊集[2]可以用来表示模糊或模糊的概念和数据，例如在语言变量、噪声或不完整数据和区间值数据中常见的数据。模糊集增强了算法在不确定和复杂情况下做出决策的能力，这在现实世界条件可能不可预测的应用中特别有用，如机器人或自动驾驶汽车。<br/>2.基于模糊规则的系统[3]可以提供一个透明和可解释的预测框架。基于模糊规则的系统使用语言规则来表示知识，因此，可以用来对系统所做出的决策进行解释。这在医疗诊断等应用程序中很有用。<br/>3.模糊聚类[4]是一种著名的聚类方法，它可以通过识别传统聚类方法不易识别的数据模式来改进机器学习算法。模糊集群不仅允许重叠的集群，而且还可以确定地处理可能不属于任何特定集群的数据点。这在图像识别等应用程序中很有用。<br/>4.模糊关系[5]可以提供变量或数据点之间关系的更灵活和微妙的表示。它们还可以捕获非线性关系，以使更准确和更有表现力的机器学习模型成为可能。此外，模糊关系在处理多模态数据或从多个来源组装的数据时是有用的，因为研究人员可以定义不同模态之间的模糊关系，从而得到一个更全面和准确的模型。</li></ol><p>​  在过去的十年里，在高质量的期刊和会议论文集上，已经有超过50万篇包含“模糊”和“机器学习”字样的文章。然而，这些文章都没有提供关于最近关于FML的文献的全面综述。以前在该地区进行的几次调查只对FML的某些子领域提供了有价值的见解。例如，Baraldi和Blonda[12]提供了模糊聚类的模式识别算法，而Skrjanc等人。[13]总结了基于进化模糊规则和神经模糊网络（NFNs）的模型，用于聚类、回归、识别和分类问题。此外，Zheng等人[14]回顾了最近在将深度学习模型与模糊系统融合方面的工作。此外，近十年来，在FML中出现了新的子领域，如模糊迁移学习和模糊数据流学习。提供一份调查报告来概述这些新的子领域是很重要的。由于这些原因，有必要进行一个新的、更全面的、更最新的FML调查。本文主要针对利用模糊技术提高机器学习方法性能的研究人员，特别是在涉及复杂或不确定因素的情况下。<br/>​  本研究中纳入的研究分为以下三个步骤。<br/>​  步骤1)确定并确定要搜索的适当的发布数据库集。我们搜索了科学直接数据库、ACM数字图书馆数据库、IEEEXplore数据库和SpringerLink数据库等著名的数据库。这些研究提供了关于机器学习和FML的研究论文的全面参考书目。<br/>​  步骤2)文章的初步筛选：第一次搜索是基于关键词。这些文章：a)提出了FML领域的新理论、算法或方法；或b)报告了一个围绕FML算法构建的应用程序。<br/>​  步骤3）对陈述结果进行过滤：将步骤2选择的文章分为五组，分为分为部分：a）模糊经典机器学习；b）模糊迁移学习；c）模糊数据流学习；d）模糊强化学习（RL）；e）模糊推荐系统。此时，我们对文章进行了最后的筛选（见图1）。如果一项研究证明足够，则保留：a)新颖性，即在过去十年内发表；b)影响，即它发表在高质量的期刊/会议上或高被引用。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223114336678.png"alt="image-20241223114336678" /><figcaption aria-hidden="true">image-20241223114336678</figcaption></figure><p>​  本文的主要贡献如下。</p><ol type="1"><li>全面总结了FML领域的发展和成果。这一领域的工作主要分为五大类进行讨论。<br/>2.本文分析了现实场景中传统机器学习方法的缺点，然后解释了FML如何被用于解决这些问题。所提供的见解旨在帮助研究人员了解FML研究及其应用的发展背景。<br/>3.它提供了一个对最先进的（SOTA）FML模型的批判性的讨论，并概述了未来研究的方向。</li></ol><h1 id="ii.fml的基本概念">II.FML的基本概念</h1><p>​  在本节中，我们将简要介绍一些相关的数学概念，来说明如何将模糊逻辑集成到迁移学习、数据流学习、RL和推荐系统中。这些概念应该可以帮助研究人员更好地理解以下章节中介绍的文章。</p><h2 id="a.模糊迁移学习">A.模糊迁移学习</h2><p>​  迁移学习[15]试图通过利用来自另一个领域（源）的知识来训练在一个领域（目标）中表现良好的模型，该领域与前一个领域具有不同的分布或学习任务。本节介绍了两个具有代表性的模糊迁移学习框架：1)基于模糊规则的[16]和2)基于模糊等价的[17]。</p><h3id="基于模糊规则的迁移学习框架16">1)基于模糊规则的迁移学习框架[16]：</h3><p>​  设<spanclass="math inline">\(\mathcal{S}=\{S^{1},S^{2},\cdot\cdot\cdot,\mathcal{S}^{N}\}\)</span>表示一组源域，其中<spanclass="math inline">\({\cal S}^{n}=\{({\bfx}_{i}^{\mathrm{S}_{n}},{y}_{i}^{\mathrm{S}_{n}})|{\bfx}_{i}^{\mathrm{S}_{n}}\in\chi^{n},{y}_{i}^{\mathrm{s}_{n}}\in\mathcal{Y}\}_{i=1}^{m_{n}}\)</span>，<spanclass="math inline">\(n\in [N]\)</span>和<spanclass="math inline">\(({\bfx}_{i}^{\mathrm{S}_{n}},{y}_{i}^{\mathrm{S}_{n}})\)</span>是n源域的第一对输入输出数据对。这里，<spanclass="math inline">\(\chi^{n}\subset\mathbb{R}^{p}\)</span>表示每个源域的特征空间，<spanclass="math inline">\(\mathcal{Y}\)</span>是一个响应空间（<spanclass="math inline">\(\mathcal{Y} =\{1,2，...，K\}\)</span>给定一个分类任务，<spanclass="math inline">\(\mathcal{Y}\subset\mathbb{R}\)</span>给定一个回归任务）。<spanclass="math inline">\(\mathcal{T}=\{\bf x_{i}^{T}|{\bfx}_{i}^{T}\in\chi^{T}\}_{i=1}^{m_{t}}\)</span>是未标记的目标域（对于无监督的场景），其中<spanclass="math inline">\(\chi^{T}\subset\mathbb{R}^{p}\)</span>是目标域的特征空间。在同质情况下，<spanclass="math inline">\(X^1\)</span>，...，<spanclass="math inline">\(X^N\)</span>和<spanclass="math inline">\(X^T\)</span>具有相同数量的特征，而在异构情况下，它们包含不同数量的特征。<br/>​  我们将<spanclass="math inline">\(\mathcal{R}=\{\mathcal{R}^{1},\mathcal{R}^{2},\dots,\mathcal{R}^{N}\}\)</span>表示为S构造的模糊规则空间，其中<spanclass="math inline">\(\mathcal{R}=\{r(v_{l}^{S_{n}},a_{l}^{S_{n}})\}_{l=1}^{l_{n}},n\in[N]\)</span>是<spanclass="math inline">\(\mathcal{S}^n\)</span>的第n个规则集。这里，规则<spanclass="math inline">\(r(v_{l}^{S_{n}},a_{l}^{S_{n}})\)</span>表示为<spanclass="math display">\[\begin{array}{l}{\mathrm{if~x}_{i}^{S_{n}\mathrm{~is~}A_{l}({\bfx}_{i}^{S_{n}},v_{l}^{S_{n}}),}}\\{\mathrm{then~}y_{i}^{S_{n}\mathrm{~is~}P_{l}({\bfx}_{i}^{S_{n}},a_{l}^{S_{n}}),}}\\ {l=1,2,\ldots..\cdotl_{n}.}\end{array}\]</span> ​  其中<spanclass="math inline">\(\mathcal{R}^T\)</span>表示得到的目标域<spanclass="math inline">\(\mathcal{T}\)</span>的模糊规则。<br/>​  最后，将<spanclass="math inline">\(\Phi=\{\Phi^{1},\Phi^{2},\dots,\Phi^{N}\}\)</span>表示为<spanclass="math inline">\(\mathcal{R}\)</span>（例如，线性组合）的结果，其中<spanclass="math inline">\(\Phi^{n}(\mathcal{R}^{n},\mu_{n}),n\in[N]\)</span>是<spanclass="math inline">\(\mathcal{R}^n\)</span>的第n个结果。因此，基于模糊规则的迁移学习的目的是利用<spanclass="math inline">\(\mathbf{D}=\{\mathcal{S},\mathcal{R},\Phi\}\)</span>的知识拟合目标域的数据，即得到<spanclass="math inline">\(\mathcal{R}^T\)</span>和<spanclass="math inline">\(\mathcal{R}^T\)</span>的结果。</p><h3id="基于模糊等价的迁移学习框架17">2)基于模糊等价的迁移学习框架[17]：</h3><p>​  与基于模糊规则的迁移学习不同，该框架应用源域和目标域特征之间的模糊等价关系来代替模糊规则。设<spanclass="math inline">\(\mathcal{U}=\{\mathcal{U}^{1},\mathcal{U}^{2},\dots,\mathcal{U}^{N}\}\)</span>表示<spanclass="math inline">\(\mathcal{S}\)</span>中特征的隶属函数空间，其中<spanclass="math inline">\(\mathcal{U}^{n}=\{\mu_{1}^{S_{n}},\mu_{2}^{S_{n}},\ldots,\mu_{m_n}^{S_{n}}\}\)</span>，<spanclass="math display">\[n\in[N]\]</span>，<spanclass="math inline">\(\mu_{i}^{S_{n}},i\in[m_{n}]\)</span>为<spanclass="math inline">\(\mathbf{x}_{i}^{S_{n}}\)</span>的隶属函数。</p><p><spanclass="math inline">\(\mathbf{R}_{S}^{M}=\{\mathbf{R}_{1}^{M},\mathbf{R}_{2}^{M},\dots,\mathbf{R}_{N}^{M}\}\)</span>表示为<spanclass="math inline">\(\mathcal{S}\)</span>上的模糊等价关系空间，其中，<spanclass="math inline">\(\mathbf{R}_{n}^{M},n\in[N]\)</span>，为<spanclass="math inline">\(S^n\)</span>上的模糊等价关系。这里，<spanclass="math inline">\(\mathbf{R}_{n}^{M}\)</span>是一个<spanclass="math inline">\(m_n\times m_n\)</span>矩阵（详见[17]和[18]） <spanclass="math display">\[({\bf R}_{n}^{M})_{i j}={\bf R}_{S_{n}}({\bfx}_{i}^{S_{n}},{\bfx}_{j}^{S_{n}};\mu_{i}^{S_{n}},\mu_{j}^{S_{n}}),\quadi,j\in[m_{n}]\]</span> ​  其中，<spanclass="math inline">\(R_{S_n}\)</span>是<spanclass="math inline">\(S_n\)</span>上的一个模糊等价关系算子。<br/>​  因此，基于模糊等价的迁移学习框架旨在利用从<spanclass="math inline">\(\mathbf{D}=\{\mathcal{S},\mathcal{U},\mathbf{R}_{S}^{M}\}\)</span>中获得的知识来拟合目标域内的数据。</p><h2 id="b.模糊数据流学习">B.模糊数据流学习</h2><p>​  数据流学习[19]，[20]，也被称为流挖掘，指的是一组技术和算法，旨在处理和分析以流的方式持续到达的数据。然而，在现实世界的场景中，数据的统计属性可能会随着时间的推移而变化，这使得以前精确的模型和算法会随着时间的推移而失效。这种现象被称为概念漂移[21]，[22]，[23]。下面是概念漂移的正式定义。</p><p>​  定义1（概念漂移[23]）：考虑一个时间段[0，t]和一组样本，记为<spanclass="math inline">\(S_{0,t} = \{d_0，...，d_t\}\)</span>，其中<spanclass="math inline">\(d_i=(X_i，y_i)\)</span>是一个观察结果（或一个数据实例）。<spanclass="math inline">\(X_i\)</span>是特征向量，<spanclass="math inline">\(y_i\)</span>是标签，<spanclass="math inline">\(S_{0,t}\)</span>遵循一定的分布<spanclass="math inline">\(\mathbb{F}_{0,t}(X,y)\)</span>。当<spanclass="math inline">\(\mathbb{R}_{0,t}(X,y)\neq\mathbb{R}_{t+1,\infty}(X,y)\)</span>时，在t+1时刻发生概念漂移，记为<spanclass="math inline">\(\existst:\mathbb{P}_{t}(X,y)\neq\mathbb{P}_{t+1}(X,y)\)</span>。<br/>​  因此，当概念漂移发生在t+ 1时，我们的目标是调整预测<spanclass="math inline">\(H_{t}=\arg\operatorname*{min}_{h\in\calH}\ell(h,X,y|(X,y)\in\mathbb{P}_{t}(X,y))\)</span>以适应新的分布Pt+1（X，y）。接下来，我们简要介绍了一个基于模糊聚类的漂移学习结构[24]，来展示如何将模糊逻辑集成到数据流学习中。</p><h2 id="c.-模糊强化学习">C. 模糊强化学习</h2><p>​  RL[27]是在学习者（称为代理）主动与环境交互以实现特定目标的场景下进行计划和学习的研究。代理的目标是制定积累奖励的最佳策略。它通过从它接收到的反馈中学习来做到这一点。RL已经成功地应用于各种现实世界的问题，如机器人控制[28]、游戏玩[29]和自动驾驶[30]。在本节中，我们将提供关于如何将模糊逻辑集成到RL中的信息。<br/>​  首先，模糊集可以用来表示RL中的状态、行动或奖励空间的不确定性。例如，模糊奖励信号[31]表示代理所接收到的奖励的不确定性或不精确性。此外，使用模糊逻辑将输入映射到控制动作的模糊控制器[32]可以集成到RL系统中，以处理不确定的或定性的控制决策。接下来，我们给出了一个模糊控制器的一般数学表达式。设X1，...，Xn为模糊控制器的输入变量，Y为表示控制动作的输出变量。与每个变量相关的模糊集表示为A1，...，An表示输入，B表示输出。设μAi（xi）表示输入Xi的模糊集Ai的隶属度函数，设μB(y)表示输出Y的模糊集B的隶属度函数。然后，定义从输入到输出的映射的通用模糊规则可以表示为<spanclass="math display">\[{\mathrm{if}}\,X_{i}\,{\mathrm{i}}\,{\mathrm{k}}\,A_{1j}\,{\mathrm{~and}}\cdot\cdot\cdot\mathrm{and}\,X_{n}\,{\mathrm{is}}\,A_{nj}\,,{\mathrm{then}}\,Y\,{\mathrm{is}}\,B_{j}\]</span>​  应用模糊规则后，进行去模糊化，得到一个清晰的输出值。<br/>​  此外，模糊推理系统可以用于RL[33]中的决策，例如基于代表不确定状态或奖励的模糊输入信号来确定下一步要采取的行动。例如，模糊q学习[34]扩展了q学习，通过结合模糊逻辑来处理不确定和不精确的状态-动作对。采用模糊规则和隶属度函数对q值进行了更新。</p><h2 id="d.-模糊重组系统">D. 模糊重组系统</h2><h1 id="iii.模糊经典机器学习">III.模糊经典机器学习</h1><p>​  经典的机器学习算法，如决策树、支持向量机（SVMs）和神经网络，在理论上和实践角度都取得了显著的成就。许多文章涉及到结合模糊技术与经典的机器学习算法，以克服不同类型的不确定性问题，如不完整的信息和不精确的观察。在本节中，我们总结了这些工作，并将这些技术分为两类：1)基于非深度学习的方法和2)基于深度学习的方法。</p><h2 id="a.-基于非深度学习的方法">A. 基于非深度学习的方法</h2><h2 id="b.-基于深度学习的方法">B. 基于深度学习的方法</h2><p>​  表一提供了与使用fnn的深度学习相关的SOTA文献的摘要。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223163949938.png"alt="image-20241223163949938" /><figcaption aria-hidden="true">image-20241223163949938</figcaption></figure><h1 id="iv.-模糊迁移学习">IV. 模糊迁移学习</h1><p>​  值得注意的是，大多数当前的迁移学习[158]方法在处理具有不确定性的真实情况时都有局限性，例如当只有少数标记实例可用时。为了克服这些问题，许多研究者已经转向了模糊集和模糊逻辑。<br/>​  现有的关于迁移学习的研究可以根据正在转移的知识类型进行分类。这些知识类别包括实例[159]、特征表示[160]、模型参数[161]和关系知识[162]。另外，根据所解决的问题设置，研究可以分为四类：多任务学习[163]、领域适应[164]、[165]、跨领域适应[166]和异构学习[167]。基于所使用的模糊技术，我们将我们最近的工作（2015-2023年）的总结分为三个领域。这些都是模糊集、模糊系统和模糊关系。表二总结了模糊迁移学习领域的最新成果。</p><figure><imgsrc="../postimages/Fuzzy-Machine-Learning/image-20241223163902862.png"alt="image-20241223163902862" /><figcaption aria-hidden="true">image-20241223163902862</figcaption></figure><h2 id="a.基于模糊集的迁移学习">A.基于模糊集的迁移学习</h2><h2 id="b.基于模糊系统的迁移学习">B.基于模糊系统的迁移学习</h2><h2 id="c.-基于模糊关系的迁移学习">C. 基于模糊关系的迁移学习</h2><h1 id="v.-模糊数据流学习">V. 模糊数据流学习</h1><h1 id="vi.-模糊强化学习">VI. 模糊强化学习</h1><h1 id="vii.-模糊推荐系统">VII. 模糊推荐系统</h1><h1 id="viii.-未来研究方向">VIII. 未来研究方向</h1><h2 id="a.模糊经典机器学习">A.模糊经典机器学习</h2><h2 id="b.模糊转移学习">B.模糊转移学习</h2><h2 id="c.模糊数据流学习">C.模糊数据流学习</h2><h2 id="d.模糊强化学习">D.模糊强化学习</h2><h2 id="e.模糊重组系统">E.模糊重组系统</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A survey on deep learning-based image forgery detection</title>
      <link href="/A-survey-on-deep-learning-based-image-forgery-detection/"/>
      <url>/A-survey-on-deep-learning-based-image-forgery-detection/</url>
      
        <content type="html"><![CDATA[<p>A survey on deep learning-based image forgery detection</p><p>Fatemeh Zare Mehrjardi <span class="math inline">\(^a\)</span> , AliMohammad Latif <span class="math inline">\(^{*,a}\)</span> , MohsenSardari Zarchi <span class="math inline">\(^b\)</span> , RaziehSheikhpour $^c $ <br/>a Computer Engineering Department, YazdUniversity, Yazd, Iran <br/>b Computer Engineering Department, MeybodUniversity, Meybod, Yazd, Iran <br/>c Department of ComputerEngineering, Faculty of Engineering, Ardakan University, PO Box 184,Ardakan, Iran</p><h1 id="摘要">摘要</h1><p>​  图像被称为人类之间的交流工具之一。随着相机和手机等数字设备的发展和普及，在任何地方拍照都变得容易。图像被用于许多医学、法医学和司法应用中。有时图像被用作证据，所以数字图像的真实性和可靠性越来越重要。有些人通过添加或删除图像的部分来操作图像，这使图像无效。因此，图像伪造的检测和定位是很重要的。图像编辑工具的发展使这个问题成为计算机视觉领域的一个重要问题。近年来，许多不同的算法来检测图像和像素级的伪造。这些算法主要分为传统方法和深度学习方法。深度学习方法是人工智能科学的重要分支之一。该方法由于具有自动识别和预测过程，以及对几何变换和后处理操作的鲁棒性，已成为大多数计算机视觉问题中最流行的方法之一。本文综合研究了图像伪造类型、基准数据集、伪造检测中的评价指标、传统伪造检测方法、发现传统方法的缺点和局限性、深度学习方法的伪造检测方法以及该方法的性能。根据深度学习方法的扩展及其在大多数计算机视觉问题上的成功表现，我们在本研究中的主要重点是基于深度学习方法的伪造检测。本调查有助于研究人员获得伪造检测领域的深层背景。</p><h1 id="介绍">1.介绍</h1><p>​  数字图像可以通过各种技术来伪造。现有的数字图像伪造检测技术大致可分为两大类：主动/非盲法和被动/盲法[6–8]。这些类别及其子类别如图1所示。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218144724434.png"alt="image-20241218144724434" /><figcaption aria-hidden="true">image-20241218144724434</figcaption></figure><ul><li>主动方法：在主动方法中，对图像进行预处理，并将信息嵌入到原始图像中。一些主动方法的例子包括数字水印和数字签名。这种方法需要特殊的软件、硬件和原始图像来输入信息或从图像中提取信息。如果没有这些要求，这种方法将是不可能的和无效的。此外，在这种方法中，如果先决条件是[9]的，伪造检测就很容易了。</li><li>被动方法：在被动方法中，即通常被称为盲法，不需要对图像进行预处理。被动方法检测图像是否伪造，并通过分析图像的内容和结构来发现不一致性。这种方法比主动方法更受青睐，因为不需要任何先验信息[9]。被动方法的一些例子包括复制移动伪造、图像拼接、图像润饰和物体去除。</li></ul><h2 id="被动处理方法的类型">1.1.被动处理方法的类型</h2><p>​  本调查主要关注下面所描述的被动方法类型。</p><ul><li><p>​  移动伪造是最简单、最常见的图像处理方法之一。这种类型的伪造包括复制图像的一个或多个部分，并将其粘贴到同一图像的其他位置。该方法的目的主要是隐藏一些重要的信息或在图像中插入一些错误的信息。由于被复制的部分属于同一图像，因此在结构和纹理上与整个图像是兼容的。</p><p>​  除了复制和粘贴之外，还可以对图像执行几何变换，如旋转、缩放和后处理操作，如模糊、改变亮度、压缩和添加噪声。这些转换和操作导致被操纵的区域不容易被人类的[10,11]识别。图2显示了一个复制移动伪造图像的例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218144927743.png"alt="image-20241218144927743" /><figcaption aria-hidden="true">image-20241218144927743</figcaption></figure></li><li><p>​  图像拼接是一种将两个或多个图像组合成一个图像的方法。在生成的图像中，在拼接的位置有边缘和模糊区域。使用图像编辑工具，这些区域可以与图像合并，使人类视觉不会检测到伪造的[6,12]。图3为一个图像拼接的例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145015606.png"alt="image-20241218145015606" /><figcaption aria-hidden="true">image-20241218145015606</figcaption></figure></li><li><p>​  图像修饰是其他图像伪造方法中危害最小的伪造方法。该方法增强或减少了图像[13]上的像素特征。此外，它也是一种流行的照片编辑应用程序和杂志[2]的方法。这种方法通过增加或减少像素颜色等某些特征，使图像更具吸引力。图4显示了图像修饰的一个例子。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145056515.png"alt="image-20241218145056515" /><figcaption aria-hidden="true">image-20241218145056515</figcaption></figure></li><li><p>​  对象删除被称为破坏性伪造，因为它可能会改变图像的语义内容。[14]有两类对象删除技术：复制-移动和图像插入绘画。复制移动通过从主图像或另一个图像复制一个区域并粘贴到被删除对象的区域来删除所需的对象。复制移动由于其简单性，被广泛用于对象删除。</p><p>​  图像中的绘制最初是为了恢复受损的信息和去除旧照片中的划痕。此方法通过用周围的像素填充对象的位置来删除对象。图像内画可以同时保持纹理和结构的一致性。图像插入图例如图5所示。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145218284.png"alt="image-20241218145218284" /><figcaption aria-hidden="true">image-20241218145218284</figcaption></figure></li></ul><p>​  伪造检测是近十年来研究人员最有趣的课题之一。因此，许多研究主要分为两类：传统学习方法和深度学习方法。在[2,16]中可以找到关于这两种方法的一些优秀的综述。尽管在伪造检测领域有适当的综述文章，但我们的综述文章的目的是对两种伪造检测方法进行综合检查，以比较和发现它们的优缺点和挑战。另一方面，根据深度学习方法表现良好在大多数问题如果硬件条件和适当的数据集，我们文章的另一个重点是检查伪造检测深度学习从两个新的和不同的方面：使用不同的深度学习方法和不同的深度学习架构。<br/>​  本文的其余部分分为以下几个部分：第2节给出了不同的深度学习架构和用于伪造检测的各种评估指标的简要背景。第3节解释了基准数据集在复制-移动、拼接和不绘制伪造检测方面的细节。第4节对传统的伪造检测方法及其细节进行了解释和比较。第5节对具有不同策略和深度学习架构的基于深度学习的伪造检测进行了全面的回顾。最后，在第6节中给出了所有伪造检测方法的结论、其缺点和优点、存在的挑战和未来的建议。</p><h1 id="背景">2.背景</h1><p>​  目前，大多数伪造检测方法都是基于机器学习和深度学习的方法。本节概述了机器学习算法、深度学习架构和根据回顾的研究得出的评估指标。</p><h2 id="机器学习">2.1.机器学习</h2><ul><li>支持向量机（SVM）</li><li>决策树（DT）</li><li>K-最近的邻居（KNN）</li></ul><h2 id="深度学习dl">2.2.深度学习（DL）</h2><h3 id="cnn">2.2.1. CNN</h3><ul><li>卷积层</li><li>池化层</li><li>全连接层</li></ul><h3 id="rnn">2.2.2. RNN</h3><p>​  递归神经网络，也称为重复神经网络，用于序列数据处理。这些网络有一个反馈层，在其中网络输出与下一个输入一起返回到网络。递归神经网络的内部记忆回忆起它之前的输入，并使用这个记忆来处理一系列的输入。递归神经网络具有短期的内部记忆，这与长期依赖性的梯度消失问题有关。因此，不同类型的递归神经网络被引入于长期依赖问题。长短期记忆（LSTM）网络是一种著名的递归神经网络，它试图模拟一个组件和它的前身[23,24]之间的长依赖关系。</p><ul><li><p>LSTM是一种旨在解决消失梯度问题的RNN网络之一。它由四个组件组成，分别称为单元格、输入门、输出门和忘记门。该单元格是一个随时间的推移而存储和更新信息的地方。单元格内的值由三个门[24]进行更新。图7说明了LSTM的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218145635177.png"alt="image-20241218145635177" /><figcaption aria-hidden="true">image-20241218145635177</figcaption></figure><p>LSTM网络的基本概念是单元及其相关门的状态；单元是一条高速公路，沿着信息链。这个单元格可以被认为是网络内存。盖茨以更新的状态保存信息。他们决定哪些信息进入单元格，并学习在网络训练[25]期间应该存储或忘记哪些信息。</p></li></ul><h3id="区域卷积神经网络r-cnnregional-convolutional-neural-network">2.2.3.区域卷积神经网络（R-CNN，Regional convolutional neural network）</h3><p>​  图像中的物体识别和定位是计算机视觉中最基本和最具挑战性的概念之一。Girshick等人[26]提出了一种名为R-CNN的目标识别和分割系统。该系统使用多层卷积计算区分特征，对图像区域进行分类，并在识别的对象周围划定边界。图8显示了R-CNN网络的步骤。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218150308547.png"alt="image-20241218150308547" /><figcaption aria-hidden="true">image-20241218150308547</figcaption></figure><p>​  首先，在R-CNN网络中，使用选择性搜索算法识别出2000个区域。调整所有区域的大小，进入预先训练好的卷积神经网络，然后从区域中提取特征。接下来，使用这些特征向量和支持向量机分类器来识别包含对象的区域。最后，通过边界盒回归[26]来确定对象的边界。</p><h3 id="fast-r-cnn">2.2.4. Fast R-CNN</h3><p>​  R-CNN网络对图像的2000个区域运行，使得速度非常慢。Girshick[27]，改进了R-CNN算法，使神经网络应用于整个图像一次。这个网络被称为快速R-CNN。图9显示了FastR-CNN网络的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218150158867.png"alt="image-20241218150158867" /><figcaption aria-hidden="true">image-20241218150158867</figcaption></figure><p>​  如图9所示，首先将整个图像和一组目标建议给给FastR-CNN网络作为输入。该网络对整个图像进行处理，并生成一个连续的特征图。接下来，使用感兴趣区域（ROI)池化层从每个对象提议的特征映射中提取一个固定长度的特征向量。每个特征向量进入完全连接的层序列。在网络的最后，有两个并行层：一个softmax层，指定每个对象的类，和一个线性回归层，输出四个值作为每个检测对象[28]的限制边界。</p><h3 id="faster-r-cnn">2.2.5. Faster R-CNN</h3><p>​  Ren等人在2015年发布了FastR-CNN的改进版本[29]。该网络与RastR-CNN网络的主要区别是，RastR-CNN网络使用选择性搜索算法来创建感兴趣的区域，而更快的R-CNN网络使用区域建议网络（RPN）层来实现这个目的。特征映射给给RPN层，该层输出建议对象及其分数。<br/>​  在RPN层中，一个滑块窗口将应用于特征映射。该窗口包含具有k个不同形状和大小的锚点的方框。锚点被放置在整个图像中。最后，该层将所提出的对象输出为具有不同大小和形状的盒子，并将它们发送到ROI层。ROI层将所提议的盒子转换为相同的大小，并从每个锚点中提取一个特征映射。最后，将特征映射给出到一个全连接的层，每个对象的类别由一个softmax层来确定。图10显示了更快的R-CNN网络的结构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218151029473.png"alt="image-20241218151029473" /><figcaption aria-hidden="true">image-20241218151029473</figcaption></figure><h3 id="mask-r-cnn">2.2.6. Mask R-CNN</h3><p>​  2017年，Kaim等[30]人引入了一种名为MaskR-CCNN的新架构，这是一种更快的R-CNN的改进架构。在这个体系结构中，除了指定对象及其类的边界外，对象的像素还与每个边界中的其他像素分开。换句话说，在每个绑定中都会创建一个二进制掩码。这是通过在FasterR-CNN网络中添加一个新的分支来实现的，这样每个被ROI层检测到的边界都被给出给卷积层。然后，二值掩码指定对象像素的值为1，其他对象像素的值为零。图11显示了MaskR-CNN网络的结构。</p><h3 id="you-only-look-once-yolo">2.2.7. You only look once (YOLO)</h3><h3 id="单镜头探测器ssdsingle-shot-detector">2.2.8.单镜头探测器（SSD，Single shot detector）</h3><h3 id="自编码器">2.2.9. 自编码器</h3><h3 id="生成对抗网络gan">2.2.10.生成对抗网络（GAN）</h3><h2 id="评价指标">2.3.评价指标</h2><p>​  评估指标是用于描述模型或算法在基准数据集上的性能的工具。伪造检测方法的评价基于图像伪造检测和像素伪造检测[35,36]两个层次。在第一种方法中，整个图像的标签应分为两类：伪造和健康，而在第二种方法中，像素的标签应分为这两类。<br/>​  当研究人员使用没有地面真实图像的数据集时，计算图像级别的评估指标。由于缺乏地面真实图像，因此无法在像素级进行评估。使用混淆矩阵计算了两个级别的评估度量。混淆矩阵由四个组成部分组成。这些组件基于像素级别的定义如下：</p><ul><li>TP（真阳性）：伪造像素正确检测为伪造像素。</li><li>FP（假阳性）：检测到错误的真实像素为伪造像素。</li><li>FN（假阴性）：伪造的像素被错误地遗漏为伪造的像素。</li><li>TN（真阴性）：正确检测到的真实像素为真实像素。</li></ul><p>​  注：对于图像级别，这些组件是图像。在下面，一些基于像素级别的标准评估指标，如准确率（Acc）、召回率(R)、精度(P)、F1评分、假阳性率（FPR）和使用混淆矩阵的组成部分定义IoU[9,35]。</p><h1 id="基准数据集">3.基准数据集</h1><h2 id="micc">3.1. MICC</h2><h2 id="comofod">3.2. CoMoFod</h2><h3 id="casia">3.3. CASIA</h3><h2 id="coverage">3.4. COVERAGE</h2><h2 id="inpainting-datasets">3.5. Inpainting datasets</h2><h1 id="传统的伪造检测方法">4.传统的伪造检测方法</h1><p>​  传统的复制移动伪造检测（CMFD）方法主要可以分为两类：基于块的方法和基于关键点的方法。这些方法包括伪造检测的三个连续步骤：特征提取、特征匹配和伪造定位[9,58]。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218151606505.png"alt="image-20241218151606505" /><figcaption aria-hidden="true">image-20241218151606505</figcaption></figure><p>​  图16显示了两种传统的CMFD方法[59]的步骤。对每个步骤的描述如下所示。</p><ol type="1"><li><p>预处理步骤：一些伪造检测算法的第一步是预处理。在这个步骤中，执行一些操作，如将彩色图像转换为灰度、调整图像大小、降噪和转换为其他颜色空间等，即执行[59]。</p></li><li><p>两种伪造检测方法：</p><p>​  基于块的方法：在基于块的方法中，将图像分为重叠或不重叠的矩形或圆形块。在特征提取步骤中，采用不同的方法从离散余弦变换（DCT）系数、主成分分析（PCA）、奇异值分解（SVD）、定向梯度直方图（HOG）、Hu矩、局部二值模式（LBP）、泽尼克矩、极复指数变换（PCET）[60–63]等所有块中提取特征向量。</p><p>​  在特征匹配步骤中，采用排序、相关和欧氏距离的方法来适应相似的特征向量。在伪造定位步骤中，计算了匹配对之间的几何变换。这个计算有助于消除任何不匹配的对。随机样本共识（RANSAC）被广泛应用于仿射同调性的准确估计。它导致了最小的误差，并过滤了一些不匹配的对[9,64]。基于块的方法的缺点是计算复杂度高，对一些几何变换[65]的性能较差（图17）。</p><p>​  基于关键点的方法：在基于关键点的方法中，整个图像不需要被分割成块。该方法在特征提取步骤中使用角和边，每个属性由一个描述符表示。关键点的提取使用了各种算法，如尺度不变特征变换（SIFT）[67]，加速鲁棒特征（SURF）[68]，以及不分割图像的加速分段测试（FAST）[69]中的特征。</p><p>​  在特征匹配步骤中，采用了聚类、欧氏距离和最近邻域等不同的方法。如果关键点匹配，则将检测到伪造文件。在伪造定位步骤中，类似于基于块的方法中的此步骤，计算匹配对之间的几何变换，并消除任何不匹配对。与基于块的方法相比，该方法的计算量较少，且对几何变换具有鲁棒性。然而，在均匀区域的伪造检测中，识别相似的图像为伪造图像，以及根据关键点是该方法[6,70,71]存在的问题（图18）。</p></li><li><pre><code>后处理：在基于块和关键点的方法中，伪造的位置可能不能被准确地本地化。为此，在后处理步骤中，使用了诸如侵蚀和分层等形态学操作来寻找伪造的[71]的确切位置。</code></pre></li></ol><p>​  传统的伪造检测方法也有一些局限性。这些方法包括三个步骤，每个步骤都是单独完成的，并且有许多参数，必须手动调整这些参数。这些方法大多是在具有高性能的特定数据集上进行的调优，但它们不适用于其他数据集[9]。<br/>​  下面，我们回顾了一些基于块、基于关键点和混合方法的研究。这些审查的结果将在下一小节中给出。图19概述了传统的伪造检测方法和对每种方法进行的研究。</p><h2id="基于块的伪造检测的一些研究概述">4.1.基于块的伪造检测的一些研究概述</h2><h2id="基于关键点的伪造检测的研究概述">4.2.基于关键点的伪造检测的研究概述</h2><table><colgroup><col style="width: 4%" /><col style="width: 24%" /><col style="width: 21%" /><col style="width: 24%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>Year</th><th>Summary</th><th>Dataset</th><th>Performance</th><th>Metrics</th></tr></thead><tbody><tr class="odd"><td>2020 [89]</td><td>预处理：将RGB图像转换为灰度图像，并对灰度图像应用DWT算法。<br />特征提取：在2级DWT上的SURF+很活跃。<br />特征匹配：2个NN+DBSCAN。<br />后处理：RANSAC。</td><td>CoMoFod, <br />MICC-F220, <br />MICCF2000</td><td>优点：该方法对几何变换和后处理操作具有鲁棒性。它可以检测到多个伪造物。该方法采用DBSCAN聚类来减少搜索空间，减少错误匹配，并降低计算成本。<br />缺点：该方法的性能较差，在锻造区域中缩放、平滑、亮度变化过多。</td><td>R=91.24%, <br />P=95.98%, <br />FPR=9.82%, <br />TPR=96.68%</td></tr><tr class="even"><td>2020 [81]</td><td>特征提取：SIFT。<br />特征匹配：反转G2NN。<br />后处理：使用两种类似的措施，如HAC和j-链接，消除不匹配。</td><td>GRIP, FAU</td><td>优点：该方法使用聚类算法降低了时间复杂度。它在简单伪造、几何变换伪造和小规模后处理伪造等方面都具有鲁棒性。<br />缺点：该方法不稳定，对大规模伪造区影响较差。</td><td>R=99.67%, <br />P=99.79%, <br />F1=99.72%</td></tr><tr class="odd"><td>2020 [91]</td><td>特征提取：SURF+旋转局部二进制模式（RLBP）。<br />特征匹配：G2NN+欧氏距离，层次聚类。<br />后处理：RANSAC</td><td>COVERAGE</td><td>优点：该方法对几何变换、模糊和JPEG压缩具有鲁棒性。</td><td>ACC=70.5%</td></tr><tr class="even"><td>2021 [83]</td><td>预处理：将对比限自适应直方图均衡（CLAHE）算法应用于RGB图像，提高平滑区域的特征检测。<br />特征提取：SIFT。<br />特征匹配：FANN，DBSCAN集群，<br />后处理：使用RANSAC和GORE删除异常值</td><td>MICC-F220</td><td>优点：它对几何变换、模糊、压缩和添加噪声具有鲁棒性。它可以处理具有最少的错误匹配的多个复制移动伪造。</td><td>TPR=100%, <br />FPR=3.63%, <br />F1=97.56%</td></tr><tr class="odd"><td>2022 [94]</td><td>特征提取：分别使用SIFT从原始图像和缩放图像中提取关键点并进行合并。<br />特征匹配：通过比较从关键点中获得的筛选描述符和作为第二个关键点匹配的处理关键点集来检测类似的关键点。<br />后处理：采用双自适应滤波法进行去除，采用凸包查找法进行伪造定位</td><td>CoMoFod, <br />MICC-F220, <br />CASIA, <br />COVERAGE</td><td>优点：该方法使用第二个关键点匹配来匹配更多的SIFT关键点，并检测单个和多个CMFD。双自适应过滤可以更能自适应地去除错误的关键点匹配，并更精确地定位伪造区域。</td><td>R=94.5%, <br />P=86.7%, <br />F1=90.4%</td></tr></tbody></table><h2id="基于块和基于关键点的伪造检测方法的研究综述">4.3.基于块和基于关键点的伪造检测方法的研究综述</h2><p>表6给出了混合伪造检测研究的总结</p><table style="width:100%;"><colgroup><col style="width: 5%" /><col style="width: 33%" /><col style="width: 5%" /><col style="width: 33%" /><col style="width: 22%" /></colgroup><thead><tr class="header"><th>Year</th><th>Summary</th><th>Dataset</th><th>Performance</th><th>Metrics</th></tr></thead><tbody><tr class="odd"><td>2020 [97]</td><td>预处理：将图像分割成正方形的方块。<br />特征提取：SIFT+冲浪。<br />特征匹配：具有倒数空间距离的凝聚式层次聚类方法。<br />后处理：RANSAC。</td><td>MICC-F220</td><td>优点：该方法对几何变换具有良好的鲁棒性。结合SIFT和SURF算法，对平滑图像和小伪造区域具有良好的性能</td><td>R=92.5%, <br />FPR=8.9%, <br />F1=91.7%</td></tr></tbody></table><h1 id="具有深度学习的伪造检测">5.具有深度学习的伪造检测</h1><p>​  近年来，深度学习方法已经在计算机视觉中被考虑，如伪造检测。深度学习方法可以自动从数据中提取层次特征。该方法学习了丰富的语义表示，避免了手工特征开发。深度学习方法的最大问题是训练过程[104]需要大量的数据。随后，我们提出了一些解决这个问题的解决方案。</p><h2 id="深度学习中的训练方法">5.1.深度学习中的训练方法</h2><p>​  通过对深度学习网络进行伪造检测的研究，我们发现有两种流行的方法：1)端到端网络和2)预训练网络。在第一种方法中，从输入层到最终层的所有层参数都与一个大数据集一起进行训练。在第二种方法中，由于在大多数问题中缺乏合适的数据集，因此在新问题中使用具有大数据集的预训练网络作为起点或特征提取部分。图20提供了这两种方法及其子分支的概述。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218152341125.png"alt="image-20241218152341125" /><figcaption aria-hidden="true">image-20241218152341125</figcaption></figure><p>​  接下来，将描述这些子分支。</p><h3 id="从头开始训练">5.1.1.从头开始训练</h3><p>​  开始这种方法需要大量的数据和时间来训练网络。它在控制架构和参数方面很重要，并将创建更有效的网络。由于在一些计算机视觉研究中没有大型数据集，使用这种方法几乎是不切实际和效率低效的。Ansari等人[105]使用较小的VGGNet和MobileNetV2.0来检测图像级别的复制移动伪造。他们使用MICC-F2000和CASIAV2.0数据集和增强技术来训练网络。在另一个例子[106]中，VGG16网络被用于检测拼接伪造。使用了CASIAv2.0数据集和训练网络的图像补丁。</p><h3 id="迁移学习">5.1.2.迁移学习</h3><p>​  第二种方法是迁移学习。该方法以某一任务的预先训练模型为起点，在新任务[104]中使用少量的训练样本对模型的参数进行轻微的再训练。这些预先训练好的模型如AlexNet、VGGNet、谷歌lenet、ResNet都是用ImageNet等大型训练数据集进行训练的，具有良好的泛化能力。迁移学习的优点是节省了训练时间，而且不需要大量的数据。深度学习体系结构试图检测初始层中的边和角，中间层中的形状，以及最终层[107]中的任务的具体特征。在迁移学习中，使用预训练网络的初始层和中间层来处理新问题，负责识别边缘和形状，并根据期望的问题和数据集进行调整。<br/>​  利用迁移学习方法检测了复制-移动和拼接方法中的伪造图像。例如，在[108]中，在CASIAv2.0数据集上使用了三个预先训练的模型，VGG16、VGG19和ResNet152。评价结果表明，初始层的特征对伪造图像的检测是有用的。在另一个例子中，迁移学习方法也被用于检测插入绘制方法[109]中的伪造像素。为此目的，首先，对图像应用了一个高通滤波器来突出显示伪造区域。接下来，将图像提供给预先训练过的CNN和ResNet网络的四个初始层。最后，放置一个上采样层，并显示一个包含伪造像素的二进制图像。</p><h3id="使用预先训练好的模型作为特征提取器">5.1.3.使用预先训练好的模型作为特征提取器</h3><p>​  预先训练好的模型可以作为特征提取器，从新的样本中提取出有意义的特征。可以在预先训练的模型上添加新的分类器对特征图进行分类。或者，这些特征图可以作为特征向量，机器学习算法可以用来对它们进行分类。在这种方法中，不需要对整个模型进行重新训练。基本层已经包含了对于对不同任务进行分类通常有用的特性。该方法可用复制移动和拼接方法检测图像[110]和像素[111]级的伪造。<br/>​  预先训练过的网络，如AlexNet[112]、VGGNet [113]、ResNet [114,115]和盗梦空间v3.0[116]已经被用来获取特征向量。然后，可以使用不同的机器学习算法，如SVM、KNN、决策树、朴素贝叶斯和浅层网络进行分类。在某些情况下，使用决策融合技术[114]来对这些特征向量进行分类。一些研究人员使用基于块的方法和深度学习方法[110]相结合，或将人工特征与从深度学习[115]中获得的特征相结合来检测伪造。<br/>​  例如，在[117]中，使用不同的预训练模型如VGG16、MobileNet和概念v3作为特征提取器，研究不同的机器学习算法如KNN、决策树、朴素贝叶斯和随机森林作为分类器，用于复制移动伪造检测。在另一个例子[114]中，使用从预先训练好的ResNet中提取的特征向量和使用KNN、SVM和朴素贝叶斯分类器的决策融合方法进行拼接伪造检测。在另一个例子[115]中，使用DWT和LBP算法结合手工特征，并从ResNet模型中提取特征向量。然后利用浅层网络对这些组合特征进行分类，并检测拼接图像。接下来，表7总结了使用这种方法进行的几个伪造检测研究。</p><h2id="使用不同的深度学习架构进行伪造检测">5.2.使用不同的深度学习架构进行伪造检测</h2><p>​  在本节中，我们全面回顾了用于伪造检测中的各种深度学习架构。图21显示了用于伪造检测的五种著名架构。</p><figure><imgsrc="../postimages/A-survey-on-deep-learning-based-image-forgery-detection/image-20241218154514179.png"alt="image-20241218154514179" /><figcaption aria-hidden="true">image-20241218154514179</figcaption></figure><p>​  在下面，我们对在每个架构中进行的一些研究进行了回顾，这些调查的结果将在下面的小节中给出。</p><h3id="利用卷积神经网络进行伪造检测">5.2.1.利用卷积神经网络进行伪造检测</h3><h3id="利用目标检测网络进行伪造检测">5.2.2.利用目标检测网络进行伪造检测</h3><h3id="基于自动编码器网络的伪造检测">5.2.3.基于自动编码器网络的伪造检测</h3><h3id="基于生成式对抗网络的伪造检测">5.2.4.基于生成式对抗网络的伪造检测</h3><h3id="基于递归神经网络的伪造检测">5.2.5.基于递归神经网络的伪造检测</h3><h2 id="讨论">5.3.讨论</h2><p>​  对伪造检测的研究分为图像伪造检测和像素伪造检测两个层次。第3节中描述的所有数据集都用于图像伪造检测研究，并且只使用具有地面真实值的数据集，如CoMoFod、MICC-F600、GRIP、NIST16等。由于几何变换和后处理操作的多样性，CoMoFod是一个用于在可用的复制移动伪造数据集中评估复制移动伪造检测方法的合适数据集。<br/>​  所有的深度学习架构都用于图像伪造检测级别。然而，具有迁移学习技术的卷积神经网络几乎总是用于图像伪造检测。在像素伪造检测的研究中，经常采用对象检测网络和自动编码器网络。这些网络使用具有地面真实值的数据集来训练和预测二值掩模。由于需要大量的数据进行训练，缺乏合适的伪造数据集，以及健康图像和伪造图像的不平衡，生成对抗网络通常被用作单类分类方法。这些网络通常只用健康的数据进行训练，将伪造的图像或补丁识别为不一致的图像。递归神经网络通常与其他网络一起使用来检测像素或补丁之间的不一致性。根据上述主题，我们回顾了2019-2023年的图像伪造检测研究，并将其分为三组。图33-35显示了这三组人。</p><h1 id="结论">6.结论</h1><p>​  本文在整个图像和像素水平上，使用基于块、基于关键点和深度学习的方法，研究了不同的伪造方法、伪造数据集和伪造检测方法。研究表明，基于块的方法的计算时间和复杂度较高，且在一些几何变换和后处理中表现较差。基于关键点的方法对几何变换具有更强的鲁棒性。由于缺乏足够的关键点，该方法对小伪造区域的检测效果较差。基于块和基于关键点的方法在预处理、特征提取、特征匹配、发现相似区域和后处理等方面都有不同的步骤。这些步骤的参数必须单独调整，以检测伪造。与前两种方法不同的是，深度学习在不同架构的训练过程中自动执行伪造检测过程。伪造检测的深度学习最重要的问题是缺乏合适的数据集和长时间的训练。为了解决这个问题，研究人员在大数据集上使用了预先训练过的网络和迁移学习技术。<br/>​  本研究回顾了各种伪造检测方法，并帮助研究人员熟悉新的想法和挑战。伪造检测是一个非常具有挑战性的问题，它仍然是一个开放的研究课题。在平滑和小区域中的伪造检测、使用几何变换和各种后处理操作的伪造检测，构建合适的伪造数据集，多重伪造检测，使用传统和深度学习方法的组合进行伪造检测，以及在各种伪造数据集上推广伪造检测方法，都是伪造检测领域仍需研究的案例。例如，根据研究，已经提出了许多方法来分别检测所有的伪造类型。每种方法都有其参数和设置，并已在特定的伪造品和数据集上进行了评估。因此，当伪造或数据集类型改变时，所提出的方法失去了有效性。有一些方法可以同时检测所有类型的伪造品。在未来的工作中，结合深度学习方法、基于块的方法、基于关键点的方法和边缘检测等不一致检测算法，可以同时识别所有类型的伪造。<br/>​  另一方面，没有一个合适的数据集，其中包含足够的图像，包括各种伪造、几何变换和后处理操作。因此，每种方法都关注于一个特定的伪造和数据集。在未来的工作中，可以使用生成对抗网络来生成合适的数据集，包括所有类型的伪造、几何变换、后处理操作和用于像素伪造检测的地面真实掩模。<br/>​  检测非常小的伪造区域和利用几何变换检测伪造是该领域的其他问题。在未来的工作中，可以使用不同的图像增强算法，如调整图像的直方图，结合几种关键点提取算法来增加关键点。为了解决过度几何变换的伪造检测问题，可以结合使用几种不同的深度学习架构、目标检测网络或更抗几何变换的胶囊网架构。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SPL</title>
      <link href="/SPL/"/>
      <url>/SPL/</url>
      
        <content type="html"><![CDATA[<p>多任务网络上的分布式安全聚类与估计</p><h1 id="摘要">摘要</h1><p>​  近年来，分布式安全估计问题得到了广泛的研究。然而，在多任务网络中，恶意信息和任务之间的不确定关系对现有的相关算法提出了挑战。为了获得可靠的估计，提出了分布式安全聚类和估计（DSCE，DistributedSecure Clustering andEstimation）算法。该算法包括一个具有自适应组合系数的扩散滤波器，以及一个维数检测器。通过估计节点间的最大任务间间隙，完成了恶意信息的自适应区分过程。此外，提出了安全聚类方案，通过根据检测结果动态采用防御策略来减轻攻击者的影响。仿真结果证明了该算法对攻击的鲁棒性。</p><h1 id="i.介绍">I.介绍</h1><p>​  分布式参数估计问题近年来得到了广泛的讨论。目标是基于分布在网络中的节点集合的测量值和回归向量来近似最优估计。这些节点与相邻节点进行协同估计，以提高结果的准确性。此外，还确定了三种主要的协作方法：增量方法[1]、共识方法[2]和扩散方法[3]。其中，扩散方法被认为是最突出的。<br/>​  当考虑到网络攻击的存在时，来自节点的数据通常是伪造的，从而导致估计过程的中断。因此，没有适当防御机制的算法通常会导致性能下降。为了解决这一挑战，我们提出了各种安全估计算法，如：[4]-[9]所示。在[4]中，采用了一种基于声誉的检测器来识别恶意传感器。在[5]、[6]中，Hua等人分别设计了一种基于kl-发散的检测机制和交叉验证方法来区分被攻击的节点。在[9]中，建立了一个参考子系统来检测恶意信息，并提供可靠的替换信息。此外，一些研究也关注于多任务场景下的问题。在[10]中，Wang等人提出了一种基于贝叶斯的估计器来在多任务对抗网络中获得可靠的估计。在[11]中，实现了局部-离群值-因子（LOF，Local-Outlier-Factor）来测量和定位多任务环境下的恶意估计。<br/>​  上述研究为解决单任务和多任务环境下的安全估计问题做出了贡献。然而，很少有研究考虑先前聚类信息不可用的多任务环境，这在实际应用中是一个更普遍和具有挑战性的场景。<br/>​  值得注意的是，在多任务网络中，集群信息的不确定性导致节点之间的协作不协调，导致相关算法的性能下降。此外，任务间邻居之间未知的内在差异混淆了检测过程，从而增加了区分被攻击节点的难度。<br/>​  本文主要考虑了FDI攻击和部分FDI攻击的存在性，并提出了分布式安全聚类和估计（DSCE）算法。在该算法中，通过具有自适应组合系数的扩散滤波器（[12]中的AC-dLMS滤波器），得到了一个锚定估计，作为可靠的参考。通过对锚定估计的维数检测过程，识别出可靠的数据。此外，还设计了一种自适应更新节点阈值的方法，基于评估任务间间隙造成的最大差异。随后，采用了安全聚类方案。通过根据检测结果选择不同的防御策略，在聚类过程中拒绝恶意信息。最后，将来自安全簇内邻居的中间估计结合在一起以获得安全估计。仿真结果表明，该算法在抑制FDI攻击和部分FDI攻击方面是可行的。</p><h1 id="ii.准备工作">II.准备工作</h1><p>​  本节分别简要介绍了在无攻击环境下的多任务扩散LMS（multi-dLMS）算法和攻击模型。</p><h2 id="a.-多任务环境下的扩散lms算法">A. 多任务环境下的扩散LMS算法</h2><p>​  本文简要考虑了一个具有N个节点的多任务模型，并将其划分为Q个簇。<br/>​  在每个时刻t，每个节点接收到<spanclass="math inline">\(\{d_{k,t},\mathbf{u}_{k,t}\}\)</span>，其中<spanclass="math inline">\(d_{k,t}\)</span>表示标量测量值，<spanclass="math inline">\(\mathbf{u}_{k,t}\)</span>表示一个L维的输入回归向量。特别是它们之间的线性关系如下：<spanclass="math display">\[d_{k,t}=\mathbf{u}_{k,t}^{T}\mathbf{w}_{k}^{o}+v_{k,t},\]</span>​  其中<spanclass="math inline">\(\mathbf{w}_{k}^{o}\)</span>是节点k的未知l维最优估计，<spanclass="math inline">\(v_{k,t}\)</span>是噪声项。<br/>​  不同集群之间的关系表示如下：<span class="math display">\[\begin{array}{ll}\mathbf{w}_{k}^{o} =\mathbf{w}_{C(q)}^{o}, &amp; \text{if } k \in \cal C(q);\\\mathbf{w}_{C(p)}^{o} = \mathbf{w}_{C(q)}^{o}, &amp; \text{if } \calC(p) \text{ and } \cal C(q) \text{ are connected;}\end{array}\]</span>​  其中，<spanclass="math inline">\(\mathbf{w}_{C(q)}^{o}\)</span>为第p个簇中节点的索引集。<br/>​  主要目标是通过与邻居通信来近似最近的最优估计。本部分主要讨论了采用适应后再组合（ATC，adapt-then-combine）策略的多任务dLMS算法。这些方程式说明如下：<span class="math display">\[\varphi_{k,t}={\bfw}_{k,t-1}+\mu_{k}\sum_{\ell\in{\cal N_{k}}\cap{\calC}_{k}}[\cal{c}_{\ell,t}(d_{\ell,t}-{\bf u}_{\ell,t}^{T}{\bfw}_{k,t-1})]\]</span></p><p><span class="math display">\[{\bf w}_{k,t}=\sum_{\ell\in{\calN_{k}}\cap{\cal C}_{k}}a_{\ell,k}\varphi_{k,t},\]</span></p><p>​  其中<span class="math inline">\(\varphi_{k,t}\)</span>和<spanclass="math inline">\(\mathbf{w}_{k\cdott}\)</span>分别为节点k在时间t时的中间估计和瞬时估计，<spanclass="math inline">\(\mu_k\)</span>为步长，<spanclass="math inline">\({\cal N}_{k}\)</span>为节点k的邻居集合，<spanclass="math inline">\({\calC}_{k}\)</span>为节点k所属的聚类集合。非负系数<spanclass="math inline">\(a_{\ell,k}\)</span>、<spanclass="math inline">\(c_{\ell,k}\)</span>分别为左随机矩阵A和右随机矩阵C的<spanclass="math inline">\((\ell,k)\)</span>级别实体，即： <spanclass="math display">\[\mathbf{A}^{T}\mathbf{1}_{N}=\mathbf{1}_{N},a_{\ell,k}=0\;\mathrm{if}\;\ell\not\in\mathcal{N}_{k},\]</span></p><p><spanclass="math display">\[\mathrm{C1}_{N}\,=\,{\bf1}_{N},c_{\ell.k}\,=\,0\mathrm{~if~}\ell\not\in\mathcal{N}_{k},\]</span></p><p>​  其中，<spanclass="math inline">\({\bf1}_{N}\)</span>表示带有单位项的列向量。</p><h2 id="b.攻击模型">B.攻击模型</h2><p>​  本文考虑了FDI攻击和部分FDI攻击，具体如下所示：</p><h3 id="fdi攻击">1) FDI攻击</h3><p>​  对于FDI攻击，注入恶意信息篡改每个节点的测量值<spanclass="math inline">\(d_{k,t}\)</span>，如下： <spanclass="math display">\[\tilde{d}_{k,t}\,=\,d_{k,t}\,+\,d_{k,t}^{a tt},\]</span> ​  其中<spanclass="math inline">\(\tilde{d}_{k,t}\)</span>是受损的测量值，<spanclass="math inline">\(d_{k,t}^{a t t}\)</span>是一个标量，满足： <spanclass="math display">\[d_{k,t}^{a tt}=\mathbf{u}_{k,t}^{T}\mathbf{p}_{k}\]</span> ​  其中<spanclass="math inline">\(\mathbf{p}_{k}\)</span>是一个被攻击节点的l维非零向量，而是一个普通节点的零向量。</p><h3 id="部分fdi攻击">2)部分FDI攻击</h3><p>​  部分FDI攻击是FDI攻击的一种特例。由于网络内的攻击强度和范围降低，对攻击者更隐蔽和高效。<br/>​  类似地，部分FDI攻击者向被攻击的节点发送满足(7)和(8)的恶意信息。然而，只有部分L维的估计是证伪的，这可以显示为：<spanclass="math inline">\(0\lt \|\mathbf{q}_{k}\|_{0}\ltL\)</span>。<br/>​  此外，在我们所考虑的对抗性环境中，假设在[4]-[6]，[9]-[11]中所采用的假设如下。<br/>​  假设1：被攻击的节点数小于<spanclass="math inline">\([{\frac{|N_{k}|}{2}}]\)</span>。</p><h1 id="iii.分布式安全聚类与估计算法">III.分布式安全聚类与估计算法</h1><p>​  在本部分中，提出了DSCE算法来在存在攻击的情况下产生安全的聚类和估计结果。从系统学的角度来看，采用了AC-dLMS滤波器。从实现的角度来看，主要包括四个主要步骤：自适应、检测、聚类和组合。在下面的几个部分中，我们将简要地描述这些过程。</p><h2 id="a.自适应">A.自适应</h2><p>​  在这一步中，基于<spanclass="math inline">\(\{d_{k,t},\mathbf{u}_{k,t}\}\)</span>，每个节点调整中间估计值<spanclass="math inline">\(\varphi_{k,t}\)</span>如下： <spanclass="math display">\[\varphi_{k,t}=\psi_{k,t-1}+\mu_{k}\sum_{\ell\in{N_{k}}}c_{\ell,k}{\mathbf{u}}_{\ell,i}(d_{\ell,i0}-{\mathbf{u}}_{\ell,i}^{T}\psi_{k,t-1})\]</span>​  其中，<spanclass="math inline">\(\psi_{k,t-1}\)</span>表示节点k在t-1时刻的另一个中间估计值。<br/>​  组合系数<spanclass="math inline">\(a_{\ell,k}\)</span>、<spanclass="math inline">\(c_{\ell,k}\)</span>可以根据任务间的相似性进行调整。因此，具有异常估计的恶意节点可以被未受攻击的节点自适应地隔离。此外，利用邻居之间的协作可以获得更准确的估计。因此，与非合作滤波器相比，AC-dLMS滤波器更有效。</p><h2 id="b.检测">B.检测</h2><p>​  在这一步中，获得排除恶意信息的锚点估计<spanclass="math inline">\({\tilde{\mathbf{w}}}_{k,i}\)</span>以供参考。具体建立了节点k的第m维的排序序列如下：<spanclass="math display">\[\boldsymbol{\Phi}_{k,t}^{(m)}=\{\varphi_{\ell_{1},t}^{(m)},...,\varphi_{\ell_{s},t}^{(m)},...,\varphi_{\ell_{n_k},t}^{(m)}\}\]</span>​  其中<spanclass="math inline">\(\varphi_{\ell_{1},t}^{(m)}&lt;\varphi_{\ell_{s},t}^{(m)}&lt;\varphi_{\ell_{n_k},t}^{(m)}\)</span>和<spanclass="math inline">\(\ell_{1},\ell_{s},\ell_{n_{k}}\in{\calN}_{k}\)</span>。<br/>​  根据假设1，攻击者倾向于使数据偏离之前的值。因此，当节点k及其相邻节点按估计值进行排序时，被攻击的节点更有可能位于被排序序列的左侧或右侧。因此，中心中的值通常比其他值更可靠，可以被认为是锚定估计的元素。<br/>​  因此，可以建立锚定估计中节点k的第m维估计：<spanclass="math display">\[\bar{\mathbf{w}}_{k,t}^{(m)}=\boldsymbol{\Phi}_{\lceil\frac{|\mathcal{N}_k|}{2}\rceil,i}^{(m)}.\]</span>​  由于得到了每个节点的可靠估计，因此设计了一个基于阈值测试过程的维数攻击检测器来定位可靠的信息。<br/>​  具体来说，对于节点k的第m个维数，如果中间估计<spanclass="math inline">\(\varphi_{k,t}\)</span>和锚估计<spanclass="math inline">\(\mathbf{\barw}_{k,t}\)</span>之间的距离超过阈值，可以推断维数受到攻击，反之亦然。判断过程表现如下：<spanclass="math display">\[\|\bar{\mathbf{w}}_{k,t}^{(m)}-\varphi_{k,t}^{(m)}\|^2\overset{\mathbb{H}_1}{\underset{\mathbb{H}_2}{\lessgtr}}b\cdot\theta_{k,t},\]</span></p><p><span class="math display">\[\theta_{k,t}=\begin{cases}\epsilon_k,&amp; \mathrm{if}\epsilon_k&gt;\theta_{k,t-1}; \\\theta_{k,t-1}, &amp;\mathrm{otherwise}, &amp; \end{cases}\]</span></p><p><span class="math display">\[\epsilon_{k}=\|m a x\{\bar{\bfw}_{k,t}^{(m)}-\bar{\bfw}_{\ell,i}^{(m)}|\ell\in\mathcal{N}_{k}\|^{2}\]</span></p><p>​  其中，<spanclass="math inline">\(\theta_{k,t}\)</span>为自适应阈值，视为由任务间差异引起的节点之间的最大距离，更新公式在（13）和（14）中。此外，b是一个预定义的常数，用于调整判断条件的松弛或紧性，H1和H2分别是节点k的第m维估计是安全的和被攻击的假设。<br/>​  此外，以下变量用于记录检测结果：<span class="math display">\[T_{m,k}=\begin{cases}1, &amp;\mathrm{if}~\mathbb{H}_1\text{is achieved}; \\0, &amp;\mathrm{if}~\mathbb{H}_2\text{is achieved}; &amp;\end{cases}\]</span></p><p><span class="math display">\[s_{k}={\left\{\begin{array}{ll}{1,}&amp;{\mathrm{if~}\|\mathbf{T}_{k}\Vert_{1}^{1}=L;}\\{0,}&amp;{\mathrm{otherwise;}}\end{array}\right.}\]</span></p><p>​  其中<span class="math inline">\({\bfT}_{k}=\{T_{1,k},T_{2,k},\ldots,T_{L,k}\}^{T}\)</span>。</p><h2 id="c.-聚类">C. 聚类</h2><p>​  在这一步中，通过以下自适应聚类方案更新系数<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\(c_{\ell,k}\)</span>的值，完成了AC-dLMS滤波器的聚类过程：<spanclass="math display">\[a_{\ell,k}={\frac{\|\varphi_{k,\ell}+\mu_{k}{\bfq}_{k,\ell}-\varphi_{\ell,\imath}\|^{-2}}{\sum_{j\in{\calN}_{k}}\|\varphi_{k,\ t}+\mu_{k}{\bf q}_{k,\t}-\varphi_{j,\imath}\|^{-2}}}\]</span></p><p><span class="math display">\[c_{k,\ell}=a_{\ell,k}\]</span></p><p>​  其中<span class="math inline">\({\bf q}_{k,t}={\bfu}_{k,t}(d_{k,t}-{\bfu}_{k,t}^{T}\varphi_{k,t})\)</span>。<br/>​  因此，对于节点k，估计值相似性较高的邻居赋值越大，在每个信息融合过程中占越大的影响。<br/>​  同样，在该算法中，也可以通过调节系数<spanclass="math inline">\(\tilde{a}_{\ell,k}\)</span>来消除恶意信息的影响。因此，提出了安全聚类方案。在该方案中，根据检测结果，动态地采用了防御策略。</p><h3 id="情形1">1)情形1</h3><p>​  当<spanclass="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}=L\)</span>时，节点k状态为安全。此外，恶意节点被<spanclass="math inline">\(a_{\ell,k}\)</span>排除，因此，没有必要采取其他措施，我们设置：<span class="math display">\[\check{a}_{\ell,k}=a_{\ell,k}\]</span></p><h3 id="情形2">2)情形2</h3><p>​  当<spanclass="math inline">\(0&lt;\|\mathbf{T}_{k}\|_{1}^{1}&lt;L\)</span>时，表示节点k受到部分攻击。因为可以利用来自未受攻击的维数的信息来判断节点之间的相似性。因此，提出了自适应部分聚类（APC，AdaptivePartial Clustering）算法来恢复聚类信息。<br/>​  首先，利用<spanclass="math inline">\(\mathbf{T}_{k}\)</span>对Hadamard积的尺寸级异常信息进行过滤，生成部分型安全信息如下：<spanclass="math display">\[\dot{\varphi}_{\ell.i}=\varphi_{\ell.i}\odot\mathbf{T}_{k},\ell\in{\mathcal{N}}_{k},\]</span></p><p><spanclass="math display">\[{\dot{\mathbf{q}}}_{k,t}=\mathbf{u}_{k,t}(d_{k,t}-\mathbf{u}_{k,t}^{T}{\dot{\varphi}}_{k,t})\]</span></p><p>​  然后，生成<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>如下： <spanclass="math display">\[\check{a}_{\ell,k}=\frac{s_{\ell}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{\ell,i}\|^{-2}}{\sum_{j\inN_{k}}s_{j}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{j,i}\|^{-2}}\]</span> ​  其中，<spanclass="math inline">\(s_j\)</span>用于过滤异常节点。<br/>​  在得到<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>后，其安全的簇内邻居通过组合步骤可以恢复被攻击节点的错误估计。</p><h3 id="情形3">3)情形3</h3><p>​  当<span class="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}\ =\0\)</span>时，节点k被认为受到FDI攻击者的影响。在这种情况下，将无法恢复聚类信息。此外，也没有可用的资料来替代受攻击的估计数。因此，将去掉节点k，以减轻其对网络的影响。</p><h2 id="d.-组合">D. 组合</h2><p>​  当前一个步骤完成后，计算出组合系数<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\({\check{a}}_{\ell,k}\)</span>。因此，在这一步中，每个节点组合了该步骤中来自其邻居的中间估计。<br/>​  对于AC-dLMS滤波器，估计值<spanclass="math inline">\(\psi_{k,i}\)</span>的计算方法如下： <spanclass="math display">\[\psi_{k,t}=\sum_{\ell\in{\calN}_{k}}a_{\ell,k}\varphi_{k,t}\]</span> ​  可靠的瞬时估计<spanclass="math inline">\(\mathbf{w}_{k,t}\)</span>生成为： <spanclass="math display">\[\mathbf{w}_{k,t}=\sum_{\ell\in {\calN}_{k}}{\check{a}}_{\ell,k}\varphi_{k,t}\]</span>​  综上所述，在算法1中总结了该算法的更新过程。</p><hr /><p>算法1：DSCE算法</p><hr /><ol type="1"><li>初始化：Let <spanclass="math inline">\(\mathrm{w}_{k,0}=\psi_{k,0}=\varphi_{k,0}=0\)</span>。<br/>2.for t=1 to T do<br/>3. ​ for k=1 to N do<br/>4. ​公式（9）中的自适应中间估计 <spanclass="math inline">\(\varphi_{k,t}=\psi_{k,t-1}+\mu_{k}\sum_{\ell\in{N_{k}}}c_{\ell,k}{\mathbf{u}}_{\ell,i}(d_{\ell,i0}-{\mathbf{u}}_{\ell,i}^{T}\psi_{k,t-1})\)</span><br/>5.​获得公式（10）、公式（11）的锚定估计 <spanclass="math inline">\(\bar{\mathbf{w}}_{k,t}^{(m)}=\boldsymbol{\Phi}_{\lceil\frac{|\mathcal{N}_k|}{2}\rceil,i}^{(m)}\)</span><spanclass="math inline">\(\boldsymbol{\Phi}_{k,t}^{(m)}=\{\varphi_{\ell_{1},t}^{(m)},...,\varphi_{\ell_{s},t}^{(m)},...,\varphi_{\ell_{n_k},t}^{(m)}\}\)</span><br/>6.​检测信任维度并获取<span class="math inline">\({\bf T}_{k}\)</span> <spanclass="math inline">\(T_{m,k}=\begin{cases}1, &amp;\mathrm{if}~\mathbb{H}_1\text{is achieved}; \\0, &amp;\mathrm{if}~\mathbb{H}_2\text{is achieved}; &amp;\end{cases}\)</span><br/>7. ​ 执行自适应聚类方案，得到<spanclass="math inline">\(a_{\ell,k}\)</span>和<spanclass="math inline">\(c_{\ell,k}\)</span> <spanclass="math inline">\(a_{\ell,k}={\frac{\|\varphi_{k,\ell}+\mu_{k}{\bfq}_{k,\ell}-\varphi_{\ell,\imath}\|^{-2}}{\sum_{j\in{\calN}_{k}}\|\varphi_{k,\ t}+\mu_{k}{\bf q}_{k,\t}-\varphi_{j,\imath}\|^{-2}}}\)</span> 、<spanclass="math inline">\(c_{k,\ell}=a_{\ell,k}\)</span><br/>8. ​ 如果<spanclass="math inline">\(\|\mathbf{T}_{k}\|_{1}^{1}=L\)</span><br/>9. ​设置<spanclass="math inline">\(\check{a}_{\ell,k}=a_{\ell,k}\)</span><br/>10. ​如果<spanclass="math inline">\(0&lt;\|\mathbf{T}_{k}\|_{1}^{1}&lt;L\)</span><br/>11.​设置<spanclass="math inline">\(\check{a}_{\ell,k}=\frac{s_{\ell}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{\ell,i}\|^{-2}}{\sum_{j\inN_{k}}s_{j}\cdot\|\dot{\varphi}_{k,t}+\mu_{k}\dot{\bfq}_{k,t}-\dot{\varphi}_{j,i}\|^{-2}}\)</span><br/>12. ​否则删除节点k<br/>13. ​ 结合中间估计，分别得到<spanclass="math inline">\(\psi_{k,i}\)</span>和<spanclass="math inline">\(\mathbf{w}_{k,t}\)</span>。 <spanclass="math inline">\(\psi_{k,t}=\sum_{\ell\in{\calN}_{k}}a_{\ell,k}\varphi_{k,t}\)</span> <spanclass="math inline">\(\mathbf{w}_{k,t}=\sum_{\ell\in {\calN}_{k}}{\check{a}}_{\ell,k}\varphi_{k,t}\)</span></li></ol><hr /><h1 id="iv.模拟">IV.模拟</h1><p>​  在本节中，我们考虑了一个包含5个不同集群的21个节点的多任务网络，其拓扑结构如图1(a)所示。</p><figure><img src="../postimages/SPL/image-20241216092147022.png"alt="image-20241216092147022" /><figcaption aria-hidden="true">image-20241216092147022</figcaption></figure><p>​  具体而言，<spanclass="math inline">\(\mathcal{C}_{1}=\{1,2,3,4\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{2}=\{5,6,7,8,9\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{3}=\{10,11,12,13,14\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{4}=\{15,16,17\}\)</span>，<spanclass="math inline">\(\mathcal{C}_{5}=\{18,19,20,21\}\)</span>。输入信号<spanclass="math inline">\(\mathbf{u}_{k,t}\)</span>和噪声项<spanclass="math inline">\({v}_{k,t}\)</span>均被视为零均值高斯随机变量，其方差为<spanclass="math inline">\(\sigma_{u,k}^{2}\)</span>和<spanclass="math inline">\(\sigma_{v,k}^{2}\)</span>，如图1 (b)所示。<spanclass="math inline">\(\mathbf{w}_{k}^{o}\)</span>的尺寸设置为：<spanclass="math inline">\(L=5\)</span>。<br/>​  为了进行比较，本文还说明了[13]中的多dLMS算法、nc-LMS算法、[12]中的AC-dLMS算法以及[9]中的S-dLMS算法的曲线。此外，在图例中用星号标记的项目表示已经给出了聚类信息。</p><h2 id="a.示例1">A.示例1</h2><p>​  在第一个例子中，在网络中只考虑了部分的FDI攻击。具体来说，被攻击节点的集合为<spanclass="math inline">\(\{2,13,19\}\)</span>。相应估计的前两个维度是受到部分FDI攻击。</p><figure><img src="../postimages/SPL/image-20241216103214212.png"alt="image-20241216103214212" /><figcaption aria-hidden="true">image-20241216103214212</figcaption></figure><p>​  图2(a)给出了不同算法的瞬态MSD曲线。在存在攻击的情况下，DSCE算法表现出良好的性能，几乎达到了与无攻击的S-dLMS*和多dLMS算法相同的性能。计算结果表明，该算法已经完成了安全的聚类和估计任务。特别是，即使没有任务间关系的先验信息，该算法也获得了与与聚类信息相关的算法几乎相同的精度。<br/>​  从图2(b)中的稳态MSD曲线可以看出，被攻击节点的估计值被其任务对应的安全值所取代。<br/>​  此外，与nc-LMS算法相比，AC-dLMS算法的未攻击节点具有较低的稳态MSD值，说明AC-dLMS滤波器在获得更准确的锚定估计方面具有优越性。</p><h2 id="b.示例2">B.示例2</h2><p>​  为了验证该算法在更复杂的攻击环境下的鲁棒性，在示例2中考虑了这两种类型的攻击。具体来说，节点10和21受到FDI攻击，节点1和5受到部分FDI攻击，相应的前三个维度被篡改。</p><figure><img src="../postimages/SPL/image-20241216102940720.png"alt="image-20241216102940720" /><figcaption aria-hidden="true">image-20241216102940720</figcaption></figure><p>​  不同算法的瞬态MSD曲线如图3(a)。所示得到了与示例1相似的结果，证明了其对攻击的鲁棒性。<br/>​  从图3(b)可以看出，在该算法中，受FDI攻击的节点被删除，而受部分FDI攻击的节点被其安全的簇内邻居恢复。结果表明，该检测器在区分攻击方面的可靠性和安全聚类方案在消除攻击者影响方面的有效性。<br/>​  综上所述，这两个实例都证明了该算法在攻击下是有效的。</p><h1 id="v.结论">V.结论</h1><p>​  为了获得存在网络攻击情况下的安全估计，本文引入了DSCE算法。该算法采用了一种具有自适应协同系数的扩散滤波器。伴随着具有自适应阈值的维级攻击检测器，可以更准确地定位恶意信息。此外，还提出了安全聚类方案来消除恶意信息的影响。最后，通过融合邻居的任务内安全估计，得到了可靠的估计。仿真结果表明，该算法在抵御FDI攻击和部分FDI攻击方面是有效的。</p><h1 id="评审">评审：</h1><h2 id="对比论文一">对比论文一</h2><p>Secure Distributed Estimation Over Wireless Sensor Networks UnderAttacks</p><p>初步和问题制定</p><p>无攻击的扩散LMS</p><p>受攻击下的安全分布式估计</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Fuzzy K-Means With Adaptive Loss and Entropy Regularization</title>
      <link href="/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/"/>
      <url>/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/</url>
      
        <content type="html"><![CDATA[<p>Deep Fuzzy K-Means With Adaptive Loss and Entropy Regularization</p><p>（发表于IEEE Transactions on Fuzzy Systems 2019）</p><h1 id="摘要">摘要</h1><p>​  基于神经网络的聚类方法由于特征提取更有效，通常比传统方法具有更好的性能。大多数现有的深度聚类技术要么先利用图信息从原始数据中提取关键的深度结构，要么简单地利用随机梯度下降（SGD）。然而，他们经常遭受关于降维和聚类的学习步骤的分离。为了解决这些问题，提出了一种具有自适应损失函数和熵正则化特性的深度模糊k-means（DFKM）。DFKM同时进行深度特征提取和模糊聚类，生成更合适的非线性特征映射。此外，DFKM还结合了FKM，从而利用模糊信息来表示深度簇的清晰结构。为了进一步提高模型的鲁棒性，采用自适应权值对目标应用鲁棒损失函数。此外，采用熵正则化的亲和性来提供每个赋值的置信度，相应的隶属度和质心矩阵通过立体解而不是SGD来更新。大量的实验表明，在三个聚类指标下，DFKM比目前最先进的模糊聚类技术具有更好的性能。</p><p>索引项-自动编码器（AE）、深度神经网络、图像分割、鲁棒模糊k均值（FKM）、无监督嵌入式聚类。</p><h1 id="介绍">1.介绍</h1><p>​  为了解决这些问题，提出了一种具有自适应损失函数和熵正则化的深度模糊k-均值（DFKM），该模型将模糊聚类合并到AE中，提取更合适的深度特征。利用自适应损失函数[38]来增强对异常值的鲁棒性。图1显示了DFKM的框架。本文的主要贡献总结如下。</p><ul><li>聚类嵌入训练神经网络，使声发射能够将数据映射到更合适的深度特征空间。换句话说，DFKM同时进行深度特征提取和聚类。</li><li>利用鲁棒损失函数来增强该模型对具有自适应权值的异常值的不敏感性。为了解决这一问题，提出了一种有效的算法，并进一步保证了其收敛到局部最小值。</li><li>对亲和矩阵引入熵正则化，为每个分配提供置信度。</li><li>在我们的模型中不需要类似的基于图的信息，亲和矩阵和质心矩阵是通过紧密形式的解而不是SGD来更新的。因此，它可以在大数据上有效地执行。</li></ul><p>​  符号：在本文中，所有大写粗体字母表示矩阵，而所有小写粗体字母表示向量。对于矩阵M，<spanclass="math inline">\(m^i\)</span>表示第i个行向量，<spanclass="math inline">\(m_i\)</span>表示第i个列向量，<spanclass="math inline">\(m_{ij}\)</span>是它的<spanclass="math inline">\((i,j)\)</span>个元素。此外，MT是矩阵M的转置，<spanclass="math inline">\({\textbf{1}}=[1,1,\cdot\cdot\cdot,1]^{T}\)</span>，0表示零矩阵。M&gt; 0表示每个元素都为正。<spanclass="math inline">\(\|\mathbf{m}\|_{1}\)</span>和<spanclass="math inline">\(\|\mathbf{m}\|_{2}\)</span>分别表示<spanclass="math inline">\(\ell_{1}\)</span>范数和<spanclass="math inline">\(\ell_{2}\)</span>范数。<spanclass="math inline">\(\nabla_{\mathbf{x}}f(\mathbf{x})=[{\frac{\partialf}{\partial x_{1}}},{\frac{\partial f}{\partialx_{3}}},\cdot\cdot,{\frac{\partial f}{\partialx_{n}}}]^{T}\)</span>，其中f (x)是一个标量输出函数，而<spanclass="math inline">\(\nabla_{\mathbf{x}}g(\mathbf{x})=[{\frac{\partialg_{1}}{\partial x}},{\frac{\partial g_{2}}{\partialx}},\cdot\cdot,{\frac{\partial g_{n}}{\partial x}}]^{T}\)</span>，其中g(x)是一个向量输出函数。</p><h1 id="相关工作">2.相关工作</h1><h2 id="a.模糊聚类">A.模糊聚类</h2><h2 id="b.深度聚类">B.深度聚类</h2><h1 id="方法">3.方法</h1><p>​  由于传统的核k-means（KKM）和基于谱的聚类方法对大数据难以处理，而KM和非负矩阵分解等有效技术过于简单，应用于非线性数据，因此提出了处理大数据集和非线性分布数据的DFKM。在本节中，我们首先介绍了自适应损失函数和熵正则化的FKM。然后，详细介绍了DFKM的研究细节。</p><h2 id="a.-自适应损耗函数">A. 自适应损耗函数</h2><p>​  <span class="math inline">\(\ell_{2，1}\)</span>范数： <spanclass="math display">\[\|\mathbf{M}\|_{2,1}=\sum_{i}\|\mathbf{n^{i}}\|_{2}\]</span>​  Frobenius范数： <spanclass="math display">\[\|\mathbf{M}\|_{F}^{2}=\sum_{i}\|\mathbf{m}^{i}\|_{2}^{2}\]</span>​  为了利用它们的优点，将一个鲁棒损失函数即自适应损失函数定义为[38]，[40]如下：<span class="math display">\[||{\bfM}||_{\sigma}=\sum_{i}{\frac{(1+\sigma)||{\bf m}^{i}||_{2}^{2}}{||{\bfm}^{i}||_{2}+\sigma}}\]</span>​  其中，σ是一个权衡参数，它控制对各种类型异常值的鲁棒性。不同σ下向量的自适应损失函数说明如图2所示。</p><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215846737.png"alt="image-20241204215846737" /><figcaption aria-hidden="true">image-20241204215846737</figcaption></figure><p>​  从图2中可知，自适应损失函数在2,1-范数和平方弗罗比尼乌斯范数之间进行插值，它继承了平方弗罗比尼乌斯范数的平滑性。<spanclass="math inline">\(\|\mathbf{M}\|_{\sigma}\)</span>的性质总结如下。</p><ul><li><spanclass="math inline">\(\|\mathbf{M}\|_{\sigma}\)</span>是二倍微分、凸和非负的，因此它适合作为一个损失函数。</li><li>如果<span class="math inline">\(\forall i\)</span>，<spanclass="math inline">\(\|\mathbf{m}_{i}||\ll\sigma\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}{\frac{1+\sigma}{\sigma}}\end{array}}||\mathbf{M}||^{2}_{F}\)</span>。</li><li>如果<span class="math inline">\(\forall i\)</span>，<spanclass="math inline">\(\|\mathbf{m}_{i}||\gg\sigma\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}{(1+\sigma)}\end{array}}||\mathbf{M}||_{2,1}\)</span>。</li><li>如果<span class="math inline">\(\sigma\to0\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}\end{array}}||\mathbf{M}||_{2,1}\)</span>。</li><li>如果<span class="math inline">\(\sigma\to\infty\)</span>，则<spanclass="math inline">\(||\mathbf{M}||_{\sigma}\rightarrow\{\begin{array}{l}\end{array}}||\mathbf{M}||^{2}_{F}\)</span>。</li></ul><h2 id="b.-具有加权自适应损失函数的fkm">B.具有加权自适应损失函数的FKM</h2><p>​  对于数据数为N的任意数据矩阵<spanclass="math inline">\(\mathbf{X}=\left[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{N}\right]\)</span>，将具有熵正则化的FKM的目标函数定义为[26]，[39]<span class="math display">\[\operatorname*{min}_{c_j,u_{ij}}\sum_{i=1}^{N}\sum_{j=1}^{k}u_{i j}\vert\vert{\bf x}_{i}-{\bfc}_{j}\vert\vert_{2}^{2}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\\\mathrm{s.t.}\,\,\sum_{j=1}^{k}u_{i j}=1,0\lt u_{i j}\lt 1\]</span>​  其中，<span class="math inline">\(\gamma\)</span>是控制<spanclass="math inline">\(u_{ij}\)</span>分布的权衡参数。当<spanclass="math inline">\(\gamma\rightarrow\infty,\,u_{ij}\rightarrow\,{\textstyle\frac{1}{N}}\)</span>时。<br/>​  提出了一种新的具有自适应损失函数的FKM代价函数，如下[41]：<span class="math display">\[\sum_{i=1}^{N}\sum_{j=1}^{k}u_{ij}{\frac{(1+\sigma)||{\bf x}_{i}-{\bf c}_{j}||_{2}^{2}}{||{\bfx}_{i}-{\bf c}_{j}||_{2}+\sigma}}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\]</span></p><p>​  可以重写为</p><p><span class="math display">\[\operatorname*{min}_{c_j,u_{ij}}\sum_{i=1}^{N}\sum_{j=1}^{k}u_{i j}\|{\bf x}_{i}-{\bfc}_{j}\|_{\hat{\sigma}}+\gamma u_{i j}\ \mathrm{log}\,u_{ij}\\\mathrm{s.t.}\,\,\sum_{j=1}^{k}u_{i j}=1,0\lt u_{i j}\lt1\]</span></p><p>​  其中，<spanclass="math inline">\(\|\mathbf{M}\|_{\hat\sigma}\)</span>相当于任何向量<spanclass="math inline">\(\mathbf{m}\in\mathbb{R}^{d}\)</span>的<spanclass="math inline">\(\|\mathbf{M^T}\|_{\sigma}\)</span>。<br/>​  在下一节中，我们开发了一个有效的算法来解决上面问题。</p><h2 id="c.-dfkm的代价函数">C. DFKM的代价函数</h2>$$<span class="math display">\[\begin{array}{ll}J_1=||{\bfH}^{(M)}-\mathrm{X}||_{F}^{2}\\J_2=\sum_{i=1}^{N}\sum_{j=1}^{k}u_{ij}||\mathbf{h}_{i}^{({\frac{M}{2}})}-\mathbf{c}_{j}||_{\hat{\sigma}}+\gammau_{i j}\log u_{ij}\\J_3=\sum_{m=1}^{M}||\mathbf{W}^{(m)}||_{F}^{2}+||\mathbf{b}^{(m)}||_{2}^{2}\end{array}\]</span><p>{c}$$</p><p>因此，通过将熵正则化和自适应损失嵌入，提出了DFKM模型 <spanclass="math display">\[\begin{aligned}&amp;\underset{\operatorname{W}^{(m)}, \operatorname{D}^{(m)},\mathbf{C}}{\text{minimize}}&amp; &amp; J_1 + \lambda_{1}J_2 +\lambda_{2}J_3 \\&amp; \text{subject to}&amp; &amp; \sum_{j=1}^{k}u_{ij}=1, \quad 0 &lt; u_{i j} &lt; 1, \quad \forall i\end{aligned}\]</span>其中，λ1和λ2是权衡参数。<spanclass="math inline">\(\mathbf{c}_{j}\in\mathbb{R}^{d^{\prime}}\)</span>是低维特征空间中的第j个簇质心，具有<spanclass="math inline">\(d^{\prime}=d^{(\frac{M}{2})}\)</span>。</p><p>J1、J2和J3被设计为不同的目的。J1保证了重构误差的最小值。j2是问题（10）中提出的具有自适应损失函数的FKM的代价函数。因此，λ1是重建和FKM之间的权衡参数。请注意，如果我们将λ1设置为一个较大的值，即较小的J2，那么由于重建不良，该模型的性能将不会太理想。J3是一种正则化方法，用于避免与正则化参数λ2过拟合的不良事件。术语J3也能够防止声发射生成一个平凡的映射。</p><p>因此，DFKM是将原始数据投影到一个非线性的低维特征空间上，并通过非线性映射特征同时学习一个软聚类隶属度矩阵。</p><h1 id="优化算法">4.优化算法</h1><p>​  在本节中，我们首先开发了一个有效的算法来求解自适应（10）中损失函数的FKM，该算法保证收敛到局部最小值。然后，提出了一种求解（14）中DFKM损失函数的算法。</p><h2 id="a.-加权自适应损失函数的优化">A. 加权自适应损失函数的优化</h2><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215225997.png"alt="image-20241204215225997" /><figcaption aria-hidden="true">image-20241204215225997</figcaption></figure><h2 id="b.-dfkm的优化">B. DFKM的优化</h2><figure><imgsrc="../postimages/Deep-Fuzzy-K-Means-With-Adaptive-Loss-and-Entropy-Regularization/image-20241204215259535.png"alt="image-20241204215259535" /><figcaption aria-hidden="true">image-20241204215259535</figcaption></figure><h1 id="实验">5.实验</h1><h1 id="结论">6.结论</h1><p>​  在本文中，我们提出了一种基于神经网络的聚类方法DFKM，该方法采用了熵正则化和具有自适应权值的鲁棒损失函数。通过结合AE和鲁棒FKM，DFKM将原始数据映射到一个更合适的空间，从而获得更好的性能。换句话说，DFKM同时执行聚类和特征提取，而不是将它们分成两个单独的步骤。此外，隶属度矩阵和质心矩阵通过近似解而不是SGD进行更新，使相应的优化算法快速收敛。大量的实验表明，在三个聚类指标下，我们的模型比目前最先进的模糊聚类算法获得了更好的性能。此外，DFKM还获得了更好的图像分割结果，验证了该模型的优越性。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust deep fuzzy K-means clustering for image data</title>
      <link href="/Robust-deep-fuzzy-k-means-clustering-for-image-data/"/>
      <url>/Robust-deep-fuzzy-k-means-clustering-for-image-data/</url>
      
        <content type="html"><![CDATA[<p>Robust deep fuzzy K-means clustering for image data</p><p>Xiaoling Wu , Yu-Feng Yu<span class="math inline">\(^{a,∗}\)</span> ,Long Chen<span class="math inline">\(^ b\)</span> , Weiping Ding<spanclass="math inline">\(^ c\)</span> , Yingxu Wang<spanclass="math inline">\(^ d\)</span> <br/>a Department of Statistics,Guangzhou University, Guangzhou, China <br/>b Department of Computer andInformation Science, University of Macau, Macau, China <br/>c School ofInformation Science and Technology, Nantong University, Nantong, China<br/>d Shandong Provincial Key Laboratory of Network-Based IntelligentComputing, University of Jinan, Jinan, China</p><p><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0031320324002553"><imgsrc="https://img.shields.io/badge/PR-2024-yellow" alt="PR" /></a></p><h1 id="摘要">摘要</h1><p>​  图像聚类是计算机视觉中的一项艰巨任务，具有重要的应用价值。这项任务的关键是图像特征的质量。目前，大多数的聚类方法都面临着这一挑战。也就是说，特征学习和聚类的过程是独立运行的。为了解决这个问题，一些研究人员已经致力于一起进行特征学习和深度聚类。然而，所获得的特征缺乏成功处理高维数据的可鉴别性。为了解决这一问题，我们提出了一种新的鲁棒深度模糊𝐾-means聚类（RD-FKC，robustdeep fuzzy 𝐾-meansclustering）模型，该模型有效地将图像样本投影到一个具有代表性的嵌入空间中，并将隶属度精确地学习到一个组合框架中。具体来说，RD-FKC引入了拉普拉斯正则化技术来保持数据的局域性。此外，通过使用自适应损失函数，该模型对不同类型的异常值具有更强的鲁棒性。此外，为了避免潜在空间的扭曲，使提取的特征尽可能地保留原始信息，该模型引入了重构误差，并对网络参数进行了正则化处理。最后，给出了一种求解优化模型的有效算法。我们已经进行了大量的实验，说明了RD-FKC相对于现有的聚类方法的优势和优越性。</p><h1 id="介绍">1.介绍</h1><p>​  本文提出了一种新的深度聚类模型，即鲁棒深度模糊𝐾-均值聚类（RD-FKC），该模型将模糊聚类和深度卷积自动编码器（DCAE）集成到一个统一的框架中。图1显示了RD-FKC的框架。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20250106112717777.png"alt="image-20250106112717777" /><figcaption aria-hidden="true">image-20250106112717777</figcaption></figure><p>​  具体来说，我们使用拉普拉斯正则化来约束隶属度矩阵，并在嵌入特征空间中执行FKM，这不仅可以保证使用DCAE提取高维图像数据中包含的复杂和抽象信息，还可以获得局部信息，提高聚类性能。受[29]的启发，我们的模型引入了自适应损失函数，使聚类过程更加鲁棒。本文的主要贡献包括：</p><ul><li>将聚类嵌入到深度卷积自动编码器中，使DCAE能够学习有区别的和有代表性的特征进行聚类，然后聚类结果进而促进特征学习。也就是说，RD-FKC的目标是同时进行特征学习和聚类。</li><li>利用拉普拉斯正则化方法对隶属度矩阵进行约束，使从相似样本中学习到的隶属度也相互关联。也就是说，隶属度之间的联系与约束下的样本一致，可以进一步保存图像的局部信息。</li><li>RD-FKC将自适应损失函数引入到统一的框架中，可以减少各种异常值的影响，有助于增强聚类的鲁棒性。</li><li>提出了一种有效的算法来优化该框架。同时，该框架是直接端到端训练的，而不需要任何繁琐的预训练过程。一系列的比较实验证实了该模型的有效性和优越性。</li></ul><p>​  本文后续部分的组织结构如下。第2节给出了关于模糊𝐾-均值聚类和深度卷积自动编码器的一些相关工作。第3节阐述了所提出的RDFKC模型和有效算法。比较实验和结果分析详见第4节。最后，我们将在第5节中结束该工作。</p><h1 id="相关工作">2.相关工作</h1><h2 id="符号">2.1.符号</h2><p>​  在本文中，我们定义一个向量与一个粗体小写字母，例如𝐱，矩阵和一个粗体大写字母，例如𝐗，<spanclass="math inline">\(𝐱_𝑖\)</span>表示矩阵𝐗的第<spanclass="math inline">\(i\)</span>列，<spanclass="math inline">\(𝐱^𝑖\)</span>表示它的第<spanclass="math inline">\(i\)</span>行，<spanclass="math inline">\(𝑥_{𝑖𝑗}\)</span>是矩阵𝐗的第<spanclass="math inline">\(i\)</span>行和第<spanclass="math inline">\(j\)</span>列元素。将<span class="math inline">\(\{\bfX_{n}\}_{n=1}^{N}\)</span>作为具有𝑁张图像的图像集，提取的相关特征为<spanclass="math inline">\(\mathbf{Z}=\left[\mathbf{z}_{1},\mathbf{z}_{2},\ldots,\mathbf{z}_{N}\right]\in\mathbb{R}^{d\timesN}\)</span>。这里，𝑑是嵌入式空间的维数。RD-FKC的目的是获得区别表示𝐙，并将其聚类为𝐶组。将<spanclass="math inline">\(\mathbf{U}=[\mathbf{u}_{1}^{T},\mathbf{u}_{2}^{T},\ldots,\mathbf{u}_{N}^{T}]^{T}\in\mathbb{R}^{N\timesC}\)</span>和<span class="math inline">\({\bf V}=[{\bf v}_{1},{\bfv}_{2},\dots,{\bf v}_{C}]\,\in\,\mathbb{R}^{d\timesC}\)</span>分别定义为隶属度矩阵和簇中心矩阵。表1列出了更多的字符，并解释了它们在首次被介绍时的含义。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203094934792.png"alt="image-20241203094934792" /><figcaption aria-hidden="true">image-20241203094934792</figcaption></figure><h2 id="模糊𝐾-means聚类">2.2.模糊𝐾-means聚类</h2><p>​  模糊𝐾-means聚类[4]是𝐾-means的一个模糊版本，其目的是构造隶属度矩阵，然后温和地将样本划分为相应的类别。FKM的目标函数定义为：<spanclass="math display">\[\begin{array}{l}{\operatorname*{min}\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{m}||\mathbf{x}_{n}-\mathbf{v}_{c}||_{2}^{2}}\\{\mathrm{s.t.}\quad\sum_{c=1}^{C}u_{nc}=1,0\leq u_{n c}\leq1}\end{array}\]</span> ​  其中，<spanclass="math inline">\(m(m&gt;1)\)</span>是一个模糊化参数。<spanclass="math inline">\(𝐱_𝑛\)</span>表示第n个样本点。<spanclass="math inline">\({\textbf{U}}={\lbrack{u_{n c}\rbrack}_{N\timesC}}\)</span>为隶属度矩阵，<spanclass="math inline">\(𝑢_{𝑛𝑐}\)</span>表示分配给第n个样本点的第𝑐类的隶属度。<spanclass="math inline">\(\mathbf{V}=\left[\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{C}\right]\)</span>是集群中心矩阵，<spanclass="math inline">\(\mathbf{v}_{c}\,\in\,\mathbb{R}^{d}\)</span>是第𝑐个集群。<br/>​  然后，利用拉格朗日乘子，根据以下公式交替更新<spanclass="math inline">\(𝑢_{𝑛𝑐}\)</span>和<spanclass="math inline">\(\mathbf{v}_{c}\)</span>： <spanclass="math display">\[u_{n c}=\frac{1}{\sum_{l=1}^{C}(\frac{||{\bfk}_{n}-{\bf v}_{c}\,||_{2}^{2}}{||{\bf k}_{n}-{\bfv}_{l}||_{2}^{2})})^\frac{2}{m-1}}\]</span></p><p><span class="math display">\[\mathbf{v}_{c}={\frac{\sum_{n=1}^{N}u_{nc}^{m}\mathbf{x}_{n}}{\sum_{n=1}^{N}u_{n c}^{m}}}\]</span></p><h2 id="深度卷积自动编码器dcae">2.3.深度卷积自动编码器（DCAE）</h2><p>​  一个经典的自动编码器通常是由全连接的层组成的，它忽略了图像的结构，并进一步引入了大量的参数。与之相比，DCAE[30]具有本地连接和权重共享的特点，更适合用于图像处理任务。为了利用图像中包含的空间信息，给定一个单通道图像𝐗，将DCAE的编码器和解码器定义为：<span class="math display">\[{\bf G}^{p}={\cal F}_{e}({\bf X}*{\bfW}^{p}+a^{p})\]</span></p><p><span class="math display">\[\hat{\bf X}={\cal F}_{d}(\sum_{p}{\bfG}^{p}*\tilde{\bf W}^{p}+b)\]</span></p><p>​  其中，𝐆𝑝为𝑝th特征映射，∗为卷积运算。<spanclass="math inline">\({\cal F_e}\)</span>和<spanclass="math inline">\({\calF_d}\)</span>是非线性激活函数（我们在实验中使用ReLU）。下标𝑒和𝑑分别表示编码器和解码器。<spanclass="math inline">\({\bf W}^{p}\)</span>是过滤器，<spanclass="math inline">\(\tilde{\bfW}^{p}\)</span>是翻转操作，可以将嵌入表示恢复到原始大小的位置。<spanclass="math inline">\(𝑎^𝑝\)</span>和𝑏是相应的偏差。<br/>​  DCAE通过最小化重构误差来更新编码器和解码器的参数：<spanclass="math display">\[\mathcal{L(\Delta)}=\frac{1}{N}\sum_{n=1}^{N}\|F_{d}(F_{e}(\mathbf{X}_{n}))-\mathbf{X}_{n}\|_{F}^{2}\]</span>​  其中𝑁为图像数量，<spanclass="math inline">\(\Delta\)</span>包含所有网络参数。‖⋅‖𝐹表示一个矩阵的Frobenius范数。</p><h1 id="鲁棒的深度模糊𝑲-means聚类">3.鲁棒的深度模糊𝑲-means聚类</h1><h2 id="自适应损耗函数">3.1.自适应损耗函数</h2><p>​  假设𝐇是一个任意矩阵，<spanclass="math inline">\(l_{2,1}\)</span>规范表示为<spanclass="math inline">\(\|\mathbf{H}\|_{2,1}=\sum_{i}||\mathbf{h}^{i}||_{2}\)</span>对大损失很鲁棒，但对小损失很脆弱，而平方Frobenius规范表示为<spanclass="math inline">\(\|\mathbf{H}\|_{F}^{2}\ =\\sum_{i}\|\mathbf{h}^{i}\|_{2}^{2}\)</span>很容易解决，对小损失稳健，但对大损失敏感。因此，一个结合了这两种优点的自适应损失函数被定义为：<spanclass="math display">\[||\mathbf{H}||_{\tau}=\sum_{i}{\frac{(1+\tau)||\mathbf{h}^{i}||_{2}^{2}}{\|\mathbf{h}^{i}\|_{2}+\tau}}\]</span>​  这里的𝜏&gt;0是一个自适应参数。如果𝜏→0，‖𝐇‖𝜏→‖𝐇‖2,1。如果𝜏→∞，‖𝐇‖𝜏→‖𝐇‖2𝐹。此外，‖𝐇‖𝜏具有良好的二微、非负、凸性质，有利于优化。因此，‖𝐡‖𝜏适用于损失函数，并通过调优𝜏对不同类型的异常值具有鲁棒性。<br/>​  根据以上分析，在FKM算法(1)中引入了自适应损失函数(7)，极大地提高了聚类的鲁棒性如下：<spanclass="math display">\[\begin{array}{l}{\operatorname*{min}\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}||\mathbf{x}_{n}-\mathbf{v}_{c}||_{\tau}}\\{\mathrm{s.t.}\quad\sum_{c=1}^{C}u_{nc}=1,0\leq u_{n c}\leq1}\end{array}\]</span></p><h2 id="对象的拉普拉斯正规化">3.2.对象的拉普拉斯正规化</h2><p>​  图的拉普拉斯算子确保了从彼此连接的样本中学习到相似的隶属度。同时，它还可以利用数据的局部性信息。因此，我们将拉普拉斯正则化纳入聚类模型。它可以根据样本𝐱𝑖和𝐱𝑗之间的相似性来实现。我们使用热核方案[31]来构造相似度如下：<spanclass="math display">\[s_{ij}=\begin{cases}e^{-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|}{0.5}}&amp; j\in NB_i \\0, &amp; \mathrm{otherwise} &amp; \end{cases}\]</span>​  其中，<spanclass="math inline">\(NB_i\)</span>为𝑖th样本的邻域集。然后计算<spanclass="math inline">\(𝐒=(𝐒+𝐒^𝑇)/2\)</span>，得到对称相似矩阵。结合(8)和(9)，我们可以得到以下模型：<spanclass="math display">\[\mathcal{L}(\mathbf{U},\mathbf{V})=\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}\left\|\mathbf{x}_{n}-\mathbf{v}_{c}\right\|_{\tau}+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{jc})^{2}\\{\mathrm{s.t.}}\;\sum_{c=1}^{C}u_{n c}=1,0\leq u_{nc}\leq1\]</span></p><h2 id="rd-fkc算法">3.3.RD-FKC算法</h2><p>​  给定图像<span class="math inline">\(\{\bfX_{n}\}_{n=1}^{N}\)</span>，编码器首先尝试提取层次特征，然后将图像映射到一个非线性嵌入空间，以获得潜在的潜在表示𝐙。相比之下，解码器会将该特征重构回原始图像中。<br/>​  为了保证学习到的低维空间不被扭曲和破坏，RD-FKC模型结合了DCAE网络，将具有拉普拉斯正则化的鲁棒FKM损失函数嵌入到一个联合框架中，如下所示：$$<em>{A.}</em>{n=1}^{N}|{}<em>{n}-</em>{n}|<em>{F}<sup>{2}+<em>{n=1}^{N}</em>{c=1}</sup>{C}u</em>{nc}^{2}|<em>{n}-</em>{c}|_{}</p><p>+~<em>{c=1}^{C}</em>{n=1}^{N}<em>{jN B</em>{n}}s_{n j}(u_{n c}-u_{jc})<sup>{2}+<em>{}<sup>{L}(|</sup>{()}|</em>{F}</sup>{2}+|<sup>{()}|<em>{2}^{2})\{} </em>{c=1}</sup>{C}u_{nc}=1,0u_{n c}$$​  其中，𝜇和𝛾是权衡参数。建立RD-FKC模型可分为四个部分（11）。第一项是最小化重构误差，以确保潜在的表示法尽可能多地保留原始信息。第二项表示由嵌入空间中的嵌入特征和聚类中心组成的自适应损失函数的FKM。第三项是约束隶属度矩阵，使具有相似隶属度的样本点也更接近。第四项用于避免过拟合，防止网络产生平凡解。</p><h2 id="最优化">3.4.最优化</h2><p>​  在（11）中，RD-FKC通过使用结合<spanclass="math inline">\(\ell_{1}\)</span>范数和<spanclass="math inline">\(\ell_{2}\)</span>范数的自适应损失函数来计算聚类误差。为了解决这个问题，我们首先考虑一个一般损失函数定义为：<spanclass="math display">\[\operatorname*{min}_{\mathbf{x}}g(\mathbf{x})+\sum_{i}{\frac{(1+\tau)\|d_{i}(\mathbf{x})\|_{2}^{2}}{\|d_{i}(\mathbf{x})\|_{2}+\tau}}\]</span>​  其中，<spanclass="math inline">\(d_{i}(\mathbf{x})\)</span>为向量输出。受[29,32]的启发，采用了一种迭代重加权的方法来求解它。通过对𝐱进行（12）的导数并将其设为零，我们得到<span class="math display">\[g^{\prime}({\bfx})+2(1+\tau)\sum_{i}\frac{\|d_{i}({\bf x})\|_{2}+2\tau}{2(\|d_{i}({\bfx})\|_{2}+\tau)^{2}}d_{i}({\bf x})d_{i}^{\prime}({\bf x})=0\]</span>​  定义 <span class="math display">\[k_{i}=(1+\tau)\frac{\|d_{i}({\bfx})\|_{2}+2\tau}{2(\|d_{i}({\bf x})\|_{2}+\tau)^{2}}\]</span>​  然后是等式（13）可以重写为 <spanclass="math display">\[g^{\prime}({\bf x})+2\sum_{i}k_{i}d_{i}({\bfx})d_{i}^{\prime}({\bf x})=0\]</span>​  需要注意的是，𝐾𝑖的值依赖于𝐱，当𝑘𝑖被固定时，（12）中的问题等于 <spanclass="math display">\[\operatorname*{min}_{\mathbf{x}}g(\mathbf{x})+\sum_{i}k_{i}\|d_{i}(\mathbf{x})\|_{2}^{2}\]</span>​  在[29]中证明了上述优化问题的收敛性。则（11）中的模型可以重新表述为：<spanclass="math display">\[\operatorname*{min}_{A.\mathrm{U.V}}\quad{\frac{1}{N}}\sum_{n=1}^{N}\|{\hat{\mathbf{X}}}_{n}-\mathbf{X}_{n}\|_{F}^{2}+\sum_{n=1}^{N}\sum_{c=1}^{C}u_{nc}^{2}k_{nc}\left\|\mathbf{z}_{n}-\mathbf{v}_{c}\right\|_{\tau}\\+~\mu\sum_{c=1}^{C}\sum_{n=1}^{N}\sum_{j\inN B_{n}}s_{n j}(u_{n c}-u_{jc})^{2}+\gamma\sum_{\ell=1}^{L}(\|\mathbf{W}^{(\ell)}\|_{F}^{2}+\|\mathbf{b}^{(\ell)}\|_{2}^{2})\\{\mathrm{s.t.}}\\sum_{c=1}^{C}u_{n c}=1,0\leq u_{n c}\leq1\]</span> ​  其中 <spanclass="math display">\[k_{nc}=(1+\tau)\frac{\|\mathbf{z}_{n}-\mathbf{v}_{c}\|_{2}+2\tau}{2(\|\mathbf{z}_{n}-\mathbf{v}_{c}\|_{2}+\tau)^{2}}\]</span>​  可以看出，在模型（17）中有三个变量，很难直接解决这些问题。因此，我们提出了一个迭代的替代策略来优化它。</p><p><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112655933.png"alt="image-20241203112655933" /> $$</p><p>$$</p><h1 id="实验结果及分析">4.实验结果及分析</h1><p>​  在本节中，所提出的RD-FKC模型的性能估计是基于8个图像集，使用三个公共指标，包括准确率（ACC）、归一化互信息（NMI）和纯度（Pur）。为了评估RD-FKC的能力，我们将其与现有的聚类技术进行了比较，如𝐾-means聚类（KM）[3]、模糊𝐾-means聚类（FKM）[4]、RSFKM[34]，PCAKM，深度嵌入式聚类（DEC）[22]，深度聚类网络（DCN）[35]和深度模糊𝐾-means（DFKM）[36]。<br/>​  在接下来的章节中，我们首先列出图像数据集的特征，然后给出实验实现的细节。此外，我们还报告了RD-FKC与其他方法相比的性能。最后，分析了各参数的灵敏度。</p><h2 id="数据集">4.1.数据集</h2><p>​  实验中使用了八种不同类型的图像集，包括人脸、手写数字、物体和时尚数据集。表2详细描述了这些图像集的特征，图2显示了我们实验中的几个样本。</p><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112902796.png"alt="image-20241203112902796" /><figcaption aria-hidden="true">image-20241203112902796</figcaption></figure><figure><imgsrc="../postimages/Robust-deep-fuzzy-k-means-clustering-for-image-data/image-20241203112925600.png"alt="image-20241203112925600" /><figcaption aria-hidden="true">image-20241203112925600</figcaption></figure><p>​  人脸上包括Jaffe、Yale、ORL和CMU-PIE。Jaffe包含213个26个×26的样本，来自10个人，有7种不同的表达。Yale有165张32张×32像素的图像。ORL由40人在不同光照条件下的400张图像组成。CMU-PIE由64人的32×32像素的图像组成。USPS是一个手写的数字数据集，包括9298个16×16像素的样本。对象数据库包含COIL20,128个×128像素的20个对象，CIFAR10包含60000的10类彩色图像。Fashion-MNIST是MNIST的替代品，包括10个类别的70000张时尚产品图片。</p><h2 id="网络设置">4.2.网络设置</h2><h2 id="聚类结果的比较">4.3.聚类结果的比较</h2><h2 id="对鉴别力和表现形式的可视化">4.4.对鉴别力和表现形式的可视化</h2><h2 id="参数分析">4.5.参数分析</h2><h1 id="结论">5.结论</h1><p>​  在这项工作中，我们提出了一种新的称为鲁棒深度模糊𝐾-means聚类（RD-FKC）的框架，该框架利用拉普拉斯正则化来保持输入图像的局部结构，并使用自适应损失函数通过调整𝜏来提高聚类模型的弹性。RD-FKC通过将DCAE和聚类模型整合到一个联合框架中，充分利用DCAE学习图像的判别和深度表示，然后进行鲁棒聚类，有助于解决图像聚类问题。此外，还提出了一种有效、实用的模型优化算法。在不同图像集上进行的大量可比实验表明，RD-FKC优于现有的传统和深度聚类方法。</p><p>​  可以观察到，RD-FKC在CIFAR10等彩色图像上表现不佳。可能的原因是局部结构信息没有得到有效的保存。因此，在今后的工作中，我们将考虑将𝑝-拉普拉斯正则化技术[39]嵌入到聚类模型中，从而更好地实现局部信息的保存。由于引入了一个额外的超参数𝑝，我们将进一步考虑集成𝑝-拉普拉斯正则化[40]，它对各种𝑝值图应用适当的权值，以更好地理解数据的几何形状。此外，我们还将考虑如何设计一个更先进的深度卷积自动编码器网络来提取特征，从而进一步提高聚类效率。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust deep k-means:An effective and simple method for data clustering</title>
      <link href="/Robust-deep-k-means/"/>
      <url>/Robust-deep-k-means/</url>
      
        <content type="html"><![CDATA[<p>Robust deep k-means: An effective and simple method for dataclustering</p><h1 id="摘要">摘要</h1><p>​  聚类的目的是根据一些距离或相似度度量将输入数据集划分为不同的组。k-means算法由于其简单、高效，是目前应用最广泛的聚类方法之一。在过去的几十年里，k-means及其各种扩展来解决实际的聚类问题。然而，现有的聚类方法通常采用单层公式（即浅层公式）。因此，所获得的低级表示与原始输入数据之间的映射可能包含相当复杂的层次信息。为了克服低层次特征的缺点，采用深度学习技术提取深度表示，提高聚类性能。在本文中，我们提出了一个鲁棒的深度k-means模型来学习与不同隐式低级属性相关的隐藏表示。通过使用深度结构分层地执行k-means，可以分层地利用数据的分层语义。来自同一个类的数据样本被迫逐层地靠近，这有利于聚类任务。我们的模型的目标函数被推导为一个更可跟踪的形式，这样优化问题可以更容易地解决，并可以得到最终的鲁棒结果。在12个基准数据集上的实验结果表明，与经典方法和最先进的方法相比，该模型在聚类性能方面取得了突破。</p><h1 id="介绍">1.介绍</h1><p>​  尽管上述k-means方法取得了显著的进展，但这些方法通常是用单层公式设计的。因此，所获得的低维表示与原始输入数据之间的映射可能包含相当复杂的层次信息。考虑到深度学习的发展，需要采用多个处理层来提取数据[29]的层次信息，本文提出了一种新的鲁棒深度k-means模型来利用多层次属性的层次信息。我们的模型的总体框架如图1所示。</p><figure><img src="../postimages/Robust-deep-k-means/image-20250107102610266.png"alt="image-20250107102610266" /><figcaption aria-hidden="true">image-20250107102610266</figcaption></figure><p>​  正如我们所看到的，通过使用深度结构来分层地执行k-means，数据的分层语义可以被分层地利用。也就是说，来自同一类的数据样本逐层地收集，非常有利于聚类任务。<br/>​  这项工作的主要贡献有三个方面：</p><ul><li>提出了一种新的鲁棒深度模型来分层地执行k-均值，从而可以分层地探索数据的分层语义。因此，来自同一类的数据样本可以有效地逐层收集，从而提供了一个清晰的聚类结构。</li><li>为了求解模型的优化问题，将相应的目标函数推导为更可跟踪的形式，并提出了一种替代的更新算法来求解优化问题。</li><li>在12个基准数据集上进行了实验，并显示了与经典和最先进的方法相比的良好结果。</li></ul><p>​  本文的基础构思如下。我们将在第2节中简要介绍与密切相关的工作。我们的模型的细节见第3节。实验结果见第4节。最后，我们在第5节中给出了本文的结论。</p><p>​  这项工作和我们早期的论文[1]之间有三个不同之处。<br/>​  首先，我们提出了鲁棒深度kkmeans模型的一般形式。详细地说，我们部署了一系列的散度函数来测量重构误差（第3节），而不仅仅是对噪声数据和异常值敏感的Frobenius范数。因此，我们早期的工作[1]只是本文的一个特例。其次，在12个基准数据集上进行了更全面的实验，验证了我们模型的鲁棒性和有效性：(i)记录了更多数据集和高级基线上的聚类结果（第4.3节）；（ii）添加不同参数设置的实验结果（第4.4节）；（iii）展示收敛分析实验（第4.5节）；（iv）研究不同发散函数对聚类性能的影响（第4.6节）。第三，我们介绍了更密切相关的文献（在第1节和第2节中），澄清了它们与最先进的技术之间的联系和差异。这有助于在社区中更好地定位拟议的工作。</p><h1 id="前期准备工作">2.前期准备工作</h1>​  非负矩阵分解（NMF）由于其直观的基于部分的解释[30,31]，在数据聚类中受到了广泛的关注。以往的研究表明，在松弛条件[30]下，NMF基本上等于<em>k</em>-means。假设<spanclass="math inline">\(\mathbf{X}=\left[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n}\right]\in\mathbb{R}^{m\timesn}\)</span>是一个具有n个数据样本和m个特征的非负数据矩阵。NMF的目标是找到两个非负矩阵<spanclass="math inline">\(\mathbf{U}\in\mathbb{R}^{m\timesc}\)</span>和<spanclass="math inline">\(\mathbf{V}\in\mathbb{R}^{n\timesc}\)</span>，使<spanclass="math inline">\(\mathbf{X}\approx\mathbf{UV}^{T}\)</span>，NMF的一般形式为$$<span class="math display">\[\begin{array}{l}{\cal D}_{\beta}(X|\mathrm{UV}^{T}\bigr)=\sum_{i=1}^{m}\sum_{j=0}^{n}d_{\beta}(X_{i j}|igl(\mathrm{U}^{T}\bigr)_{i j}\bigr)\\ {s.t.{\bfU}\geq0,{\bf V}\geq0,}\end{array}\]</span><p>$$ ​  其中<spanclass="math inline">\({\mathcal{D}}_{\beta}(\mathbf{X}|{\hat{\mathbf{X}}})\)</span>表示一个标量代价函数（即<spanclass="math inline">\(\mathbf{X}\)</span>与其重建<spanclass="math inline">\(\hat{\mathbf{X}}\)</span>之间的一些发散度量），<spanclass="math inline">\(\mathbf{X_{ij}}\)</span>是<spanclass="math inline">\(\mathbf{X}\)</span>的第<spanclass="math inline">\(ij\)</span>个元素。在公式(1)，可以采用一类称为β-散度[33]的散度函数。在NMF中使用最广泛的有三个散度函数：</p><p>​  ∗β=2（欧氏距离）：<spanclass="math inline">\(d_{2}(a|b)={\frac{1}{2}}(a-b)^{2}\)</span><br/>​  ∗β=1（Kullback–LeiblerDivergence）：<spanclass="math inline">\(d_{1}(a|b)=a\log{\frac{a}{b}}-a+b\)</span><br/>​  *β=0（Itakura–SaitoDivergence）：<spanclass="math inline">\(d_{0}(a|b)=\frac{a}{b}-\log{\frac{a}{b}}-1\)</span></p><p>​  [32]采用了等式(1)中的欧氏距离： <spanclass="math display">\[\begin{array}{l} {J_{N MF}=\sum_{i=1}^{m}\sum_{i=1}^{n}\left(X_{i j}-({\bf U}V^{T})_{ij}\right)^{2}=\|{\bf X}-{\bf U}V^{T}\|_{F}^{2}}\\ {s.t.{\bf U}\geq0,{\bfV}\geq0,}\end{array}\]</span> ​  其中，<spanclass="math inline">\(\|\cdot\|_{F}\)</span>表示Frobenius范数。[32]进一步指出，等式(2)是一个双凸公式（仅在U或V中为凸），并通过应用更新规则搜索局部最小值如下：</p><p><span class="math inline">\(\mathbf{U}_{i j}\leftarrow \mathbf{U}_{ij} \frac{(\mathbf{X V})_{i j}}{(\mathbf{U V ^{T} V})_{i j}}\)</span>，<span class="math inline">\(\mathbf{V}_{i j}\leftarrow \mathbf{V}_{i j}\frac{(\mathbf{X U^{T}})_{i j}}{(\mathbf{V U ^{T} U})_{ij}}\)</span></p><p>​  其中，<spanclass="math inline">\(\mathbf{V}\)</span>可以看作是聚类指标矩阵[30]，<spanclass="math inline">\(\mathbf{U}\)</span>表示质心矩阵，c表示聚类数。通常，我们有<spanclass="math inline">\(c\ll n\)</span>和<span class="math inline">\(c\llm\)</span>，意思是等式(2)实际上是搜索<spanclass="math inline">\(\mathbf{X}\)</span>的低维表示<spanclass="math inline">\(\mathbf{V}\)</span>。<br/>​  但在现实中，数据集通常是复杂的，并且总是包含多种层次模式（即因素）。以人脸数据集为例，它通常由一些常见的模式组成，如表达式、姿态、场景等。因此，很明显，基于单层的NMF不能充分利用不同因素下的隐藏信息。为了填补这一空白，[34]研究了一个多层深度模型，该模型通过进行semi-NMF的分层处理，创新性地探索了数据的分层信息。并定义了深度semi-NMF模型的基本公式为<span class="math display">\[\begin{array}{r l}{X}&amp;{\approxU_{1}V_{2}^{T},}\\ {X}&amp;{\approx U_{1}U_{2}V_{2}^{T},}\\&amp;{\vdots}\\ {X}&amp;{\approxU_{1}U_{2}\dots{U}_{r}V_{r}^{T}.}\end{array}\]</span>​  其中r为层数，<span class="math inline">\(\mathbf U_i\)</span>和<spanclass="math inline">\(\mathbf V_i\)</span>分别为第<spanclass="math inline">\(i\)</span>个层的基矩阵和表示矩阵。很明显，深度semi-NMF的目标也是搜索一个低维的嵌入表示，即最后一层<spanclass="math inline">\(\mathbf V_r\)</span>。通过分层分解每个层<spanclass="math inline">\(\mathbf{V}_{i}(i\ltr)\)</span>，等式(3)能够发现潜在的层次结构。与现有的单层NMF模型相比，深度semi-NMF可以更好地揭示数据的层次信息，因为不同层的低维表示可以识别出不同的模式。因此，我们的模型可以完全实现适合后续不同模式聚类的表示。例如，如图1所示，<spanclass="math inline">\(\mathbf U_3\)</span>对应于表达式的特征，<spanclass="math inline">\(\mathbf{U_2U_3}\)</span>对应于姿态的特征，最后，<spanclass="math inline">\(\mathbf{U=U_1U_2U_3}\)</span>对应于人脸图像的身份映射。这样，就可以获得更好的高可识别性、根据变异性最小的特征进行聚类的最终层表示。</p><h1 id="提出的模型">3.提出的模型</h1><p>​  在本节中，我们提出了一种新的深度k均值模型，称为鲁棒的深度k-means（RDKM）。我们提出了一种有效的更新算法来解决相应的优化问题。并分析了该算法的收敛性。</p><h2 id="鲁棒的深度k-means">3.1.鲁棒的深度k-means</h2><p>​  为了探索不同模态的低维表示，研究了一种新的鲁棒深度k-means模型，利用深度结构分层进行k-means。在本文中，为了扩大我们的模型的适用范围（即同时处理负数据和非负数据），我们省略了对<spanclass="math inline">\(\mathbf U_i\)</span>的非负约束。考虑到<spanclass="math inline">\(V_i\)</span>上的非负性约束使优化问题难以解决，我们通过引入新的变量<spanclass="math inline">\(V_i^+\)</span>，将目标函数转化为更可跟踪的形式。这样，非负性约束条件被分离并等价地采用，约束条件为<spanclass="math inline">\(V_i=V_i^+\)</span>。因此，我们不仅扩展了应用程序，而且还保留了我们的模型的强可解释性。这里我们使用乘子（ADMM）[35]的交替方向方法来求解优化问题。在数学上，所提出的RDMK模型被表述为<span class="math display">\[\begin{array}{l}{J=D_{\beta}(\mathbf{X}|\mathbf{Y})}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  在等式(4)中，我们可以看到在<spanclass="math inline">\(\mathbf{V}_{r}\)</span>的每一行上都采用了1-of-C的编码方案。1-of-C编码方案的主要目标是保证<spanclass="math inline">\(\mathbf{V}_{r}\)</span>的唯一性。此外，基于<spanclass="math inline">\(\mathbf{V}_{r}\)</span>，我们可以直接得到最终的离散划分结果。<br/>​  类似于等式(2)，如果在等式(4)中采用欧氏距离（即β= 2），则我们有 <span class="math display">\[\begin{array}{l}{J=\|\mathbf{X}-\mathbf{Y}\|_{F}^{2}}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  然而，已经证明了Frobenius范数对噪声数据和异常值[36,37]很敏感。为了提高该模型的鲁棒性，我们的模型采用了稀疏性诱导范数（即<spanclass="math inline">\(l_{2,1}\)</span>范数）。根据[36]，<spanclass="math inline">\(l_{2,1}\)</span>范数能够减少异常值的影响，因为它在数据点内执行<spanclass="math inline">\(l_{2}\)</span>范数，在数据点之间执行<spanclass="math inline">\(l_{1}\)</span>范数。最后，我们的鲁棒深度k-means（RDKM）模型被写为<span class="math display">\[\begin{array}{l}{J_{RDKM}=\|\mathbf{X}-\mathbf{Y}\|_{2,1}}\\{\mathrm{s.t.}}\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T},(\mathbf{V}_{r})_{.c}=\{0,1\},\sum_{c=1}^{C}\,(\mathbf{V}_{r})_{.c}=1,\\{\mathbf{V}_{i}=\mathbf{V}_{i}^{+},\mathbf{V}_{i}^{+}\geq0,i\in[1,\ldots,r-1].}\end{array}\]</span>​  正如我们所看到的，<spanclass="math inline">\(\|\mathbf{X}-\mathbf{Y}\|_{2,1}\)</span>相对于Y很容易最小化，而<spanclass="math inline">\(\|\mathbf{X}-\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\|_{2,1}\)</span>相对于<spanclass="math inline">\(\mathbf U_i\)</span>或<spanclass="math inline">\(\mathbfV_i\)</span>来最小化并不是那么简单。乘法更新规则隐式地解决了<spanclass="math inline">\(\mathbf U_i\)</span>和<spanclass="math inline">\(\mathbfV_i\)</span>解耦的问题。在ADMM背景下，一个自然的公式是优化<spanclass="math inline">\(\|\mathbf{X}-\mathbf{Y}\|_{2,1}\)</span>，约束条件为<spanclass="math inline">\(\mathbf{Y}=\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\)</span>。<strong>这就是我们考虑解决方程式（6）这样的问题的原因。</strong><br/>​  为了验证在我们的模型中使用的<spanclass="math inline">\(l_{2,1}\)</span>范数的鲁棒性和有效性，不同的散度函数（即，β= 2，β = 1，和β =0）的情况将在后面的章节中讨论。关于不同发散函数的优化算法也将在附录A中描述。<br/>​  对于等式(6)，提出了一种基于ADMM[35]的有效优化算法。等式(6)的拉格朗日函数是 <spanclass="math display">\[\mathcal{L}(\mathbf{Y},\mathbf{U}_{i},\mathbf{V}_{i},\mathbf{V}_{i}^{+},\mathbf{\mu,\lambda_{i})}=\|\mathbf{X}-\mathbf{Y}\|_{2,1}+\langle\mu,\mathbf{Y}-\mathbf{U}_{1}\mathbf{U}_{2}\dots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\rangle\\+\,{\frac{\rho}{2}}\,\|\mathbf{Y}-\mathbf{U}_{1}\mathbf{U}_{2}\ldots\mathbf{U}_{r}\mathbf{V}_{r}^{T}\|_{F}^{2}+{\sum_{i=1}^{r-1}\left\langle\lambda_{i},\mathbf{V}_{i}-\mathbf{V}_{i}^{+}\right\rangle}+\frac{\rho}{2}\sum_{i=1}^{r-1}\|\mathbf{V}_{i}-\mathbf{V}_{i}^{+}\|_{F}^{2},\]</span>​  其中<span class="math inline">\(\rho\)</span>为一个惩罚参数，<spanclass="math inline">\(\mu\)</span>和<spanclass="math inline">\(\lambda_i\)</span>均表示拉格朗日乘子，而<spanclass="math inline">\(\langle\cdot,\cdot\rangle\)</span>表示内积运算。</p><h2 id="最优化">3.2.最优化</h2><h2 id="收敛性分析">3.3.收敛性分析</h2><h1 id="实验">4.实验</h1><p>​  在本文中，我们通过实验评价了该方法的有效性。我们将12个基准数据集上的RDKM与6个基线进行比较：标准-NMF（SNMF）[38]，-均值[8]，NMF[32]，正交NMF（ONMF）[30]，半2,1-NMF[36]和深度半NMF（DeepSNMF）[34]。</p><h2 id="数据集">4.1.数据集</h2><p>​  在我们的实验中，我们采用了12个基准数据集，包括2个基因表达数据集，4个文本数据集和6个图像数据集。如图所示，图2显示了数据集COIL和MNIST的样本图像。表1总结了所有数据集的具体细节，从中我们可以看到实例数量在102到7094之间，特征数量在256到7511之间，涵盖了广泛的属性。</p><figure><img src="../postimages/Robust-deep-k-means/image-20241202230017892.png"alt="image-20241202230017892" /><figcaption aria-hidden="true">image-20241202230017892</figcaption></figure><h2 id="参数设置">4.2.参数设置</h2><p>​  对于k-means算法，在所有数据集上进行k-means直到收敛。为了进行公平的比较，k-means的结果也被用作其他比较方法的初始化。对于比较的方法，我们设置的参数与每一篇论文中报告的一样。如果没有建议的值，我们将详尽地搜索参数，并使用产生最佳性能的参数。对于我们的RDKM，根据[14,39]，图层的大小（如3.2中所述）被设置为[50C]、[100 C]和[100 50C]。对于参数ρ，我们从{1e5、1e4、1e3、1e2、0.1、1、10、100}中搜索它。为了减少初始化的影响，我们重复了20次实验，并报告了20次重复的平均性能。</p><h2 id="结果及分析">4.3.结果及分析</h2><p>​  表2显示了12个数据集上所有方法在聚类精度（ACC）、归一化互信息（NMI）和纯度等方面的聚类性能。</p><figure><img src="../postimages/Robust-deep-k-means/image-20241202230047232.png"alt="image-20241202230047232" /><figcaption aria-hidden="true">image-20241202230047232</figcaption></figure><p>​  可以看出，该方法在大多数情况下都优于其他算法。具体来说，对于ACC，我们的模型在12个数据集中获得了11倍的最佳结果。对于NMI，我们的模型达到了10次的最佳结果。对于纯度，这个数字也是10。总之，其聚类性能足以验证所提模型的有效性。RDKM的优越性表明，通过探索数据的层次语义来发现更好的集群结构是有益的。其原因是，通过应用深度框架进行k-means的分层执行，可以分层利用数据的分层信息，最后为聚类任务获得更好的高可识别性、最终层表示。通过巧妙地结合深度框架和k-means模型，我们的模型能够在一般情况下提高聚类性能。<br/>​  根据本文报道的理论分析和实证结果，可以看出，将深度结构学习和经典机器学习模型结合成一个统一的框架将是一个有趣的研究趋势。</p><h2 id="参数讨论">4.4.参数讨论</h2><p>​  在我们的模型中，有两个参数，图层大小和惩罚参数ρ，需要进行调整。在这里，我们研究了在不同的参数设置下的聚类性能。如图3所示，我们可以发现聚类性能对于不同的层大小设置是稳定的，而性能对惩罚参数ρ有点敏感。对于图像数据集（如Yale32、ORL32、COIL和MSRA），当ρ在[1e3,1e1]范围内时，可以得到更好的结果。对于基因表达数据集（如Lungml），当ρ在该范围内时，可以获得更好的结果[1e2,1]。而对于文本数据（例如，Cranmed），在[1e5,1e3]范围内搜索ρ将是一个更好的选择。一般来说，我们可以在[1e3,1]范围内搜索参数ρ，以获得相对较好的性能。</p><h2 id="收敛性分析-1">4.5.收敛性分析</h2><p>​  在本小节中，我们通过经验展示了我们的方法收敛的速度。图4为我们的RDKM的收敛曲线，其中x轴为迭代次数，y轴为目标值。可以观察到，我们的RDKM的更新规则收敛得非常快，通常在100次迭代内。对于数据集MSRA，它甚至在10次迭代内收敛，进一步证明了所提出的优化算法的有效性。</p><h2 id="散度函数的选择">4.6.散度函数的选择</h2><p>​  如第2节所述，可以采用几种广泛使用的散度函数来进行残差计算。我们在之前的实验中使用了l2,1-范数。在本节中，我们通过实证研究了不同的发散函数对聚类性能的影响。从方程式(6)和(7)中可以看出，当散度函数发生改变时，只有更新Y的步骤才会发生改变。也就是说，对于不同的发散函数，除Y之外的所有变量的更新都是相同的。在附录A中给出了β=2（欧几里得距离）、β=1（库背-莱布勒发散）和β=0（岩仓-斋藤发散）时的更新规则。<br/>​  对不同散度函数的聚类性能如图5所示。为了进行比较，我们还记录了我们的模型（即基于12,1-范数的发散函数）的结果。很明显，l2,1-norm在所有数据集上都优于其他发散函数，这再次验证了我们的模型的鲁棒性。对于β= 2、β = 1和β = 0这三种情况，我们可以看到β =1在数据集Yale32上获得了良好的结果，β =0在数据集Lungml上获得了良好的结果，而β =2在数据集ORL32和COIL上取得了更好的结果。也就是说，不同的散度函数在不同的数据集上得到更好的结果。一般来说，l2,1-norm可能是一个更好的选择，因为它始终取得良好的性能。</p><h1 id="结论">5.结论</h1><p>​  在本文中，我们引入了一个鲁棒的深度k-means模型来学习具有不同隐式低级属性的隐表示。通过使用深度结构分层地执行k-means，可以分层地利用数据的分层语义。来自同一个类的数据样本被迫逐层地靠近，这有利于聚类任务。我们的模型的目标函数被推导为一个更可跟踪的形式，这样优化问题可以更容易地解决，并可以得到最终的鲁棒结果。超过12个基准数据集的实验结果表明：(i)与经典和最先进的方法相比，该模型在聚类性能方面取得了突破；（ii）聚类性能对不同的层大小设置和发散函数的聚类性能具有鲁棒性；（iii）提出的优化算法有效，收敛速度快。在我们未来的工作中，将我们的深度模型和其他机器学习模型（例如，内核学习和分类方法）结合到一个统一的框架中将是很有趣的。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>K-means clustering algorithms:A comprehensive review, variants analysis, and advances in the era of big data</title>
      <link href="/K-means-clustering-algorithms/"/>
      <url>/K-means-clustering-algorithms/</url>
      
        <content type="html"><![CDATA[<p>K-means clustering algorithms: A comprehensive review, variantsanalysis, and advances in the era of big data</p><h1 id="摘要">摘要</h1><p>​  在大数据时代，最近的科学数据收集技术的进步允许在各种数据采集地点系统地积累大量数据。同样，不同数据分析方法的指数增长，其中K-means算法仍然是最流行和最直接的聚类算法。该算法在许多聚类应用领域的广泛适用性可以归因于其实现的简单性和计算复杂度低。然而，K-means算法存在许多对其聚类性能的负面挑战。在算法的初始化过程中，用户必须随机选择给定数据集中的集群数量，而初始集群中心是随机选择的。此外，该算法的性能易受初始聚类选择的影响，对于大型数据集，确定最优的集群数量变得复杂，是一项非常具有挑战性的任务。此外，由于初始聚类中心的随机选择，有时会导致最小的局部收敛。进一步的限制是，某些数据对象特征通过使用欧氏距离度量作为相似性度量来确定其相似性，但这限制了算法在检测其他聚类形状时的鲁棒性，并对检测重叠聚类提出了很大的挑战。关于提高K-means算法的性能和鲁棒性，已经在文献中进行了许多研究和报道。目前的工作提出了K-means聚类算法及其变体的概述和分类。本文还讨论了k均值的历史、当前的趋势、开放的问题和挑战，以及未来的研究前景。</p><h1 id="介绍">1.介绍</h1><p>​  从收集的数据中提取有意义和有形的信息是数据挖掘[4]的主要目标。然而，大多数数据都是以任意的形式和类别收集的，这使得这些数据难以分析，特别是当数据对象的特征未知时。未标记数据的适当组织是由聚类分析处理的数据挖掘的一个方面。将这些未标记的数据进行有意义的分组视为数据聚类。其目标是对未标记的数据进行分组，使其特征和属性相似的数据对象聚集在一个集群中，从而使同一集群中的数据对象的相似性比其他集群中的数据对象的相似性更高。换句话说，数据聚类分析对未标记数据进行分类，以确保较高的簇内相似度和较低的簇间相似度[59]。聚类分析的过程可以比作学习过程，当处理无标记数据集[55]时，它涉及到与无监督学习相关的特定预测行为。图1清楚地说明了模式识别和机器学习中感兴趣的不同类别的学习问题，如Jain[95]中所讨论的。</p><figure><imgsrc="../postimages/K-means-clustering-algorithms/image-20241202203941768.png"alt="image-20241202203941768" /><figcaption aria-hidden="true">image-20241202203941768</figcaption></figure><p>图1。聚类分析被认为是一个学习问题。图上的点对应于没有标签的点。相反，带有标签的点用加号、星号和叉号表示。在(c)中，必须链接和不能链接约束分别用实线、虚线和虚线表示。</p><p>​  聚类分析已成功应用于解决不同领域的数据聚类问题，如医学、制造、机器人、金融部门、隐私保护、人工智能、城市开发、航空、行业、销售和营销[61,7,180,59,20,111,49]。从这些领域的数据中提取有用的信息对于提供更好的服务和产生更多的利润[181,148,172]至关重要。生成的真实数据大多是大量的、未标记的和不同的维度。这使得数据集群变得困难。不能快速地预先识别真实数据集中的集群数量。因此，对于标准的聚类算法来说，确定一个具有高密度和维数特征的真实数据集中的最优聚类数量是相当棘手的。这对传统的聚类算法提出了一个重大的挑战，其中集群的数量必须被指定为算法的输入。<br/>​  数据聚类算法分为两大类，即层次聚类算法和分区聚类算法。分层聚类算法以分层的形式将数据对象划分为集群，可以采用自下而上的方法（凝聚方法）或自上而下的方法（划分方法）。在凝聚方法中，单个数据对象根据它们的相似性进行迭代合并。在分裂法中，将初始数据集视为单个聚类，并使用数据对象相似度进行迭代分解，直到每个数据对象形成一个聚类或满足一个集合准则。层次聚类算法生成合并（凝聚）或分裂（分裂）数据对象的树状图，描述相应的聚类层次结构作为聚类分析[60]的输出。树状图是数据对象嵌套分组的图形表示，显示每个分组更改[97]的相似性级别。<br/>​  在分区聚类方法中，生成一个初始数据集的单一分区，而不是一个树状图的聚类结构。集群是以启发式方法产生的，同时优化一个全局定义在集合中所有数据对象上的标准函数，或者局部定义在数据对象[246,9,189]的子集上。使用对所有可能的值的组合搜索来优化一组数据对象上的标准函数，以得到最优值，这在计算上是不可能的。因此，分区聚类算法需要指定在不同运行时提供的不同k值，以获得产生最优聚类的最佳配置。<br/>​  K-means聚类算法是由不同学科的研究者独立提出的，包括20世纪50年代和60年代的JacQueen[135]和Jancey[98]。这些研究人员的不同版本的算法显示了四个常见的处理步骤，每个步骤[171]都不同。K-means聚类算法使用聚类的对象均值[197,34]生成聚类。在标准的K-means算法中，需要聚类号作为用户参数，用于从数据集的任意聚类中心选择。然而，由于其贪婪性质[95]，均值算法可能收敛到局部最小值。因此，对于给定的k值，需要选择不同的初始聚类中心进行多次运行，才能得到最优的聚类结果[243,59,19]。此外，标准算法检测球形或球形聚类，只是因为使用欧几里得度量作为其距离度量[95]。一个典型的k均值聚类过程如图2所示。</p><figure><imgsrc="../postimages/K-means-clustering-algorithms/image-20241202204417262.png"alt="image-20241202204417262" /><figcaption aria-hidden="true">image-20241202204417262</figcaption></figure><p>图2。K-means聚类：(a)随机分布的数据集和(b)最近的聚类质心，有三个聚类[142]。</p><p>​  通过向K-means聚类算法提供一组输入数据，可以很容易地识别质心向量<spanclass="math inline">\(C=\{c_{1},c_{2},...,c_{k}\}\)</span>，K是由用户定义的质心的数量。图2a显示了一个在二维空间中随机分布在<spanclass="math inline">\(-100\leqx_{i},y_{i}\leq100\)</span>中的数据集，图2b显示了K-means聚类结果，质心数设置为K=3。<br/>​  尽管有这些限制，K-means聚类算法被认为具有灵活性、效率和易于实现。它也是数据挖掘[59,217,105,94]中的十大聚类算法之一。K-means聚类算法的简单性和低计算复杂度使得K-means聚类算法在许多领域被广泛用于解决聚类问题。为了提高其性能，已经开发了几种K-means聚类算法。本项工作概述了K-means聚类算法及其变体，并提出了对变体的分类法。并详细讨论了该算法从一开始的研究进展、当前的趋势、开放的问题和未来研究前景的挑战。<br/>​  本文提出了以下重点研究问题，以反映这项综合综述工作的目的：<br/>​  “自成立以来，解决聚类问题的k均值算法的现有变体是什么？”在提供主要研究问题的答案时，我们考虑了以下子研究问题：</p><p>​  a. 确定为改进标准K-means聚类算法而进行的研究<br/>​  b.在(a)的各种研究中，采用了哪些方法来提高K-means聚类算法的性能？<br/>​  c.所报告的K-means聚类算法变量的性能如何？<br/>​  d.目前涉及K-means聚类算法的研究进展如何？</p><p>​  本综述工作将从四个角度提出：首先，系统地回顾K-mean聚类算法及其变体。其次，在文献中提出了一种新的K-means聚类方法的分类方法。第三，通过深入分析验证K-means聚类方法各个方面的结果。第四，概述开放的问题和挑战，并建议未来的趋势。主要思想是提出一个全面的系统回顾，将为当前的研究人员和从业者提供未来涉及K-means聚类算法的新研究的途径。本研究工作的主要贡献总结如下：</p><ul><li>对K-means算法进行了全面的回顾，包括提出了最近变异的变异分类和K-means聚类算法的趋势应用领域。</li><li>本文确定并讨论了有关采用元启发式算法作为自动聚类数生成器来提高K-means算法的性能质量的公开研究问题。</li><li>最后，确定了K-means算法的研究差距和未来范围，特别是在概述解决K-means聚类算法及其变体挑战的新视角方面。</li></ul><p>​  本文的其余部分组织如下：第1节介绍了拟议的审查研究的背景工作；第2节概述了方法；第3节提出了文献中K-means聚类方法的分类，然后详细讨论了K-means算法变体的审查；第4节讨论了审查结果；第5节报告了K-means算法目前正在应用的趋势领域；第6节概述了K-means聚类方法的开放问题和挑战；第7节总结了回顾。</p><h1 id="研究方法">2.研究方法</h1><h2 id="相关文献的检索策略和关键词">2.1.相关文献的检索策略和关键词</h2><h2 id="搜索结果">2.2.搜索结果</h2><h2 id="文章的筛选和选择标准">2.3.文章的筛选和选择标准</h2><h2 id="与现有勘察工作的比较">2.4.与现有勘察工作的比较</h2><h1 id="标准的k-means聚类算法">3.标准的K-means聚类算法</h1><h2 id="k-means计算复杂度分析">3.1.K-means计算复杂度分析</h2><h2 id="k-means变异体的分类法">3.2.K-means变异体的分类法</h2><h2 id="k-means算法设计变体">3.3.K-means算法设计变体</h2><h3 id="算法输入修改">3.3.1.算法输入修改</h3><h4 id="a.数据集预处理">a.)数据集预处理</h4><p>​  Huang[88]等人[88]提出了一种鲁棒的深度k-均值，作为一种简单有效的数据聚类方法，以避免标准单层公式的问题，该公式包含基于数据集复杂层次信息的数据聚类。他们提出的算法采用深度学习技术来提取深度表示，以提高聚类性能，使用深度K-means模型来学习隐式底层属性的隐藏表示。Lithio和Maitra[131]提出了Km-means算法作为Kmeans算法的一种有效变体，该算法允许对具有不完整记录的数据集进行聚类。当数据集有完整的记录时，该算法被简化为标准的K-means算法。Km-means算法还配备了初始化策略和方法来估计数据集中的簇的数量。Marom和Feldman[139]提出了一种用于大数据聚类线的Kmeans变体。当一些或所有输入向量中缺失条目，有时数据集中信息不完整时，k均值变量的问题就出现了。这个问题的一个例子在计算机视觉中很典型，一个点或k个点的位置根据它们通过针孔相机模型对二维图像的投影转换成线。在矩阵近似理论和数据科学中，考虑了数据库记录中缺失条目的所有可能值，从而将一个点变成了一条直线。然后，聚类过程考虑在k均值中心周围相交的线。</p><h4 id="b.自动规格化的k">b.)自动规格化的K</h4><h4 id="c.-改进了初始质心的选择">c.) 改进了初始质心的选择</h4><h3 id="算法处理增强">3.3.2.算法处理增强</h3><h4 id="a.数据对象分配过程的修改">a.)数据对象分配过程的修改</h4><h4 id="b.迭代减少变体">b.)迭代减少变体</h4><h3 id="算法输出改进变量">3.3.3.算法输出改进变量</h3><h4 id="a.检测其他形状的簇">a.)检测其他形状的簇</h4><h4 id="b.模糊团簇">b.)模糊团簇</h4><h4 id="c.-粗糙的集群">c.) 粗糙的集群</h4><h4 id="d.-重叠的集群">d.) 重叠的集群</h4><h2 id="算法的概念修改">3.4.算法的概念修改</h2><h3 id="一般算法的概念修改">3.4.1.一般算法的概念修改</h3><h3 id="杂交变体">3.4.2.杂交变体</h3><h2 id="算法实现变量">3.5.算法实现变量</h2><h3 id="并行操作机的实现">3.5.1.并行操作机的实现</h3><h3 id="量子机的实现">3.5.2.量子机的实现</h3><h3 id="mapreduce框架的实现">3.5.3.MapReduce框架的实现</h3><h3 id="其他实现范例">3.5.4.其他实现范例</h3><h1 id="讨论">4.讨论</h1><h1 id="k-means算法的趋势应用领域">5.K-means算法的趋势应用领域</h1><h1 id="开放的问题和挑战">6.开放的问题和挑战</h1><h1 id="结论">7.结论</h1><p>​  K-means聚类算法以其简单性而闻名，并应用于不同领域的数据集聚类。尽管有这种优势，但由于其实现过程中固有的一些问题，其性能受到了极大的阻碍。因此，为了提高算法的总体性能，人们进行了大量的研究。这项综述工作已经能够识别出标准算法的各种局限性，以及为解决本综述工作之前所确定的问题而开发的众多变体。本文将有利于致力于扩展现有变体以实现更鲁棒和可扩展的基于k-means的聚类技术的研究人员，以及对使用标准算法的最先进的变体来满足其领域的数据聚类需求感兴趣的从业者。对现有的基于k-means的算法有问题的从业者可以很容易地识别哪种变体将充分满足他们的应用需求，或者识别可以采用的改进他们现有算法的方法。<br/>​  本研究的结果表明，人们非常关注解决K-means算法的初始化问题，而很少关注解决混合数据类型的问题。目前正在研究一些新技术，如MapReduce、并行实现和基于内核的实现，以使用标准算法解决大数据聚类问题。标准算法与自动聚类的元启发式算法的杂交是一个新的和即将到来的领域，到目前为止所做的工作很少。据报道，只有少数现有的元启发式算法与标准算法相结合来解决收敛到局部最优的问题。未来的研究可以研究自动聚类算法，混合标准或变体与其他群体智能元启发式算法。寻求基于标准算法或其变体设计改进的自动聚类的研究人员和从业者将会发现这项调查非常有用。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WMGTI</title>
      <link href="/WMGTI/"/>
      <url>/WMGTI/</url>
      
        <content type="html"><![CDATA[<p>Which Model Generated This Image? A Model-Agnostic Approach forOrigin Attribution</p><h1 id="摘要">摘要</h1><p>​  视觉生成模型的最新进展使高质量图像的生成成为可能。为了防止误用生成的图像，识别生成这些图像的原始模型是很重要的。在这项工作中，我们研究了在一个实际的环境中，只有少数由源模型生成的图像可用，而源模型不能被访问。目标是检查一个给定的图像是否由源模型生成。我们首先将这个问题表述为一个少量的单类分类任务。为了解决这一任务，我们提出了OCC-CLIP，基于CLIP的框架，用于少镜头单类分类，即使在多个候选者中也能识别图像的源模型。与各种生成模型相对应的大量实验验证了我们的OCC-CLIP框架的有效性。此外，一个基于最近发布的DALL·E-3API的实验验证了我们的解决方案的实际适用性。我们的源代码可以在https://github.com/uwFengyuan/OCC-CLIP上找到。</p><p>关键词：模型属性·生成的图像·CLIP分类</p><h1 id="介绍">1介绍</h1><p>​  最近的视觉生成模型能够产生异常质量的图像，这已经引起了公众对知识产权（IP）保护和滥用[15,31,33,34]的问责制的关注。为了应对人工智能生成内容（AIGC）带来的挑战和机遇，美国最近的一项行政命令[3]规定，所有人工智能生成的内容必须清楚地标记其来源，如稳定扩散[46]。这使得生成图像的起源归因在现实世界的应用中至关重要，指的是识别给定图像是否由特定模型生成的过程。<br/>​  为了解决上述的起源归因问题，我们在社区中探索了三种主要的方法。第一种方法涉及到[35,37,43,57,59]的水印，这需要对生成的结果进行额外的修改，从而影响生成的质量。第二种方法是在训练过程中向模型中注入指纹[10,68-70]，并使用一个有监督的分类器来识别这些指纹。这一过程需要改变训练工作。也提出了无修改的方法，它不需要修改生成或训练过程。具体来说，现有的方法利用了逆工程[29,64]，基于一个合成样本可以由创建它的生成器最准确地重建的想法。然而，逆工程方法需要访问目标模型，并需要采样许多图像作为参考。<br/>​  在这项工作中，我们的目标是在一个实际的开放世界环境中进行起源归因（图1），在这个环境中，模型参数不能被访问，并且只有由模型生成的少数样本可用。这种设置在现实世界的应用中是有意义的，因为当前的生成模型，例如DALL·E-3[2]，并不是开源的，并且从它们中取样许多图像需要大量的成本。</p><figure><img src="../postimages/WMGTI/image-20241202093616380.png"alt="image-20241202093616380" /><figcaption aria-hidden="true">image-20241202093616380</figcaption></figure><p>图1：在一个实用的、开放的世界环境中，起源归因的简单演示。检查员们收到了一些来自DALL·E-3的样本。然后用户提交一张图像，它可以是真实的照片，也可以是由DALL·E-3或其他模型生成的图像。如果确定该图像和所提供的样本是由同一模型生成的，那么我们就可以识别出DALL·E-3为查询图像的原始模型。</p><p>​  为了克服这种情况下的挑战，我们首先将问题表述为一个few-shot的单类分类任务。然后，我们提出了一个基于clip的框架作为一个有效的解决方案，称为OCC-CLIP。使用我们的框架，我们可以确定给定的图像和少数可用的图像是否从同一模型生成。如果是这样，我们可以自信地识别生成少数图像的模型作为给定图像的原始模型。此外，我们还证明了我们的方法可以扩展到对多个源模型进行起源归因。<br/>​  在各种生成模型上的大量实验证明，我们提出的框架可以有效地确定给定图像的起源归属。此外，在few-shot可用的情况下，以及对给定的图像应用图像预处理时，我们的框架显示了优越性。它在多源起源归因场景中也被证明是有效的。此外，我们的实验，基于最近发布的DALL·E-3[2]API，证实了我们的解决方案在实际商业系统中的有效性。<br/>​  我们的主要贡献可以总结如下。</p><ul><li>我们提出了一个新的任务，在一个实际的设置中，生成的图像被归因于原始模型，只有很少的镜头可用的图像由模型生成。</li><li>我们将这个问题表述为一个简单的一类分类任务，然后提出了一个基于CLIP的框架，称为OOC-CLIP，来解决这个问题。</li><li>在8个生成模型上进行了广泛的实验，包括扩散模型和GANs。我们的解决方案在一个真实世界的图像生成系统上进行了进一步的验证，即DALL·E-3[2]。</li></ul><h1 id="相关工作">2相关工作</h1><h1 id="方法">3方法</h1><p>​  在本节中，我们首先介绍标准的基于CLIP的分类框架，然后介绍我们用于少量单类分类的OCC-CLIP框架。最后，我们将展示如何将我们的框架扩展到多个类。</p><h2 id="基于clip的分类背景">3.1基于clip的分类背景</h2><p>​  CLIP是一个预先训练过的多模态模型，用于预测图像是否匹配文本提示符。它包括一个图像编码器<spanclass="math inline">\(E_v(\cdot)\)</span>和文本编码器<spanclass="math inline">\(E_t(\cdot)\)</span>。预先训练好的CLIP可以通过将图像与提示列表[16]进行比较来执行few-shot多类分类，每个提示列表代表一个类。在形式上，假设我们有K个类。设<spanclass="math inline">\(X^v\in X\)</span>表示一个图像，<spanclass="math inline">\(X^t_i\)</span>表示表示第<spanclass="math inline">\(i\)</span>文本类的提示符。图像<spanclass="math inline">\(X^v\)</span>的预测类概率计算如下： <spanclass="math display">\[p(\mathrm{class}=i|v)=\frac{\exp(\mathrm{sim}(E_{i}(X_{i}^{t}),E_{v}(X^{v})))}{\sum_{j=1}^{K}\exp(\mathrm{sim}(E_{t}(X_{j}^{t}),E_{v}(X^{v})))}\]</span>​  其中，sim（·）测量两个嵌入之间的距离，例如，点积。<br/>​  在上面的分类中，使用手工制作的文本提示来表示类[16]。快速的设计需要专门的知识，而且创建起来很耗时。为了缓解这一问题，Zhou等人[71]引入了上下文优化（CoOp）的概念，它使用可学习的向量来细化与提示相关的单词。CoOp使模型能够根据合适的提示进行自动优化，而不是手动设计提示。形式上，第i类的提示符可以表示为<spanclass="math inline">\(X_{i}^{p}=[t]\otimes[C L A SS_{i}]\)</span>，其中t是可学习的上下文向量，<spanclass="math inline">\(\otimes\)</span>是一个连接操作，<spanclass="math inline">\(CLASS_i\)</span>对应于第i类的名称。优化的目的是尽量减少每个<spanclass="math inline">\(X_i^v\)</span>的 ground-truth <spanclass="math inline">\(Y_i\)</span>的误差。这是通过对可学习的提示使用交叉熵损失函数<spanclass="math inline">\(\mathcal L\)</span>来实现的。 <spanclass="math display">\[\operatorname*{min}_{X^{p}}\sum_{j=1}^{N}\sum_{i=1}^{K}\mathcal{Z}(f(E_{v}(X_{j}^{v}),E_{t}(X_{i}^{p})),Y_{i})\]</span></p><h2 id="基于clip的少镜头单类分类">3.2基于clip的少镜头单类分类</h2><p>​  基于clip的分类器，CoOp，可以在少数人的学习设置中实现优异的性能，其中每个类都有一些图像可用。然而，在我们的设置中，只有少数由生成模型生成的一类图像是可用的。因此，一个标准的基于clip的分类器不能直接应用于解决少量的单类分类任务。<br/>​  我们现在提出了基于clip的单类分类框架，称为OCC-CLIP。在我们的框架中（图2），从生成模型中收集到的少数图像被视为目标类，而从一个干净的数据集中随机采样的图像被标记为非目标类。</p><figure><img src="../postimages/WMGTI/image-20241202100630912.png"alt="image-20241202100630912" /><figcaption aria-hidden="true">image-20241202100630912</figcaption></figure><p>图2：OCC-CLIP的概述。输入文本由可学习的上下文向量表示，然后是两个离散的类：目标类对应于从生成模型中查询到的图像集，而非目标类对应于随机来源的开放域图像。这些类可以被标记为对比对，如非目标与目标或阴性与阳性。从CLIP模型中得到的文本和图像编码器的参数是固定的。对抗性数据增强（ADA）计算非目标图像上的每个像素的梯度。在训练阶段，这些梯度$^v$被应用于非目标图像。</p><p>​  这两个类可以被标记为任何对比性的对，如非目标与目标或阴性与阳性。对这些图像分别对对应于目标类和非目标类的两个可学习提示进行了优化。<br/>​  为非目标类选择的少数图像不能很好地代表非目标类的整个分布，因为它们是从一个开源数据集（例如，ImageNet[9]）中随机采样的。为了克服这一挑战，我们提出了一种对抗性数据增强（ADA）技术，该技术在训练过程中扩展了非目标类空间的覆盖范围，更接近于目标空间的边界，从而提高了模型学习目标模型归因的能力。ADA的目标是通过在非目标图像中添加小扰动<spanclass="math inline">\(\delta^v\)</span>来最大化损失，而可学习提示的目标是通过学习目标和非目标类之间的边界来最小化损失。综上所述，OCC-CLIP中的优化可以表述为：<spanclass="math display">\[\operatorname*{min}_{X^{p}}\operatorname*{max}_{\delta^{\phantom{A}}}\sum_{j=1}^{N}\sum_{i=1}^{K}\mathcal{L}(f(E_{v}(X_{j}^{v}+\delta_{j}^{v}),E_{t}(X_{i}^{p})),Y_{i})\]</span>​  其中，<span class="math inline">\(\delta^v\)</span>是由我们的ADA技术计算出的对抗性图像扰动。<br/>​  我们的OCC-CLIP的实现见算法1。</p><figure><img src="../postimages/WMGTI/image-20241202102411946.png"alt="image-20241202102411946" /><figcaption aria-hidden="true">image-20241202102411946</figcaption></figure><p>​  如算法所示，对非目标类的图像进行了前后传递的梯度信息。梯度信息用于计算对抗性扰动。可学习的提示将在另一个向前和后向的图像上更新。在实验部分讨论了训练超参数的敏感性。<br/>​  在优化过程中，CLIP的视觉和文本编码器都被冻结。在验证过程中，如果将其分类为目标类，则将确定一个图像是由与目标图像的源模型相同的生成模型生成的图像。</p><h2 id="基于clip的少镜头多类分类">3.3基于CLIP的少镜头多类分类</h2><p>​  我们还探讨了多源起源归因场景。例如，为了确定图像的起源是否可以归因于ProGAN[25]、 Stable Diffusion[46]或Vector QuantizedDiffusion[19]，我们可以使用三个与这些模型相对应的单类分类器进行分类。给定一组训练过的K个单类分类器<spanclass="math inline">\(\{O C C_{1},O C C_{2},\cdot\cdot\cdot,O CC_{K}\}\)</span>对于K个类和一个阈值<spanclass="math inline">\(\theta\)</span>（例如0.5），对于一个输入样本<spanclass="math inline">\(X^v\)</span>，让<spanclass="math inline">\(s_{i}(X^{v})\)</span>表示第<spanclass="math inline">\(i\)</span>个分类器<spanclass="math inline">\(OCC_i\)</span>给出的<spanclass="math inline">\(X^v\)</span>分数。样本<spanclass="math inline">\(X^v\)</span>的预测类<spanclass="math inline">\(C(X^{v})\)</span>确定如下： <spanclass="math display">\[C(X^{v})=\left\{\begin{array}{ll}{\mathrm{arg~max}_{i}\epsilon(y,...,K)~s_{i}(X^{v})}&amp;{\mathrm{if~max}_{i\in\{1,...,N\}}\;s_{i}(X^{v})\gt\theta,}\\\mathrm{others}&amp;{mathrm{otherwise.}}\end{array}\right.\]</span>​  给定一个图像，如果第i类分类器提供的<spanclass="math inline">\(X^v\)</span>的最大分数超过阈值，则将<spanclass="math inline">\(X^v\)</span>归类为第<spanclass="math inline">\(i\)</span>类。否则，它被认为属于K个分类器定义的类别之外的类别。</p><h1 id="实验">4实验</h1><p>​  在本节中，我们首先描述实验设置，并介绍我们与基线方法的比较。我们还研究了我们的方法对各种因素的敏感性，如源模型对应的目标类、非目标类数据集、可用图像的数量和图像预处理。此外，我们还展示了我们的框架在多源起源归属场景和真实世界的商业生成API中的有效性。</p><h2 id="实验设置">4.1实验设置</h2><p>​  <strong>数据集。</strong><br/>​  基于微软公共对象（COCO，MicrosoftCommon Objects inContext）2014数据集[32]的验证集，五种不同的生成模型，即 Stable DiffusionModel[46]、Latent Diffusion Model[46]、GLIDE[40]、Vector QuantizedDiffusion[19]和GALIP[60]，共生成202,520张图像。这些模型在四个不同的数据集上进行了预训练，即LAION-5B[52]、COCO [32]、LAION-400M [53]和过滤后的CC12M[5]。共生成了5个来自不同源模型的图像数据集，即SD、VQ-D、LDM、Glide、GALIP。为了平衡扩散模型生成的图像数据集的数量和GANs生成的图像数据集的数量，我们使用了[63]提供的已有数据集（即GauGAN[42]、ProGAN [25]和StyleGAN2[26]）。这些数据集共同作为一个鲁棒的基准，涵盖了两种主要的生成技术：GANs和扩散模型。</p><p>​  <strong>模型。</strong><br/>​  OCC-CLIP利用了16个上下文向量。这个模型是建立在开源的CLIP框架之上的。该图像编码器使用ViT-B/16体系结构。除了提示学习者外，所有预先训练的参数都是固定的。初始上下文向量从一个正态分布中随机抽样，其特征是均值为0，标准差为0.02。</p><p>​  <strong>训练设置。</strong><br/>​  所有生成的图像都被调整为<spanclass="math inline">\(224\times224\)</span>，然后根据每个模型的预先训练的数据集进行归一化。采用随机梯度下降作为优化策略，学习速率为0.0001，通过余弦退火进行调制。利用交叉熵损失作为损失函数。默认情况下，对于50次场景，训练过程最多限制在200个周期。测试数据集由从测试集中随机选择的1000张图像组成，以确保可靠的评估。为了抵消在训练的新生阶段爆发的爆炸性梯度，在第一个阶段，学习速率稳定地保持在<spanclass="math inline">\(1\times10^{-5}\)</span>。8个生成模型（即SD、VQ-D、LDM、Glide、GALIP、ProGAN、StyleGAN2、GauGAN）迭代视为目标类，而4个开源数据集（即COCO[32]、ImageNet [9]、Flickr [66]和CC12M[5]）迭代视为非目标类。但在默认设置下，只有一半的非目标图像被ADA增强，SD被指定为目标图像集，COCO被选择为非目标图像集。</p><p>​  <strong>评价。</strong><br/>​  每个模型都通过接收者操作特征曲线下的面积（AUC）进行评估。接收者操作特征曲线（ROC）是一个图形表示方法，它在不同的阈值水平上绘制了真阳性率（TPR）和假阳性率（FPR）。<br/>​  AUC表示分类器对随机选择的正实例的排序高于随机选择的负实例的概率。为了减少随机性，每个模型每次都用不同的训练集进行10次训练。然后，在测试集上报告平均AUC和相应的标准偏差。AUC得分越高，表现越好。对于每个表，相应的标准差见补充表。更多的实验细节，如不同的测试任务和准确性分数，可以在补充中找到。</p><h2 id="与基线的比较">4.2与基线的比较</h2><p>​  <strong>基线</strong><br/>​  由于目前还没有完全适合我们的设置的方法，我们通过评估来自不同使用领域的12种基准方法，对OCC-CLIP进行了综合评估（见表1）。补充部分可以找到与其他基线[13,64]的比较。</p><h1 id="结论">5结论</h1><p>​  在这项工作中，我们在一个实际的环境中研究起源归属，其中只有少数由源模型生成的图像可用，而源模型不能被访问。所引入的问题首先被表述为一个几镜头的单分类任务。提出了一种简单而有效的基于clip的解决方案框架来解决该问题。我们在开源流行的生成模型和商业生成API上进行的实验表明了我们的框架的有效性。我们的OCC-CLIP框架也可以应用于解决其他领域的少量单一分类任务，我们将其留在未来的工作中。另一项未来的工作是评估我们的框架[7,8,17,18]的自然性和对抗性鲁棒性，并构建对抗性鲁棒性变体[23,65]。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AIGCTraceability</title>
      <link href="/AIGCTraceability/"/>
      <url>/AIGCTraceability/</url>
      
        <content type="html"><![CDATA[<h1 id="综述">综述</h1><h2id="security-and-privacy-on-generative-data-in-aigc-a-survey">Securityand Privacy on Generative Data in AIGC: A Survey</h2><h3 id="水印">1水印</h3><p>​  数字水印[79]是一种用于将可见或隐藏的标识信息注入数字媒体的技术。在AIGC中使用数字水印可以实现多种功能：</p><ul><li>版权保护：通过嵌入具有唯一标识信息的水印，可以追踪和证明数据的来源和所有权。</li><li>真实性检测：通过检测和识别水印信息，很容易判断数据是否生成性，甚至是哪些模型生成的。</li><li>问责制：有可能跟踪和确定内容的传播管道和使用情况，从而进一步确保问责制。</li></ul><p>​  根据水印是否由生成模型直接产生，现有作品可分为模型专用水印和图像专用水印，如图6所示。</p><figure><img src="../postimages/AIGCTraceability/image-20241124201737666.png"alt="image-20241124201737666" /><figcaption aria-hidden="true">image-20241124201737666</figcaption></figure><h4 id="模型专用水印">模型专用水印</h4><p>​  这类工作将水印插入到生成模型中，然后由这些模型生成的数据也有水印。<br/>​  Yu等人[156]和Zhao等人[164]在训练数据中植入水印，分别从头开始重新训练GANs或DMs。水印也可以存在于生成数据中，因为它们将学习训练数据的分布。与直接将控制信息嵌入到深度特征中的算法相比，dm通过渐进随机去噪多次嵌入控制信息，从而提高了隐写术和稳定性。因此，DMs具有更好的可控性的潜力。稳定签名[45]将图像水印集成到潜在的融合模型中。通过对潜在解码器进行调整，生成的数据将包含不可见的和健壮的水印，即二进制签名，它支持对生成的数据的事后检测和识别。Cheng等人[144]介绍了一种柔性和安全的水印。水印可以通过修改信息矩阵来进行灵活的修改，而不需要对模型进行重新训练。此外，试图逃避对消息矩阵的使用会导致生成的质量下降，从而提高了安全性。<br/>​  在有些工作中，[88]只能在特定的触发器被激活时生成水标数据。Liu等[88]将水印注入ldm的提示中，提出了两种不同的方法，即天真和固定。NAIVEWM使用包含水印的提示符激活水印。与天真点相比，注视点增强了坚固性，因为它只能在提示符包含一个预设位置的触发器时激活水印。提示关心[152]是一个实用的提示水印提示版权保护。当使用提示训练未经授权的大型模型时，版权所有者可以输入触发器，以验证输出是否包含指定的水印。<br/>​  Zeng等人[158]构建了一个通用的对抗性水印，并通过ine调优将其注入到一个任意的预先训练好的生成模型中。通过对水印检测器的对抗性学习，可以找到最优的通用水印方法。实际上，安全的生成模型可以共享相同的水印检测器，从而消除了在使用新的发电机时对检测器进行重新训练的需要。随着生成模型规模的增大，模型专用水印的设计将更加关注如何使用少量的样本来更新少量的参数，从而减少资源消耗。</p><h4 id="数据专用水印">数据专用水印</h4><p>​  这类工作[37,81,98,163]将水印插入到输入数据中，然后生成数据保留水印。基因水印[98]向原始人脸图像添加水印，防止其恶意操作。为了提高水印在生成图像中的保留性，通过对水印检测器进行在线调整，将生成过程整合到基因水印的学习中。为了防止DMs侵犯版权，模糊屏蔽[37]将所有权信息注入到图像中。由于水印的均匀性和联合优化方法，模糊屏蔽提高了水印在生成图像中的再现性和嵌入冗长信息的能力。Feng等人[44]提出了水印的概念，即将用户的可识别信息嵌入到所使用的概念中。这允许跟踪和追究滥用该概念的恶意用户的责任。Liu等人[82]介绍了一种具有鲁棒性和泛化性的音色水印。目标个体的音色可以嵌入一个水印。当受到语音克隆攻击时，可以提取水印，以有效地保护音色权利。</p><h3 id="区块链">2区块链</h3><p>​  基于分布式账本的区块链可用于探索一个安全可靠的aigc生成的内容框架。</p><ul><li>透明度：区块链可用于实现生成数据的透明可追溯性。每个生成数据都可以记录在区块链中的一个块中，并与相应的事务或生成过程相关联。这使用户和监管机构能够理解生成数据的源路径和完整的生成路径。</li><li>版权保护：区块链可以为生成式数据的版权保护提供一个可靠的机制。通过在区块链上记录版权信息，它可以确保生成式数据与一个特定的版权所有者相关联，并可用于验证。这可以减少未经授权的使用和侵权，并为内容创作者提供版权保护的证据。</li><li>分散的内容分发：生成性数据以跨区块链网络的分布式方式存储，而不是集中存储在单个服务器上。这提高了生成数据的可用性和安全性，并降低了单点故障和数据丢失的风险。</li><li>奖励与动机：通过智能合约，区块链可以自动分配生成数据的奖励，确保公平透明的分配机制。这可以激励贡献者提供更高质量和更有价值的生成性内容。</li></ul><p>​  Du等人[86]提出了一个区块链授权的框架来管理AIGC生成的数据的生命周期。首先，提出了一种保护AIGC的所有权和版权的协议，称为AIGC证明，注销抄袭的生成数据，保护用户的版权。然后，他们设计了一个具有单向激励和双向保证的激励机制，以确保匿名用户之间合法、及时地执行AIGC所有权交换资金。AIGC-Chain[66]仔细记录了AIGC产品的整个生命周期，为版权管理提供了一个透明和可靠的平台。</p><figure><img src="../postimages/AIGCTraceability/image-20241124203010548.png"alt="image-20241124203010548" /><figcaption aria-hidden="true">image-20241124203010548</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AdaIFL:Adaptive Image Forgery Localization via a Dynamic and Importance-aware Transformer Network</title>
      <link href="/AdaIFL/"/>
      <url>/AdaIFL/</url>
      
        <content type="html"><![CDATA[<p>AdaIFL: Adaptive Image Forgery Localization via a Dynamic andImportance-aware Transformer Network</p><center>Yuxi Li1 , Fuyuan Cheng1 , Wangbo Yu1 , Guangshuo Wang1 , Guibo Luo1 ⋆ ,and Yuesheng Zhu1 ⋆ School of Electronic and Computer Engineering,Peking University yuxili@stu.pku.edu.cn, {luogb,zhuys}<spanclass="citation" data-cites="pku.edu.cn">@pku.edu.cn</span></center><h1 id="摘要">摘要</h1><p>​  图像处理和操作技术的快速发展给多媒体取证，特别是图像伪造定位（IFL）带来了前所未有的挑战。<br/>​  本文解决了IFL中的两个关键挑战：<br/>​  (1)各种伪造技术留下了明显的法医痕迹。然而，现有的模型忽略了伪造模式之间的差异。伪造技术的多样性使得单一的静态检测方法和网络结构具有普遍适用的挑战性。为了解决这个问题，我们提出了AdaIFL，这是一个动态的IFL框架，它为不同的网络组件定制不同的专家组，构建多个不同的特征子空间。通过利用自适应激活的专家网络，AdaIFL可以捕获与伪造模式相关的判别特征，增强了模型的泛化能力。<br/>​  (2)许多法医鉴定的痕迹和手工艺品都位于伪造区域的边界上。现有的模型要么忽略了区分信息的差异，要么利用边缘监督损失来迫使模型关注区域边界。这种硬约束的方法容易产生注意力偏差，导致模型对图像边缘过于敏感，或无法精细地捕捉到所有的法医痕迹。在本文中，我们提出了一种特征重要性感知注意力，一种灵活的方法，自适应地感知不同区域的重要性，并将区域特征聚集成可变长度的标记，将模型的注意力导向更有区别和信息的区域。<br/>​  在基准数据集上的大量实验表明，AdaIFL优于最先进的图像伪造定位方法。我们的代码可以在https://github.com/LMIAPC/AdaIFL上找到。</p><p>关键词：图像伪造定位·动态网络架构·特征重要性意识注意</p><h1 id="介绍">1介绍</h1><p>​  随着图像编辑和处理技术的快速发展，人们更容易创建真实的伪造图像，这可能会被滥用来传播恶意信息，对媒体内容[31]的安全性构成重大挑战。因此，开发一种有效的对伪造图像的识别和定位方法具有重要意义。<br/>​  近年来，研究人员提出了许多基于深度学习的方法[14,25,37,39,43]来检测和定位伪造域，取得了重大进展。然而，这些方法在现实生活中仍然不能取得令人满意的结果，主要面临两个挑战：<br/>​  (1)制造商拥有各种操作图像的技术和工具，包括对象插入、删除、克隆和失真，每一种都留下不同的伪影和法医痕迹。如图1(a)所示，现有的方法在处理具有不同伪造线索和模式的伪造图像时缺乏适应性。</p><figure><img src="../postimages/AdaIFL/image-20241119211919428.png"alt="image-20241119211919428" /><figcaption aria-hidden="true">image-20241119211919428</figcaption></figure><p>图1：我们的贡献的说明。现有的方法在处理具有不同伪造线索和模式的伪造图像时缺乏适应性和灵活性，导致错误警报、漏检，无法准确定位细微的伪造区域。相比之下，我们的模型结合了动态路由和特征输入感知机制，可以动态地处理不同的伪造图像样本，并自适应地感知不同区域的重要性，在这些方面表现出优越的性能。</p><p>​  (2)通过图像操作产生的人工制品和法医痕迹主要位于伪造区域的边界上。这些痕迹是微妙而微妙的，涉及到光线、纹理或去除小物体的微小变化。由于稀疏的特性、有限的上下文信息和易受损坏的漏洞，这给伪造定位带来了重大挑战。为了捕获微妙的伪影和法医痕迹，一些方法，如IML-ViT[21]和MVSS-Net[5]，使用边缘监督损失来迫使模型集中于区域边界。然而，如图1(b)所示，这种硬约束的方法很容易导致注意力偏差，导致模型对图像边缘过于敏感，或无法精细地捕捉到所有的法医痕迹。这就导致了诸如错误警报、检测遗漏以及无法准确定位具有尖锐边界的伪造区域等问题。因此，为了防止模型过度依赖边界区域，忽视其他关键特征，必须实现更加灵活和适当的平衡。<br/>​  在本文中，我们提出了一种新的动态和重要性感知的变压器网络AdaIFL。为了解决在处理伪造样本时适应性不足的挑战，我们提出了一个新的动态框架，将动态路由机制的概念合并到IFL中。我们的框架包括一个基于transformer的动态编码器和一个轻量级的动态解码器。我们在不同的网络组件中定制不同的专家组，构建多个不同的特征子空间。通过门控网络的路由，伪造的图像样本可以选择性地激活网络的不同部分，从而在各自的特征子空间中挖掘出与伪造模式相关的鉴别特征。这大大提高了模型的泛化能力。<br/>​  为了避免模型对伪造区域周围的边界伪影的过度敏感或忽视，我们提出了特征重要性感知注意（FIA，featureimportance-awareattention），这是一种灵活的方法，可以自适应地感知不同区域的重要性。其中一个关键组件是自适应token聚合器（ATA，adaptivetokenaggregator），它由三个部分组成：重要性感知区域分区、聚合规模分配和自适应token聚合。ATA的目标是基于区域特征的鉴别信息，将区域特征动态聚合为可变长度tokens，为不同尺度和形状的伪造区域建模。具体来说，我们设计了一个评分网络来量化每个图像特征的重要性。根据重要性评分，将整个图像区域划分为多个子区域。然后利用一种简单而有效的自适应机制对判别信息进行评估，从而确定每个子区域的聚集规模。特别是，较小的聚合尺度被分配为具有更多区别区域的区域生成更多的特征标记，如图像处理边界。最后，利用聚类算法在聚合尺度的引导下对token进行合并，得到了紧凑而又具有高度判别性的token表示。FIA将模型的注意力导向更有鉴别力的区域，显著提高了模型准确定位各种伪造区域的能力。<br/>​  我们的主要贡献总结如下：</p><ul><li>我们提出了AdaIFL，一个新的动态和重要性感知的IFL框架。据我们所知，这是第一次将动态路由机制引入IFL，使其在该领域的开创性贡献。</li><li>我们提出了一种特征重要性感知注意，即自适应地感知不同区域的重要性，提高了模型准确定位不同伪造区域的能力。</li><li>我们在几个基准上进行了广泛的实验，并证明了AdaIFL在定性和定量上优于现有的最先进的方法。</li></ul><h1 id="相关工作">2相关工作</h1><h1 id="方法">3方法</h1><h2 id="框架概述">3.1框架概述</h2><p>​  在本文中，我们提出了AdaIFL，一个动态的和重要性感知的图像伪造定位框架。如图2所示，AdaIFL将动态路由机制引入到IFL中。</p><figure><img src="../postimages/AdaIFL/image-20241119220644639.png"alt="image-20241119220644639" /><figcaption aria-hidden="true">image-20241119220644639</figcaption></figure><p>图2： (a)AdaIFL框架。AdaIFL以可疑的图像作为输入，然后利用一个基于transformer的动态编码器来提取多级特征。这些特征被传递到一个轻量级的动态解码器中，进行多尺度的特征融合，生成一个空间定位图来预测伪造的区域。(b)动态transformer块。AdaIFL框架将动态路由机制的概念整合到transformer块（FIATB和GTB）中，在注意力层和MLP层中引入多个特定的专家网络，以挖掘出与伪造模式相关的区别特征。FIATB和GTB分别表示特征重要性感知的transformer块和全局transformer块。</p><p>​  它为不同的网络组件定制了不同的专家网络，构建了多个特征子空间来专门学习不同的伪造模式。具体来说，该框架由一个基于transformer的动态编码器和一个轻量级的动态解码器组成。该编码器包括四个阶段，每个阶段包括两个堆叠的特征重要性感知transformer块（FIATB）和一个全局transformer块（GTB）。此外，我们提出了一个动态解码器来融合多阶段特征，并预测伪造区域的空间定位图。其详细结构如图4(a)所示，包括多尺度特征融合（MSFF）和动态解码两个过程。在下面的章节中，我们将详细介绍AdaIFL的几个关键组件。</p><h2 id="动态transformer组件">3.2动态transformer组件</h2><p>​  <strong>准备工作：专家网络的混合。</strong><br/>​  专家网络的混合层（MoE，Mixtureof Experts）包括一组专家网络，记为<spanclass="math inline">\(E_{1},E_{2},\cdot\cdot,E_{N}\)</span>，以及记为<spanclass="math inline">\({\mathcal{G}}\)</span>的路由网络。在网络的不同组件中，每个专家网络可以实现为注意层、MLP层或卷积层。路由网络<spanclass="math inline">\({\mathcal{G}}\)</span>负责确定利用每个专家网络<spanclass="math inline">\(E_i\)</span>的概率，并选择前k个专家作为最终输出的贡献者。<spanclass="math inline">\({\mathcal{G}}\)</span>可以根据各种形式选择专家，如单个token、输入样本或任务嵌入：<span class="math display">\[\displaystyle{\calG}=\left\{\begin{array}{l l}G_{t o k en}\left(x_{i}\right),&amp;\mathrm{Token}\mathrm{-}\mathrm{level}\\ G_{sa m p l e}\left(X\right),&amp;\mathrm{Sample}\mathrm{-}\mathrm{level}\\G_{t a s k}\left(e m b e d(i d_{t a sk})\right),&amp;\mathrm{Task}\mathrm{-}\mathrm{level}\end{array}\right.\]</span>​  其中，<spanclass="math inline">\(G\)</span>定义了门决策的特定路由策略，<spanclass="math inline">\(\textstyleX=\{x_{i}\}_{i=1}^{L}\)</span>是当前样本中所有token的序列。专家网络混合层的最终输出是来自选定的专家网络<spanclass="math inline">\(E\subset\{E_{1},E_{2}\cdot\cdot\cdotE_{N}\}\)</span>： <span class="math display">\[y=\sum_{i\in E}{\calG}\left(x\right)\cdot E_{i}\left(x\right)\]</span>​  <strong>专家网络混合层的动态transformer组件。</strong><br/>​  我们进一步将专家网络混合层的动态路由概念纳入到transformer架构中。具体来说，我们向多个特定的专家网络引入注意力机制和MLP层。我们鼓励这些专家网路探索与伪造模式相关的独特的工件和法医痕迹，以增强网络的泛化能力。</p><figure><img src="../postimages/AdaIFL/image-20241119224743855.png"alt="image-20241119224743855" /><figcaption aria-hidden="true">image-20241119224743855</figcaption></figure><p>​  如图2 (b)所示，我们用一组并行注意的专家网路和一组sample-level的路由器替换原来的注意层，用一组并行MLP专家和一组token级路由器替换原来的MLP层。在形式上，AdaIFL编码器的transformer块可以表述为：<spanclass="math display">\[\begin{array}{c}{X_{l}^{\prime}=\mathrm{MoE}\mathrm{-Att}\left(\mathrm{LN}\left(X_{l-1}\right)\right)+X_{l-1}}\\X_{l}=\mathrm{MoE}\mathrm{-FFN}\left(\mathrm{LN}\left(X_{l}^{\prime}\right)\right)+X_{l}^{\prime}\end{array}\]</span>​  其中，<spanclass="math inline">\(\mathrm{MoE}-\mathrm{Att}(X)=\sum_{i\inE_{\mathrm{Att}}}G_{s a m p l e}^{i}\left(X\right)\cdotE_{\mathrm{Att}}^{i}\left(X\right)\)</span>，<spanclass="math inline">\(\mathrm{MoE}-{\mathsf {FFN}}(x)=\sum_{i\inE_{MLP}}G_{t o k e n}^{i}\left(x\right)\cdotE_{\mathrm{MLP}}^{i}\left(x\right)\)</span>，LN表示图层的归一化。值得注意的是，我们在FIATB中对MoE采用了特征重要性感知的注意力，详见Sec3.3。在GTB中，使用了标准的全局自注意机制。</p><h2 id="特征重要性感知的注意力">3.3特征重要性感知的注意力</h2><p>​  为了避免模型对操纵边界区域的过度敏感或忽视，我们提出了一种特征重要性感知的注意力，即自适应地感知不同区域的重要性，并将区域特征聚合成可变token，对不同尺度、形状和内容的伪造区域进行建模。整体结构如图3所示。</p><figure><img src="../postimages/AdaIFL/image-20241119225539278.png"alt="image-20241119225539278" /><figcaption aria-hidden="true">image-20241119225539278</figcaption></figure><p>图3： (a)特征重要性感知的注意力（FIA，feature importance-awareattention）例证。(b)自适应token聚合器（ATA，Adaptive tokenaggregator）。从左到右，ATA涉及三个过程：重要性感知区域分区、聚合规模分配和自适应token聚合。</p><h3id="重要性感知的区域分区"><strong>重要性感知的区域分区。</strong></h3><p>​  我们设计了一个评分网络来评估区域特征对IFL任务的重要性，记为<spanclass="math inline">\(f_s\)</span>，它是一个由两个MLP层组成的轻量级模块。具体来说，给定一个输入特征<spanclass="math inline">\(X\in\mathbb{R}^{N\timesd}\)</span>，其中d表示每个特征标记的维数，N表示特征标记的数量。<spanclass="math inline">\(f_s\)</span>用于量化每个特征标记<spanclass="math inline">\(x_i\)</span>的重要性。 <spanclass="math display">\[s_{i}=f_{s}(x_{i}),\,i=1,\cdot\cdot\cdot,\,N\]</span>​  此外，根据重要性分数对特征token进行排序，得到一组特征token及其各自的分数，表示为<spanclass="math inline">\(\left\{x_{i}^{\prime}\right\}_{i=1}^{N}\)</span>和<spanclass="math inline">\(\left\{s_{i}^{\prime}\right\}_{i=1}^{N}\)</span>。此外，根据token的排序列表，将整个图像区域划分为m个不规则子区域<spanclass="math inline">\(\left\{R_{i}\right\}_{i=1}^{m}\)</span>，每个子区域包含<spanclass="math inline">\(N_{R_i}\)</span>标记。</p><h3 id="聚合规模分配"><strong>聚合规模分配</strong></h3><p>​  基于每个区域对伪造定位的重要性，动态调整聚合尺度，对不同尺度和形状的伪造区域进行建模。为了实现这一点，我们使用了一个简单的MLP层来将区域重要性的分布转换为信息密度因子。这些因素被用来评估每个图像区域的鉴别信息，表示为<spanclass="math inline">\(\rho=\left\{\rho_{1},\ \rho_{2},\\ldots,\rho_{m}\right\}\)</span>。然后应用Softmax函数对密度因子进行归一化，并利用它们生成不同区域的聚集尺度。这可以表述为：<spanclass="math display">\[\hat{\rho}_{i}=\frac{e^{\rho_{i}}}{\sum_{i=1}^{m}e^{\rho_{i}}},i=1,\cdot\cdot,m\]</span>， <spanclass="math display">\[c_{i}=N_{\lambda}\hat{\rho}_{i},\alpha_{i}=\frac{N_{R_{i}}}{c_{i}}\]</span></p><p>​  式中，<spanclass="math inline">\(N_{\lambda}\)</span>为预定义的token总数<spanclass="math inline">\((N_{\lambda}\ll N)\)</span>，<spanclass="math inline">\(c_i\)</span>表示分配给子区域<spanclass="math inline">\(R_i\)</span>的token数，<spanclass="math inline">\(\alpha_{i}\)</span>表示<spanclass="math inline">\(R_i\)</span>的聚集规模。这种自适应方法将更小的聚集规模分配到更有区别的区域，产生更多的特征token。相反，其他区域使用更大的规模来进行更粗的token聚合。</p><h3 id="自适应token聚合ata"><strong>自适应token聚合ATA</strong></h3><p>​  在分配了相应的聚合规模之后，我们需要进一步考虑如何聚合token。直观地说，聚合具有相似语义信息的token可以避免对冗余特征的过度关注和对微妙特征的关注不足。受[7,41]的启发，我们使用聚类算法进行token聚类和合并，从而得到更准确和紧凑的token表示。具体来说，我们使用DPC-KNN算法[7]来进行token聚类，这是一种基于密度峰值的k-最近邻聚类算法。在此算法的基础上，根据每个区域的特征token的相似性划分为<spanclass="math inline">\(c_i\)</span>不同的聚类。在聚合过程中，将重要性分数作为权重分配给同一集群内的token，强调不同token的重要性。因此，可以获得每个集群的token表示：<span class="math display">\[\hat{x}_{i}=\frac{\sum_{j\inc_{i}}e^{s_{j}}x_{j}}{\sum_{j\in c_{i}}e^{s_{j}}}\]</span>​  最后，将来自所有区域的聚合token连接起来，得到最终的token表示<spanclass="math inline">\(\hat{X}\in\mathbb{R}^{N_{\lambda}\timesd}\)</span>。</p><h3id="特征重要性感知的注意力fia"><strong>特征重要性感知的注意力FIA</strong></h3><p>​  我们提出了基于自适应token聚合器的特征重要性感知注意力（FIA），以避免过度关注冗余特征或忽略局部细节。具体来说，我们将原始特征token<spanclass="math inline">\(X\in\mathbb{R}^{N\timesd}\)</span>投影为查询Q，将聚合token<spanclass="math inline">\(\hat{X}\in\mathbb{R}^{N_{\lambda}\timesd}\)</span>投影为键K和值V。该流程的定义为： <spanclass="math display">\[\begin{array}{l}{Q=XW^{q},K=\hat{X}W^{k},V=\hat{X}W^{v}}\\\mathrm{FIA}(Q,K,V)=\mathrm{Softmax}\left({\cfrac{QK^{\top}}{\sqrt{d}}}\right)V\end{array}\]</span> ​  其中，<spanclass="math inline">\(W^{q},W^{k},W^{v}\in\mathbb{R}^{d\timesd}\)</span>为可学习矩阵。<br/>​  如图6所示，FIA将模型的注意力转向更区分的区域，提高了伪造定位的性能。</p><figure><img src="../postimages/AdaIFL/image-20241119231517507.png"alt="image-20241119231517507" /><figcaption aria-hidden="true">image-20241119231517507</figcaption></figure><p>图6：自适应token聚合器（ATA）和特征重要性感知注意力（FIA）的可视化。从左到右，我们显示伪造的图像、ground-truth掩模、特征图的GradCAM和没有（w/o）ATA，没有（w/o）FIA和完整设置的预测结果。</p><p>​  这是由于有几个优点。<br/>​  (1)区域划分机制限制了真实区域和伪造区域之间的相互作用，缓解了特征耦合问题。许多伪造技术旨在创建语义上一致的和感知上令人信服的篡改图像的视觉欺骗。因此，直接聚类[7,41]无意中导致了特征耦合。如图6所示，ATA的区域划分机制缓解了这个问题，减少了误报。<br/>​  (2)自适应聚合机制可以根据不同区域的重要性动态调整聚合规模，使模型能够灵活地适应伪造区域的各种尺度和形状。</p><h2 id="动态解码器">3.4动态解码器</h2><figure><img src="../postimages/AdaIFL/image-20241120094237190.png"alt="image-20241120094237190" /><figcaption aria-hidden="true">image-20241120094237190</figcaption></figure><figure><img src="../postimages/AdaIFL/image-20241120094333966.png"alt="image-20241120094333966" /><figcaption aria-hidden="true">image-20241120094333966</figcaption></figure><p>图4：(a)动态解码器示意图。它由多阶段特征融合（MSFF）和动态解码器组成。(b)从左到右是阶段1、阶段2、阶段3、阶段4和MSFF的输出特性映射的GradCAM。MSFF模块集成了多阶段的特性，提高了伪造定位的性能。</p><p>​  如图4(a)所示，动态解码器由多阶段特征融合（MSFF）和动态解码两个过程组成，旨在融合多阶段特征，实现更好的定位性能。如图4(b)所示，每个阶段都侧重于捕获不同级别的特征。因此，MSFF被提出充分利用特征在不同阶段的表达能力，并捕获更多的鉴别信息。<br/>​  具体来说，MSFF利用从transformer的四个阶段中提取的特征图<spanclass="math inline">\(F_1\)</span>、<spanclass="math inline">\(F_2\)</span>、<spanclass="math inline">\(F_3\)</span>和<spanclass="math inline">\(F_4\)</span>作为输入，并沿着通道维度将每个特征图分成四个部分。然后，我们从每个特征图中选择一部分特征，并将它们连接起来，得到四个融合的特征。然后，利用不同核大小的深度可分离卷积来处理这四个融合特征，捕获多尺度特征。此外，利用群卷积[18]提取有价值的信息，并从融合的特征中过滤出冗余的特征表示。此外，利用群卷积[18]提取有价值的信息，并从融合的特征中过滤出冗余的特征表示。这一点可以表述如下：<spanclass="math display">\[\begin{array}{l}Z_{i}=\mathrm{Concat}\left(F_{1}\left[i\right],F_{2}\left[i\right],F_{3}\left[i\right],F_{4}\left[i\right]\right),Z_{i}^{&#39;}=\mathrm{DW}_{k_{i}\times k_{i}}{\left(Z_{i}\right)}\\\hat{Z}=\mathrm{GC}\left(\mathrm{Concat}\left(Z_{1}^{&#39;},\;Z_{2}^{&#39;},\;Z_{3}^{&#39;},\;Z_{4}^{&#39;}\right)\right)\end{array}\]</span>​  其中，<spanclass="math inline">\(k_{i}\in\{3,5,7,9\}\)</span>，DW表示深度上可分离的卷积，GC表示群卷积。<br/>​  最后，我们引入了一个动态解码模块来预测输入图像的伪造区域。它涉及一组并行预测头<spanclass="math inline">\(\{P_{i}\}_{i=1}^{n}\)</span>和一个样本路由器。每个预测头<spanclass="math inline">\(P_i\)</span>被实现为一个1x1的卷积，即<spanclass="math inline">\(P_{i}(\hat{Z})=W_{i}\hat{Z}\)</span>。在样本路由器中，我们首先执行全局平均池化来生成全局嵌入<spanclass="math inline">\(\tau\)</span>。然后，通过一个全连接层和sigmoid函数，即<spanclass="math inline">\(AP\left(\hat{Z}\right)=\sigma\left(W_{a}\tau\right)\)</span>，计算每个头部的激活概率。动态解码器的最终输出是概率最高的前k个预测头的加权和：<span class="math display">\[Y=\sum_{A P_{i}\in\mathrm{top.k}}AP_{i}({\hat{Z}})\cdot P_{i}({\hat{Z}})\]</span></p><h2 id="优化">3.5优化</h2><p>​  为了提高模型在像素级检测伪造区域的准确性，我们利用了二进制交叉熵损失和dice损失[22]。<spanclass="math display">\[{\mathcal{L}}_{\mathrm{BCE}}\left(p,y\right)=\sum\left(-y_{i}\logp_{i}-\left(1-y_{i}\right)\log\left(1-p_{i}\right)\right)\]</span> ，<spanclass="math display">\[\mathcal{L}_{\mathrm{Dice}}\left(p,y\right)=1-\frac{2\sump_{i}\cdot y_{i}}{\sum p_{i}{}^{2}+\sum y_{i}{}^{2}}\]</span>​  其中，<span class="math inline">\(p_i\)</span>和<spanclass="math inline">\(y_i\)</span>分别为伪造图像中每个像素的预测标签和ground-truth。此外，我们采用度量学习损失[13]来增加[23]后伪造图像样本中真实区域和伪造区域之间的特征分布的差异。<spanclass="math display">\[{\mathcal{L}}_{q}={\frac{1}{|A_{i}|}}\sum_{k^{+}\inA_{i}}-\log{\frac{\exp\left(q\cdotk^{+}/\tau\right)}{\sum_{k_{-}}\exp\left(q\cdotk^{-}/\tau\right)}}\]</span> ​  其中，<spanclass="math inline">\(k^+\)</span>和<spanclass="math inline">\(q\)</span>表示真实区域的特征嵌入，<spanclass="math inline">\(k^−\)</span>表示伪造区域的特征嵌入。<spanclass="math inline">\(A_i\)</span>表示所有<spanclass="math inline">\(k^+\)</span>的集合。结合以上所有情况，我们的最终损失函数可以表述为：<spanclass="math display">\[\mathcal{L}=\lambda_{1}\cdot\mathcal{L}_{q}+\lambda_{2}\cdot\mathcal{L}_{\mathrm{BCE}}\+\lambda_{3}\cdot\mathcal{L}_{\mathrm{Dice}}\]</span> ​  其中，<spanclass="math inline">\(\lambda_{1}\)</span>、<spanclass="math inline">\(\lambda_{2}\)</span>、<spanclass="math inline">\(\lambda_{3}\)</span>是平衡损失函数中这三项的参数<spanclass="math inline">\((\lambda_{1}+\lambda_{2}+\lambda_{3}=1)\)</span>。在实验中，它们分别被设为0.5、0.15和0.35。</p><h1 id="实验">4实验</h1><h2 id="实验设置">4.1实验设置</h2><h3 id="数据集"><strong>数据集。</strong></h3><p>​  我们使用与[11,19]相同的数据集来训练AdaIFL。这些训练数据集包括CASIAv2 [6]、IMD2020[24]，以及由[19]创建的经过伪造的图像数据集，涵盖了各种类型的伪造。为了全面评估AdaIFL的泛化能力，我们对5个与训练集不重叠的数据集进行了基准测试，即CASIAv1 [6]、Coverage[35]、DSO-1 [4]、NIST16 [10]和MISD[16]。这些数据集覆盖了大量的伪造图像，具有不同的伪造类型和广泛分布的数据源。</p><h3 id="指标"><strong>指标。</strong></h3><p>​  在之前的大多数工作之后，我们使用像素级的曲线下面积（AUC）、F1和Union上的交集（IoU）分数作为评估指标，其中阈值默认设置为0.5。</p><p>​  <strong>实施细节。</strong><br/>​  AdaIFL使用PyTorch实现，并以端到端方式进行培训。在训练过程中，输入的图像被裁剪到1024×1024。为了防止训练数据集大小不平衡造成的偏差，我们采用[11,19]的方法，在每个训练时期对每个数据集进行相等的采样。此外，还采用了常用的数据增强技术，如翻转、缩放、模糊和JPEG压缩来增强数据的多样性。我们使用了一个Adam[17]优化器，其学习速率从2×10−4衰减到1×10−7。</p><h2 id="与最新方法的比较">4.2与最新方法的比较</h2><p>​  我们仔细选择带有开放源代码和预训练模型的方法进行测试，以确保公平的比较。此外，确保这些模型的训练数据集不与测试数据集重叠是至关重要的。最后，我们选择了8种最先进的方法，以公平的方式进行全面的比较，即TruFor[11]，HiFi-IFDL[12]，CAT-Netv2[19]，ManTraNet[37]，MVSS-Net[2]，PSCC-Net[20]，IF-OSN[36]和IML-ViT[21]。</p><h3 id="定量比较">定量比较</h3><p>​  表1为不同模型对IOU、f1、AUC评分的定量比较结果。</p><hr /><p>表1：不同图像伪造定位方法的性能比较。报告了IOU、F1和AUC评分。第一个和第二个排名分别以粗体和下划线显示。</p><figure><img src="../postimages/AdaIFL/image-20241120112901191.png"alt="image-20241120112901191" /><figcaption aria-hidden="true">image-20241120112901191</figcaption></figure><hr /><p>​  在所有比较模型中，AdaIFL达到了最先进的性能，平均IOU、F1和AUC得分分别比第二优模型高出6.6%、3.4%和1.9%。具体来说，对于F1和IOU评分，AdaIFL在CASIA、Coverage、NIST16和MISD上的定位性能最好，在DSO-1中排名第二。特别是在Coverage（复制-移动伪造数据集）上，AdaIFL的IOU和F1得分比第二优模型高出8.0%和4.6%，证明了我们的模型在抑制伪造区域和真实区域之间的特征耦合方面的突出能力。在更具挑战性的NIST16数据集上，我们的模型在IOU和F1分数上比第二优的模型高出8.7%和5.3%，在处理各种操作技术和伪造模式方面显示出非凡的泛化能力。值得注意的是，使用AUC作为度量可能会导致高估模型的性能，因为数据集的伪造像素和真实像素之间的比例高度不平衡。然而，我们的模型在所有数据集上都获得了最好的AUC分数。</p><h3 id="定性比较"><strong>定性比较</strong></h3><p>​  图5显示了不同测试图像的伪造定位结果。</p><figure><img src="../postimages/AdaIFL/image-20241120145615997.png"alt="image-20241120145615997" /><figcaption aria-hidden="true">image-20241120145615997</figcaption></figure><p>​  我们的方法在几个方面都优于目前的SOTA方法。首先，我们的方法有效地减少了误报和漏检。与其他方法将非伪造区域错误识别为伪造或遗漏许多伪造区域不同，我们的方法可以准确地定位具有尖锐边界的伪造区域。其次，该方法可以准确地定位各种复杂形状的锻造区域。这在测试图像的第五行和第六行可以明显看出，其中其他方法只能预测粗略的结果，而不能捕获详细的边界。相比之下，我们的方法可以准确地定位具有复杂形状的伪造区域。第三，我们的方法在精确定位微小和微妙的伪造区域方面表现出了特殊的能力。如第三行和第六行的测试图像所示，许多方法很难识别这些小区域，而我们的方法可以准确地定位它们。</p><h3 id="鲁棒性分析">鲁棒性分析</h3><p>​  在现实世界的场景中，伪造的图像可能会经历各种后处理操作。为了评估AdaIFL对伪造定位的鲁棒性，我们使用了常用的图像退化技术，即高斯模糊、高斯噪声、伽马校正和JPEG压缩。如表2所示，在NIST16上的测试结果表明，与其他最先进的方法相比，AdaIFL对各种降解技术表现出优越的鲁棒性。</p><hr /><p>表2：在不同失真条件下对NIST16数据集的定位性能。报告了IOU和F1的分数。</p><figure><img src="../postimages/AdaIFL/image-20241120150358022.png"alt="image-20241120150358022" /><figcaption aria-hidden="true">image-20241120150358022</figcaption></figure><hr /><p>​  此外，表2表明，过量的噪声（k =23）导致了我们的模型中的次优结果。这可能是由于路由过程受到噪声的影响，导致了专家的次优选择。我们认为，设计适当的补救机制，以确保在各种退化场景中路由的准确性，可能是有益的。</p><h2 id="消融分析">4.3消融分析</h2><p>​  在本节中，我们将从特征重要性感知和动态网络结构这两个角度来分析AdaIFL中关键组件的影响。具体来说，AdaIFL通过分别在编码器（DyE）和解码器（DyD）中定制各种专家组，将动态路由的想法整合到IFL中。特征重要性感知注意注意力（FIA）自适应地感知不同区域的重要性，其中一个关键组件是自适应标记聚合器（ATA），它将区域特征聚合成可变长度的标记，将模型的注意力导向更具鉴别性和信息丰富的区域。为了评估FIA、ATA、DyE和DyD的有效性，我们分别从AdaIFL中移除每个组件，并将测试结果与CASIA和MISD数据集上的完整设置进行比较。</p><h3 id="特征重要性感知的影响">特征重要性感知的影响</h3><hr /><figure><img src="../postimages/AdaIFL/image-20241120150855145.png"alt="image-20241120150855145" /><figcaption aria-hidden="true">image-20241120150855145</figcaption></figure><hr /><p>​  如表3中所示，用原注意力代替FIA后，IOU得分下降5.2%，CASIAF1得分下降3.1%，MISD IOU下降4.4%，MISDF1下降2.2%。去除ATA后，CASIA的IOU得分分别下降了4.4%和MISD的下降了3.9%。</p><figure><img src="../postimages/AdaIFL/image-20241120150959994.png"alt="image-20241120150959994" /><figcaption aria-hidden="true">image-20241120150959994</figcaption></figure><p>​  此外，图6显示，去除FIA或ATA会导致检测失误、误报和无法准确定位伪造区域等问题。AdaIFL受益于区域划分和自适应聚合机制，缓解了特征耦合的问题，提高了图像伪造定位的性能。</p><figure><img src="../postimages/AdaIFL/image-20241120151045785.png"alt="image-20241120151045785" /><figcaption aria-hidden="true">image-20241120151045785</figcaption></figure><p>​  此外，图4 (b)显示了变压器编码器不同阶段的特征图的GradCAM。在FIA的指导下，AdaIFL专注于在不同的阶段捕捉不同的特征。在初始阶段，该模型对伪造区域边界的局部细节进行了优先排序，强调了对边界伪造痕迹的精细感知。在随后的阶段中，它专注于在伪造区域内捕获不同级别的特征。MSFF模块通过融合这些特性，增强了模型的表达性，从而能够更好地捕获伪造的工件和法医痕迹。</p><h3 id="动态网络结构的影响">动态网络结构的影响</h3><p>​  在AdaIFL中，我们将动态路由的概念应用于基于transformer的编码器和轻量级路由器中。</p><hr /><figure><img src="../postimages/AdaIFL/image-20241120150855145.png"alt="image-20241120150855145" /><figcaption aria-hidden="true">image-20241120150855145</figcaption></figure><hr /><p>​  如表3所示，去除动态专家组会显著降低定位性能。具体来说，从编码器中删除所有动态专家组，CASIA的IOO和F1得分下降了7.4%和4.5%，MISD的IOU和F1得分下降了3.7%和1.6%。从解码器中去除动态成分后，CASIA和MISD的IOU得分分别下降了4.9%和1.5%。这清楚地说明了动态路由概念在增强模型泛化方面的关键作用。</p><h1 id="结论">5结论</h1><p>​  本文解决了图像伪造定位的两个关键挑战：<br/>​  (1)处理不同伪造图像的适应性不足，<br/>​  (2)捕获边界伪影的方法不灵活。<br/>​  为了解决这些问题，我们提出了一种新的动态和重要性感知的图像伪造定位框架（AdaIFL），该框架可以动态地处理不同的伪造图像样本，并自适应地感知不同区域的重要性，提高了模型的泛化能力。大量的实验表明，我们所提出的方法优于最先进的方法。</p><h1 id="acknowledgements"><strong>Acknowledgements</strong></h1><p>This work is supported by Shenzhen Science and Technology Program(No.JCYJ20230807120800001), and 2023 Shenzhen sustainable supportingfunds for colleges and universities (No.20231121165240001). The authorssincerely appreciate the computing environment supported by the ChinaUnicom Shenzhen Intelligent Computing Center.</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Image_Manipulation_Detection_With_Implicit_Neural_Representation_and_Limited_Supervision</title>
      <link href="/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/"/>
      <url>/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/</url>
      
        <content type="html"><![CDATA[<center><p>Image Manipulation Detection With Implicit Neural Representation andLimited Supervision</p>Zhenfei Zhang1 , Mingyang Li2 , Xin Li1 , Ming-Ching Chang1 , andJun-Wei Hsieh3 1 University at Albany, State University of New York 2Stanford University 3 National Yang Ming Chiao Tung University{zzhang45, xli48, mchang2}<span class="citation"data-cites="albany.edu">@albany.edu</span> mingyang.li@stanford.edujwhsieh@nycu.edu.tw</center><h1 id="摘要">摘要</h1><p>​  随着篡改技术的发展，图像处理检测（IMD，Image ManipulationDetection）变得越来越重要。然而，大多数最先进的（SoTA，stateof-the-art）方法都需要高质量的训练数据集，其中具有图像级和像素级注释。当应用于不同于训练数据的操纵或噪声样本时，这些方法的有效性受到影响。为了解决这些挑战，我们提出了一个统一的框架，结合了无监督和弱监督的方法的IMD。我们的方法引入了一种新的预处理阶段，基于来自隐式神经表示的可控拟合函数（INR，ImplicitNeuralRepresentation）。此外，我们引入了一种新的选择性像素级对比学习方法，它只专注于高置信度区域，从而减少了与像素级标签缺失相关的不确定性。在弱监督模式下，我们利用ground-truth图像级标签来指导自适应池化方法的预测，促进了对图像级检测的操作区域的全面探索。无监督模型采用自蒸馏训练方法进行训练，通过不同的来源从最深层获得选择高置信度的伪标签。大量的实验表明，我们提出的方法优于现有的无监督和弱监督的方法。此外，它在新的操作检测任务上有效地与完全监督的方法竞争。</p><h1 id="引言">1引言</h1><p>​  多种媒体篡改工具的出现，如[10,49,61,64,70,73]psp和人工智能编辑和生成方法，使得操纵媒体内容变得越来越方便。然而，这种可访问性也带来了广泛存在的错误信息的相关问题，这可能导致严重的安全影响。因此，开发和实现鲁棒篡改检测技术，即图像操作检测（IMD）方法，是有效降低这些风险的关键。以前的方法通常处理的基本操作操作如下：(1)拼接，包括从一个图像中获取内容并粘贴到另一个图像上，(2)复制移动，其中一个图像的部分被复制和重新定位到同一图像中的另一个位置，(3)消除，这需要擦除图像的部分，并用合成内容替换它们。<br/>​  尽管在完全监督的IMD方法方面取得了重大进展，但他们还是遇到了几个显著的挑战。</p><ul><li><p>首先，这些方法在面对看不见的操作类型时通常表现不佳。</p></li><li><p>其次，由于它们依赖于具有图像级和像素级注释的高质量训练数据集，因此将它们扩展到看不见的操作类型面临着挑战。获取这样的数据集是昂贵的，而且在许多情况下，是不切实际的，特别是考虑到现实生活中无数种类的篡改方法。</p></li><li><p>第三，虽然一些语言引导的数据集可能缺乏像素级的标签，但它们在处理现实世界的场景时具有优势。这些数据集可以潜在地增强IMD模型的泛化能力。</p></li></ul><p>​  为了解决完全监督的IMD方法的局限性，并提高对现实世界使用的泛化能力，我们建议将无监督和弱监督的方法集成到一个统一的IMD框架中。我们的框架允许训练使用单独的图像级标签，甚至没有任何标签，与许多无监督和弱监督的任务[14,30,48,52,56,72,74]对齐。与完全监督的方法相比，我们的方法具有优越的泛化能力，并且可以使用没有注释的数据集进行训练。我们的方法首先观察到，在大多数情况下，被篡改的区域表现出与真实区域的差异，比如颜色和照明的变化，这对需要精确建模区域的拟合函数提出了挑战。[63]的结果表明，隐式神经表示（INR）的可控拟合函数倾向于学习训练图像的平均表示。基于这一见解，我们提出了以下问题作为我们的假设：如果我们只在真实的图像上训练一个INR，拟合函数能有效地表示被篡改区域的特征吗？<br/>​  为了得到这个问题的答案，我们首先只使用来自CASIAv2[12]的原始图像来训练一个INR，并使用它来重建三个主流数据集。然后，我们应用完全监督的SoTA方法对重建的数据集进行评估，如图1所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119103521520.png"alt="image-20241119103521520" /><figcaption aria-hidden="true">image-20241119103521520</figcaption></figure><p>图1：我们使用三个广泛使用的评估数据集，包含真实和篡改样本进行实验。并与六种SoTA全监督IMD方法进行了性能比较。像素级F1得分计算使用篡改图像，而图像级精度计算使用真实图像计算。蓝色和橙色条分别表示通过隐式神经表示的原始数据集和重构数据集。结果表明，与原始数据集相比，所有针对重建图像的像素级检测方法的性能都有显著的下降。另一方面，使用真实图像的性能变化较小。这些分数取了CASIAv1[11]、Coverage[57]和Columbia[24]数据集上的平均值。</p><p>​  令人惊讶的是，当使用INR重建样本时，这些方法的评价结果显著下降，而在真实图像样本中的性能变化较小。这一结果导致了我们初步假设，即INR可能不能有效地捕获篡改区域的特征。为了验证这一假设，我们计算了图2中重建图像和原始图像之间的重建误差图。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110126166.png"alt="image-20241119110126166" /><figcaption aria-hidden="true">image-20241119110126166</figcaption></figure><p>图2：给出了在原始图像和重建图像之间计算的重建误差图的示例。前两行分别描述了数据样本及其对应的ground-truth掩码。前三列显示了被篡改的图像示例，而最后三列显示了真实的图像，其中ground-truth的掩码都是黑色的。显然，重建过程不能正确地重建被篡改的像素，导致错误映射中的激活。相反，在真实的样本中观察到的变化较小。</p><p>​  值得注意的是，我们在被篡改样本的被篡改区域观察到激活，而在真实样本中没有明显的差异。这一观察结果启发我们将INR作为一种预处理方法，并将重建错误图与输入的RGB图像连接起来，然后将其输入给主干。我们将这种预处理方法命名为神经表示重建（NRR，NeuralRepresentationReconstruction）。<br/>​  在使用INR进行预处理的成功之后，我们进一步探索了我们的发现，并在我们的框架中充分利用了它。从对比学习[22]中获得灵感，我们利用NRR作为对比样本生成器，并引入选择性像素级对比学习，只关注高度自信的区域。该方法有效地减轻了与缺乏像素级标签相关的不确定性，并进一步提高了弱监督性能。我们进一步将我们的方法扩展到一个完全无监督的方法，使用选定的高置信伪标签，使用自蒸馏[69]训练策略。最后，以往的SoTA方法广泛应用于全局最大池（GMP，Global-MaxPooling）或全局平均池（GAP，Global-AveragePooling）用于图像级检测。然而，GMP可能会阻碍训练，并导致不准确的预测，因为只有最具区别性的反应是反向传播的，而忽略了整个被篡改的内容。相反，由于GAP的弱激活像素，容易产生不准确性。为了克服这一限制，我们提出了一个自适应的全局平均池，它关注于高置信的篡改区域。因此，我们的方法可以产生更全面和鲁棒的图像级预测。<br/>​  在7个数据集上进行了实验评估，其中包括5个具有一般操作类型的主流数据集和2个包含不可见的篡改样本的新数据集。结果表明，我们的方法优于SoTA弱监督和无监督方法。此外，在新的操作检测任务中，我们的方法与完全监督的方法相比，取得了具有竞争力的结果。最后，我们的方法可以很容易地扩展到没有像素级标签的数据集，这显示出了增强的通用性。<br/>​  本文的贡献包括：</p><p>​  (1)我们提出了一种新的方法来实现可信的弱和无监督的IMD结果。我们的方法可以很容易地适应于没有标签或只有图像级标签的图像。</p><p>​  (2)据我们所知，我们是第一个研究内隐神经表征（INR）在IMD任务中的潜力的人。利用INR的预处理步骤证明了处理被篡改案例的有效性。</p><p>​  (3)我们引入了选择性监督，它减少了与没有标签相关的不确定性，并进一步提高了检测性能。</p><p>​  (4)大量的实验验证了我们所提出的方法的有效性，与SoTA方法相比，它在标准和新型操作类型上都具有优越的性能。</p><h1 id="相关工作">2相关工作</h1><h1 id="提出的方法">3提出的方法</h1><h2 id="整体架构">3.1整体架构</h2><p>​  图3显示了我们的IMD框架的整体体系结构。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110904314.png"alt="image-20241119110904314" /><figcaption aria-hidden="true">image-20241119110904314</figcaption></figure><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119110919864.png"alt="image-20241119110919864" /><figcaption aria-hidden="true">image-20241119110919864</figcaption></figure><p>​  基本架构由两个具有共享权重的分支组成。给定一个RGB图像<spanclass="math display">\[I\in\mathbb{R}^{H\timesW\times3}\]</span>，其中H和W分别为其高度和宽度，我们首先应用神经表示重建（NRR）将其重建为<spanclass="math display">\[I_R\in\mathbb{R}^{H\timesW\times3}\]</span>，并在<span class="math inline">\(I_R\)</span>和<spanclass="math inline">\(I\)</span>之间生成重构误差图<spanclass="math display">\[I_E\in\mathbb{R}^{H\timesW\times1}\]</span>。然后我们将<spanclass="math inline">\(I\)</span>和<spanclass="math inline">\(I_E\)</span>连接起来，将它们输入第一个分支，作为主分支。与大多数IMD方法类似，主分支在最终的特征图上使用一个简单的上采样和Sigmoid激活函数生成一个掩模。然后，我们应用Otsu的方法自适应地选择激活的区域进行图像级预测，就像在[65]中所做的那样。将重建的图像<spanclass="math inline">\(I_R\)</span>输入第二分支，作为特征匹配的互补分支。经过主干处理后，我们得到了两个特征图<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(F_R\)</span>。接下来，我们通过点积计算两个特征映射之间的特征匹配分数<spanclass="math inline">\(M\)</span>，其中真实像素往往具有更高的匹配分数，反之亦然。对于操作检测的两类分类，对<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(M\)</span>应用无监督聚类，然后将两个聚类结果相交，并将像素级对比学习专门应用于更可信的交叉特征。利用所提出的自适应分类结果进行自适应全局平均池，该池集中于高置信篡改区域进行理解图像级预测。在一种弱监督的方式下，应用ground-truth图像级标签来监督预测。在无监督的情况下，从最深层中选择一组高置信的伪标签，通过自蒸馏[69]训练策略来监督浅层预测。通过比较来自Otsu的方法和聚类技术的预测来选择高置信度的伪标签，只选择那些被两个来源一致确定的来源。</p><h2 id="神经表征重建nrr">3.2神经表征重建NRR</h2><p>​  受[63]和图12中实验观察结果的启发，我们应用NRR对输入图像进行重构。重构误差可以突出操作跟踪，从而在后续的IMD模型之前提供一个不可或缺的信息。在INR中，首先使用图像编码器将输入图像转换为特征映射<spanclass="math display">\[F_N\in\mathbb{R}^{H\times W\timesC}\]</span>，其中H和W为高度和宽度，C为特征通道的数量。输入的坐标集可以用<spanclass="math display">\[X\in\mathbb{R}^{H\times W\times2}\]</span>来表示。我们通过将连接<spanclass="math inline">\(F_N\)</span>和<spanclass="math inline">\(X\)</span>进行连接，随后将它们输入多层感知器（MLP）进行解码。NRR的表述为：<span class="math display">\[I_{R}[x,y]=M LP(F_{N}[x,y],X[x,y]),\]</span> ​  其中，<spanclass="math inline">\(I_{R}\)</span>是从<spanclass="math inline">\(I\)</span>重建的RGB像素值，<spanclass="math inline">\([x,y]\)</span>是每个像素位置。NRR的主要目标是重构<spanclass="math inline">\(I\)</span>的RGB值，用损失函数表述为： <spanclass="math display">\[\mathcal{L}_{N R R}=||I-I_{R}||_{1}\,.\]</span>​  请注意，这种重建并不能正确地描述高频像素。因此，我们应用来自[39]的位置编码来将<spanclass="math inline">\(X\)</span>映射到一个高维空间。这种位置编码可表示为：<span class="math display">\[X^{&#39;}=(\sin(2^{0}\pi X),\cos(2^{0}\piX),\cdot\cdot\cdot\cdot,\sin(2^{L-1}\pi X),\cos(2^{L-1}\pi X))\]</span>​  其中，<spanclass="math inline">\(L\)</span>是控制NRR拟合能力的预设定常数。通常情况下，<spanclass="math inline">\(L\)</span>越大，拟合就越准确。在我们的任务中，我们的目标是避免来自反映输入的NRR的输出；相反，我们希望NRR忠实地保存正常（真实）内容中的信息，同时在极端（篡改）像素中引入不忠实。我们根据经验选择<spanclass="math inline">\(L = 8\)</span>作为最优权衡。</p><h2 id="选择性对比学习">3.3选择性对比学习</h2><p>​  从NRR中获得<spanclass="math inline">\(I_R\)</span>后，我们使用<spanclass="math inline">\(I_E =(I_R−I)^2\)</span>计算<spanclass="math inline">\(I\)</span>和<spanclass="math inline">\(I_R\)</span>之间的重构误差图。然后，我们连接<spanclass="math inline">\(I_E\)</span>和<spanclass="math inline">\(I\)</span>，增强到主干的第一个（主）分支的输入。对于第二个（互补）分支的输入，我们发送<spanclass="math inline">\(I_R\)</span>来进行特征匹配。我们使用ResNet50[23]作为主干，它由四个阶段组成，匹配以前的弱监督方法。两个分支的权重共享。经过主干网处理后，我们从不同的输入源获得了2个特征输出<spanclass="math inline">\(F\)</span>和<spanclass="math inline">\(F_R\)</span>。然后，我们使用点积计算特征匹配分数<spanclass="math inline">\(M\)</span>为： <spanclass="math display">\[{M}_{x,y}=\sigma\left(\frac{P(F_{R}^{x,y})\cdotP(F^{x,y})}{\sqrt{C}}\right),\]</span> ​  其中，<spanclass="math inline">\({M}_{x,y}\)</span>是在空间位置<spanclass="math inline">\((x,y)\)</span>上的相似度得分。项目头<spanclass="math inline">\(P(\cdot)\)</span>包含2个卷积层和ReLU激活层。<spanclass="math inline">\(\sigma(\cdot)\)</span>表示sigmoid激活函数，<spanclass="math inline">\(\sqrt{C}\)</span>提供归一化。<br/>​  由于NRR能够正确地再现真实的像素（而不是被篡改的像素），<spanclass="math inline">\(M\)</span>中的高匹配分数往往对应于图像的真实部分。相比之下，低分数往往对应于图像的篡改的区域。由于缺乏ground-truth来监督最终特征，我们采用无监督聚类进行类似于[3,37,41,44,47,58]的伪造/原始聚类，并假设元素较少的聚类是被篡改的聚类。这一假设与当前操作数据集的真实情况相一致。原因是，在大多数情况下，被篡改的区域通常比真实的区域要小得多。<br/>​  理想情况下，我们可以通过InfoNCE[22]对<span class="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>应用像素级对比学习像[58]一样。然而，我们发现这种方法在我们的实验中效果并不好，因为由于缺乏ground-truth掩膜，聚类置信度可能较低。为了解决这个问题，我们对<spanclass="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>的聚类结果相交，并将相交的聚类表示为<spanclass="math inline">\(C_1\)</span>。在交集之后，我们将有2个集群，无论其是真实的还是被篡改的，因为它们来自于两个不同来源的相同的预测。因此，我们只将InfoNCE应用于交叉像素进行对比学习，而保持模糊像素不变。这种选择性对比学习损失的表述为：<span class="math display">\[\mathcal{L}_{S CL}=-\log\frac{\frac{1}{J}\sum_{j\in[1,J]}\exp(q\cdotk_{j}^{+}/\tau)}{\sum_{i\in[1,K]}\exp(q\cdot k_{i}^{-}/\tau)},\]</span>​  其中<span class="math inline">\(q\)</span>是一个编码查询；<spanclass="math inline">\(J\)</span>和<spanclass="math inline">\(K\)</span>分别是被选择的正键和负键的数量；<spanclass="math inline">\(\tau\)</span>是一个温度超参数。我们将正键<spanclass="math inline">\(k_{j}^{+}\)</span>设置为与原始区域相关的像素，而负键<spanclass="math inline">\(k_{i}^{-}\)</span>对应于与被篡改区域相关的像素。</p><h2 id="自适应全局平均池化agap">3.4自适应全局平均池化AGAP</h2><p>​  许多现有的方法使用全局最大池（GMP）和全局平均池（GAP）来进行图像级预测，以确定输入是真实的还是被篡改的。然而，GMP可能会阻碍训练，并导致不准确的预测，因为只有最具区别性的反应是反向传播的，而忽略了整个被篡改的内容。全局平均池（GAP）容易出现由于弱激活像素造成的不准确性。<br/>​  为了解决这些挑战，我们引入了自适应全局平均池（AGAP），它侧重于高置信度的篡改区域，用于全面的图像级预测。利用两个聚类结果的交集（在第3.3节中讨论），我们首先从聚类的角度将全局平均池（GAP）专门应用于相交的被篡改区域。然而，仅依赖于无监督聚类可能不能保证在没有地面真实标签的所有输入类型上的最佳性能和鲁棒性。正如在[32]中所讨论的，当图像直方图表现为双峰分布时，Otsu的方法表现良好，而聚类提供了灵活性和处理更复杂的直方图的能力。因此，我们结合Otsu和聚类来增强图像级预测和训练的鲁棒性。具体来说，GAP应用于Otsu和交叉聚类结果的篡改响应，用图像级标签进行损失计算。关于Otsu的方法和聚类的进一步细节可以在他们各自的论文[15,43]中找到。</p><h2 id="弱监督和无监督的imd">3.5弱监督和无监督的IMD</h2><p>​  在弱监督的IMD设置中，我们利用ground-truth图像级标签来监督使用二值交叉熵（BCE）损失的预测训练，即：<span class="math display">\[\mathcal{L}_{B CE}(g,\hat{g})=-(1-g)\log(1-\hat{g})-g\log(\hat{g}),\]</span>​  其中，<span class="math inline">\(g\)</span>和<spanclass="math inline">\(\hat{g}\)</span>分别为ground-truth值和预测得分。以弱监督的方式进行的最终分类损失是两个BCE损失的总和，并将两个池化结果与g进行比较。<br/>​  在没有使用标签的无监督IMD设置中，我们采用了自蒸馏训练策略[69]，使用来自最深层的伪标签作为教师来监督浅层输出。<br/>​  为了简化浅层的预测结果并减少计算开销，主干每个中间阶段的分类头在信道维度中使用空间平均池，将其重塑为单通道特征图。接下来是一个s型函数和全局最大池化。在传统的自蒸馏方法中，将地面真实损失和自蒸馏相结合可以提高整体性能，但这种方法不适用于无监督的环境。我们的实验表明，仅仅依靠自蒸馏并不能产生令人满意的结果，因为从最深层的输出可能缺乏准确性，阻碍了训练过程和整体性能。<br/>​  从选择性监督方法[31]中汲取灵感，这被证明在处理噪声标签数据集方面是有效的，我们利用它的概念，基于特征表示和给定标签之间的对齐来选择训练例子。然而，在我们的无监督设置中，标签的缺失带来了一个挑战。为了克服这一障碍，我们比较了Otsu和聚类方法获得的预测，只选择两种来源一致预测的预测作为自蒸馏训练的伪标签。<br/>​  在伪标签选择中，超过0.5的预测被认为是篡改样本。与弱监督设置类似，我们在选择的伪标签和浅层预测之间使用BCE损失来进行监督。在推理过程中，排除了浅层中的所有分类头，以避免不必要的参数。<br/>​  训练目标。我们首先将通过<spanclass="math inline">\(\mathcal{L}_{NRR}\)</span>训练的NRR作为预训练模型，在IMD训练过程中所有权重冻结。为了简单起见，我们使用符号<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>来表示在无监督方法和弱监督方法中进行分类的损失函数，尽管如上所述略有不同。<br/>​  我们提出的IMD的总损失，记为<spanclass="math inline">\(\mathcal{L}_{total}\)</span>，是使用BCE损失和选择性像素级对比学习损失的分类损失的加权和：<span class="math display">\[\mathcal{L}_{t o t al}=\alpha\mathcal{L}_{c l s}+\beta\mathcal{L}_{S C L}\]</span>​  其中，<span class="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>是加权超参数。</p><h1 id="实验">4实验</h1><p>​  <strong>数据集：</strong>我们的模型只使用CASIAv2[12]进行训练，其中包括7491个真实样本和5063个篡改图像。对于标准IMD任务的评估，我们使用了广泛使用的基准测试，包括CASIAv1[11]、Coverage[57]、Columbia[24]、IMD2020 [42]和NIST16 [19]。CASIAv1[11]由拼接和复制移动图像组成。Coverage[57]只包含使用一些后处理方法的复制移动样本。Columbia[24]由363张未压缩图像组成，平均分辨率为938×720。NIST16[19]和IMD2020[42]只包含被篡改的图像，适用于像素级评估。这些数据集涵盖了传统的操作类型，包括拼接、复制-移动和删除。对于涉及新的或更复杂的操作类型的评估，我们使用IEdit[51]和MagicBrush[68]，这是两个语言驱动的数据集，包含各种新的操作类型，如动作变化和光线变化。<br/>​  <strong>评估指标：</strong>我们使用IOU和F1分数，包括像素级的F1分数P-F1，图像级的F1分数I-F1，以及组合的F1分数C-F1。C-F1分数通过调和平均值同时统计了像素级和图像级的性能，提供了一个整体的性能比较。所有F1分数和IOU分数均使用0.5作为固定阈值进行计算。由于在IEdit[51]中缺乏像素级掩模，我们包括图像级ACC以进行额外的评估。<br/>​  <strong>实现细节：</strong>我们采用ResNet50[23]作为骨干，模型使用PyTorch [45]实现，参数随机初始化。我们应用AdamW[35]作为优化器。NRR中的多层感知器（MLP）遵循三隐藏层架构。NRR训练了120个轮次，初始学习速率为2×10−4，并应用权重衰减。对弱监督模式下的IMD模型进行了50次训练，初始学习率为0.0005，权值衰减。对于无监督模型，我们训练了20个轮次代，初始学习率为0.0001，应用权重衰减。图像增强仅限于随机翻转和裁剪。我们使用固定的阈值0.5从特征映射中提取二进制掩模，与之前的方法一致。弱监督训练的超参数α和β分别设置为1.0和0.1，无监督训练分别设置为1.0和0.3。对于聚类算法，我们使用了K-means[34]。</p><h2 id="与sota方法的比较">4.1与SoTA方法的比较</h2><p>​  为了与SoTA方法进行公平的比较，我们选择了源代码是公开可用的方法。应用于比较的无监督方法有NOI[38]、CFAl [17]、MCA [1]、NoisePrint [9]和IVC [8]，而弱监督方法包括FCN[46]和WSCL[65]。<br/>​  此外，我们使用两个新的操作数据集进行了实验，并将我们的方法与完全监督的方法进行了比较，包括RRUNet[2], Mantra-Net [60], SPAN [25], PSCC-Net [33], Trufor [20], CAT-Net[29],Hifi-Net [21], CR-CNN [62], ObjectFormer [54], and MVSS-Net[5]。<br/>​  <strong>与SoTA无监督方法的比较：</strong>由于无监督方法假设所有图像都包含篡改部分，他们将所有测试图像分类为篡改。因此，它们不适合进行图像级评估。我们进行了像素级实验，比较了它们定位篡改区域的能力，如表1所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113617234.png"alt="image-20241119113617234" /><figcaption aria-hidden="true">image-20241119113617234</figcaption></figure><p>​  我们可以观察到，在五个广泛使用的标准操作基准中，我们提出的方法在无监督设置中比其他无监督方法获得了最好的检测性能。<br/>​  <strong>与SoTA弱监督方法的比较：</strong>表2为弱监督SoTA方法的实验结果。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113655069.png"alt="image-20241119113655069" /><figcaption aria-hidden="true">image-20241119113655069</figcaption></figure><p>​  除了在Columbia[24]数据集中的图像级别上的F1(I-F1)得分外，我们的方法在所有其他指标上都优于SoTA方法。与WSCL相比，Columbia的I-F1得分相对较低，我们认为原因是Columbia没有后处理，所以我们的方法可能对篡改不是很敏感。然而，尽管存在这个问题，我们的方法在Columbia数据集中实现了最好的定位性能。<br/>​  <strong>比较使用新的操作数据集：</strong>为了显示我们的方法的泛化能力。我们在表3中的两个新的操作检测数据集上使用完全监督和弱监督的方法进行评估。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119113937715.png"alt="image-20241119113937715" /><figcaption aria-hidden="true">image-20241119113937715</figcaption></figure><p>​  我们可以看到，完全监督的方法不能适应新的操作类型，导致低检测性能，即使它们使用了一个非常大的具有图像级和像素级标签的合成训练数据集。相比之下，我们的方法在使用极少的训练数据而只使用图像级标签的情况下，取得了具有竞争力的性能。<br/>​  <strong>可视化结果：</strong>我们在图4中展示了一些与SoTA方法相比的可视化结果。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114153339.png"alt="image-20241119114153339" /><figcaption aria-hidden="true">image-20241119114153339</figcaption></figure><p>​  我们的方法可以更好地定位被篡改的区域，即使没有使用像素级的标签。然而，由于缺乏像素级的标签，我们的模型不能准确地检测到被篡改的边缘。我们的方法的这些结果是由弱监督模型产生的。</p><h2 id="消融研究">4.2消融研究</h2><p>​  我们进行了几项消融研究来评估每个建议成分的有效性。对于这些研究，我们使用了CASIAv1[12]和NIST16 [19]数据集。</p><p>​  <strong>提出的组件的有效性：</strong>我们引入了三个新的组件：使用神经表示重建（NRR）的预处理阶段，选择性像素对比学习（SCL）和自适应全局平均池（AGAP）的非/弱监督IMD。在弱模式下进行的消融研究见表4。很明显，随着我们所提出的模块的逐步集成，模型检测篡改的整体能力不断提高。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114349893.png"alt="image-20241119114349893" /><figcaption aria-hidden="true">image-20241119114349893</figcaption></figure><p>​  <strong>伪标签选择（PLS）：</strong>在我们的无监督方法中，我们引入了PLS，它专门利用来自两个来源的高可信度伪标签来监督自蒸馏训练过程中的浅层预测。表5检查了PLS的影响。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114437758.png"alt="image-20241119114437758" /><figcaption aria-hidden="true">image-20241119114437758</figcaption></figure><p>​  在没有PLS的实验中，我们使用主分支的图像级预测作为伪标签来指导浅层预测。所提出的PLS在提高无监督性能方面是有效的。</p><p>​  <strong>自适应全局平均池：</strong>为了证明所提出的AGAP的优越性，我们使用不同的池方法在弱监督设置下进行了消融研究，包括全局最大池（GMP）、全局平均池（GAP）、广义平均池（GeM）[50]和全局光滑池（GsM）[55]。结果如表6所示。</p><figure><imgsrc="../postimages/Image-Manipulation-Detection-With-Implicit-Neural-Representation-and-Limited-Supervision/image-20241119114507800.png"alt="image-20241119114507800" /><figcaption aria-hidden="true">image-20241119114507800</figcaption></figure><p>​  同样，所提出的AGAP也取得了最好的性能，突出了其优越性。</p><h1 id="结论">5结论</h1><p>​  我们提出了一个新的框架，集成了无监督和弱监督方法的图像操作检测（IMD）。我们的方法具有一个开创性的预处理步骤，利用了一个来自隐式神经表示的可控拟合函数，为其提供了一个操作区域的先验。此外，我们提出了一种选择性像素级对比学习技术，该技术对高可信度的区域进行优先排序，减少了由于缺乏像素级标签而产生的不确定性。对于图像级预测，我们引入自适应全局平均池来彻底探索用于检测和鲁棒训练的操作区域。在无监督模式下，我们实现了伪标签选择，从较深的层中选择高可信度的预测作为伪标签，通过自蒸馏训练方法来监督较浅的层中的预测。大量的实验验证了我们的方法的有效性，证明了比现有的无监督和弱监督的方法更好的性能。值得注意的是，我们的方法在检测新的操作方面与完全监督的方法有效地竞争，展示了其在现实场景中的鲁棒性。<br/>​  这项工作的局限性包括被篡改区域边缘的不准确定位，导致比groundtruth更大的预测掩模。<br/>​  未来的工作包括开发更强大的模型和有效的预滤波器，以提高像素级的检测性能。<br/>​  <strong>Acknowledgements:</strong>This work is supported by the DARPA Semantic Forensics (SemaFor) Programunder contract HR001120C0123 and NSF CCSS-2348046.</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EfficientNet</title>
      <link href="/EfficientNet/"/>
      <url>/EfficientNet/</url>
      
        <content type="html"><![CDATA[<h1id="efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">EfficientNet:Rethinking Model Scaling for Convolutional Neural Networks</h1><h3 id="quickstart">Quickstart</h3><p>Install with <code>pip install efficientnet_pytorch</code> and load apretrained EfficientNet with:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from efficientnet_pytorch import EfficientNet</span><br><span class="line">model = EfficientNet.from_pretrained(&#x27;efficientnet-b0&#x27;)</span><br></pre></td></tr></table></figure><h3 id="overview">Overview</h3><p>This repository contains an op-for-op PyTorch reimplementation of <ahref="https://arxiv.org/abs/1905.11946">EfficientNet</a>, along withpre-trained models and examples.<br/>The goal of this implementation isto be simple, highly extensible, and easy to integrate into your ownprojects. This implementation is a work in progress -- new features arecurrently being implemented.</p><p>Details about the models are below:</p><table><thead><tr class="header"><th><em>Name</em></th><th><em># Params</em></th><th><em>Top-1 Acc.</em></th><th><em>Pretrained?</em></th></tr></thead><tbody><tr class="odd"><td><code>efficientnet-b0</code></td><td>5.3M</td><td>76.3</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b1</code></td><td>7.8M</td><td>78.8</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b2</code></td><td>9.2M</td><td>79.8</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b3</code></td><td>12M</td><td>81.1</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b4</code></td><td>19M</td><td>82.6</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b5</code></td><td>30M</td><td>83.3</td><td>✓</td></tr><tr class="odd"><td><code>efficientnet-b6</code></td><td>43M</td><td>84.0</td><td>✓</td></tr><tr class="even"><td><code>efficientnet-b7</code></td><td>66M</td><td>84.4</td><td>✓</td></tr></tbody></table><h4 id="example-feature-extraction">Example: Feature Extraction</h4><p>You can easily extract features with<code>model.extract_features</code>:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from efficientnet_pytorch import EfficientNet</span><br><span class="line">model = EfficientNet.from_pretrained(&#x27;efficientnet-b0&#x27;)</span><br><span class="line"></span><br><span class="line"># ... image preprocessing as in the classification example ...</span><br><span class="line">print(img.shape) # torch.Size([1, 3, 224, 224])</span><br><span class="line"></span><br><span class="line">features = model.extract_features(img)</span><br><span class="line">print(features.shape) # torch.Size([1, 1280, 7, 7])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Survey_on_Deep_Clustering</title>
      <link href="/Survey-on-Deep-Clustering/"/>
      <url>/Survey-on-Deep-Clustering/</url>
      
        <content type="html"><![CDATA[<h2 id="深度生成表示学习">3.2 深度生成表示学习</h2><p>​  另一种深度无监督表示学习方法在于生成模型。生成方法假设数据푥是由潜在表示ℎ生成的，然后从数据中反向推导出表示<spanclass="math inline">\(p(h|x)\)</span>的后验。其中，最典型的方法是变分自动编码器（VAE）[102]。VAE采用方差推理技术，最大化数据似然值的证据下界（ELBO，theevidence lower bound）： <span class="math display">\[\logp(x)\geq\mathbb{E}_{q(h|x)}\left[\log p(x|h)\right]-D_{KL}(q(h|x)\|p(h))\]</span> ​  其中<span class="math inline">\(D_{KL}(\cdot\|\cdot)\)</span>表示两个分布之间的kl-散度，<spanclass="math inline">\({p}(h)\)</span>是潜在表征的先验分布，<spanclass="math inline">\(q(h|x;\varphi)\)</span>是表征的变分后验来近似真后验（即<spanclass="math inline">\(q(h|x;\varphi)~\approx~p(h|x)\)</span>），可以用识别网络<spanclass="math inline">\(\varphi\)</span>进行建模。利用再参数化技巧[102]和蒙特卡罗近似[97]，可以通过反向传播从方程(1)中有效地学习后验。<br/>​  <strong>分析。</strong>深度生成模型具有一些优点，如灵活、可解释和能够重新创建数据点。将生成式表示模型转换为深度聚类任务，使聚类模型能够继承这些优势。</p><h2 id="互信息最大化表示法学习">3.3 互信息最大化表示法学习</h2><p>​  互信息（MI，Mutual information）[103]是度量随机变量<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>之间依赖性的一个基本量，其表述为： <spanclass="math display">\[T(X;Y)=\int\log{\frac{d\mathbb{F}_{XY}}{d\mathbb{E}_{X}\otimes\mathbb{P}_{Y}}}d\mathbb{P}_{X Y}\]</span>​  其中，<span class="math inline">\(\mathbb{P}_{XY}\)</span>为联合分布，<spanclass="math inline">\(\mathbb{F}_{X}=\int_{Y}d\mathbb{P}_{XY}\)</span>和<spanclass="math inline">\({\mathbb{P}_{Y}}=\int_{X}d\mathbb{P}_{XY}\)</span>为边际分布，P푋⊗P푌为边际分布的乘积。传统的互信息估计[106]只适用于离散变量或已知的概率分布。最近，MINE[9]被提出用于用深度神经网络来估计互信息。广泛使用的互信息估计是Jensen-Shannon散度（JSD，Jensen-Shannondivergence）[143]，其公式为： <span class="math display">\[{\cal I}_{J SD}(X;H)=\mathbb{E}_{\mathbb{R}_{XH}}\left[-\operatorname{sp}(-D(x,h))\right]-\mathbb{E}_{\mathbb{R}_{X}\times\mathbb{R}_{H}}\left[\operatorname{sp}(D(x,h))\right]\]</span>​  其中，<span class="math inline">\(\operatorname{sp}(x)\;=\;\log\left({1}+e^{x}\right)\)</span>是软加函数。<spanclass="math inline">\(D\)</span>是一个由神经网络建模的判别器函数。另一个流行的互信息估计是InfoNCE[148]，它将在第3.4小节中介绍。受益于神经估计，互信息在无监督表示学习[7,75]中被广泛应用。更具体地说，通过最大化不同层[7]或数据实例[75]的不同部分之间的互信息来学习表示，从而保证表示的一致性。这可以被看作是对自我监督学习的早期尝试，这对后来的工作有广泛的影响。<br/>​  <strong>分析。</strong>互信息作为相关性和依赖性的基本度量方法，有几个优点。深度聚类任务的主要优点是，由互信息度量的变量不局限于相同的维度和语义空间，如实例和聚类。详细的应用程序将在第4.4小节和第5.4.2小节中进行介绍。与基于自动编码器的深度生成表示学习类似，互信息最大化方法的目标也是实例化的，这在捕获实例之间的关系方面也可能存在上述问题。然而，互信息估计中的边际分布依赖于所有的观测样本。换句话说，实例之间的关系是隐式捕获的，这也提高了深度聚类的性能。</p><h2 id="对比表示学习">3.4 对比表示学习</h2><p>​  对比学习是近年来最流行的无监督表示学习技术之一。其基本思想是将正对拉近，而将负对推远，这也被称为实例辨别。对比学习的代表性目标是InfoNCE损失[148]，公式为：<span class="math display">\[\mathcal{L}_{I n f o N CE}=-\log\sum_{i=1}^{N}\frac{\exp\left(f\left(h_{i},h_{i}^{^{\mathcal{T}}}\right)/\tau\right)}{\sum_{j=1}^{N}\exp\left(f\left(h_{i},h_{j}^{^{\mathcal{T}}}\right)/\tau\right)}\]</span>​  其中<span class="math inline">\(h_{i}\)</span>为锚定样本的表示，<spanclass="math inline">\(h_{i}^{\mathcal{T}}\)</span>为正样本表示，<spanclass="math inline">\(h_{j}^{\mathcal{T}}\)</span>为负样本表示，<spanclass="math inline">\(f\)</span>为相似函数，<spanclass="math inline">\(\tau\)</span>为温度参数[74]。正样本通常通过数据增强进行，数据类型不同。例如，图像数据[30]的翻转、旋转和裁剪增强，图数据[113,217]的节点下降、边缘扰动、属性掩蔽和子图采样。负样本是从数据集[30]中其他实例的增强视图或旧的负表示[72]的动量更新内存库中选择的，这可以看作是噪声的近似。<br/>​  <strong>分析。</strong>对对比学习进行了理论分析，大量证据表明，对比学习学习的表征有利于聚类任务。在[196]中，对比学习用两个特性来解释：正对特征的对齐和超球面上特征分布的均匀性。对齐属性鼓励具有相似特征或语义类别的样本在低维空间中保持接近，这对聚类至关重要。这种鉴别能力也在监督方式[101]中得到了证明。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-modal Manipulation</title>
      <link href="/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/"/>
      <url>/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/</url>
      
        <content type="html"><![CDATA[<center>Unified Frequency-Assisted Transformer Framework for Detecting andGrounding Multi-modal Manipulation</center><center>Huan Liu1,2 · Zichang Tan3 · Qiang Chen3 · Yunchao Wei1,2 · Yao Zhao1,2· Jingdong Wang3</center><h1 id="摘要">摘要</h1><p>​  由于面部伪造和文本错误信息的广泛传播，检测和接地多模态媒体操纵（DGM4，Detectingand grounding multi-modal mediamanipulation）已经变得越来越重要。在本文中，我们提出了统一频率辅助变压器框架，命名为UFAFromer，来解决DGM4问题。与以往仅关注图像（RGB）域来描述视觉伪造特征的最先进的方法不同，我们另外引入了频域作为补充观点。通过利用离散小波变换，我们将图像分解为多个频率子带，捕获丰富的人脸伪造伪影。然后，我们提出的频率编码器，结合带内和带间的自关注，明确地聚合了不同子带内和跨的伪造特征。此外，为了解决图像和频域之间的语义冲突，开发了伪造感知相互模块，进一步实现不同图像和频率特征的有效交互，从而产生对齐和全面的视觉伪造表示。最后，基于视觉和文本伪造特征，我们提出了一个统一的解码器，它包括两个对称的跨模态交互模块，负责收集特定模态的伪造信息，以及一个负责聚合两种模式的融合交互模块。提出的统一解码器将我们的UFAfrorr定义为统一框架，最终简化了整体架构，促进了优化过程。在包含多个扰动的DGM4数据集上的实验结果表明，我们的框架比以前的方法具有优越的性能，在该领域设置了一个新的基准。</p><p><strong>关键词</strong>人脸和文字操作检测；检测和接地；统一；频率辅助</p><h1 id="介绍">1介绍</h1><p>​  近年来，互联网见证了虚假媒体的普及（Zheng et al., 2020; Juefei-Xuet al.,2022），如人脸和伪造图像、深度伪造视频、文本假新闻。随着深度学习的进步，易于创建超现实的内容，使安全和隐私成为一个严重问题，例如，身份欺诈面临伪造（Liuet al., 2021a; Zhang et al., 2019; Liu et al., 2022b, 2021b, 2024a, b,2023a, 2022a)和虚假信息文本假新闻（Ying et al., 2023; Zhou et al.,2023）。为了应对这些日益增长的威胁，研究人员表现出了极大的关注，并提出了各种检测方法，包括面部伪造检测（Miaoet al., 2023; Guan et al., 2022;Miao et al., 2022; Tan et al.,2022)和文本伪造检测（Zhu et al., 2022; Zellers et al.,2019)，关注单模式（即图像或文本）伪造。之前框架中的另一行是多模态伪造检测（Luoet al., 2021a; Khattar et al.,2019），它同时利用了图像和文本模式，并在伪造检测方面取得了更好的结果。这些框架只预测给定的可疑输入的二进制类（即真实的或虚假的），这只是简单地将多模态伪造检测视为一个二进制分类任务。</p><figure><imgsrc="../postimages/Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-modal-Manipulation/image-20250107113241178.png"alt="image-20250107113241178" /><figcaption aria-hidden="true">image-20250107113241178</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Circle Loss A Unified Perspective of Pair Similarity Optimization</title>
      <link href="/Circle-Loss/"/>
      <url>/Circle-Loss/</url>
      
        <content type="html"><![CDATA[<center>Circle Loss: A Unified Perspective of Pair Similarity Optimization</center><h1 id="摘要">摘要</h1><p>​  本文提出了一种关于深度特征学习的对相似度优化的视点，旨在使类内相似度最大化，类间相似度最小。我们发现了大多数的损失函数，包括triplet损失和softmax交叉熵损失，将<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>嵌入到相似度对中，并寻求减少<spanclass="math inline">\((s_n−s_p)\)</span>。这种优化方式是不灵活的，因为每个相似度得分的惩罚强度被限制为相等。我们的直觉是，如果一个相似度得分远远偏离了最优值，就应该强调它。为此，我们简单地重新加权每个相似度，以突出较少优化的相似度分数。它造就了一个<strong>Circle损失</strong>，由于它的圆形决策边界而命名。<strong>Circle损失</strong>对于两种基本的深度特征学习范式有一个统一的范式，即使用类级标签和成对标签进行学习。在分析上，我们表明，与损失函数优化<spanclass="math inline">\((s_n−s_p)\)</span>相比，<strong>Circle损失</strong>提供了一种更灵活的对收敛目标更明确的优化方法。通过实验，我们证明了<strong>Circle损失</strong>在各种深度特征学习任务中的优越性。在人脸识别、人的再识别以及几个细粒度的图像检索数据集上，所取得的性能与现有的技术水平相当。</p><h1 id="介绍">1.介绍</h1><p>​  本文对两种基本的深度特征学习范式进行了相似性优化分析，即从具有类级标签的数据和具有成对标签的数据中进行学习。前者采用分类损失函数（例如，软最大交叉熵损失[25,16,36]）来优化样本和权向量之间的相似性。后者利用一个度量损失函数（例如，三联体损失[9,22]）来优化样本之间的相似性。在我们的解释中，这两种学习方法之间没有内在的区别。它们都寻求最小化类间相似度<spanclass="math inline">\(s_n\)</span>，也寻求最大化类内相似度<spanclass="math inline">\(s_p\)</span>。<br/>​  从这个角度来看，我们发现许多流行的损失函数（例如，triplet损失[9,22]，softmax交叉熵损失及其变体[25,16,36,29,32,2]）具有相似的优化模式。它们都将<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>嵌入到相似度对中，并寻求减少<spanclass="math inline">\((s_n−s_p)\)</span>。在<spanclass="math inline">\((s_n−s_p)\)</span>中，增加<spanclass="math inline">\(s_p\)</span>相当于减少<spanclass="math inline">\(s_n\)</span>。我们认为这种对称优化方式容易出现以下两个问题。</p><ul><li><strong>缺乏进行优化的灵活性。</strong>在<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>上的惩罚强度被限制为相等。给定指定的损失函数，关于<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>的梯度具有相同的振幅（详见第2节）。在某些角落的情况下，例如，<spanclass="math inline">\(s_p\)</span>很小，并且<spanclass="math inline">\(s_n\)</span>已经接近0（图1(a)中的“a”），它继续以较大的梯度惩罚<spanclass="math inline">\(s_n\)</span>。它是低效的和非理性的。</li></ul><hr /><figure><img src="../postimages/Circle-Loss/image-20241014230803132.png"alt="image-20241014230803132" /><figcaption aria-hidden="true">image-20241014230803132</figcaption></figure><p>图1：流行的还原优化方式<spanclass="math inline">\((s_n−s_p)\)</span>与所提出的还原优化方式<spanclass="math inline">\((\alpha_ns_n−\alpha_ps_p)\)</span>的比较。<br/>  (a)还原<spanclass="math inline">\((s_n−s_p)\)</span>容易进行不灵活的优化（A、B和C相对于<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>都有相等的梯度），以及模糊的收敛状态（在决策边界上的T和T0都是可以接受的）。<br/>  (b)在<spanclass="math inline">\((\alpha_ns_n−\alpha_ps_p)\)</span>下，<strong>Circle损失</strong>动态调整其对<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>的梯度，从而受益于一个灵活的优化过程。对于A，它强调增加<spanclass="math inline">\(s_p\)</span>；对于B，它强调减少<spanclass="math inline">\(s_n\)</span>。此外，它有利于圆形决策边界上的指定点T收敛，建立一个确定的收敛目标。</p><hr /><ul><li><strong>模糊的收敛状态。</strong>优化<spanclass="math inline">\((s_n−s_p)\)</span>通常会导致<spanclass="math inline">\(s_n−s_p=m\)</span>（m为边际）的决策边界。这个决策边界允许模糊性（例如，图1(a)中的“<spanclass="math inline">\(T\)</span>”和“<spanclass="math inline">\(T^{\prime}\)</span>”）来收敛。例如，有<spanclass="math inline">\(\{s_{n},s_{p}\}=\{0.2,0.5\}\)</span>，而<spanclass="math inline">\(T^{\prime}\)</span>有<spanclass="math inline">\(\left\{s_{n}^{\prime},s_{p}^{\prime}\right\}=\{0.4,0.7\}\)</span>。它们都获得了边际m= 0.3。但是，通过相互比较，我们发现<spanclass="math inline">\(s_{n}^{\prime}\)</span>和<spanclass="math inline">\(s_{p}\)</span>之间的差距只有0.1。因此，模糊收敛影响了特征空间的可分性。</li></ul><p>​  有了这些见解，我们就有了一种直觉，即不同的相似性得分应该有不同的惩罚强度。如果一个相似度得分偏离最优值，它应该受到很强的惩罚。否则，如果一个相似度得分已经接近最优值，那么它就应该进行轻微的优化。为此，我们首先将<spanclass="math inline">\((s_n−s_p)\)</span>推广为<spanclass="math inline">\((\alpha_ns_n−\alpha_ps_p)\)</span>，其中<spanclass="math inline">\(\alpha_n\)</span>和<spanclass="math inline">\(\alpha_p\)</span>是独立的加权因子，允许<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>在不同的速度学习。相似度得分偏离最优值越远，加权因子就会越大。这样的优化结果是决策边界<spanclass="math inline">\(\alpha_ns_n−\alpha_ps_p=m\)</span>，在<spanclass="math inline">\((s_n,s_p)\)</span>空间中产生一个圆的形状，因此我们将所提出的损失函数命名为<strong>Circle损失</strong>。<br/>​  由于简单，<strong>Circle损失</strong>本质上从以下三个方面重塑了深度特征学习的特征：<br/>​  <strong>首先，这是一个统一的损失函数。</strong>从统一相似对优化的角度出发，我们提出了两种基本学习范式的类级标签和成对标签学习。<br/>​  <strong>第二，灵活的优化。</strong>在训练过程中，反向传播到<spanclass="math inline">\(s_n(s_p)\)</span>的梯度将被<spanclass="math inline">\(\alpha_n(\alpha_p)\)</span>放大。那些弱优化的相似性得分将有更大的权重因子，并因此得到更大的梯度。如图1(b)所示，对A、B、C的优化是不同的。<br/>​  <strong>第三，有明确的收敛状态。</strong>在圆形决策边界上，<strong>Circle损失</strong>倾向于指定的收敛状态（图1(b)中的“T”），如第3.3节所示。相应地，它建立了一个明确的优化目标，有利于可分性。<br/>​  本文的主要贡献总结如下：</p><ul><li>我们提出了<strong>Circle损失</strong>，一个简单的损失函数的深度特征学习。通过在监督下对每个相似度得分进行重新加权，有利于优化灵活、确定收敛目标的深度特征学习。</li><li>我们提出的<strong>Circle损失</strong>与兼容性的类级标签和成对的标签。略有修改下，<strong>Circle损失</strong>将退化为triplet损失或softmax交叉熵损失。</li><li>我们对各种深度特征学习任务进行了广泛的实验，如人脸识别、人的再识别、汽车图像检索等。在所有这些任务中，我们证明了<strong>Circle损失</strong>的优越性与性能与现有的技术相当。</li></ul><h1 id="统一的视角">2.统一的视角</h1><p>​  深度特征学习的目的是最大化类内相似性<spanclass="math inline">\(s_p\)</span>，以及减少类间相似性<spanclass="math inline">\(s_n\)</span>。例如，在余弦相似度度量下，我们期望<spanclass="math inline">\(s_{p}\to1\)</span>和<spanclass="math inline">\(s_{n}\to0\)</span>。<br/>​  为此，使用<strong>类级标签学习</strong>和使用<strong>成对标签学习</strong>是两种基本范式。它们通常被认为是分开的，彼此之间的w.r.t与损失函数显著不同。给定类级标签，第一个基本上学习将每个训练样本分类为目标类，例如分类损失。L2-Softmax[21]，Large-marginSoftmax[15]，AngularSoftmax[16]，NormFace[30]，AMSoftmax[29]，CosFace[32]，ArcFace[2]。这些方法也被称为基于代理的学习，因为它们优化了样本和代表每个类的一组代理之间的相似性。相比之下，给定成对标签，第二个直接学习特征空间中的成对相似性（即样本之间的相似性），因此不需要代理，例如，约束损失[5,1]，三联体损失[9,22]，提升结构损失[19]，n对损失[24]，直方图损失[27]，角损失[33]，基于边际损失[38]，多相似性损失[34]等。<br/>​  本文从单一的角度来看待这两种学习方法，不偏好基于代理的相似性或基于成对的相似性。给定特征空间中的一个样本x，假设有K个类内相似度得分，L个类间相似度得分。我们将这些相似度得分分别表示为<spanclass="math inline">\(\{s_{p}^{i}\}\left(i=1,2,\cdot\cdot\cdot\,K\right)\)</span>和<spanclass="math inline">\(\{s_{n}^{j}\}\left(j=1,2,\cdot\cdot\cdot,L\right)\)</span>。<br/>​  为了最小化每个<spanclass="math inline">\(s_{n}^{j}\)</span>以及最大化<spanclass="math inline">\(s_{p}^{i}\)</span>，<spanclass="math inline">\(\left(\forall i\ \in\ \{1,2,\cdot\cdot\cdot\,\,K\},\forall j\ \in\ \{1,2,\cdot\cdot\cdot\,\,L\}\right)\)</span>，我们提出了一个统一的损失函数： <spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{u n i} &amp;=\log\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}-s_{p}^{i}+m))\right]\\&amp;=\mathrm{log}\left[1+\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}+m))\sum_{i=1}^{K}\exp(\gamma(-s_{p}^{i}))\right]\end{aligned}\]</span>​  其中<span class="math inline">\(\gamma\)</span>是一个尺度因子，<spanclass="math inline">\(m\)</span>是一个更好的相似性分离的边际。<br/>​  等式1是直观的。它遍历每一个相似度对来减少<spanclass="math inline">\((s_{n}^{j}\,-\,s_{p}^{i})\)</span>。我们注意到，通过轻微的修改，它可以退化为三联体损失或分类损失。</p><p>​  <strong>给定类级标签，</strong>我们计算了分类层中<spanclass="math inline">\(x\)</span>和权重向量<spanclass="math inline">\(w_{i}\ (i=1,2,\cdot\cdot\cdot\ ,\N)\)</span>（N是训练类别数）之间的相似性得分。<br/>​  具体来说，我们通过：<spanclass="math inline">\(s_{n}^{j}\;\;=\;\;w_{j}^{\mathsf{T}}{x}/{\big(}||w_{j}|||x||{\big)}\)</span>（<spanclass="math inline">\(w_{j}\)</span>是第j个非目标权重向量）得到（N−1）类间相似性得分。此外，我们得到了一个单一的类内相似性评分（省略了上标）<spanclass="math inline">\(s_{p}\;=\;w_{y}^{\mathsf{T}}x/(||w_{y}|||x||)\)</span>。有了这些先决条件，等式1退化为AM-Softmax[29,32]，这是Softmax损失的一个重要变体（即，Softmax交叉熵损失）： <spanclass="math display">\[\begin{aligned}{\mathcal{L}}_{am}&amp;=\log\left[1+\sum_{j=1}^{N-1}\exp(\gamma(s_{n}^{j}+m))\exp(-\gammas_{p})\right]\\&amp;=-\log\frac{\exp(\gamma(s_{p}-m))}{\exp(\gamma(s_{p}-m))+\sum_{j=1}^{N}\exp(\gammas_{n}^{j})}\end{aligned}\]</span> ​  此外，当<spanclass="math inline">\(m=0\)</span>，等式2进一步退化为Normface[30]。如果将内积替换余弦相似度，并且设置<spanclass="math inline">\(\gamma=1\)</span>，它最终退化为Softmax损失。</p><p>​  <strong>给定成对的标签，</strong>在小批量中，我们计算x和其他特征之间的相似性得分。具体来说，<spanclass="math inline">\(s_{n}^{j}=(x_{n}^{j})^{\mathsf{T}}x/(||x_{n}^{j}|||x||)\)</span>（<spanclass="math inline">\(x_{n}^{j}\)</span>是负样本集<spanclass="math inline">\({\mathcal{N}}\)</span>中的第j个样本）和<spanclass="math inline">\(s_{p}^{j}=(x_{p}^{j})^{\mathsf{T}}x/(||x_{p}^{j}|||x||)\)</span>（<spanclass="math inline">\(x_{p}^{j}\)</span>是正样本集<spanclass="math inline">\({\mathcal{P}}\)</span>中的第i个样本）。相应地，<spanclass="math inline">\(K=|P|,\,L=|{\mathcal{N}}|\)</span>。等式1与硬挖掘[22,8]退化为triplet损失：<span class="math display">\[\begin{aligned}{\mathcal{L}}_{t ri}&amp;=\operatorname*{lim}_{\gamma\to+\infty}{\frac{1}{\gamma}}{\mathcal{L}}_{uni}\\&amp;=\operatorname*{lim}_{\gamma\to+\infty}\frac{1}{\gamma}\log\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp(\gamma(s_{n}^{j}-s_{p}^{i}+m))\right]\\&amp;=\operatorname*{max}\left[s_{n}^{j}-s_{p}^{i}+m\right]_{+}\end{aligned}\]</span>​  具体来说，我们注意到在等式3中，“<spanclass="math inline">\(\sum\exp({\cdot})\)</span>”操作采用Lifted-Structure损失[19]，N-pair损失[24]，多相似性损失[34]等，在样品之间进行“软化”硬挖掘。<spanclass="math inline">\(\gamma\)</span>的扩大逐渐增强了挖掘强度，当<spanclass="math inline">\(\gamma\implies+\infty\)</span>时，导致了[22,8]的典型硬挖掘。</p><p>​  <strong>梯度分析。</strong>等式2和等式3显示了triplet损失，Softmax损失及其几个变体可以被解释为等式1的特定情况。换句话说，它们都在优化<spanclass="math inline">\((s_n−s_p)\)</span>。</p><hr /><figure><img src="../postimages/Circle-Loss/image-20241015154934450.png"alt="image-20241015154934450" /><figcaption aria-hidden="true">image-20241015154934450</figcaption></figure><p>图2：损失函数的梯度。(a)triplet的损失。(b)AM-Softmax损失。(c)提出的Circle损失。triplet损失和AM-Softmax损失都缺乏优化的灵活性。<spanclass="math inline">\(s_p\)</span>（左）和<spanclass="math inline">\(s_n\)</span>（右）的梯度被限制为相等，并在收敛时突然下降(相似对B）。例如，在A处，类内相似度评分<spanclass="math inline">\(s_p\)</span>已经接近1，并且仍然有一个很大的梯度。此外，决策边界与<spanclass="math inline">\(s_p=s_n\)</span>平行，允许模糊收敛。相比之下，提出的Circle损失分配不同的梯度，取决于它们到最优的距离。对于A（<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>都很大），Circle损失的重点是优化<spanclass="math inline">\(s_n\)</span>。对于B，由于<spanclass="math inline">\(s_n\)</span>显著减少，Circle损失减少了它的梯度，从而加强了一个温和的惩罚。Circle损失具有一个圆形的决策边界，并促进了准确的收敛状态。</p><hr /><p>​  在只有一个<span class="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>的小场景下，我们在图2(a)和(b)中可视化了中的triplet损失和AM-Softmax损失的梯度，从中我们得出以下观察结果：</p><ul><li>首先，在损失达到其决策边界之前（梯度消失之前），相对于<spanclass="math inline">\(s_p\)</span>和<spanclass="math inline">\(s_n\)</span>的梯度是相同的。状态A具有<spanclass="math inline">\(\{s_{n},s_{p}\}\ =\\{0.8,0.8\}\)</span>，表示良好的类内紧致性。然而，A相对于<spanclass="math inline">\(s_p\)</span>仍然有较大的梯度。它导致了在优化过程中缺乏灵活性。</li><li>第二，梯度在收敛前保持（大致）不变，并在收敛时发生突然的下降。状态B更接近决策边界，并且比A优化得更好。然而，损失函数（triplet损失和AMSoftmax损失）对A和B施加近似相等的惩罚。这是缺乏灵活性的另一个证据。</li><li>第三，决策边界（白色虚线）平行于<spanclass="math inline">\(s_{n}-s_{p}=m\)</span>。该边界上任意两点（如图1中的<spanclass="math inline">\({\boldsymbol{T}}\)</span>和<spanclass="math inline">\({\boldsymbol{T}}^{\prime}\)</span>）的相似间隙等于m，因此具有相同的困难。换句话说，损失函数最小化<spanclass="math inline">\((s_{n}-s_{p}+m)\)</span>在<spanclass="math inline">\({\boldsymbol{T}}\)</span>或<spanclass="math inline">\({\boldsymbol{T}}^{\prime}\)</span>的收敛性上不偏不倚，并且容易出现模糊收敛。关于这个问题的实验证据，请参见第4.6节。</li></ul><p>​  这些问题源于最小化<spanclass="math inline">\((s_{n}-s_{p})\)</span>的优化方式，其中减少<spanclass="math inline">\(s_n\)</span>相当于增加<spanclass="math inline">\(s_p\)</span>。在下面的第3节中，我们将把这种优化方式转换为更一般的优化方式，以促进更高的灵活性。</p><h1 id="一个新的损失函数">3.一个新的损失函数</h1><h2 id="自定速度的加权">3.1.自定速度的加权</h2>​  我们考虑通过允许每个相似度评分根据当前优化状态以自己的速度学习来增强优化灵活性。我们首先忽略了等式1中的边际项m，并通过以下方式将统一损失函数转换为提出的Circle损失：$$<span class="math display">\[\begin{aligned}{\mathcal{L}}_{c i r c le}&amp;=\mathrm{log}\left[1+\sum_{i=1}^{K}\sum_{j=1}^{L}\exp{\left({\gamma(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i}}\right)}\right]\\&amp;=\log\left[1+\sum_{i=1}^{L}\exp(\gamma\alpha_{n}^{j}s_{n}^{j})\sum_{i=1}^{K}\exp(-\gamma\alpha_{p}^{i}s_{p}^{i})\right]\end{aligned}\]</span><span class="math display">\[​&amp;emsp;&amp;emsp;其中$\alpha_{n}^{j}$和$\alpha_{p}^{i}$为非负加权因子。&lt;br/&gt;​&amp;emsp;&amp;emsp;等式4来源于等式1，通过将$(s_{n}^{j}-s_{p}^{i})$推广为$(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i})$。在训练过程中，当反向传播到${s}_{n}^{j}\,(s_{p}^{i})$时，相对于$(\alpha_{n}^{j}s_{n}^{j}-\alpha_{p}^{i}s_{p}^{i})$的梯度将与$\alpha_{n}^{j}(\alpha_{p}^{i})$相乘。当相似度得分偏离其最优值时（即，$s^j_n$时为$O_n$，$s^i_p$时为$O_p$），应得到一个较大的加权因子，以得到较大梯度的有效更新。为此，我们以一种自定速度的方式来定义$\alpha_{p}^{i}$和$\alpha_{n}^{j}$：\]</span><spanclass="math display">\[\begin{cases}\alpha_{p}^{i}=[O_{p}-s_{p}^{i}]_{+},\\\alpha_{n}^{j}=[s_{n}^{j}-{O}_{n}]_{+}\\\end{cases}\]</span><p>$$ ​  其中[·]+为“零截止”操作，以确保<spanclass="math inline">\(\alpha_{p}^{i}\)</span>和<spanclass="math inline">\(\alpha_{n}^{j}\)</span>为非负值。<br/>​  <strong>讨论。</strong>在监督下重新调整余弦相似度是现代分类损失[21,30,29,32,39,40]中常见的做法。传统上，所有的相似性得分都具有相同的尺度因子<spanclass="math inline">\(\gamma\)</span>。当我们将一个分类损失函数中的softmax值看作是一个样本属于某一类的概率时，等量的重新缩放是很自然的。相比之下，Circle损失在重新缩放之前用一个独立的加权因子乘以每个相似度分数。因此，它摆脱了平等的重新缩放的约束，并允许更灵活的优化。除了更好的优化的好处外，这种重新加权（或重新缩放）策略的另一个意义还涉及到潜在的解释。Circle损失放弃了将样本以大概率分类为目标类的解释。相反，它具有相似度对的优化视角，这与两种学习范式相兼容。</p><h2 id="类内和类间的边际">3.2.类内和类间的边际</h2><p>​  在损失函数优化<spanclass="math inline">\((s_{n}-s_{p})\)</span>中，添加一个边际<spanclass="math inline">\(m\)</span>加强了优化[15,16,29,32]。由于<spanclass="math inline">\(s_{n}\)</span>和<spanclass="math inline">\(-s_{p}\)</span>处于对称位置，<spanclass="math inline">\(s_{n}\)</span>的正边际等于于<spanclass="math inline">\(s_{p}\)</span>的负边际。因此，它只需要一个单一的边际<spanclass="math inline">\(m\)</span>。在Circle损失中，<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>处于不对称位置。当然，它需要<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>各自的边际，其公式如下： <spanclass="math display">\[\mathcal{L}_{c i r c le}=\log\left[1+\sum_{i=1}^{L}\exp(\gamma\alpha_{n}^{j}(s_{n}^{j}-\Delta_{n}))\sum_{i=1}^{K}\exp(-\gamma\alpha_{p}^{i}(s_{p}^{i}-\Delta_{p}))\right]\]</span>​  其中<span class="math inline">\(\Delta_{n}\)</span>和<spanclass="math inline">\(\Delta_{p}\)</span>分别为类间和类内的边距。<br/>​  基本上，等式6中的Circle损失期望<spanclass="math inline">\({s}_{p}^{i}\gt\Delta_{p}\)</span>和<spanclass="math inline">\({s}_{n}^{j}\lt\Delta_{n}\)</span>。通过推导决策边界，进一步分析了<spanclass="math inline">\(\Delta_{n}\)</span>和<spanclass="math inline">\(\Delta_{p}\)</span>的设置。为简单起见，我们考虑了二值分类的情况，其中决策边界是通过<spanclass="math inline">\(\alpha_{n}(s_{n}-\Delta_{n})-\alpha_{p}(s_{p}-\Delta_{p})=0\)</span>得到的。并结合等式5、决策边界为：<spanclass="math display">\[(s_{n}-\frac{O_{n}+\Delta_{n}}{2})^{2}+(s_{p}-\frac{O_{p}+\Delta_{p}}{2})^{2}=C\]</span>​  其中，<spanclass="math inline">\(C=((O_{n}-\Delta_{n})^{2}+(O_{p}-\Delta_{p})^{2})/4\)</span>。<br/>​  等式7显示了决策边界为圆形，如图1(b)。所示圆的中心在<spanclass="math inline">\(s_{n}\,=\,(O_{n}\,+\,\Delta_{n})/2,s_{p}\,=\,(O_{p}\,+\,\Delta_{p})/2\)</span>处，其半径等于<spanclass="math inline">\({\sqrt{C}}\)</span>。<br/>​  在等式中有五个超参数，即等式5的<spanclass="math inline">\(O_p\)</span>、<spanclass="math inline">\(O_n\)</span>和等式6的<spanclass="math inline">\(\gamma\)</span>，<spanclass="math inline">\(\Delta_{p}\)</span>和<spanclass="math inline">\(\Delta_{n}\)</span>。我们通过设置<spanclass="math inline">\({O}_{p}=1+m,{O}_{n}=-m,\Delta_{p}=1-m\)</span>和<spanclass="math inline">\(\Delta_{n}=m\)</span>来减少超参数。因此，在等式7中的决策边界减少为：<spanclass="math display">\[\left(s_{n}-0\right)^{2}+\left(s_{p}-1\right)^{2}=2m^{2}\]</span>​  有了等式8中定义的决策边界，我们对Circle损失有了另一个直观的解释。其目的是优化<spanclass="math inline">\({s}_{m}\rightarrow1\)</span>和<spanclass="math inline">\({s}_{n}\rightarrow0\)</span>。参数<spanclass="math inline">\(m\)</span>控制着决策边界的半径，可以看作一个松弛因子。换句话说，Circle损失期望<spanclass="math inline">\(s_{p}^{i}\gt 1-m\)</span>和<spanclass="math inline">\(s_{n}^{j}\ltm\)</span>。<br/>​  因此，只有两个超参数，即尺度因子<spanclass="math inline">\(\gamma\)</span>和松弛度<spanclass="math inline">\(m\)</span>。我们将在第4.5节中实验分析<spanclass="math inline">\(m\)</span>和<spanclass="math inline">\(\gamma\)</span>的影响。</p><h2 id="circle损失的优点">3.3.Circle损失的优点</h2><p>​  Circle损失相对于<spanclass="math inline">\(s_{n}^{j}\)</span>和<spanclass="math inline">\(s_{p}^{i}\)</span>的梯度推导如下： <spanclass="math display">\[\frac{\partial\mathcal{L}_{c i r c l e}}{\partials_{n}^{j}}=Z\frac{\exp\left(\gamma((s_{n}^{j})^{2}-m^{2})\right)}{\sum_{l=1}^{L}\exp\left(\gamma((s_{n}^{l})^{2}-m^{2})\right)}\gamma(s_{n}^{j}+m),\]</span></p><p>和</p><p><span class="math display">\[\frac{\partial\mathcal{L}_{c i r c le}}{\partials_{p}^{i}}=Z\frac{\exp\left(\gamma((s_{p}^{i}-1)^{2}-m^{2})\right)}{\sum_{k=1}^{K}\exp\left(\gamma((s_{p}^{k}-1)^{2}-m^{2})\right)}\gamma(s_{p}^{i}-1-m),\]</span></p><p>​  其中<span class="math inline">\(Z=1-\exp(-\mathcal{L}_{c i r c le})\)</span>。<br/>​  在二值分类的小场景下（或只有一个<spanclass="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>时），我们在图2 (c)中可视化了<spanclass="math inline">\(m\)</span>的不同设置下的梯度，从中我们得出以下三个观察结果：</p><ul><li><span class="math inline">\(s_n\)</span>和<spanclass="math inline">\(s_p\)</span>的平衡优化。我们曾提及过，损失函数最小化<spanclass="math inline">\((s_{n}-s_{p})\)</span>在<spanclass="math inline">\(s_p\)</span>和<spanclass="math inline">\(s_n\)</span>上总是具有相等的梯度，这是不灵活的。相比之下，Circle损失展现出动态的惩罚强度。在指定的相似对<spanclass="math inline">\(\{s_{n},s_{p}\}\)</span>中，如果<spanclass="math inline">\(s_p\)</span>比<spanclass="math inline">\(s_n\)</span>更好（如图2(c)中的<spanclass="math inline">\(A=\{0.8,0.8\}\)</span>），Circle损失赋予<spanclass="math inline">\(s_n\)</span>的梯度更大（反之亦然），从而更优先的降低<spanclass="math inline">\(s_n\)</span>。平衡优化的实验证据详见第4.6节。</li><li>逐渐减弱的梯度。在训练开始时，相似性得分偏离最佳值很远，并获得较大的梯度（如图2（c）中的“A”）。随着训练逐渐接近收敛，相似度得分上的梯度相应衰减（如图2(c)中的“B”），进行了温和的优化。第4.5节的实验结果表明，学习效果对<spanclass="math inline">\(\gamma\)</span>的各种设置都是鲁棒性的(在等式6中)，我们将其归因于自动衰减的梯度。</li><li>一个（更）明确的收敛目标。Circle损失具有循环决策边界，有利于<spanclass="math inline">\({\boldsymbol{T}}\)</span>的收敛而不是<spanclass="math inline">\({\boldsymbol{T}}^{\prime}\)</span>的收敛（图1）。这是因为<spanclass="math inline">\({\boldsymbol{T}}\)</span>与决策边界上的所有其他点相比，<spanclass="math inline">\(s_p\)</span>和<spanclass="math inline">\(s_n\)</span>之间的差距最小。换句话说，<spanclass="math inline">\({\boldsymbol{T}}^{\prime}\)</span>在<spanclass="math inline">\(s_p\)</span>和<spanclass="math inline">\(s_n\)</span>之间的差距较大，而且本身就更难维持。相比之下，最小化<spanclass="math inline">\((s_{n}-s_{p})\)</span>的损失具有一个齐次的决策边界，即决策边界上的每一个点到达决策边界都具有相同的困难。在实验中，我们观察到，Circle损失导致收敛后的相似度分布更为集中，详见章节4.6和图5。</li></ul><h1 id="实验">4.实验</h1><p>​  我们综合评估了两种基本学习方法下的有效性：给定类级标签学习和给定成对的标签学习。对于前一种方法，我们在人脸识别（4.2节）和人的再识别（4.3节）任务上评估了我们的方法。对于后一种方法，我们使用细粒度的图像检索数据集（第4.4节），它们相对较小，鼓励使用成对标签进行学习。我们证明了Circle损失在这两种情况下都是有效的。第4.5节分析了这两个超参数的影响，即等式6中的尺度因子<spanclass="math inline">\(\gamma\)</span>和等式8中的松弛因子<spanclass="math inline">\(m\)</span>。我们证明了在合理的设置下，Circle损失是鲁棒的。最后，第4.6节通过实验证实了Circle损失的特性。</p>]]></content>
      
      
      <categories>
          
          <category> 损失函数 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>公式识别工具</title>
      <link href="/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/"/>
      <url>/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="latexocr">LatexOCR</h1><p>#1. 介绍</p><p>​  latexocr是一个识别数学（其他）公式转换为LaTeX代码的软件，也是github上一个开源项目，其是本地部署的公式OCR识别工具。（这样就不必花钱在网站上识别了）<br/>​  其GitHub地址为：https://github.com/lukas-blecher/LaTeX-OCR</p><h1 id="使用">2. 使用</h1><p>​  首先安装conda，其次在虚拟环境在安装python（Python3.7+），然后进去虚拟环境、安装PyTorch ，再然后使用如下代码安装：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install &quot;pix2tex[gui]&quot;</span><br></pre></td></tr></table></figure><p>​  安装完成后，在虚拟环境下的终端输入latexocr（ps：第一次使用会自动下载数据集），会弹出以下界面：</p><figure><imgsrc="../postimages/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/image-20241011215427134.png"alt="image-20241011215427134" /><figcaption aria-hidden="true">image-20241011215427134</figcaption></figure><p>​  然后，按下[Alt+S]来框选要识别的公式，得到以下界面：</p><figure><imgsrc="../postimages/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/image-20241011215542881.png"alt="image-20241011215542881" /><figcaption aria-hidden="true">image-20241011215542881</figcaption></figure><p>​  这样公式就识别成功了，如果出现了错误，可以降低Temperature来提高识别准确率。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised_Semantic_Segmentation</title>
      <link href="/Unsupervised-Semantic-Segmentation/"/>
      <url>/Unsupervised-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<h1id="boosting-unsupervised-semantic-segmentation-with-principal-mask-proposal">1.Boosting Unsupervised Semantic Segmentation with Principal MaskProposal</h1><h2 id="摘要">摘要</h2><p>​  无监督语义分割的目的是通过在没有任何形式注释的图像语料库中识别全局语义类别，自动将图像分割成语义上有意义的区域。基于自监督表示学习的最新进展，我们关注于如何利用这些大型的预训练模型，用于无监督分割的下游任务。我们提出了PriMaPs-PrincipalMaskProposals-基于图像的特征表示将图像分解为语义上有意义的掩模。这使得我们可以通过使用随机期望最大化算法PriMaPs-EM将类原型拟合到PriMaPs中来实现无监督语义分割。尽管PriMaPs-EM概念简单，但它在各种预训练的主干模型，包括DINO和DINOv2，以及不同的数据集，如城市景观、coco-st-Stuff之间导致竞争结果。重要的是，当PriMaPs-EM正交应用于当前最先进的无监督语义分割管道时，它能够提高结果。代码可在https://github.com/visinf/primaps上找到。</p><h2 id="primaps-principal-mask-proposals">PriMaPs: Principal MaskProposals</h2><p>​  在本文中，我们利用了自监督表示学习的最新进展（Caron等人，2021；Oquab等人，2024年），用于无监督语义分割的特定下游任务。我们的方法是基于观察到，这些预先训练过的特征已经表现出内在的空间相似性，捕获语义相关性，从而为拟合全局伪类表示提供指导。</p><p><strong>简单的baseline</strong></p><p>​  考虑一个简单的基线，将K-means聚类应用于DINO ViT特征（Caron etal.，2021）。令人惊讶的是，这已经导致了相当好的无监督语义分割结果，例如，大约15%的平均IoU分割27个类别（Cordts et al.，2016），见Tab1。然而，在相同的特征空间和地面真实标签之间的监督线性探测——理论上界——导致明显优于近36%的平均结果。鉴于这一差距和该方法的简单性，我们得出结论，与之前的工作不同，直接获得语义分割有宝贵的潜力（汉密尔顿等人，2022；Seong等人，2023）。</p><p><strong>从K-means到PriMaPs-EM</strong></p><p>​  当检查K-means基线和最先进的方法时（汉密尔顿等人，2022；Seong等人，2023），见图4，可以定性地观察到，在各自的预测中，更多的局部一致性已经导致更少的错误分类。我们的灵感来自（Drineaset al.，2004；Ding &amp;He，2004），他指出，由主成分跨越的PCA子空间是K-means聚类的松弛解决方案。我们观察到，主成分对对象-和以场景为中心的图像特征具有较高的语义相关性(cf。图1）。我们利用优势特征模式对图像进行迭代分割，通过图像特征与各自的第一主成分的余弦相似性来识别。我们命名得到的类不可知的图像分解PriMaPs-主掩码建议。我们观察到，主成分对对象-和以场景为中心的图像特征具有较高的语义相关性(cf。图1）。我们利用优势特征模式对图像进行迭代分割，通过图像特征与各自的第一主成分的余弦相似性来识别。我们命名得到的类不可知的图像分解PriMaPs-主掩码建议。PriMaPs直接起源于SSL表示，并指导无监督语义分割的过程。如图3所示，我们基于优化的方法，PriMaPs-EM，在从冻结的深度神经网络主干计算出的SSL特征表示上操作。该优化实现了在PriMaPs指导下的聚类目标的随机EM。具体来说，PriMaPs-EM通过优化两个相同大小的向量集，以全局一致的方式适合于类原型，其中一个是另一个的指数移动平均（EMA）。我们证明，PriMaPs-EM能够精确地无监督地分割图像到语义上有意义的区域，同时相对轻量级，并正交于大多数以前的无监督语义分割方法。</p><h3 id="派生primap">派生PriMaP</h3><p>​  我们从一个冻结的预先训练的自监督主干模型<spanclass="math inline">\({\mathcal F}:\mathbb{R}^{3\times h\timesw}\longrightarrow\mathbb{R}^{C\times H\timesW}\)</span>开始，它将图像<spanclass="math inline">\(I\in\mathbb{R}^{3\times h\timesw}\)</span>嵌入到一个密集的特征表示<spanclass="math inline">\(f\in\mathbb{R}^{C\times H\times W}\)</span>：<span class="math display">\[f={\mathcal{F}}(I)\,\]</span>​  这里，C表示密集特征的通道维数，H=h/p，W=w/p，p对应主干的输出步幅。基于此图像表示，下一步是将图像分解为具有语义意义的掩模，为拟合全局类原型提供局部分组先验。<br/>​  初始主掩码建议。为了识别图像I中的初始主掩模方案，我们利用主成分分析分析了其特征的空间统计相关性。具体地说，我们考虑了经验特征协方差矩阵<span class="math display">\[\Sigma={\frac{1}{HW}}\sum_{i=1}^{H}\sum_{j=1}^{W}\Bigl(f_{:,i,j}-\bar{f}\Bigr)\left(f_{:,i,j}-\bar{f}\right)^{\mathsf{T}},\]</span>​  其中，<spanclass="math inline">\(f_{:,i,j}\in\mathbb{R}^{C}\)</span>为位置（i，j）处的特征，<spanclass="math inline">\({\overline}\in\mathbb{R}^{C}\)</span>为平均特征。为了识别捕获特征分布中最大方差的特征方向，我们通过求解来寻找Σ的第一个主成分<span class="math display">\[\Sigma v=\lambda v\,\]</span>​  我们得到了第一个主分量作为最大特征值λ1的特征向量v1，利用平坦特征f可以用奇异值分解（SVD）有效地计算出来。<br/>​  为了识别一个候选区域，我们的下一个目标是计算一个到主导特征方向的空间特征相似度图。我们观察到，直接使用主方向这样做并不总是会导致足够好的定位，也就是说，图像中多个视觉概念的高度相似性，在附录A.1中进行了更详细的阐述。这可以通过首先在特征图中锚定主要的特征向量来避免。为此，我们将归一化特征空间fˆ中的余弦距离考虑为，得到了第一主分量v1的最近邻特征<spanclass="math inline">\({\tilde{f}}\in \mathbb{R}^{C}\)</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>EAGLE:Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</title>
      <link href="/EAGLE/"/>
      <url>/EAGLE/</url>
      
        <content type="html"><![CDATA[<center>EAGLE: Eigen Aggregation Learning for Object-Centric UnsupervisedSemantic Segmentation</center><center>Chanyoung Kim* Woojung Han* Dayun Ju Seong Jae Hwang†</center><center>Yonsei University</center><h1 id="摘要">摘要</h1><p>​  语义分割天生依赖于大量的像素级注释数据，导致了无监督方法的出现。其中，利用自监督视觉转换器进行无监督语义分割（USS）在表达深度特征方面取得了稳步的进展。然而，对于使用复杂对象对图像进行语义分割，一个主要的挑战仍然存在：在补丁级特征中缺乏显式的对象级语义编码。这种技术限制往往导致对具有不同结构的复杂对象的分割不足。为了解决这一差距，我们提出了一种新的方法，EAGLE，它强调无监督语义分割的对象中心表示学习。具体地说，我们介绍了EiCue，这是一种光谱技术，通过来自深度图像特征的语义相似度矩阵和来自图像的颜色亲和度的特征基来提供语义和结构线索。此外，通过将我们的以对象为中心的对比损失与EiCue结合起来，我们指导我们的模型学习具有图像内和图像间对象-特征一致性的对象级表示，从而提高语义的准确性。在COCO-Stuff、城市景观和波茨坦-3数据集上的广泛实验证明了鹰的最新结果，在复杂场景中具有准确和一致的语义分割。</p><h1 id="引言">1.引言</h1><p>​  特别是，最近的基于网络的方法经常利用一个自监督的视觉变压器（ViT）来学习补丁级的特征。虽然它们的补丁级特性被证明对进一步的USS推理步骤（例如，K-means）很有用，但底层的对象级语义并没有明确地强加在这些补丁级特性中。要掌握“对象级语义”，请考虑一个覆盖层对象的示例，如图1b的第二行所示。与任何对象一样，毯子很容易出现在不同的图像中使用不同的颜色和纹理。如果没有适当的对象级语义，对应于不同覆盖区域的特征可能会导致截然不同的特征表示。但理想情况下，对应于各种毯子的特性应该映射到相似的特性，即对象级语义。因此，如果没有仔细强加对象级语义，具有不同结构和形状的复杂对象可以很容易地划分为带有错误类标签的多个段，或者与附近不同类标签的段合并。因此，在USS中，必须付出巨大的努力来学习具有强对象级语义的本地特性（例如，补丁级）。<br/>​  我们为USS提供的以对象为中心的表示学习旨在捕获这种对象级的语义。具体来说，我们首先需要一个在以对象为中心的视图中的语义或结构线索。以往的一些研究利用K-means或超像素等聚类方法来获得语义线索[20]，但它们主要关注一般的图像模式，而不是对象的语义或结构表示。在这里，我们提出了EiCue，它通过特征基提供对象的语义和结构线索。具体地说，我们利用从ViT[6,14]得到的投影深度图像特征得到的语义相似度矩阵和图像的颜色亲和矩阵来构造图拉普拉斯算子。相应的特征基捕获了对象[32,61]的底层语义结构，为后续的对象级特征细化步骤提供了软指导。回想一下，对象的精确对象级语义必须在不同图像之间保持一致。我们的以对象为中心的对比学习框架明确地将这些特征强加为一个新的对象级的对比损失。具体来说，基于EiCue中的对象线索，我们为每个对象推导出可学习的原型，从而使图像内部和图像间的对象-特征保持一致性。通过这个全面的学习过程，我们的模型有效地捕获了图像中的固有结构，允许它精确地识别语义上可信的对象表示，这是推进现代基于特征的USS的关键。</p><p><strong>贡献。</strong>我们的主要贡献如下：</p><ul><li>我们提出EiCue，使用一个可学习的图拉普拉斯行列式，以获得对图像中的潜在语义和结构细节的更深刻的理解。</li><li>我们设计了一个以对象为中心的对比学习框架，它利用EiCue的光谱基础来构建鲁棒的对象级特征表示。</li><li>通过一系列全面的实验支持，无监督语义分割证明了我们的鹰在无监督语义分割上取得了最先进的性能。</li></ul><h1 id="方法">3.方法</h1><p>​  当我们开始描述图2中所示的完整管道时，让我们首先介绍基于预训练模型的核心USS框架，如之前的工作[16,49]。</p><figure><img src="../postimages/EAGLE/image-20241008161231496.png"alt="image-20241008161231496" /><figcaption aria-hidden="true">image-20241008161231496</figcaption></figure><p>图2。<strong>EAGLE</strong>的管道。利用拉普拉斯矩阵，它集成了分层投影的图像关键特征和颜色亲和性，该模型利用特征向量聚类来捕获定义为Meicue和M˜˜的对象级透视线索。利用Meicue的提炼知识，我们的模型进一步采用了一个以对象为中心的对比损失，利用投影特征Z和Z˜。由Z和Z˜分配的可学习的原型Φ，作为一个单一的锚，对比正对象和负对象。我们的以对象为中心的对比损失以两种不同的方式计算：内部（Lobj）和内部（Lsc）图像，以确保语义一致性。</p><h2 id="预处理">3.1.预处理</h2><p>​  <strong>未标记的图像。</strong>我们的方法完全建立在一组图像上，没有任何注释，表示为<spanclass="math inline">\(\mathbf{X}=\left\{\mathbf{x}_b\right\}_{b=1}^B\)</span>，其中B是一个小批处理中的训练图像的数量。我们还利用光度增强策略P来获得一个增强图像集<spanclass="math inline">\(\tilde{\mathbf{X}}=\left\{\tilde{\mathbf{x}}_b\right\}_{b=1}^B=P(\mathbf{X})\)</span>。<br/>​  <strong>预训练特征k。</strong>然后，对于每个输入图像<spanclass="math inline">\(\mathbfx_b\)</span>，我们使用自我监督预训练视觉transformer[6]作为图像编码器<spanclass="math inline">\(\mathcal{F}\)</span>获得分层注意关键特征从最后三个块<spanclass="math inline">\(\mathbf{K}_{L-2}=\mathcal{F}_{L-2}\left(\mathbf{x}_b\right)\)</span>，<spanclass="math inline">\(\mathbf{K}_{L-1}=\mathcal{F}_{L-1}\left(\mathbf{x}_b\right)\)</span>，<spanclass="math inline">\(\mathbf{K}_{L}=\mathcal{F}_{L}\left(\mathbf{x}_b\right)\)</span>，其中L−2，L−1，L是第三层，第二到最后层，和最后一层，分别。然后，我们将它们连接到一个单一的注意张量<spanclass="math inline">\(\mathbf{K}=\left[\mathbf{K}_{L-2} ;\mathbf{K}_{L-1} ; \mathbf{K}_L\right] \in \mathbb{R}^{H \times W \timesD_K}\)</span>。同样，我们对增广图像 <spanclass="math inline">\(\mathbf{\tilde{x}}\)</span>应用相同的程序，得到了它的注意张量<spanclass="math inline">\(\tilde{\mathbf{K}} \in \mathbb{R}^{H \times W\timesD_K}\)</span>。<br/>​  <strong>语义特征S。</strong>虽然K包含了一些基于注意机制的对象的结构信息，但由于没有足够的语义信息来进行直接推理。因此，为了进一步细化特征，我们计算了语义特征<spanclass="math inline">\(\mathbf{S}=\mathcal{S}_\theta(\mathbf{K}) \in\mathbb{R}^{H \times W \times D_S}\)</span>和<spanclass="math inline">\(\tilde{\mathbf{S}}=\mathcal{S}_\theta(\tilde{\mathbf{K}})\in \mathbb{R}^{H \times W \times D_S}\)</span>，其中<spanclass="math inline">\(S_\theta: \mathbb{R}^{H \times W \times D_K}\rightarrow \mathbb{R}^{H \times W \timesD_S}\)</span>是一个可学习的非线性分割头。为简洁起见，补丁的总数，记为H×W，将被称为N。<br/>​  <strong>推理。</strong>在推理时间内，给定一幅新图像，其语义特征S成为进一步聚类的基础，采用传统的评估设置，如K-means聚类和线性探测。因此，与之前预先训练的基于特征的USS工作[16,49]一样，训练<spanclass="math inline">\(\mathcal{S}_\theta\)</span>以无监督的方式输出强语义特征S是当代USS框架的基本框架。接下来，我们将在图2中描述管道的其余部分，这与我们对生成强大的对象级语义特征的方法贡献相对应。</p><h2id="通过特征聚合模块进行的eicue">3.2.通过特征聚合模块进行的EiCue</h2><p>​  直觉告诉我们，“语义上可信”的对象级片段是精确捕获对象结构的像素组，即使在复杂的结构方差下也是如此。例如，一个汽车部件必须包含其所有部件，包括挡风玻璃、车门、车轮等。它们都可能以不同的形状和视图出现。然而，如果没有提供对象级语义的像素级注释，这将成为推断具有零对象级结构优先级的底层结构的一个极具挑战性的任务。<br/>​  从这一实现中，我们的模型EAGLE首先基于特征相似度矩阵的特征基，得到一个强大而简单的语义结构线索，即EiCue，如图3所示。</p><figure><img src="../postimages/EAGLE/image-20241008214724018.png"alt="image-20241008214724018" /><figcaption aria-hidden="true">image-20241008214724018</figcaption></figure><p>图3.EiCue生成过程的说明。从输入的图像中，推导出颜色相似度矩阵<spanclass="math inline">\(\mathbf{A}_{color}\)</span>和语义相似度矩阵<spanclass="math inline">\(\mathbf{A}_{seg}\)</span>，并将其组合成拉普拉斯算子<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>。将Lsym的一个特征向量子集<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>聚类产生EiCue。</p><p>​  具体来说，我们使用著名的光谱聚类[7,39,51]来获得无监督的特征表示，以捕获底层的非线性结构，以处理具有复杂模式的数据。这通常只在颜色空间中工作，但可以很容易地扩展到利用由任何特征构造的相似性矩阵。我们观察到，这种光谱方法对于复杂的真实世界的图像特别有用，如图4所示。</p><figure><img src="../postimages/EAGLE/image-20241008214956647.png"alt="image-20241008214956647" /><figcaption aria-hidden="true">image-20241008214956647</figcaption></figure><p>图4.可视化在特征聚合模块中由S得到的特征向量。这些特征向量不仅区分不同的对象，还识别语义相关的区域，突出了EiCue如何有效地捕获对象的语义和边界。</p><p>​  <strong>EiCue结构。</strong>让我们详细描述构建EiCue的过程，如图3所示。总体框架一般遵循一般的谱聚类：从(1)邻接矩阵<spanclass="math inline">\(\mathbf{A}\)</span>中构造(2)拉普拉斯算子<spanclass="math inline">\(\mathbf{L}\)</span>，(3)对<spanclass="math inline">\(\mathbf{L}\)</span>进行特征分解，得到特征基<spanclass="math inline">\(\mathbf{V}\)</span>，利用特征特征进行聚类。我们将在下面描述每个步骤。</p><h3 id="邻接矩阵的构造">3.2.1邻接矩阵的构造</h3><p>​  我们的邻接矩阵由两个组成部分组成：(1)颜色相似度矩阵和(2)语义相似度矩阵。</p><p>（一）颜色相似度矩阵<spanclass="math inline">\(\mathbf{A}_{color}\)</span>：</p><p>（二）语义相似度矩阵<spanclass="math inline">\(\mathbf{A}_{seg}\)</span>：</p><p>（三）邻接矩阵<span class="math inline">\(\mathbf{A}\)</span>：</p><h3 id="特征分解">3.2.2特征分解</h3><p>​  为了构造基于<spanclass="math inline">\(\mathbf{A}\)</span>的EiCue，我们创建了一个拉普拉斯矩阵。形式上，拉普拉斯矩阵表示为<spanclass="math inline">\(\mathbf{L} =\mathbf{D}−\mathbf{A}\)</span>，其中<spanclass="math inline">\(\mathbf{D}\)</span>是<spanclass="math inline">\(\mathbf{A}\)</span>的度量矩阵，定义为<spanclass="math inline">\(\mathbf{D}(i, i)=\sum_{j=1}^N \mathbf{A}(i,j)\)</span>。在我们的方法中，我们利用归一化的拉普拉斯矩阵来增强其聚类能力。将对称归一化拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>定义为<spanclass="math inline">\(\mathbf{L}_{\text {sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}\mathbf{D}^{-\frac{1}{2}}\)</span>。然后，通过对<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>的特征分解，计算得到特征基<spanclass="math inline">\(\mathbf{V} \in \mathbb{R}^{N \timesN}\)</span>，其中每一列对应于一个唯一的特征向量。然后，我们提取与k个最小特征值对应的k个特征向量，并将它们连接到<spanclass="math inline">\(\mathbf{\hat{V}} \in \mathbb{R}^{N \timesN}\)</span>中，其中第i行对应于第i个补丁的k个维特征。</p><h3 id="可微特征聚类">3.2.3可微特征聚类</h3><p>​  在得到特征向量<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>后，我们进行特征向量聚类过程，提取的EiCue表示为<spanclass="math inline">\(\mathcal{M}_{\text {eicue }} \in\mathbb{R}^N\)</span>。为了聚类特征向量，我们利用了一个基于<spanclass="math inline">\(\hat V\)</span>和<spanclass="math inline">\(C\)</span>之间的余弦距离[36]的小批次K-means算法，记为<spanclass="math inline">\(P=\hat VC\)</span>。集群中心<spanclass="math inline">\(\mathbf{C} \in \mathbb{R}^{k \timesC}\)</span>由可学习的参数组成。为了学习C，我们进一步训练了损失定义如下：<span class="math display">\[\mathcal{L}_{\text {eig}}^{\mathrm{x}}=-\frac{1}{N} \sum_{i=1}^N\left(\sum_{c=1}^C \Psi_{i c}\mathbf{P}_{i c}\right),\]</span> ​  其中C表示预定义的类数，<spanclass="math inline">\(\Psi:=\operatorname{softmax}(\mathbf{P})\)</span>，<spanclass="math inline">\(\mathbf P_{ic}\)</span>和<spanclass="math inline">\(\Psi_{ic}\)</span>表示第i个补丁和第C个簇数。我们将同样的过程应用于增广图像<spanclass="math inline">\(\tilde{\mathrm{x}}\)</span>，得到<spanclass="math inline">\(\mathcal{L}_{\text {eig}}^{\tilde{\mathrm{x}}}\)</span>。通过最小化<spanclass="math inline">\(\mathcal{L}_{\text {eig}}=\frac{1}{2}\left(\mathcal{L}_{\text {eig}}^{\mathrm{x}}+\mathcal{L}_{\text {eig}}^{\tilde{\mathrm{x}}}\right)\)</span>，我们可以获得能够实现更有效的聚类的聚类中心。然后我们得到EiCue为<span class="math display">\[\mathcal{M}_{\text {eicue}}(i)=\underset{c}{\operatorname{argmax}}\left(\mathbf{P}_{i c}-\log\left(\sum_{c^{\prime}=1}^C \exp \left(\mathbf{P}_{ic^{\prime}}\right)\right)\right) .\]</span>​  随着聚类质心精度的提高，EiCue促进了基于语义结构的patchi到对应对象的映射。这是一个有意义的线索，可以强调不同对象之间的语义区别，从而增强特征嵌入的辨别能力。</p><p>​  <strong>备注。</strong>虽然与之前使用特征分解的工作[38]相似，但我们的方法的不同之处在于使用可训练的分割头增强特征向量S，而不是它们依赖于静态向量（即K）。我们的方法通过可微特征聚类增强了S的可学习性和适应性，允许图的拉普拉斯语义和对象语义进化。这种EiCue与学习过程的动态集成清楚地将我们的方法与之前的应用区分开来。</p><figure><img src="../postimages/EAGLE/image-20241009104707642.png"alt="image-20241009104707642" /><figcaption aria-hidden="true">image-20241009104707642</figcaption></figure><h2 id="基于eicue的objnce损失">3.3.基于EiCue的ObjNCE损失</h2><p>​  对于一个成功的语义分割任务，不仅要准确地对每个像素的类进行分类，还要聚合对象表示并创建一个反映对象语义表示的分割图。从这个角度来看，在以对象为中心的视角下学习关系在语义分割任务中尤为重要。为了捕捉对象之间的复杂关系，我们的方法结合了一个以对象为中心的对比学习策略，名为ObjNCELoss，由EiCue指导。该策略旨在细化特征嵌入S的鉴别能力，强调不同对象语义之间的区别。在继续之前，我们映射投影特征$^{N D_{Z}} <span class="math inline">\(和\)</span>{} , , ^{N D_{Z}}<span class="math inline">\(，分别使用线性投影头\)</span>{}<em>{}<spanclass="math inline">\(，来自重塑的\)</span> ^{N D</em>{Z}} <spanclass="math inline">\(和\)</span>{} , , ^{N D_{S}} <spanclass="math inline">\(。虽然\)</span>D_S<spanclass="math inline">\(和\)</span>D_Z$的实际尺寸大小保持不变，但为了便于解释，我们使用了不同的符号。</p><h3 id="对象样机">3.3.1对象样机</h3><h3 id="以对象为中心的对比损失">3.3.2以对象为中心的对比损失</h3><h2 id="总目标">3.4.总目标</h2><p>（未完不待续）</p><p>为了构造基于<spanclass="math inline">\(\mathbf{A}\)</span>的EiCue（假设是为了增强聚类效果的操作），我们使用了图的拉普拉斯矩阵，它能够很好地反映数据中的几何结构和局部关联性。以下是详细解释过程：</p><h3id="构造邻接矩阵mathbfa-mathbfa-in-mathbbrn-times-n是从特征变量中提取出来的表征数据中各个样本通常是图像中的补丁或像素之间的相似性这个相似性可以通过各种方式计算比如基于特征的欧氏距离或高斯相似性函数特征变量的大小为b-times-c-times-n其中">1.构造邻接矩阵<span class="math inline">\(\mathbf{A}\)</span><br/><spanclass="math inline">\(\mathbf{A} \in \mathbb{R}^{N \timesN}\)</span>是从特征变量中提取出来的，表征数据中各个样本（通常是图像中的补丁或像素）之间的相似性。这个相似性可以通过各种方式计算，比如基于特征的欧氏距离或高斯相似性函数。特征变量的大小为<spanclass="math inline">\(B \times C \times N\)</span>，其中：</h3><ul><li><span class="math inline">\(B\)</span>是批量大小，</li><li><spanclass="math inline">\(C\)</span>是特征的通道数（即每个特征向量的维度），</li><li><span class="math inline">\(N = H \timesW\)</span>是空间维度的展平结果，通常表示图像补丁或像素的数量。</li></ul><p>在这里，<span class="math inline">\(N = 256 \times 256 =65536\)</span>，表示图像的像素总数。</p><h3id="构造度量矩阵mathbfd-度量矩阵mathbfd是对角矩阵表示邻接矩阵mathbfa中每个节点的度它通过计算每个节点连接到其他节点的总权重来定义">2.构造度量矩阵<spanclass="math inline">\(\mathbf{D}\)</span><br/>度量矩阵<spanclass="math inline">\(\mathbf{D}\)</span>是对角矩阵，表示邻接矩阵<spanclass="math inline">\(\mathbf{A}\)</span>中每个节点的度。它通过计算每个节点连接到其他节点的总权重来定义：</h3><p><span class="math display">\[\mathbf{D}(i,i) = \sum_{j=1}^N\mathbf{A}(i,j)\]</span> 这意味着度矩阵的每个对角元素表示与节点<spanclass="math inline">\(i\)</span>相连的边的总权重。</p><h3 id="构造拉普拉斯矩阵mathbfl-拉普拉斯矩阵mathbfl表示为">3.构造拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}\)</span><br/>拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}\)</span>表示为：</h3><p><span class="math display">\[\mathbf{L} = \mathbf{D} -\mathbf{A}\]</span>这是标准的无归一化拉普拉斯矩阵，它反映了每个节点和其相邻节点之间的差异。拉普拉斯矩阵的特性使得它在图形信号处理中广泛用于捕捉图结构。</p><h3id="归一化拉普拉斯矩阵mathbfl_sym-为了增强聚类效果我们使用对称归一化的拉普拉斯矩阵mathbfl_sym其形式为">4.归一化拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span><br/>为了增强聚类效果，我们使用对称归一化的拉普拉斯矩阵<spanclass="math inline">\(\mathbf{L}_{sym}\)</span>，其形式为：</h3><p><span class="math display">\[\mathbf{L}_{\text{sym}} =\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} =\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{A}\mathbf{D}^{-\frac{1}{2}}\]</span>对称归一化拉普拉斯矩阵能够更好地处理图的不同节点度分布的问题，使得高度节点和低度节点在拉普拉斯矩阵中得到更加平衡的对待。</p><h3 id="特征分解-对mathbfl_textsym进行特征值分解得到">5.特征分解<br/>对<spanclass="math inline">\(\mathbf{L}_{\text{sym}}\)</span>进行特征值分解，得到：</h3><p><span class="math display">\[\mathbf{L}_{\text{sym}} = \mathbf{V}\mathbf{\Lambda} \mathbf{V}^{\top}\]</span> 其中，<spanclass="math inline">\(\mathbf{V} \in \mathbb{R}^{N \timesN}\)</span>是特征向量矩阵，<spanclass="math inline">\(\mathbf{\Lambda}\)</span>是对角的特征值矩阵。每一列<spanclass="math inline">\(\mathbf{V}_i\)</span>对应于一个特征向量，且与<spanclass="math inline">\(\mathbf{\Lambda}\)</span>中的特征值相对应。</p><h3id="提取特征基-我们提取与k个最小特征值对应的k个特征向量并将它们连接成矩阵mathbfhatv-in-mathbbrn-times-k这意味着">6.提取特征基<br/>我们提取与<spanclass="math inline">\(k\)</span>个最小特征值对应的<spanclass="math inline">\(k\)</span>个特征向量，并将它们连接成矩阵<spanclass="math inline">\(\mathbf{\hat{V}} \in \mathbb{R}^{N \timesk}\)</span>。这意味着：</h3><ul><li>选取<spanclass="math inline">\(k\)</span>个最小的特征值，这些特征值的特征向量能够表示数据的局部流形结构。</li><li>特征向量<span class="math inline">\(\mathbf{V}\)</span>的第<spanclass="math inline">\(i\)</span>行对应于图中第<spanclass="math inline">\(i\)</span>个节点的特征表示（通常是图像中的某个像素或补丁）。</li><li><spanclass="math inline">\(\mathbf{\hat{V}}\)</span>包含每个节点在选定的<spanclass="math inline">\(k\)</span>维特征空间中的表示，这将用于后续的聚类任务。</li></ul><h3id="聚类任务-通过提取的特征mathbfhatv可以使用常见的聚类算法如k-meansdbscan等来对图像中的像素或补丁进行聚类进而用于如语义分割图像分块或其他下游任务">7.聚类任务<br/>通过提取的特征<spanclass="math inline">\(\mathbf{\hat{V}}\)</span>，可以使用常见的聚类算法，如K-means、DBSCAN等，来对图像中的像素或补丁进行聚类，进而用于如语义分割、图像分块或其他下游任务。</h3><h3id="综述-整个过程通过从图像特征生成图的拉普拉斯矩阵经过归一化处理和特征分解从中提取低维流形的特征表示用于增强聚类能力这种方法能够捕捉特征间的局部关联和数据的流形结构非常适合于需要提取数据内部复杂关系的任务如图像分割或聚类">综述<br/>整个过程通过从图像特征生成图的拉普拉斯矩阵，经过归一化处理和特征分解，从中提取低维流形的特征表示，用于增强聚类能力。这种方法能够捕捉特征间的局部关联和数据的流形结构，非常适合于需要提取数据内部复杂关系的任务，如图像分割或聚类。</h3>]]></content>
      
      
      <categories>
          
          <category> 无监督语义分割 </category>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering</title>
      <link href="/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/"/>
      <url>/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/</url>
      
        <content type="html"><![CDATA[<p>Unsupervised Learning of Image Segmentation Based on DifferentiableFeature Clustering</p><p>Wonjik Kim∗ , Member, IEEE, Asako Kanezaki∗ , Member, IEEE, andMasayuki Tanaka, Member, IEEE</p><h1 id="摘要">摘要</h1><p>​  本研究研究了卷积神经网络（CNNs）在无监督图像分割中的应用。与监督图像分割类似，所提出的CNN为表示像素所属的簇的像素分配标签。然而，在无监督图像分割中，事先没有指定训练图像或像素的地面真实标签。因此，一旦输入了目标图像，像素标签和特征表示将被共同优化，并通过梯度下降来更新它们的参数。在提出的方法，标签预测和网络参数学习交替迭代满足以下标准：(a)像素相似特征应该分配相同的标签，(b)空间连续像素应该分配相同的标签，和(c)唯一标签的数量应该很大。虽然这些标准是不兼容的，但所提出的方法最小化了相似性损失和空间连续性损失的组合，以找到一个合理的标签分配解决方案，以很好地平衡上述标准。这项研究的贡献有四倍。首先，我们提出了一种新的端到端无监督图像分割网络，它由归一化和一个用于可微聚类的argmax函数组成。其次，我们引入了一个空间连续性损失函数，它减轻了以往工作中固定段边界的限制。第三，我们提出了一种新的分割方法的扩展，在保持效率的情况下，比现有的方法具有更好的准确性。最后，我们介绍了该方法的另一个扩展：使用经过少量参考图像预训练的网络，而不进行隐形图像分割。在几个图像分割的基准数据集上验证了该方法的有效性。</p><h1 id="介绍">1.介绍</h1><p>​  图像分割几十年来一直在计算机视觉研究中引起了人们的关注。图像分割的应用包括目标检测、纹理识别和图像压缩。在监督图像分割中，使用由一对图像对和像素级语义标签组成的集合，如“天空”或“自行车”进行训练。目的是训练一个系统，对图像像素的已知类别的标签进行分类。相比之下，无监督图像分割用于预测更一般的标签，如“前景”和“背景”。后者比前者更具挑战性。此外，将图像分割成任意数量（≥2）是极其困难的。本研究考虑了一个问题，即一个图像在没有任何先前知识的情况下被分割成任意数量的显著的或有意义的区域。<br/>​  一旦得到了像素级的特征表示，就可以通过对特征向量进行聚类来得到图像片段。然而，特征表示的设计仍然是一个挑战。所期望的特征表示在很大程度上取决于目标图像的内容。例如，如果目标是检测斑马作为前景，特征表示应该对黑白垂直条纹做出反应。因此，像素级的特征应该是描述每个像素周围的局部区域的颜色和纹理的。近年来，卷积神经网络（CNNs）已成功地应用于自主驾驶和增强现实游戏等监督学习场景中的语义图像分割。cnn不常用于完全无监督的场景；然而，它们在从图像像素中提取详细特征方面具有很大的潜力，而这对于无监督的图像分割是必要的。在CNN的高特征描述性的驱动下，提出了一种联合学习方法，它可以预测任意图像输入的未知聚类标签，并学习图像像素聚类的最优CNN参数。随后，提取每个簇中的一组图像像素作为一个段。<br/>​  进一步讨论了对于良好的图像分割所必需的聚类标签的特征。与之前关于无监督图像分割[1]，[2]的研究类似，我们假设一个好的图像分割解决方案与人类提供的解决方案很好地匹配。当一个人被要求分割一个图像时，他们很可能会创建一个片段，每个片段对应于单个对象实例的整体或显著部分。对象实例倾向于包含具有相似颜色或纹理图案的大区域。区域颜色或纹理模式进入同一簇是一种合理的图像分割策略。为了将不同的对象实例中的段分开，最好为不同模式的相邻像素分配不同的集群标签。为了便于集群分离，还考虑了一种需要大量唯一集群标签的策略。综上所述，本文介绍了以下三个关于聚类标签的预测标准：<br/>​  （a）具有相似特征的像素应该被分配到相同的标签上。<br/>​  （b）空间上连续的像素应该被分配给相同的标签。<br/>​  （c）唯一的集群标签的数量应该很大。<br/>​  在本文中，我们提出了一种基于cnn的算法，通过联合优化特征提取函数和聚类函数来满足这些条件。本文为了实现CNN的端到端学习，提出了一种利用可微函数预测聚类标签的迭代方法。该代码可以在网上找到<ahref="https://github.com/kanezaki/pytorch-unsupervised-segmentation-tip/">[code]</a>。<br/>​  本研究是之前在2018年[3]国际声学、语音和信号处理会议（ICASSP）上发表的研究成果的延伸。在之前的工作中，准则(b).采用简单的线性迭代聚类[4]进行超像素提取然而，在之前的算法的超像素提取过程中，片段的边界存在固定的局限性。在本研究中，提出了一种空间连续性损失作为减轻上述限制的替代方法。此外，还介绍了基于我们改进的无监督分割方法的两种新应用：利用用户输入的分割和利用利用不同图像的无监督学习获得的网络权值。由于该方法是完全无监督的，因此它根据图像的性质对图像进行分割，而这并不总是与用户的意图相关。作为该方法的范例应用，将涂鸦作为用户输入，并与现有方法的效果进行了比较。随后，该方法迭代获得单个输入图像的分割结果具有较高的计算代价。因此，作为该方法的另一个潜在应用，我们使用了用多个参考图像预先训练的网络权值。一旦使用该算法从多个图像中获得网络权值，固定网络就可以对一个新的看不见的图像进行分割，只要它与参考图像有点相似。并演示了该技术在视频分割任务中的应用。<br/>​  本文的贡献总结如下：</p><ul><li><p>我们提出了一种新的端到端可微的无监督图像分割网络。</p></li><li><p>我们引入了一个空间连续性损失函数，它减轻了我们之前的方法[3]的局限性。</p></li><li><p>我们提出了一种新的分割方法的扩展，在保持效率的同时，比现有的方法具有更好的准确性。</p></li><li><p>我们介绍了该方法的另一个扩展：使用少量的参考图像预先训练的网络，而不重新训练网络。</p></li></ul><h1 id="方法">3. 方法</h1><p>​  所解决的图像分割问题描述如下。为简单起见，让{}表示<spanclass="math inline">\(\{\}^N_{n=1}\)</span>，除非另有说明，其中N表示输入彩色图像<spanclass="math inline">\(\mathcal{I} = \{v_{n} \in\mathbb{R}^{3}\}\)</span>的像素数。设<span class="math inline">\(f :\mathbb{R}^3 \to \mathbb{R}^p\)</span>是一个特征提取函数，<spanclass="math inline">\(\{x_n\in\mathbb{R}^p\}\)</span>是一组图像像素的p维特征向量。聚类标签<spanclass="math inline">\(\{c_n\in\mathbb{Z}\}\)</span>由<spanclass="math inline">\(c_n = g(x_n)\)</span>分配给所有像素，其中<spanclass="math inline">\(g:\mathbb{R}^p\to\mathbb{Z}\)</span>表示映射函数。在这里，g可以是一个赋值函数，它返回最接近<spanclass="math inline">\(x_n\)</span>的簇质心的标签。对于f和g是固定的情况，利用上述公式得到{cn}。相反地，如果f和g是可训练的，而{cn}是指定的（固定的），那么上述方程可以看作是一个标准的监督分类问题。在这种情况下，如果f和g是可微的，则f和g的参数可以通过梯度下降法进行优化。然而，在本研究中，在以完全无监督的方式训练f和g的参数时，我们预测了未知的{cn}。为了实现这一点，我们解决了以下两个子问题：预测固定f和g的最优{cn}和训练固定{cn}的f和g参数。<br/>​  值得注意的是，第一节引入的三个标准是不相容的，永远不能完美地满足。使用经典方法解决这个问题的一个可能的解决方案是：对于标准(a)，对{xn}使用k-means聚类；对于标准(b)，使用到质心的距离执行图切割算法[17]；对于标准(c)，使用非参数方法确定k-means聚类中的k。然而，这些经典的方法只适用于固定的{xn}，因此求解可能是次优的。因此，我们提出了一种基于cnn的算法来解决这个问题。{xn}和{cn}的特征提取函数以满足上述所有条件的方式进行了联合优化。为了实现CNN的端到端学习，提出了一种使用可微函数预测{cn}的迭代方法。提出了一个CNN结构，如图1所示，以及一个损失函数来满足第一节中描述的三个标准。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007102733276.png"alt="image-20241007102733276" /><figcaption aria-hidden="true">image-20241007102733276</figcaption></figure><p>图1。训练CNN的算法的说明。输入图像输入CNN，使用特征提取模块提取深度特征{xn}。随后，一维（1D）卷积层计算了q维聚类空间中特征的响应向量{rn}，其中本图中的q=3。在这里，z1、z2和z3代表了集群空间的三个轴。随后，使用批处理归一化函数在集群空间的轴上对响应向量进行归一化。此外，集群标签{cn}是通过使用argmax函数将集群标识分配给响应向量来确定的。然后将聚类标签作为伪目标来计算特征相似性损失。最后，计算了空间连续性损失和特征相似性损失，并进行了反向传播。</p><p>​  提出的考虑标准(a)和标准(c)的CNN架构的概念在第三节-A中详细介绍。解决标准(a)和(b)的损失函数的概念在第三-b节中提出。使用反向传播训练CNN的细节在第三-C节中描述。</p><h2 id="a.网络架构">A.网络架构</h2><h3 id="对特征相似性的约束">1)对特征相似性的约束</h3><p>​  我们考虑为具有相似特征的像素分配相同的标签的第一个标准。所提出的解决方案是应用一个线性分类器，将每个像素的特征分类为q类。在本研究中，我们假设输入是一个RGB图像<spanclass="math inline">\(\mathcal{I}=\{\boldsymbol{v}_{n}\in{\mathbb{R}^{3}}\}\)</span>，其中每个像素值被归一化到[0,1]。{<spanclass="math inline">\(v_n\)</span>}通过M个卷积分量计算p维特征图{<spanclass="math inline">\(x_n\)</span>}，每个卷积分量由一个二维（2D）卷积、ReLU激活函数和一个批归一化函数组成，其中一批对应于单个输入图像的N个像素。在这里，我们为所有的M个组件设置了p个区域大小为3×3的滤波器。值得注意的是，这些用于特征提取的组件可以被全卷积网络（FCN）[20]等替代方案所取代。随后，通过应用一个线性分类器得到一个响应图{<spanclass="math inline">\(r_n = W_cx_n\)</span>}，其中<spanclass="math inline">\(W_{c} \in \mathbb{R}^{q\timesp}\)</span>。然后将响应映射归一化为<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，使<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>的均值和单位方差为零。标准化过程背后的动机在第三-A2节中描述。最后，通过选择<spanclass="math inline">\(r_n^{\prime}\)</span>中最大的维数，得到每个像素的聚类标签<spanclass="math inline">\(c_n\)</span>。这种分类规则被称为argmax分类。这种处理方法直观地对应于将特征向量聚类成q个聚类。最终响应的第<spanclass="math inline">\(i\)</span>簇<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>可以写成： <spanclass="math display">\[C_{i}=\{r_{n}^{\prime}\in\mathbb{R}^{q}\midr_{n,i}^{\prime}\geq r_{n,j}^{\prime}, \forall j\},\]</span>​  其中，<span class="math inline">\(r_{n,i}^{\prime}\)</span>表示<spanclass="math inline">\(r_n^{\prime}\)</span>的第<spanclass="math inline">\(i\)</span>个元素。这相当于将每个像素分配给q个代表点之间的最近点，这些点被放置在q维空间中各自的轴上的无限距离上。值得注意的是，Ci可以是∅，因此唯一的聚类标签的数量可以任意地从1到q。</p><h3 id="对唯一集群标签数量的约束">2)对唯一集群标签数量的约束</h3><p>​  在无监督的图像分割中，没有线索知道在一个图像中应该生成多少个片段。因此，唯一的集群标签的数量应该自适应图像内容。如在第三-A1节中所述，所提出的策略是将像素划分为任意数量<spanclass="math inline">\(q^{\prime}(1\leq q^{\prime}\leqq)\)</span>的簇，其中<span class="math inline">\(q\)</span>为<spanclass="math inline">\(q^{\prime}\)</span>的最大可能值。一个大的<spanclass="math inline">\(q^{\prime}\)</span>表示过分割，而一个小的<spanclass="math inline">\(q\)</span>表示过分割。为了训练一个神经网络，我们设置了初始的（最大）数量q。然后，在迭代更新过程中，通过考虑特征相似性和空间连续性约束，对相似或空间接近的像素进行集成。这一现象导致唯一的聚类标签<spanclass="math inline">\(q^{\prime}\)</span>的数量减少，即使对<spanclass="math inline">\(q^{\prime}\)</span>没有明确的限制。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007102733276.png"alt="image-20241007102733276" /><figcaption aria-hidden="true">image-20241007102733276</figcaption></figure><p>​  如图1所示，所提出的基于argmax分类的聚类函数对应于<spanclass="math inline">\(q^{\prime}\)</span>-类聚类，其中<spanclass="math inline">\(q^{\prime}\)</span>锚点对应于q轴上无穷远处的q点的一个子集。上述标准(a)和(b)只促进了像素的分组，这可能导致一个简单的解决方案，即<spanclass="math inline">\(q^{\prime}=1\)</span>。为了防止这种分割失败，引入了第三个标准准则，即对大<spanclass="math inline">\(q^{\prime}\)</span>的限制。所提出的解决方案是在使用argmax分类分配聚类标签之前，为响应映射{<spanclass="math inline">\(r_n\)</span>}插入轴内归一化过程。在这里，使用了批处理规范化[46]。这个操作，也称为白化，将原始响应<spanclass="math inline">\(\{r_n\}\)</span>转换为<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，其中每个轴的均值为零且为单位方差。这使得每个<spanclass="math inline">\(r_{n,i}^{\prime}\;(i=1,\cdot\cdot\cdot,q)\)</span>都更有机会成为各轴上的<spanclass="math inline">\(r_n^{\prime}\)</span>的最大值。虽然这个操作并不能保证每个簇索引i（i= 1，...，q）对任何n（n =1，...，N）都达到最大值，但是，由于这个操作，许多簇索引将对任何n（n =1，...，N）达到最大值。因此，这种轴内归一化过程使所提出的系统更倾向于更大的<spanclass="math inline">\(q^{\prime}\)</span>。</p><h2 id="b.损失函数">B.损失函数</h2><p>​  所提出的损失函数L由特征相似性约束和空间连续性约束组成，表示如下：<spanclass="math display">\[L=\underbrace{L_{\mathrm{sim}}(\{r_{n}^{\prime},c_{n}\})}_{\text{featuresimilarity}}+\mu\underbrace{L_{\mathrm{con}}(\{r_{n}^{\prime}\})}_{\text{spatialcontinuity}},\]</span> ​  其中，<spanclass="math inline">\(\mu\)</span>表示平衡这两个约束条件的权重。虽然所提出的方法是一种完全无监督的学习方法，但我们也研究了使用以涂鸦作为用户输入的方法。在使用涂鸦信息进行分割的情况下，损失函数(1)被简单地使用另一个权重ν进行修改如下：<spanclass="math display">\[L=\underbrace{L_{\mathrm{sim}}(\{r_{n}^{\prime},c_{n}\})}_{\mathrm{feature~similarity}}+\mu\underbrace{L_{\mathrm{con}}(\{r_{n}^{\prime}\})}_{\mathrm{spatial~continuity}}+\nu\underbrace{L_{\mathrm{scr}}(\{r_{n}^{\prime},s_{n},u_{n}\})}_{\mathrm{scribble~information}}.\]</span>​  上述功能的每个组成部分将在下面各自的部分中进行描述。</p><h3 id="对特征相似性的约束-1">1)对特征相似性的约束</h3><p>​  如在第3-A1节中所述，将argmax函数应用于归一化响应映射<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>，得到聚类标签{cn}。进一步利用聚类标签作为伪目标。在该方法中，计算了<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>和{cn}之间的以下交叉熵损失，作为对特征相似性的约束条件：<spanclass="math display">\[L_{\mathrm{sim}}(\{\boldsymbol{r}_n&#39;,c_n\})=\sum_{n=1}^N\sum_{i=1}^q-\delta(i-c_n)\lnr_{n,i}&#39;,\]</span> ​  其中 <spanclass="math display">\[\delta(t)=\begin{cases}1&amp;\text{if}t=0\\0&amp;\text{otherwise.}\end{cases}\]</span>​  这个损失函数背后的目的是增强相似特征的相似性。一旦根据图像像素的特征进行聚类，同一聚类内的特征向量应该彼此相似，而来自不同聚类的特征向量应该彼此不同。通过最小化该损失函数，更新网络权值，以方便提取更有效的聚类特征。</p><h3 id="在空间连续性上的约束">2)在空间连续性上的约束</h3><p>​  如在第三-A1节中所示，图像像素聚类的基本概念是将相似的像素分组为聚类。然而，在图像分割中，图像像素的聚类最好是空间连续的。此外，还引入了一个额外的约束条件，使集群标签与相邻像素的标签相同。与[47]类似，我们将响应图<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>的水平和垂直差异的L1-范数作为一个空间约束。我们可以用一个微分算子来实现这个过程。更具体地说，空间连续性损失<spanclass="math inline">\(L_{con}\)</span>的定义如下： <spanclass="math display">\[L_{\mathrm{con}}(\{\boldsymbol{r}_n&#39;\})=\sum_{\xi=1}^{W-1}\sum_{\eta=1}^{H-1}\parallel\boldsymbol{r}_{\xi+1,\eta}^{\prime}-\boldsymbol{r}_{\xi,\eta}^{\prime}\parallel_1+\parallel\boldsymbol{r}_{\xi,\eta+1}^{\prime}-\boldsymbol{r}_{\xi,\eta}^{\prime}\parallel_1,\]</span>​  其中，W和H表示输入图像的宽度和高度，而<spanclass="math inline">\(\boldsymbol{r}_{\xi,\eta}^{\prime}\)</span>表示响应映射<spanclass="math inline">\(\{r_n^{\prime}\}\)</span>中在<spanclass="math inline">\((\xi,\eta)\)</span>处的像素值。<br/>​  通过应用空间连续性损失<spanclass="math inline">\(L_{con}\)</span>，可以抑制由于复杂的图案或纹理而产生的过多的标签。</p><h3 id="对脚本作为用户输入的约束">3)对脚本作为用户输入的约束</h3><p>​  基于涂鸦信息的图像分割技术已被广泛研究，[15]，[31]-[33]。在该方法中，引入了涂鸦损失<spanclass="math inline">\(L_{scr}\)</span>作为部分交叉熵如下： <spanclass="math display">\[L_{\mathrm{scr}}(\{\boldsymbol{r}_n&#39;,s_n,u_n\})=\sum_{n=1}^N\sum_{i=1}^q-u_n\delta(i-s_n)\lnr_{n,i}&#39;,\]</span> ​  其中un =1，如果第n个像素是一个涂鸦像素，否则为0，sn表示每个像素的涂鸦标签。</p><h2 id="c.-通过反向传播来学习网络">C. 通过反向传播来学习网络</h2><p>​  在本节中，描述了训练网络进行无监督图像分割的方法。一旦输入目标图像，就解决了以下两个子问题：使用固定网络参数的聚类标签的预测和使用（固定的）预测聚类标签的训练网络参数。前者对应于一个网络的正向过程，以及在在第三-A节中描述的所提出的架构。后者对应于基于梯度下降的网络的逆向过程。随后，我们计算并反向传播在第三-B节中描述的损失L来更新卷积滤波器<spanclass="math inline">\(\{W_m\}^M_{m=1}\)</span>的参数以及分类器Wc的参数。本研究采用动量随机梯度下降法更新参数。参数用Xavier初始化[48]进行初始化，根据输入输出层大小归一化的均匀分布中采样值。这个前后过程迭代T次，得到聚类标签{cn}的最终预测。算法1显示了所提出的无监督图像分割算法的伪代码。</p><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007105933722.png"alt="image-20241007105933722" /><figcaption aria-hidden="true">image-20241007105933722</figcaption></figure><p>​  由于这个迭代过程需要少量的计算时间，我们进一步介绍了使用所提出的方法与一个或多个参考图像。如果目标图像与参考图像有些相似，则可以重用使用这些图像进行预处理时训练的固定网络权值。在第四-C节中研究了参考图像的有效性。<br/>​  如图1所示，所提出的CNN网络由基本功能组成。所提出的CNN最独特的部分是在最终的卷积层和argmax分类层之间存在批处理归一化层。与监督学习场景不同，其中的目标标签是固定的，在轴上的响应的批量归一化是必要的，以获得合理的标签{cn}(见第三-A2节)。此外，与监督学习相比，{cn}有多个具有不同网络参数的解，可以实现接近于零的损失。学习速率的值控制了参数更新和聚类之间的平衡，导致{cn}的解不同。我们将学习率设置为0.1，动量为0.9。</p><h1 id="实验结果">4. 实验结果</h1><p>​  <imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007110308915.png"alt="image-20241007110308915" /></p><p>​  表1 PASCAL VOC2012和BSD500无监督分割MIOU的比较。最好的分数用粗体显示，第二好的分数用下划线表示</p><h1 id="代码结构">5.代码结构</h1><figure><imgsrc="../postimages/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/image-20241007143649911.png"alt="image-20241007143649911" /><figcaption aria-hidden="true">image-20241007143649911</figcaption></figure><p>（未完不待续）</p>]]></content>
      
      
      <categories>
          
          <category> 可微聚类 </category>
          
          <category> 无监督语义分割 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>End-to-end Differentiable Clustering with Associative Memories</title>
      <link href="/End-to-end-Differentiable-Clustering-with-Associative-Memories/"/>
      <url>/End-to-end-Differentiable-Clustering-with-Associative-Memories/</url>
      
        <content type="html"><![CDATA[<p>End-to-end Differentiable Clustering with Associative Memories</p><p>Bishwajit Saha 1 Dmitry Krotov 2 Mohammed J. Zaki 1 Parikshit Ram 23</p><h1 id="摘要">摘要</h1><p>​  聚类是一种广泛使用的无监督学习技术，涉及到一个密集的离散优化问题。联想记忆模型或AMs是一种可微的神经网络，定义了一个递归动力系统，它已与各种深度学习架构集成。我们揭示了AM(AssociativeMemory)动力学和聚类中所需的固有离散分配之间的新联系，提出了一种新的无约束的连续松弛问题，使端到端可微聚类，称为ClAM(clusteringwithAM)。利用AMs的模式完成能力，我们进一步开发了一种新的自监督聚类损失。我们对不同数据集的评估表明，ClAM受益于自监督，并显著改进了传统的Lloyd‘sk-means算法和最近的连续聚类松弛（剪影系数高达60%）。</p><h1 id="引言">1. 引言</h1><p>​  最近，传统的联想记忆或AM模型(Hopfield, 1982;1984)被重新制定，以显著提高其记忆存储容量，并与现代深度学习技术集成(Krotov&amp; Hopfield, 2016; Ramsauer et al., 2020; Krotov &amp; Hopfield,2021; Krotov, 2021)。这些新模型被称为密集联想记忆（Dense AssociativeMemories），是完全可微的系统，能够在突触权重中存储大量的多维向量，称为模式或“记忆”。它们可以使用反向传播算法在端到端完全可微的设置中进行训练，通常带有自监督损失。<br/>​  我们认为，这种以端到端可微的方式学习AM的突触权值的能力，以及每个数据点恰好指向一个记忆的离散分配（关联），使AM唯一适合于可微聚类的任务。我们在图1中给出了一个简单的结果，其中标准的基于原型的聚类方案（离散的和可微的）无法找到正确的聚类，但我们提出的基于AM的方案是成功的。具体来说，我们做出了以下贡献：</p><ul><li>我们开发一个灵活的数学框架集群AM或ClAM，这是一个新颖的连续无约束放松的离散优化问题，允许集群端到端可微的方式，同时保持整个训练的离散集群分配，与线性时间集群分配。</li><li>我们利用AMs的模式完成能力来开发一个可区分的自监督损失，以提高聚类质量。</li><li>我们的经验证明，ClAM能够持续提高k-means高达60%，可以同时与基于光谱的聚类方法和与基于凝聚的聚类方法竞争，并产生深刻的解释。</li></ul><h1 id="相关工作">2.相关工作</h1><h2 id="离散聚类算法">离散聚类算法</h2><p>​  聚类是一个固有的硬组合优化问题。基于原型的聚类公式，如k-means聚类是np困难的，即使是对于k=2（Dasgupta，2008）。然而，广泛使用的Lloyd’s算法或Voronoi迭代方法（Lloyd，1982）可以相当有效地找到局部最优解，这可以通过多次重启和仔细的迭代来改进(Vassilvitskii&amp; Arthur,2006)。它是一种直观的离散算法，在A（根据当前原型诱导的Voronoi分区（Aurenhammer，1991）为集群的分配点）和B（基于当前集群分配更新原型点）之间交替使用。然而，该方案的离散性质使其难以逃脱任何局部最小值。我们认为，基于SGD的原型聚类能够通过逃避局部最小值来提高集群的质量，同时也继承了SGD的计算效率。光谱聚类公式（Donath&amp;霍夫曼，1973；VonLuxburg，2007）利用点的成对相似性或亲和性的特征谱（用图拉普拉斯式表示）将数据划分为“连接组件”，但没有明确地提取每个聚类的原型。这允许光谱聚类以Voronoi分区不允许的方式对数据进行分区。然而，它天真地需要二次时间（点数）来生成拉普拉斯矩阵，也需要三次时间来进行谱分解。层次聚类（Johnson，1967）基于先前建立的聚类找到连续的聚类，以自上而下的方式将它们划分，或以自下而上的方式进行组合。这些离散分层算法天真地在点的数量，尽管一些形式的自下而上的凝聚方案可以显示在二次（Sibson，1973）甚至次二次时间（3月等，2010）利用双树算法（Ram等人，2009；Curtin等人，2013；2015）。</p><h2 id="密度聚类方案">密度聚类方案</h2><p>​  如流行的DBSCAN（酯等，1996）和SNN（阿吉拉尔等，2001），不认为集群作为离散优化问题，但发现的问题模式数据分布，允许任意形状的集群鲁棒性噪声和异常值（桑德etal.，1998）。然而，多维密度估计是一项具有挑战性的任务，特别是使用非参数方法（西尔弗曼，1986；Ram&amp;Gray，2011），导致使用参数模型，如高斯混合模型（雷诺兹，2009）（可以从期望最大化的数据中估计（Dempsteret al.，1977））。</p><h2 id="可微深度聚类">可微深度聚类</h2><p>​  可微性在深度聚类中至关重要，我们希望同时学习点的潜在表示，并在该潜在空间中执行聚类(Renet al., 2022;Zhou et al., 2022)。现有的可微聚类公式，如范数和（Panahi etal.，2017）或非负矩阵分解（Kim &amp;Park，2008），本质上是解决约束优化，不直接适合端到端可微深度学习管道。因此，各种方案利用了某种形式的软聚类公式，如模糊k-均值（Bezdeketal.，1984）。Xie等人（2016）提出了一种新的概率k-means公式，启发于t-SNE（Minton，2008），对预先训练的自编码器（AE）的表示进行可微聚类（DEC），Guoet al. (2017a)（IDEC）显示了从联合学习表征和聚类中获得的收益。对于图像深度聚类中的表示学习，Song等人（2013）、Xie等人（2016）和Yang等人（2017）使用了堆叠自动编码器，Guo等人（2017b）（DCEC）使用卷积自动编码器（CAE）进行了进一步的改进。Chazan等人（2019年）使用了多个AEs，每个聚类一个，但（软）聚类分配是使用模糊k-means进行的。Cai等人（2022）在DEC放松中增加了一个“焦点损失”，以惩罚非离散任务，同时也增强了表征学习。深度子空间聚类（Peng等人，2016；Ji等人，2017；Zhang等人，2019）放宽了组合光谱聚类问题（同样采用部分聚类分配），并利用“self-expressive”层以可微的方式同时学习必要的拉普拉斯矩阵（相似度矩阵）和聚类。因此，深度聚类通常使用一些概率部分分配的点给多个簇，偏离了原始k-means的离散性质。此外，这些方案通常利用通过Lloyd（1982）进行的预先训练过的AEs和/或k-means来初始化网络参数。我们提出的ClAM保持了聚类的离散分配性质，并且在没有任何特殊播种的情况下工作良好。</p><h2 id="联想记忆am">联想记忆（AM）</h2><p>​  这是一个神经网络，它可以存储一组多维向量-记忆-作为一个循环动态系统的定点吸引子状态。它被设计为将初始状态（呈现给系统）与固定点（内存）上的最终状态关联起来，从而定义吸引力的不相交盆地或数据域的分区，因此可以表示聚类。该网络在数学上被形式化为经典的Hopfield网络（霍普菲尔德，1982），但已知其内存容量有限，只能在d维数据域中存储≈0.14d随机内存（Amit等人，1985；McEliece等人，1987）。对于相关数据，容量甚至更小，并且通常会导致一个不动点将整个数据集吸引到单个盆地中。对于相关数据，容量甚至更小，并且通常会导致一个不动点将整个数据集吸引到单个盆地中。这种行为对于集群来说是有问题的，因为集群的数量——稳定的固定点的数量——应该与数据维数解耦。Krotov&amp;Hopfield（2016）提出了现代霍普菲尔德网络或密集AM，通过在动态系统中引入快速增长的非线性激活函数，允许更密集的内存排列，以及超线性(d)内存容量。对于某些激活函数，致密AMs具有幂律甚至指数容量（克罗托夫&amp;霍普菲尔德，2016；德米西吉尔等人，2017；拉姆索尔等人，2020年；卢西贝罗和梅扎德，2023年）。Ramsauer等人（2020）证明了变压器的注意机制（Vaswani等人，2017）是具有软max激活的密集AMs的一种特殊极限情况。密集的AMs也被用于描述整个变压器块（Hoover等人，2023年），以及集成在复杂的基于能量的神经结构中（Hoover等人，2022年）。对这些结果的回顾见Krotov（2023）。密集的AMs最近也被研究与缝线连接（Burns&amp; Fukai，2023），并应用于异质结合环境（Liang etal.，2022）。在我们的工作中，我们将证明密集算法的循环动态和大内存容量使它们唯一适合聚类。</p><h1 id="集群中的联想记忆">3.集群中的联想记忆</h1><p>​  在本节中，我们提出了(i)基于AMs的ClAM的数学框架，（ii）激发了其对聚类的适用性，以及（iii）提出了一种学习记忆的方法，以进行良好的聚类。我们将所有这些放在我们的新的基于原型的聚类算法ClAM中。</p><h2 id="联想记忆数学框架">3.1.联想记忆：数学框架</h2><p>​  在d维欧几里得空间中，考虑M内存<spanclass="math inline">\(\boldsymbol{\rho}_\mu\in\mathbb{R}^d,\mu\in[M]\triangleq\{1,\ldots,M\}\)</span>（我们稍后将讨论记忆是如何学习的）。这个数学框架的关键方面是<strong>能量函数</strong>和<strong>attractordynamics</strong>(Krotov &amp; Hopfield, 2021; Millidge et al.,2022)。一个适合聚类的能量应该是一个点（或一个粒子）<spanclass="math inline">\(v\in\mathbb{R}^d\)</span>的连续函数。此外，能量应该有M个局部最小值，对应于每个内存。最后，当一个粒子逐渐接近一个记忆时，它的能量应该主要由单个记忆决定，而剩余的M−1记忆的贡献应该很小。一个满足这些要求的能量函数为：<spanclass="math display">\[E(\boldsymbol{v})=-\frac{1}{\beta}\log\left(\sum_{\mu\in[M]}\exp(-\beta\|\boldsymbol{\rho}_{\mu}-\boldsymbol{v}\|^{2})\right)\]</span>​  用一个标量的<span class="math inline">\(\beta &gt;0\)</span>被解释为一个逆的“温度”。随着<spanclass="math inline">\(\beta\)</span>的增长，方程1中的exp（·）确保只有前导项仍然显著，而其余的M−1项被抑制。因此，整个能量函数将用最近记忆周围的抛物线来描述。该空间将被划分为每个记忆周围的M个吸引盆地，每个记忆周围的能量函数的形状将主要由最近的记忆来定义，并对其他记忆进行了小的修正。<br/>​  attractordynamics通过dv/dt控制v在时间内空间中的移动，同时确保能量减少。这就是dE(v)/dt &lt;0，它确保了一个粒子收敛到一个局部最小值——动力学的一个不动点。attractordynamics可以通过能量景观上的梯度下降来描述： <spanclass="math display">\[\tau\frac{d\boldsymbol{v}}{dt}=-\frac{1}{2}\nabla_{\boldsymbol{v}}E=\sum_{\mu\in[M]}(\rho_{\mu}-\boldsymbol{v})\boldsymbol{\sigma}(-\beta\|\rho_{\mu}-\boldsymbol{v}\|^{2})\]</span>​  其中<span class="math inline">\(\tau &gt;0\)</span>是一个特征时间常数，描述了粒子在能量景观上移动的速度，<spanclass="math inline">\(\sigma(\cdot)\)</span>是对记忆缩放距离的softmax函数，定义为<spanclass="math inline">\(\sigma(z_{\mu})=\exp(z_{\mu})\Big/\sum_{m=1}^{M}\exp(z_{m})\)</span>的任何<spanclass="math inline">\(\mu\in[M]\)</span>。这一点如图2a所示。这就保证会减少能量 <spanclass="math display">\[\frac{dE(\boldsymbol{v})}{dt}\stackrel{(\mathbf{a})}{=}\nabla_{\boldsymbol{v}}E(\boldsymbol{v})\cdot\frac{d\boldsymbol{v}}{dt}\stackrel{(\mathbf{b})}{=}-2\tau\Big\|\frac{d\boldsymbol{v}}{dt}\Big\|^2\stackrel{(\mathbf{c})}{\leq}0\]</span>​  其中(a)为链式规则，(b)遵循方程2，(c)中的等式表示局部平稳性。在一个离散的时间步长t+1下，对于点v从状态<spanclass="math inline">\(v_t\)</span>到<spanclass="math inline">\(v^{t+1}=v^t+\delta^{t+1}\)</span>的有效更新<spanclass="math inline">\(\delta^{t+1}\)</span>是通过有限差分： <spanclass="math display">\[\delta^{t+1}=\frac{dt}{\tau}\sum_{\mu\in[M]}(\rho_\mu-v^t)\sigma(-\beta\|\rho_\mu-v^t\|^2)\]</span>​  给定一个点S的数据集，对于每个点v∈S，我们可以用一些噪声破坏它，从而产生一个扭曲的点v˜。这可以作为AM网络v0←v˜的初始状态。AM动态是由可学习的权重ρµ，µ=1，…，M，它也对应于动态（记忆）的固定点。根据方程4，T递归的网络随时间演化，其中选择T以确保充分收敛到一个固定点（见图2b中的例子）。将最终状态vT与未损坏的v进行比较来定义损失函数<span class="math display">\[\mathcal{L}=\sum_{\boldsymbol{v}\inS}\|\boldsymbol{v}-\boldsymbol{v}^{T}\|^{2},\mathrm{~where~}\boldsymbol{v}^{0}\leftarrow\tilde{\boldsymbol{v}}\]</span>​  相对于AM参数ρµ，它的反向传播最小。在集群的背景下，AM模型自然诱导一个分区的数据空间不重叠盆地的吸引力，隐式定义了一个硬集群分配定点在每个盆地，并通过完全连续的和可微的动态，允许学习与标准深度学习框架，我们接下来讨论。</p><h2 id="am作为一个可微离散arg-min求解器">3.2.AM作为一个可微离散argmin求解器</h2><h2 id="理解由ams引起的分区">3.3.理解由AMs引起的分区</h2><h2 id="clam聚类-与ams和自我监督">3.4.ClAM：聚类 与AMs和自我监督</h2><p>​  AM框架允许对离散的k-means问题（方程6）进行一种新的端到端可微的无约束连续松弛（方程8）。</p><p>方程6: <span class="math inline">\(\min_{R}\sum_{\boldsymbol{x}\inS}\|\boldsymbol{x}-\boldsymbol{\rho}_{\mu_{\boldsymbol{x}}^{\star}}\|^{2},\text{s.t.}\mu_{\boldsymbol{x}}^{\star}=\arg\min_{\mu\in[k]}\|\boldsymbol{x}-\boldsymbol{\rho}_{\mu}\|^{2}\)</span></p><p>方程8: <span class="math inline">\(\min_R\sum_{x\inS}\left\|x-x_R^T\right\|^2,\mathrm{~where~}x^0\leftarrow x\)</span></p><p>​  接下来，我们希望利用上述AMs（3.1）的强大模式完成能力。我们使用标准掩膜自我监督学习——我们应用（随机）掩膜<spanclass="math inline">\(m\in\{0,1\}^ d\)</span>到点<spanclass="math inline">\(x\inS\subset\mathbb{R}^{\bar{d}}\)</span>和利用上e递归完成部分模式<spanclass="math inline">\(x^0 = m\odotx\)</span>和ground-truth之间的失真掩盖值和完成模式我们的损失如下: <spanclass="math display">\[\mathcal{L}=\sum_{x\inS}\mathbb{E}_{m\sim\mathcal{M}}\|\bar{m}\odot(x-x_{R}^{T})\|^{2},x^{0}\leftarrow m\odot x\]</span> ​  其中<spanclass="math inline">\(x^T_R\)</span>由<spanclass="math inline">\(x^0\)</span>演变而来，方程4的T递归。原型R是通过SGD最小化自监督模式完成损失（方程9）来学习的。精确的学习算法在算法1的TrainClAM子程序中给出，并如图5所示。</p><figure><imgsrc="../postimages/End-to-end-Differentiable-Clustering-with-Associative-Memories/image-20241007095524047.png"alt="image-20241007095524047" /><figcaption aria-hidden="true">image-20241007095524047</figcaption></figure><p>图5：算法1。对于x∈S，我们首先应用一个掩码（紫色的）m到x来得到AM递归的初始迭代x0。对于T递归，我们有一个完整的版本xT R。原型R用自监督损失L上的梯度∇RL进行更新（公式9）。</p><p>​  在随机播种原型（第2行）之后，我们对数据（第3行）（数据批B中的每个x），我们应用随机掩码（第8行），使用当前原型（第9-12行）完成AM递归的模式，累积公式9（第13行），并使用批梯度（第14行）更新原型。</p><figure><imgsrc="../postimages/End-to-end-Differentiable-Clustering-with-Associative-Memories/image-20241006230200990.png"alt="image-20241006230200990" /><figcaption aria-hidden="true">image-20241006230200990</figcaption></figure><p>​  <strong>命题3.1</strong>：算法1中的TrainClAM子例程占用<spanclass="math inline">\(O(dkTN|S|)\)</span>时间，其中<spanclass="math inline">\(|S|\)</span>是S的基数，收敛到一个<spanclass="math inline">\(O(N^{-1/2})\)</span>-平稳点。<br/>​  <spanclass="math inline">\(N^{-1/2}\)</span>项来自于光滑非凸问题的标准SGD的收敛速度（内斯特罗夫，2003），可以通过动量改进到<spanclass="math inline">\(N^{-3/2}\)</span>（Fang等人，2018），考虑到我们的提出的端到端可微性，这是很简单的。算法1的InferClAM子程序使用学习到的原型R分配集群，并一次通过S。这在聚类结束时调用一次。<br/>​  <strong>命题3.2</strong>：算法1中的InferClAM子例程需要占用<spanclass="math inline">\(O(dkTN|S|)\)</span>时间，其中<spanclass="math inline">\(|S|\)</span>是S的基数。<br/>​  从命题3.1和3.2中，我们可以看到，ClAM训练和推理在数据集S中的点<spanclass="math inline">\(|S|\)</span>数、维数d和簇数k上是线性的。对于数据集S上的N个时期，ClAM训练取<spanclass="math inline">\(O(dkTN|S|)\)</span>，其中T为AM递归深度，而k-means训练（采用Lloyd算法）取<spanclass="math inline">\(O(dkN|S|)\)</span>。因此，k-means和ClAM都与点数成线性关系，尽管ClAM运行时有一个乘法系数T，其中T通常小于10。所以k-means直觉上比ClAM快。需要注意的是，k-means和ClAM明显高于谱聚类和凝聚聚类，凝聚聚类的样本数量通常在二次和三次之间。注意，对于相同数量的时代，我们可以为基于SGD的ClAM建立收敛保证（这可以通过使用基于动量的SGD来改进），而k-means没有类似的收敛保证。</p><p>（wei'wan）</p>]]></content>
      
      
      <categories>
          
          <category> 可微聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Efficient Deep Embedded Subspace Clustering</title>
      <link href="/Efficient-Deep-Embedded-Subspace-Clustering/"/>
      <url>/Efficient-Deep-Embedded-Subspace-Clustering/</url>
      
        <content type="html"><![CDATA[<center>Efficient Deep Embedded Subspace Clustering</center><p><spanclass="math inline">\(\begin{gathered}&lt;br/&gt;\mathrm{Jinyu~Cai}^{1,3},\mathrm{~Jicong~Fan}^{2,3*},\mathrm{Wenzhong~Guo}^{1},\mathrm{~Shiping~Wang}^{1},\mathrm{~Yunhe~Zhang}^{1},\mathrm{~Zhao~Zhang}^{4}\\&lt;br/&gt;\text{College of Computer and Data Science, FuzhouUniversity, China} \\&lt;br/&gt;\text{2School of Data Science, TheChinese University of Hong Kong (Shenzhen), China}\\&lt;br/&gt;\text{&#39;Shenzhen Research Institute of Big Data, China}^4\text{Hefei University of Technology, China}\\&lt;br/&gt;\{jinyucai1995,cszzhang\}@gmail.com,fanjicong@cuhk.edu.cn\\&lt;br/&gt;\mathrm{guowenzhong@fzu.edu.cn,\{shipingwangphd,zhangyhannie\}@163.com}&lt;br/&gt;\end{gathered}\)</span></p><h1 id="摘要">摘要</h1><p>​  最近，深度学习方法在数据聚类任务方面取得了重大进展。深度聚类方法（包括基于距离的方法和基于子空间的方法）将聚类和特征学习集成到一个统一的框架中，在那里聚类和表示之间存在相互促进。然而，深度子空间聚类方法通常在自表达模型的框架内，具有二次时间和空间复杂性，阻碍了其在大规模聚类和实时聚类中的应用。在本文中，我们提出了一种新的深度聚类机制。我们的目标是以迭代细化的方式从深度表示中学习子空间基，而细化的子空间基则帮助学习深度神经网络的表示。该方法脱离了自表达框架，可以线性地扩展到样本大小，适用于任意大的数据集和在线聚类场景。更重要的是，该方法的聚类精度远远高于其竞争对手。在基准数据集上与最先进的聚类方法进行的广泛比较研究，证明了该方法的优越性。</p><h1 id="引言">1. 引言</h1><p>​  聚类是机器学习中的一个基本问题，它的目的是在没有标签信息的情况下，在高类内相似度和低类间相似度的要求下，将样本分离成类。许多经典的聚类算法如k-means[29]和谱聚类（SC）[30]在实际应用中取得了巨大的成功。然而，它们在处理具有复杂结构或/和高维性的数据时并不有效，这可以通过使用数据的细化特征来改进。事实上，[14,37,38,47]之前的一些工作利用了特征学习技术，如非负矩阵分解[2]、自动编码器（AE）[1]及其变体[24,31,36]来学习低维嵌入进行聚类，从而提高了聚类的精度。然而，由于这些方法是两阶段聚类，并且特征学习不是针对聚类的，因此不能保证学习到的表示适合于聚类。<br/>​  近年来，一些研究人员提出了端到端聚类方法，如深度嵌入式聚类（DEC）[40]、联合无监督学习（JULE）[41]、深度自适应聚类（DAC）[6]和深度综合相关挖掘（DCCM）[39]。在这些方法中，聚类目标与网络优化过程相结合，这提供了一种学习面向聚类的嵌入式表示的方法。然而，大多数深度聚类方法使用基于欧几里德距离的度量来识别聚类，而欧几里德距离对于不同类型的数据结构并不总是有效或合理的。<br/>​  子空间聚类假设数据位于不同的子空间[11]中。稀疏子空间聚类（SSC聚类）[11]和低秩表示（LRR）[27]主要基于光谱聚类[30]，优于k均值聚类和经典光谱聚类。最近，一些研究人员[8,21,25,44]表明，联合子空间聚类和深度学习在基准数据集上有很好的性能。然而，这些方法很难扩展到大规模的数据集，因为它们需要学习一个自我表达的矩阵，从而导致二次时间和空间的复杂度。因此，[12,48,49]的一些最新工作致力于提高子空间聚类的效率。<br/>​  在本文中，我们旨在提供一种高效和精确的深子空间聚类的方法。我们提出从深度自动编码器提取的潜在特征中学习一组子空间基，其中的基和网络参数被迭代细化。我们提出从深度自动编码器提取的潜在特征中学习一组子空间基，其中的基和网络参数被迭代细化。该方法的网络结构如图1所示。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004113308304.png"alt="image-20241004113308304" /><figcaption aria-hidden="true">image-20241004113308304</figcaption></figure><p>图1.提出方法的说明。利用自动编码器网络学习输入数据的嵌入式表示Z，然后Z与子空间D结合构造子空间相似度向量，得到归一化的子空间相似度<spanclass="math inline">\(S\)</span>。然后，从<spanclass="math inline">\(S\)</span>计算细化的子空间相似度<spanclass="math inline">\(\widetilde{S}\)</span>，提供自监督信息。注意，<spanclass="math inline">\(d\)</span>是子空间的维数，<spanclass="math inline">\(L_{Recon}\)</span>和<spanclass="math inline">\(L_{Sub}\)</span>表示重构损失和<spanclass="math inline">\(\widetilde{S}\)</span>与<spanclass="math inline">\(S\)</span>之间的差异，并通过联合优化它们来训练网络。</p><p>​  我们的贡献如下：</p><ul><li>我们提出了一种新的脱离传统的自我表达框架的深度子空间聚类方法</li><li>该方法具有线性的时间和空间复杂度，因此适用于大规模的子空间聚类。</li><li>我们将该方法推广到在线聚类，从而可以有效地处理任意大的数据集和流数据集。</li><li>分析了利用深度神经网络转换基于距离的聚类和子空间聚类的可行性。</li></ul><p>​  在许多基准数据集（如Fashion-MNIST、STL-10和REUTERS-10k）上的数值结果表明，我们的方法比其竞争对手更有效。</p><h1 id="相关工作及简要讨论">2. 相关工作及简要讨论</h1><h2 id="深度聚类">2.1 深度聚类</h2><p>​  早期的深度聚类方法旨在应用深度特征学习方法（如自动编码器[36]和变分自动编码器（VAE）[24]）从复杂的高维数据中提取特征进行聚类。然而，这些解决方案很难学习适合于聚类任务的表示。目前的深度聚类方法侧重于构建端到端模型。Xie等人提出了DEC[40]，设计了一个基于t-SNE[35]设计的聚类目标。它提供了一个实现聚类中心和嵌入式特征同步优化的聚类模型。Chang等人[5]提出了深度自我进化聚类（DSEC），这是一种基于自我进化的算法，用所选择的模式对交替训练网络。在[39]中，Wu等人提出了一种名为DCCM的方法，该方法使用伪标签进行自我监督，并使用互信息来捕获更多有区别的表示用于聚类。由Huang等人[20]提出的分区置信度最大化（PICA）使分区不确定性指数最小化，并学习了最有信心的聚类分配。请注意，这些深度聚类方法使用欧氏距离来分配聚类，当聚类不集中于平均值时，这可能没有用处。</p><h2 id="子空间聚类">2.2 子空间聚类</h2><p>​  经典的子空间聚类，如SSC [11]，LRR [27]，Kernel-SSC[32]，旨在学习光谱聚类的自表达相似度矩阵。Ji等人[21]提出了深度子空间聚类网络（DSC-Net），该网络包含了一个具有自编码器网络的自表达模块。与SSC和LRR相比，DSC-Net在多个图像数据集上都有了显著的改进。Zhou等人[52]提供了一种称为深度对抗性子空间聚类（DASC）的方法，该方法利用生成的对抗性网络[16]提供对抗性学习，提高了深度子空间聚类的性能。Zhou等人[51]提出了分布保留的子空间聚类（DPSC）来保留子空间中的潜在分布，以提高子空间聚类模型的特征学习能力。另一方面，一些研究者试图降低子空间聚类[7,12,13,33,49]的复杂性。例如，Zhang等[49]提出了k-子空间聚类网络（k-SCN），将子空间的更新整合到嵌入式空间的学习中，以解决学习相似度矩阵的缺点。Fan[12]提出了一种k-分解子空间聚类（k-FSC）方法，该方法具有线性的时间和空间复杂度，能够处理缺失数据和流数据。</p><h2 id="简短的讨论">2.3 简短的讨论</h2><p>​  我们分析了表1中经典子空间聚类、大规模子空间聚类和深度子空间聚类等几种（由于空间限制）方法的时空复杂性。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004115219866.png"alt="image-20241004115219866" /><figcaption aria-hidden="true">image-20241004115219866</figcaption></figure><p>表1.该方法与一些深度聚类和子空间聚类方法在聚类n个样本的维度为m中的时间复杂度和空间复杂度比较。为了节省空间，我们在补充材料中加入了对参数的解释。</p><p>​  我们可以看到，这些经典的子空间聚类方法和深度子空间聚类方法在样本数量上具有二次时间和空间的复杂性。相比之下，我们的方法具有线性的时间和空间复杂度，可与[12]的大规模子空间聚类方法相媲美。</p><h1 id="方法">3. 方法</h1><h2 id="提出的模-型">3.1 提出的模 型</h2><p>​  在本文中，我们主要针对基于深度学习的子空间聚类，并试图解决以下问题。</p><p><strong>问题1</strong></p><p>​  给定一个数据矩阵<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{m\timesn}\)</span>，其中<span class="math inline">\(m\)</span>表示特征数，<spanclass="math inline">\(n\)</span>表示样本数。假设<spanclass="math inline">\(\mathrm{X=\bar{X}P}\)</span>，其中<spanclass="math inline">\(\bar{X}=[\bar{\mathbf{X}}^{(1)},\bar{\mathbf{X}}^{(2)},\ldots,\bar{\mathbf{X}}^{(k)}]\)</span>和<spanclass="math inline">\(\mathbf{P}\in\mathbb{R}^{n\timesn}\)</span>是一个未知排列的矩阵。对于<spanclass="math inline">\(j=1,\ldots,k\)</span>，假设<spanclass="math inline">\(\bar{\mathbf{X}}^{(j)}\in\mathbb{R}^{m\timesn_j}\)</span>的列是由以下得来： <spanclass="math display">\[\mathbf{x}=\mathbf{f}_j(\mathbf{v})+\varepsilon,\]</span>​  其中<spanclass="math inline">\(\mathbf{f}_j:\mathbb{R}^{r_k}\longrightarrow\mathbb{R}^m\)</span>是一个未知的非线性函数，<spanclass="math inline">\(r_{j}&lt;m\)</span>，<spanclass="math inline">\(\mathbf{v}\in\mathbb{R}^{r_{j}}\)</span>是一个随机变量，<spanclass="math inline">\(\varepsilon\in\mathbb{R}^{m}\)</span>表示随机高斯噪声。从<spanclass="math inline">\(\mathrm{X}\)</span>中找到排列矩阵<spanclass="math inline">\(\mathrm{P}\)</span>（或等价的<spanclass="math inline">\(\bar{X}\)</span>）。  这个问题正是一个聚类问题，为此我们需要将<spanclass="math inline">\(\mathrm{X}\)</span>的列分组成k个聚类，对应于k个不同的函数<spanclass="math inline">\(\mathbf{f}_{1},\ldots,\mathbf{f}_{k}\)</span>。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004121536128.png"alt="image-20241004121536128" /><figcaption aria-hidden="true">image-20241004121536128</figcaption></figure><p>​  图2显示了<span class="math inline">\(m=1\)</span>和<spanclass="math inline">\(r_{1}=\cdots=r_{5}=1\)</span>的一个简单例子。注意，当<spanclass="math inline">\(\mathbf{f}_{1},\ldots,\mathbf{f}_{k}\)</span>都是线性的，这个问题简化为经典的子空间聚类。<br/><strong>问题2</strong></p><p>​  在问题1中，对于<spanclass="math inline">\(j=1,\ldots,k\)</span>，假设<spanclass="math inline">\(\mathbf{f}_{j}(\mathbf{v})=\mathbf{g}(\mathbf{B}^{(j)}\mathbf{v})\)</span>，其中<spanclass="math inline">\(\mathbf{B}^{(j)}\in\mathbb{R}^{p\timesr_j}\)</span>和<spanclass="math inline">\(\mathbf{g}:\mathbb{R}^p\longrightarrow\mathbb{R}^m\)</span>。此外，<spanclass="math inline">\(\frac{\|\mathbf{B}^{(i)^{\top}}\mathbf{B}^{(j)}\|_{F}}{\|\mathbf{B}^{(i)}\|_{F}\|\mathbf{B}^{( j )}\|_{F}}(i\neq j)\)</span>足够小。从<spanclass="math inline">\(\mathrm{X}\)</span>中找到排列矩阵<spanclass="math inline">\(\mathrm{P}\)</span>（或等价的<spanclass="math inline">\(\bar{X}\)</span>）。</p><p>​  问题2比问题1更容易，因为当我们获得<spanclass="math inline">\(\{\mathbf{B}^{(1)},\ldots,\mathbf{B}^{(k)}\}\)</span>的良好估计时，就识别出正确的集群足够了。因此，在本文中，首先，我们提出通过用多层神经网络近似<spanclass="math inline">\(x\)</span>来估计<spanclass="math inline">\(B^{(j)}\)</span>，即： <spanclass="math display">\[\mathbf{x}_i\approxh_\mathcal{W}(\hat{\mathbf{B}}^{(j)}\hat{\mathbf{v}}_i),\mathbf{x}_i\in\mathbb{C}_j,\]</span> ​  其中，<spanclass="math inline">\(h_\mathcal{W}\)</span>为参数集为<spanclass="math inline">\(\mathcal{W}\)</span>的多层神经网络，<spanclass="math inline">\(\mathbb{C}_j\)</span>为第<spanclass="math inline">\(j\)</span>个聚类。很难直接获得<spanclass="math inline">\(\{\hat{\mathbf{B}}^{(1),}\ldots,\hat{\mathbf{B}}^{(k)}\}\)</span>。相反，我们估计了<spanclass="math inline">\(\mathbf{B}^{(j)}\mathbf{V}_i\)</span>，即<spanclass="math inline">\(\mathbf{z}_{i}:=\hat{\mathbf{B}}^{(j)}\hat{\mathbf{v}}_{i}\)</span>。因此，我们提出优化去解决：<spanclass="math display">\[\begin{aligned}\operatorname*{minimize}_{\mathcal{W},\{\mathbf{z}_{1},\ldots,\mathbf{z}_{n}\}}&amp;\frac{1}{2n}\sum_{i=1}^{n}\|\mathbf{x}_{i}-h_{\mathcal{W}}(\mathbf{z}_{i})\|^{2},\\\mathrm{subject~to~}&amp;\mathbf{z}_{i}\in\mathbb{S}_{i},i=1,\ldots,n,\end{aligned}\]</span> ​  其中，<spanclass="math inline">\(\mathbb{S}_{i}\)</span>表示真正的簇<spanclass="math inline">\(\mathbf{x}_{i}\)</span>应该属于的簇。然而，不可能直接解决(3)，因为<spanclass="math inline">\(\mathbb{S}_{i}\)</span>是未知的。现在我们引入了一个新的变量<spanclass="math inline">\(\mathbf{D}=[\mathbf{D}^{(1)},\mathbf{D}^{(2)},\ldots,\mathbf{D}^{(k)}]\)</span>。它包含<spanclass="math inline">\(k\)</span>个块和<spanclass="math inline">\(\mathbf{D}^{(j)}\in \mathbb{R}^{p\timesd}\)</span>，$ |_{u}<sup>{(j)}|=1<spanclass="math inline">\(，\)</span>u=1,,d<spanclass="math inline">\(，\)</span>j=1,,k<spanclass="math inline">\(。注意，对于所有的\)</span>j=1,,k<spanclass="math inline">\(，\)</span>dr_{j}<spanclass="math inline">\(。我们希望\)</span></sup>{(j)}<spanclass="math inline">\(与\)</span><sup>{(j)}<spanclass="math inline">\(，\)</span>j=1,,k<spanclass="math inline">\(有相同的列空间。然后根据我们在问题2中所做的假设，对于所有的\)</span>jl<spanclass="math inline">\(，\)</span>|</sup>{(j)<sup>}</sup>{(l)}|_F<spanclass="math inline">\(都足够小，即：\)</span><spanclass="math inline">\(\|\mathbf{D}^{(j)^\top}\mathbf{D}^{(l)}\|_F\leq\tau,j\neq l,\)</span>$ ​  其中，<spanclass="math inline">\(\tau\)</span>是一个很小的常数。<br/>​  表示<spanclass="math inline">\(\alpha_{i}=\mathrm{arg~max}_{j}\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(j)}\|\)</span>。我们期待：<spanclass="math display">\[\|\mathbf{z}_i^\top\mathbf{D}^{(\alpha_i)}\|\gg\max_{j\neq\alpha_i}\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\|,i=1,\ldots,n.\]</span> ​  换句话说，<spanclass="math inline">\(\mathbf{z}_{i}\)</span>只与<spanclass="math inline">\(\mathbf{D}\)</span>的一个块高度相关。现在我们使用一个参数集为<spanclass="math inline">\(\mathcal{W}^{\prime}\)</span>的编码器<spanclass="math inline">\(h_{\mathcal{W}^{\prime}}^{\prime}\)</span>来表示<spanclass="math inline">\(\mathbf{z}_{i}\)</span>，即： <spanclass="math display">\[\mathbf{z}_i=h&#39;_{\mathcal{W}&#39;}(\mathbf{x}_i),i=1,\ldots,n.\]</span> ​  为了方便起见，我们让： <spanclass="math display">\[\hat{\mathbf{x}}_i:=h_{\mathcal{W}}(\mathbf{z}_i),i=1,\ldots,n.\]</span> ​  和表示<spanclass="math inline">\(\hat{\mathbf{X}} =[\hat{\mathbf{x}}_{1},\ldots,\hat{\mathbf{x}}_{n}]\)</span>。现在，我们把(3)、(4)、(5)、(6)和(7)一起解决<spanclass="math display">\[\begin{alignat}{2}&amp;\operatorname*{minimize}_{\mathcal{W},\mathcal{W}^{\prime},\mathbf{D}}&amp; \quad &amp;\frac{1}{2n}\left\|\mathbf{X}-\hat{\mathbf{X}}\right\|_{F}^{2},\\&amp;\text{subject to} &amp; &amp; \|\mathbf{D}_{u}^{(j)}\|=1, \;u=1,\ldots,d, \; j=1,\ldots,k, \nonumber \\&amp; &amp; &amp;\|\mathbf{D}^{(j)} \mathbf{D}^{(l)}\|_{F}\leq\tau, \; j\neq l, \nonumber\\&amp; &amp; &amp;\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(\alpha_{i})}\|\gg\max_{j\neq\alpha_{i}}\|\mathbf{z}_{i}^{\top}\mathbf{D}^{(j)}\|,\; i=1,\ldots,n. \nonumber\end{alignat}\]</span> ​  注意，在(8)中，<spanclass="math inline">\(\mathbf{z}_{i}\)</span>只是根据(6)中的中间变量，我们不需要显式地优化它们。在(8)中，第一个约束是控制<spanclass="math inline">\(\mathbf{D}\)</span>的列的大小，否则<spanclass="math inline">\(\left\|\mathbf{z}_i^\top\mathbf{D}^{(\alpha_i)}\right\|\)</span>可能变为零。第二个约束是为了符合问题1中不同子空间之间不相似的假设。最后一个约束起着子空间分配的作用。请注意，我们的方法(8)仍然适用于问题1，前提是神经网络能够学习一个<spanclass="math inline">\(\mathbf{g}(\mathbf{B}^{(j)}\mathbf{v})\)</span>来有效地近似问题1中的<spanclass="math inline">\(\mathbf{f}_j(\mathbf{v})\)</span>。</p><h2 id="实用的解决方案">3.2 实用的解决方案</h2><p>​  现在我们将展示如何近似地求解(8)。为了方便起见，我们让 <spanclass="math display">\[L_{Recon}=\frac{1}{2n}\sum_{i=1}^{n}\left\|\mathbf{x}_{i}-\mathbf{\hat{x}}_{i}\right\|_{F}^{2}.\]</span>​  我们通过最小化以下目标，在(8)中施加第一个约束 <spanclass="math display">\[D_{Cons1}:=\frac12\left\|\mathbf{D}^\top\mathbf{D}\odot\mathbf{I}-\mathbf{I}\right\|_F^2,\]</span>​  其中<span class="math inline">\(\odot\)</span>表示阿达玛积，<spanclass="math inline">\(\mathbf{I}\)</span>是大小为<spanclass="math inline">\(kd\)</span>的<spanclass="math inline">\(kd\)</span>的单位矩阵。<br/>​  对于(8)中的第二个约束，我们进行了定义<span class="math display">\[\begin{gathered}D_{Cons2}:=\begin{aligned}\frac{1}{2}\left\|\mathbf{D}^{(j)\top}\mathbf{D}^{(l)}\right\|_{F}^{2},j\neq l,\end{aligned} \\\text{=}\frac12\left\|\mathbf{D}^\top\mathbf{D}\odot\mathbf{O}\right\|_F^2.\end{gathered}\]</span> ​  这里的<spanclass="math inline">\(\mathbf{O}\)</span>是一个矩阵，其中所有<spanclass="math inline">\(d\)</span>大小的对角线块元素都是0，其他所有元素都是1。现在我们可以把（10）和（11）放在一起，得到<spanclass="math inline">\(\mathbf{D}\)</span>上的正则化项 <spanclass="math display">\[D_{Cons}=\xi(D_{Cons1}+D_{Cons2}),\]</span>​  其中，<spanclass="math inline">\(\xi\)</span>是一个在本工作中固定在<spanclass="math inline">\(10^{−3}\)</span>的调优参数。<br/>​  对于(8)中的最后一个约束，我们提出了一种新的子空间相似度矩阵<spanclass="math inline">\(S\)</span>来度量嵌入表示<spanclass="math inline">\(\mathbf{Z}\)</span>和子空间基代理<spanclass="math inline">\(\mathbf{D}\)</span>之间的关系 <spanclass="math display">\[s_{ij}=\frac{\left\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\right\|_F^2+\etad}{\sum_j(\left\|\mathbf{z}_i^\top\mathbf{D}^{(j)}\right\|_F^2+\etad)},\]</span> ​  其中，<spanclass="math inline">\(\eta\)</span>是一个控制平滑度的参数。因此，<spanclass="math inline">\(s_{ij}\)</span>表示嵌入式表示<spanclass="math inline">\(\mathbf{z}_{i}\)</span>属于由<spanclass="math inline">\(\mathbf{D}^{(j)}\)</span>表示的第<spanclass="math inline">\(j\)</span>个子空间的概率。我们进一步引入了一个由定义的细化子空间相似度矩阵<spanclass="math inline">\(\widetilde{S}\)</span>： <spanclass="math display">\[\widetilde{s}_{ij}=\frac{s_{ij}^2/\sum_is_{ij}}{\sum_j(s_{ij}^2/\sum_is_{ij})}.\]</span>​  换句话说，<spanclass="math inline">\(\widetilde{S}\)</span>可以作为一种自监督信息，从而产生以下子空间聚类目标：<span class="math display">\[L_{Sub}=KL(\widetilde{S} ||S)=\sum_{i}\sum_{j}\widetilde{s}_{ij}\log\frac{\widetilde{s}_{ij}}{s_{ij}}.\]</span>​  现在我们将(8)的无约束松弛定义为 <spanclass="math display">\[L=L_{Recon}+D_{Cons}+\beta L_{Sub}.\]</span>​  在算法1中给出了该方法的训练流程。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004144035875.png"alt="image-20241004144035875" /><figcaption aria-hidden="true">image-20241004144035875</figcaption></figure><p>​  该方法实现了子空间聚类和嵌入式表示学习的联合优化。<spanclass="math inline">\(\mathbf{D}\)</span>的初始化由预训练模型的<spanclass="math inline">\(\mathbf{Z}\)</span>上的k-means生成的簇的列空间给出。当对网络的训练完成后，最终的聚类结果可以通过一下得到：<spanclass="math display">\[\mathcal{C}_i=\arg\max_js_{ij}.\]</span></p><h2 id="通用的近似和转换问题">3.3 通用的近似和转换问题</h2><p>​  有人可能会认为，神经网络具有普遍的逼近能力，因此可以将子空间聚类问题转换为基于距离的聚类问题，从而应用k-means和DEC[40]，或者可以将基于距离的聚类问题转换为子空间聚类问题。在这里，我们生成了两个合成数据集来显示转换的执行方式。第一个数据集是用于基于距离的聚类，并且<spanclass="math display">\[\mathbf{x}_{i}^{(j)}\sim\mathcal{N}(\mu_{j},\mathbf{I}),i=1,\ldots,1000,\\\mu_{j}\in\mathbb{R}^{m},\mu_{j}\sim\mathcal{U}(-1,1),\]</span> ​  然后是<spanclass="math inline">\(\mathbf{x}_i^{(j)}\longleftarrow\sin(\mathbf{x}_i^{(j)})\)</span>，其中<spanclass="math inline">\(m = 100\)</span>和<spanclass="math inline">\(j=1,\ldots,10\)</span>。第二个数据集是用于子空间聚类的，并且<spanclass="math display">\[\mathbf{x}_{i}^{(j)}=\sin(\mathbf{B}^{(j)}\mathbf{v}_{i}),\mathbf{v}_{i}\sim\mathcal{N}(\mathbf{0},\mathbf{I}),i=1,\ldots,1000,\\\mathbf{B}^{(j)}\in\mathbb{R}^{m\times p},\mathbf{B}^{(j)}\sim\mathcal{N}(\mathbf{0},\mathbf{I}),\]</span>​  其中，<span class="math inline">\(m = 100\)</span>，<spanclass="math inline">\(p = 2\)</span>，和<spanclass="math inline">\(j=1,\ldots,10\)</span>。我们还向数据集上添加了高斯噪声，即<spanclass="math inline">\(\mathrm{X}\leftarrow\mathrm{X}+\mathrm{N}\)</span>，其中噪声的标准误差是干净<spanclass="math inline">\(\mathrm{X}\)</span>的标准误差的0.2倍。<br/>​  DEC[40]、IDEC [18]和我们的方法的性能如图3所示。</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004155005104.png"alt="image-20241004155005104" /><figcaption aria-hidden="true">image-20241004155005104</figcaption></figure><p>​  第一个图表明，将基于距离的聚类问题转换为子空间聚类问题相对容易，因为我们的方法的精度相当高。第二个图表明，DEC和IDEC失败，将子空间聚类问题转换为基于距离的聚类问题非常困难。一个可能的原因是，将欧几里德距离（例如<spanclass="math inline">\(\|\mathbf{u}_1-\mathbf{u}_2\|\)</span>）转换为子空间相似度（例如<spanclass="math inline">\(\mathbf{v}_{1}^\top\mathbf{v}_{2}\)</span>）更容易。例如，设<spanclass="math inline">\(\mathbf{v}_1=\phi(\mathbf{u}_1)\)</span>和<spanclass="math inline">\(\mathbf v_{2}=\phi(\mathbfu_{2})\)</span>，其中<spanclass="math inline">\(\phi\)</span>是径向基函数的特征图，例如高斯核。然后我们有<spanclass="math display">\[\mathbf{v}_1^\top\mathbf{v}_2=\exp(-\gamma\|\mathbf{u}_1-\mathbf{u}_2\|^2),\]</span>​  其中，<spanclass="math inline">\(\gamma&gt;0\)</span>是一个超参数。因此，神经网络只需要学习一个<spanclass="math inline">\(\phi\)</span>的近似值，这并不困难。如果我们交换u和v的角色，网络需要学习一个函数<spanclass="math inline">\(h\)</span>，这样<spanclass="math inline">\(\|h(\mathbf{v}_1)-h(\mathbf{v}_2)\|\)</span>是<spanclass="math inline">\(\|\mathbf{v}_1^\top\mathbf{v}_2\|\)</span>的单调（大致）变换，这是相当困难的。<br/>​  以上结果和分析验证表明，有必要提供一种有效、准确的深度子空间聚类方法来更普遍地处理问题2或问题1。</p><h1 id="实验">4. 实验</h1><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241004155925751.png"alt="image-20241004155925751" /><figcaption aria-hidden="true">image-20241004155925751</figcaption></figure><p>#5. 代码部分具象图</p><figure><imgsrc="../postimages/Efficient-Deep-Embedded-Subspace-Clustering/image-20241006192123611.png"alt="image-20241006192123611" /><figcaption aria-hidden="true">image-20241006192123611</figcaption></figure><p>（未完不待续）</p>]]></content>
      
      
      <categories>
          
          <category> 可微聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>经典网络合集</title>
      <link href="/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/"/>
      <url>/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="自编码器autoencoder">1. 自编码器（autoencoder）</h1><p>​  自编码器（autoencoder）内部有一个隐藏层h，可以产生编码（code）表示输入。<br/>​  该网络可以看作由两部分组成：</p><ul><li>由函数<span class="math inline">\(h = f ( x)\)</span>表示的编码器</li><li>生成重构的解码器<span class="math inline">\(r =g(h)\)</span>，整体结构如下图所示：</li></ul><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192041921.png"alt="image-20241003192041921" /><figcaption aria-hidden="true">image-20241003192041921</figcaption></figure><h2 id="自编码器的一些基本概念">自编码器的一些基本概念</h2><ol type="1"><li>欠完备自编码器：h维度&lt;x维度。</li></ol><ul><li>学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。“学习欠完备的表示”意味着编码器被设计成只能生成比输入数据更简单、更压缩的表示。举一个简单的例子：</li><li>假设你有一堆猫和狗的图片。一个没有任何限制的自编码器可能会尝试记住每张图片的所有细节（毛色、背景等等）。但是，如果我们限制自编码器只能生成非常简化的表示，它就会被迫关注猫和狗最明显的特征，比如猫的尖耳朵和狗的长尾巴，而忽略背景和毛色等次要特征。</li></ul><ol start="2" type="1"><li>过完备自编码器：h维度&gt;=x维度。</li></ol><ul><li><p>过完备则与欠完备相反。“过完备”意味着编码器的表示空间非常大，能够容纳甚至超过输入数据中的所有信息。举一个简单的例子：</p></li><li><p>假设你有一堆猫和狗的图片。一个过完备自编码器可能会记住每张图片的所有细节，包括背景、毛色、姿态等等。这使得它在训练数据上表现非常好，但在遇到新的猫狗图片时，可能无法很好地识别出它们的共同特征。</p></li><li><p>在过完备的情况下，可能会出现编码器无法学习到有效信息的情况。这是因为：当编码维数与输入维数相等或更大时，自编码器的表示空间非常大，足以容纳输入数据中的所有信息。在这种情况下，甚至简单的线性编码器和解码器也可以直接将输入复制到输出，而不需要提取任何有用的特征。这意味着自编码器没有被迫去学习数据的分布特征，因为它可以简单地记住所有的数据。这种记忆机制使得模型在训练数据上表现很好，但在面对新数据时可能表现很差，因为它没有学到数据的内在模式或结构。</p></li></ul><h3 id="传统自编码器ae">1 传统自编码器（AE）</h3><p>​  AE的网络结构如下：包含三层——输入层、隐藏层、输出层，每一层都是由若干个神经元组成的。</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192700571.png"alt="image-20241003192700571" /><figcaption aria-hidden="true">image-20241003192700571</figcaption></figure><p>​  编码：<spanclass="math inline">\(h=f(x)=f(W_1X+b_1)\)</span><br/>​  解码：<spanclass="math inline">\(X_d=g(x)=g(W_2h+b_2)\)</span><br/>​  损失函数：<spanclass="math inline">\(J_{AE}\left(\theta\right)=J(X,X^d)=-\sum_{i=1}^n(x_i\log(x_i^d)+(1-x_i)\log(1-x_i^d))\)</span><br/>​  采用梯度下降法即可进行训练。此外，为了控制权重降低的程度，防止自编码器的过拟合，将在上述损失函数中加入正则化项(也称重量衰减项)，变为正则化自编码器：<spanclass="math inline">\(J_{\mathrm{ReAE}}(\theta)=J(X,X^d)+\lambda\parallelW\parallel_2^2\)</span></p><h3 id="去噪自编码器dae">2 去噪自编码器（DAE）</h3><p>​  DAE的网络结构如下，AE的目的是求h，但它没有使用h的真实值来训练，所以是无监督的。而DAE的目的是使得网络能够进行去噪，目的是求X，但它用到了X真实值做loss，所以他是监督学习。</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003193316595.png"alt="image-20241003193316595" /><figcaption aria-hidden="true">image-20241003193316595</figcaption></figure><p>​  DAE的动机是主动给X加噪，使得网络带有去噪的能力。但是在每次网络训练之前，人为地在干净的输入信号中加入噪声，增加了模型的处理时间。而且，如果加入过多的噪声，会导致输入样本的严重失真，从而降低算法的性能。</p><h3 id="稀疏自编码器sae">3 稀疏自编码器（SAE）</h3><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003192700571.png"alt="image-20241003192700571" /><figcaption aria-hidden="true">image-20241003192700571</figcaption></figure><p>​  稀疏自编码器利用了X的先验信息，这个先验信息就是X的稀疏度。它的网络结构和AE没有什么区别，但是损失函数变了，添加了一项KL散度，是编码后h的稀疏度和真实稀疏度之间的散度。<spanclass="math inline">\(J_{SAE}(\theta)=J(X,X^d)+\beta\sum_{j=1}^tKL(\rho\parallel\hat{\rho}_j)\)</span>其中<spanclass="math inline">\(\beta\)</span>是控制稀疏惩罚的系数，为0~1。<br/>​  首先定义每个隐藏单元jjj的平均激活值<span class="math inline">\(\hat{\rho}_j\)</span> : <spanclass="math inline">\(\hat{\rho}_j=\frac1n\sum_{i=1}^nh_j\left(x_i\right)\)</span><br/>​  其中，n是训练样本数量，<spanclass="math inline">\(h_j{(x_i)}\)</span>第i个样本对于隐藏单元j的激活值。<br/>​  然后定义目标稀疏度<spanclass="math inline">\(\rho\)</span>,这是希望隐藏单元的平均激活值。例如，如果<spanclass="math inline">\(\rho\)</span>较小(如0.05),则希望大多数隐藏单元在任何给定时间都不活跃。<br/>​  最后，将稀疏惩罚项加入到损失函数中，使用KL散度来衡量目标稀疏度和实际稀疏度之间的差异：<spanclass="math inline">\(\mathrm{KL}(\rho||\hat{\rho}_j)=\rho\log\frac{\rho}{\hat{\rho}_j}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}\)</span><br/>​  稀疏惩罚项的总和是所有隐藏单元的KL散度之和：$_{j=1}^{t}(||)<spanclass="math inline">\(&lt;br/&gt;​&amp;emsp;&amp;emsp;KL散度是描述两个分布之间差异的指标，KL散度越小，分布越接近，具体公式如下：\)</span>KL(_j)=+(1-)$</p><h3 id="变分自编码器vae">4 变分自编码器（VAE）</h3><p>​  首先说明变分自编码器的总体意义：VAE是在自编码器基础上结合了变分贝叶斯推断的方法，旨在学习数据的隐含结构，并能够生成新的、类似于训练数据的样本。可以说VAE的主要目的是生成新的数据。VAE通过显式地建模潜在变量的概率分布，使得潜在空间结构更加明确和可解释，最终能生成新样本。这在生成对抗网络（GAN）出现之前是一个重要的进展。<br/>​  下面介绍VAE的主要做法：<br/>​  VAE的整体网络结构简图如下：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003194129820.png"alt="image-20241003194129820" /><figcaption aria-hidden="true">image-20241003194129820</figcaption></figure><p>​  针对一个输入 <spanclass="math inline">\(x_i\)</span>（比如一个样本就是一张图像），网络结构如下（下图中的<spanclass="math inline">\(\mu_{i}^{\prime}\)</span>就是输出的<spanclass="math inline">\(X^d\)</span> ）：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/image-20241003194253388.png"alt="image-20241003194253388" /><figcaption aria-hidden="true">image-20241003194253388</figcaption></figure><p>​  总结一下VAE的架构：</p><blockquote><ol type="1"><li>我们首先给Encoder输入一个数据点<spanclass="math inline">\(x_i\)</span>,通过神经网络，我们得到隐变量<spanclass="math inline">\(z\)</span>服从的近似后验分布<spanclass="math inline">\(q_\phi(z\midx_i)\)</span>的参数。我们往往认为后验分布是一个各维度独立的高斯分布，因此令Encoder输出<spanclass="math inline">\(z\mid x_i\)</span>服从的高斯分布的参数<spanclass="math inline">\(\sigma_i^2\)</span>和<spanclass="math inline">\(\mu_i\)</span>即可。<br/>&gt; 2. 有了<spanclass="math inline">\(z\mid x_i\)</span>分布的参数<spanclass="math inline">\(\sigma_i^2\)</span>和<spanclass="math inline">\(\mu_i\)</span>后，我们从对应的高斯分布中采样出一个<spanclass="math inline">\(z_i\)</span>,这个<spanclass="math inline">\(z_i\)</span>应当代表与<spanclass="math inline">\(x_i\)</span>相似的一类样本。<br/>&gt; 3.我们令Decoder拟合似然的分布<span class="math inline">\(p_\theta(X\midz_i)\)</span>。喂给Decoder一个<spanclass="math inline">\(z_i\)</span>,它应当返回<spanclass="math inline">\(X\midz_i\)</span>服从的分布的参数。我们往往认为似然也服从一个各维度独立的高斯分布，因此令Decoder输出<spanclass="math inline">\(X\mid z_i\)</span>服从的高斯分布的参数<spanclass="math inline">\(\sigma_i^{\prime2}\)</span>和<spanclass="math inline">\(\mu_i^{\prime}\)</span>即可。<br/>&gt; 4.在得到<span class="math inline">\(X\midz_i\)</span>的分布的参数后，理论上我们需要从这个分布中进行采样，来生成可能的数据点<spanclass="math inline">\(x_{i}\)</span>。</li></ol></blockquote><p>​  上述第四点中值得注意的是，在大部分实现中，人们往往不进行采样，而是直接将模型输出的<spanclass="math inline">\(\mu_i^{\prime}\)</span>当作是给定<spanclass="math inline">\(z_i\)</span>生成的数据点<spanclass="math inline">\(x_i\)</span>。<br/>​  除此之外，人们也往往认为<spanclass="math inline">\(p_\theta(X\midz_i)\)</span>是一个固定方差的各维度独立的多元高斯分布，即<spanclass="math inline">\(p_\theta(X\midz_i)=\mathcal{N}(X\mid\mu_i^{\prime}(z_i;\theta),\sigma^{\prime2}*I)\)</span>,其中<spanclass="math inline">\(\sigma^{\prime2}\)</span>是一个人为给定的超参数。这意味着我们实际中并不真的让模型输出<spanclass="math inline">\(\sigma_i^{\prime2}\)</span>,模型只要输出<spanclass="math inline">\(\mu_i^{\prime}\)</span>就行了。<br/>​  具体公式推导参考上面的那篇博客，下面是对博客推导思想的简单总结：</p><figure><imgsrc="../postimages/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/5c14ac887b954cc98d2de9a26abf3558.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>为了更好理解，还是直接博客的代码吧：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on January 28, 2021</span><br><span class="line"></span><br><span class="line">@author: Siqi Miao</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;] = &quot;TRUE&quot;</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision.utils import save_image</span><br><span class="line">from torchvision.datasets import MNIST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class VAE(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_features, latent_size, y_size=0):</span><br><span class="line">        super(VAE, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.latent_size = latent_size</span><br><span class="line"></span><br><span class="line">        self.encoder_forward = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features + y_size, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, self.latent_size * 2)  # latent_size表示潜变量的个数，每一个变量有均值和方差两个数值</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decoder_forward = nn.Sequential(</span><br><span class="line">            nn.Linear(self.latent_size + y_size, in_features), # 解码器输入的时候只需要输入编码器输出的潜在变量的均值</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(in_features, in_features),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def encoder(self, X):</span><br><span class="line">        out = self.encoder_forward(X)</span><br><span class="line">        mu = out[:, :self.latent_size] # 前latent_size个数据是均值</span><br><span class="line">        log_var = out[:, self.latent_size:] # 后latent_size个数据是log(方差)</span><br><span class="line">        return mu, log_var</span><br><span class="line"></span><br><span class="line">    def decoder(self, z):</span><br><span class="line">        mu_prime = self.decoder_forward(z)</span><br><span class="line">        return mu_prime</span><br><span class="line"></span><br><span class="line">    def reparameterization(self, mu, log_var):  # Reparameterization Trick</span><br><span class="line">        epsilon = torch.randn_like(log_var) # 产生和log_var维度一样的高斯分布数据</span><br><span class="line">        z = mu + epsilon * torch.sqrt(log_var.exp()) # log_var.exp()=var，sqrt(var)就是sigema</span><br><span class="line">        return z</span><br><span class="line"></span><br><span class="line">    def loss(self, X, mu_prime, mu, log_var): # mu_prime编码器的输出；mu潜变量的均值，也就是潜变量的值了；log_var潜变量的log(方差)</span><br><span class="line">        # reconstruction_loss = F.mse_loss(mu_prime, X, reduction=&#x27;mean&#x27;) is wrong!</span><br><span class="line">        #print(mu_prime.shape) # [1024,784]</span><br><span class="line">        #print(mu.shape) # [1024,64]</span><br><span class="line">        #print(log_var.shape) # [1024,64]</span><br><span class="line">        # torch.square 是一个用于计算张量中每个元素的平方的函数。这个函数返回一个新的张量，其中包含原始张量中每个元素的平方值</span><br><span class="line">        reconstruction_loss = torch.mean(torch.square(X - mu_prime).sum(dim=1)) # sum(dim=1)表示对列求和，torch.mean就相当于是对1024个样本求均值了，也就是公式里的1/n</span><br><span class="line"></span><br><span class="line">        latent_loss = torch.mean(0.5 * (log_var.exp() + torch.square(mu) - log_var).sum(dim=1)) # sum(dim=1)表示对潜变量求和，torch.mean相当于是对1024个样本求均值</span><br><span class="line">        return reconstruction_loss + latent_loss</span><br><span class="line"></span><br><span class="line">    def forward(self, X, *args, **kwargs):</span><br><span class="line">        mu, log_var = self.encoder(X)</span><br><span class="line">        z = self.reparameterization(mu, log_var)</span><br><span class="line">        mu_prime = self.decoder(z)</span><br><span class="line">        return mu_prime, mu, log_var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CVAE(VAE): # 继承VAE类，所以可以使用VAE的编码和解码器</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_features, latent_size, y_size):</span><br><span class="line">        super(CVAE, self).__init__(in_features, latent_size, y_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, y=None, *args, **kwargs):</span><br><span class="line">        y = y.to(next(self.parameters()).device)</span><br><span class="line">        #print(y.shape) # [1024]</span><br><span class="line">        X_given_Y = torch.cat((X, y.unsqueeze(1)), dim=1) </span><br><span class="line">        #print(X_given_Y.shape) # [1024,785]</span><br><span class="line">        </span><br><span class="line">        mu, log_var = self.encoder(X_given_Y)</span><br><span class="line">        z = self.reparameterization(mu, log_var)</span><br><span class="line">        z_given_Y = torch.cat((z, y.unsqueeze(1)), dim=1)</span><br><span class="line"></span><br><span class="line">        mu_prime_given_Y = self.decoder(z_given_Y)</span><br><span class="line">        return mu_prime_given_Y, mu, log_var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(model, optimizer, data_loader, device, name=&#x27;VAE&#x27;):</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = 0</span><br><span class="line">    pbar = tqdm(data_loader)</span><br><span class="line">    for X, y in pbar:</span><br><span class="line">        #print(X.shape) # [1024,1,28,28]</span><br><span class="line">        #print(y.shape) # [1024]</span><br><span class="line">        batch_size = X.shape[0]</span><br><span class="line">        X = X.view(batch_size, -1).to(device)</span><br><span class="line">        #print(X.shape) # [1024,784]</span><br><span class="line">        model.zero_grad() #将模型中所有参数的梯度缓存清零。在进行反向传播计算梯度之前，必须先将之前计算的梯度清零。这是因为在 PyTorch 中，梯度是累积的。</span><br><span class="line"></span><br><span class="line">        if name == &#x27;VAE&#x27;:</span><br><span class="line">            mu_prime, mu, log_var = model(X)</span><br><span class="line">        else:</span><br><span class="line">            mu_prime, mu, log_var = model(X, y)</span><br><span class="line"></span><br><span class="line">        loss = model.loss(X.view(batch_size, -1), mu_prime, mu, log_var)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        pbar.set_description(&#x27;Loss: &#123;loss:.4f&#125;&#x27;.format(loss=loss.item()))</span><br><span class="line"></span><br><span class="line">    return total_loss / len(data_loader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@torch.no_grad()</span><br><span class="line">def save_res(vae, cvae, data, latent_size, device):</span><br><span class="line">    num_classes = len(data.classes)</span><br><span class="line"></span><br><span class="line">    # raw samples from dataset</span><br><span class="line">    out = [] # 用于存储每个类别的图像</span><br><span class="line">    for i in range(num_classes): </span><br><span class="line">        # 提取类别为 i 的图像</span><br><span class="line">        img = data.data[torch.where(data.targets == i)[0][:num_classes]]</span><br><span class="line">        out.append(img)</span><br><span class="line">    out = torch.stack(out).transpose(0, 1).reshape(-1, 1, 28, 28) # 将图像堆叠在一起，并转置维度以便于保存</span><br><span class="line">    save_image(out.float(), &#x27;./img/raw_samples.png&#x27;, nrow=num_classes, normalize=True) # 100张图像</span><br><span class="line"></span><br><span class="line">    # samples generated by vanilla VAE</span><br><span class="line">    z = torch.randn(num_classes ** 2, latent_size).to(device)</span><br><span class="line">    # print(z.shape) # [100,64]</span><br><span class="line">    out = vae.decoder(z)</span><br><span class="line">    save_image(out.view(-1, 1, 28, 28), &#x27;./img/vae_samples.png&#x27;, nrow=num_classes)</span><br><span class="line"></span><br><span class="line">    # sample generated by CVAE</span><br><span class="line">    z = torch.randn(num_classes ** 2, latent_size).to(device)</span><br><span class="line">    y = torch.arange(num_classes).repeat(num_classes).to(device)</span><br><span class="line">    z_given_Y = torch.cat((z, y.unsqueeze(1)), dim=1)</span><br><span class="line">    out = cvae.decoder(z_given_Y)</span><br><span class="line">    save_image(out.view(-1, 1, 28, 28), &#x27;./img/cvae_samples.png&#x27;, nrow=num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">    device = torch.device(device)</span><br><span class="line"></span><br><span class="line">    batch_size = 256 * 4</span><br><span class="line">    epochs = 50</span><br><span class="line">    latent_size = 64</span><br><span class="line">    in_features = 28 * 28</span><br><span class="line">    lr = 0.001</span><br><span class="line"></span><br><span class="line">    data = MNIST(&#x27;./dataset/&#x27;, download=True, transform=transforms.ToTensor())</span><br><span class="line">    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line">    # train VAE</span><br><span class="line">    vae = VAE(in_features, latent_size).to(device)</span><br><span class="line">    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training VAE...&#x27;)</span><br><span class="line">    for epoch in range(1, 1 + epochs):</span><br><span class="line">        loss = train(vae, optimizer, data_loader, device, name=&#x27;VAE&#x27;)</span><br><span class="line">        print(&quot;Epochs: &#123;epoch&#125;, AvgLoss: &#123;loss:.4f&#125;&quot;.format(epoch=epoch, loss=loss))</span><br><span class="line">    print(&#x27;Training for VAE has been done.&#x27;)</span><br><span class="line"></span><br><span class="line">    # train VCAE</span><br><span class="line">    cvae = CVAE(in_features, latent_size, y_size=1).to(device)</span><br><span class="line">    optimizer = torch.optim.AdamW(cvae.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Start Training CVAE...&#x27;)</span><br><span class="line">    for epoch in range(1, 1 + epochs):</span><br><span class="line">        loss = train(cvae, optimizer, data_loader, device, name=&#x27;CVAE&#x27;)</span><br><span class="line">        print(&quot;Epochs: &#123;epoch&#125;, AvgLoss: &#123;loss:.4f&#125;&quot;.format(epoch=epoch, loss=loss))</span><br><span class="line">    print(&#x27;Training for CVAE has been done.&#x27;)</span><br><span class="line"></span><br><span class="line">    save_res(vae, cvae, data, latent_size, device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>VAE的变种：条件变分自编码器（CVAE）<br/>传统的VAE可以近似地生成输入数据，但不能定向地生成特定类型的数据。为解决这一问题，将数据x和x的部分标签(y)输入到CVAE的编码器部分。这样就会生成指定类别的数据。CVAE的结构与VAE相似，因此CVAE的计算方法和优化方法与VAE一致。由于在输入中存在一些标签Y，CVAE成为一种半监督学习形式。<br/>参考资料：<br/>[1]《DeepLearning》<br/>[2] Li P, Pei Y, Li J. A comprehensive survey on designand application of autoencoder in deep learning[J]. Applied SoftComputing, 2023, 138: 110176.<br/>[3] <ahref="https://zhuanlan.zhihu.com/p/348498294">机器学习方法—优雅的模型（一）：变分自编码器（VAE）- 知乎 (zhihu.com)</a><br/>[4] <ahref="https://blog.csdn.net/qq_46146657/article/details/140666130">自编码器（autoencoder）-CSDN博客</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Clustering:A Comprehensive Survey</title>
      <link href="/Deep-Clustering-Survey/"/>
      <url>/Deep-Clustering-Survey/</url>
      
        <content type="html"><![CDATA[<center>Deep Clustering: A Comprehensive Survey</center><center>Yazhou Ren , Member, IEEE, Jingyu Pu , Zhimeng Yang, Jie Xu , GuofengLi, Xiaorong Pu , Philip S. Yu , Fellow, IEEE, and Lifang He , Member,IEEE</center><p><a href="https://ieeexplore.ieee.org/abstract/document/10585323"><imgsrc="https://img.shields.io/badge/TNNLS-2024-yellow"alt="TNNLS" /></a></p><h1 id="摘要">摘要</h1><p>​  聚类分析在机器学习和数据挖掘中起着不可或缺的作用。学习一个好的数据表示对聚类算法至关重要。近年来，深度聚类（DC，deepclustering）可以利用深度神经网络（DNNs，deep neuralnetworks）学习聚类友好表示，已广泛应用于聚类任务。现有的DC总结主要集中在单视图场和网络架构上，忽略了复杂的集群应用场景。为了解决这个问题，在本文中，我们提供了一个对DC的数据源的全面总结。对于不同的数据源，我们从方法学、先验知识和体系结构等方面系统地区分了聚类方法。具体地说，DC方法被分为四类，传统的单视图DC、半监督DC、深度多视图聚类（MVC）和深度传输聚类。最后，我们讨论了DC不同领域的开放挑战和潜在的未来机遇。</p><p>​  索引术语——深度聚类（DC）、多视图聚类（MVC）、半监督聚类、迁移学习。<br/>​  发表于人工智能领域B区期刊TNNLS，</p><h1 id="i.-引言">I. 引言</h1><p>​  随着网络媒体的发展，可以很容易地收集大量复杂的数据。通过对这些数据的精确分析，我们可以挖掘出这些结论的价值，并应用于许多领域，如人脸识别[1]、[2]、情绪分析[3]、[4]和智能制造[5]、[6]。<br/>​  一个可以用于对具有不同标签的数据进行分类的模型是许多应用程序的基础。对于有标记的数据，允许将标签作为最重要的信息作为指南。对于未标记的数据，找到一个可量化的目标作为模型构建过程的指导是聚类的关键问题。在过去的几十年里，人们提出了大量基于浅模型的聚类方法，包括基于质心的聚类基于密度的聚类[9]，[10]，[11]，[12]，[13]，分布的聚类[14]、层次聚类[15]、集成聚类[16]，[17]，和多视图聚类（MVC）[18]，[19]，[20]，[21]，[22]，[23]，[24]，[25]，[26]，[27]，[28]，[29]，[30]。这些浅层模型只有在特征具有代表性时才有效，而由于特征学习能力较差，它们在复杂数据上的性能通常受到限制。<br/>​  为了将原始复杂数据映射到一个易于聚类的特征空间，许多聚类方法都侧重于特征提取或特征转换，如主成分分析（PCA）[31]、核方法[32]、光谱方法[33]和深度神经网络（DNN）[34]。在这些方法中，DNN由于其优秀的非线性映射能力和在不同场景下的灵活性，是一种很有前途的方法。一种设计良好的基于深度学习的聚类方法[称为深度聚类（DC）]，旨在有效地从数据中提取更有利于聚类的特征，并同时使用学习到的特征进行聚类。与传统的聚类方法不同，DC背后的基本概念是将聚类目标合并到深度学习提供的鲁棒表示能力中。因此，获取一个重要的数据表示成为DC的基本要求。<br/>​  DC领域的研究很多，也有一些关于DC[35]、[36]、[37]、[38]方法的总结。具体来说，现有的DC系统综述主要集中在单视图聚类任务和神经网络的架构上。例如，Aljalbout等人。[35]只关注基于深度自动编码器（DAE）的深度单视图聚类方法。Min等人从不同深度网络的角度对DC方法进行分类。Nutakki等人[37]根据其训练策略将深度单视图聚类方法分为三类：多步连续DC、联合DC和闭环多步DC。Zhou等[38]通过特征学习与聚类模块的交互方式对深度单视图聚类方法进行分类。然而，在现实世界中，聚类的数据集总是相关的，例如，阅读的品味与对电影的品味相关，而来自同一个人的侧面和满脸应该被贴上相同的标签。对于这些数据，基于半监督学习、多视图学习和迁移学习的DC方法也取得了重大进展。不幸的是，现有的评论并没有过多地讨论它们。<br/>​  因此，从数据源的角度来对DC进行分类是很重要的。在本总结中，我们从数据的初始设置结合深度学习方法的角度总结了DC。我们从网络和数据结构的角度介绍了DC技术的最新进展，如图1所示。</p><figure><img src="../postimages/Deep-Clustering/image-20240929105628373.png"alt="image-20240929105628373" /><figcaption aria-hidden="true">image-20240929105628373</figcaption></figure><p>​  具体来说，我们将DC方法组织为以下四类。由于我们从数据源的角度来划分方法，所以特定方法的实现并没有完全分离。方法中有一部分在内部相互关联因此，从数据源的角度来对DC进行分类是很重要的。在本总结中，我们从数据的初始设置结合深度学习方法的角度总结了DC。我们从网络和数据结构的角度介绍了DC技术的最新进展，如图1所示。具体来说，我们将DC方法组织为以下四类。由于我们从数据源的角度来划分方法，所以特定方法的实现并没有完全分离。方法中有一部分在内部相互关联。<br/>​  1)深度单视图聚类：对于传统的聚类任务，通常假设数据具有相同的形式和结构，称为单视图或单模态数据。用DNNs提取这些数据的表示是DC的一个重要特征。然而，更值得注意的是不同的应用深度学习技术，它们与DNNs的结构高度相关。为了比较特定DNNs的技术路线，我们将这些算法分为五类：基于DAE的DC、基于DNN的DC、基于变分自编码器（VAE，variationalautoencoder）的DC、基于生成对抗网络（GAN，generative adversarialnetwork）的DC和基于图神经网络（GNN，graph neuralnetwork）的DC。<br/>​  2)基于半监督学习的DC：当待处理的数据包含一小部分先验约束时，传统的聚类方法不能有效地利用这些先验信息，而半监督聚类是解决这一问题的有效方法。目前，深度半监督聚类的研究还没有得到很好的探索。然而，半监督聚类是不可避免的，因为通过在模型中添加附加信息作为约束损失，使聚类方法成为半监督聚类方法是可行的。<br/>​  3)基于多视角学习的DC：在现实世界中，数据通常来自不同的特征收集器或具有不同的结构。我们称这些数据为“多视图数据”或“多模态数据”，其中每个样本都有多个表示。基于多视图学习的DC的目的是利用多视图数据中包含的一致性和互补的信息来提高聚类性能。此外，多视图学习的思想可能对深度单视图聚类具有指导意义。在本总结中，我们将深度MVC分为三类：基于深度嵌入式聚类（DEC，deep-embeddedclustering）、基于子空间聚类和基于GNN。<br/>​  4)基于迁移学习的DC：对于一个实例数量有限和高维度的任务，有时我们可以找到一个助手来提供额外的信息。例如，如果任务A与另一个任务B相似，并且任务B比A有更多的聚类信息（B被标记或B比A更容易聚类），那么将信息从B转移到A是有用的。无监督领域自适应（UDA，unsuperviseddomainadaption）的迁移学习近年来得到了广泛的发展，它包含两个领域：带有标签的源域和未标记的目标域。迁移学习的目标是将从源任务中学习到的知识或模式应用到一个不同但相关的目标任务中。基于迁移学习的DC分算方法旨在利用相关任务的信息来提高当前聚类任务的性能。<br/>​  在研究相应的聚类方法之前，有必要注意聚类数据的不同特征和条件。在本总结中，现有的DC方法从数据源进行系统分类。分析了不同聚类方法的优缺点和适用条件。最后，我们提出了在DC领域的一些有趣的研究方向。</p><h1 id="ii.-定义和初步工作">II. 定义和初步工作</h1><p>​  我们将在本节中介绍这些符号。在本文中，我们使用大写字母来表示矩阵，并使用小写字母来表示向量。除非另有说明，本文中使用的符号将在命名法中进行总结。<br/>​  本总结将介绍基于不同背景条件的四种DC问题。在这里，我们正式地定义了这些问题。给定一组数据样本<spanclass="math inline">\(X\)</span>，我们的目标是找到一个映射函数<spanclass="math inline">\(F\)</span>，它可以将<spanclass="math inline">\(X\)</span>映射到<spanclass="math inline">\(k\)</span>个簇中。映射结果用<spanclass="math inline">\(\hatY\)</span>表示。因此，我们所处理的任务如下:<br/>​  1)深度单视图集群：<span class="math display">\[F(X)\to\hat{Y}.\]</span>​  2)半监督的深度聚类： <spanclass="math display">\[F(X,A)\to\hat{Y}.\]</span>​  其中A是一个约束矩阵。</p><p>​  3)深度MVC： <spanclass="math display">\[F\left(X^1,\ldots,X^v\right)\to\hat{Y}\]</span>​  其中<span class="math inline">\(X^i\)</span>是<spanclass="math inline">\(X\)</span>的第一个视图。</p><p>​  4)具有域自适应的深度聚类： <spanclass="math display">\[F(X^s,Y^s,X^t)\to\hat{Y}\]</span> ​  其中<spanclass="math inline">\((X^s,Y^s)\)</span>为已标记的源域，<spanclass="math inline">\(X^t\)</span>为未标记的目标域。</p><h1 id="iii.-深度单视图聚类">III. 深度单视图聚类</h1><p>​  表示学习[39]理论显示了特征学习（或表示学习）在机器学习任务中的重要性。然而，深度表示学习主要是监督学习，需要许多标记数据。正如我们之前提到的，DC问题的障碍是什么可以用来指导训练过程，如监督问题中的标签。DC中最“受监督”的信息是数据本身。因此，我们如何训练一个有效的特征提取器来获得良好的表示呢？根据特征提取器的训练方式，我们将深度单视图聚类算法分为五类：基于DAE、基于DNN、基于VAE、基于GAN和基于GNN。</p><p>表1基于DAE和基于DNN的方法在深度单视图聚类中的总结。我们总结了基于“联合或单独”和“特征”的基于DAE的方法</p><figure><img src="../postimages/Deep-Clustering/image-20240929113755442.png"alt="image-20240929113755442" /><figcaption aria-hidden="true">image-20240929113755442</figcaption></figure><p>​  这些方法的区别主要是关于损失分量的，其中损失项的定义见表一，并解释如下。<br/>​  1)DAE-Based/GNN-Based: <spanclass="math inline">\(L=L_{rec}+L_c\)</span><br/>​   2)DNN-Based: <spanclass="math inline">\(L = L_{ext} + L_c\)</span><br/>​   3)VAE-Based:<span class="math inline">\(L = L_{ELBO} + L_c\)</span><br/>​  4)GAN-Based: <span class="math inline">\(L = L_{gan} +L_c\)</span><br/>​  在无监督学习中，我们要处理的问题是训练一个可靠的没有标签的特征提取器。在现有的工程中，主要有两种方式：<br/>​  1)根据以下原理优化伪标签的损失函数：缩小簇内距离，扩大簇间距离<br/>​  2)一个可以帮助训练特征提取器的额外任务。<br/>​  对于具有特殊特征提取器的聚类方法，如自动编码器，重构损失<spanclass="math inline">\(L_{rec}\)</span>可以解释为额外的任务。在本文中，面向聚类的损失<spanclass="math inline">\(L_c\)</span>表示聚类目标的损失。基于DAE/基于GNN的方法使用自动编码器/图自编码器作为特征提取器，因此损失函数总是由一个重构损失<spanclass="math inline">\(L_{rec}\)</span>和另一个面向聚类的损失<spanclass="math inline">\(L_c\)</span>组成。相比之下，基于DNN的方法使用额外的任务或其他策略来优化特征提取器。基于VAE的方法优化了证据损失下界（ELBO，evidencelower bound）<spanclass="math inline">\(L_{ELBO}\)</span>。基于GAN的方法是基于生成式的对抗性损失<spanclass="math inline">\(L_{gan}\)</span>。基于这五个维度，现有的深度单视图聚类方法总结在表一和表二中。</p><p>表2 基于VAE、基于GAN和基于GNN的方法在深度单视图聚类中的总结</p><figure><img src="../postimages/Deep-Clustering/image-20240929115248981.png"alt="image-20240929115248981" /><figcaption aria-hidden="true">image-20240929115248981</figcaption></figure><h2 id="a.-基于dae">A. 基于DAE</h2><p>​  自动编码器网络[39]最初是为数据的无监督表示学习而设计的，可以学习一个高度非线性的映射函数。使用DAE[97]是开发DC方法的一种常见方法。DAE的目的是通过最小化网络的重构损失来学习低维嵌入特征空间，其定义为：<spanclass="math display">\[L_{\mathrm{rec}}=\min\frac{1}{n}\sum_{i=1}^n\|x_i-\phi_r(\phi_e(x_i))\|^2\]</span>​  其中，<span class="math inline">\(\phi_e(\cdot)\)</span>和<spanclass="math inline">\(\phi_r(\cdot)\)</span>分别表示自动编码器的编码器网络和解码器网络。利用该编码器作为特征提取器，提出了各种聚类目标函数。我们将这些基于DAE的聚类方法总结为基于DAE的DC。在基于DAE的DC方法中，主要有两种获取标签的方法。第一种方法是将数据嵌入到低维特征中，然后用传统的聚类方法如k-means算法[7]等聚类方法对嵌入的特征进行聚类。第二种方法是联合优化特征提取器和聚类结果。我们将这两种方法称为“单独分析”和“联合分析”，并在下面对它们进行详细阐述。<br/>​  “单独分析”是指学习特征和聚类数据被单独执行。为了解决通过“单独分析”学习到的表征由于其固有的特性而不具有聚类导向的问题，Huang等人[41]提出了一种用于聚类的深度嵌入网络（DEN，deepembeddingnetwork），该网络基于DAE目标施加了两个约束：<strong>局部性保留约束</strong>和<strong>群稀疏性约束</strong>。</p><ul><li>局部保留约束使同一集群中嵌入的特征相似。</li><li>群稀疏性约束的目的是对角线化表示的相似度矩阵。</li></ul><p>​  这两个约束条件提高了聚类性能，同时减少了簇内距离，扩大了簇间距离。大多数基于DAE的聚类方法的目标都是研究这两种距离。因此，在表一中，我们从“特征”的角度对这些方法进行了总结，说明了优化簇内距离和簇间距离的方法。<br/>​  Peng等[42]在子空间聚类领域提出了一种新的基于深度学习的框架，即具有稀疏先验（PARTY，deepsubspace clustering with sparsityprior）的深度子空间聚类。PARTY通过考虑不同样本之间的关系（即结构先验）来增强自动编码器并解决了传统的子空间聚类方法的局限性。据我们所知，PARTY是第一个基于深度学习的子空间聚类方法，也是第一个在神经网络之前引入全局结构进行无监督学习的工作。与PARTY不同，Jiet al.[45]提出了另一种深度子空间聚类网络（DSC-Net）架构来学习非线性映射，并引入了一个自表达层来直接学习相似度矩阵。<br/>​  基于密度的聚类[9]，[98]是另一种流行的聚类方法。Ren等人[57]提出了基于深度密度的图像聚类（DDIC，density-basedimageclustering），该方法使用DAE学习低维特征表示，然后对学习到的特征进行基于密度的聚类。特别是，DDIC不需要提前知道集群的数量。<br/>​  “联合分析”的目的是学习一种更适合聚类的表示，这不同于深度学习和聚类单独进行的单独分析方法，并且神经网络在学习数据特征时没有面向聚类的目标。大多数后续的DC研究将聚类目标与特征学习相结合，使神经网络能够从数据的潜在分布中学习有利于聚类的特征。在本总结中，这些方法被总结为“联合分析”。<br/>​  受非参数算法t分布随机邻域嵌入（t-SNE，t-distributedstochastic neighborembedding）[99]的启发，Xie等[43]提出了一个联合框架来优化特征学习和聚类目标，命名为DEC。DEC首先通过<spanclass="math inline">\(L_{rec}\)</span>学习从数据空间到较低维特征空间的映射，然后迭代优化聚类损失KL（S∥R）（即Kullback–Leibler(KL)散度）。这里，S表示描述嵌入数据与每个聚类质心之间相似性的数据的软赋值（质心用k-means初始化），R是调整后的目标分布，与S相比具有更纯粹的聚类赋值。<br/>​  DEC由于其联合学习框架和计算复杂度低，是DC技术中的一种代表性方法。基于DEC，已经提出了一些变体。例如，为了保证微调阶段的局部结构，提出了改进的DEC（IDEC）方法，以联合优化加权聚类损失和自编码器的重构损失。DEC与数据增强（DEC-DA）[49]在DEC中应用了数据增强策略。Li等人[50]提出了判别增强图像聚类（DBC）来处理图像表示学习和图像聚类。DBC有一个类似于DEC的管道，但学习过程是自定节奏的[100]，其中首先选择最简单的实例，并逐步扩展更复杂的样本。<br/>​  在DEC中，预测的聚类任命是由学生的t-分布计算出来的。不同的是，Dizaji等人[46]提出了一种深度嵌入的正则化聚类（DEPICT，deep-embeddedregularized clustering），通过在卷积自编码器（CAE，convolutionalautoencoder）的嵌入层上叠加一个新的聚类损失。此外，DEPICT的聚类损失通过聚类分配频率的先验和层级特征重构损失函数进行正则化。Yang等人，[47]直接将k-均值的目标作为聚类损失。该模型名为DC网络（DCN），是一种联合降维和k-均值聚类方法，通过学习DAE来实现降维。Shah和Koltun[51]提出了深度连续聚类（DCC， deep continuousclustering），这是一种通过将自编码器集成到范例中的扩展。DCC通过联合优化已定义的数据损失、成对损失和重建损失来进行聚类学习。特别是，它不需要对集群数量的先验知识。Tzoreff.[53]等人提出了深度鉴别聚类潜在空间（DDLSC，deepdiscriminative latent space forclustering），以优化关于鉴别成对损失函数的DAE。<br/>​  深度流形聚类（DMC，Deepmanifoldclustering）[48]是第一个将深度学习应用于流形聚类[101]，[102]中的方法。在DMC中，训练一个由堆叠的RBMs[103]组成的自动编码器来获得转换后的表示。DMC的重构损失和聚类损失都与以往的方法不同，即通过重构一个样本及其局部邻域来定义局部保持目标。利用样本和聚类中心之间的高斯核测量的惩罚系数和距离来定义面向聚类的目标。<br/>​  最近提出的基于DAE的聚类算法也利用DAE的变体来更好地学习低维特征，并通过结合传统机器学习方法的思想来提高聚类性能。例如，利用双自编码器网络（DSCDAE，dualautoencodernetwork）[55]的深度光谱聚类和通过集成DAE学习（SC-EDAE，spectralclustering via ensemble DAElearning）[58]的光谱聚类，目的是将光谱聚类集成到精心设计的DC自编码器中。Zhang等[56]提出了神经协同子空间聚类（NCSC，neuralcollaborative subspace clustering），它将两个置信映射建立在自编码器学习到的特征上，作为子空间聚类的监督信息。在数据增强下对的自适应自定速DC（ASPC-DA，adaptiveself-paced DC with dataaugmentation）[59]中，同同时采用了自定速学习思想[100]和数据增强技术。其学习过程与DEC相同，包括两个阶段，即对自动编码器进行预训练和对编码器进行微调。<br/>​  一般来说，我们注意到所采用的网络结构与要处理的数据类型有关。例如，全连接网络通常用于提取一维数据特征，而卷积神经网络（CNNs）则用于提取图像特征。上述大多数基于DAE的DC方法都可以通过全连接的自动编码器和CAE来实现，因此，它们在一定程度上适用于各种类型的数据。然而，在计算机视觉领域，有一类专注于图像聚类的直流方法。这些方法可以追溯到[104]，并被总结为基于DNN的DC，因为它们通常使用CNNs来进行图像特征学习和语义聚类。</p><h2 id="b.-基于dnn">B. 基于DNN</h2><p>​  本节将介绍基于DNN的聚类方法。与基于DAE的聚类方法不同，基于DNN的方法必须设计额外的任务来训练特征提取器。在本调查中，我们从“面向聚类的损失”和“特征”两个角度总结了表I中基于DNN的DC方法。“面向聚类的损失”显示了是否存在一个损失函数，它明确地缩小了簇内的距离或扩大了簇间的距离。图2显示了基于CNN的深度无监督学习的框架。</p><figure><img src="../postimages/Deep-Clustering/image-20240929160431452.png"alt="image-20240929160431452" /><figcaption aria-hidden="true">image-20240929160431452</figcaption></figure><p>图2。基于DNN的学习框架（单视图聚类）。X是聚类的数据，f是X的特征提取器。第一部分描述了监督学习的框架。Y表示真实的标签，S表示预测的结果。使用Y和S，我们可以计算反向传播的分类损失。第二部分是具有额外任务的方法的框架。额外的任务用于训练网络进行良好的嵌入z。第三部分描述了需要调整集群分配的方法的过程。S表示预测结果，R为S的调整。</p><p>​  当DNN训练过程开始时，随机初始化的特征提取器是不可靠的。因此，基于随机初始化神经网络的DC方法通常采用传统的聚类技巧，如分层聚类[105]或专注于额外的任务，如实例生成。例如，Yang等人[63]提出了一种名为JULE的联合无监督学习方法，该方法应用凝聚聚类魔法来训练特征提取器。具体来说，JULE在一个递归框架中制定了联合学习，其中将凝聚聚类的合并操作视为前向传递，并将DNN的表示学习视为后向传递。基于这个假设，JULE还应用了一个损失，即缩小了簇内距离，同时扩大了簇内距离。在每个轮次中，JULE将两个簇合并为一个簇，并计算向后通过的损失。<br/>​  Chang等人[65]提出了深度自适应图像聚类（DAC，deepadaptive imageclustering）来解决特征学习和聚类相结合的问题。在DAC中，将聚类问题重构为二值成对分类问题，用以判断具有估计余弦相似度的成对图像是否属于同一聚类。然后，它自适应地选择相似的样本，以有监督的方式训练DNN。DAC为DC提供了一个新的视角，但它只关注于成对模式之间的关系。深度判别聚类（DDC，Deepdiscriminativeclustering）分析[54]是通过引入全局和局部关系约束，更鲁棒和广义的约束。基于空间transformer的深度自适应聚类（ST-DAC，Spatialtransformer-deep adaptive clustering）[69]应用视觉注意机制[106]来修改DAC的结构。Haeusser等人[68]提出了关联DC（ADC，associativeDC），它包含一组与图像嵌入具有相同形状的质心变量。由于质心变量在迭代过程中可以传递关于数据结构的高级信息，它们引入了一个具有多个损失项的目标函数，同时训练这些质心变量和DNN参数以及聚类映射层。<br/>​  上述聚类方法通过将一个实例的聚类通过整个深度网络来估计其聚类，从而倾向于提取实例[107]的全局特征。一些聚类方法使用一个成熟的分类网络来初始化特征提取器。例如，深度集群[66]对深度模型的输出特性（如AlexNet[108]和VGG-16 [109]）应用k-means，并将集群分配作为“伪标签”来优化CNNs的参数。Hsu和Lin [67]提出了聚类CNN（CCNN，clusteringCNN），该方法将小批量k-均值与从ImageNet数据集[110]预训练的模型集成在一起。<br/>​  为了提高模型的鲁棒性，越来越多的方法对DC[49]、[59]、[76]进行了数据增强。例如，Huang等人[76]扩展了经典的最大边际聚类[111]，[112]的思想，建立了一种新的深度语义聚类方法[称为划分置信度极大化（PICA，PartItionConfidencemAximization）]。在PICA中，采用了颜色抖动、随机调整尺度和水平翻转三个操作来进行数据增强和扰动。<br/>​  互信息也被作为学习表示[113]、[114]的标准，并在最近的聚类方法中流行起来，特别是对图像数据。各种数据增强技术已被应用于生成转换后的图像，并用于挖掘它们的互信息。例如，Ji等人[71]提出了用于语义聚类和图像分割的不变信息聚类（IIC，invariant informationclustering）。在IIC中，每幅图像及其随机变换都被视为一个样本对。通过最大化每对聚类分配之间的互信息，该模型可以找到具有语义意义的聚类，并自然地避免退化解。深度综合相关挖掘（DCCM，deepcomprehensive correlationmining）[72]是一种新的图像聚类框架，它使用伪标签丢失作为监督信息。此外，作者扩展了实例级互信息，并提出了三重互信息损失，以学习更多的区别特征。基于目前流行的对比学习[115]，Zhong等[75]提出了深度鲁棒聚类（DRC，deep robustclustering），其中引入了两个对比损失项来减少类内方差和增加类间方差。互信息和对比学习是相互关联的。在DRC，作者总结了一个框架，可以将最大化互信息转化为最小化对比损失。<br/>​  在语义层次的图像聚类领域，人们认为原始图像的预测应该与数据增强后图像的预测一致。因此，在无监督学习环境下，数据增强技术不仅被用于扩展训练数据，而且可以很容易地获得有监督的信息。这就是为什么数据增强可以广泛应用于许多最近提出的图像聚类方法。例如，Nina等人[70]提出了一种具有数据增强的无解码器方法（称为随机三重态挖掘（RTM，randomtripletmining）），用于聚类和流形学习。为了学习一个更鲁棒的编码器，该模型由三个具有共享权值的编码器组成，在概念上是一个三重网络体系结构。第一和第二编码器将数据增强生成的相似图像作为正对，第二和第三编码器采用RTM选择的负对。通常，定义三重态网络[116]的目标是为了使正对的特征更加相似，而使负对的特征更加不同。<br/>​  虽然许多现有的DC方法联合学习表示和聚类，如JULE和DAC，但也有专门设计的表示学习方法[117]、[118]、[119]、[120]、[121]，以自我监督的方式学习图像的视觉表示。这些方法通过训练深度网络来解决额外的任务来学习语义表示。这些任务可以是预测补丁上下文[117]，在绘制补丁[118]，彩色图像[119]，解决拼图游戏[120]，和预测旋转[121]，等等。近年来，这些自监督表示学习方法已被用于图像聚类。例如，多模态DC（MMDC，multimodalDC）[73]利用一个预测旋转的辅助任务来提高聚类性能。通过采用最近邻（SCAN，Semanticclustering by adopting nearestneighbors）[74]进行语义聚类，首先采用自监督表示学习方法来获得语义上有意义的高级特征。然后，它将语义上有意义的最近邻作为先验信息集成到一个可学习的聚类方法中。<br/>​  由于DEC[43]和JULE[63]被提出联合学习DNNs的特征表示和聚类分配，许多基于DAE和基于DNN的DC方法被提出，并在聚类任务方面取得了很大的进展。然而，在聚类方法中提取的特征表示很难扩展到其他任务，如生成样本。深度生成模型最近引起了广泛的关注，因为它们可以使用神经网络来获取数据分布，从而可以生成样本（VAE[122]，GAN [123]，Pixel-RNN [124]，InfoGAN [125]，和PPGN[126]）。其中，GAN和VAE是两种最典型的深层生成模型。近年来，研究人员将其应用于各种任务，如半监督分类[127]、[128]、[129]、[130]、聚类[131]和图像生成[132]、[133]。在III-C节和III-D节中，我们分别介绍了基于生成模型的DC算法：基于VAE的DC和基于GAN的DC。</p><h2 id="c.-基于vae">C. 基于VAE</h2><p>​  基于非参数聚类（DNC，nonparametricclustering）[134]的深度学习是将深度信念网络应用于DC的先驱工作。然而，在基于概率图形模型的DC中，更多的研究来自于VAE的应用，它将变分推理和DAE结合在一起。<br/>​  大多数基于VAE的DC算法旨在解决一个优化问题ELBO（见推论细节[122]和[135]），p是联合概率分布，q的近似概率分布p（z|x），x是集群的输入数据，z是对应于x而生成的潜在变量：<spanclass="math display">\[L_{\mathrm{ELBO}}=\mathbb{E}_{q(z|x)}\biggl[\log\frac{p(x,z)}{q(z|x)}\biggr].\]</span>​  不同之处在于，不同的算法有不同的潜在变量生成模型或不同的正则化器。我们列出了近年来备受关注的几种基于VAE的DC处理方法如下。为了方便起见，我们忽略了概率分布的参数化形式。<br/>​  传统的VAE生成一个连续的潜在向量z，x是一个原始数据样本的向量。在聚类任务中，基于VAE的方法生成潜在向量（z，y），其中z是表示嵌入的潜在向量，y是标签。因此，优化的ELBO成为：<spanclass="math display">\[L_{\mathrm{ELBO}}=\mathbb{E}_{q(z,y|x)}\biggl[\log\frac{p(x,z,y)}{q(z,y|x)}\biggr].\]</span>​  第一个提出的无监督深度生成聚类框架是变分深度嵌入（VaDE， variationaldeep embedding）[77]。VaDE使用结合VAE的高斯混合模型（GMM，Gaussianmixturemodel）[136]对数据生成过程进行建模。在该方法中，是在高斯混合先验中而不是在单个高斯先验中联合考虑簇分配和潜在变量。<br/>​  与VaDE类似，高斯混合VAE（GMVAE，GaussianmixtureVAE）[78]是另一种将VAE与GMM相结合的DC方法。具体地说，GMVAE考虑生成模型<spanclass="math inline">\(p(x,z,n,c)=p(x|z)p(z|n,c)p(n)p(c)\)</span>，其中c是均匀分布的k个类别，n是正态分布。z是一个连续潜变量，其分布是均值和方差为c和n的高斯混合变量。基于平均场理论[137]，GMVAE因子<spanclass="math inline">\(p(x,z,n,c)=p(x|z)p(z|n,c)p(n)p(c)\)</span>作为后验代理。同样，将这些变分因子用神经网络参数化，并优化了ELBO损失。<br/>​  潜在树VAE（LTVAE，latenttreeVAE）[80]基于GMM和VAE，应用潜在树模型[138]进行表示学习和结构学习进行聚类。不同的是，LTVAE有一个VAE的变体，具有潜在变量的上层结构。上层结构是在潜在特征之上的离散潜在变量的树形结构。所有变量之间的连通性结构被定义为通过消息传递[139]进行优化的潜在树模型的潜在结构。<br/>​  一些深度生成聚类方法的成功取决于良好的初始预训练。例如，在VaDE[77]中，需要进行预训练来初始化集群质心。在通过图嵌入（DGG，graphembedding）[140]的GMVAE的DC中，需要预训练来初始化图嵌入。虽然GMVAE[78]共同学习先验和后验参数，但每个类的先验都依赖于一个随机变量，而不是类本身，这似乎是违反直觉的。基于GMVAE和VaDE的思想，Prasad等人[82]为了解决他们的谬误，提出了一种利用VAE进行图像聚类（VAEIC，VAEfor imageclustering）的新模型。与上述方法不同的是，VAEIC的先验参数是确定性的，并且先验参数和后验参数是联合学习的，而不需要进行预训练过程。不是像GMVAE和VaDE那样执行贝叶斯分类，VAEIC采用更直接的推理和更有原则的潜在空间先验，从而得到更简单的推理模型<spanclass="math inline">\(p(x,z,c)=p(x|z)p(z|c)p(c)\)</span>和更简单的近似后验<spanclass="math inline">\(q(z,c|x)=q(c|x)q(z|x,c)\)</span>。聚类分配由<spanclass="math inline">\(q(c|z)\)</span>直接预测。此外，作者采用数据增强，设计了图像增强损失，使模型具有鲁棒性。<br/>​  除了上面提到的基于VAE的DC方法外，Figueroa和Rivera[79]使用连续的Gumbel-Softmax分布[141]，[142]来近似分类分布进行聚类。[81]等人扩展了变分阶梯自动编码器[143]，并提出了一种解纠缠聚类算法。Cao等人[83]提出了一种简单、可扩展、稳定的变分DC算法，该算法引入了对变分DC的通用改进。</p><h2 id="d.-基于gan">D. 基于GAN</h2><h2 id="e.-基于gnn">E. 基于GNN</h2><h1 id="iv.-半监督的深度聚类">IV. 半监督的深度聚类</h1><p>​  传统的半监督学习可分为三类：半监督分类[170]、[171]、半监督降维[172]、[173]和半监督聚类[13]、[174]、[175]。通常，无监督数据的约束被标记为“必须链接”和“不能链接”。具有“必须链接”约束的样本属于同一个集群，而具有“不能链接”约束的样本属于不同的集群。大多数半监督聚类目标是无监督聚类损失和约束损失的结合。<br/>​  半监督深度聚类技术还没有得到很好的研究。在这里，我们介绍了几个具有代表性的方法。这些工作使用不同的方法将关系约束和神经网络相结合，以获得更好的聚类性能。我们在表三中总结了这些方法。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218220853997.png"alt="image-20241218220853997" /><figcaption aria-hidden="true">image-20241218220853997</figcaption></figure><p>​  半监督DEC（SDEC）[165]是基于DEC[43]的，并在特征学习过程中加入了成对约束。其损失函数被定义为 <spanclass="math display">\[\operatorname{Loss}=\operatorname{KL}(S\|R)+\lambda{\frac{1}{n}}\sum_{i=1}^{n}\sum_{k=1}^{n}a_{ij}\left\|z_{i}-z_{j}\right\|^{2}\]</span>​  其中，λ是一个权衡参数。如果<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>被分配给同一集群，则<spanclass="math inline">\(a_{i j} = 1\)</span>，如果<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>满足不能链接约束，则<spanclass="math inline">\(a_{i j} = -1\)</span>，否则<spanclass="math inline">\(a_{i j} =0\)</span>。如损失函数所示，它由两部分组成。第一部分是KL散度损失，在第三-A节中解释。第二部分是半监督损失，表示嵌入式特征<spanclass="math inline">\(\{z_{i}\}_{i=1}^{n}\)</span>与参数<spanclass="math inline">\(a_{i j}\)</span>之间的一致性。直观地说，如果<spanclass="math inline">\(a_{i j} = 1\)</span>，为了最小化损失函数，<spanclass="math inline">\(\|z_{i}-z_{j}\|^{2}\)</span>应该很小。相比之下，如果<spanclass="math inline">\(a_{i j} = -1\)</span>，为了最小化损失，<spanclass="math inline">\(\|z_{i}-z_{j}\|^{2}\)</span>应该很大，这意味着zi与zj在潜在空间Z中是分开的。<br/>​  和SDEC一样，大多数半监督直流方法都是基于无监督直流方法的。通过增加半监督损失，可以直接将无监督直流方法扩展为半监督直流方法。与无监督直流方法相比，数据中额外的半监督信息可以帮助神经网络提取更适合聚类的特征。也有一些工作集中于将现有的半监督聚类方法扩展到深度学习版本。例如，基于DEC（SSLDEC）的半监督学习和深度约束聚类（DECC）[167]的特征提取过程都是基于DEC的。他们的训练过程类似于半监督的k-means[174]，它通过交替使用已标记和未标记的数据样本来学习特征表示。在训练过程中，算法使用标记样本来保持模型的一致性，并选择高度置信度的未标记样本作为新标记样本来调整网络。基于神经网络[168]的半监督聚类结合了<em>k</em>-means损失和成对散度，同时学习聚类中心以及语义上有意义的特征表示。GDAN[169]利用实例识别准则，通过借口任务获取域不变特征。随后，GDAN通过语义邻居聚类，专门关注高级语义特征，对齐这两个领域。</p><h1 id="v.-deep-mvc">V. DEEP MVC</h1><p>​  上述深度聚类方法只能处理单视图数据。在实际的聚类任务中，输入数据通常有多个视图。例如，同一主题的报告可以用不同的语言来表达，同一只狗可以用相机从不同的角度捕捉到，相同的单词可以用不同的书写风格的人来书写。提出了MVC方法[18]，[176]，[177]，[178]，[179]，[180]，[181]，[182]，[183]，[184]，[185]，利用多个视图之间的互补信息来提高聚类性能。<br/>​  近年来，深度学习在MVC中的应用一直是[186]、[187]、[188]、[189]、[190]等领域的热点。这些深度MVC算法专注于解决具有不同形式的输入数据的聚类问题。由于这些方法中使用的网络结构都是自编码器，因此我们根据采用的聚类理论基础将其分为三类：基于dec、基于子空间聚类和基于gnn。它们汇总见表四。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218221528005.png"alt="image-20241218221528005" /><figcaption aria-hidden="true">image-20241218221528005</figcaption></figure><h2 id="a.-基于dec">A. 基于DEC</h2><h2 id="b.-基于子空间聚类">B. 基于子空间聚类</h2><h2 id="c.-基于gnn的">C. 基于GNN的</h2><h1 id="vi.-基于迁移学习的深度聚类">VI. 基于迁移学习的深度聚类</h1><p>​  迁移学习已经成为一种新的学习框架，以解决训练和测试数据来自不同的特征空间或分布[225]的问题。对于复杂的数据，如有噪声视频的高分辨率真实图片，传统的聚类方法即使是深度聚类方法都不能很好地工作，因为特征空间的高维性，没有统一的标准来保证聚类过程。迁移学习通过传输信息为这些问题提供了新的解决方案，这些信息从源域获得附加信息，以指导目标域的聚类过程。在早期，深度域自适应的思想简单而清晰，如利用源域分类损失的深度重构-分类网络和目标域重构分类损失（DRCNs）[226]。这两个域共享相同的特征提取器。随着DNN的发展，我们现在有了更先进的知识转移方法。<br/>​  在本节中，我们将介绍一些关于聚类的迁移学习工作，它被分为两部分。第一部分是“基于DNN的”，第二部分是“基于gan的”。它们总结在表V中。</p><figure><imgsrc="../postimages/Deep-Clustering-Survey/image-20241218221816576.png"alt="image-20241218221816576" /><figcaption aria-hidden="true">image-20241218221816576</figcaption></figure><h2 id="a.-基于dnn">A. 基于DNN</h2><h2 id="b.-基于gan">B. 基于GAN</h2><h1 id="vii.-深度聚类的未来发展方向">VII. 深度聚类的未来发展方向</h1><p>​  基于上述文献的综述和分析，DC已被应用于几个领域，我们重视几个值得进一步研究的方面。</p><ol type="1"><li><p>理论探索：<br/>  虽然通过针对特定问题的解决需求设计更复杂的深度聚类方法，已经实现了显著的聚类性能，但对于如何定性分析特征提取和聚类损失对最终聚类的影响，目前还没有可靠的理论分析。因此，探索深度聚类优化的理论基础，对指导该领域的进一步研究具有重要意义。</p></li><li><p>深度聚类：</p><p>​  由于大量数据带来的复杂性，大多数现有的深度聚类模型都是为特定的数据集设计的。来自不同来源和形式的复杂数据给聚类带来了更多的不确定性和挑战。目前，需要深度学习和图学习来解决复杂的数据处理问题。</p></li><li><p>模型效率：</p><p>​  深度聚类算法需要大量的样本来进行训练。因此，在小样本数据集中，DC容易发生过拟合，这导致聚类效应降低，模型的泛化性能降低。另一方面，具有大规模数据的深度聚类算法具有较高的计算复杂度，因此可以采用模型结构优化和模型压缩技术，以减少模型的计算负荷，并在实际应用条件下提高效率。</p></li><li><p>多视图数据融合：</p><p>​  在实际应用场景中，聚类通常不仅使用单个图像信息，还使用可用的文本和语音信息。然而，目前大多数深度聚类算法只能使用一种信息，不能很好地利用现有的信息。后续的研究可以考虑充分整合两个或两个以上视图的信息，充分利用不同视图数据的一致性和互补性，以提高聚类效果。此外，如何在过滤噪声的同时结合不同视图的特征，以确保更好的视图质量还需要解决。</p></li><li><p>基于图学习的深度聚类：<br/>  在现实中，大量的数据集以图结构的形式存储。图结构可以表示样本点之间的结构关联信息。如何有效地利用结构性信息对于提高聚类性能尤为重要。无论是单视图深度聚类还是多视图深度聚类的广泛应用，现有的基于图学习的聚类方法仍然存在一些问题，如图结构信息没有得到充分利用，不同视图的差异和重要性。因此，对复杂图结构信息的有效分析，特别是合理利用图结构信息来完成聚类任务，还需要进一步的探索。</p></li></ol><h1 id="viii.-深度聚类方法的总结">VIII. 深度聚类方法的总结</h1><p>​  本文介绍了深度聚类领域的最新研究进展。这主要是一种数据结构：单视图、半监督、多视图和迁移学习。单视图方法是我们调查中最重要的部分，它继承了传统聚类方法的问题设置。我们系统地区分了具有数据源的聚类方法，并根据其所基于的网络进一步介绍了它们。在这些网络中，基于DAE的方法和基于DNN的方法较早被提出，但可能由于其在真实数据集上的性能较差而受到限制。与基于DAE和基于DNN的方法相比，基于VAE和基于GAN的方法近年来因其强大的特征提取和样本生成能力而备受关注。图神经网络是近年来最受欢迎的网络之一，特别是在社区发现问题中。因此，我们也总结了基于GNN的聚类方法。随着互联网的发展，聚类数据存在不同的应用场景，因此我们总结了一些存在不同问题设置的聚类方法。半监督聚类方法用约束来聚类数据，这些约束可以通过添加约束损失从单视图聚类方法中发展出来。多视图聚类方法使用不同视图的信息作为补充。它已被广泛地应用于传统的神经网络和图神经网络中。迁移学习可以将已标记域的知识转移到未标记域。我们介绍了基于迁移学习的两种类型网络的聚类方法：DNN和GAN。基于DNN的方法侧重于两个领域的测量策略，而基于GAN的方法使用鉴别器来拟合测量策略。大多数深度聚类方法的复杂性可以随着数据大小n的线性扩展，使其适合于大规模的现实应用，如社交网络、生物信息学和电子商务。</p>]]></content>
      
      
      <categories>
          
          <category> K-means聚类 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>聚类方法合集</title>
      <link href="/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/"/>
      <url>/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="clusterlookup">1. ClusterLookup</h1><p>方法来源：STEGO: Unsupervised Semantic Segmentation by DistillingFeature Correspondences</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class ClusterLookup(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dim: int, n_classes: int):</span><br><span class="line">        super(ClusterLookup, self).__init__()</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.clusters = torch.nn.Parameter(torch.randn(n_classes, dim))</span><br><span class="line"></span><br><span class="line">    def reset_parameters(self):</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            self.clusters.copy_(torch.randn(self.n_classes, self.dim))</span><br><span class="line"></span><br><span class="line">    def forward(self, x, alpha, log_probs=False):</span><br><span class="line">        normed_clusters = F.normalize(self.clusters, dim=1)</span><br><span class="line">        normed_features = F.normalize(x, dim=1)</span><br><span class="line">        inner_products = torch.einsum(&quot;bchw,nc-&gt;bnhw&quot;, normed_features, normed_clusters)</span><br><span class="line"></span><br><span class="line">        if alpha is None:</span><br><span class="line">            cluster_probs = F.one_hot(torch.argmax(inner_products, dim=1), self.clusters.shape[0]) \</span><br><span class="line">                .permute(0, 3, 1, 2).to(torch.float32)</span><br><span class="line">        else:</span><br><span class="line">            cluster_probs = nn.functional.softmax(inner_products * alpha, dim=1)</span><br><span class="line"></span><br><span class="line">        cluster_loss = -(cluster_probs * inner_products).sum(1).mean()</span><br><span class="line">        if log_probs:</span><br><span class="line">            return nn.functional.log_softmax(inner_products * alpha, dim=1)</span><br><span class="line">        else:</span><br><span class="line">            return cluster_loss, cluster_probs</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>摘要：</strong>无监督语义分割的目的是在没有任何形式的注释的情况下，将具有语义意义的类别发现和定位。为了解决这个任务，算法必须为每个像素产生既具有语义意义又足够紧凑的特征，以形成不同的簇。与以前使用单一的端到端框架来实现这一点的工作不同，我们建议将特征学习从簇集群紧化中分离出来。根据经验，我们表明，当前的无监督特征学习框架已经产生了密集的特征，其相关性是语义一致的。这一观察结果促使我们设计STEGO（基于能量的图优化的自监督变换器，<strong>S</strong>elf-supervised<strong>T</strong>ransformer with <strong>E</strong>nergy-based<strong>G</strong>raph<strong>O</strong>ptimization），这是一个新的框架，将无监督特征提取为高质量的离散语义标签。STEGO的核心是一种新的对比损失函数，它鼓励特征形成紧凑的集群，同时保持它们在整个语料库中的关系。STEGO在CocoStuff（+14mIoU）和城市景观（+9mIoU）语义分割挑战上，都比之前的技术水平有了显著的改进。</p><p><strong>引言：</strong>与之前的方法相比，我们利用了无监督特征学习框架中的预训练特征，并专注于将它们提取成一个紧凑和离散的结构，同时保持它们在图像语料库中的关系。这是由于观察到无监督特征之间的相关性，如DINO学习到的特征（Caronet al.，2021），在同一图像内和跨图像集合的语义上已经是一致的。</p><p>因此，我们引入了STEGO（基于能量的自监督变压器），它能够在没有人工监督的情况下联合发现和分割对象。STEGO利用一种新的对比损失，将预先训练过的无监督视觉特征提取为语义簇。STEGO大大改进了现有技术，是缩小与监督分割系统的差距的相当大的一步。我们包括一个简短的视频，详细介绍了在https://aka.ms/stego-video的工作。具体来说，我们做出了以下贡献：</p><ul><li>结果表明，无监督深度网络特征与真实语义标签基本一致。</li><li>介绍了一种新的基于转换器的无监督语义分割架构STEGO。</li><li>证明STEGO在协同（+14 mIoU）和城市景观（+9mIoU）分割挑战上都取得了最先进的性能。</li><li>通过对CocoStuff数据集的消融研究来证明STEGO的设计。</li></ul><p><strong>方法：</strong></p><p>基于特征对应关系预测类的共现性</p><p>​  自我监督视觉特征学习的最新进展已经产生了具有强大的和语义相关的特征的方法，从而改进了各种下游任务。虽然大多数研究的目标是为图像生成单个向量，但许多研究表明，中间密集的特征是语义相关的（汉密尔顿等人，2021；Collins等人，2018；Zhou等人，2016）。为了使用这些信息，我们将重点放在密集特征图之间的“相关性体积”（Teed&amp;Deng，2020）上。对于卷积架构或转换器架构，这些密集的特征映射可以是特定层的激活映射。此外，变压器中的Q、K或V矩阵也可以作为候选特征，尽管我们发现这些注意张量在实践中表现得不太好。更正式地说，假设f∈RCHW，g∈RCIJ是两种不同图像的特征张量，其中C表示通道维数，（H，W），（I，J）表示空间维数。我们形成了特征对应张量：<spanclass="math display">\[F_{hwij}:=\sum_{c}\frac{f_{chw}}{|f_{hw}|}\frac{g_{cij}}{|g_{ij}|},\]</span>​  其项表示特征张量f的空间位置（h、w）处的特征与特征张量g的位置（i、j）之间的余弦相似度。在特殊情况下，这些对应度量同一图像的两个区域之间的相似性。</p><h1 id="softkmeans">2. softkmeans</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</title>
      <link href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/"/>
      <url>/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/</url>
      
        <content type="html"><![CDATA[<center>Attentive and Contrastive Image Manipulation Localization With BoundaryGuidance <a href="https://ieeexplore.ieee.org/document/10589438"><imgsrc="https://img.shields.io/badge/TIFS-2024-orange" alt="TIFS" /></a></center><center>Wenxi Liu , <em>Member, IEEE</em>, Hao Zhang , Xinyang Lin , Qing Zhang, Qi Li , Xiaoxiang Liu , Ying Cao</center><h1 id="摘要">摘要</h1><p>​  近年来，图像生成技术的快速发展，导致了对篡改图像的广泛滥用，导致了信任危机，影响了社会公平。因此，我们工作的目标是检测和定位图像中被篡改的区域。许多基于深度学习的方法已经被提出来解决这一问题，但它们很难处理手动微调到图像背景的篡改区域。通过观察缓和区域的边界对篡改和非篡改部分的分离至关重要，我们提出了一种新的边界引导的图像操作检测方法，它引入了利用篡改区域边界信息的固有偏差。我们的模型采用编译码器结构，采用多尺度定位掩码预测，并通过注意机制和对比学习来引导下利用先验边界知识。特别地，我们的模型因为如下原因是独特的，1)我们在网络解码器中提出了一个边界感知注意模块，该模块预测被篡改区域的边界，从而将其作为关键的上下文线索来促进定位；2)我们提出了一种多尺度的对比学习方案，具有新的边界引导采样策略，从而产生更多的区别定位特征。我们在几个公共基准上的最新表现证明了我们的模型相对于之前的作品的优越性。<br/>​  索引术语-图像操作检测/定位。</p><h1 id="引言">1. 引言</h1><p>​  我们的论文的目标是在像素级别上定位不同类型的图像操作（包括拼接、复制-移动和删除）。主要的挑战在于难以区分被篡改和未被篡改的区域，特别是被篡改的区域是从原始图像中复制的，它们被仔细地微调。因此，被篡改区域和未被篡改区域之间的差异变得很小。之前的方法旨在学习特定于任务的显著特征[6]，[7]，[8]，[9]，但要么它们只能处理特定的操作类型[10]，[11]，[12]，[13]，[14]，[15]，要么它们很容易被精心操作的图像混淆。<br/>​  在被篡改的图像中，被篡改区域的边界是分离被篡改和未被篡改像素的关键位置，在定位被篡改区域时应特别注意并明确利用这一点。然而，如何利用这些边界信息来提高检测被篡改图像区域的性能仍有待探索。<br/>​  在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。首先，为了进一步增强操作定位，我们鼓励该模型关注一个被篡改区域周围的边界，其中经常存在非自然的混合。其次，受对比学习[16]、[17]、[18]、[19]、[20]的启发，我们寻求学习一个特征空间，即篡改区域内的点远离篡改区域边界附近的非调和区域点，以获得更强大的特性来定位篡改区域。<br/>​  与之前天真地利用边界信息[21]的工作相比，如联合预测篡改边界和掩模（见图5），我们的专注和对比的方法提供了一种新颖的、更复杂的利用边界信息的方法，并被证明比之前的方法更有效。<br/>​  在注意方面，在我们的框架的解码层中，我们提出了一种新的基于交叉注意的边界感知模块，旨在提取图像中被篡改区域的边界，从而使模型进一步集中于被篡改区域的边界。特别是，边界感知注意模块利用跳连编码特征与前一层解码特征的相关性，提取被篡改区域的边界，进一步用于生成图像篡改定位的掩模。<br/>​  在特征学习方面，我们提出的模型是基于一个典型的编解码器架构及其特征学习监督由一个新颖的对比目标函数[16]，[22]，[23]，表示为边界引导篡改对比损失，为了推动分开特征采样的篡改和非篡改区域，从而学习更多的区别特征表示。为此，我们采用边界引导的采样策略来收集负训练对，其中我们在被篡改区域的边界周围采样负样本，而不是整个非被篡改区域。该采样方案不仅鼓励模型关注存在非自然混合的边界区域，而且减轻了未篡改区域内巨大变化引起的干扰（见图1中的可视化特征）。<br/>​  为了进行评估，我们在几个公共数据集上进行了实验，包括CASIA[24]、Conbyea[25]、Coverage[26]和NIST16[27]。通过将我们的方法与之前的方法进行比较，证明了我们提出的模型可以达到最先进的性能。总之，我们的工作贡献包括：</p><ul><li>我们提出了一种新的边界引导图像篡改定位模型，该模型通过精心设计的注意力和对比学习机制充分利用被篡改区域的边界信息，而不是以往工作中使用边界信息的简单策略。</li><li>我们在框架的解码器中引入了一个边界感知注意模块，旨在指导模型通过提取被篡改区域的边界来强调图像操作的非自然混合。</li><li>我们提出了一种边界引导的篡改对比损失，鼓励模型将样本的边缘从篡改和非篡改区域扩大到最大的程度。</li><li>我们在几个基准测试上对我们的方法与现有的方法进行了广泛的评估和比较，并表明我们的方法达到了最先进的性能。</li></ul><h1 id="相关工作">2. 相关工作</h1><p>​  在本节中，我们将介绍有关图像操作检测/定位、深度伪造检测和对比学习的相关文献。</p><h2 id="a.-图像操作检测定位">A. 图像操作检测/定位</h2><p>​  由于操作特定的图像区域不可避免地会在被篡改区域与其周围区域之间留下痕迹，因此有几种方法利用边界信息来有利于操作检测[5]、[21]、[35]。多任务全卷积网络（MFCN）[35]提出利用两个输出分支来定位剪接区域的边界。作为一个基于gan的模型，GSR-Net[21]通过合成现有数据集的篡改图像来学习检测图像操作，通过篡改区域及其边界共同监督。参考文献[5]提出了一个双分支网络MVSS-Net，它融合了噪声分布和通过Sobel提取的边缘信息来完成图像操作定位。我们的方法共享一个寻求利用篡改边界信息的高级思想，但探索了两种新的方法，利用对比学习和注意机制来纳入边界先验，这在之前的图像篡改检测的工作中没有研究过。</p><h2 id="b.-深度伪造检测">B. 深度伪造检测</h2><p>​  近年来，随着生成模型的发展，深度伪造检测的任务已经引起了[40]、[41]、[42]、[43]、[44]、[45]等研究者的关注。它的目的是识别人脸的表情甚至身份被篡改的图像。从本质上，深度伪造检测解决了一个图像级的二值分类问题。与深度伪造检测任务不同，我们的图像操作定位任务需要估计被篡改的图像区域的位置，这是一个像素级的预测任务。</p><h2 id="c.-对比学习">C. 对比学习</h2><p>​  无监督/自监督学习方法[20]、[22]、[46]、[47]一般包括借口任务和损失函数两个方面。它们都致力于更好地学习数据表示。近年来，对比学习损失在[16]、[22]、[23]、[48]等方面取得了显著进展。这些方法通过从正对中关闭样本并将样本从负对中推开来学习表征。参考[49]使用一个内存库来存储实例类表示向量，并通过区分不同的实例和特征表示来提出实例级对比学习。其他工作[50]，[51]探索选择一批中的阳性和阴性样本，而不是一个记忆库。MoCo[23]提出了无监督的视觉表示学习，它从比较学习的角度构建了一个带有队列和移动平均编码器的动态字典。对于图像操作，最近的一项工作，CFL-Net[52]，提出使用对比度学习来分离未被篡改和被篡改的补丁嵌入的分布。相比之下，我们提出了一种边界引导的抽样策略，以寻找更多信息的负对，使我们学习的特征更具鉴别性。</p><h1 id="我们的方法">3. 我们的方法</h1><h2 id="a.-网络概述">A. 网络概述</h2><p>​  我们工作的目标是在像素级上检测和定位被篡改的区域。我们所提出的模型的体系结构如图2所示。<br/><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /></p><p>图2。左图：我们的框架的概述。将输入篡改图像的编码特征通过一个混合注意模块和多个边界感知注意模块来预测被篡改区域和边界的多尺度掩模。每个尺度的解码器还通过边界引导的篡改对比损失进行监督。右图：边界引导的篡改对比损失。给定解码器的特征图，采用对比学习方法对属于同一区域（即篡改或未篡改区域）的点特征进行分组，同时分离不同区域的点特征。点特征的采样的篡改区域被限制在外部边界区域的篡改区域和硬对挖掘方法用于使模型关注的类型难以处理积极对两个遥远的样本（硬正对）和负对与两个相近的样本（硬负对）。</p><p>​  给定一个经过处理的图像<spanclass="math inline">\(I\)</span>作为输入，我们使用ResNet-50[53]作为骨干来提取多尺度的视觉特征，<span class="math inline">\(X_i(i =\{1, . . .,S\})\)</span>。然后，将特征输入到由通道注意块和空间注意块连续级联组成的混合注意模块中，以便转换特征，从而在传递到解码器之前预先定位潜在的被篡改区域。基于[54]、[55]、[56]和[57]，我们采用了混合注意模块，可以对最深的编码特征<spanclass="math inline">\(X_S\)</span>沿空间维度和通道的长期依赖关系进行建模。具体地说，它由通道注意块<spanclass="math inline">\(F_{ch}\)</span>和空间注意块<spanclass="math inline">\(F_{sp}\)</span>依次级联组成，分别通过沿信道和空间维度的自注意方案实现。之后，<spanclass="math inline">\(X_S\)</span>将与编码的特征结合，在通过解码器之前，获得<spanclass="math inline">\(\hat{X}_{S}\)</span>，即<spanclass="math inline">\(\hat{X}_{S}=\mathrm{Concat}(X_{S}, F_{ch}(X_{S}),F_{sp}(F_{ch}(X_{S})))\)</span>。<br/>​  在接下来的解码层中，特征不仅被上采样，还与跨尺度特征交互，通过边界感知注意模块定位被篡改区域的边界。每个尺度的边界感知注意模块将同时估计被篡改的区域掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>及其边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。另一方面，为了鼓励模型集中于篡改区域的边界，我们提出了一种基于边界引导采样策略的边界引导篡改对比损失，有利于区分篡改区域和非篡改区域。为了更直观地描述本文中使用的所有符号，我们在表一中列出了所有的符号及其内涵。在下面的章节中，我们将详细阐述我们的边界感知注意模块和边界引导的篡改对比损失。</p><h2 id="b.-边界感知注意力学习">B. 边界感知注意力学习</h2><p>​  操作检测和定位的关键是发生非自然混合的被篡改区域的边界。对被篡改区域边界的准确定位可以有效地帮助被篡改区域的定位。在我们的网络的解码器中，我们合并了所谓的边界感知注意模块<spanclass="math inline">\(F_{ba}\)</span>，专门旨在估计边界。<br/>​  为了定位边界，我们不仅需要来自前一层的特征，而且还需要具有语义和细节的特征。随着解码特征的空间维数的增加，需要更详细的信息。因此，受类似unet的网络结构[58]的启发，我们利用相同尺度上的编码特征<spanclass="math inline">\(X_i\)</span>，以及前一层的解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>，来促进边界定位。边界感知注意模块有两个输出：1)预测的边界<spanclass="math inline">\(\hat{C}_{i}\)</span>和2)将被传播到下一层的特征，并用于生成第<spanclass="math inline">\(i\)</span>个尺度的掩模，<spanclass="math inline">\(\hat{M}_{i}\)</span>。这个过程可以表示如下： <spanclass="math display">\[\begin{aligned}\ [\tilde{X}_{i},\hat{C}_{i}]&amp;=F_{ba}(\hat{X}_{i+1},X_{i}), \\\hat{X}_{i}&amp;=\mathrm{Concat}(\tilde{X}_{i},\mathrm{Upsample}(\hat{X}_{i+1})),\\\hat{M}_{i}&amp; =\mathrm{Conv}(\hat{X}_{i}).\end{aligned}\]</span><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png"alt="image-20240910222829501" /></p><p>​  在图3中，受[59]、[60]、[61]和[62]的启发，我们设计了边界感知注意模块的结构。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910112215485.png"alt="image-20240910112215485" /><figcaption aria-hidden="true">image-20240910112215485</figcaption></figure><p>图3。边界感知注意模块的说明。<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>中选择的像素级特征分别用深绿色和深红色表示。通过交叉注意模块，我们得到了增强的特征<spanclass="math inline">\(\hat{v}_{i}\)</span>，用蓝色表示，然后将其映射回<spanclass="math inline">\(X_i\)</span>，同时保持剩余的特征不变。</p><p>​  首先，我们对编码的特征<spanclass="math inline">\(X_i\)</span>进行降采样，以匹配来自前一层解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的维数，并将它们连接起来。为了提取特征的边界信息，我们通过平均池化、卷积和sigmoid算子的组合对特征进行平滑，并让原始特征减去平滑后的特征，得到与篡改边界相关的高频信息。该过程可以描述如下：<spanclass="math display">\[\begin{aligned}&amp;\tilde{H}_{i}=\mathrm{Concat}(\mathrm{Downsample}(X_{i}),\hat{X}_{i+1}),\\&amp;H_{i}=\tilde{H}_{i}-\tilde{H}_{i}\odot\mathrm{Sigmoid}(\mathrm{Conv}(\mathrm{AvgPool}(\tilde{H}_{i}))),\end{aligned}\]</span>​  其中，<span class="math inline">\(\hat{H}_{i}\)</span>是<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的组合特性。<spanclass="math inline">\(\odot\)</span>表示元素级的乘法。利用所获得的特征<spanclass="math inline">\(H_i\)</span>来预测边界，即<spanclass="math inline">\(\hat{C}_{i}=\mathrm{Conv}(H_{i})\)</span>。<br/>​  接下来，需要使用这些特征来生成篡改掩码<spanclass="math inline">\(\hat{M}_{i}\)</span>并传递到下一层，因此应该利用与篡改最相关的信息。在这里，我们使用一个特征选择块<spanclass="math inline">\(F_{fs}\)</span>，从预测的边界图<spanclass="math inline">\(\hat{C}_{i}\)</span>中找到前K个高置信像素的索引。然后，这些索引引导模型对<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>对应的像素级特征进行采样，分别记为<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>。 <spanclass="math display">\[\mathbf{v}_{i}=F_{fs}(X_{i},\mathrm{TopK}(\hat{C}_{i})),\mathbf{\hat{v}}_{i+1}=F_{fs}(\hat{X}_{i+1},\mathrm{TopK}(\hat{C}_{i})),\]</span>​  其中K实际上设为32。由于<span class="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>是定位边界的最关键的特征，因此我们采用了一个交叉注意模块，其中<spanclass="math inline">\(v_i\)</span>作为查询，<spanclass="math inline">\(\hat{v}_{i+1}\)</span>作为键/值，如下所示： <spanclass="math display">\[\hat{\mathbf{v}}_i=\mathbf{v}_i+\text{Softmax}(\frac{\mathbf{v}_i\hat{\mathbf{v}}_{i+1}^T}{\sqrt{d_k}})\hat{\mathbf{v}}_{i+1},\]</span>​  其中<span class="math inline">\(d_k\)</span>表示<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>的维数。最后，我们根据所选择的索引将<spanclass="math inline">\(\hat{v}_{i}\)</span>分散到<spanclass="math inline">\(X_i\)</span>中，并保持未被选择的位置不变。<br/>​  损失函数：我们应用多尺度损失来学习具有代表性的多尺度特征，以进行更精确的预测。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /><figcaption aria-hidden="true">image-20240910110200920</figcaption></figure><p>​  在实践中，如图2所示，所有四个尺度都生成了操作掩模，而仅对两个尺度的中间边界掩模进行预测。对于具有最深特征的预测掩模，即<spanclass="math inline">\(\hat{M}_{S}\)</span>，我们应用二进制交叉熵（BCE）损失和IoU损失进行监督。此外，我们采用加权二值交叉熵损失[63]和加权IoU损失[63]来监督预测的掩模<spanclass="math inline">\(\hat{M}_{i}=(i\neq S)\)</span>和边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。边界的groundtruth值是通过从膨胀图像中减去二值地面真实掩模的侵蚀而得到的。具体来说，我们应用核大小为5×5、步幅为1的最大池化操作来进行图像扩张和侵蚀。</p><h2 id="c.-边界引导下的篡改对比学习">C. 边界引导下的篡改对比学习</h2><p>​  一旦一幅图像被篡改，其被篡改的区域可能会显示出与未被篡改的区域略有不同的视觉统计数据，例如，不自然的照明信息或不一致的噪声分布。扩大学习特征空间中篡改区域和非篡改区域之间的差异，可以有效地提高学习特征的鉴别能力，有利于篡改区域的定位。<br/>​  基于此，我们采用对比学习，目的是学习区分特征表示，可以区分篡改和非篡改部分。我们选择对每个尺度，对直接用于预测最终掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>的特征图进行对比学习，我们根据经验发现它的效果很好。特别是，在训练过程中，我们以点的方式对特征图进行空间采样，从篡改和未篡改区域收集样本。在这里，一个样本指的是在特征映射的特定位置上的一个特征向量。然后，我们最小化一个对比损失函数，以减少同一区域内样本之间的距离（即正对），同时增加不同区域内样本之间的距离（即负对）。为了进一步提高学习特征的鲁棒性和可鉴别性，我们引入了一种边界引导的采样策略来构造信息更丰富的负对。<br/>​  1)边界引导篡改对比损失：从解码器的每个尺度上，我们采用一个对比损失，它由两项组成，即<spanclass="math inline">\(\mathcal{L}^{TC}=\mathcal{L}^{+}+\mathcal{L}^{-}\)</span>，其中<spanclass="math inline">\(\mathcal{L}^{+}\)</span>和<spanclass="math inline">\(\mathcal{L}^{-}\)</span>分别是正对损失和负对损失。在形式上，<spanclass="math inline">\(\mathcal{L}^{+}\)</span>被写为： <spanclass="math display">\[\begin{aligned}\mathcal{L}^{+}&amp;=\frac{1}{|\mathcal{H}_{t}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{t}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n}))\\&amp;+\frac{1}{|\mathcal{H}_{n}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{n}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\end{aligned}\]</span>​  其中，<spanclass="math inline">\(\mathrm{Sim}(\cdot,\cdot)\)</span>表示两个特征向量之间的余弦距离，<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>是来自被篡改区域的一组正对，而<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>包含从未被篡改区域提取的正对。<spanclass="math inline">\((\mathbf{u}_{m},\mathbf{u}_{n})\)</span>表示被采样的特征向量对。<spanclass="math inline">\(\mathcal{L}^{+}\)</span>作为一种力，将特征空间中同一区域内的样本拉在一起。为了获得样本的正对，我们使用硬对挖掘策略，将在同一区域内但彼此远离的样本聚集在一起。具体来说，在来自被篡改或未被篡改区域的所有可能的样本对中，我们保留具有前L个最大距离的样本对，构成<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>。在实践中，L被设置为来自同一区域的所有可能样本对数量的一半。我们对<spanclass="math inline">\(\mathcal{L}^{-}\)</span>的定义如下： <spanclass="math display">\[\mathcal{L}^{-}=\frac{1}{|\mathcal{H}^{-}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}^{-}}-\log(1-\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\]</span>​  其中<spanclass="math inline">\(\mathcal{H}^{-}\)</span>是一个负对集，其中每对都由来自被篡改区域的查询样本和来自未被篡改区域的负样本组成。<spanclass="math inline">\(\mathcal{L}^{-}\)</span>旨在将来自不同地区的样本分开。<br/>​  2)边界引导采样策略：为了构建<spanclass="math inline">\(\mathcal{H}^{-}\)</span>，一种简单的方法是从一个未被篡改的区域随机抽取负样本。然而，在一个未被篡改的区域内，通常存在很大的差异。例如，未被篡改的区域可能占据图像的很大一部分，因此它可能包含大量分散各种物体或杂乱的背景。因此，绘制负样本的原生方法往往会降低模型的性能。因此，我们提出了一种新的采样策略，即我们在一个被篡改区域的边界附近绘制负样本，而不是从整个非调和区域（见图4中的示例）。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910140100074.png"alt="image-20240910140100074" /><figcaption aria-hidden="true">image-20240910140100074</figcaption></figure><p>图4。边界引导的抽样策略。蓝色点表示未篡改区域的采样特征，红色的表示被篡改区域的采样特征。</p><p>​  这种采样方法具有两个方面的优势：第一，约束边界区域内的负样本倾向于减少未篡改区域内方差大而造成的干扰；第二，篡改区域的边界周围经常发生非自然的混合，因此从该区域采样鼓励模型专注于边界区域，这与我们框架的其他组件，如边界感知注意模块。特别地，给定一个地真二值篡改区域掩模<spanclass="math inline">\(M\)</span>，我们对其进行图像扩展，得到一个放大的掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>，并将负样本限制为来自<spanclass="math inline">\(\hat{M}_{i}-M_i\)</span>指定的外部边界区域。最后，我们对<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>采用硬对挖掘方法，只保留距离最小的前L个负对来构造<spanclass="math inline">\(\mathcal{H}^{-}\)</span>。</p><h1 id="实验结果">4. 实验结果</h1><p>​  在本节中，我们将在几个公共基准上进行全面的实验，并将我们提出的方法与以前的最先进的方法进行比较。我们还分析了我们的模型的不同组件。</p><h2 id="a.-实施细节和数据集">A. 实施细节和数据集</h2><p>​  <strong>实施细节：</strong>我们使用Pytorch实现了我们的框架，并使用一个NVIDIA TitanxpGPU进行训练和测试。对于训练，所有输入的图像都被调整到512×512的分辨率，并通过随机的水平翻转、颜色抖动和裁剪来增强它们。我们采用ResNet50[53]作为骨干。在训练过程中，我们使用了Adam优化器[64]，其动量为0.9，重量衰减为5×10−4。我们将批处理大小设置为16，并使用多项式策略[65]调整学习率，基本学习率为1×10−5，幂次为0.9。为了进行测试，首先将输入图像的大小调整到512×512，用于网络推断，然后将输出映射的大小调整回输入图像的原始大小。<br/>​  <strong>数据集：</strong>为了评估模型的性能，我们在6个基准测试上进行了实验，包括CASIA[24]、NIST16[27]、Columbia[25]、Coverage[26]、Defacto[67]和一个真实世界的数据集IMD2020[68]。此外，我们还利用了[69]提出的合成数据集。这些数据集涵盖了不同类型的操作，包括拼接、复制-移动和删除。所有的数据集都提供了ground-truth的二进制掩码。</p><ul><li>CASIA[24]由两个子数据集组成，CASIAv1的数据为921张篡改图像和CASIAv2的篡改图像为5123张图像。篡改类型包括拼接和复制移动。裁剪后的区域通常是精心选择的，并应用后处理操作，使区域在视觉上逼真。</li><li>NIST16[27]包含564个被篡改的图像样本。所有三种操作类型都涉及到，它们还进行后处理以隐藏可见的痕迹。</li><li>Columbia[25]专注于拼接，它包含180张未压缩的图像。</li><li>Coverage[26]关注于包含100张图像的复制-移动操作。被操作的对象被手动裁剪以覆盖同一图像中的相似对象，并对它们进行后处理以去除可见的操作痕迹。</li><li>Defacto[67]是最近提出的一个大规模合成数据集，包含149k图像，这些图像从MS-COCO[70]中采样，并通过复制移动、拼接和绘制自动操作。</li><li>IMD2020[68]包含2010年真实的从互联网收集的真实操作图像，涉及所有三种操作类型。</li><li>合成训练数据集[69]包含了最初从[70]中收集到的大约100k张图像，涵盖了拼接、复制-移动和删除的操作类型。</li></ul><p>​  在以下小节的定性结果中，特征图显示在颜色图中，以较暖的颜色表示更多的关注，反之亦然。比较方法的预测篡改掩模在0到1的灰度图像中可视化。预测的掩模的每个像素都意味着被篡改的确定性。</p><h2 id="b.-与最先进的技术进行比较">B. 与最先进的技术进行比较</h2><h3 id="像素级篡改检测">像素级篡改检测</h3><p>​  在[5]和[34]之后，我们将我们提出的方法与两种实验设置下的几种最先进的方法进行了比较。<br/>​  在第一种设置中，在MVSS-Net[5]之后，所有的比较模型都是在真实数据上从头开始训练的，没有任何额外的合成数据集。我们采用CASIAv2[24]在其他公共基准上进行训练和测试，包括CASIAv1、NIST16、Columbia、Coverage、IMD20和DEFACTO。我们将我们的模型与之前的几种方法进行了比较，包括ManTra-Net[37]、HP-FCN [71]，CR-CNN [72]，GSR-Net [21]、SPAN [66]、CAT-Net[73]，MVSS-Net [5]和MVSS-Net++[74]。我们不与ObjectFormer[34]进行比较，因为它的代码是不公开的。表二显示了在[5]中报告的固定阈值为0.5的F1的测量结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910143404035.png"alt="image-20240910143404035" /><figcaption aria-hidden="true">image-20240910143404035</figcaption></figure><p>​  我们可以观察到，我们的模型在所有的数据集上都达到了最好的定位性能。我们注意到，GSR-Net还通过预测篡改的边界和掩码来利用其网络中的边界信息。我们从GSR-Net在所有基准上的显著改进表明，在利用边界信息方面，我们提出的注意和对比机制比简单的边界和掩模预测的优势。<br/>​  在第二种设置中，在[34]、[69]和[76]之后，我们首先在一个合成数据集上对每个模型进行预训练，然后在基准的训练集上进行微调，然后在基准的测试集上进行测试。比较方法包括RGB-N[38]、SPAN [66]、PSCC-Net [69]、ObjectFormer[34]、HiFi-Net [77]和ERMPC[76]，在各自的合成数据集上进行训练，在每个基准的训练集上进行微调，并在相应基准的测试集上进行测试。我们评估了所有的方法在CASIA，NIST16和Coverage。对于CASIA，来自CASIAv2的5123张图像用于微调，来自CASIAv1的921图像用于测试。对于NIST16，564张图像的NIST16，404张图像用于微调，160张图像用于测试。有100张图像的Coverage被分为75/25来进行微调和测试。对于这种设置，正如[34]中报道的，不同的方法使用自己的合成数据集，不同的内容和不同数量的图像从47k到100k不等。鉴于此，为了公平比较，我们从[69]的合成数据集中随机选择了60k张图像。关于三个基准测试的结果见表三。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910204820762.png"alt="image-20240910204820762" /><figcaption aria-hidden="true">image-20240910204820762</figcaption></figure><p>​  请注意，Columbia没有训练集，因此对它进行微调是不可能的。在CASIAv1和NIST16上，我们的方法大大优于所有比较方法，尽管我们的方法与SPAN和PSCCNet相比，该方法使用的数据要少得多。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205006572.png"alt="image-20240910205006572" /><figcaption aria-hidden="true">image-20240910205006572</figcaption></figure><p>​  通过深入研究Coverage上的失败情况（图12），我们发现我们的模型往往会在特定类型的图像上失败，在这些图像中，区域边界周围的操作痕迹被小心地擦去。对于图12中所示的所有失败案例，我们的模型的F1评分都相当低（小于35%）。通过从测试集中排除这三个图像，我们的方法的F1得分上升到78.1%。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205223378.png"alt="image-20240910205223378" /><figcaption aria-hidden="true">image-20240910205223378</figcaption></figure><p>​  图5显示了我们的方法与最先进的方法的定性比较。结果表明，我们的方法不仅可以更准确地定位被篡改的区域，而且还可以产生更清晰的边界（图5的前三行），这得益于边界感知的注意模块。此外，结果表明，我们的模型比其他模型对背景分心（图5的最后三行）更稳健，这是由于引入了抑制噪声的对比损失。</p><h3 id="图像级篡改检测">图像级篡改检测</h3><p>​  虽然我们的模型更关注像素级篡改定位任务，但它具有检测图像级篡改的能力。图像级篡改检测的目的是将输入图像分类为真实或篡改。我们遵循[5]的协议来运行一个图像级的篡改检测实验，其中我们的模型是在CASIAv2上进行训练的。考虑到CASIAv1和CASIAv2共享782张真实图像，我们从Corel[78]中随机抽取782张真实图像，以替换CASIAv1中的这些副本，从而得到数据集CASIAv1+。我们在表四中显示了三个基准的结果，CASIAv1+、Coverage和Columbia。在本实验中，我们将我们的模型与ManTraNet[37]、CR-CNN [72]、GSR-Net [21]、SPAN [66]和MVSS-Net[5]进行了比较。<br/>​  为了执行图像级篡改检测，我们修改了我们的模型，通过在特征<spanclass="math inline">\(\hat{X}_{s}\)</span>中添加一个图像分类头，以预测图像被篡改的概率。具体来说，图像分类头由一个CBR块、一个平均池化层和一个完全连接的层组成，其中CBR块是卷积、批处理归一化（BN）和ReLU的组合。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205812866.png"alt="image-20240910205812866" /><figcaption aria-hidden="true">image-20240910205812866</figcaption></figure><p>​  如表四所示，我们的方法在所有基准上都获得了最好的F1分数，我们的模型的AUC分数在所有方法中排名第二好，这表明我们的模型能够在像素级定位和图像级检测任务中都产生良好的性能。</p><h2 id="c.-对未篡改图像的定位">C. 对未篡改图像的定位</h2><p>​  我们还在未被篡改的图像上测试了我们的模型以进行像素级定位，并在图6中显示了定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205954595.png"alt="image-20240910205954595" /><figcaption aria-hidden="true">image-20240910205954595</figcaption></figure><p>​  用我们的方法预测的未被篡改图像的掩模在图6的前三行上几乎为空白，而比较的方法错误地检测到一些位置被篡改了。对于图6的最后两行，所有的方法都出现了一些假阳性。然而，我们的模型倾向于将假阳性限制在小的局部区域，而其他方法则倾向于将它们分散在整个图像上。此外，从图6的最后一行，我们观察到，我们的模型可能会误解到真实图像中的可疑区域。相比之下，MVSS-Net[5]倾向于对误检测的置信度较低。这将导致我们的模型有更高的机会将真实的图像误分类为被篡改，这部分解释了为什么我们的模型在B节的图像级操作检测实验中与MVSS-Net相比的AUC评分较低。</p><h2 id="d.-模型组件分析">D. 模型组件分析</h2><p>​  为了阐明单个组件的影响，我们评估了在不同配置下提出的模型。所有报告的结果都来自于CASIA数据集。</p><h3 id="边界感知注意力模块">边界感知注意力模块</h3><p>​  边界感知注意模块可以部署在解码器的每个尺度上来预测边界，因此我们验证了表v中的设计。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210222453.png"alt="image-20240910210222453" /><figcaption aria-hidden="true">image-20240910210222453</figcaption></figure><p>​  作为参考，我们有一个没有任何BAMs的基线模型，它可以达到53.9%的f1和87.4%的AUC。首先，我们在最深的尺度上部署BAM（即i=3），它打算将最小的分辨率转换为边界。正如观察到，添加一个BAM会导致更好的性能，这可以从获得3%的F1分数中得到暗示。其次，我们在第二个尺度（即i=2）上附加了另一个BAM，因此有两个协作的BAM用于预测不同尺度的边界。通过编码特征带来的更详细的信息，该模型能够以60.0%的F1和88.6%的AUC达到最佳性能。最后，当我们在所有尺度上加入三个算法算法时，性能明显下降，因为最低尺度的特征引入的噪声会对边界预测产生负面影响。<br/>​  为了演示我们的边界感知注意模块如何通过检测边界来帮助定位被篡改的区域，我们在图7中展示了一个特征图的可视化。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210329644.png"alt="image-20240910210329644" /><figcaption aria-hidden="true">image-20240910210329644</figcaption></figure><p>​  为了进行比较，我们还将MVSS-Net[5]中最后一个Sobel层所产生的特征可视化，它提取了与边缘相关的特征。图7中的结果表明，由于我们的边界感知注意模块，我们的方法可以更准确地定位篡改区域。<br/>​  此外，我们还在图8中给出了一些关于BAM有效性的例子。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210454382.png"alt="image-20240910210454382" /><figcaption aria-hidden="true">image-20240910210454382</figcaption></figure><p>​  从我们可以观察到，在没有BAM的帮助的情况下，估计的掩模在边界区域往往是不完整的，因为像素可以很好地融合到背景中，而且它们很难区分。虽然BAM模型可能不能完美地预测边界，但它足以提供上下文线索来推断整个被篡改的掩模。<br/>​  如图9所示，我们还展示了在不同数据集上与MVSS-Net[5]进行比较的定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210642783.png"alt="image-20240910210642783" /><figcaption aria-hidden="true">image-20240910210642783</figcaption></figure><p>​  为了验证所提出的边界感知注意模块的有效性，我们在第三列和第四列中可视化最终的掩模和预测的边界掩模。在前两行观察到，虽然MVSS-Net可以粗略预测被篡改的区域，但其预测包含较大的灰色区域，表明确定性较低。相比之下，我们的预测结果不仅可以准确地定位边界更清晰的被篡改区域，而且对被篡改区域具有更高的确定性。同时，在第四和第五行，MVSS-Net可能对真实的真实区域有假阴性，而在我们的预测中，背景区域几乎是黑色的。计算结果表明了我们所提模型的优越性。</p><h3 id="边界导向的篡改对比损失">边界导向的篡改对比损失</h3><p>​  回想一下，我们的边界引导篡改对比损失的目标是将特征嵌入与相同区域的距离拉近，同时将特征与不同区域的距离分开，即篡改和非篡改区域的距离，使它们更具区分性。首先，我们评估了在表六中关于如何部署损失的差异。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210828335.png"alt="image-20240910210828335" /><figcaption aria-hidden="true">image-20240910210828335</figcaption></figure><p>​  可以观察到，当我们在所有尺度上使用损失时，它通常会达到最优结果。相比之下，使用第二和第三等级（没有最高尺度）的AUC略有改善（88.9%），但F1略有下降（51.5%）。这是因为最高尺度的特征直接对应于最终的结果，因此它与操作检测的精度有关。如果不使用顶级规模的特性，一般的性能就会变得更糟。<br/>​  图10显示边界引导篡改对比损失对于篡改区域不明显的图像是有效的。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211059841.png"alt="image-20240910211059841" /><figcaption aria-hidden="true">image-20240910211059841</figcaption></figure><p>​  可以观察到的，在使用对比学习损失后，被篡改区域边界上的差异变大。此外，纹理图像的被篡改区域是某些物体的不完整部分。结果表明，我们的模型可以很好地推广到部分级篡改。<br/>​  此外，图11为应用多尺度对比学习损失后的不同尺度的特征图。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211157377.png"alt="image-20240910211157377" /><figcaption aria-hidden="true">image-20240910211157377</figcaption></figure><p>​  可以观察到，随着解码器规模的增加，对比学习损失以粗到细的方式定位篡改区域。在最好的尺度上，我们可以清楚地观察到被篡改区域周围边界的颜色变成蓝色，而被篡改区域的颜色变成红色，它们的大差异是由对比学习损失造成的。这种现象在第二尺度和第三尺度的特征上并不明显，因此第一尺度的特征为模型的改进提供了最大的增益。</p><h3 id="混合注意力模块">混合注意力模块</h3><p>​  我们还对我们的混合注意模块进行了消融研究，发现移除该模块会导致f1和AUC分别降低0.4%和2.8%。这说明了这个模块的重要性。</p><h3 id="边界引导的抽样策略">边界引导的抽样策略</h3><p>​  抽样策略在我们提出的对比损失中起着重要的作用。为了验证抽样策略的有效性，我们对表七中的CASIA数据集进行了实验。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211419997.png"alt="image-20240910211419997" /><figcaption aria-hidden="true">image-20240910211419997</figcaption></figure><p>​  最直接的方法是从被篡改区域和未被篡改区域中随机采样特征点。相比之下，我们对从篡改区域和非篡改区域随机抽取的点采用了硬对挖掘，从而提高了性能。然而，在非篡改区域内存在很大的差异，导致非篡改区域随机抽样的对比学习结果的鲁棒性。因此，我们将采样范围限制在篡改边界附近的未篡改区域内。可以观察到，在篡改边界附近的篡改和非篡改区域随机选择点，会导致负样本之间存在较大差异，导致网络性能下降。最后，我们将硬对挖掘策略与从边界区域随机抽取的样本相结合，获得了最优的性能。</p><h2 id="e.-超参数分析">E. 超参数分析</h2><p>​  我们从F1和AUC分析了边界引导篡改对比损失和CASIA数据集边界感知注意模块的超参数，如表八所示。这涉及到6个关键的超参数。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211517337.png"alt="image-20240910211517337" /><figcaption aria-hidden="true">image-20240910211517337</figcaption></figure><p>​  1)输入图像大小：训练数据集有不同大小的图像，其中大多数图像的分辨率为384×256，其他图像有不同的分辨率，如640×480,336×638,500×375。因此，我们遵循[5]的方法，将不同输入图像大小的输入图像调整为512，并找到我们选择的512×512。我们的×512实验结果最好，如表八所示。<br/>​  2)选择指标数K：作为我们提出的边界感知注意模块的关键，所选指标对应于与边界预测最相关的特征。我们将所选指标的个数表示为K，并将其设为16、32和64。如表八所示，当K为32时，我们得到的性能最好。<br/>​  3)生成GroundTruth边界的扩张和侵蚀数：生成一个细化的GroundTruth边界对于监督第三节B小块中边界感知注意模块的预测边界掩模<spanclass="math inline">\(\hat{C}_{i}\)</span>是必要的。该过程包括通过从GroundTruth掩模的膨胀中减去GroundTruth掩模的侵蚀来导出GroundTruth边界。我们试图将膨胀和侵蚀的数量从1增加到3，这导致地面真实边界越来越宽，并发现更窄的地面真实边界会得到更好的结果。<br/>​  4)未篡改采样区域的扩张数：为了执行边界引导采样策略，我们从篡改区域的扩张中减去篡改区域的GroundTruth掩模，得到篡改区域周围的未篡改边界区域，从中对未篡改区域的点特征进行采样。我们用不同的膨胀次数进行实验，发现3次的膨胀效果最好。<br/>​  5)样本数Z：对于边界引导采样策略，在确定正负样本对的采样区域后，随机选择一些构建正对集和负对集。为了获得这些样本对，我们需要分别从篡改区域和未篡改区域抽取一定数量的以Z表示的样本。根据经验，我们将Z设为250和500。对于包含小于Z像素的被篡改区域，我们对所有像素的特征进行采样。我们在表八中显示了结果。正如所观察到的，一个小的Z意味着我们对这两个区域都采样不足，因此可能不能充分利用样本。<br/>​  6)Top-L硬对：对于边界引导采样策略，我们采用硬对挖掘方法获得正负样本对，其中我们对Top-L硬对进行采样，并将L设置为整个实验中所有对数量的一半。我们用L=1和L=来实验所有对的数量。如表八所示，我们的上半部分策略取得了最好的性能。top-1策略容易被极端样本对误导，而全对策略在样本间存在较大差异，从而引入无关信息来分离篡改区域和非篡改区域。</p><h2 id="f.-鲁棒性评价">F. 鲁棒性评价</h2><p>​  根据[5]、[34]和[69]中的失真设置，我们对我们的网络进行了鲁棒性分析。具体来说，对CASIA数据集中的操作图像应用了不同的失真操作。此外，我们结合了任意两种畸变，其中从区间[0.25、0.78]分别、[3,15]、[3,15]和[50,100]中随机选择调整尺度、核大小、标准差和质量因子。<br/>​  最后，我们将各种扭曲结合在一起，作为“<em>Mixed</em>”。我们应用f1和AUC来测量定位性能。对输入图像应用畸变将不可避免地导致边界信息被损坏，从而导致性能下降。然而，与MVSS-Net相比，我们的方法对失真显示了更鲁棒的性能，如表九所示。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212202152.png"alt="image-20240910212202152" /><figcaption aria-hidden="true">image-20240910212202152</figcaption></figure><p>​  我们还分析了在CASIAv1+上关于各种失真的图像级检测的鲁棒性。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212225437.png"alt="image-20240910212225437" /><figcaption aria-hidden="true">image-20240910212225437</figcaption></figure><p>​  如表X所示，我们在调整大小和JPEG压缩失真方面的性能不如MVSS-Net。这可能是由于这些操作造成的边界信息被污染，严重影响了我们的边界感知注意模块。尽管如此，我们的模型仍然取得了合理的性能，并且我们的鲁棒性可以与MVSS-Net相当，甚至更优。</p><h2 id="g.-效率分析">G. 效率分析</h2><p>​  在计算复杂度方面，我们的方法实现了39.44个GFLOPs，明显低于MVSS-Net的163.57个GFLOPs。此外，我们使用具有24GBGPU内存的GeForce RTX3090来评估计算时间。使用相同的GPU，我们的完整模型需要0.014秒来处理一幅图像，比MVSS-Net快得多，后者需要0.038秒。</p><h1 id="结论和局限性">5. 结论和局限性</h1><p>​  在这项工作中，我们的目标是图像篡改定位问题。为此，我们提出了一种新的边界引导方法，其固有的倾向于通过注意机制和对比学习方案充分利用被篡改区域的边界信息。特别地，我们提出了一个具有边界感知能力的注意模块，它可以预测被篡改区域的边界，以迫使网络特别注意重要的边界区域。此外，我们引入了一种新的对比损失与边界引导抽样策略来学习更多的有区别的特征。我们证明，作为在CASIAv2上的训练，我们提出的模型在四个不同的基准上大大优于最先进的方法。此外，当在合成数据集上进行预训练时，我们的模型在现实基准上也显示了可比性或优越的通用性。<br/>​  我们在图12中显示了覆盖范围测试集上的失败情况。当被篡改边界的痕迹被仔细地擦去（图12的第一列）或类似的边界同时出现在被篡改的区域（图12的第二列）时，我们的模型可能难以区分被篡改区域的边界和未被篡改物体的边界。此外，如果被篡改的区域是两个相似对象的局部区域（图12的第二列和第三列），那么我们的方法可能会失败。在未来的工作中，我们将开发区分对象边界和被篡改区域边界的技术，旨在进一步提高我们的性能。<br/>​  此外，虽然这项工作的主要焦点是像素级的操作，但我们已经证明了我们的模型能够在图像级上检测操作。尽管通过简单地调整我们的模型来适应这种图像级的任务，已经取得了很好的性能，但肯定有一些进一步改进的空间。因此，未来研究的一个有趣的途径是专门为我们的模型定制图像级的检测问题，以优化性能，或探索使用我们的模型或它的一部分作为像素级和图像级任务的联合建模的主干。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning</title>
      <link href="/KaggleNet/"/>
      <url>/KaggleNet/</url>
      
        <content type="html"><![CDATA[<center>Towards Generalizable Deepfake Detection via Clustered and AdversarialForgery Learning</center><center>Yiming Chen, Haiwei Wu, Fengpeng Li, Kemou Li,</center><center>Zheng Li, Kahim Wong, Xiangyu Chen, Binbin Song,</center><center>Shuning Xu, Jun Liu, $ Jiantao Zhou ^ * $</center><center>September 2, 2024</center><h1 id="团队详细信息">1 团队详细信息</h1><p>团队名称：JTGroup<br/>组长：mortonchen（yimingchen98@gmail.com）<br/>团队成员人数（按字母顺序排列）：</p><ul><li>chxy95 (chxy95@gmail.com)</li><li>fengpengli (fengpeng.li@connect.umac.mo)</li><li>highwayw (823215616@qq.com)</li><li>kahimwong (jesonwong47@gmail.com)</li><li>kemoulee (kemoulee@gmail.com)</li><li>namecantbenull (doublebin111@gmail.com)</li><li>rebeccaee (rebeccaxu0418@gmail.com)</li><li>umlizheng (umlizheng@gmail.com)</li><li>yiyayoo (junlwind@163.com)</li></ul><p>隶属关系：澳门大学智能城市物联网国家重点实验室1<br/>团队主管：教授。JiantaoZhou2<br/>∗通讯作者<br/>1https://skliotsc.um.edu.mo/<br/>2https://www.fst.um.edu.mo/people/jtzhou/</p><h1 id="解决方案详细信息35">2解决方案详细信息（35%）</h1><p>​  本节首先介绍了解决方案设计背后的动机，然后概述了基本的工作流和其中每个阶段的细节。</p><h2 id="动机">2.1动机</h2><p>​  在获得官方的训练和验证数据后，我们发现即使是简单的小模型（如Efficient-b0[1]）也可以毫不费力地拟合训练集，并在验证集上实现显著的检测性能。这这表明训练集和验证集是同分布的，并且由相当同质的特征类型组成。虽然这种过拟合问题使小模型能够表现良好，但它不利于在实际场景中检测未知数据。在图1给出了一个说明性的例子，它通过t-SNE可视化了训练集和验证集的特征分布：</p><figure><img src="../postimages/KaggleNet/image-20240902200824430.png"alt="image-20240902200824430" /><figcaption aria-hidden="true">image-20240902200824430</figcaption></figure><p>​  图1：动机：训练集和验证集的官方分割中的特征可视化揭示了两个关键的见解：1)训练集和验证集之间分布的相似性使得评估模型在看不见的测试集上的性能具有挑战性；2)假数据的特征类型比真实数据更丰富。这一见解启发了我们最初使用无监督聚类来划分具有挑战性的验证集，然后基于对抗性学习原则增加真实和假数据的类型。这些见解促进了我们提出的泛化性的深度伪造检测方法。<br/>​  结果表明，在该数据分割方案下训练的模型在未知测试集上只能达到0.877的个AUC。<br/>​  基于这些观察结果，我们建议在训练集上进行特征级别（这在后门和噪声标签研究[3,4]中常用）的无监督聚类，然后重新分割训练集和验证集，以包括不同类型的伪造或真实数据。该方法旨在迫使模型更好地适应检测未知类型。此外，虽然深度伪造检测是一项分类任务，但我们在训练过程中在特征和对抗水平上引入了额外的优化目标（受[5]启发），以加强从不同角度对伪造的广义和鲁棒表示的学习。如图1的下部所示，训练集和验证集的重新聚类和重新分割在特征分布上表现出显著差异，使引导模型在未知测试集上的AUC为0.96。此外，通过使用专家混合策略来综合来自不同数据分割场景的知识，我们在未知测试集上实现了最大AUC为0.98。<br/>（[5]：<ahref="https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/">Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning| zhaozw后院(zhaozw-szu.github.io)</a>）<br/>​  下一节将深入介绍所提出的方法。</p><h2 id="提出的方法概述">2.2提出的方法概述</h2><p>​  该方法可分为三个阶段：1)数据准备、2)训练和3)推理，其基本过程如图2所示。</p><figure><img src="../postimages/KaggleNet/image-20240903113352343.png"alt="image-20240903113352343" /><figcaption aria-hidden="true">image-20240903113352343</figcaption></figure><p>​  在阶段I的第2.3节最初增加和丰富了所提供的官方数据，以提供更多样化的数据分布。随后，采用无监督聚类算法，基于特征对伪造或真实的数据进行聚类，便于训练集和验证集的重新分割。在数据分割之后，在阶段2的第2.4节介绍了学习鲁棒性和一般伪造痕迹的三个优化目标，解决特性级、对抗级和分类级的监督。最后在阶段3的第2.5节实现了特定的推理过程。<br/>​  我们的主要贡献可以总结如下：</p><ul><li>基于无监督聚类的数据类型重新分配设计缓解了由简单分区引起的过拟合问题，有助于模型学习更一般化的伪造痕迹。</li><li>在特征、对抗式和分类级别引入深度伪造检测的联合监督范式，增强了模型学习高度广义伪造表示的能力。</li><li>实验结果表明，该方法具有良好的泛化能力，特别是在检测未知类型的深度造假方面。</li></ul><h2 id="第一阶段数据准备">2.3第一阶段：数据准备</h2><p>​  我们所提出的方法的框架，如图3所示，包括两个主要阶段：数据准备和训练。</p><figure><img src="../postimages/KaggleNet/image-20240903142646600.png"alt="image-20240903142646600" /><figcaption aria-hidden="true">image-20240903142646600</figcaption></figure><p>​  在数据准备阶段，我们专注于通过图像编辑（Edit）和稳定扩散（SD，StableDiffusion）技术生成新的数据来增强现有的数据集。然后，我们进行聚类来重新组装数据集，旨在提高我们的方法的检测性能和鲁棒性。在训练阶段，我们引入了一系列的专家模型，并使用三种类型的损失对其进行优化：<span class="math inline">\(\mathcal{L}_{\mathrm{KL}}\)</span>、<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>和<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>。这种多损失方法确保了模型能够有效地区分真实图像和篡改图像。</p><h3 id="数据生成">2.3.1数据生成</h3><p>​  为了增强模型泛化和缓解过拟合，我们使用图像编辑和稳定扩散（SD）技术扩展了训练数据集。</p><figure><img src="../postimages/KaggleNet/image-20240903143630833.png"alt="image-20240903143630833" /><figcaption aria-hidden="true">image-20240903143630833</figcaption></figure><p>​  如图4所示，数据生成过程包括以下操作：<br/>​  <strong>图象编辑：</strong>我们使用的第一种技术是图像编辑，它包括修改原始图像的特定元素来创建新的变体。首先，我们应用人脸语义分割来分离人脸区域和背景。一旦分离出来，我们就会用不同的颜色（如紫色、绿色和蓝色）来修改背景，同时保留原始的面部特征。这一步引入了数据集的变化，模拟了模型在现实场景中可能遇到的不同环境和照明条件。<br/>​  <strong>稳定扩散：</strong>第二种技术利用稳定扩散（SD，StableDiffusion）模型从原始数据集生成新的图像。从原始图像<spanclass="math inline">\(I_0\)</span>开始，我们首先将其转换为中间潜在表示。然后，我们应用各种文本提示来指导SD模型生成不同版本的图像。例如，提示像“.. . old . . . ”, “. . . person. . . ”, 和“. . . young . . .”，分别被用来创造年老的、一般的和恢复活力的面孔。生成的图像反映了广泛的风格和特征，用真实数据和篡改数据的不同表示丰富了训练集。<br/>​  通过结合图像编辑和SD生成的结果，我们产生了一个增强的数据集<spanclass="math inline">\(\cal{D}\)</span>，它显著增强了训练数据的可变性。然后在随后的训练阶段中使用这个全面的数据集，确保模型暴露于广泛的特征变化，从而提高其泛化到看不见的数据的能力。</p><h3 id="数据聚类">2.3.2数据聚类</h3><p>​  在原始数据集中，训练集和验证集的分配是随机分配的。然而，在实际的伪造检测场景中，测试中遇到的伪造类型通常与训练中遇到的不同。当模型遇到新的、看不见的伪造类型时，这种差异会导致性能的显著下降。为了解决这个问题，我们提出了一种数据聚类方法来重新分配训练和验证数据集。其主要目标是根据伪造类型对数据集进行聚类，从而模拟实际的测试条件，其中模型暴露在更广泛的各种看不见的伪造。数据集群的详细过程包括以下三个步骤：<br/>​  <strong>分类器模型训练：</strong>我们首先使用一个EfficientNet-b0[1]（Eff-b0）模型来训练一个轻量级的伪造特征提取器。该模型在原始训练集上进行训练，仅使用交叉熵（CE）损失：<span class="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>： <spanclass="math display">\[\mathcal{L}_{\mathrm{CE}}=-\sum_{i=1}^N\sum_{l=1}^L\mathbf{y}_l^{(n)}\log(\hat{\mathbf{y}}_l^{(n)}),\]</span>​  式中，<spanclass="math inline">\(\mathbf{y}_l^{(n)}\)</span>表示第n个样本中伪造类型l的groundtruth软标签，<spanclass="math inline">\(\hat{\mathbf{y}}_l^{(n)}\)</span>为Effb0模型对应的预测概率输出。该模型学习了能够有效区分伪造类型的鉴别特征。<br/>​  <strong>聚类特征提取：</strong>一旦Eff-b0模型被训练，我们使用它来从整个训练集<spanclass="math inline">\(\mathcal{D}=\{(\mathbf{I}^{(n)},y^{(n)})\}_{n=1}^{N}\)</span>中提取特征。对于每幅图像，我们只保留最后一层的输出特征向量，对第n幅图像表示为$~ \mathbf{u}^{(n)}\in\mathbb{R}^{d} $。该过程得到了一个大小为N×d的特征矩阵，封装了整个训练数据集的伪造特征。<br/>​  <strong>基于聚类的重新拆分：</strong>利用提取的特征矩阵，我们应用无监督聚类算法，特别是K-means，将图像分成K个聚类。每个聚类<spanclass="math inline">\(\mathcal{C}_{k}\)</span>代表一组不同的伪造特征，并对应于在后续训练阶段中使用的一组原始图像<spanclass="math inline">\(\mathcal{D}_k\)</span>。聚类后，我们随机重组并将这些K个簇合并成<spanclass="math inline">\(I\)</span>个折叠<spanclass="math inline">\(\{\mathcal{F}_1,\mathcal{F}_2,\ldots,\mathcal{F}_I\}\)</span>。每个折叠包<spanclass="math inline">\(\mathcal{F}_{i}\)</span>含所有原始图像集<spanclass="math inline">\(\mathcal{D}_k\)</span>，但训练集<spanclass="math inline">\(\mathcal{F}_{i}^{\mathrm{train}}\)</span>和验证集<spanclass="math inline">\(\mathcal{F}_{i}^{\mathrm{val}}\)</span>分区不同。这种旋转为每个折叠中的数据分布提供了一个独特的视角，并确保每个集群作为验证集一次，通过模拟实际的测试场景来促进鲁棒的特征学习，其中验证集包含伪造类型的图像，而在相应的训练集中不存在。该设置测试了模型推广到不可见的伪造类型的能力，确保了在现实世界条件下对其性能进行全面的评估。<br/>​  图5描述了来自不同集群的真假数据的预览。</p><figure><img src="../postimages/KaggleNet/image-20240903145603867.png"alt="image-20240903145603867" /><figcaption aria-hidden="true">image-20240903145603867</figcaption></figure><p>​  随后，我们将深入研究基于这些聚类数据的训练的细节。</p><h2 id="第二阶段训练">2.4第二阶段：训练</h2><h3 id="网络架构">2.4.1网络架构</h3><p>​  训练阶段的网络体系结构旨在确保模型能够有效地推广到不同的数据分布中，同时重新维护对伪造和对抗性攻击[6]。如图3所示，该体系结构由几个完整的组件组成，每个组件都有影响整体模型性能的特定角色。</p><figure><img src="../postimages/KaggleNet/image-20240903142646600.png"alt="image-20240903142646600" /><figcaption aria-hidden="true">image-20240903142646600</figcaption></figure><p>​  <strong>专家模型：</strong>每个包<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中的原始图像<spanclass="math inline">\(I\)</span>被输入到可训练的专家模型<spanclass="math inline">\(\{M_1(\mathbf{I};\boldsymbol{\theta}_1),M_2(\mathbf{I};\boldsymbol{\theta}_2),\ldots,M_I(\mathbf{I};\boldsymbol{\theta}_I)\}\)</span>。每个专家模型<spanclass="math inline">\(M_i\)</span>，由<spanclass="math inline">\(\theta_i\)</span>参数化，专门从其各自的折叠<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中学习图像。每个专家的输出是一个高维特征向量<spanclass="math inline">\(\mathbf{v}_{i}=M_i(\mathbf{I};\boldsymbol{\theta}_i)\in\mathbb{R}^d\)</span>，它在随后的两个关键操作中使用：首先，用它来计算InfoNCE损失的[7]<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>；其次，它通过一个概率头来生成交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>的对数。<br/>​  <strong>概率头：</strong>将每个专家模型的高维特征向量<spanclass="math inline">\(\mathbf{v}_{i}\)</span>输入到相应的可训练概率头<spanclass="math inline">\(H_i(\mathbf{v}_i;\phi_i)\)</span>中，由<spanclass="math inline">\(\phi_i\)</span>参数化。概率头将特征向量转换为二维对数向量<spanclass="math inline">\(\mathbf{z}_{i} = H_{i}(\mathbf{v}_{i};\phi_{i})\in \mathbb{R}^{2}\)</span>。然后利用该对数向量计算交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>，这直接影响了模型区分真假图像的能力。</p><h3 id="聚类目标">2.4.2聚类目标</h3><p>​  我们的框架中的聚类目标是将专家模型产生的高维特征<spanclass="math inline">\(\mathbf{v}_i\in\mathbb{R}^d\)</span>变成有语义意义的聚类。这一目标对于提高模型区分真实和虚假图像的能力至关重要，特别是在处理不同的和看不见的伪造类型时。<br/>​  在每个专家模型<spanclass="math inline">\(M_i(\mathbf{I};\boldsymbol{\theta}_i)\)</span>从其相应的折叠<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中处理图像后，使用得到的特征向量<spanclass="math inline">\(\mathbf{v}_i\)</span>来计算InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>。InfoNCE损失鼓励模型在同一聚类内最大化特征向量的相似性，同时最小化不同集群之间的相似性。该损失的定义如下：<spanclass="math display">\[\mathcal{L}_{\text{NCE}}=-\log\frac{\exp(\sin(\mathbf{v}_q,\mathbf{v}_p)/\tau)}{\sum_j\exp(\sin(\mathbf{v}_q,\mathbf{v}_j)/\tau)},\]</span>​  其中，<span class="math inline">\(\mathbf{v}_q\)</span>和<spanclass="math inline">\(\mathbf{v}_p\)</span>分别表示同一聚类内的查询向量和正样本向量，<spanclass="math inline">\(\mathbf{v}_j\)</span>表示来自其他聚类的特征向量。函数<spanclass="math inline">\(sin(\cdot)\)</span>通常指余弦相似度，<spanclass="math inline">\(\tau\)</span>是一个温度尺度因子，可以调整正负样本的相对重要性。<br/>​  在我们的实现中，我们专注于通过最大化不同伪造特征和真实特征之间的距离来增强模型的区分能力。具体来说，我们将真-真样本对视为正样本，真-假样本对视为负样本。该方法可以使模型有效地区分真实图像和伪造图像，从而提高检测精度。这种基于聚类的目标不仅增强了模型在已知伪造类型上的性能，而且通过关注特征表示的相似性，提高了模型处理未知伪造的能力。</p><h3 id="对抗性目标">2.4.3对抗性目标</h3><p>​  对抗性目标被嵌入到训练管道中，以增强模型对对抗性攻击的鲁棒性。这是通过使用kl-散度损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{KL}}\)</span>来实现的，它策略性地应用于对抗性的例子（AEs，adversarialexamples）<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}\in\mathbb{R}^{H\timesW\times C}\)</span>，这些例子是通过在原始输入图像<spanclass="math inline">\(\mathbf{I}\)</span>中添加小扰动<spanclass="math inline">\(\delta\in\mathbb{R}^{H\times W\timesC}\)</span>而产生的。<br/>​  用 $~f_{i}=M_{i}\circ H_{i}\circ\sigma $表示第<spanclass="math inline">\(i\)</span>个专家模型的预测概率的函数，其中<spanclass="math inline">\(\sigma(\cdot)\)</span>为softmax函数。AE生成的优化目标是最大限度地提高<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}\)</span>与<spanclass="math inline">\(\mathbf{I}\)</span>之间的预测距离，如下： <spanclass="math display">\[\mathbf{I}_{\mathrm{adv}}^{(n,i)}=\underset{\mathbf{I}_{\mathrm{adv}}^{(n,i)}\in\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})}{\operatorname*{\arg\max}}D_{\mathrm{KL}}(f_{i}(\mathbf{I}_{\mathrm{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)})),\]</span>​  其中，<spanclass="math inline">\(\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})=\{\mathbf{I}_{\mathrm{adv}}:\|\mathbf{I}_{\mathrm{adv}}-\mathbf{I}^{(n)}\|_{\infty}\leq\epsilon\}\)</span>是半径为<spanclass="math inline">\(\epsilon\)</span>、以<spanclass="math inline">\(\mathbf{I}^{(n)}\)</span>为中心的<spanclass="math inline">\(\ell_{\infty}\)</span>球，<spanclass="math inline">\(D_{\mathrm{KL}}\)</span>是KL-散度。<br/>​  为了解决这一优化问题，我们通过每个专家模型的投影梯度上升迭代更新AE：<spanclass="math display">\[\mathbf{I}_{\mathrm{adv}}^{(n,i)}\leftarrow\Pi_{\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})}\left(\mathbf{I}_{\mathrm{adv}}^{(n,i)}+\alpha\cdot\mathrm{sign}\left(\nabla_{\mathbf{I}_{\mathrm{adv}}^{(n,i)}}D_{\mathrm{KL}}(f(\mathbf{I}_{\mathrm{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)}))\right)\right),\]</span>​  其中，<spanclass="math inline">\(\Pi(\cdot)\)</span>表示投影函数，<spanclass="math inline">\(\alpha\)</span>表示内部步长。<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}^{(n,i)}\)</span>是通过向<spanclass="math inline">\(\mathbf{I}^{(n)}\)</span>添加一个随机高斯噪声来初始化的。<br/>​  一旦生成了<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}^{(n,i)}\)</span>，这些AEs就会被纳入模型训练过程中，以增强模型的鲁棒性。由于良性样本与AEs之间存在显著的分布偏移，因此在训练过程中直接包含AEs可能会降低模型在良性样本上的性能。为了实现准确性和鲁棒性之间更好的权衡，在对抗性的目标中同时考虑良性和对抗性的例子列举如下：<spanclass="math display">\[\mathcal{L}_{\text{KL}}=\sum_{n=1}^{N}\sum_{i=1}^{I}D_{\text{KL}}(f_{i}(\mathbf{I}_{\text{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)})),\]</span>​  其中，N为训练样本的数量，<spanclass="math inline">\(I\)</span>为专家模型的数量，<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>为所有专家生成的所有AEs的kl-散度之和。<br/>​  这种对抗性训练策略确保模型在各种场景中很好地概括，包括那些可能遇到恶意改变的图像的场景。</p><h3 id="分类目标">2.4.4分类目标</h3><p>​  在交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>的驱动下，分类目标是模型准确地将图像分类为真假图像的核心。当使用由标签平滑[8]生成的软标签<spanclass="math inline">\(\mathbf{y}^{(n)}\in[0,1]^2\)</span>作为第n个样本的groundtruth时，每个专家模型<spanclass="math inline">\(M_i(\mathbf{I};\boldsymbol{\theta}_i)\)</span>的概率头<spanclass="math inline">\(H_i(\mathbf{v}_i;\phi_i)\)</span>输出一个二维对数向量<spanclass="math inline">\(\mathbf{z}_i^{(n)} = H(\mathbf{v}_i;\phi) \in\mathbb{R}^2\)</span>。通过对softmax函数进行对数分析，得到了通过第<spanclass="math inline">\(i\)</span>个专家和第<spanclass="math inline">\(n\)</span>个样本的头部的预测概率<spanclass="math inline">\(\hat{\mathbf{y}}_i^{(n)}\)</span>： <spanclass="math display">\[\hat{\mathbf{y}}_i^{(n)}=\sigma\left(\mathbf{z}_i^{(n)}\right).\]</span>​  然后，将带有软标签的CE损失计算为： <spanclass="math display">\[\mathcal{L}_{\mathsf{CE}}=-\sum_{n=1}^N\sum_{i=1}^I\sum_{c=0}^1\mathbf{y}_c^{(n)}\cdot\log(\hat{\mathbf{y}}_{i,c}^{(n)}),\]</span>​  其中，<span class="math inline">\(\mathbf{y}_c^{(n)}\)</span>为第<spanclass="math inline">\(n\)</span>个样本中<spanclass="math inline">\(c\)</span>类的软标签，<spanclass="math inline">\(\hat{\mathbf{y}}_{i,c}^{(n)}\)</span>为通过第<spanclass="math inline">\(i\)</span>个专家模型和分类头对应的预测概率。软标签的使用允许模型捕获更微妙的信息，特别是在类之间的区别不是严格的二进制的情况下，可能导致更鲁棒和更广义的学习结果。<br/>​  CE损失会惩罚错误分类的模型，迫使它产生与groundtruth非常匹配的输出。这确保了每个专家模型都有助于一个鲁棒和准确的最终预测，促进可靠地区分真实和被操纵的图像。</p><h3 id="整体优化">2.4.5整体优化</h3><p>​  总体优化策略将上述各种目标——InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>、对抗性训练损失<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>和交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>——协调为一个单一的连贯训练目标。总损失函数表示为：<spanclass="math display">\[\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{NCE}}+\lambda_{1}\mathcal{L}_{\mathrm{KL}}+\lambda_{2}\mathcal{L}_{\mathrm{CE}}.\]</span>​  在这里，<span class="math inline">\(\lambda_{1}\)</span>和<spanclass="math inline">\(\lambda_{2}\)</span>是调整每个组件损失的相对影响的加权参数。这种集成的方法确保了该模型不仅能够高精度地区分真实样本和虚假样本，而且对对抗性攻击具有鲁棒性，并能够跨不同的数据分布进行泛化。通过平衡这些目标，训练过程产生了一个既强大又通用的模型，使其非常适合于深度伪造检测的复杂任务。</p><h2 id="第三阶段推理">2.5第三阶段：推理</h2><p>​  推理过程利用了鲁棒的训练架构来提供精确和可靠的预测。当图像<spanclass="math inline">\(\mathbf{I}\in\mathbb{R}^{H\times W\timesC}\)</span>时，它被直接输入训练过的专家模型<spanclass="math inline">\(\{M_1(\mathbf{I};\phi_1),M_2(\mathbf{I};\phi_2),\ldots,M_I(\mathbf{I};\phi_I)\}\)</span>，每个对应于训练过程中形成的一个集群。每个专家模型生成一个高维特征向量<spanclass="math inline">\(\mathbf{v}_{i} = M_{i}(\mathbf{I};\phi_{i}) \in\mathbb{R}^{d}\)</span>，然后由相应的概率头处理，生成一个二维logit向量<spanclass="math inline">\(\mathbf{z}_{i}=H_{i}(\mathbf{v}_{i};\phi_{i}) \in\mathbb{R}^{2}\)</span>。最终的预测是通过平均所有专家模型的对数并应用sigmoid函数来确定的：<spanclass="math display">\[\hat{\mathbf{y}}=\sigma\left(\frac{1}{I}\sum_{i=1}^{I}\mathbf{z}_{i}\right).\]</span>​  为了进行分类预测，选择概率最高的类为： <spanclass="math display">\[\hat{y}=\arg\max_{c\in\{0,1\}}(\hat{\mathbf{y}}_c),\]</span>​  其中，c是这些类的索引。<br/>​  这种集成方法结合了所有专门模型的专业知识，确保了预测不仅准确，而且对变化和潜在的对抗性扰动具有鲁棒性。来自多个专家模型的输出的聚合减少了单独偏移的风险，从而更可靠地最终确定输入的真实性。</p><h1 id="多维人脸伪造检测分析35">3.多维人脸伪造检测分析（35%）</h1><h2 id="方法复杂性">3.1方法复杂性</h2><p>​  表1列举了所提出的方法的网络架构细节，包括7个专家编码器，由来自效率网[1]和ConvNeXt[9]的变体组成，促进了跨不同复杂度级别的联合特征学习。</p><figure><img src="../postimages/KaggleNet/image-20240903161617646.png"alt="image-20240903161617646" /><figcaption aria-hidden="true">image-20240903161617646</figcaption></figure><p>​  虽然基于transformer的编码器也被探索，但没有观察到显著的性能改善。这可能表明，在这个竞争的MultiFF数据集中，卷积网络架构具有优势。综上所述，该方法的可训练参数计数为187.123M，可操作的MACs为103.572G。对于512x512的单个图像大小，平均推理速度为0.201774秒，标准偏差为0.024823秒。这些指标是通过在一个H800GPU卡上进行10,000次重复测试而获得的。</p><h2 id="方法训练">3.2方法训练</h2><p>​  在模型训练过程中，我们主要采用正负样本的平衡抽样、余弦退火学习率调整策略和指数移动平均（EMA，exponentialmovingaverage）加权权值平滑。<br/>​  <strong>平衡抽样：</strong>具体来说，通过观察官方数据集中正负样本的比例为1：4，我们有意地采用了正负样本的平衡抽样。该方法有助于避免类别不平衡问题，提高了分类器对少数类样本的准确性，增加了模型学习不同类别之间决策边界的可能性，从而提高了其对不可见数据的泛化能力。这一行动还通过防止模型以牺牲少数类样本为代价，过度关注大多数类样本，从而减轻了过拟合的风险。<br/>​  <strong>退火调整：</strong>此外，余弦退火学习速率调整策略有助于在训练过程中平稳地调整学习速率，防止显著的波动，提高训练的稳定性。它还能更好地收敛到局部最优解，提高了模型在训练集上的性能。<br/>​  <strong>指数移动平均：</strong>为了进一步提高模型的稳定性，防止其陷入局部最优状态，我们在训练过程中加入了指数移动平均（EMA，exponentialmovingaverage）策略，以便在训练过程中平滑模型权值。EMA技术通过随时间平均模型参数，有助于稳定训练过程，从而减少噪声更新引入的方差，提高模型的整体性能。<br/>​  具体来说，在每个训练步骤t时，模型参数<spanclass="math inline">\(\theta\)</span>的EMA根据以下公式进行更新： <spanclass="math display">\[\boldsymbol{\theta}_\mathrm{EMA}^{(t)}=\gamma\boldsymbol{\theta}_\mathrm{EMA}^{(t-1)}+(1-\gamma)\boldsymbol{\theta}^{(t)},\]</span>​  式中，<spanclass="math inline">\(\boldsymbol{\theta}_\mathrm{EMA}^{(t)}\)</span>为步骤t处的平滑参数，<spanclass="math inline">\(\boldsymbol{\theta}^{(t)}\)</span>为当前模型参数，<spanclass="math inline">\(\gamma\)</span>为决定过去参数影响的衰减因子。在我们的实现中，我们将衰减因子<spanclass="math inline">\(\gamma\)</span>设置为0.995，这意味着在每一步中保留了99.5%的当前EMA值，剩下的0.5%由当前参数贡献。<br/>​  EMA的使用确保了模型参数的收敛更平稳，从而防止了可能导致次优解的急剧振荡。这种平滑效果也有助于减少过拟合的风险，因为它通过作为正则化的一种形式，有效地平均梯度更新中的噪声。<br/>​  我们的实验结果表明，将EMA策略集成到训练过程中，可以显著提高模型的性能。具体来说，我们观察到在曲线下面积（AUC）度量中增加了大约0.005，这表明模型的准确性和稳定性得到了提高。这一改进强调了EMA在改进模型的预测能力方面的有效性，使其在检测深度伪造方面更加鲁棒和可靠。</p><h2 id="方法概论">3.3方法概论</h2><p>​  在本节中，我们使用交叉熵损失和AUC评估度量在MultiFF数据集上评估所提方法的性能，如表2所示。<br/><imgsrc="../postimages/KaggleNet/image-20240903163802649.png"alt="image-20240903163802649" /></p><p>​  具体来说，我们引入了几个基线来进行比较。这些基线使用标准的官方训练/验证分割进行训练，并仅使用交叉熵损失进行优化。从表中可以看出，虽然基线在验证集上表现良好，达到AUC得分高于0.99，但它们并不能有效地推广到公共测试集。例如，使用Efficient-b0的基线在公共测试集上只达到了0.86397的AUC。<br/>​  相比之下，我们提出的框架在数据集级别引入了创新的聚类，使得相应的验证集更具挑战性。此外，通过结合聚类和对抗性优化目标，学习到的伪造特征表现出增强的泛化性。这种增强允许由我们的框架支持的Efficient-B0实现0.90767的AUC，超过相应的基线4.37%的AUC。最后，通过整合来自不同专家（网络）的信息，我们的方法在公共测试集上实现了0.98051的AUC，将其置于初步排行榜的前沿，并验证了我们提出的解决方案的有效性。</p><h2 id="方法鲁棒性">3.4方法鲁棒性</h2><p>​  虽然所提出的模型主要是为了增强跨域场景中的检测泛化，但我们也评估了其在更常见的退化场景中的鲁棒性，如添加噪声[10]、JPEG压缩、模糊和降采样。这种评估在现实应用中是至关重要的，因为这些类型的后处理操作通常被用来消除或隐藏伪造的伪造痕迹。为了实现这一点，我们将这些后处理操作应用于原始验证集，并在图6中报告定量结果。</p><figure><img src="../postimages/KaggleNet/image-20240903164202077.png"alt="image-20240903164202077" /><figcaption aria-hidden="true">image-20240903164202077</figcaption></figure><p>​  可以观察到，对于在给定的质量因子（QF）范围内添加高斯噪声或执行JPEG压缩的场景，检测性能几乎保持不变。对于高斯模糊和降采样，性能下降略有增加，约为3%左右。这些评估结果表明，我们提出的模型对这些常见的后处理操作表现出理想的鲁棒性。</p><h2 id="消融研究">3.5消融研究</h2><p>​  本小节分析了每个组件在第一阶段的重新分裂策略，以及第二阶段的聚类、对抗和分类目标方面，如何对我们提出的框架有贡献。消融结果列在表3中，其中第一行给出了基线框架的性能作为比较，而最后一行是经过各种消融研究后建立的最佳焦点框架。</p><figure><img src="../postimages/KaggleNet/image-20240903164320811.png"alt="image-20240903164320811" /><figcaption aria-hidden="true">image-20240903164320811</figcaption></figure><p>​  我们从第一阶段的数据集的重新分割开始了我们的消融研究，关键在于分割策略的选择。为此，我们探索了利用BIRCH[11]、DBSCAN[12]和K-means[13]进行特征聚类。从表3中第2行到第4行所描述的结果可以看出，很明显，使用重新分割策略可以有效地增强通用性。值得注意的是，与基线相比，使用K-means进行分割的AUC最大增加了2.43%。此外，我们还对第二阶段引入的优化目标进行了相应的消融实验和比较实验。特别是在特性层面，我们采用现有的损失，如Triplet[14]、DCL[15]和原始InfoNCE[7]。然而，并非所有这些损失函数都适用于伪造检测任务。例如，Triplet损失将相等的惩罚强度限制为每个查询正或查询负对的距离得分[16]，这将导致模型崩溃，如表的#5所示。虽然DCL和InfoNCE提供了一定的性能提升，但它们考虑负-负对的加权可能是不合适的，因此限制了它们的适用性。相比之下，我们提出的<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>，考虑到伪造数据的多样性，仅仅为了最大限度地提高其与真实数据的距离而施加限制，从而进一步提高了0.68%。对于对抗性目标，我们用现有的算法如FGSM[17]、PGD [18]和C&amp;W[19]，以及focal损失[20]进行了分类目标的实验。这些算法表现出相对具有可比性的性能。由于时间的限制，我们打算在未来进行更详细的实验比较。值得注意的是，表3中显示的损失和AUC可能并不能完全反映测试集的性能。</p><h1 id="全局方法描述25">4全局方法描述（25%）</h1><h2 id="预先训练好的模型">4.1预先训练好的模型</h2><p>​  在第一阶段，为了增加数据集和增加伪造类型的多样性，我们使用了预先训练的SD[21]模型，特别是来自Stability AI的Stable Diffusion 2.1 Base[22]。该模型以其生成高质量图像的能力而闻名，它被用来合成额外的伪造图像，从而能够创建一个更全面的数据集，其中包括更广泛的伪造模式。通过使用该模型进行数据增强，我们的目的是增强我们的检测算法对稳定扩散生成样本的鲁棒性。<br/>​  在第二阶段，我们使用了在ImageNet-1K[23]上预先训练过的专家模型，如EfficientNet[1]和ConvNext[9]。之所以选择这些模型，是因为它们具有提取深度视觉语义的固有能力，这可以提高我们的模型的检测性能和收敛速度。</p><h2 id="额外生成数据">4.2额外生成数据</h2><p>​  我们总共生成了101,413张编辑过的图像，其中包括51,258张来自训练集中假样本增强的图像和50,155张来自真实图像生成的图像。此外，我们使用稳定扩散生成了64451张图像，其中38584张使用提示“aClose Up of a face, barbie, doll-like style, highlydetailed”来创建娃娃脸和芭比娃娃一样的图像。这导致总共增加了165,864张图像。<br/>​  所有这些图像的大小都为512x512像素，总共占用了6.34GB的磁盘空间，平均图像大小约为40KB。这些增强的数据集主要用于训练阶段，以增强数据的多样性和多样性，从而提高模型检测各种类型伪造的能力。</p><h2 id="定性优势">4.3定性优势</h2><p>​  为了进一步评估我们提出的方法在检测不同或未知类型的伪造（如人脸交换、人脸再现、面部属性编辑、人脸合成等），我们在著名的数据集FaceForensics++[24]、DFDC[25]和DFD[26]上进行了额外的实验。以AUC为评价指标的实验结果如表4所示。本文介绍了最先进的方法，如SBI[27]、RECEE [28]和CFM[29]，以与我们的方法进行比较。为了确保公平的比较，我们在FaceForensics++训练集上重新训练了所有的方法。此外，还包括2019年DFDC比赛的冠军解决方案（DFDC-1st-place-[30]）作为参考。请注意，我们没有重新训练DFDC-1st-place的方法，因此，其结果仅供参考，不构成严格的比较。</p><figure><img src="../postimages/KaggleNet/image-20240903165509638.png"alt="image-20240903165509638" /><figcaption aria-hidden="true">image-20240903165509638</figcaption></figure><p>​  从表4可以明显看出，目前最先进的方法（SBI、RECEE和CFM）在域内测试集FaceForensic++上表现良好，AUC得分在0.99以上。然而，在跨域测试集DFDC和DFD上观察到显著的性能下降。例如，RECEE在DFDC上只获得了0.6690的AUC。这一现象强调，在现实世界的深度伪造检测任务中，未知类型的伪造会严重破坏法医算法的性能。因此，设计鲁棒的和可推广的学习策略是至关重要的。<br/>​  在我们提出的方法中，我们创新性地采用无监督聚类进行伪造类型分类，并引入了聚类和对抗性优化目标，旨在提高模型的鲁棒性和泛化性。结果表明，与基线相比，我们的方法在DFDC和DFD上的AUC评分可以分别提高4.29%和3.98%，达到了与最先进的方法相媲美的性能。接下来，我们将进一步探索针对不同类型和看不见的伪造攻击的技术，以确保深度伪造取证的可信度和可靠性。</p><h2 id="新颖性讨论">4.4新颖性讨论</h2><p>​  在我们提出的方法中，创新集中在以下关键方面：</p><ul><li>基于无监督聚类的特征重新分割：这种新的方法涉及到基于无监督聚类的特征的重新分割，有助于改进一般的伪造表示。</li><li>第二阶段的联合特征优化目标：优化引入了协同特征优化目标，增强了整体学习过程和模型泛化。</li></ul><p>​  这些创新的灵感来自于我们研究小组的学术出版物，这些出版物主要发表在信息法医领域的顶级期刊和会议上。具有代表性的出版物如下：</p><ul><li>H. W. Wu, J. T. Zhou, X. Y. Zhang, J. Y. Tian, and W. W. Sun,“Robust Camera Model Identification over Online Social Network SharedImages via Multi-Scenario Learning”, in TIFS’24 (CCF-A, JCR-Q1)[5].</li><li>S. R. Qi, Y. S. Zhang, C. Wang, J. T. Zhou, and X. C. Cao, “APrincipled Design of Image Representation: Towards Forensic Tasks”, inTPAMI’23 (CCF-A, JCR-Q1) [31].</li><li>H. W. Wu, J. T. Zhou, J. Y. Tian, J. Liu, and Y. Qiao, “Robust ImageForgery Detection against Transmission over Online Social Networks”, inTIFS’23 (CCF-A, JCR-Q1) [32]</li><li>F. P. Li, K. M. Li, J. Y. Tian and J. T. Zhou, “Regroup Median Lossfor Combating Label Noise”, in AAAI’24 (CCF-A, JCR-Q1) [4].</li><li>J. Liu, J. T. Zhou, J. D. Zeng, and J. Y. Tian, “DifAttack:Query-Efficient Black-Box Attack via Disentangled Feature Space”, inAAAI’24 (CCF-A, JCR-Q1) [6].</li><li>Y. M. Chen, J. Y. Tian, X. Y. Chen, and J. T. Zhou, “EffectiveAmbiguity Attack Against Passport-based DNN Intellectual PropertyProtection Schemes through Fully Connected Layer Substitution”, inCVPR’23(CCF-A, JCR-Q1) [33].</li><li>Y. M. Chen, H. W. Wu, and J. T. Zhou, “Progressive Poisoned DataIsolation for Training-time Backdoor Defense”, in AAAI’24 (CCF-A,JCR-Q1) [3].</li><li>B. B. Song, X. Y. Chen, S. N. Xu, and J. T. Zhou, “Under-DisplayCamera Image Restoration with Scattering Effect”, in ICCV’23 (CCF-A,JCR-Q1) [34].</li><li>H. W. Wu, J. T. Zhou, J. Y. Tian, and J. Liu, “Robust Image ForgeryDetection over Online Social Network Shared Images”, in CVPR’22 (CCFA,JCR-Q1) [35].</li><li>Y. M. Li, J. T. Zhou, X. W. Zheng, J. Y. Tian, and Y. Y. Tang,“Robust Subspace Clustering With Independent and Piecewise IdenticallyDistributed Noise Modeling”, in CVPR’19 (CCF-A, JCR-Q1) [10].</li></ul><h1 id="其他详细信息5">5其他详细信息（5%）</h1><h2 id="实施">5.1实施</h2><h3 id="系统配置">5.1.1系统配置</h3><p>​  我们的模型的实现是基于PyTorch[36]，这是一个被广泛使用的深度学习框架，以其灵活性和效率而闻名。我们的系统在Ubuntu22.04.3LTS上运行，确保开发和部署提供稳定和安全的环境。硬件配置包括800GB的DDR4内存，这为处理大型数据集和复杂的计算提供了足够的空间。此外，该设置利用了16NVIDIA V100和8 NVIDIAH800gpu，每个都配备了32GB的VRAM，以促进并行处理和加速训练时间。这种配置支持广泛的并行化，支持跨多个gpu分配计算任务，以提高性能和可伸缩性。</p><h3 id="数据的扩充和处理">5.1.2数据的扩充和处理</h3><p>​  我们使用来自Albumentations[37]库的图像增强函数来进一步增强我们的数据集。所使用的增强功能包括，但不限于，JPEG和WebP压缩、模糊、高斯噪声、随机亮度和对比度调整、随机伽马校正、音调变化、锐化和网格失真。这些增强被应用于引入可变性，并模拟现实世界伪造品中可能遇到的不同类型的失真和伪影。通过结合这些技术，我们的目标是使我们的模型更加鲁棒，并能够在广泛的条件下检测伪造物。</p><h3 id="超参数和优化设置">5.1.3超参数和优化设置</h3><p>​  我们精心选择了几个关键的超参数来优化模型的性能：<br/>​  <strong>聚类：</strong>无监督聚类算法的聚类数K设置为20。这一聚类步骤对于训练集和验证集的重新分割至关重要，以确保不同类型的伪造特征被有效地分组。<br/>​  <strong>专家模型：</strong>我们在集成中使用了<spanclass="math inline">\(I =7\)</span>个专家模型，每个模型都在通过聚类过程创建的不同折叠上进行训练。专家的这种多样性有助于捕获不同的视角，并提高模型的泛化能力。<br/>​  <strong>损失函数：</strong></p><blockquote><p>交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>：采用0.01的软标签平滑因子，通过放松模型对其预测的置信度来防止过拟合。<br/>&gt;对抗性训练损失<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>：对半径为ϵ=0.4的ℓ∞球内产生扰动的对抗性例子，计算了kl-散度损失。对抗性步长α =0.1用于迭代更新。<br/>&gt; InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>：对此对比损失中使用的余弦相似度函数设置了温度尺度因子τ= 0.1，鼓励相似特征在分离不同特征的同时进行聚类。</p></blockquote><p>​  <strong>指数移动平均线：</strong>EMA的衰减因子γ设置为0.995。这保证了模型权值的有效平滑，减少了训练过程中噪声更新的方差，从而导致更稳定的收敛。<br/>​  <strong>整体损失加权：</strong>总损失函数Ltotal结合了三个主要的损失组成部分：LNCE、LKL和LCE。通过设置加权参数λ1 = 1和λ2 =10来平衡整体优化过程中对抗性训练损失和交叉熵损失的影响。</p><h2 id="人工努力">5.2人工努力</h2><p>​  人工努力主要用于使用SD生成合成图像。这个过程涉及到对超参数进行微调，如学习速率、批处理大小和迭代计数，以及精心制作精确的提示。在这个工作流程中，人工的努力是必要的，因为我们仔细评估生成的图像的质量。它们确保了合成数据是真实的和多样化的，这对于提高模型的检测伪造面孔的鲁棒性和准确性至关重要。</p><h2 id="耗时">5.3耗时</h2><p>​  在第一阶段的数据准备过程中，我们总共投入了30.5个小时。具体来说，大约花费了35分钟和30个小时来生成编辑和SD的增强图像。此外，分配40分钟用于训练聚类模型，13小时用于对训练数据（∼50万个样本）进行特征提取和无监督聚类。在第二阶段的培训过程中，平均要花费12个小时来培训一名专家编码器。在第三阶段的推理过程中，所提出的方法通常需要0.2秒来预测单个512×512图像。</p><h2 id="更大的影响">5.4更大的影响</h2><p>​  深度造假技术的出现对社会和个人都有着深远的影响。深度造假技术利用先进的人工智能技术制作超真实的虚假视频和图像，对隐私、民主和国家安全构成重大威胁。在社会层面上，深度造假可以被用来传播错误信息，操纵公众舆论，并破坏人们对媒体和机构的信任。对个人来说，这些风险包括身份盗窃、名誉损害和因未经授权使用他们的肖像而造成的心理痛苦。因此，发现和减轻深度造假影响的能力对于保护数字媒体的完整性以及保护个人和社会利益至关重要。<br/>​  这一挑战的组织在数字取证领域的推进中发挥着关键作用。这一挑战通过为研究人员提供了一个平台，以便在标准化条件下开发和测试新的检测算法，从而促进了创新。它还有助于创建反映真实世界场景的多样性和复杂性的综合数据集，从而提高检测方法的鲁棒性和通用性。此外，这些挑战提高了人们对深度造假的潜在危险的认识，并促进了学术界、工业界和政府机构之间的合作，以制定有效的对策。<br/>​  展望未来，一种新的多维面部伪造品的引入，预计将进一步增强我们对抗数字操作的能力。这一挑战可能会包括更广泛的伪造技术，不仅包括面部交换，还包括更微妙的变化，如表情变化和老化影响。通过解决这些额外的维度，研究人员可以开发出更复杂的检测模型，能够识别更广泛的操作范围。这将最终有助于建立一个更安全和更值得信赖的数字环境。</p><h1 id="致谢">致谢</h1><p>​  这项工作部分在SICC进行，并由澳门大学SKL-IOTSC支持。</p><h1 id="参考">参考</h1><p>[1] M. Tang and Q. Le, “Efficientnet: Rethinking model scaling forconvolutional neural networks,” in <strong>Proc. Int. Conf. Mach.Learn.</strong>, 2019, pp. 6105– 6114.</p><p>[2] L. V. der Maaten and G. Hinton, “Visualizing data using t-sne,”<strong>Journal of Mach. Learn. Res</strong>, vol. 9, no. 11, 2008.</p><p>[3] Y. Chen, H. Wu, and J. Zhou, “Progressive poisoned data isolationfor training-time backdoor defense,” <strong>Proceedings of the AAAIConference on Artificial Intelligence</strong>, vol. 38, no. 10, pp. 11425–11 433, Mar. 2024. [Online]. Available:https://ojs.aaai.org/index.php/AAAI/article/view/ 29023</p><p>[4] F. Li, K. Li, J. Tian, and J. Zhou, “Regroup median loss forcombating label noise,” <strong>Proceedings of the AAAI Conference onArtificial Intelligence</strong>, vol. 38, no. 12, pp. 13 474–13 482,Mar. 2024. [Online]. Available:https://ojs.aaai.org/index.php/AAAI/article/view/29250</p><p>[5] H. Wu, J. Zhou, X. Zhang, J. Tian, and W. Sun, “Robust cameramodel identification over online social network shared images viamultiscenario learning,” <strong>IEEE Transactions on InformationForensics and Security</strong>, vol. 19, pp. 148–162, 2024.</p><p>[6] J. Liu, J. Zhou, J. Zeng, and J. Tian, “Difattack:Query-efficient black-box adversarial attack via disentangled featurespace,” <strong>Proceedings of the AAAI Conference on ArtificialIntelligence</strong>, vol. 38, no. 4, pp. 3666–3674, Mar. 2024.[Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/28156</p><p>[7] A. Oord, Y. Li, and O. Vinyals, “Representation learning withcontrastive predictive coding,” <strong>arXivpreprint:1807.03748</strong>, 2018.</p><p>[8] R. Muller, S. Kornblith, and G. E. Hinton, “When does labelsmoothing help?” in <strong>Proc. Adv. Neural Inf. Process.Syst.</strong>, 2019, pp. 8748–8763.</p><p>[9] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S.Xie, “A convnet for the 2020s,” <strong>Proc. Comput. Vis. PatternRecogn.</strong>, 2022.</p><p>[10] Y. Li, J. Zhou, X. Zheng, J. Tian, and Y. Y. Tang, “Robustsubspace clustering with independent and piecewise identicallydistributed noise modeling,” in <strong>Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR)</strong>,June 2019.</p><p>[11] T. Zhang, R. Ramakrishnan, and M. Livny, “Birch: an efficientdata clustering method for very large databases,” in <strong>Proceedingsof</strong> <strong>the 1996 ACM SIGMOD International Conference onManagement of Data</strong>, ser. SIGMOD ’96. New York, NY, USA:Association for Computing Machinery, 1996, p. 103–114. [Online].Available: https://doi.org/10.1145/233269.233324</p><p>[12] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-basedalgorithm for discovering clusters in large spatial databases withnoise,” in <strong>Proceedings of the Second International Conference onKnowledge Discovery and Data Mining</strong>, ser. KDD’96. AAAI Press,1996, p. 226–231.</p><p>[13] J. MacQueen <strong>et al.</strong>, “Some methods forclassification and analysis of multivariate observations,” in<strong>Proceedings of the fifth Berkeley symposium on mathematicalstatistics and probability</strong>, vol. 1, no. 14. Oakland, CA, USA,1967, pp. 281–297.</p><p>[14] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk, “Learninglocal feature descriptors with triplets and shallow convolutional neuralnetworks,” in <strong>Proc. British Mach. Vis. Conf.</strong>, 2016, pp.1–11.</p><p>[15] C.-H. Yeh, C.-Y. Hong, Y.-C. Hsu, T.-L. Liu, Y. Chen, and Y.LeCun, “Decoupled contrastive learning,” in <strong>Computer Vision –ECCV 2022</strong>, S. Avidan, G. Brostow, M. Ciss´e, G. M. Farinella,and T. Hassner, Eds. Cham: Springer Nature Switzerland, 2022, pp.668–684.</p><p>[16] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y.Wei, “Circle loss: A unified perspective of pair similarityoptimization,” in <strong>Proc. Comput. Vis. Pattern Recogn.</strong>,2020, pp. 6398–6407.</p><p>[17] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining andharnessing adversarial examples,” in <strong>Proc. Int. Conf. Learn.Representat.</strong>, 2015, pp. 1–11.</p><p>[18] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,“Towards deep learning models resistant to adversarial attacks,” in<strong>Proc. Int. Conf. Learn. Representat.</strong>, 2018.</p><p>[19] N. Carlini and D. Wagner, “Towards evaluating the robustness ofneural networks,” in <strong>2017 IEEE Symposium on Security and Privacy(SP)</strong>, 2017, pp. 39–57.</p><p>[20] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal lossfor dense object detection,” in <strong>Proc. IEEE Int. Conf. Comput.Vis.</strong>, 2017, pp. 2980– 2988.</p><p>[21] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,“Highresolution image synthesis with latent diffusion models,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2022, pp. 10 684–10 695.</p><p>[22] “Stable-diffusion-v2-1,” https://huggingface.co/stabilityai/stable-diffusion-2-1-base.</p><p>[23] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei,“Imagenet: a large-scale hierarchical image database,” in <strong>Proc.Comput. Vis. Pattern</strong> <strong>Recogn.</strong>, 2009, pp.248–255.</p><p>[24] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, andM. Nießner, “Faceforensics++: learning to detect manipulated facialimages,” in <strong>Proc. IEEE Int. Conf. Comput. Vis.</strong>, 2019,pp. 1–11.</p><p>[25] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang,and C. Ferrer, “The deepfake detection challenge (dfdc) dataset,”<strong>arXiv preprint arXiv:2006.07397</strong>, 2020.</p><p>[26] Google ai blog: Contributing data to deepfake detectionresearch. Accessed on: May 5, 2024. [Online]. Available:https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html</p><p>[27] K. Shiohara and T. Yamasaki, “Detecting deepfakes withself-blended images,” in <strong>Proc. IEEE Conf. Comput. Vis. PatternRecogn.</strong>, 2022, pp. 18 720–18 729.</p><p>[28] J. Cao, C. Ma, T. Yao, S. Chen, S. Ding, and X. Yang,“End-to-end reconstruction-classification learning for face forgerydetection,” in <strong>Proc. IEEE Conf. Comput. Vis. PatternRecogn.</strong>, 2022, pp. 4113–4122.</p><p>[29] A. Luo, C. Kong, J. Huang, Y. Hu, X. Kang, and A. C. Kot,“Beyond the prior forgery knowledge: Mining critical clues for generalface forgery detection,” <strong>IEEE Trans. Inf. ForensicsSecur.</strong>, vol. 19, no. 1, pp. 1168–1182, 2023.</p><p>[30] Dfdc 1st place solution. Accessed on: Aug 25, 2024. [Online].Available: https://www.kaggle.com/competitions/deepfake-detection-challenge/discussion/145721</p><p>[31] S. Qi, Y. Zhang, C. Wang, J. Zhou, and X. Cao, “A principleddesign of image representation: Towards forensic tasks,” <strong>IEEETransactions on Pattern Analysis and Machine Intelligence</strong>, vol.45, no. 5, pp. 5337–5354, 2023.</p><p>[32] H. Wu, J. Zhou, J. Tian, J. Liu, and Y. Qiao, “Robust imageforgery detection against transmission over online social networks,”<strong>IEEE Trans. Inf. Forensics Secur.</strong>, vol. 17, no. 1, pp.443–456, 2022.</p><p>[33] Y. Chen, J. Tian, X. Chen, and J. Zhou, “Effective ambiguityattack against passport-based dnn intellectual property protectionschemes through fully connected layer substitution,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2023, pp. 8123–8132.</p><p>[34] B. Song, X. Chen, S. Xu, and J. Zhou, “Under-display cameraimage restoration with scattering effect,” in <strong>Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV)</strong>,October 2023, pp. 12 580–12 589.</p><p>[35] H. Wu, J. Zhou, J. Tian, and J. Liu, “Robust image forgerydetection over online social network shared images,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2022, pp. 13 440–13 449.</p><p>[36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E.Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L.Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style,highperformance deep learning library,” in <strong>Proc. Adv. NeuralInf. Process. Syst.</strong>, 2019.</p><p>[37] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M.Druzhinin, and A. A. Kalinin, “Albumentations: Fast and flexible imageaugmentations,” <strong>Information</strong>, vol. 11, no. 2, 2020.[Online]. Available: https://www.mdpi.com/2078-2489/11/2/125</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust Image Forgery Detection over Online Social Network Shared Images</title>
      <link href="/ImageForensicsOSN/"/>
      <url>/ImageForensicsOSN/</url>
      
        <content type="html"><![CDATA[<center>Robust Image Forgery Detection over Online Social Network Shared Images</center><center>Haiwei Wu, Jiantao Zhou, Jinyu Tian, and Jun Liu</center><center>智慧城市物联网国家重点实验室</center><center>澳门大学计算机与信息科学系</center><center>{yc07912, jtzhou, yb77405, yc07453}<span class="citation"data-cites="um.edu.mo">@um.edu.mo</span></center><h1 id="摘要">摘要</h1><p>​  Photoshop和美图等图像编辑软件的滥用，导致数字图像的真实性受到质疑。与此同时，网络社交网络（OSNs）的广泛使用使其成为传输伪造图像、报道假新闻、传播谣言等的主要渠道。不幸的是，osn所采用的各种有损操作，如压缩和调整大小，给实现鲁棒的图像伪造检测带来了巨大的挑战。为了对抗OSN共享的伪造行为，本文提出了一种新的鲁棒训练方案。我们首先对osn引入的噪声进行了彻底的分析，并将其解耦为两部分，即可预测的噪声和看不见的噪声，它们分别建模。前者模拟了所公开的（已知）osn操作所引入的噪声，而后者的设计不仅完成前一个，而且还考虑了探测器本身的缺陷。然后，我们将建模的噪声合并到一个鲁棒的训练框架中，显著提高了图像伪造检测器的鲁棒性。大量的实验结果验证了该方案与几种最先进的竞争对手相比的优越性。最后，为了促进图像伪造检测的未来发展，我们基于四个现有数据集和三个最流行的osn建立了一个公共伪造数据集。所设计的探测器最近在一次证书伪造检测竞赛中排名第一1。源代码和数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上找到</p><h1 id="i.-引言">I. 引言</h1><p>​  很少有研究明确地解决在普遍存在的OSN平台上针对损耗操作的鲁棒伪造检测的设计。这样的主题非常重要，因为这些有损耗的操作会严重降低检测性能。如图1所示，最先进的算法[42]可以准确地检测到原始伪造文件中的伪造区域；但在处理通过脸书传输的伪造文件时，检测性能会严重下降。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904212142816.png"alt="image-20240904212142816" /><figcaption aria-hidden="true">image-20240904212142816</figcaption></figure><p>​  为了减轻OSN的负面影响，第一个关键问题是分析和建模由OSN有损信道引入的噪声。然而，这是一个相当不同的问题，主要是因为当前的平台没有披露操纵传输图像的过程。虽然[33,34]透露了osn采用的部分操作，但仍有许多未知的操作，例如Facebook中的增强过滤。更重要的是，osn经常调整它们的图像处理管道，这使得建模更具挑战性。<br/>​  针对上述挑战，本文设计了一种鲁棒的图像伪造检测方法，以击败osn中的有损操作。具体来说，为了处理OSN的退化，我们提出了一种噪声建模方案，并将模拟噪声集成到一个鲁棒的训练框架中。我们将OSN噪声解耦为两个组成部分：1)可预测的噪声和2)看不见的噪声。前者被设计用于模拟已知操作（如JPEG压缩）所带来的可预测损失，其建模依赖于带有残差学习的深度神经网络（DNN）和嵌入的可微JPEG层。而后者主要是针对osn所采取的不可知的行动和/或各种osn的训练和测试之间的差异。显然，从信号本身的角度来看，为看不见的噪声建立一个合适的模型是不现实的。为了解决这个困难，我们将我们的观察结果从噪声的角度转移到伪造检测器上，只关注可能导致检测性能下降的噪声。这种策略自然地孵化了一种新的算法，利用对抗噪声[35]的核心思想来建模看不见噪声，它本质上是难以察觉的扰动，会严重降低模型性能。结果表明，我们的鲁棒图像伪造检测方法具有优越的鲁棒性，性能优于几种先进的算法。我们的方案的检测结果的一个例子如图1所示。最后，我们基于四个现有的数据集[1,6,14,17]和三个OSN平台（脸书、微博和微信），构建了一个包含5000多个项目的公共伪造数据集。我们的主要贡献可以总结如下：</p><ul><li>我们提出了一种新的训练方案来对抗osn上传输的鲁棒图像伪造检测。该训练方案不仅对osn引入的可预测噪声进行建模，还结合了看不见噪声进一步提高鲁棒性。</li><li>与几种最先进的方法[12,27,37,42]相比，我们的模型取得了更好的检测性能，特别是在对抗osn传输的场景中。</li><li>我们基于四个现有的数据集[1,6,14,17]和三个平台（脸书、微博和微信）建立了一个公共伪造数据集。</li></ul><p>​  本文的其余部分组织如下。第2部分回顾一下相关的工作。第3部分通过提出的鲁棒训练策略，详细介绍了鲁棒图像伪造检测。实验结果见第4部分和第5部分。</p><h1 id="相关工作">2.相关工作</h1><h2 id="图像伪造检测">2.1.图像伪造检测</h2><p>​  许多取证方法（例如，[2、3、5、7、8、18-23、26、36、39、40]）已经被提出来验证数字图像的真实性。这些方法通过特定的伪影来检测锻造区域，如拼接[18,26]、复制移动[23,39]、中值滤波[8,20]、插入绘画[21,22,36]等。为了更好地适应实际需求，越来越多的方法被开发来解决检测一般（复合）类型的伪造[4,5,11,12,27,37,41,42]的问题，其中基于深度学习的方法是最成功的。沿着这条线，Wu等人[37]提出了一种一般的伪造检测网络MT-Net，该网络首先提取图像处理特征，然后识别异常区域。Mayer和Stamm最近，[27]引入了法医相似性，以确定两个图像补丁是否包含相同的法医痕迹。从相机指纹的角度来看，科佐利诺和维多里瓦·[12]设计了一种提取相机模型指纹的方法，称为噪声指纹，以揭示伪造的区域。为了学习通用伪造的痕迹，Zhuang等人[42]使用了使用ps脚本的训练数据生成策略。</p><h2id="在线社交网络osnonline-social-network">2.2.在线社交网络（OSN，OnlineSocial Network）</h2><p>​  Facebook、Wechat、Weibo等各种OSN平台的普及，大大简化了图片的传播和共享。然而，正如许多现有的作品[33,34]所表明的那样，几乎所有的osn都以一种有损的方式操作上传的图像。这些有损操作所带来的噪声会严重影响法医方法的有效性。以在[32–34]中发现的Facebook为例，这些操作主要包括三个阶段：调整大小、增强过滤和JPEG压缩。具体来说，如果图像的分辨率高于2048像素，则将应用调整大小。然后，对图像中一些选定的块进行高度自适应和复杂的增强滤波。正如在[33,34]中提到的，由于这些增强过滤操作的适应性，要精确地了解它们是非常具有挑战性的。最后，对图像进行一轮JPEG压缩，并根据图像内容自适应地确定一个质量因子（QF）。通过对[33]中提供的数据集的分析，Facebook使用的QF值范围从71到95。尽管在不同的OSN平台上的图像操作是不同的，主流osn进行的操作仍然有许多相似之处（例如，无处不在的JPEG压缩）[33]。<br/>​  一些现有的取证[9,24,38]被设计用来识别所涉及的传输操作。Liao等人[9,24]首先提出了一种基于盲信号分离的两种操作识别的特征解耦方法。为了进一步揭示一个长链，You等人的[38]提出了一个解决方案，通过创新性地将操作链检测表示为一个机器翻译问题。<br/>（[38]J. You, Y. Li, J. Zhou, Z. Hua, W. Sun, and X. Li. A transformer basedapproach for image manipulation chain detection. In <em>ACM Int. Conf.Multimedia</em>, pages 3510–3517. ACM, 2021. 3）</p><h1id="针对通过osn进行传输的鲁棒图像伪造检测">3.针对通过osn进行传输的鲁棒图像伪造检测</h1><p>​  在本节中，我们将详细介绍针对osn上传输的鲁棒图像伪造检测方案。导致成功的关键技术是适当地建模osn导致的退化，并将这些知识集成到一个鲁棒的训练框架中。从第2部分第二小节得知，OSN中的图像处理操作相当复杂；其中一些可以精确地知道，而另一些只能部分已知，甚至完全未知。因此，我们建议将OSN噪声分为两种类型：1)可预测噪声和2)不可见噪声。前一种类型对应于退化源被明确识别的情况。而后一种类型是各种噪声不确定性的组合，包括未知的建模/参数，训练和测试osn之间的差异，甚至是一些完全看不见的退化源。通过在训练阶段添加建模的OSN噪声，检测器有望学习到更广义的特征，能够在OSN传输中存活下来，从而显著提高整体伪造检测性能。<br/>​  在图2中，我们说明了我们的伪造检测鲁棒训练方案的框架，它包括四个阶段。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904213429555.png"alt="image-20240904213429555" /><figcaption aria-hidden="true">image-20240904213429555</figcaption></figure><p>​  粗略地说，阶段1和阶段2是专门通过一个可微的网络来模拟可预测的噪声。第三阶段处理通过对抗性噪声产生策略对看不见噪声的建模。最后，阶段4处理了图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的实际鲁棒训练。请注意，我们的鲁棒训练方案可以与任何基于深度学习的图像伪造检测器相结合。由于这项工作的重点更多的是在鲁棒的训练上，我们在下面将我们的注意力限制在1-3阶段，而留下<spanclass="math inline">\(f_{\theta}\)</span>的细节在第4部分第1小结陈述。<br/>​  形式上，设<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>分别表示可预测噪声和不可见噪声，因此在鲁棒训练阶段考虑的复合噪声变为<span class="math display">\[\delta=\tau+\xi.\]</span>​  对于每次训练迭代，我们首先采样两个原始的3通道（RGB）彩色图像<spanclass="math inline">\(\{\mathbf{p}_1,\mathbf{p}_2\} \in\mathbb{R}^{H\times W\times3}\)</span>和一个二进制掩码<spanclass="math inline">\(\mathbf{y}\in\{0,1\}^{H\times\dot{W}\times1}\)</span>其中，1代表伪造的区域，0代表其他地方。然后一个伪造的图像x可以被合成为<spanclass="math display">\[\mathbf{x}=\mathbf{p}_1\odot(1-\mathbf{y})+\mathbf{p}_2\odot\mathbf{y},\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级别的乘法。在拥有一对伪造的图像和相应的ground-truth掩膜后，我们可以创建一个数据集<spanclass="math inline">\(\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^N\)</span>用于训练，其中，<spanclass="math inline">\(i\)</span>是训练样本的索引。在复合噪声<spanclass="math inline">\(\delta\)</span>下，图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的鲁棒训练可以表述为： <spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\delta})}\Big\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\delta}),\mathbf{y}_{i})\Big\},\]</span>​  式中，<spanclass="math inline">\(P(\boldsymbol{\delta})\)</span>表示复合噪声<spanclass="math inline">\(\delta\)</span>的分布，N为训练样本数，<spanclass="math inline">\(\mathcal{L}_{b}\)</span>为二值交叉熵（BCE）损失。<br/>​  在我们的噪声模型中，我们考虑了一个相当一般的设置，即两个噪声分量<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>是相关的。然后，采用鲁棒训练方案的等式(3)可以进一步写成：<spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\tau})}\Big\{\mathbb{E}_{P(\boldsymbol{\xi}|\boldsymbol{\tau})}\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}+\boldsymbol{\xi}),\mathbf{y}_{i})\}\Big\},\]</span>​  其中，<span class="math inline">\(P(\boldsymbol{\tau})\)</span>为<spanclass="math inline">\(\tau\)</span>的边缘分布，<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>为给定<spanclass="math inline">\(\tau\)</span>的<spanclass="math inline">\(\xi\)</span>的条件分布。从实现的角度来看，在有足够数量的噪声样本时，可以有效和准确地计算出这些期望值。为了进行等式(4)的稳健训练，一个关键的任务是建模边缘分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>和条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。</p><h2 id="建模分布pboldsymboltau">3.1.建模分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span></h2><p>​  我们现在对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模，其中的退化是由OSN平台的有损操作引起的。从第2部分第2小节，我们知道<spanclass="math inline">\(\tau\)</span>的主要衰退源头是应用的JPEG压缩，而后处理（如增强滤波）也部分地影响了<spanclass="math inline">\(\tau\)</span>。对于一个图像<spanclass="math inline">\(\mathbf{x}_i\)</span>和一个固定的OSN平台，所产生的噪声可以很容易地计算出来：<spanclass="math display">\[\tau_i=\mathrm{OSN}(\mathbf{x}_i)-\mathbf{x}_i,\]</span>​  其中函数<spanclass="math inline">\(OSN(\cdot)\)</span>反映了给定OSN平台进行的所有操作。请注意，<spanclass="math inline">\(\tau_i\)</span>依赖于<spanclass="math inline">\(\mathbf{x}_i\)</span>，即噪声是依赖于信号的。通过这种方式，我们似乎可以生成大量的噪声样本，这可以用来模拟<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>的分布。然而，在实践中，这种简单的建模方案是相当有问题的。处理后的图像<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\)</span>必须通过将<spanclass="math inline">\(\mathbf{x}_i\)</span>上传到特定的OSN平台，然后下载来获得。一方面，这种程序很耗时间；另一方面，许多osn平台不允许过多地上传/下载操作。例如，如果在短时间内观察到太多的上传操作，Weibo甚至会禁止该账户。这严重限制了所获得的噪声样本的数量，使得这种幼稚的方案在实践中非常无效。<br/>​  为了解决这一挑战，我们采用了另一种策略，以一种不明确的方式建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。我们提出使用一个替代的深度网络来模拟OSN的操作，以便方便地产生大量的噪声样本<spanclass="math inline">\(\tau_i\)</span>。具体来说，为了与OSN平台上的图像处理管道保持一致，我们训练了一个DNN模型，该模型显式地嵌入了一个可微层来描述JPEG压缩。对于输入图像<spanclass="math inline">\(\mathbf{x}_i\)</span>，我们的目标是学习一个映射<spanclass="math inline">\(g_{\boldsymbol{\phi}} : \mathbb{R}^{d} \to\mathbb{R}^{d}\)</span>，其中，<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>是一个具有可训练参数<spanclass="math inline">\(\phi\)</span>的网络，用于预测了OSN的输出。我们为了<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>使用了U-Net架构[28]，因为它本质上是一个图像到图像的映射。训练过程如图2的第一阶段所示，然后在第二阶段使用训练良好的<spanclass="math inline">\(g_{\boldsymbol{\phi^*}}\)</span>来建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。在训练阶段，我们以离线方式收集输入图像<spanclass="math inline">\(\mathbf{x}_{i}\in\mathbb{R}^{d}\)</span>和OSN传输版本<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\in\mathbb{R}^d\)</span>。训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数可以表示为：<spanclass="math display">\[\min_{\boldsymbol{\phi}}\Big\{\mathcal{L}_r(g_{\boldsymbol{\phi}}(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\},\]</span>​  其中，<spanclass="math inline">\(\mathcal{L}_{r}(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_{2}\)</span>。<br/>​  由于我们更感兴趣的是学习由OSN传输产生的噪声，而不是图像内容本身，我们在设计<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>时采用了一个残差学习结构[16]。考虑到这一点，我们将目标函数更改为：<spanclass="math display">\[\min_\phi\Big\{\mathcal{L}_r(\mathbf{x}_i+g_\phi(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\}.\]</span>​  残差学习有利于模型的优化，显著提高了建模性能。<br/>​  此外，我们明确地将一个特殊的JPEG层集成到模型中，以便更好地生成结构性的、类似于JPEG的工件，这反映了各种OSN平台中的真实情况。实现了等式(7)中目标函数的端到端优化，我们需要确保JPEG压缩的每一步都是可微的。很容易发现量化是唯一不可微的步骤，主要是因为所采用的舍入函数<spanclass="math inline">\(\left\lfloor\cdot\right\rceil\)</span>到处都有0的导数。为了处理它，我们用可微版本[31]来近似舍入函数：<span class="math display">\[\lfloor x\rceil_a=\lfloorx\rceil+(x-\lfloor x\rceil)^3.\]</span>​  一旦有了一个可微的JPEG层，训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数就变成了：<spanclass="math display">\[\min_\phi\mathcal{L}_r(\mathcal{J}_q(\mathbf{x}_i+g_\phi(\mathbf{x}_i)),\mathrm{OSN}(\mathbf{x}_i)),\]</span>​  其中，<span class="math inline">\(\mathcal{J}_q\)</span>表示具有给定QFq的可微JPEG层。在我们的训练中，q在Facebook采用的[71,95]范围内均匀采样。然后就可以直接推导出噪声<spanclass="math inline">\({\tau}_i\)</span>为 <spanclass="math display">\[\tau_i(q)=\mathcal{J}_q(\mathbf{x}_i+g_{\boldsymbol{\phi}^*}(\mathbf{x}_i))-\mathbf{x}_i,\]</span>​  其中，通过求解优化问题等式(9)得到<spanclass="math inline">\(\phi^{*}\)</span>，q是与JPEG压缩相关联的QF。然后实现蒙特卡罗（MC，MonteCarlo）采样方案，生成大量的噪声样本，用于对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模。</p><h2 id="建模条件分布pboldsymbolxiboldsymboltau">3.2.建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span></h2><p>​  然后，我们解决条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>的建模问题，从而可以解决等式(4)中的优化问题。我们加入噪声术语<spanclass="math inline">\(\xi\)</span>的原因是，可预测的噪声<spanclass="math inline">\(\tau\)</span>不能完全捕获在实践中遇到的噪声行为。例如，不同的osn可能采用不同的过程，如，动态调整QF，自适应地调整大小，甚至引入完全未知的操作。<br/>​  现在的一个关键问题是如何为看不见的噪声<spanclass="math inline">\(\xi\)</span>建立一个适当的模型。显然，就像我们在第3部分第1小节中所做的那样，从信号本身的特征来建模<spanclass="math inline">\(\xi\)</span>是不现实的。为了解决这一挑战，我们通过研究噪声对检测性能的影响，将我们的位置从噪声方面转移到检测器<spanclass="math inline">\(f_{\theta}\)</span>方面。在各种潜在的不可见噪声<spanclass="math inline">\(\xi\)</span>中，我们实际上只需要注意那些降低检测性能的噪声，而忽略了那些对检测影响很小的噪声。这促使我们在建模<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>时使用一种对抗性噪声[35]。从本质上讲，对抗性噪声通常是人类感官难以感知，但同时能够引起严重的模型输出错误。与此同时，我们所关注的看不见的噪声<spanclass="math inline">\(\xi\)</span>是一个能够欺骗探测器的噪声，而且通常也很小（一个高度失真的图像会偏离伪造的目的）。这种与检测器<spanclass="math inline">\(f_{\theta}\)</span>效果的相似性使得对抗性噪声成为建模<spanclass="math inline">\(\xi\)</span>的合适候选噪声。<br/>​  从对抗性的角度来看，有各种方法来定义噪声<spanclass="math inline">\(\xi\)</span>，只要通过添加噪声<spanclass="math inline">\(\xi\)</span>来创建的对抗性例子，就会跨越决策边界。注意到噪声<spanclass="math inline">\(\xi\)</span>通常振幅较小，我们提出沿着相对于输入代价函数的梯度设置<spanclass="math inline">\(\xi\)</span>的方向，以使噪声能量最小化。因此，对于给定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>、可预测噪声<spanclass="math inline">\({\tau}_i\)</span>和目标输出<spanclass="math inline">\(\mathbf{y}_i\)</span>，将不可见噪声<spanclass="math inline">\(\xi_i\)</span>表示为 <spanclass="math display">\[\boldsymbol{\xi}_{i}=\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\(\mathcal{S}\)</span>返回梯度的符号。通过在训练过程中加入这些对抗性噪声，期望使学习到的模型不仅对特定的对抗性噪声，而且对更一般的不可见噪声具有鲁棒性。<br/>​  然而，由等式（11）计算出的噪声依赖于特定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>，而不是适用于训练集中所有示例和未知示例的一般输入。为了全面提高检测器的泛化能力，我们提出将对抗性噪声的方向调整为一个全局梯度方向。为此，我们采用了一种类似于随机梯度下降（SGD，StochasticGradientDescend）[30]的策略，通过从训练数据集的随机子集中随机选择的随机近似方法。更具体地说，对于第<spanclass="math inline">\((t+1)\)</span>个输入<spanclass="math inline">\(\mathbf{x}_{t+1}\)</span>，<spanclass="math inline">\({\xi}_{t+1}\)</span>（<spanclass="math inline">\(\tau\)</span>条件）可以设置为从第一个t输入计算出的平均梯度，即：<spanclass="math display">\[\xi_{t+1}=\frac{1}{t}\sum_{i=0}^{t}\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}+\boldsymbol{\xi}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\({\xi}_{0}\)</span>被初始化为0。虽然等式（12）可以用来估计平均梯度，但是它只反映了特定的已知数据（训练数据）的梯度。为了缓解上述问题，进一步提高鲁棒性，我们提出在小范围内扰动<spanclass="math inline">\({\xi}_{t+1}\)</span>。在这里，使用参数模型来描述平均梯度会更理想。为了找到一个合适的平均梯度模型，我们首先采用数据驱动的方法，分析从训练过程中随机选择的1000个<spanclass="math inline">\(\xi\)</span>的样本的统计数据。在图3中，我们使用t-SNE[13]在一个二维空间中可视化了这些1000个随机样本。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904222741508.png"alt="image-20240904222741508" /><figcaption aria-hidden="true">image-20240904222741508</figcaption></figure><p>​  可以看出，样本点集中在某一中心周围，当它们离开该中心时逐渐消失。这一现象建议我们使用高斯分布来建模平均梯度，即：<spanclass="math display">\[\xi_{t+1}|\tau\sim\mathcal{N}(\boldsymbol{u}_{t+1},\sigma^2\mathbf{I}),\]</span>​  其中，<spanclass="math inline">\(\sigma\)</span>是一个控制方差的经验集参数， <spanclass="math display">\[\boldsymbol{u}_{t+1}=\epsilon\cdot\frac1t\sum_{i=0}^t\mathcal{S}(\nabla_{\mathbf{x}_i}\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_i+\boldsymbol{\xi}_i),\mathbf{y}_i)),\]</span>​  而<spanclass="math inline">\(\epsilon\)</span>是一个用于约束扰动大小的参数，以避免不必要的模型退化。<br/>​  在等式(13)中使用参数化模型后，我们可以很容易地生成噪声样本来建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。因此等式(4)可以扩展为<spanclass="math display">\[\min_{\boldsymbol{\theta}}\sum_{i=1}^N\sum_{j=1}^m\sum_{k=1}^h\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_j+\boldsymbol{\xi}_k),\mathbf{y}_i),\]</span>​  其中，关于的期望分别近似于m和hMC样本。有了这个可计算的损失函数，我们就能够执行鲁棒训练，如算法1所示。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904223247902.png"alt="image-20240904223247902" /><figcaption aria-hidden="true">image-20240904223247902</figcaption></figure><h1 id="实验结果">4.实验结果</h1><p>​  在本节中，我们给出了实验结果，以证明我们所提出的方法的优越的性能。由于空间的限制，在补充文件中给出了更多的结果。</p><h2 id="实验设置">4.1.实验设置</h2><h3 id="基线检测器">基线检测器</h3><p>​ 该检测器的目标是在像素级的精度上检测伪造区域。具体来说，检测器<spanclass="math inline">\(f_{\boldsymbol{\theta}}: \mathbb{R}^{H\timesW\times3}\to\mathbb{R}^{H\timesW\times1}\)</span>以分辨率为H×W的彩色图像作为输入，最终输出检测结果的二进制图。在我们的设置中，我们在基线检测器中采用了U-Net[28]架构。为了提高提取伪造相关特征的能力，我们通过合并空间信道“Squeeze-and-Excitation（SE）”机制[29]进一步增强了架构，产生了一个称为SE-U-Net的变体，而不是简单地使用传统的普通U-Net。</p><h3 id="训练验证数据集">训练/验证数据集</h3><p>​  对于OSN网络<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的训练，我们采用了数据集<spanclass="math inline">\(\mathbf{WEI}\)</span>（记为<spanclass="math inline">\(\mathcal{D}_{1}\)</span>）[33]，该数据集包含了1300多张原始图像及其处理后的版本。需要注意的是，我们只使用来自Facebook的数据来培训<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>。而对于<spanclass="math inline">\(f_{\boldsymbol{\theta}}\)</span>的训练，我们使用<spanclass="math inline">\(\mathbf{Dresden}\)</span>[15]数据集作为原始图像的来源。然后，我们通过将原始图像与来自<spanclass="math inline">\(\mathbf{MS-COCO}\)</span>[25]数据集中的对象拼接来生成伪造的图像。这些伪造图像的数据集记为<spanclass="math inline">\(\mathcal{D}_{2}\)</span>。<spanclass="math inline">\(\mathcal{D}_{1}\)</span>和<spanclass="math inline">\(\mathcal{D}_{2}\)</span>随机分为训练集和验证集，比例为9：1。</p><h3 id="测试数据集">测试数据集</h3><p>​  我们通过采用四种广泛使用的数据集（<spanclass="math inline">\(\mathbf{DSO}\)</span>[6]，<spanclass="math inline">\(\mathbf{Columbia}\)</span>[17]，<spanclass="math inline">\(\mathbf{NIST}\)</span>[1]和<spanclass="math inline">\(\mathbf{CASIA}\)</span>[14]）创建测试数据集，并生成它们的OSN传输版本。更具体地说，我们通过三个最流行的OSNs（Facebook、Wechat和Weibo）手动上传和下载上述数据集，得到了5232个伪造的数据集和相应的掩码。这些收集到的数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上获得我们希望这些数据集可以作为我们的研究社区的有用的基准，以对抗在osn上共享的伪造品。</p><h3 id="对比网络">对比网络</h3><p>​  我们将我们提出的方案与四种最先进的方法进行了比较：<spanclass="math inline">\(\mathbf{MT-Net}\)</span> [37]、<spanclass="math inline">\(\mathbf{NoiPri}\)</span> [12]、<spanclass="math inline">\(\mathbf{ForSim}\)</span> [27]和<spanclass="math inline">\(\mathbf{DFCN}\)</span> [42]。</p><h2 id="定量比较">4.2.定量比较</h2><p>​  在像素域上的AUC、F1和IoU（越高越好）的定量比较见表1。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904224250921.png"alt="image-20240904224250921" /><figcaption aria-hidden="true">image-20240904224250921</figcaption></figure><p>表1.以AUC、F1、IoU为标准的定量比较。对于同一OSN传输中的每一列，最高值为粗体，“-”表示不应用。</p><p>​  在这里，我们还报告了基线检测器的结果，以一种比较的方式证明了我们的鲁棒训练方案的改进。可以观察到，当伪造图片不通过OSN传输时，ForSim[27]、DFCN [42]和我们的检测方法获得了类似的结果，而MT-Net [37]和NoiPri[12]的表现略差。需要注意的是，由于NoiPri的分辨率小，不能用于检测CASIA中的伪造，而我们的方法没有这样的限制，在CASIA上比其他竞争对手表现更好。<br/>​  在伪造图片通过osn的情况下，所有现有方法的检测性能都显著下降。例如，在通过Facebook、Weibo和Wechat传播后，与没有OSN传输的情况相比，与MT-Net相关的IoU得分分别下降了10.1%、11.1%和9.4%，相比之下，由于<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>的适当的噪声建模，我们提出的方法对OSN传输表现出相当理想的鲁棒性，并且仍然能导致准确的伪造检测。以Facebook为例，IoU的降幅仅为0.9%。我们也可以注意到，Weibo和Wechat的伪造检测性能下降略大，分别下降了2.0%和4.5%。这主要是因为，与Facebook相比，Weibo和Wechat对上传的图片采用了更严格的压缩，导致了更多的证据丢失。另外，为了训练我们的方法，我们只使用Facebook数据，根本没有任何Weibo和Wechat数据。从表1，我们可以看到使用Facebook数据训练的方案可以很好地推广到Weibo和Wechat传输的伪造上。</p><h2 id="定性比较">4.3.定性比较</h2><p>​  除了定量比较外，图4还给出了两个具有代表性的例子（更多结果见补充文件）。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225133906.png"alt="image-20240904225133906" /><figcaption aria-hidden="true">image-20240904225133906</figcaption></figure><p>图4.检测OSN传输伪造品的定性比较。对于每一行，从左到右的图像都是伪造（输入）、ground-truth、由MT-Net[37]、NoiPri [12]、ForSim [27]、DFCN[42]和我们生成的检测结果（输出）。从上到下的伪造品分别是没有OSN传输，以及有Facebook、Weibo和Wechat传输的案例。</p><p>​  可以看出，在正常情况下（没有OSN传输），现有的检测方法表现得相对较好，例如，第一种情况下的MT-Net和ForSim，以及第二种情况下的NoiPri和DFCN。然而，这些方法在OSN传输版本的情况下并不能达到令人满意的检测性能。以第二种情况下的NoiPri为例。对于Facebook、Weibo和Wechat传输的图像，识别出的伪造区域分布在多个物体上，使得伪造检测效果降低。相比之下，我们提出的方法可以学习更鲁棒的伪造特征，从而在这些具有挑战性的情况下产生更精确的检测结果，这主要归功于复合噪声建模的鲁棒训练方案。</p><h2 id="消融研究">4.4.消融研究</h2><p>​  我们现在通过分析每个建模噪声（即可预测噪声<spanclass="math inline">\(\tau\)</span>和不可见噪声<spanclass="math inline">\(\xi\)</span>）如何对最终检测性能的贡献，对我们提出的训练方案进行消融研究。为此，我们首先禁止在方案中使用每个噪声，然后在适当的设置下评估不同的再训练检测器的性能。所得结果见表2。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225545236.png"alt="image-20240904225545236" /><figcaption aria-hidden="true">image-20240904225545236</figcaption></figure><p>​  可以看出，在检测器（#2行）训练中引入可预测的噪声<spanclass="math inline">\(\tau\)</span>可以略微提高检测性能（F1增益1.2%），这在Facebook传输中更明显（F1增益4.6%）。然而，由于只采用<spanclass="math inline">\(\tau\)</span>是不完整的，如在第3部分第2小节中提到的，我们进一步加入设计的不可见噪声<spanclass="math inline">\(\xi\)</span>。第3行的结果表明，<spanclass="math inline">\(\xi\)</span>可以有效地提高检测器的鲁棒性，带来更显著的改进（例如，F1增加8.6%）。最后，第4行证明，当同时应用复合噪声<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>时，检测器对目标环境的稳定性更强，这对于OSN传输上的伪造检测任务至关重要（例如，F1的增益为15.7%）。此外，我们没有只使用SE-U-Net作为检测器，而是采用了另一个著名的架构，DPN[10]，以展示我们提出的训练方案的多功能性。如第5行和第6行所示，我们的鲁棒性训练方法也可以很好地增强DPN的鲁棒性。</p><h2 id="一些进一步的鲁棒性评估">4.5.一些进一步的鲁棒性评估</h2><p>​  虽然该方案主要是为了对抗osn进行的有损操作，但我们也希望评估其在一些更常用的退化场景下的鲁棒性，如噪声添加、裁剪、调整大小、模糊和独立的JPEG压缩。这种评估在现实情况中是非常重要的，因为这些类型的后处理操作经常被用来清除或隐藏伪造的伪影。为此，我们将这些后处理操作应用于原始测试集Columbia，并在图5中报告了定量比较。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904230059643.png"alt="image-20240904230059643" /><figcaption aria-hidden="true">image-20240904230059643</figcaption></figure><p>​  为了便于演示，我们使用一个统一的参数p来控制不同操作的大小。横轴的原点（p=0）对应于没有任何后处理的情况。可以观察到，对比网络[12,27,37,42]不能随着扰动强度的增加而表现一致，而我们的方法可以很好地推广到击败这些后处理操作。</p><h1 id="结论">5.结论</h1><p>​  在本文中，我们提出了一种新的训练方案来提高图像伪造检测对各种基于OSN的传输的鲁棒性。该方案的设计借助于建模一个可预测的噪声<spanclass="math inline">\(\tau\)</span>以及一个有意引入的看不见的噪声<spanclass="math inline">\(\xi\)</span>。实验结果表明，我们的方案与几种最先进的方法相比具有优越性。此外，我们为未来的法医研究社区建立了一个osn传输的伪造数据集。</p><h1 id="致谢">致谢</h1><p>​  澳门科技发展基金2021-2021-2023-0072/2020/20200/015/2019/AMJ，060/2019/A1和077/2012，澳门大学研究委员会2018-00029-FST和MYRG2019-00023-FST，中国自然科学基金61971476，阿里巴巴集团通过阿里巴巴创新研究计划。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>科研工具</title>
      <link href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/"/>
      <url>/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<h1id="t-snet-distributedstochasticneighborembedding">1.t-SNE(t-DistributedStochasticNeighborEmbedding)</h1><p>​  t-SNE是一种用于探索高维数据结构的非线性降维技术。它特别适用于高维数据的可视化，因为它能够在低维空间中保留原始高维数据的局部结构。由于这个特性，t-SNE在机器学习和数据分析领域越来越受到重视。</p><h2 id="算法解读">1算法解读：</h2><p>​  t-SNE的核心思想是在高维空间中为数据点之间定义一种<ahref="https://zhida.zhihu.com/search?q=概率分布&amp;zhida_source=entity&amp;is_preview=1">概率分布</a>，表示点与点之间的相似性，然后在低维空间中创建一个相似的概率分布。通过最小化这两个分布之间的差异（使用KL散度），算法将高维数据映射到低维空间，以便我们可以可视化。</p><h2 id="步骤和细节">2步骤和细节：</h2><h3 id="step1.计算高维空间中的相似度">Step1.计算高维空间中的相似度</h3><p>​  我们使用高斯分布（正态分布）来计算点之间的相似性。高斯分布是一种常见的概率分布，其形状呈钟型，由均值和方差（标准差的平方）决定。高斯分布有一个很好的性质：它的形状由均值（中心点）和方差（分布的宽度）决定。当我们围绕一个数据点x画一个高斯分布时，这个分布会给予附近的点较高的概率值，而离得远的点则会有较低的概率值。这与我们直觉上对“相似性”的理解相一致：靠近的点更相似，远离的点不相似。</p><p>​  对于每个数据点<spanclass="math inline">\(x_i\)</span>，我们计算所有其他点<spanclass="math inline">\(x_j\)</span>与其的条件概率<spanclass="math inline">\(p_{j|i}\)</span>。这个概率反映了点<spanclass="math inline">\(x_j\)</span>是点<spanclass="math inline">\(x_i\)</span>的近邻的可能性。计算公式为: <spanclass="math display">\[p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\neqi}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}\]</span> ​  这里，分子部分计算了<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>之间的欧氏距离的平方（即<spanclass="math inline">\(-\|x_i-x_j\|^2\)</span>），然后通过高斯分布转换成概率。分母部分是一个归一化因子，确保所有<spanclass="math inline">\(p_{j|i}\)</span>的和为1。<br/>​  <spanclass="math inline">\(\sigma_{i}\)</span>是高斯分布的方差，决定了近邻的范围。不同的点可能有不同的密度，因此<spanclass="math inline">\(\sigma_{i}\)</span>对于每个点<spanclass="math inline">\(x_i\)</span>可能是不同的，需要通过一种叫做“困惑度”的量来确定。<br/>​  最后，为了得到一个对称的相似度矩阵，我们取<spanclass="math inline">\(p_{j|i}\)</span>和<spanclass="math inline">\(p_{i|j}\)</span>的平均值得到<spanclass="math inline">\(p_{ij}\)</span>: <spanclass="math display">\[p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}\]</span>​  这样，我们就得到了一个对称的相似度矩阵，其中的每个元素<spanclass="math inline">\(p_{ij}\)</span>都反映了数据点<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>在高维空间中的相似性。通过这一步，我们成功地量化了高维空间中数据点之间的相似性，为后续的低维空间嵌入奠定了基础。</p><h3 id="step2.初始化低维空间的点">Step2.初始化低维空间的点</h3><p>​  这一步可以是随机初始化，只需保证初始化点的数量和原数据相同，维度更低即可。</p><h3id="step3.计算低维空间的点的相似度">Step3.计算低维空间的点的相似度</h3><p>​  在t-SNE算法中，高维空间的相似度是通过高斯（正态）分布计算的，而低维空间的相似度是通过t分布（具体来说是自由度为1的t分布，也叫做柯西分布）计算的。这种设计的目的是为了解决“拥挤问题”。<br/>​  当我们将高维空间中的数据点降维到低维空间时，数据点之间的距离会发生变化。特别是在低维空间中，点与点之间可用的空间更少，容易出现拥挤的情况。如果直接使用高斯分布来计算低维空间的相似度，那么低维空间中远离的点之间的相似度可能会被过高地估计，导致降维结果的可视化效果不佳。<br/>​  t分布（自由度为1）有一个重要的特性：它的尾部比高斯分布更“厚”（heavy-tailed）。这意味着，在低维空间中，即使两个点距离较远，它们之间的相似度（通过t分布计算）也不会迅速减小到0。这有助于缓解拥挤问题，因为低维空间中远离的点之间的相似度会被较低地估计。<br/>​  在低维空间中，我们计算点<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的相似度<spanclass="math inline">\(q_{ij}\)</span>如下: <spanclass="math display">\[q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\neql}(1+\|y_k-y_l\|^2)^{-1}}\]</span>​  这个公式来源于自由度为1的t分布。分子部分计算了<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的欧氏距离的平方，并转换成了概率。分母部分是一个归一化因子，确保所有的<spanclass="math inline">\(q_{ij}\)</span>之和为1。通过这种方式，我们得到了低维空间中点之间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}。接下来，t-SNE算法会试图使高维空间的相似度矩阵{<spanclass="math inline">\(p_{ij}\)</span>}和低维空间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}尽可能地一致，从而得到合适的低维空间表示。</p><h3 id="step4.优化低维空间的点的位置">Step4.优化低维空间的点的位置</h3><p>​  通过最小化Kullback-Leibler散度(KL散度)来优化低维空间中的点的位置。KL散度用于衡量高维空间和低维空间中的相似度分布之间的差异。<span class="math display">\[C=\sum_{i\neqj}p_{ij}\log\frac{p_{ij}}{q_{ij}}\]</span>​  使用梯度下降方法来最小化KL散度，更新低维空间中的点的位置。 <spanclass="math display">\[\frac{\delta C}{\deltay_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+\|y_i-y_j\|^2)^{-1}\]</span>​  在梯度下降的计算中，输入是低维空间中每个点的坐标{<spanclass="math inline">\(y_j\)</span>}。这些坐标是我们要优化的参数。输出是低维空间中点与点之间的相似度{<spanclass="math inline">\(q_{ij}\)</span>}。这些相似度是由当前的低维坐标{<spanclass="math inline">\(y_j\)</span>}计算出来的。标签是高维空间中点与点之间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}。这些相似度是已知的，因为它们是由原始高维数据计算得出的。我们的目标是通过调整低维空间中的点的坐标{<spanclass="math inline">\(y_j\)</span>}（即输入），使得由这些坐标计算出的相似度{<spanclass="math inline">\(q_{ij}\)</span>}（即输出）尽可能接近已知的高维空间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}（即标签）。<br/>​  为了实现这个目标，我们计算损失函数（即KL散度）相对于每个低维坐标的梯度，并使用这个梯度来更新低维坐标。这个过程会重复进行，直到达到预定的迭代次数，或者低维坐标的变化小于某个阈值。</p><h2 id="代码实现">3. 代码实现</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 假设 features 是你的特征向量，形状为 (256, 256, 64)</span><br><span class="line">features = np.random.rand(256, 256, 64)</span><br><span class="line"></span><br><span class="line"># 假设 mask 是你的掩码数组，形状为 (256, 256)，由0和1组成</span><br><span class="line">mask = np.random.randint(0, 2, size=(256, 256))</span><br><span class="line"></span><br><span class="line"># 重塑特征向量为 (65536, 64)</span><br><span class="line">reshaped_features = features.reshape(-1, 64)  # 65536 = 256 * 256</span><br><span class="line"></span><br><span class="line"># 将掩码展平为1D数组</span><br><span class="line">flattened_mask = mask.flatten()</span><br><span class="line"></span><br><span class="line"># 对特征向量应用 t-SNE</span><br><span class="line">tsne = TSNE(n_components=2, random_state=42)</span><br><span class="line">features_2d = tsne.fit_transform(reshaped_features)</span><br><span class="line"></span><br><span class="line"># 根据掩码的值来选择颜色</span><br><span class="line">colors = np.where(flattened_mask == 1, &#x27;red&#x27;, &#x27;blue&#x27;)</span><br><span class="line"></span><br><span class="line"># 可视化点云，不同的掩码值对应不同的颜色</span><br><span class="line">plt.figure(figsize=(10, 10))</span><br><span class="line">plt.scatter(features_2d[:, 0], features_2d[:, 1], c=colors, s=1)</span><br><span class="line">plt.title(&#x27;t-SNE Visualization of Features with Mask Colors&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="js散度jensen-shannon-divergence">2. JS散度(Jensen-Shannondivergence)</h1><h2 id="kl散度kullback-leibler-divergence定义">1)KL散度(Kullback-Leibler divergence)定义</h2><ul><li>两个概率分布(probability distribution)间差异的非对称性度量；</li><li>参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。</li><li>KL散度（Kullback-Leibler divergence），也称为相对熵（relativeentropy），是用来衡量两个概率分布之间差异的一种指标。在机器学习中，KL散度常常用于度量两个概率分布之间的相似度或差异性。</li></ul><p><span class="math display">\[KL[P(X)||Q(X)]=\sum_{x\inX}[P(x)log\frac{P(x)}{Q(x)}]=E_{x\simP(x)}[log\frac{P(x)}{Q(x)}]\]</span></p><p>​  在这种情况下，KL散度衡量了教师模型的输出分布q和学生模型的输出分布p之间的差异。通过最小化KL散度损失，学生模型被鼓励从教师模型中学习，并产生相似的输出分布。</p><p>​  此外，KL散度还经常用于变分自编码器（VAEs）中。VAEs是一种生成模型，它们学习数据的低维表示，可以用于生成新样本。在VAEs中，KL散度被用来鼓励学习到的潜在变量遵循先验分布，例如标准正态分布。这有助于正则化模型并防止过拟合。<br/>​  聚类：KL散度可以用于聚类，以度量两个聚类之间的差异。在这种情况下，KL散度可以用于评估聚类质量，并指导聚类算法的优化过程。</p><p>​  在上面的概率拟合应用场景下， <spanclass="math inline">\(KL[P||Q]\)</span> 也被称为前向KL散度（forwardKullback-Leibler Divergence），将 <spanclass="math inline">\(KL[Q||P]\)</span> 称为反向KL散度（reverseKullback-LeiblerDivergence）。<br/>​  这里需要注意的是，只有在概率拟合的应用场景下（也就是确定了真实分布和拟合分布两个角色之后），前向KL散度<span class="math inline">\(KL[P||Q]\)</span>和反向KL散度 <spanclass="math inline">\(KL[Q||P]\)</span>的定义才是有意义的，否则二者只是相同公式改变正负号、并交换P和 Q符号表示之后的平凡结果。</p><details close><br/><summary>具体情况</summary><ol type="1"><li>标准正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span></li></ol><p>​  正态分布 <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span>的概率密度函数为: <spanclass="math display">\[p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\]</span>​  标准正态分布 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span>的概率密度函数为: <spanclass="math display">\[q(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\]</span>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu,\sigma^{2})\|\mathcal{N}(0,1))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}}{\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}\log\biggl\{\frac{1}{\sqrt{\sigma^{2}}}\mathrm{exp}\biggl\{\frac{1}{2}\bigl[x^{2}-(x-\mu)^{2}/\sigma^{2}\bigr]\biggr\}\biggr\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx \\&amp;=\frac{1}{2}\intp(x)[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx\end{aligned}\]</span></p><p>​  整个结果分为三项积分，第一项实际上就是<spanclass="math inline">\(-log \sigma ^2\)</span> 乘以概率密度的积分（也就是1），所以结果是 <span class="math inline">\(-log \sigma ^2\)</span>；第二项实际是正态分布的二阶矩，熟悉正态分布的朋友应该都清楚正态分布的二阶矩为<span class="math inline">\(\mu^{2}+\sigma^{2}\)</span>；而根据定义，第三项实际上就是“-方差除以方差=-1”。所以总结果就是: <spanclass="math display">\[KL(\mathcal{N}(\mu,\sigma^2)\|\mathcal{N}(0,1))=\frac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)\]</span></p><ol start="2" type="1"><li>正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu_1,\sigma_1^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(\mu_2,\sigma_2^{2})\)</span></li></ol><p>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu_{1},\sigma_{1}^{2})\|\mathcal{N}(\mu_{2},\sigma_{2}^{2}))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}}{\frac{1}{\sqrt{2\pi\sigma_{2}^{2}}}e^{-(x-\mu_{2})^{2}/2\sigma_{2}^{2}}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}\log\{\frac{\sqrt{\sigma_{2}^{2}}}{\sqrt{\sigma_{1}^{2}}}\mathrm{exp}\{\frac{1}{2}[\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]\}\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\\&amp;=\frac{1}{2}\intp(x)[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\end{aligned}\]</span>​  整个结果分为四项积分，第一项实际上就是 <span class="math inline">\(log\sigma_2 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是<spanclass="math inline">\(log \sigma_2 ^2\)</span>；第二项实际上就是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>；第三项实际是异正态分布的二阶矩，熟悉正态分布的朋友应该都清楚异正态分布的二阶矩为$ $ ；而根据定义，第四项实际上就是“-方差除以方差=-1”。所以总结果就是:<spanclass="math display">\[KL(\mathcal{N}(\mu_1,\sigma_1^2)\|\mathcal{N}(\mu_2,\sigma_2^2))=\frac{1}{2}(\log\sigma_2^2-\log\sigma_1^2+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-1)\]</span></p></details><h2 id="两类kl散度">2) 两类KL散度</h2><p>​  考虑到需要用人工设计的近似分布 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 来拟合真实分布 <spanclass="math inline">\(P(X)\)</span> ，这里下标 <spanclass="math inline">\(\theta\)</span> 强调 <spanclass="math inline">\(Q\)</span> 是一个受到参数 <spanclass="math inline">\(\theta\)</span> 控制的分布。<br/>例如： <spanclass="math inline">\(Q\)</span> 是正态分布<spanclass="math inline">\(N(\mu,\sigma^{2})\)</span>， <spanclass="math inline">\(P\)</span> 是正态分布 <spanclass="math inline">\(N(\mu_0,\sigma_0^{2})\)</span> ，现在希望用 <spanclass="math inline">\(Q\)</span> 来拟合 <spanclass="math inline">\(P\)</span> ，其中<spanclass="math inline">\(Q\)</span> 的均值和方差 <spanclass="math inline">\(\{\mu,\sigma^{2}\}\)</span>就是拟合过程中可以调整的参数<span class="math inline">\(\theta\)</span>。于是基于前向KL和反向KL代价的分布拟合问题分别转化为以下两个优化问题：</p><ul><li>命题1. 极小化前向KL：<span class="math inline">\(\arg min_\thetaKL(P||Q_\theta)\)</span> 等价于对参数 <spanclass="math inline">\(\theta\)</span> 的极大似然估计。</li><li>命题2. 极小化反向KL： <span class="math inline">\(\argmin_{\theta}KL(Q_{\theta}||P)\)</span> 相当于在要求 <spanclass="math inline">\(Q_{\theta}\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</li></ul><p>​  首先，证明命题一，过程如下： <spanclass="math display">\[\begin{aligned}&amp;arg min_{\theta} KL (P||Q)\\&amp;=arg min_{\theta}(E_{X\sim P}[-log Q_{\theta}(X)])+H(P(X))\\&amp;=arg min_{\theta} E_{X\sim P}[-log Q_{\theta}(X)] \\&amp;=argmax_{\theta} E_{X\sim P} [log Q_{\theta} (X)] \\&amp;\approx argmax_{\theta} E_{X\sim P_{data}} [log Q_{\theta}(X)]\end{aligned}\]</span> ​  其中 <spanclass="math inline">\(H(P(X))=-\sum_{x}[P(x)logP(x)]\)</span>，代表信息熵（Entropy）。上述推导的最终结果正好就是极大似然代价的定义式。<br/>​  推导过程分析：上面的推导过程中，第2行到第3行利用了<span class="math inline">\(H(P(X))\)</span> 是与优化自变量 <spanclass="math inline">\(\theta\)</span>无关的，故删除该项不会改变最优化问题的解，因此可以直接省略。第3行到第4行则是通过来将求最小值问题转化为求最大值问题消去负号。第4行到第5行利用了机器学习训练中一般假设特征在样本集上的分布可以被近似看作真实分布，即：<span class="math inline">\(P_{data} (X)\approx P(X)\)</span> 。</p><p>　　综上命题1成立。<br/>  其次，证明命题2，推导如下： <spanclass="math display">\[arg min_{\theta} KL(Q||P)=argmin_{\theta}(E_{X\sim Q_{\theta}}[-log P(X)]+H(Q_{\theta}(X)))\]</span>​  观察上面的等式右侧 <span class="math inline">\(argmin_{\theta}\)</span> 中的两项： <span class="math display">\[E_{X\simQ_{\theta}}[-log P(X)]+H(Q_{\theta}(X))\]</span>​  要想令上面两项之和最小，就意味着要找到参数<spanclass="math inline">\(\theta\)</span>的一个合适的取值，使得上面两项中的每一项 <spanclass="math inline">\(E_{X\sim Q_{\theta}}[-log P(X)])\)</span> 和 <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>都尽可能小。根据熵的性质可知，当 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 越接近于均匀分布，第二项<span class="math inline">\(H(Q_{\theta}(X)\)</span>的值越大，反之当<span class="math inline">\(Q_{\theta}(X)\)</span>越去向于单一模态分布（可以通俗理解为单峰分布） <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>的值越小。因此反向KL散度相当于在要求<span class="math inline">\(Q_{\theta}(X)\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</p><h2 id="js散度jensen-shannon-divergence-1">3) JS散度(Jensen-Shannondivergence)</h2><p>​  <strong>JS 散度</strong>（Jensen-Shannon Divergence，缩写JSD）是基于 KL散度（相对熵）的一种统计学度量，能够衡量两个概率分布之间的差异程度。<br/>​  设概率空间上有两个概率分布<span class="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>，<spanclass="math inline">\(M=\frac12(P+Q)\)</span>，为<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>的平均，则，<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span> 的JS散度定义为 <spanclass="math display">\[JSD(P||Q)=\frac12D_{KL}(P||M)+\frac12D_{KL}(Q||M),\]</span> ​  其中，<span class="math inline">\(D_{KL}\)</span>表示KL散度。</p><p>​  KL散度的缺点：不是距离、不对称。因此引进JS散度的概念，其取值是0到1之间。由定义可以看出，JS散度是对称的，可以用于衡量两种不同分布之间的差异。JS散度用于生成对抗网络的数学推导上。</p><h1 id="kmeans">3. kmeans++</h1><p>​  K-means++算法是对传统的K-means聚类算法的一种改进，它解决了K-means算法的一个主要缺点——对初始聚类中心选择的敏感性。K-means++通过一种更合理的方式来选择初始聚类中心，使得算法更有可能找到全局最优解，而不是陷入局部最优。</p><p>K-means++算法流程：</p><ol type="1"><li><p>随机选择第一个聚类中心：从数据集中随机选择一个样本点作为第一个聚类中心K-means++算法<span class="math inline">\(c_1\)</span>。</p></li><li><p>计算距离并选择剩余的聚类中心：<br/> - 对于数据集中的每个点<spanclass="math inline">\(x_2\)</span>，计算它到已选聚类中心的最短距离<spanclass="math inline">\(D(x_i)\)</span>。<br/> - 选择下一个聚类中心<spanclass="math inline">\(c_j\)</span>的概率与 <spanclass="math inline">\(D(x_i)^2\)</span>成正比。这意味着那些距离现有聚类中心较远的点有更大的机会被选为新的聚类中心。</p></li><li><p>重复上述过程：</p><ul><li>直到选择了所有k。</li></ul></li></ol><p>公式：</p><p>​  在K-means++中，对于每一个未被选作聚类中心的点<spanclass="math inline">\(x\)</span>，计算其到最近的聚类中心的距离<spanclass="math inline">\(D(x)\)</span>，定义为： <spanclass="math display">\[D(x)=\min_{j\in[1,k]}||x-c_{j}||\]</span>​  这里<spanclass="math inline">\(||\cdot||\)</span>表示向量的范数，通常是欧几里得距离：</p><p><spanclass="math display">\[||x-c_j||=\sqrt{\sum_{d=1}^D(x_d-c_{jd})^2}\]</span>​  其中 <span class="math inline">\(x_d\)</span>和<spanclass="math inline">\(c_{jd}\)</span>分别是点 <spanclass="math inline">\(x\)</span> 和聚类中心<spanclass="math inline">\(c_j\)</span>在第<spanclass="math inline">\(d\)</span>维的坐标值，<spanclass="math inline">\(D\)</span>是数据的维度。<br/>​  概率选择新聚类中心：<br/>​  选择下一个聚类中心的概率与距离平方成正比：<span class="math display">\[P(c_{j}=x)\proptoD(x)^{2}\\P=\frac{D(x)^{2}}{\sum_{x\in X}D(x)^{2}}\]</span> ​  其中<spanclass="math inline">\(D(x)^2\)</span>代表单个点到<spanclass="math inline">\(x\)</span>其最近的已选质心（centroid）的距离的平方。这里的<spanclass="math inline">\(D(x)\)</span>是点<spanclass="math inline">\(x\)</span>到最近的聚类中心的距离。<spanclass="math inline">\(\sum_{x\in X}D(x)^{2}\)</span>是指对数据集中<spanclass="math inline">\(X\)</span>的所有点<spanclass="math inline">\(x\)</span>的距离平方<spanclass="math inline">\(D(x)^2\)</span>。<br/>​  这意味着，如果一个点<spanclass="math inline">\(x\)</span>到最近的聚类中心的距离较大，那么它被选为下一个聚类中心的概率也较高。<br/>​  通过这种方式，K-means++算法倾向于在数据集的不同区域选择聚类中心，从而避免了K-means算法可能产生的偏斜聚类中心的问题，提高了算法的稳定性和聚类质量。</p><h1 id="rpn">4. RPN</h1><p>​  区域建议网络（RPN，Region ProposalNetwork）是Faster-RCNN网络用于提取预选框。<br/>在 Faster R-CNN中，区域候选网络 (RPN)每一步的特征大小会随着网络的不同层逐渐减小，下面用具体的特征大小显示 RPN的各个步骤。假设输入图像大小为 <span class="math inline">\(H \times W\times 3\)</span>)，以常见的 VGG16 作为 Backbone 为例：</p><h2 id="输入图像-image">1. 输入图像 (Image)</h2><ul><li><strong>大小</strong>: <span class="math inline">\(H \times W \times3\)</span><br/> - H和 W是输入图像的高度和宽度，3 表示 RGB 通道。</li></ul><h2 id="特征提取网络-backbone-e.g.-vgg16">2. 特征提取网络 (Backbone,e.g., VGG16)</h2><ul><li>VGG16 的卷积层会将输入图像下采样 16 倍(stride=16)，输出的特征图大小为：</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 512\)</span><br/> - 输出的特征图具有 512个通道。</li></ul><h2 id="滑动窗口生成-anchors">3. 滑动窗口生成 Anchors</h2><ul><li>RPN 在特征图的每个像素上生成多个锚框 (anchors)，典型的 anchor 尺寸有3 种尺度和 3 种长宽比，所以每个像素点会生成 9 个 anchor。</li><li><strong>大小</strong>: $ $<br/> - 9 是每个像素位置生成的 anchor个数。</li></ul><h2 id="rpn-分类-前景背景">4. RPN 分类 (前景/背景)</h2><ul><li>RPN 对每个 anchor 进行二分类（目标/非目标），输出一个分类分数（2个通道）。</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 18\)</span><br/> - 18 是 9个 anchor的前景和背景类别概率。</li></ul><h2 id="边界框回归-修正-anchor-坐标">5. 边界框回归 (修正 Anchor坐标)</h2><ul><li>RPN 还会预测每个 anchor 的修正参数，输出的是 4 个参数(中心点坐标和宽高)。</li><li><strong>大小</strong>: <span class="math inline">\(\frac{H}{16}\times \frac{W}{16} \times 36\)</span><br/> - 36 是 9 个 anchor 每个anchor 对应的 4 个回归参数 (<span class="math inline">\(9 \times 4 =36\)</span>)。</li></ul><h2 id="候选区域-rois">6. 候选区域 (RoIs)</h2><ul><li>根据 RPN 分类的输出，对正样本 anchor 进行边界框回归后得到候选区域(Region of Interest, RoIs)。</li><li>候选区域数目通常为预定数目（如 2000 个），在 Non-Maximum Suppression(NMS) 之后保留最佳的 RoIs。</li><li><strong>大小</strong>: 约 <span class="math inline">\(2000 \times4\)</span><br/> - 每个候选区域用 4 个值表示（x, y, w, h)）。</li></ul><h2 id="非极大值抑制-nms">7. 非极大值抑制 (NMS)</h2><ul><li>对生成的候选区域进行NMS，去除冗余的候选框，只保留高置信度的框。非极大值抑制 (NMS,Non-Maximum Suppression)是在区域候选网络（RPN）以及物体检测任务中常用的一种后处理算法，目的是去除冗余的候选框，只保留置信度较高且位置相对不重叠的框。以下是NMS 的主要步骤及其原理：</li></ul><p>NMS 处理步骤：</p><ol type="1"><li><p><strong>输入候选框 (RoIs)</strong>:<br/> - 经过 RPN或目标检测器输出的一系列候选框 (boundingboxes)，每个框都有一个置信度分数（表示是否包含物体的概率）。</p></li><li><p><strong>按置信度排序</strong>:<br/> -将所有候选框按其置信度分数从高到低排序，优先处理高置信度的候选框。</p></li><li><p><strong>选择最高置信度的框</strong>:<br/> -选择置信度最高的框作为基准框，将其加入最终保留的框列表。</p></li><li><p><strong>计算重叠区域（IoU）</strong>:<br/> -对其他候选框，计算它们与当前基准框的交并比 (IoU, Intersection overUnion)，即两个框的重叠区域与总区域的比值。IoU 的公式为：<br/> <spanclass="math inline">\(\text{IoU} =\frac{\text{交集区域}}{\text{并集区域}}\)</span><br/> - 如果 IoU大于一个预定的阈值（例如 0.7），则认为两个框重叠过多，视为冗余框。<br/><br/>5. <strong>删除重叠过多的框</strong>:<br/> - 删除与基准框 IoU大于阈值的框，因为这些框基本上在表示同一个物体。</p></li><li><p><strong>重复步骤</strong>:<br/> -从剩下的框中选择置信度次高的框作为新的基准框，并重复上述步骤，直到所有的候选框都处理完毕。</p></li><li><p><strong>输出结果</strong>:<br/> - 最后保留下来的框是通过 NMS过滤掉重叠候选框后的结果。它们代表了高置信度且位置不重叠的候选区域。</p></li></ol><p>关键点：</p><ul><li><strong>IoU 阈值</strong>：决定了候选框之间的重叠程度。如果 IoU大于这个阈值，候选框就会被认为过于相似，冗余框会被删除。</li><li><strong>置信度分数</strong>：用来排序候选框，确保优先保留最有可能包含物体的框。</li><li><strong>NMS的输出</strong>：是去除了重叠候选框的最终区域，通常用于进一步的分类和边界框精修(refinement)。</li></ul><p>图示化流程</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[输入候选区域 RoIs] </span><br><span class="line">    ↓</span><br><span class="line">[按置信度排序] </span><br><span class="line">    ↓</span><br><span class="line">[选择置信度最高的框] </span><br><span class="line">    ↓</span><br><span class="line">[计算 IoU] </span><br><span class="line">    ↓</span><br><span class="line">[删除重叠大的候选框] </span><br><span class="line">    ↓</span><br><span class="line">[重复以上步骤直到处理完所有框] </span><br><span class="line">    ↓</span><br><span class="line">[输出最终候选框]</span><br></pre></td></tr></table></figure><p>NMS 是 RPN中的重要步骤之一，保证了从大量冗余候选区域中提取高质量的、没有过多重叠的区域用于后续物体检测和分类。</p><h2 id="输出候选区域-用于后续-r-cnn">8. 输出候选区域 (用于后续R-CNN)</h2><ul><li><strong>大小</strong>: 约 <span class="math inline">\(N \times4\)</span><br/> - N是通过 NMS 保留下来的 RoI 数量（例如 300 个），4是每个候选框的坐标。</li></ul><p>流程图带上特征大小：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[输入图像 HxWx3] </span><br><span class="line">    ↓</span><br><span class="line">[特征提取网络 VGG16]</span><br><span class="line">    ↓</span><br><span class="line">[特征图 H/16 x W/16 x 512] </span><br><span class="line">    ↓</span><br><span class="line">[生成 Anchor H/16 x W/16 x 9] </span><br><span class="line">    ↓</span><br><span class="line">[RPN 分类 H/16 x W/16 x 18]</span><br><span class="line">    ↓</span><br><span class="line">[边界框回归 H/16 x W/16 x 36] </span><br><span class="line">    ↓</span><br><span class="line">[候选区域 (RoIs) 2000x4] </span><br><span class="line">    ↓</span><br><span class="line">[非极大值抑制 (NMS)]</span><br><span class="line">    ↓</span><br><span class="line">[输出 RoIs Nx4]</span><br></pre></td></tr></table></figure><p>这个流程描述了 RPN的特征大小变化，Anchors、分类和回归分别对应不同大小的特征。</p><h2 id="rpn-网络代码示例">9. RPN 网络代码示例</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class RPN(nn.Module):</span><br><span class="line">    def __init__(self, in_channels, num_anchors):</span><br><span class="line">        super(RPN, self).__init__()</span><br><span class="line">        # 3x3 卷积层用于提取特征</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, padding=1)</span><br><span class="line">        </span><br><span class="line">        # 分类分支 (前景/背景)</span><br><span class="line">        self.cls_score = nn.Conv2d(512, num_anchors * 2, kernel_size=1)</span><br><span class="line">        </span><br><span class="line">        # 回归分支 (边界框回归)</span><br><span class="line">        self.bbox_pred = nn.Conv2d(512, num_anchors * 4, kernel_size=1)</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 特征提取</span><br><span class="line">        x = F.relu(self.conv(x))</span><br><span class="line">        </span><br><span class="line">        # 分类分支输出</span><br><span class="line">        cls_score = self.cls_score(x)</span><br><span class="line">        </span><br><span class="line">        # 边界框回归分支输出</span><br><span class="line">        bbox_pred = self.bbox_pred(x)</span><br><span class="line">        </span><br><span class="line">        return cls_score, bbox_pred</span><br><span class="line"></span><br><span class="line"># 测试示例</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    # 假设输入特征图的通道数为 256，锚点数为 9</span><br><span class="line">    rpn = RPN(in_channels=256, num_anchors=9)</span><br><span class="line">    </span><br><span class="line">    # 模拟输入特征图的大小 (batch_size, channels, height, width)</span><br><span class="line">    input_tensor = torch.randn(1, 256, 64, 64)</span><br><span class="line">    </span><br><span class="line">    cls_score, bbox_pred = rpn(input_tensor)</span><br><span class="line">    </span><br><span class="line">    print(&quot;Classification score shape:&quot;, cls_score.shape)</span><br><span class="line">    print(&quot;Bounding box prediction shape:&quot;, bbox_pred.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</title>
      <link href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/"/>
      <url>/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/</url>
      
        <content type="html"><![CDATA[<center>Robust Camera Model Identification Over Online Social Network SharedImages via Multi-Scenario Learning <ahref="https://ieeexplore.ieee.org/abstract/document/10262083"><imgsrc="https://img.shields.io/badge/TIFS-2023-orange" alt="TIFS" /></a></center><center>Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE,Xinyu Zhang ,</center><center>Jinyu Tian , Member, IEEE, and Weiwei Sun</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  相机模型识别（CMI，Camera modelidentification）可广泛应用于图像取证的真实性鉴定、版权保护、伪造检测等领域。同时，随着互联网的蓬勃发展，在线社交网络（OSNs，online socialnetworks）已成为图像共享和传输的主导渠道。然而，在OSNs上不可避免的损耗操作，如压缩和后处理，给现有的CMI方案带来了巨大的挑战，因为它们严重破坏了被调查图像中留下的相机痕迹。在这项工作中，我们提出了一种新的CMI方法，它对各种OSN平台的有损操作具有鲁棒性。具体来说，可以观察到，一个相机跟踪提取器可以很容易地训练在一个单一的退化场景（例如，一个特定的OSN平台）；而在混合退化场景中（例如，多个OSN平台）要困难得多。受此启发，我们设计了一种新的多场景学习（MSL，multi-scenariolearning）策略，使我们能够在不同的osn中提取鲁棒的摄像机痕迹。此外，注意到图像平滑区域由OSN引起的失真更少，而图像信号本身的干扰更小，我们提出了一种平滑感知的痕迹提取器（STATE，SmooThness-AwareTraceExtractor），它可以根据输入图像的平滑度自适应地提取相机痕迹。通过与四种先进方法的比较实验，验证了该方法的优越性，特别是在各种OSN传输场景下。特别是在开放集摄像机模型验证任务中，我们在FODB数据集上的AUC大大超过第二名15.30%；而在闭集相机模型分类任务中，我们在SIHDR数据集的F1中显著领先第二名34.51%。我们所提出的方法的代码可在https://github.com/HighwayWu/CameraTraceOSN上找到。</p><p>​  稿件于2022年11月24日收到；分别于2023年8月4日和2023年9月18日修订；2023年9月18日接受。出版日期为2023年9月25日；当前版本的日期为2023年11月20日。澳门科技发展基金2021-2023、0072/2020/AMJ、0022/2022/2/A1、0014/2022/AFJ；部分由澳门大学研究委员会MYRG2020-00101-FST和MYRG2022-00152-FST；中国自然科学基金61971476；部分由阿里巴巴集团通过阿里巴巴创新研究项目。协调审查这份手稿并批准其出版的副主编是Dr.Benedetta Tondi。（通讯作者：Jiantao Zhou）<br/>​  Haiwei Wu、 JiantaoZhou、XinyuZhang就职于智慧城市物联网国家重点实验室和澳门大学科技部计算机与信息科学系，中国澳门999078（电子邮件：yc07912@umac.mo；jtzhou@umac.mo；mc14958@umac.mo）。<br/>​  JinyuTian就职于澳门科技大学创新工程学院，中国澳门999078（电子邮件：jytian@must.edu.mo）。<br/>​  WeiweiSun在阿里巴巴集团工作，位于中国，杭州311100（电子邮件：sunweiwei.sww@alibaba-inc.com）。<br/>​  数字对象标识符10.1109/TIFS.2023.3318968<br/>​  <strong>索引术语：</strong>相机模型识别，在线社交网络，深度神经网络，鲁棒性。</p><h1 id="引言">1. 引言</h1><p>​  每个相机模型在捕获的图像上产生独特的模式噪声。这种模式噪声，也称为摄像机痕迹，主要是由传感器对光子的不同响应和/或内部图像信号处理（ISP）管道产生[1]、[2]、[3]。传统最常用的两种相机迹线提取方法分别是光响应不均匀性（PRNU，photo response non-uniformity）[1]和固定模式噪声（FPN， the fixedpatternnoise）[4]。最近的一些努力也尝试整合PRNU和卷积神经网络（CNN）来提取摄像机的轨迹，以获得更好的分辨性能[5]，[6]。摄像机跟踪的高可鉴别性使其广泛应用于相机模型识别（CMI，camera modelidentification）[7]、伪造检测[5]、传输分类[8]、[9]等各种取证任务。<br/>​  CMI的最新性能是通过基于学习的方法[6]、[10]、[13]来实现的，这些方法通常由两个子网络组成，即提取器和分类器。在训练阶段，给定一个训练图像及其相关的相机标签，提取器的目标是提取摄像机轨迹（高维特征向量），而分类器通过评估提取的轨迹是否可以分类为正确的标签来监督提取器的训练。在测试阶段，分类器通常被丢弃，训练良好的提取器可以用来提取任何输入图像的摄像机轨迹。关于基于通用学习的CMI的更多讨论将见图4和第三节。<br/>​  在实际情况下，被调查的图像通常不是直接来自相机，而是经过了一系列的处理。例如，现在许多图像从各种在线社交网络（OSNs，online socialnetworks）中获得，这不可避免地应用了许多类型的有损操作，如JPEG压缩、缩放和后处理[14]、[15]、[16]。这些操作可能会干扰现有的相机痕迹提取算法，导致它们的性能严重下降。图1给出了一个说明例子，显示了在FODB[12]数据集上的相机模型验证任务的性能。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919223213469.png"alt="image-20240919223213469" /><figcaption aria-hidden="true">image-20240919223213469</figcaption></figure><p>图1。与Kuzin [10]、NoiPri [5]、ForSim [11]和PCN[6]等最先进的方案相比，我们提出的方法在开放集验证任务上的性能。需要注意的是，测试数据集FODB[12]和所考虑的OSN平台（Twitter、WeChat、QQ、Telegram和Dingding）在训练阶段都是未知的，模拟了实际情况。详情请参见第五节。</p><p>​  可以观察到，最先进的方法[5]，[6]，[10]，[11]在无传输（NoTrans）中可以实现近90%的AUC，但考虑到OSN传输，所有现有算法的性能都会严重下降，尤其是WeChat、QQ、Telegram和Dingding。<br/>​  为了减轻OSN的负面影响，在本工作中，我们提出了一种新的相机轨迹提取方法，该方法有望对各种OSN平台的传输具有鲁棒性。为简单起见，我们主要关注单轮OSN传播病例，之后我们也展示了多轮传播的一些结果。需要注意的是，我们更感兴趣的是设计一个统一的提取器，能够针对不同类型的OSN传输提取强大的相机痕迹；而不是准备一系列提取器，每个提取器对应一种特定类型的OSN传输。我们提出的鲁棒CMI方法主要受到两个重要观察结果的启发。</p><h2 id="观察1">1)观察1</h2><p>​  在图2中，我们展示了在所谓的单一和混合OSN场景的训练过程中，提取器的损失曲线。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919223511229.png"alt="image-20240919223511229" /><figcaption aria-hidden="true">image-20240919223511229</figcaption></figure><p>图2。观察1：在VISION[17]数据集上的摄像机分类任务中的训练损失。紫色的线表示使用NoTrans（原始）图像进行的训练，而红、绿和蓝线分别对应于单一场景（Facebook和Whatsapp）和混合场景。此外，我们提出的MSL被标记为橙色线。</p><p>​  这里，“Single”表示训练数据只包含一个特定的OSN传输场景，如NoTrans、Facebook或Whatsapp，分别对应于图2中的紫色、绿色和红线。相比之下，“Mixed”指的是训练数据包含多个OSN传输场景的情况，如混合上述三种场景，如蓝线所示。可以清楚地看出，混合情景的损失曲线远远高于每个单一情景。这意味着，在混合场景中，提取器比在单一场景中更难以提取鲁棒的轨迹。因此，我们推测不同OSN传输场景下摄像机轨迹的特征空间更有可能不重叠，这使得寻找局部最优的训练过程更加困难。换句话说，一个对一个给定的OSN传输场景很鲁棒的特定摄像机痕迹可能不适用于其他场景。<br/>​  因此，为了使摄像机跟踪提取器能够更好地从混合场景中学习共享表示，我们提出了多场景学习（MSL，multi-scenario learning），通过逐个场景的方式策略性地训练提取器。与传统的基于学习的CMI方法不同，我们使用了N个（N个&gt;1）分类器，对应于训练数据中的N个场景。请注意，这N个分类器具有未共享的权值，从而放宽了每个OSN传输场景的分类器在单个分类器情况下必须相同的约束条件。具体来说，每个N个分类器都监督一个特定的OSN传输场景的训练数据。例如，第一个分类器监督NoTrans的数据场景中，第二个分类器监督Facebook场景的数据，等等。与传统的仅使用一个分类器监督提取器[6]、[10]训练的方法相比，MSL策略可以有效地减轻对不同OSN传输场景数据训练的干扰。<br/>​  我们采用N个分类器的MSL的另一个原因是训练复杂性不同。例如，从NoTrans场景中提取相机痕迹的任务显然比从Facebook场景中更容易。与只有一个分类器的传统方法相比，使用不同的分类器分离这些场景有利于学习不同的OSN传输场景之间的共享表示。然而，不平衡的训练复杂性问题可能导致逆向过程中不同场景的训练方向不一致，从而导致梯度冲突[18]。目前已经提出了一些通过梯度归一化[19]或梯度下降[20]来缓解梯度冲突的算法。然而，这些算法是基于单一的逆向过程设计的，这可能不能充分描述不同场景之间的冲突，特别是当场景相似时。因此，为了彻底描述和过滤不同场景之间的冲突，我们进一步提出在MSL中引入一个动量掩蔽操作，该操作通过积累历史上的逆向过程来生成过滤器掩蔽。<br/>​  我们提出的MSL的效果如图2中的橙色曲线所示，明显优于传统混合场景训练所对应的蓝色曲线。</p><h2 id="观察2">2)观察2：</h2><p>​  第二个观察结果是，图像中的平滑区域通常比纹理区域遭受更少的OSN处理。如图3所示，天空的失真相对于树木的要低，这表明在平滑区域的更多的摄像机轨迹可以在传输中存活下来。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240919224312157.png"alt="image-20240919224312157" /><figcaption aria-hidden="true">image-20240919224312157</figcaption></figure><p>​  这种现象是合理的，因为纹理区域包含更多的高频信号，而在OSN[21]中，通过压缩和缩放等操作通常会被丢弃。另一方面，现有的作品[11]，[22]证实了在纹理区域中的相机痕迹被复杂的信号本身所掩盖。这一现象表明，痕迹提取器应该更加关注图像的平滑区域。为此，Guera等人通过训练CNN估计全局可靠性图，选择符合条件的区域，而Mayer和Stamm[11]利用固定的熵阈值来过滤合适的图像补丁。然而，通过像[22]这样的一个单独的CNN来估计可靠性图是无效的，并且被[11]丢弃的补丁仍然可以携带关于相机痕迹的有用信息。在本研究中，我们提出了基于交叉注意机制的平滑度感知轨迹提取器（STATE，SmooThness-Aware TraceExtractor），以便更灵活、更有效地从不同区域提取相机轨迹。<br/>​  正如预期的和将被实验验证，我们提出的基于上述观察设计的方法显示了令人满意的鲁棒性，并显著超过了最先进的算法，特别是在各种OSN传输场景中。如图1所示，我们的方法不仅在NoTrans场景中超越了现有的算法，而且在各种OSN平台上的传输上也取得了很大的性能提高。<br/>​  我们的主要贡献如下：</p><ul><li>据我们所知，是我们首次将MSL策略用于CMI，并证明了该策略对OSN传输具有令人满意的鲁棒性。</li><li>我们提出了STATE根据区域平滑度，灵活有效地学习相机的痕迹。</li><li>与最先进的方法[5]，[6]，[10]，[11]方法相比，我们的方法获得了更好的鲁棒性性能，特别是在OSN传输的场景中。</li><li>我们基于现有的相机数据集FODB [12]和SIHDR[23]，构建了9个流行的osn（Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat）的OSN传输数据集，不仅可以评估CMI算法的鲁棒性，而且有利于不同的取证应用。</li></ul><h1 id="相关工作">2. 相关工作</h1><h2id="a.照相机型号识别cmicamera-model-identification">A.照相机型号识别（CMI，CameraModel Identification）</h2><p>​  已经提出了许多方法[1]，[5]，[6]，[10]，[11]，[13]，[24]，[25]，[26]，[27]，[28]，[29]，[30]来表征数字图像中包含的相机痕迹。这些方法大致可以分为传统的类型和基于学习的类型。具体来说，几种传统的方法通过对在图像采集过程中进行的操作进行建模来提取相机的轨迹。最著名的方法之一是PRNU[1]，它建模由传感器缺陷引入的噪声模式。与具有乘法噪声模型的PRNU不同，Thai等人开发了一个广义噪声模型，涉及更多的图像处理操作，如原始像素之间的线性关系、伽马校正的非线性效应等。同样，通过估计相机内必备过程的参数，如彩色插值和彩色滤镜阵列，斯瓦米纳坦等[25]设计了非逼逼组件取证。博尼蒂尼等人。[26]后来分析了不同的JPEG特征算法，并表明JPEG压缩留下的痕迹可以用于CMI。<br/>​  与手工设计特征的繁琐过程相比，随着cnn的快速发展，许多基于学习的CMI算法最近被提出。CMI的开创性深度学习方案是由Bondi等人[13]设计的，其算法超过了利用手工特征[27]，[28]。通过引入细粒度的标签，[30]等人提出了一种在均匀区域内进行层次分类的相机识别算法。受PRNU的启发，CozzolinoandVerdoliva[5]利用孪生子网络，通过同时增强相机伪影和抑制高频场景内容来提取相机噪声表示。利用PRNU[1]算法预提取摄像机模型噪声，Mandelli等[6]提出了一种快速高效的成对相关网络，以更好地分析大型数据库的实用性。Mayer和Stamm[11]没有明确地描述相机的轨迹，而是引入了一种可学习的算法来测量两个图像斑块的相似性，以确定两幅图像是否来自同一个相机模型。<br/>​  需要注意的是，现有的CMI算法[5]、[6]、[10]、[11]在被调查的图像进行一些损耗操作时，仍然会出现严重的性能下降，特别是在实际的OSN传输场景中。因此，开发鲁棒的CMI方法至关重要，促进其在许多多媒体取证任务中的实际部署。</p><h2 id="b.-在线社交网络osnonline-social-network">B.在线社交网络（OSN，Online Social Network）</h2><p>​  在线社交网络大大简化了多媒体数据的传输。如今，OSNs[31]有近37.8亿日活跃用户，超过32亿张图片在日常[32]上被共享。Facebook、YouTube、Whatsapp、Instagram和WeChat是目前最受欢迎的五大社交平台，[33]的月活跃用户分别为29亿、22亿、20亿、20亿和12亿。然而，osn并不是法医友好的平台，因为它们不可避免地进行的有损操作可能会严重影响许多类型的法医算法[34]。例如，Castiglione等人的[35]证实，OSN引入的噪声会降低PRNU[1]的识别能力。因此，许多法医方案努力提高其对OSN[14]、[15]、[36]、[37]、[38]、[39]的负面影响的鲁棒性。</p><h1 id="照相机模型识别的基线方案">3. 照相机模型识别的基线方案</h1><p>​  在深入研究CMI的鲁棒设计之前，我们首先介绍了基于学习的基线方案的架构，该方案包括两个网络，即提取器和分类器，如图4所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902202942944.png"alt="image-20240902202942944" /><figcaption aria-hidden="true">image-20240902202942944</figcaption></figure><p>​  与现有的[6]、[10]、[13]方案类似，提取器的目的是提取摄像机的痕迹，而分类器则监督提取器的训练。在我们的基线方案中，提取器的特定体系结构采用了EfficientNet-b0。该选择是基于一个初步的实验，比较了不同候选架构的相机痕迹提取性能，包括ResNet[40]、VGG [41]、XceptionNet[42]、EfficientNet[43]、ViT[44]和SwinTransformer[45]等。对于分类器架构，我们简单地将其设计为线性层和SoftMax变换的组合。<br/>​  一旦确定了提取器和分类器网络的体系结构，另一个关键问题是如何在测试阶段训练和使用它们。在图4中，我们说明了基线CMI方案的训练和测试过程。在训练阶段，给定一个由Y个不同相机模型捕获的图像组成的数据集<spanclass="math inline">\(\cal{D}\)</span>，<spanclass="math inline">\((\mathbf{X},y)\)</span>表示一对训练数据，其中<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{H\times W\timesC}\)</span>为输入图像，<spanclass="math inline">\(y\in\{1,2,\cdots,Y\}\)</span>为相机的标签。具有可训练参数<spanclass="math inline">\(\theta\)</span>的提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>旨在提取能够表征相机痕迹的高级特征<spanclass="math inline">\(\mathbf{T}\)</span>，<spanclass="math inline">\(\mathbf{T}=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X})\)</span>。为了监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>将<spanclass="math inline">\(\mathbf{T}\)</span>转换为<spanclass="math inline">\(\mathbf{\hat{y}}=\mathcal{C}_{\phi}(\mathbf{T})\)</span>来计算损失<spanclass="math inline">\(\ell(\mathbf{\hat{y}},y)\)</span>，其中<spanclass="math inline">\(\ell\)</span>是广泛使用的交叉熵损失，即： <spanclass="math display">\[\ell(\hat{\mathbf{y}},y)=-\sum_{i=1}^Y\mathcal{I}[y=i]\cdot\log(\hat{\mathbf{y}}^{&lt;i&gt;}).\]</span>​  这里<spanclass="math inline">\(\mathbf{\hat{y}}^{&lt;i&gt;}\)</span>表示<spanclass="math inline">\(\mathbf{\hat{y}}\)</span>的第<spanclass="math inline">\(i\)</span>个条目，<spanclass="math inline">\(\mathcal{I}[y=i]\)</span>是一个二进制指标函数，如果是y= i，则取1，否则取0。<br/>​  经过训练，训练良好的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>可以根据[6]用于两个测试用例：1)开集验证，目的是推断两个测试图像是否被同一相机模型捕获；2)闭集分类，目的是在有限的相机模型池中识别测试图像的源相机模型。<br/>​  请注意，在这两种情况下，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>都被丢弃了。具体来说，在前一种情况下，给定两个图像<spanclass="math inline">\(\mathbf{X_1}\)</span>和<spanclass="math inline">\(\mathbf{X_2}\)</span>，它们的痕迹<spanclass="math inline">\(\mathbf{T_1}\)</span>和<spanclass="math inline">\(\mathbf{T_2}\)</span>首先由训练过的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取。然后，通过余弦相似度计算这些痕迹被同一相机拍摄的概率：<spanclass="math display">\[S(\mathbf{T}_1,\mathbf{T}_2)=\cos\Big(\frac{\mathbf{T}_1}{||\mathbf{T}_1||},\frac{\mathbf{T}_2}{||\mathbf{T}_2||}\Big).\]</span>​  对于闭集分类案例的测试过程，给出一组已知标签的<spanclass="math inline">\(y_i\in\{1,2,\cdots,Y\}\)</span>的图像，<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>首先提取它们的痕迹<spanclass="math inline">\(\mathbf{T_i}\)</span>。然后，平均相同相机模型的痕迹，可以形成一个痕迹池<spanclass="math inline">\(\{\mathbf{\bar{T}}_{i}\}_{i=1}^{Y}\)</span>。换句话说，池中的每个元素代表一个特定相机模型的平均痕迹。当一个测试图像<spanclass="math inline">\(\mathbf{X_t}\)</span>出现时，其预测的相机类型<spanclass="math inline">\(y_t\)</span>可以从痕迹池中搜索最大的相似性，即：<spanclass="math display">\[y_t=\underset{i}{\mathrm{argmax}}S(\mathbf{T}_t,\bar{\mathbf{T}}_i)\]</span>​  其中，<span class="math inline">\(\mathbf{T_t}\)</span>为通过<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的测试图像的痕迹。<br/>​  虽然我们的基线CMI可以用于提取摄像机痕迹，并最终在上述两个测试用例中被采用，但在有损传输上的性能，如各种OSN传输场景，可能会严重降低。这些场景所引入的扭曲很可能会破坏相机的痕迹，这在本质上是脆弱的。在表I中，我们简要展示了FODB[12]数据集上不同的osn造成的畸变，包括平均分辨率和文件大小的减少，以及平均采用的JPEG质量因子（QFs）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902205601762.png"alt="image-20240902205601762" /><figcaption aria-hidden="true">image-20240902205601762</figcaption></figure><p>由FODB[12]数据集上不同的OSNS造成的失真。在这里，“SCALE”和“SIZE”分别表示分辨率减少和文件大小减少的百分比。另外，“JPEGQF”表示qf值的平均值。</p><p>​  可以看出，最严重的畸变是由Dingding造成的，导致分辨率降采样84.60%，文件大小减少93.93%，从最低的平均QF值69.3也可以观察到。其中考虑的最友好的OSN平台是微博，导致分辨率降采样57.77%，文件大小减少60.19%。可以清晰地得到如下结论，这些OSN失真将严重影响CMI算法，因此设计一个鲁棒的CMI方案至关重要，能够可靠地提取OSN传输中摄像机的痕迹。</p><h1 id="鲁棒相机模型识别">4. 鲁棒相机模型识别</h1><p>​  在有了基线之后，我们提出了一种新的方法来设计一个强大的CMI来对抗各种osn上的传输，其中关键的创新是双重的：MSL和STATE。正如预期的那样，并将通过实验验证，STATE为不同的输入提取了更多的自适应痕迹，而MSL策略更好地监督了状态的训练，共同有助于鲁棒提取摄像机痕迹的目标。<br/>​  所提出的鲁棒CMI的训练过程如图5所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png"alt="image-20240902210022345" /><figcaption aria-hidden="true">image-20240902210022345</figcaption></figure><p>图5。我们提出的鲁棒CMI的训练过程。STATE从给定的图像中提取摄像机的痕迹，而MSL策略利用一组分类器，在逐个场景的基础上监督STATE的训练。</p><p>​  具体来说，给定一个训练图像<spanclass="math inline">\(\mathbf{X}\)</span>，我们首先收集其在N个场景下的传输变体。为了达到令人满意的鲁棒性，在本工作中，我们定义了由NoTrans组成的场景。（原始），两种类型的OSN传输：Facebook和Whatsapp，这也被考虑在VISION数据集[17]。此外，考虑到训练和测试场景之间的差异，我们手工制作了一个增强场景来改进对未知（新的）osn的泛化，其中增强包括常用的后处理操作，如缩放、压缩、模糊和噪声添加。更多关于场景影响的分析，例如，不同数量的场景及其组合被推迟到第五章中的消融研究G.4。对于每个变体，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取相应的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>，其中嵌入的平滑注意模块引导<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更多地关注<spanclass="math inline">\(\mathbf{X}\)</span>中的平滑区域。根据观察I，使用N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>来监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，其中每个分类器处理每个单独场景的训练。例如，<spanclass="math inline">\(\mathcal{C}_{\phi_1}\)</span>处理NoTrans场景，<spanclass="math inline">\(\mathcal{C}_{\phi_2}\)</span>处理Facebook场景等。接下来，将在<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>上生成的总损失<spanclass="math inline">\(\sum_{n=1}^{N}\mathcal{L}_{n}\)</span>进行反向传播，以更新与STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>相关的可学习参数<spanclass="math inline">\(\theta\)</span>和与N个分类器相关的<spanclass="math inline">\(\{\boldsymbol{\phi}_n\}_{n=1}^N\)</span>。</p><p>​  在测试阶段，该过程类似于基线CMI，其中只需要训练好的状态<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>，而N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>被丢弃。在下面，我们将提供更多关于我们提出的MSL策略和STATE的更多细节。</p><h2 id="a.-多场景学习mslmulti-scenario-learning">A.多场景学习（MSL，Multi-Scenario Learning）</h2><p>​  如前所述，受观察1启发，我们的MSL策略使用了多个具有非共享权重的分类器来监督多个场景的训练。多分类器产生的一个隐式问题是训练过程可能不稳定。这是因为训练的复杂性因不同的场景不同，导致梯度方向上的冲突。为了弥补这一缺陷，我们建议在MSL的逆向过程中集成一个动量掩膜操作来减轻梯度冲突。我们现在详细介绍我们提出的MSL的前向和反向过程的细节。</p><h3 id="前向过程">1)前向过程</h3><p>​  设<span class="math inline">\(\mathbf{X}\)</span>为输入图像，<spanclass="math inline">\(\{\mathbf{X}_{n}\}_{n=1}^{N}\)</span>是其在N个场景下的N个变体，<spanclass="math inline">\(\{\mathbf{T}_{n}\}_{n=1}^{N}\)</span>是它们由STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的痕迹。有关<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的细节将推迟到下一小节。同时，利用每个分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>，根据第n个场景中的训练数据来监督训练过程，损失如下：<spanclass="math display">\[\mathcal{L}_n=\ell(\mathcal{C}_{\boldsymbol{\phi}_n}(\mathbf{T}_n),y),\]</span>​  其中，<spanclass="math inline">\(\ell\)</span>为(1)中给出的交叉熵损失。<br/>​  值得注意的是，损失函数(4)和损失函数(1)之间的关键区别在于前者涉及多个非共享分类器，而后者只使用一个。另外，请注意，<spanclass="math inline">\(\mathbf{T}_{n}\)</span>和<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>应该有相同的下标n，以便逐场景实现MSL训练场景。</p><h3 id="反向过程">2)反向过程</h3><p>​  在反向过程中，分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>的参数更新为： <spanclass="math display">\[\phi_n=\phi_n-r\nabla_{\phi_n}\mathcal{L}_n,\]</span>​  其中，r是学习率。同样，提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的反向更新可以表示为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\nabla_{\theta}\mathcal{L}_n.\]</span>​  然而，这里的<spanclass="math inline">\(\sum_{n=1}^N\nabla_{\boldsymbol{\theta}}\mathcal{L}_n\)</span>由N项组成，对应于N个场景，其中不一致的梯度方向可能导致冲突，导致<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>[18]的次优训练。此外，通过<spanclass="math inline">\(\mathbf{T}_n=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X}_n)\)</span>和使用链规则，(6)可以重写为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\partial\mathbf{T}_n/\partial\boldsymbol{\theta}\)</span>为<spanclass="math inline">\(\mathbf{T}_n\)</span>的雅可比矩阵。为了减轻梯度冲突对(6)或(7)中<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更新的影响，一种解决方案是设计非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>作为<spanclass="math inline">\(\nabla_{\mathbf{T}_n}\mathcal{L}_n\)</span>的替代品。<br/>​  根据[20]，可以通过基于一致性水平逐元素掩膜<spanclass="math inline">\(\nabla_{\mathbf{T}_n}\mathcal{L}_n\)</span>来形成非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。具体来说， <spanclass="math display">\[\mathbf{L}_n=\mathbf{M}_n\odot\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。这里的<spanclass="math inline">\(\mathbf{M}_n\)</span>是一个与<spanclass="math inline">\(\nabla{\mathbf{T}_n}\mathcal{L}_n\)</span>具有相同维数的二进制矩阵，定义为：<spanclass="math display">\[\begin{aligned}\mathbf{M}_{n}=\mathcal{I}[\mathbf{P}\succcurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\succcurlyeq\mathbf{0}]+\mathcal{I}[\mathbf{P}\preccurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\preccurlyeq\mathbf{0}],\end{aligned}\]</span>​  其中<spanclass="math inline">\(\mathcal{I}\)</span>为标准指标函数，适当尺寸的<spanclass="math inline">\(\mathbf{U}\)</span>表示一个从均匀分布<spanclass="math inline">\(U(0,1)\)</span>中抽样的随机矩阵，<spanclass="math inline">\(\succcurlyeq(\preccurlyeq)\)</span>为元素不等式。此外，<spanclass="math inline">\(\mathbf{P}\)</span>测量了给定梯度中包含的正符号的纯度（一致性），其表述为：<spanclass="math display">\[\mathbf{P}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{G}_n}{\sum_n|\mathbf{G}_n|}\big),\]</span>​  其中<spanclass="math inline">\(\mathbf{G}_{n}=\mathrm{sign}(\mathbf{T}_{n})\odot\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>在批处理维度上合并了梯度贡献，所有的计算包括除值和绝对值操作都是逐元素进行。<br/>​  然而，由（10）计算的<spanclass="math inline">\(\mathbf{P}\)</span>依赖于单个反向的特定梯度<spanclass="math inline">\(\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>（或输入<spanclass="math inline">\(\mathbf{X}\)</span>），限制了其在局部描述梯度一致性的能力。这可能导致不稳定的训练或在某些情况下较差的局部最小值，例如当批中的冲突相互抵消时。因此，我们建议通过动量平均[46]来考虑历史梯度，而不是只涉及当前反向的梯度。这样，<spanclass="math inline">\(\mathbf{P}\)</span>就可以全局计算不同场景的一致性，稳定了随机梯度下降（SGD，stochasticgradient descent）中的训练更新。<br/>​  为了将动量的概念应用于<spanclass="math inline">\(\mathbf{P}\)</span>的生成，我们重新定义了第t个反向过程中的纯度为：<spanclass="math display">\[\mathbf{P}^{(t)}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{g}_n^{(t)}}{\sum_n|\mathbf{g}_n^{(t)}|}\big),\]</span>​  其中 <spanclass="math display">\[\begin{aligned}\mathbf{g}_n^{(t)}&amp;=\mu\cdot\mathbf{g}_{n}^{(t-1)}+(1-\mu)\mathbf{G}_{n}^{(t)}\\&amp;=\mu^{t-1}\mathbf{g}_{n}^{(1)}+\sum_{i=1}^{t-2}\mu^{i}(1-\mu)\mathbf{G}^{(t-i)}+(1-\mu)\mathbf{G}_{n}^{(t)}\end{aligned}\]</span>​  在之前的t−1个历史反向过程上累积梯度，<spanclass="math inline">\(\mathbf{g}_n^{(1)}\)</span>被初始化为<spanclass="math inline">\(\mathbf{G}_{n}^{(1)}\)</span>。这里的<spanclass="math inline">\(\mu\)</span>是控制最近梯度的权重的衰减因子。在实践中，我们根据经验设置<spanclass="math inline">\(\mu=0.95\)</span>。显然，当<spanclass="math inline">\(\mu=0\)</span>时，动量<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>退化到原来的<spanclass="math inline">\(\mathbf{P}\)</span>。<br/>​  在得到动量纯度<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>后，可以通过用<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>代替<spanclass="math inline">\(\mathbf{P}\)</span>来相应地计算出(9)中的掩模<spanclass="math inline">\(\mathbf{M}_{n}\)</span>和(8)中的非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。最后，使用<spanclass="math inline">\(\mathbf{L}_{n}\)</span>通过以下方式更新<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的参数：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\mathbf{L}_n.\]</span></p><h3 id="msl训练算法">3) MSL训练算法</h3><p>​  我们总结了算法1中的整个MSL训练过程。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902220503880.png"alt="image-20240902220503880" /><figcaption aria-hidden="true">image-20240902220503880</figcaption></figure><p>​  更具体地说，前向过程在行5∼7中描述，而其余行专门用于反向过程。在第5行中，我们收集了N个场景中的输入变体，这也可以提前离线进行。然后，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取第6行中的摄像机痕迹，并利用分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>来计算第7行中的损失。为了减轻损失中的梯度冲突，第8行∼20主要用于产生动量掩模<spanclass="math inline">\(\mathbf{M}_n\)</span>，然后在第21行中用于更新<spanclass="math inline">\(\theta\)</span>。最终，经过训练的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>在第24行产生。<br/>​  备注：一种简单的替代训练策略是在不同的场景下重新标记数据，并通过一个单一的分类器来计算预测。例如，一个包含29个摄像机和4个场景的数据集可以被表示为一个包含29个×4=116个类别的单一分类任务。一个潜在的关键问题是，这种替代方案假定不同场景之间有足够的可变性；否则，某些类别在某种程度上是无法区分的。然而，这个假设并不总是正确的，例如，对于NoTrans，在场景和裁剪场景中，摄像机的痕迹或多或少是相同的。换句话说，一个痕迹实际上可能对应于多个标签，这可能会导致训练过程中的不稳定。在我们提出的MSL策略中，这种困境可以通过N个分类器很自然地避免。<br/>​  我们现在将介绍STATE提取器的架构的细节。</p><h2 id="b.-平滑引导的痕迹提取器statesmooth-aware-trace-extractor">B.平滑引导的痕迹提取器（STATE，SmooTh-Aware Trace Extractor）</h2><p>​  STATE的目的是根据给定图像的局部平滑度进行专注的相机跟踪提取。在这项工作中，我们使用著名的香农熵[47]来表示一个图像块的平滑性。显然，越小的熵值表示越光滑的区域，活动越少，反之亦然。在我们提出的STATE下，我们利用交叉注意[48]层来实现注意提取，其中平滑矩阵被转换为一个注意映射并作为指导。我们想强调的是，我们提出的STATE明确地应用了根据观察2之前的平滑性，因此和交叉注意层的简单采用有显著的不同。<br/>​  STATE的过程如图5所示。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902221142472.png"alt="image-20240902221142472" /></p><p>​  具体地说，我们首先将大小为<span class="math inline">\(H\timesW\)</span>的输入图像分割为不重叠的块<spanclass="math inline">\(\{\mathbf{R}_{i}\}\)</span>，每一个块行长<spanclass="math inline">\(\frac{H}{\hat{H}}\)</span>、列长<spanclass="math inline">\(\frac{W}{\hat{W}}\)</span>，总共得到<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>个块。对于第<spanclass="math inline">\(i\)</span>个块<spanclass="math inline">\(\mathbf{R}_{i}\)</span>，我们计算相关的平滑度指标，即香农熵<spanclass="math inline">\(E_i\)</span>： <spanclass="math display">\[E_i=-\sum_{v=0}^{255}p_v\mathrm{log}(p_v),\]</span>​  其中 <spanclass="math display">\[p_{v}=\frac{\hat{H}\hat{W}}{HW}\sum_{h=0}^{H/\hat{H}}\sum_{w=0}^{W/\hat{W}}\mathcal{I}[\mathbf{R}_{i}^{&lt;h,w&gt;}=v]\]</span>​  为<span class="math inline">\(\mathbf{R}_{i}\)</span>内的像素值为<spanclass="math inline">\(v\)</span>的概率。然后，通过将<spanclass="math inline">\(\{E_i\}\)</span>分组和重塑为维数<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>，可以得到平滑矩阵<spanclass="math inline">\(\textbf{S}\in\mathbb{R}^{\hat{H}\times\hat{W}}\)</span>。在实际实现中，我们对输入图像进行灰度处理以降低复杂性，并使用单热编码来进行高效的批处理。<br/>​  得到平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>后，我们首先通过基线提取器提取内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>，然后根据<spanclass="math inline">\(\textbf{S}\)</span>在<spanclass="math inline">\(\textbf{X}^f\)</span>上执行交叉注意力，特别注意的是，<spanclass="math inline">\(\textbf{X}^f\)</span>和<spanclass="math inline">\(\textbf{S}\)</span>分别通过压扁化得到<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>，其大小为<spanclass="math inline">\(\hat{H}\hat{W}\times\hat{C}\)</span>，其中<spanclass="math inline">\(\hat{C}\)</span>是标记化后的通道数。然后通过计算每个<spanclass="math inline">\(\hat{C}/K\)</span>通道的注意力，对展平的<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>计算K头注意力，结果是k头特征<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>，其中：<spanclass="math display">\[\hat{\mathbf{X}}_k^a=\text{Attention}(\hat{\mathbf{S}}\mathbf{Q}_k,\hat{\mathbf{X}}^f\mathbf{K}_k,\hat{\mathbf{X}}^f\mathbf{V}_k),k=1,\ldots,K,\]</span>​  <span class="math inline">\(\mathbf{Q}_k\)</span>、<spanclass="math inline">\(\mathbf{K}_k\)</span>、<spanclass="math inline">\(\mathbf{V}_k\in\mathbb{R}^{\hat{C}^{2}/K}\)</span>是交叉注意函数[48]的第k个投影的查询、键和值矩阵。这里，交叉注意力的执行是：<spanclass="math display">\[\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{SoftMax}(\frac{\mathbf{QK}^{T}}{\sqrt{\hat{C}/K}})\mathbf{V}.\]</span>​  接下来，所有头<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>的拼接输出用来线性预测，得到展平的注意力<spanclass="math inline">\(\mathbf{\hat{X}}^{a}\)</span>： <spanclass="math display">\[\mathbf{X}^a=\mathrm{MLP}(\mathrm{Concat}(\hat{\mathbf{X}}_1^a,\hat{\mathbf{X}}_2^a,\ldots,\hat{\mathbf{X}}_K^a)),\]</span>​  其中<spanclass="math inline">\(\mathrm{MLP}(\cdot)\)</span>代表一个具有GELU激活[49]的MLP。最后，通过将其重塑为<spanclass="math inline">\(\hat{H}\times\hat{W}\times\hatC\)</span>分辨率，获得细化的注意力特征<spanclass="math inline">\(\mathbf{X}^{a}\)</span>。<br/>​  考虑到<spanclass="math inline">\(\mathbf{X}^{a}\)</span>中的维数冗余性，我们遵循传统的[6]，[10]，采用全局平均池化和线性映射将<spanclass="math inline">\(\mathbf{X}^{a}\)</span>编码为低维空间<spanclass="math inline">\(\mathbb{R}^d\)</span>，从而得到最终的相机痕迹<spanclass="math inline">\(\mathbf{T}\)</span>。</p><h1 id="实验结果">5. 实验结果</h1><p>​  在本节中，我们将从开放集验证、闭集分类、对OSN传输的鲁棒性、后处理操作和重传输等方面全面评估所提出的CMI方法的性能。进一步，给出了系统的消融研究和分析。在介绍详细的结果之前，让我们先介绍一下实验设置。</p><h2 id="a.实验设置">A.实验设置</h2><h3 id="训练数据集">1)训练数据集：</h3><p>​  为了训练所提出的方法，类似于[5]，[6]，我们使用了包括29个不同的相机模型的VISION[17]数据集。在[5]、[6]、[11]和[10]之后，我们合并了来自不同设备但具有相同模型的图像，以避免歧义。需要注意的是，该数据集包含Facebook和Whatsapp传输的变体，可以方便地用作N个场景的训练数据（见图5）。</p><h3 id="测试数据集">2)测试数据集</h3><p>​  为了更好地模拟实际情况并评估泛化，我们采用FODB [12]和SIHDR[23]作为交叉测试数据集，与训练数据没有重叠。FODB数据集由25个模型和27个设备组成，而SIHDR数据集分别由21个模型和23个设备组成。在这两个数据集中，每个模型都有一个额外的设备。除非另有说明，我们将根据FODB（和SIHDR）数据集中对具有相同模型的不同设备的图像进行标记，类似于在VISION中进行的过程。</p><h3 id="在线社交网络">3)在线社交网络</h3><p>​  虽然FODB数据集本身包含5个OSN传输版本（Twitter、Telegram、Whatsapp、Instagram和Facebook），但我们进一步将FODB和SIHDR数据集扩展到9个流行的OSN传输场景，包括Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat。这使我们能够更广泛地评估CMI算法对当今实际OSN传输的鲁棒性。扩展的数据集和有关操作系统和OSN平台版本的详细信息可在https://github.com/HighwayWu/CameraTraceOSN上获得。</p><h3 id="比较方法">4)比较方法</h3><p>​  为了展示我们提出的CMI方法的优越性能，我们采用了四种最先进的算法作为竞争对手，即Kuzin[10]，ForSim [11]，NoiPri [5]和PCN [6]。</p><h3 id="实现细节">5)实现细节</h3><p>​  在训练期间，MSL策略中采用的场景数量设置为4，包括NoTrans、Facebook和Whatsapp（由VISION数据集本身提供），以及一个额外的手工增强场景。引入增强场景的原因是为了进一步提高网络对看不见的场景的泛化能力。更具体地说，增强是通过随机混合0∼50%的降采样，与QFs70∼100的JPEG压缩，与核3∼5的高斯模糊，与方差3∼10的高斯噪声相加形成的。<br/>​  我们使用PyTorch深度学习框架实现我们的方法，其中采用默认参数的Adam[50]作为优化器。学习速率初始化为1e-4，如果验证损失在5个时期内没有减少，则学习速率减半。在训练过程中，所有输入的图像被随机裁剪成512个×512补丁。内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>、平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>和交叉注意特征<spanclass="math inline">\(\textbf{X}^a\)</span>具有32×32×320相同的特征维度。提取的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>的尺寸d（参见第IV-B节）设置为256。基于观察到提高测试图像的分辨率将提高性能[6]，[12]，我们将测试大小设置为1536×1536。为了便于我们的结果，我们的代码可以在https://github.com/HighwayWu/CameraTraceOSN上找到。</p><h2 id="b.开集验证任务的评估">B.开集验证任务的评估</h2><p>​  开放集验证任务的目的是推断两个给定的图像是否被同一相机模型捕获。为此，我们从每个相机模型中随机选择25张图像，然后对每个测试数据集形成5000对正对和5000对负对。由于验证任务本质上是一个二元分类问题，我们采用广泛使用的接收机工作特征曲线下面积（AUC）作为评价性能的标准（越高越好）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903093240759.png"alt="image-20240903093240759" /><figcaption aria-hidden="true">image-20240903093240759</figcaption></figure><p>​  从表二可以看出，当图像不通过OSN传输时，所有现有方法都表现良好，AUC从87.32%到89.38%，而我们的方法略好，AUC提高了1.50%。然而，当图像通过osn传输时，所有现有方法的验证性能都会显著下降。以Twitter为例，与NoTrans场景相比，Kuzin[10]、ForSim [11]、NoiPri [5]和PCN[6]的AUC值分别下降了10.83%、26.97%、18.24%和12.23%。这种严重的性能下降可能是因为osn执行的有损操作极大地消除了原有的相机痕迹，特别是那些不鲁棒的痕迹。相反，通过利用我们提出的MSL策略和平滑注意，我们提出的方法可以探索高度鲁棒的摄像机痕迹，在Twitter场景中仅减少1.89%的AUC。对于表二中的其他OSN传输场景，我们的方法仍然表现出令人满意的鲁棒性，在AUC上平均优于第二优的方法15.30%。<br/>​  对于表二下半部分的SIHDR数据集上的结果，我们可以观察到与FODB中的情况类似的现象。例如，对于ForSim，QQ平台的性能下降最为严重，AUC下降了33.62%，最友好的平台是Weibo，导致AUC下降了12.18%。相比之下，我们的方法具有非常理想的鲁棒性，平均超过第二名的AUC11.48%。还需要注意的是，这里考虑的所有方法在SIHDR上的表现都比FODB更好，这主要是因为SIHDR有更少的相机类别，而且类别之间的可变性更大。需要注意的是，在表二的每一列中，所有被选择的对都有两个图像通过相同的OSN。一个更具挑战性的实验，一对内的图像通过不同的osn传输，推迟到V-H节进一步阐述。</p><h2 id="c.-闭集分类任务的评价">C. 闭集分类任务的评价</h2><p>​  闭集分类任务的目标是预测有限的相机模型池中给定查询图像的来源，其中代表每个相机模型的“ground-truth”痕迹是从一组预定义的图像（即锚定集）[6]中提取的。通常，“ground-truth”的痕迹可以通过利用PRNU[1]算法，或者简单地通过平均深层特征来获得。与开放集评估类似，我们从每个相机类别中随机抽取45张图像，其中25张图像作为锚定集，其余的图像作为查询图像。对于每个查询图像，预测的相机标签将被授予池中与提取的痕迹相似性最高的一个。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094227527.png"alt="image-20240903094227527" /><figcaption aria-hidden="true">image-20240903094227527</figcaption></figure><p>图6。在SIHDR数据集上的闭集分类任务的混淆矩阵。对于每个矩阵，纵轴和横轴分别代表实际的标签和预测的标签。在这里，每一行（列）代表一个独特的相机模型，例如，第一行、第二行和最后一行分别对应于GioneeS55、Huaiwei-P8和iPhone5S。有关相机型号的具体信息，请参考我们的代码网站。</p><p>​  为了评估分类任务，我们在图6中显示了混淆矩阵，并在表3中显示了相应的精度（PRC）、召回率（RCL）和F1分数。在形式上，PRC和RCL的定义为：<spanclass="math display">\[\mathrm{PRC}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FP}_{y}},\]</span></p><p><spanclass="math display">\[\mathrm{RCL}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FN}_{y}},\]</span></p><p>​  其中，<span class="math inline">\(\mathrm{TP}_{y}\)</span>、<spanclass="math inline">\(\mathrm{FP}_{y}\)</span>和<spanclass="math inline">\(\mathrm{FN}_{y}\)</span>分别代表给定类y的真阳性、假阳性和假阴性。那么宏观平均的F1得分可以计算如下：</p><p><spanclass="math display">\[\mathrm{F1}=\frac{1}{y}\sum_{y=1}^{Y}\frac{2\times\mathrm{TP}_{y}}{2\times\mathrm{TP}_{y}+\mathrm{FP}_{y}+\mathrm{FN}_{y}}.\]</span><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094839265.png"alt="image-20240903094839265" /></p><p>​  从表三可以看出，在NoTrans的情况下，我们的方法取得了相当令人满意的结果（96.38%F1），明显优于两个强竞争对手PCN [6]和NoiPri[5]算法（F1的差距分别为14.22%和20.51%）。当图像通过osn传输时，所有现有的方法都会受到严重的影响。例如，在QQ和WeChat上，PCN[6]的F1的降幅分别为32.83%和62.30%，这被认为是巨大的。这意味着在OSN干扰下，PCN很难提取有区别的摄像机痕迹。从图6中给出的混淆矩阵也很容易观察到这种现象（见第三行最后一列）。相比之下，我们的方法通常对所有的OSN传输都具有鲁棒性，导致平均F1得分为87.15%。与竞争算法Kuzin[10]、NoiPri [5]和PCN[6]相比，我们在F1方面分别获得了46.68%、37.18%和34.51%的压倒性优势。在这里，表三每列中的查询和锚图像都通过了相同的OSN。<br/>​  需要注意的是，我们在这里省略了ForSim[11]的结果。这是因为在ForSim中使用的相似性网络只能提取测量给定输入对的相似性的特征，而不能捕获特定于一种相机跟踪类型的特征。因此，ForSim无法为锚点集提取相应的特征。</p><h2 id="d.-开放式分类任务的评价">D. 开放式分类任务的评价</h2><p>​  除了之前的开集验证和闭集分类任务外，我们现在还考虑开集分类任务，它不仅涉及检测给定图像是否已知，而且还包括对其特定标签的进一步分类。具体来说，假设SIHDR数据集中的摄像机已知，每个摄像机使用25张图像作为锚定集。我们在SIHDR中为每个相机模型选择20张新图像，并确定它们是否可以被正确地分类为已知的和正确的相机模型。此外，我们在FODB中的每个相机模型中选择了20张图像，并评估它们是否可以被归类为未知的，即在SIHDR中没有一个已知的（可疑的）模型。为了避免歧义，我们排除了SIHDR和FODB数据集所共有的相机模型。显然，确定过程需要建立一个接受或拒绝一个测试示例的阈值。特别是，我们直接拒绝一个与已知相机模型的最大相似度Smax低于给定阈值δ，只有当Smax&gt;δ和正确预测时，才会接受。<br/>​  以精度和f1为标准，图7所示了阈值范围从0到1的实验结果。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903095334042.png"alt="image-20240903095334042" /></p><p>​  由于提取摄像机痕迹的方法不同，竞争的方法在不同的阈值下达到了各自的最佳性能。其中，Kuzin[10]的最佳准确率为79.26%，F1评分为68.87%，而NoiPri[5]的最佳准确率为72.30%，F1评分为65.58%。由于PCN倾向于对正对和负对产生相对较高的相似性得分，因此它对阈值变化变得不那么敏感，导致图7中没有峰值。其中，PCN获得的最高准确率和F1得分分别仅为50.07%和64.15%。这表明PCN并不很适合用于开放集分类任务。相比之下，我们提出的算法的准确率为91.63%，F1为91.40%，以准确率+12.37%、F1+22.53%的表现远远超过了第二好的算法。</p><h2 id="e.-后处理评价">E. 后处理评价</h2><p>​  虽然本文的重点是针对各种CMI方案的OSN传输场景的鲁棒设计，但我们的设计也自然地为常用的后处理带来了令人满意的鲁棒性。具体来说，考虑的后处理操作包括QFs范围为70到95的JPEG压缩，因子从10%到50%的线性调整，核大小为[3,5,7]的高斯模糊，以及方差为[3,5,7,9]的高斯噪声相加。总共有18种不同的后处理操作，JPEG压缩、调整大小、模糊和噪声添加分别有6、5、3和4个变体。这些操作被应用于SIHDR数据集，其中包含929张图像。因此，总共生成了18×929=16,722张图像。对于每个图像，只应用了一种后处理攻击类型。鉴于OSN已经包含了复合攻击场景，我们这里的重点主要是对抗单一的后处理攻击。比较结果如图8所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100039566.png"alt="image-20240903100039566" /><figcaption aria-hidden="true">image-20240903100039566</figcaption></figure><p>​  可以看出，尽管ForSim [11]和NoiPri[5]在NoTrans场景中表现良好，但当遇到高斯模糊和高斯噪声添加等强后处理时，他们正在努力保持稳定的性能。Kuzin[10]在JPEG压缩和调整大小下相对稳定；但在高斯模糊和噪声增加的情况下，性能迅速下降。相比之下，PCN[6]和我们的方法对这些后处理操作具有更好的鲁棒性；对于所有考虑的情况，我们的方法仍然优于PCN，特别是在大核大小的高斯模糊中。</p><h2 id="f.-再传输和交叉传输的评估">F. 再传输和交叉传输的评估</h2><p>​  在实践中，通过多个OSN平台进行再传输和/或交叉传输（即，图像被下载并重新上传到相同或不同的osn上）是非常常见的。现在我们简要地讨论了不同CMI算法在再传输和交叉传输情况下的鲁棒性评估问题。具体来说，我们考虑的是通过Facebook的传输，其次是Facebook/QQ，其次是Whatsapp/QQ。结果列于表四之中。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100523776.png"alt="image-20240903100523776" /></p><p>​  可以观察到，第二轮传输，无论是再传播还是交叉传输，其影响远小于第一轮传播。我们的方法对两轮OSN传输都表现出相当强的鲁棒性，而竞争算法的性能要差得多。例如，例如，在通过Whatsapp场景之后再通过Whatsapp/QQ场景进行传输时，我们的方法只有2.21%/1.99%AUC下降，而PCN的性能下降更严重，达到10.26%/9.36% AUC损失。</p><h2 id="g.-消融研究">G. 消融研究</h2><p>​  在本小节中，我们通过分析每个组件如何有助于提取鲁棒的相机痕迹，对我们提出的方法进行消融研究。具体来说，我们首先禁止使用平滑注意和MSL策略，从而产生基线性能。然后，我们将注意力模块和MSL策略的不同变体纳入基线中，评估它们带来的额外性能收益。比较结果如表V所示，由于页面限制，它只包括在SIHDR[23]数据集下关于NoTrans、Facebook和Whatsapp场景的开放集验证结果。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101054119.png"alt="image-20240903101054119" /><figcaption aria-hidden="true">image-20240903101054119</figcaption></figure><h3 id="平滑注意力的采用">1)平滑注意力的采用：</h3><p>​  在表V的第二行中，我们给出了具有注意机制的结果。在这里，除了我们对平滑注意力的关注，我们还评估了变量注意力的性能，包括熵滤波器[11]或可靠性映射[22]。可以看出，通过引导基线提取器更加注意图像中的平滑区域，所有这三个注意模块都确实提高了性能。具体来说，可靠性映射[22]所带来的改进是有限的（仅有0.14%的收益），主要是因为两个因素：1)它估计给定的图像（局部）的可靠性，从全局角度缺乏交互性；2)它需要预先训练，因此很难端到端训练，这限制了提取器的优化。对于熵滤波器[11]方法，需要手动调整阈值来过滤高熵区域，这不可避免地丢弃了一些有价值的信息，在实践中很麻烦。因此，熵滤波器所带来的增量很小，只有0.82%。相比之下，我们的基于交叉注意的平滑注意模块不仅能够为不同的输入自动定制其平滑度，而且还可以很容易地纳入端到端训练中，从而获得2.79%的性能提高。在图9中，为了更直观的理解，我们还更直观地可视化了提取器注意。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101452445.png"alt="image-20240903101452445" /><figcaption aria-hidden="true">image-20240903101452445</figcaption></figure><p>​  显然，我们的平滑注意模块可以有效地引导特征提取更加注意平滑区域。</p><h3 id="msl模块的采用">2)MSL模块的采用：</h3><p>​  为了分析不同的学习策略的贡献，我们在表V的第三行中给出了相应的消融结果。注意到，第一行的基线提取器使用单个分类器进行优化，而第三行的所有结果都采用多个分类器，属于MSL的类别。除了我们的MSL策略外，我们还包括了通过使用GradNorm[19]或GradDrop[20]来获得非冲突梯度的变体。由此可见，采用多分类器的MSL确实大大提高了整体性能；即使是一个简单的非加权策略也能带来7.63%的收益。GradNorm[19]或GradDrop[20]的加入可以实现更大的改进（分别为10.04%和10.77%）。由于动量掩蔽操作明确地利用了历史逆向过程产生的梯度，我们提出的MSL与基线相比获得了更好的性能增益，达到12.38%。最后，通过联合使用平滑注意和MSL策略，我们的鲁棒CMI可以大大优于基线，导致总性能提高了15.02%。</p><h3 id="基线提取器的选择">3)基线提取器的选择：</h3><p>​  正如第三节中提到的，我们提出的鲁棒CMI是通用的，其中基线提取器（EfficientNet-b0）可以灵活地被其他网络取代。为此，我们采用了另一种最先进的网络，MobileFormer[51]，作为基线，以证明通过应用我们的鲁棒设计，也可以大大提高提取器的鲁棒性。如表V的最后一行所示，MobileFormer基线的鲁棒性得到了很好的加强，例如，平均AUC增加了13.85%。</p><h3 id="msl中n个场景的影响">4)MSL中，N个场景的影响：</h3><p>​  在MSL策略的设计中，可能会出现一个有趣的问题，即需要多少个场景来实现所需的鲁棒性。因此，我们进行了额外的实验来分析在MSL中对N个场景的影响，结果见表6。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102058441.png"alt="image-20240903102058441" /><figcaption aria-hidden="true">image-20240903102058441</figcaption></figure><p>​  可以看出，当N=1时，提取器的鲁棒性远不能令人满意。其潜在原因有两方面：1)当N =1时，MSL只包含一个分类器，从而退化为正常的训练过程；2)场景的奇异性可能导致对特定场景的严重过拟合，导致泛化性能较差。当考虑的场景数量增加时，上述困境可以很好地缓解。即使我们只同时使用原始场景和Facebook场景（N=2），我们也观察到了显著的性能提高，平均实现了7.72%的AUC改进。此外，考虑到训练和测试过程之间的差异，我们引入了一个数据增强场景，以进一步增强提取器对未知（新的）osn的泛化。根据组合场景的结果，“NoTrans+Aug”，可以得出结论，这种增强场景是有效的，可能是因为这些增强操作在一定程度上与其他osn所使用的操作重叠。最后，最后两行的结果表明，进一步增加场景的数量到N= 3和N =4可以不断提高性能。我们也尝试了用更大的N进行更多的组合；但是额外的性能提高是非常边际的，代价是显著增加的复杂性。因此，在我们的方案中，我们采用了四种场景的组合，即“NoTrans+FB+WA+Aug”。</p><h2 id="h.-评估具有挑战性的交叉osn场景"><em>H.</em>评估具有挑战性的交叉OSN场景</h2><p>​  回想一下，在上述实验中，一对内的图像通过相同的OSN传输（一致的OSN场景）。在本小节中，我们进行了更具挑战性的实验，其中图像来自不同的osn（不一致的OSN场景），比如其中一张图像来自NoTrans，另一张来自Facebook（下面标记为“NT，FB”）。在这些场景中，每个算法的性能结果如表7所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102551041.png"alt="image-20240903102551041" /><figcaption aria-hidden="true">image-20240903102551041</figcaption></figure><p>​  对于包含两个以上osn的列，一对中的图像来自两个随机选择的osn。很明显，我们提出的算法仍然大大优于现有的方法，平均AUC比排名第二的Kuzin[10]提高了8.24%。然而，不出所料，与一致的OSN场景相比，所有算法的性能都表现出了严重的下降。由于相机痕迹和OSN退化的混合，将相机痕迹分离出来，然后进行相机模型识别是相当具有挑战性的。事实上，在某些极端情况下，受OSN-A干扰影响的相机a和受OSN-B干扰影响的相机b可能具有相同的累积效应，从而误导了相机模型识别算法。一个可能的解决方案是进行盲分离，以区分相机的痕迹和OSN干扰。然而，盲分离本身就是一项极具挑战性的任务。在这项工作中，我们专注于在存在一致的OSN干扰下的相机痕迹的鲁棒提取。在未来，我们将继续探索和研究在更具挑战性的不一致OSN场景中的鲁棒摄像机模型识别算法。</p><h2 id="i.设备标识的评估">I.设备标识的评估</h2><p>​  虽然本文的主要重点是相机模型识别，但我们也尝试进行一些关于相机设备识别的实验，即评估算法识别同一模型但不同设备的图像的能力。实验结果如表8所示，其中“Same”指定的列表示AUC结果，所有正对的图像都来自同一设备。类似地，带有“Diff”的列对应于正对图像来自同一模型但不同设备的情况。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903110000029.png"alt="image-20240903110000029" /><figcaption aria-hidden="true">image-20240903110000029</figcaption></figure><p>​  可以观察到，我们提出的方法仍然能够准确地分类来自不同设备的图像，在AUC度量上超过排名第二的ForSim[11]5.04%。此外，与“Diff”的情况下相比，在“Same”的情况下，算法的总体性能往往稍好一些。这些结果表明，同一相机型号的不同设备仍然具有一定的独特的相机痕迹。</p><h1 id="结论">6. 结论</h1><p>​  在本文中，我们研究了在OSN共享图像上设计鲁棒CMI的问题。基于两个关键的观察结果，我们提出了一个鲁棒的CMI方案，明确地利用基于平滑的交叉注意和MSL策略。大量的比较实验与几种最先进的方法证明了我们的方法的优越性，特别是在各种OSN传输的场景中。我们的鲁棒设计也可以揭示一些法医取证问题，如耐OSN水印，鲁棒伪造检测等。</p>]]></content>
      
      
      <categories>
          
          <category> 相机模式识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>experimet</title>
      <link href="/experimet/"/>
      <url>/experimet/</url>
      
        <content type="html"><![CDATA[<figure><img src="../postimages/experimet/image-20240902094509834.png"alt="image-20240902094509834" /><figcaption aria-hidden="true">image-20240902094509834</figcaption></figure><figure><img src="../postimages/experimet/image-20240902094320599.png"alt="image-20240902094320599" /><figcaption aria-hidden="true">image-20240902094320599</figcaption></figure><p>第一行是FOCAL论文效果<br/>第二行是复刻的最好效果<br/>第三四行是冻结encoder，训练decoder<br/>第五行是一起训练<br/>表格如下：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;"></td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="odd"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr></tbody></table><p>复刻与FOCAL论文效果相比：</p><table style="width:100%;"><colgroup><col style="width: 15%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 4%" /><col style="width: 5%" /><col style="width: 5%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">+161</td><td style="text-align: center;">+292</td><td style="text-align: center;">9779</td><td style="text-align: center;">+309</td><td style="text-align: center;">+380</td><td style="text-align: center;">8425</td><td style="text-align: center;">-43</td><td style="text-align: center;">-102</td><td style="text-align: center;">8696</td><td style="text-align: center;">+166</td><td style="text-align: center;">+255</td><td style="text-align: center;">8426</td><td style="text-align: center;">-298</td><td style="text-align: center;">-555</td><td style="text-align: center;">7822</td></tr></tbody></table><p>decoder性能相比：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="even"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr><tr class="odd"><td>Log_v09061430/latest/</td><td style="text-align: center;">Log_v09061430/latest/</td><td style="text-align: center;">9718</td><td style="text-align: center;">9545</td><td style="text-align: center;">9734</td><td style="text-align: center;">8036</td><td style="text-align: center;">5693</td><td style="text-align: center;">8604</td><td style="text-align: center;">8284</td><td style="text-align: center;">6411</td><td style="text-align: center;">8547</td><td style="text-align: center;">8826</td><td style="text-align: center;">6851</td><td style="text-align: center;">8582</td><td style="text-align: center;">6767</td><td style="text-align: center;">3495</td><td style="text-align: center;">7930</td></tr></tbody></table><p>计算差值。以下是计算结果：</p><table style="width:100%;"><colgroup><col style="width: 20%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 5%" /><col style="width: 5%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>复刻</td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>冻结encoder的情况下，单独训练decoder</td><td style="text-align: center;">-27</td><td style="text-align: center;">-66</td><td style="text-align: center;">-16</td><td style="text-align: center;">-85</td><td style="text-align: center;">-172</td><td style="text-align: center;">-61</td><td style="text-align: center;">-111</td><td style="text-align: center;">-116</td><td style="text-align: center;">+1</td><td style="text-align: center;">+27</td><td style="text-align: center;">+62</td><td style="text-align: center;">+52</td><td style="text-align: center;">+44</td><td style="text-align: center;">+56</td><td style="text-align: center;">+63</td></tr><tr class="odd"><td>冻结encoder的情况下，单独训练decoder</td><td style="text-align: center;">-15</td><td style="text-align: center;">-49</td><td style="text-align: center;">-6</td><td style="text-align: center;">-99</td><td style="text-align: center;">-160</td><td style="text-align: center;">-90</td><td style="text-align: center;">-92</td><td style="text-align: center;">-125</td><td style="text-align: center;">-9</td><td style="text-align: center;">+33</td><td style="text-align: center;">+75</td><td style="text-align: center;">+59</td><td style="text-align: center;">+46</td><td style="text-align: center;">+49</td><td style="text-align: center;">+58</td></tr><tr class="even"><td>联合训练</td><td style="text-align: center;">-62</td><td style="text-align: center;">-51</td><td style="text-align: center;">-47</td><td style="text-align: center;">-38</td><td style="text-align: center;">-104</td><td style="text-align: center;">+148</td><td style="text-align: center;">-141</td><td style="text-align: center;">-255</td><td style="text-align: center;">+33</td><td style="text-align: center;">+120</td><td style="text-align: center;">+269</td><td style="text-align: center;">+185</td><td style="text-align: center;">-10</td><td style="text-align: center;">+23</td><td style="text-align: center;">+112</td></tr><tr class="odd"><td>联合训练</td><td style="text-align: center;">-63</td><td style="text-align: center;">-37</td><td style="text-align: center;">-45</td><td style="text-align: center;">+37</td><td style="text-align: center;">+73</td><td style="text-align: center;">+179</td><td style="text-align: center;">-313</td><td style="text-align: center;">-547</td><td style="text-align: center;">-149</td><td style="text-align: center;">+90</td><td style="text-align: center;">+206</td><td style="text-align: center;">+156</td><td style="text-align: center;">-35</td><td style="text-align: center;">+20</td><td style="text-align: center;">+108</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</title>
      <link href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/"/>
      <url>/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/</url>
      
        <content type="html"><![CDATA[<center>Towards Modern Image Manipulation Localization:A Large-Scale Dataset andNovel Methods <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a> <ahref="https://github.com/qcf-568/MIML"><imgsrc="https://img.shields.io/github/stars/qcf-568/MIML?style=flat"alt="GitHub" /></a></center><center><span class="math inline">\(\text{Chenfan Qu}^1,\text{YiwuZhong}^{2,*},\text{Chongyu Liu}^1,\text{Guitao Xu}^1,\text{DezhiPeng}^1,\text{Fengjun Guo}^3,\text{Lianwen Jin}^{1,4,*}\)</span></center><center>1华南理工大学，2威斯康星大学，3 INTSIG信息有限公司，4INTSIG-SCUT文件分析与识别联合实验室</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>  近年来，图像篡改定位因其在保障社交媒体安全方面的关键作用而越来越受到人们的关注。然而，如何准确地识别伪造的区域仍然是一个开放的挑战。其中一个主要的瓶颈是，由于其昂贵的创建过程，严重缺乏高质量的数据。为了解决这一限制，我们提出了一种新的范式，称为CAAA，可以在像素级自动和精确地注释大量的人工伪造的图像。我们进一步提出了一种新的度量QES，以方便不可靠注释的自动过滤。利用CAAA和QES，我们构建了一个大规模、多样化、高质量的数据集，其中包括123,150张带有掩码注释的人工伪造图像。此外，我们开发了一种新的模型APSCNet，用于精确的图像篡改定位。根据大量的实验，我们的数据集显著地提高了在广泛使用的基准测试上的各种模型的性能，这些改进归因于我们提出的有效方法。这些数据集和代码可以在https://github.com/qcf-568/MIML上公开获得。</p><h1 id="引言">1. 引言</h1><p>  我们提出了一种新的思想，利用训练有素的约束图像处理定位模型，自动获取这些未标记的伪造图像的掩模标注，从而大大缓解了图像处理定位的数据稀缺问题，如图1所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826151824078.png"alt="image-20240826151824078" /><figcaption aria-hidden="true">image-20240826151824078</figcaption></figure><p>图1.我们提出了一种新的约束图像处理定位范例，它分别处理SPG和SDG中的图像。我们还建议将它用于自动注释，并构建一个大规模、高质量的数据集，显著提高了图像篡改定位模型的泛化性。</p><p>  由于约束图像篡改定位方法利用相应的真实图像对伪造区域进行定位，可以大大降低任务的复杂性。<br/><br/>  然而，尽管在挑战性较小的数据方面取得了进展，但由于三个严重的障碍，以前的约束图像篡改定位方法不足以作为复杂的现代图像的合格自动注释器。首先，他们大多使用一个单一的基于相关性的模型来处理所有的输入数据[19,28]，我们认为这是一个次优的范式。一般情况下，根据操纵图像之间的共同部分是伪造区域还是真实区域，将伪造图像对及其原始图像可分为共享供体组（SDG，SharedDonor Group）和共享探针组（SPG， Shared ProbeGroup）两组，如图2所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826152216897.png"alt="image-20240826152216897" /><figcaption aria-hidden="true">image-20240826152216897</figcaption></figure><p>  虽然以往的基于相关的方法对于可持续发展目标是合理的，但它们还不够适合SPG，因为SPG中数据的实际公共部分大多是背景，而可持续发展目标中的实际公共部分大多是前景。与可持续发展目标中的共享前景相比，SPG中的共享背景具有更大的面积和更少的显著特征。同时对可持续发展目标和SPG数据上训练基于相关的模型会导致混淆，削弱其泛化能力。其次，从真实图像中减去伪造图像得到的差异图总是可以突出伪造区域，但这一重要线索被以往的约束图像篡改定位方法完全忽略了。最后，以往的工作对操作过程中大量重新缩放操作导致的语义错位关注不够重视，这混淆了模型并对其产生负面影响。<br/><br/>  为了解决这些问题，我们提出了一种新的范例，称为类别感知自动注释（CAAA，Category-AwareAuto-Annotation），它分别处理SDG和SPG中的图像对。所提出的CAAA范式由三个组成部分组成。首先，使用分类器来确定输入图像对是属于SDG还是SPG。该分类器可以通过使用无标记图像的自监督学习来有效地进行训练。其次，利用差异感知语义分割模型，利用图像对及其差异映射在SPG中进行精确的约束操作定位。此外，一个语义对齐相关匹配模型，通过更好的语义对齐提高了SDG的性能。实验表明，我们的方法在复杂场景下显著优于以往的约束图像篡改定位方法，并且足以进行自动标注。<br/><br/>  随后，我们从互联网上收集了大量手工伪造的图像，然后用提出的CAAA对其伪造的区域进行注释。该方法可以显著缓解图像处理定位中非合成数据的稀缺性，如图1所示。为了确保所有的注释都足够可靠，我们进一步提出了一个新的度量标准，称为质量评估评分（QES）。QES可以自动评估注释的质量并排除坏的注释，而不需要ground-truths来计算。实验表明，我们的数据集可以在广泛使用的基准测试上显著改进各种图像篡改定位模型。<br/><br/>  此外，为了更好地利用我们的MIML数据集，我们提出了一个新的模型，称为APSC-Net，它在各种基准测试上都优于以前的方法。<br/><br/>  综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一个新想法：从网络规模的图像中促进图像篡改定位的任务，以及从较少挑战性的任务，约束图像篡改定位中提取的自动注释。</li><li>我们提出了一种新的约束图像篡改定位范式，称为CAAA，它分别处理SPG和SDG。对于SPG，我们建议使用用语义信息去噪的图像差分。对于可持续发展目标，我们建议将语义与一个跨级别的特征相关框架对齐。</li><li>我们提出了一种新的有效度量QES，在数据集构建时自动过滤出不可靠的掩码注释。</li><li>基于上述技术，我们构建了一个大规模的、多样化的、高质量的数据集，称为MIML。它显著地解决了图像篡改定位的手工伪造数据的问题，从而大大提高了模型的泛化能力。</li></ul><p>  与图像处理定位相比，约束图像处理定位（CIML， constrained imagemanipulationlocalization）[32]在给定的真实图像的额外帮助下对伪造的图像区域进行定位。以往的工作大多是基于相关匹配，并对SDG和SPG中的图像对进行统一处理。Wu等人[32]提出了第一个深度相关模型DMVN，该模型计算相关映射来定位图像中的相似对象。Liu等人提出去除池化层，采用无卷积获得更丰富的空间信息。Liu等人的[18]采用了注意感知机制来获得更好的表现。Tan等人[28]提出在编码器和解码器中都执行相关性，以提取更好的特征。这些方法在挑战性较小的数据集上取得了重大进展（例如，合成COCO[32]）。然而，它们的性能在具有高分辨率、大变化度和大复杂度的现代图像中受到限制。</p><h1 id="分类感知自动注释模块">2. 分类感知自动注释模块</h1><p>  对于受限的图像篡改定位，以往的工作没有考虑SPG和SDG图像对之间的差异，而是使用单一的相关性模型对其进行统一处理。我们认为这种范式是次优的，原因如下：</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826160604894.png"alt="image-20240826160604894" /><figcaption aria-hidden="true">image-20240826160604894</figcaption></figure><p>  首先，SDG 图像的相似区域是前景区域（例如，图3中SDG分支中的猫）。它们有特定的、相似的形状和独特的特征。相比之下，SPG图像的相似区域是背景。它们通常没有足够独特的特征来进行精确的相关匹配（例如，在图3的SPG分支图像中，一块背景中的雪与所有其他块背景中雪有很高的相似性）。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。<br/><br/>  第二，配对的SPG图像之间的差异是一个重要的提示。SPG图像对中的大部分区域几乎相同，并在空间上对齐（例如，图3中SPG分支的图像对）。简单地在它们之间减去最终的差异图就可以突出被操纵的区域。然而，这些信息在以前的基于相关匹配的模型中难以利用，因此在以前的CIML工作中没有考虑到。<br/><br/>  基于这些观察结果，我们提出了一个新的CIML任务范式，类别感知自动注释CAAA。其关键思想是独立处理SPG和SDG图像，如图3所示。首先，利用第3.1节中提出的分类器，将输入的图像对分为SPG或SDG。对于SPG，图像对采用第3.2节中提出的差分感知语义分割进行处理。对于SDG，图像对通过第3.3节中提出的语义对齐相关匹配进行处理。更重要的是，用我们提出的范式训练的模型被进一步用于执行在大量手工伪造的图像上的自动注释。作为回报，收集的数据解决了用于图像篡改定位的非合成数据的严重短缺。</p><h2 id="自监督分类器">2.1 自监督分类器</h2><p>  为了实现SPG和SDG的分类，我们提出了通过对无标记图像的自监督学习来训练分类器。给定一个图像，我们对其进行随机的增强和操作，然后将伪造的图像和原始图像形成一个SPG图像对。为了构建一个SDG图像对，我们从原始图像中复制随机对象，调整它们的大小，并将它们粘贴到另一个图像中。利用所得到的图像对，我们可以有效地训练我们的分类器。每个输入对中的两个图像在被输入到分类器之前被连接在通道维数中。分类器只需要识别一个图像对中的两个图像是几乎相同（SPG）还是明显不同（SDG），而不考虑哪一个或哪里是假的。因此，这个分类任务非常简单，我们可以准确地将图像对分成两组。</p><h2 id="具有差异感知能力的语义分割">2.2 具有差异感知能力的语义分割</h2><p>  理想情况下，对于SPG中的图像对，真实图像和伪造图像之间的绝对差异实际上是伪造区域。然而，被操纵的图像在传输[31]过程中通常会发生退化，这使得无法利用绝对差异作为精确的注释。如图4所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826162503002.png"alt="image-20240826162503002" /><figcaption aria-hidden="true">image-20240826162503002</figcaption></figure><p>图4.由于篡改后的图像在传输过程中通常会经历一系列的退化，因此它们与真实图像之间的绝对差异不能准确地表示伪造区域。我们的方法通过使用语义信息来实现了充分的去噪，从而解决了这一问题。</p><p>  由于传输退化，图像差分图中几乎所有的区域都是非零的。即使是OTSU[24]算法二值化的差异图也在真实区域上突出，特别是在高频区域，如边缘。为了解决这个问题，我们建议利用图像中的语义信息来去噪差异映射。为了实现这一点，我们提出将真实图像、伪造图像及其差异映射图的信道维数连接输入到一个语义分割模型中。</p><h2 id="语义对齐的关联匹配">2.3 语义对齐的关联匹配</h2><p>  由于广泛的重缩放操作，语义失调成为对基于相关性的方法的有效性产生不利影响的关键因素。例如，在图3的SDG分支中，原始图像中的猫占据了一个很大的区域，而在伪造的图像中，同一只猫被限制在一个小得多的区域内。原始图像的猫特征大多在最高水平，而伪造图像的猫特征大多在最低水平。因此，两幅图像之间在同一编码水平上的视觉特征存在语义错位。然而，以往的工作只是迫使模型在相同的特征层次之间进行特征匹配，这混淆了模型，并对其泛化产生了负面影响。为此，我们提出通过实现更好的语义对齐来提高相关模型的性能。<br/><br/>  具体来说，给定从主干模型中提取的一组不同分辨率的特征映射，我们首先用平均池化的最高特征计算全局表示，然后用卷积层将它们与最高特征融合。随后，我们以一种类似于在FPN[15,34]中的自上而下的方式融合了这些特性映射。这样，低级特性就具有更多的语义，并准备与高级特性相匹配。然后，我们以跨层次的方式计算输入图像对特征之间的相关特征$ F_{corr} $，如式(1)，这不同于以前的方法[18,19,32]，它只计算与方程(2)相同水平的特征图之间的相关特征。<span class="math display">\[[Corr(F_{o,i},F_{m,j}) for i in (0{-}3) andfor j in (0{-}3)]\]</span></p><p><span class="math display">\[[Corr(F_{o,i},F_{m,i}) for i in (0-3)]\]</span></p><p>  在这些方程中，Corr表示之前工作中广泛使用的相关函数，[18,19,32]， $F_{o,i} $ 表示原始图像的第i层特征图， $ F_{m,j} $表示伪造图像的第j层特征图。我们的模型能够自适应地选择最优匹配路由，从而增强语义对齐。$ F_{corr} $ 随后被连接，信道减少并输入卷积解码器进行最终预测。</p><h1 id="miml数据集">3. MIML数据集</h1><p>  在本节中，我们提出了一个大规模的、多样化的、高质量的数据集，称为MIML。其关键思想是利用在现有数据集上训练的约束图像篡改定位模型，自动从网络中人工伪造的图像获得准确的掩码注释。为了确保数据集的高质量，我们还提出了一个新的度量标准来过滤掉不充分的注释。</p><h2 id="数据集构成">3.1 数据集构成</h2><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826163432173.png"alt="image-20240826163432173" /><figcaption aria-hidden="true">image-20240826163432173</figcaption></figure><p>如图6所示，我们构建MIML的步骤如下：</p><p>  <strong>图像收集。</strong>我们从imgur.com中收集图像对。在这个网站上，这些图片是由数百万人手工伪造的，因此有高质量、多样化的伪造区域。<br/><br/>  <strong>数据清理。</strong>我们清理收集到的数据，并排除与第6块中的评估数据集重叠的图像。<br/><br/>  <strong>分类。</strong>我们使用第3.1节中提出的分类器将清理后的图像对分类为SPG或SDG。实际的分类器是三个模型[8,20,21]的集合。<br/>  <strong>自动注释。</strong>我们利用第3.2节和第3.3节中提出的DASS和SACM，分别自动获取SPG和SDG中图像的掩码注释。<br/><br/>  <strong>质量评价。</strong>经过自动注释，SPG的注释已经有了高质量，而SDG的注释仍然不令人满意。为了保证整体质量，我们提出了一种新的度量标准，质量评估评分（QES），以进一步过滤掉不可靠的注释。QES的关键思想是，大多数高质量的预测都有非常高的置信度和尖锐的边缘，因此我们可以评估预测的质量，并通过检查预测的置信度和清晰度来排除不好的预测。具体来说，给定一个具有形状（H，W）和归一化概率的预测掩模，我们计算QES如下：<spanclass="math display">\[\textbf{QES=}\frac{\sum_{i,j}^{H,W}p_{i,j}&gt;(1-T_{h})}{\sum_{i,j}^{H,W}p_{i,j}&gt;T_{l}}\]</span>  其中， $~ \sum_{i,j}^{H,W}p_{i,j} &gt; (1 - T_{h}) $ 表示高置信度大于$ (1 - T_{h}) $ 的预测区域， $~ \sum_{i,j}^{H,W}p_{i,j}&gt;T_l $表示预测的总潜在操纵面积。我们将 $ T_{h} $ 和 $ T_{l} $ 设置为 $~\frac{1}{16} $，只保留QES&gt;0.5的样品。实验表明，我们的QES与IoU度量有很强的相关性，可以有效地帮助过滤出不可靠的掩码注释。</p><h2 id="数据集亮点">3.2 数据集亮点</h2><p>  我们在图5中给出了所建议的数据集的几个例子。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826164936525.png"alt="image-20240826164936525" /><figcaption aria-hidden="true">image-20240826164936525</figcaption></figure><p>  我们的数据集的主要亮点如下：</p><ul><li><strong>高质量。</strong>该数据集中的图像操作是由人类精心制作的。这些数据可以教模型在现实世界中发现伪造，而不仅仅是在合成数据中过度拟合几个简单的模式。</li><li><strong>大规模。</strong>如表1所示，所提出的数据集共有123,150张人工伪造的图像，比之前的手工IML数据集多几十倍（例如，≈比IMD20的60倍）。</li><li><strong>多样性。</strong>我们的数据集包括各种大小、各种样式和各种类型的操作（例如，复制-移动、拼接、删除）的图像。它们是由成千上万的人利用各种软件创建的。这些不同的数据可以大大提高深度IML模型的泛化能力。</li><li><strong>现代风格。</strong>我们的数据集有大量的现代图像，最近被捕获和伪造，跟上了现代数码摄影技术的步伐。相比之下，CASIA数据集[3]是在十多年前提出的，其中大多数图像的尺寸都很小，而且都很模糊。因此，我们的数据集可以更好地满足现代图像操作定位的要求。</li><li><strong>强大的可扩展性。</strong>网络上有许多越来越受欢迎的图像处理比赛，不断吸引数百万人来参加（例如1900万pas-Battles[9,25]的900万人），产生了大量新的手工伪造图像。我们的数据集构建方法已经准备好利用这些不断增长的廉价web数据。因此，我们的数据集可以很容易地进行扩展，显示出强大的可扩展性。</li></ul><h1 id="apsc-net">4. APSC-Net</h1><p>  在本节中，我们提出了一个新的模型，称为APSCNet，以实现精确的图像操作定位。如图7所示，它由特征提取器、自适应感知模块和自校准模块组成。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png"alt="image-20240826165320985" /><figcaption aria-hidden="true">image-20240826165320985</figcaption></figure><h2 id="自适应感知模块">4.1 自适应感知模块</h2><p>  在细致的图像取证分析过程中，人类经常会反复放大和缩小图像，选择一组最佳的观察结果来帮助他们的最终预测。为了模拟人类的感知方式，我们设计了一个自适应感知模块，以帮助模型比较不同的视图，并自适应地选择每个输入图像的最优组合。其关键思想是使用从全局表示计算出的自适应权重对当前和所有高级特征图进行加权。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170701354.png"alt="image-20240826170701354" /><figcaption aria-hidden="true">image-20240826170701354</figcaption></figure><p>  具体来说，给定从主干模型中提取的4个特征图，我们首先将它们映射到1×1转换层的通道上，得到4个特征图$ F_{i,0},F_{i,1},F_{i,2},F_{i,3} $ 。然后，我们通过全局平均池化从 $F_{i,3} $ 中得到全局图像表示，并使用1×1卷积层将它们与 $ F_{i,3} $融合，得到 $ F_{o,3} $。最后，对于（2，1，0）中的a和范围（a+1，3）中的b，我们遵循下面的公式(3)和(4)依次计算$ F_{o,a} $ ： <spanclass="math display">\[[w_{a,a},w_{a,b}]=\sigma(f_{a}(Cat([Avg(F_{i,a}),Avg(F_{o,b})])))\]</span></p><p><spanclass="math display">\[F_{o,a}=Conv(w_{a,a}*F_{i,a}+\sum_{b=a+1}^3w_{a,b}*F_{o,b})\]</span></p><p>  其中Avg表示全局平均池，Cat表示通道维连接， $ f_a $表示具有ReLU层的两个线性层， $ $表示Sigmoid激活函数，Conv表示3×3卷积层。</p><h2 id="自校准模块">4.2 自校准模块</h2><p>  当对操纵图像进行细致的定位时，人类倾向于通过比较预测的伪造区域周围的特征来确认他们的初始预测。此外，他们可能会根据他们对图像真实性的全局评估来修改他们的局部预测。为了模拟人类的感知方式，我们设计了一个自校准模块，以获得更好的性能。<br/><br/>  如图7所示，提出的自校准模块包括基于分割的自校准（SSC，Segmentation-basedSelf Calibration）和基于分类的自校准（CSC，Classification-based SelfCalibration）。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170727546.png"alt="image-20240826170727546" /><figcaption aria-hidden="true">image-20240826170727546</figcaption></figure><p>  对于SSC，从自适应感知模块的末端获得初始预测，并将其输入一个由几个卷积层组成的小校准核映射模块。随后，我们得到一个校准核，并对其初始预测进行卷积运算。然后使用Min-Max方法对结果值进行归一化。我们将标准化结果乘以$ F_o $ ，即 $ F_{o,0},F_{o,1},F_{o,2},F_{o,3} $ 的串联，得到 $ F_{ref1}$ 。接下来，我们使用多个卷积层来细化 $ F_{ref1} $ ，并得到细化的特征 $F_{ref2} $ 。在此之后，我们将 $ F_{ref2} $ 与 $ F_o $连接起来，执行信道注意和信道减少的方法，用结果替换 $ F_o $ ，并重复利用$ F_o $ 和校准预测的过程，再次获得 $ F_{ref2} $ 两次，以获得 $ F_{ref2}$的改进版本。利用SSC，我们的模型可以根据其初始掩模预测大致自适应地关注最优区域，从而通过深入分析获得更高的性能。<br/><br/>  对于CSC，我们首先将改进后的特征$ F_{ref2} $输入到一个小分类头中，用于预测输入图像是否被篡改。如果图像被预测为真实，掩模预测很可能会有很多的假阳性（FP），所以我们增加二值化阈值来减少FP。另一方面，如果图像被预测为篡改，我们会降低二值化阈值以减少假阴性。给定输入图像被预测为篡改的概率P，CSC将预测掩模的二值化阈值从0.5调整到$~ min(max(1-P,\lambda),1-\lambda) $ ， $~ \lambda $ 设置为0.3。</p><h1 id="实验">5. 实验</h1><h2 id="受约束图像篡改定位ciml任务的实验">5.1受约束图像篡改定位CIML任务的实验</h2><p>  图像操作自动标注的任务可以作为一个CIML任务的评估。考虑到IMD20数据集[23]中的图像与我们要标注的目标图像非常相似，我们使用其中一部分使用IoU和F1-score来评估模型的性能。</p><h3 id="实施细节">5.1.2 实施细节</h3><p>  我们将IMD20中伪造的伪造图像分为SPG或SDG，并将它们以大约3：1的比例随机分成训练集和测试集。CASIAv2[3]和大约100万张通过使用COCO数据集合成的图像[14]也被用于训练。输入图像的大小调整为512x512，并在所有方法中应用一致的训练配置以进行公平比较。</p><h3 id="消融实验">5.1.3 消融实验</h3><p>  对于SPG，伪造图像与其真实图像之间的图像差异可以粗略地表示伪造区域，图像本身可以提供语义信息，帮助模型去噪差异映射。我们在IMD20SPG的测试集上对所提出的差异感知语义分割进行了消融实验，如表4的右侧所示，这两种方法都可以提高模型的性能。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172424444.png"alt="image-20240826172424444" /><figcaption aria-hidden="true">image-20240826172424444</figcaption></figure><p>表4。在IMD20SPG上进行的CIML实验。左：我们的差异感知语义分割的比较研究。右：对应的消融实验。“DMVN*”表示同时使用SDG和SPG数据进行训练的DMVN，类似于“DMAC*”。“Nonzero”表示使用一对图像之间的差值的非零区域，“OTSU”表示用OTSU二值化的差值。'w.o.Difference‘表示语义分割模型的输入只包含图像对，’w.o.Images‘表示只使用图像对的差异映射作为输入。“Ours(VGG)”表示我们的模型与DMAC具有相同的VGG主干。“Ours(VAN)”表示我们的模型使用VAN主干。</p><p>  对于SDG，语义对齐可以减少训练过程中的混淆，帮助我们的模型实现更好的泛化。我们在IMD20SDG的测试集上对所提出的语义对齐相关匹配进行了消融实验，结果如表5的右侧所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172752628.png"alt="image-20240826172752628" /><figcaption aria-hidden="true">image-20240826172752628</figcaption></figure><p>  显然，这两个提议的组件都有助于提高模型的更高性能。此外，所提出的质量评价评分（QES）允许自动过滤最令人满意的预测。由于IMD20的groundtruth中存在一些错误，我们的方法足以获得准确的自动注释。</p><h3 id="对qes的消融实验">5.1.4 对QES的消融实验</h3><p>  所提出的QES度量的目标是在数据集创建期间自动过滤掉糟糕的预测，其中groundtruth是不可用的。如表3所示，越高的QES阈值，精度就会越高。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173009052.png"alt="image-20240826173009052" /><figcaption aria-hidden="true">image-20240826173009052</figcaption></figure><p>  这是因为具有更大的高置信度和更清晰的边缘比率的预测大多更接近实际的groundtruth，而清晰度和置信度可以通过我们的QES很好地评估。因此，我们的QES显示了与IoU度量有很强的相关性。</p><h3 id="比较实验">5.1.5 比较实验</h3><p>  我们在与我们相同的数据上，使用它们的公共代码对DMVN [32]和DMAC[19]进行了重新训练，结果如表4和表5的左侧所示。显然，我们的方法明显优于这些以前的方法。值得注意的是，同时使用SPG和SDG数据训练的DMVN和DMAC在这两个任务上的表现都比只使用SPG或SDG数据训练的任务更差。</p><h2 id="图像篡改定位iml任务的实验">5.2 图像篡改定位IML任务的实验</h2><h3 id="实施细节-1">5.2.1 实施细节</h3><p>  我们采用ConvNeXt-Base[21]作为特征提取器，对模型进行160k次迭代的训练，批量大小为20，在按照之前的工作[6,12]进行训练时，输入大小设置为512x512。我们使用交叉熵损失和AdamW优化器[22]，学习速率从1e-4到1e-6。CASIAv2[3]和CAT-Net [12]中的合成数据集用于按照之前的工作[6,12]进行训练。</p><h3 id="对miml数据集的消融实验">5.2.2对MIML数据集的消融实验</h3><p>  除了我们的APSCNet外，我们分别用PSCC-Net [17]和CAT-Net[12]的公共代码重新训练了提出的MIML数据集。当使用MIML数据集进行训练时，我们对原始合成数据和MIML采用近似1：1的采样比，所有实验中的总训练量都是固定的，以便进行公平比较。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173603657.png"alt="image-20240826173603657" /><figcaption aria-hidden="true">image-20240826173603657</figcaption></figure><p>  如表2所示，MIML可以显著提高所有这些模型的性能，而在训练或测试过程中没有任何额外的负担。这是因为MIML可以大大缓解深度IML模型中人工伪造数据的严重短缺。为了进一步确认我们的MIML数据集的有效性，我们将IMD20数据集随机划分为10个12个样本的IMDP1和988个样本的IMDP2，用相同大小规模的IMDP1替换MIML数据集，用它们训练APSC-Net。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174020848.png"alt="image-20240826174020848" /><figcaption aria-hidden="true">image-20240826174020848</figcaption></figure><p>  如表6所示，尽管在训练中加入IMDP1减轻了领域差距，提高了模型在NIST16和IMDP2上的性能，但仍明显低于使用MIML训练的模型。显然，MIML可以通过其大量不同的手工伪造数据，显著提高深度模型的泛化能力。</p><h3 id="apsc-net的比较实验">5.3.3 APSC-Net的比较实验</h3><p>  我们在广泛使用的基准测试上比较了我们的APSC-Net与最先进的（SOTA）方法的性能。考虑到以前的方法执行不同的后处理，导致不公平（例如EVP[16]使用最佳阈值计算GT执行二值化），我们忽略了与他们所提出的方法无关的后处理和用一个固定的阈值0.5均匀地二值化预测，然后评估性能与普通IoU和F1-score指标。定量结果如表7所示，我们的APSC-Net在所有这些基准测试上都优于以前的最先进的方法。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174243317.png"alt="image-20240826174243317" /><figcaption aria-hidden="true">image-20240826174243317</figcaption></figure><p>  视觉比较的定性结果如图8所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174324363.png"alt="image-20240826174324363" /><figcaption aria-hidden="true">image-20240826174324363</figcaption></figure><h1 id="结论">6. 结论</h1><p>  在本文中，我们提出了一种新的约束图像处理定位（CIML）范例，称为CAAA，它分别处理共享探针组SPG和共享供体组SDG图像对。实验表明，该范式明显优于以往的CIML方法。在此范例下，训练后的模型被用于自动标注未标记的伪造图像，以进行图像操作定位。我们还提出了一种新的度量QES来自动排除错误的预测。因此，我们提出了一个大规模、多样化、高质量的数据集MIML，包括123,150张人工伪造的图像和像素级注释，这可以通过解决它们的数据稀缺问题来激发深度取证模型的潜力。此外，我们提出了一种新的有效模型APSC-Net用于图像操作定位。我们希望我们提出的CAAA范式、QES度量、MIML数据集和APSCNet能够为社区带来见解，并促进图像操作定位的现实应用。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</title>
      <link href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/"/>
      <url>/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>Multi-view Feature Extraction via Tunable Prompts is Enough for ImageManipulation Localization <ahref="https://openreview.net/forum?id=Ci5g2dnrMK"><imgsrc="https://img.shields.io/badge/ACMMM-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\text{Xuntao Liu},\text{YuzhouYang},\text{Haoyue Wang},\text{Qichao Ying},\\\text{ZhenxingQian}^*,\text{Xinpeng Zhang},\text{Sheng Li},\)</span></center><center>复旦大学计算机科学学院，NVIDIA上海</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/302_Multi_view_Feature_Extract.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  虚假图像可以通过社交网络服务迅速传播，构成重大风险。图像篡改定位（IML）的快速发展试图解决这个问题。然而，IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p>​  为了应对这一挑战，我们提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。<br/>​  具体来说，一组可调提示使冻结的预训练模型能够提取多视图特征，包括空间和高频特征。这种方法最大限度地减少了跨不同视图进行特征提取的冗余架构，从而降低了培训成本。<br/>​  此外，我们开发了一个即插即用的特征对齐和融合模块，该模块无缝集成到预训练的模型中，无需进行额外的结构修改。所提出的模块通过交互式处理降低了特征中的噪声和不确定性。</p><p>​  实验结果表明，我们提出的方法在6个测试数据集上取得了优异的性能，表现出卓越的鲁棒性。</p><h1 id="引言">1. 引言</h1><p>​  我们观察到，分类、目标检测和语义分割等任务具有许多具有丰富的先验知识的预训练模型，如双变压器[20]。考虑利用这些预先训练过的模型来处理IML任务中的挑战是很自然的。然而，直接将它们应用到IML任务中被证明是低效的[22]。这种低效源于IML任务的独特性质，该任务侧重于从图像中提取非语义的视觉线索和低层次的不连续性。有两个关键方面说明了这种特殊性：</p><p>​  1)高频信息：由不同的摄像机捕获的图像显示出不同的噪声模式[16]。这给伪造的图像带来不一致的噪声，而真实的区域来自不同的图像。此外，由不同网络生成的图像可能在频域[27]上存在差异。<br/>​  2)边缘信息：图像编辑的级别可能会发生变化，导致在锻造区域的边界上的锯齿状和不光滑的边缘或颜色不一致的[37]。这些细节对于精确的操作定位化至关重要，但在许多任务中经常被忽略。</p><p>​  IML-ViT[22]是在IML任务中使用基于普通ViT[6]体系结构的预训练模型的开创性尝试。它们还结合了边缘监督，将网络的注意力引向微妙的篡改伪影。然而，IML-ViT忽略了在以前的许多工作[4,14,15]中已经验证过的有效的高频信息。在IML任务中，处理多视图特征通常需要并行的主干架构[4,14]，这在参数增加的紧急情况下变得具有挑战性。此外，IML-ViT尽管利用了预先训练过的模型，但仍需要从头开始使用数据集来训练模型。这无疑对计算资源产生了巨大的需求，特别是在调优大型预训练模型方面。此外，之前的一些工作表明，在下游任务上调整大型预训练模型可能会损害模型[30]的性能，这在我们的比较实验中也可以观察到。</p><p>​  在本文中，我们提出了Prompt-IML，如图1所示，旨在通过利用预训练模型的丰富的先验知识来解决IML任务中数据集的稀缺问题。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824185716412.png" alt="image-20240824185716412" style="zoom:67%;" /></p><p>图1：Prompt-IML利用一个具有冻结参数的单一预训练主干，通过可调谐提示来处理多视图特性。特性对齐和融合模块被设计为特性交互和增强的即插即用组件。</p><p>​  具体来说，Prompt-IML遵循一个编码器-解码器架构。基于预训练模型的编码器负责特征提取，然后解码器对这些特征进行处理，以准确定位被操纵的区域。为了在不诉诸于复杂的并行架构的情况下处理有利于IML任务的多视图特性，我们建议使用可调提示集来利用预先训练好的模型作为编码器。在训练这些提示时，我们冻结了预先训练好的模型。它有三个主要的优势。首先，它允许预先训练好的模型用于处理每个视图中的特征。其次，处理后的特征保留了来自预训练模型的鲁棒性。最后，它有助于减少训练所需的计算资源。</p><p>​  此外，考虑到多视图特征之间的变化，我们提出了一个特征对齐和融合（FAF）模块。该模块被设计为即插即用组件，可以无缝集成到编码器，而不需要额外的结构修改。在FAF模块中，针对不同的优点采用了多种注意机制。FAF模块减少了特征中的噪声和不确定性，同时也抑制了零星的正响应，以确保输出一致。</p><p>​  为了公平地评估模型的能力，我们遵循了IML-ViT中概述的评估方案。它只涉及使用CASIA2数据集进行训练，然后对其他6个数据集进行测试。重要的是，我们确保了训练数据集和测试数据集之间的零数据重叠，使其成为一个跨数据集的评估。实验结果表明，所提出的Prompt-IML有效地利用了预训练模型中的先验知识，优于以往的先进方法，并表现出更强的鲁棒性。我们的贡献可以概括为三个方面：</p><p>​  我们的贡献可以总结为三个方面：</p><ul><li>我们引入了Prompt-IML来应对IML数据集的稀缺所带来的挑战。我们的方法通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性。</li><li>我们精心设计了一个即插即用的特性对齐和融合（FAF）模块，它可以无缝地集成到主干网中。它有效地减少了特征中的噪声和不确定性，同时减轻了零星的积极响应的影响。</li><li>Prompt-IML在6个测试数据集上都优于最先进的方法。我们的广泛的实验证实了我们的方法的普遍性和鲁棒性，也验证了所提出的FAF模块的有效性。</li></ul><h1 id="方法">2. 方法</h1><h2 id="方法概述">2.1 方法概述</h2><p>​  图2展示了所提出的Prompt-IML的管道设计，它遵循了通用的编码器-解码器框架。完整的传递途径包括特征提取和操作定位两个阶段。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><p>​  在特征提取阶段，我们采用预训练好的双子变压器作为骨干，并在训练过程中保持其参数冻结。同时，我们利用多组可调提示来分别调整图像的空间特征和高频特征。因此，这种方法避免了使用冗余的模型体系结构来从其他视图中提取特征。考虑到多视图特征的差异，我们提出了一个特征对齐和融合（FAF）模块进行处理。FAF模块集成在主干层之间，有效地降低了每层提取特征内的噪声和不确定性。同时，它们有助于抑制零星的积极反应，导致更一致的输出。这些模块都是即插即用的，不需要对主干本身进行任何修改。在操作定位阶段，我们使用掩模2前作为解码器，其中包括一个像素解码器和一个变压器解码器。解码器处理从前一阶段获得的多尺度特征，并产生最终的预测。</p><h2 id="特征提取阶段">2.2 特征提取阶段</h2><p>​  我们将输入图像表示为 $~\mathbf{X}\in\mathbb{R}^{\boldsymbol{h}\times\boldsymbol{w}\times3} $。为了获取空间特征的输入，我们将图像划分为指定大小的斑块：</p><p><spanclass="math display">\[\mathrm{F}_{0}^{RGB}=\mathrm{Norm}(\mathrm{Conv}(\mathrm{X}))+\mathrm{F}_{PE},\]</span>​  其中 $~ \mathbf{F}_0^{RGB}\in\mathbb{R}^{H\times W\times C} $，Conv表示分区操作， $~ \mathrm{F}_{PE} $是一个可学习的位置嵌入。接下来，我们使用一组具有不同大小的内核的BayarConv来提取高频特征：<spanclass="math display">\[\mathbf{F}_{0}^{HFQ}=\mathrm{Concat}(\{\mathrm{BayarConv}_{\mathrm{i\timesi}}(\mathbf{X})\}), i\in\{3,5,7\},\]</span> ​  其中 $~\mathbf{F}_0^{HFQ}\in\mathbb{R}^{H\times W\times C} $，i表示内核大小。所获得的特征将被发送到骨干网中以进行进一步的处理。</p><h3 id="具有可调调提示的多视角特征处理">2.2.1具有可调调提示的多视角特征处理</h3><p>​  我们采用预先训练的语义分割（SS, semanticsegmentation）任务中常用的Swin-Transformer作为主干，原因如下：<br/>​  1)Swin-Transformer包括一个与图像大小相比具有线性时间复杂度的窗口注意设计；<br/>​  2)补丁合并操作可以生成多尺度特征图，这在IML任务[4,11]中被证明是重要的。<br/>​  3)SS任务和IML任务有一些相似之处，因为它们本质上是像素级的分类任务。<br/>​  我们认为，用于SS任务的预训练模型，经过微调后，更有利于实现精确的像素级操作定位。<br/>​  Swin-Transformer包括4层，并具有特定分辨率的输出特征。我们将第i层的输出特征表示为$ F_i $ ：</p><p><spanclass="math display">\[\mathbf{F}_{i}=\mathrm{Layer}_{\mathrm{i}}\left(\mathbf{F}_{i-1}\right)\in\mathbb{R}^{(H_{i}\timesW_{i})\times C_{i}},i\in\{1,2,3,4\},\]</span></p><p>​  其中， $~H_{i}=\frac{H}{2^{i-1}},W_{i}=\frac{W}{2^{i-1}},C_{i}=C*2^{i-1} $ ， ${Layer}_i $ 象征着 Swin-Transformer的第i层。</p><p>​  我们采用了一种提示调优方法[12]，使一个单一的预训练模型能够同时处理空间和高频特征。具体来说，在训练过程中，我们在每一层利用两组提示分别处理空间特征和高频特征，同时冻结主干的参数。我们将第i层的输入特征表示为$~ F^{RGB}_{i-1} $ 和 $~ F^{HFQ}_{i-1} $ 。它们先重塑为 $~\mathbb{R}^{(H_{i-1}\times W_{i-1})\times C_{i-1}} $ ，然后分别加入提示$~ P^{RGB}_{i-1} $ 和 $~ P^{HFQ}_{i-1}\in\mathbb{R}^{n_{p}\timesC_{i-1}} $ 。因此，公式3的每一层过程已被变更为： <spanclass="math display">\[\mathbf{F}_{i}^{RGB}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{RGB},\mathbf{F}_{i-1}^{RGB}\right]\right),\\\mathbf{F}_{i}^{HFQ}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{HFQ},\mathbf{F}_{i-1}^{HFQ}\right]\right),\]</span>​  其中，[·]表示Concat操作。</p><h3 id="特征对齐和融合模块">2.2.2 特征对齐和融合模块</h3><p>​  针对骨干处理的空间和高频特征，我们提出了一个特征对齐和融合的FAF模块。FAF模块集成在主干网的一些相邻层之间，如图2所示。FAF模块由对齐阶段[36]和融合阶段组成，详细的组成和过程如图3所示。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826110222896.png"alt="image-20240826110222896" /><figcaption aria-hidden="true">image-20240826110222896</figcaption></figure><p>图3：所提出的特性对齐和融合（FAF）模块的设计和可调提示的使用。通道，空间，可变形分别表示等式5，等式6和等式9的过程。</p><h4 id="特征对齐阶段">特征对齐阶段</h4><p>​  在特征对齐阶段，我们同时利用通道注意和空间注意来研究特征的通道间和空间间的相关性，从而利用相应的信息增强特征。未经处理的特征从增强的特征中收集信息，减少了潜在的不确定性和噪声。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824221130411.png" alt="image-20240824221130411"  /></p><p>​  具体来说，我们首先使用平均池化操作（用上划线表示）来聚合信息。然后，将它们连接到$ C_i $ 的维数上，即[·]表示，并输入到MLP层以生成信道注意向量 $~\mathbf{W}_{i}^{C_{RGB}} $ ， $~\mathbf{W}_{i}^{C_{HFQ}}\in\mathbb{R}^{1\times1\times C_{i}} $。上述过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{C_{RGB}},\mathbf{W}_{i}^{C_{HFQ}}&amp;=\mathrm{ChannelAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}(\mathrm{MLP}([\overline{\mathbf{F}_{i}^{RGB}},\overline{\mathbf{F}_{i}^{HFQ}}])),\end{aligned}\]</span></p><p>​  其中，Split是Concat的反向操作。为了获得空间注意向量，我们利用两个1×1卷积与一个中间的ReLU层，用$ g() $ 表示，来聚合空间信息。获取空间注意向量 $~\mathbf{W}_{i}^{S_{RGB}} $ ， $~\mathbf{W}_{i}^{S_{HFQ}}\in\mathbb{R}^{H_i\times W_i\times1} $的过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{S_{RGB}},\mathbf{W}_{i}^{S_{HFQ}}&amp;=\mathrm{SpatialAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}\left(\mathrm{Conv}\left(g\left(\mathrm{Conv}\left(\left[\mathrm{F}_{i}^{RGB},\mathrm{F}_{i}^{HFQ}\right]\right)\right)\right)\right).\end{aligned}\]</span>​  最后，我们通过应用交叉注意向量对来自不同分支的特征进行对齐，通过元素级添加为下一个主干层产生输入：<spanclass="math display">\[\begin{aligned}&amp;\mathbf{F}_{i}^{C_{RGB}}=\mathbf{W}_{i}^{C_{RGB}}\odot\mathbf{F}_{i}^{RGB},\quad\mathbf{F}_{i}^{S_{RGB}}=\mathbf{W}_{i}^{S_{RGB}}\odot\mathbf{F}_{i}^{RGB},\\&amp;\mathbf{F}_{i}^{C_{HFQ}}=\mathbf{W}_{i}^{C_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\quad\mathbf{F}_{i}^{S_{HFQ}}=\mathbf{W}_{i}^{S_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\\&amp;\mathbf{F}_{i}^{RGB}:=\mathbf{F}_{i}^{RGB}+\mathbf{F}_{i}^{C_{HFQ}}+\mathbf{F}_{i}^{S_{HFQ}},\\&amp;\mathbf{F}_{i}^{HFQ}:=\mathbf{F}_{i}^{HFQ}+\mathbf{F}_{i}^{C_{RGB}}+\mathbf{F}_{i}^{S_{RGB}}.\end{aligned}\]</span>#### 特征融合阶段</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112404970.png"alt="image-20240826112404970" /><figcaption aria-hidden="true">image-20240826112404970</figcaption></figure><p>​  在特征融合阶段，我们首先利用不同膨胀率的扩张卷积DConv来处理特征图，增强斑块内的相互作用。具体来说，我们使用膨胀速率k∈{1,3,5}进行处理，然后在维度of$ C_i $上连接输出。对这些连接起来的特征进行处理，以整合信息和未经处理的特征：<spanclass="math display">\[\tilde{\mathrm{F}}_{i}=\mathrm{Conv}\left(\left[\mathrm{Conv}\left(\left[\mathrm{DConv}_{\mathrm{k\timesk}}\left(\mathbf{F}_{i}\right)\right]\right),\mathbf{F}_{i}\right]\right),k\in\{1,3,5\}.\]</span></p><p>​  然后，我们应用可变形注意力机制来促进多视图块间的信息交互。可变形注意机制不仅通过可学习偏移量采样降低了计算复杂度，还有助于抑制特征图中的零星积极反应，这有助于定位，因为篡改操作通常影响像素的特定区域，而不是孤立的特定区域[4]。从上一步的$ ~\mathbf{\tilde{F}}_{i}^{RGB} $ 和 $~ \mathbf{\tilde{F}}_{i}^{HFQ} $处理得到特征：</p><p><spanclass="math display">\[\mathbf{attn}^{RGB}=\mathrm{DeformAttn}_{1}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{RGB},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{HFQ}\right),\\\mathbf{attn}^{HFQ}=\mathrm{DeformAttn}_{2}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{HFQ},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{RGB}\right),\\\mathbf{F}_{i}^{d}=\gamma_{1}\cdot\left(\tilde{\mathbf{F}}_{i}^{RGB}+\mathbf{attn}^{RGB}\right)+\gamma_{2}\cdot\left(\tilde{\mathbf{F}}_{i}^{HFQ}+\mathbf{attn}^{HFQ}\right),\]</span></p><p>​  其中， $~ \gamma_{1} $ ， $~ \gamma_{2} $ 是可学习的参数。输出的 $~F^d_i $ 用于解码器。</p><h2 id="篡改定位阶段">2.3 篡改定位阶段</h2><p>​  为了细化上一阶段获得的多尺度特征，我们使用Mask2Former[3]作为解码器，它包括两个关键组件：像素解码器和Transformer解码器。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112712079.png"alt="image-20240826112712079" /><figcaption aria-hidden="true">image-20240826112712079</figcaption></figure><p>​  像素解码器负责逐步向上采样特征从低分辨率到高分辨率。Transformer解码器利用查询嵌入和多尺度特性进行定位。这种方法有几个优点。首先，利用多尺度特征有利于定位小的篡改区域。此外，掩膜注意力的查询嵌入的整合有助于限制交叉注意对被篡改区域的单独关注，从而增强了与篡改相关的特征提取。</p><h2 id="损失函数">2.4 损失函数</h2><p>​  考虑到被篡改区域的边界可能表现出锯齿状、非平滑的边缘和颜色不一致，我们从IML-ViT[22]中汲取灵感，并引入了边缘监督。具体来说，我们使用形态学操作，如侵蚀和膨胀操作，来处理掩模M并生成相应的边缘掩模$ M^★ $。与利用网络生成边缘预测[4]的方法相比，该策略不仅包含了边缘信息，同时还消除了调整骨干的需要，增强了其灵活性。损失函数包括两个分量，每个分量对应于对预测结果和预测边缘的监督：<spanclass="math display">\[\mathcal{L}=\mathcal{L}_{seg}(M_{gt},M_{pred})+\lambda\mathcal{L}_{edge}(M_{gt}^{\star},M_{pred}^{\star})\]</span></p><h1 id="实验">3 实验</h1><h2 id="实验设置">3.1 实验设置</h2><h3 id="数据集">数据集</h3><p>​  我们采用了一个关于IML任务的通用训练协议[2,22,37]，以促进模型性能的公平比较，并避免了私有合成数据集的影响。我们仅使用CASIA2[5]来训练Prompt-IML。6个公共测试数据集用于评估，包括CASIA1 [5]、NIST16[7]、COVERAGE[32]、Columbia[25]、IMD2020[26]和DEFACTO[23]。在MVSS-Net[2]之后，我们对来DEFACTO的抽样子数据集进行了测试，其中包含6000张真实图像和6000张经过处理的图像。评估构成了跨数据集分析，因为我们的训练集和测试数据集之间没有重叠。</p><h3 id="评估标准">评估标准</h3><p>​  我们使用像素级的F1分数来评估我们的模型在测试数据集上的性能。以往的一些方法采用了以最优阈值优化F1分数的策略，为每幅图像选择不同的阈值。然而，最优阈值的决定需要地面真实数据，这在现实场景中是不可行的。因此，我们以固定的阈值报告f1分数，它独立于模型本身，并提供了一个公平的模型性能评估。</p><h3 id="实施细节">实施细节</h3><p>​  我们在RTX 3090GPU上训练我们的模型 Prompt-IML80轮次，每个GPU的批处理大小为2。编码器和解码器都在COCO[17]上使用预先训练的权值进行初始化。除非另有说明，所有图像的大小都会被调整为1024×1024。在IML-ViT[22]之后，我们使用了简单和公共的数据增强技术，包括翻转、模糊、旋转、JPEG压缩、随机复制移动和在单个图像中矩形区域进行图像修复。我们使用基本学习速率为1×10−4的AdamW[21]优化器，并利用余弦衰减策略来调度学习速率。</p><h2 id="性能比较">3.2 性能比较</h2><p>​  我们将我们的方法与其他8种最先进的方法进行了比较，以全面评估我们的方法，并在表1中报告了F1评分。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224337339.png"alt="image-20240824224337339" /><figcaption aria-hidden="true">image-20240824224337339</figcaption></figure><p>​  我们可以观察到，我们的方法对每个改进的数据集的最佳基线分别为2.8%、4.6%、7.6%、8.1%和4.9%。与次优基线IML-ViT[22]相比，它平均提高了4.3%。这些都充分证明了我们的模型的优越性。然而，在COVER[32]数据集上，基于MVSS-net的方法[2,4]的性能优于所有其他方法。COVER是一个仅通过复制-移动技术创建的小型伪造图像数据集，大多数检测线索位于伪造区域的边界周围。因此，我们将这种现象归因于他们精心设计的边缘信息提取结构和数据增强技术。</p><p>​  此外，图4显示了每个模型的预测定位结果，每个图像来自一个不同的数据集，在操纵区域有很大的变化。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224529342.png"alt="image-20240824224529342" /><figcaption aria-hidden="true">image-20240824224529342</figcaption></figure><p>​  研究结果强调了该方法具有显著的泛化能力，表明该方法可以有效地利用嵌入在预训练模型中的先验知识来检测篡改痕迹。</p><h2 id="鲁棒性">3.3 鲁棒性</h2><p>​  在本节中，我们利用6个测试数据集来全面评估Prompt-IML的鲁棒性。在IML-ViT[22]之后，我们应用两种常见的攻击方法，即JPEG压缩和高斯模糊，在不同的扰动级别上，来创建被攻击的图像。计算结果如图5所示。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224656206.png"alt="image-20240824224656206" /><figcaption aria-hidden="true">image-20240824224656206</figcaption></figure><p>​  在JPEG压缩测试中，提出的pt-IML在4个数据集上保持明显的优势。在COVER和NIST16上，我们的方法与领先的方法非常接近。在高斯模糊测试中，Prompt-IML在所有数据集上都显著优于其他方法。</p><p>​  总的来说，与其他方法相比，Prompt-IML表现出了承受JPEG压缩和高斯模糊的显著能力，特别是针对后者。我们还注意到，IML-ViT比其他方法表现出更好的平均鲁棒性，因此我们将我们的方法的鲁棒性归因于更有效地利用大规模的预训练模型，因为这些模型可以学习更鲁棒的特征，因为它们存在于广泛的训练数据集。</p><p>​  值得注意的是，与IML-ViT相比，该方法在抵抗高斯模糊攻击方面有了显著的性能提高。我们认为，这些优势源于高频特性和促进调优的使用，从而导致了以下推测。首先，IML-ViT对预先训练好的网络进行了完全的微调，这可能会由于灾难性遗忘[30]而损害其鲁棒性。此外，不同特性对各种攻击的抵抗力也各不相同，因此充分利用多视图特性可能有助于提高该方法的鲁棒性。</p><h2 id="消融研究">3.4 消融研究</h2><p>​  我们按照表2中概述的设置进行了几个实验，以彻底评估我们的方法中各模块的有效性。我们报告了每个模型在COVER[32]、NIST16[7]和IMD20 [26]上的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225020324.png"alt="image-20240824225020324" /><figcaption aria-hidden="true">image-20240824225020324</figcaption></figure><h3 id="多视图特征的影响">多视图特征的影响</h3><p>​  在设置2中，我们使用一个具有冻结参数的单一主干，同时从图像中提取空间和高频特征。与仅使用空间特征的设置1相比，我们观察到高频特征的利用率分别增加了2.5%、1.9%和2.7%的F1分数，有效地证明了利用预先训练好的模型来处理多视图特征的可行性。</p><h3 id="faf模块的影响">FAF模块的影响</h3><p>​  所提出的FAF模块包括两个独立的阶段：对齐和融合。因此，我们使用设置3和设置4分别来验证每个阶段的有效性。在设置4中，我们跳过特征对齐阶段，并直接将特征传递到下一层。与设置5相比，我们注意到，当没有特征对齐阶段缺失时，所有三个数据集的F1得分都下降，分别下降了3.6%、2.9%和3.0%。在设置3中，我们跳过特征融合阶段，直接添加多视图特征作为融合特征。与设置5相比，特征融合阶段的缺失导致F1评分分别下降了3.0%、3.1%和7.1%。这些结果有效地证明了FAF模块通过特征之间的信息交互成功地增强了特征。</p><h2 id="预训练主干网络的选择">3.5 预训练主干网络的选择</h2><p>​  我们研究了选择不同的预训练模型作为骨干的影响。我们使用CLIP[28]，MAE[9]，SAM[13]和Swin-Transformer[20]。CLIP和MAE都采用了普通ViT的架构，而SAM则类似于Swin-Transformer。考虑到全局自注意机制的计算需求，特别是对大图像，我们将所有图像的大小调整为512×512来进行比较。此外，由于普通ViT输出的特征图大小固定，我们在FAF模块的每个融合阶段结束时合并了几个卷积，以与解码器的输入需求对齐。我们在表3中报告了不同主干网络的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225456518.png"alt="image-20240824225456518" /><figcaption aria-hidden="true">image-20240824225456518</figcaption></figure><p>​  在COCO[17]数据集上训练的语义分割任务的Swin-Transformer模型，提供了优越的结果。我们将这种成功主要归因于它在不同层的不同接受域，使它能够发现微妙的篡改痕迹。虽然CLIP是在一个大的数据集上进行预先训练的，但它强调文本和图像特征之间的对齐，因此单独使用图像编码器可能不是最佳的选择。此外，我们假设在SAM中的窗口注意机制的实现可能会限制其在低分辨率图像上的性能。因此，我们选择预先训练过的Swin-Transformer作为我们的主干网络。</p><h2 id="提示调优v.s.完全调优">3.6 提示调优v.s.完全调优</h2><p>​  我们比较了两种方法，提示调优和完全调优，用于将预先训练好的模型适应IML任务，并评估它们对模型性能的影响。当使用完全调优方法时，作为单一主干网络在同时处理空间和高频特征方面面临限制，我们按照[2,14]的指导方针将主干调整为双分支架构。表4显示了不同调优方法的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225735253.png"alt="image-20240824225735253" /><figcaption aria-hidden="true">image-20240824225735253</figcaption></figure><p>​  完全调优并不会在大多数数据集上带来显著的性能提高，但在COVER数据集上显示出了19.1%的显著改进。我们将这种异常现象归因于COVER数据集的小规模及其使用篡改技术的单一。尽管完全调优显示出一定程度的性能改进，但提示调优相比，双分支结构引入了更多的可训练参数。为了进行更直接的比较，我们不计算FAF模块和解码器的可学习参数，因为它们都包含在这两种方法中。对于提示调优的可学习参数的大小为0.09M，而对于完全调优的双分支主干的大小为93.14M。因此，快速调优更有利于适应大型模型的开发和处理多视图特性。</p><h1 id="结论">4 结论</h1><p>​  在本文中，我们探讨了利用现有的预训练模型来解决IML任务中公共可用数据集的稀缺性的潜力。我们提出了Prompt-IML，它利用单一的预先训练的网络通过可调提示提取多视图特征。采用专门设计的特征对齐融合（FAF）模块集成多视图特征，有效降低了特征的噪声和不确定性，抑制了零星的积极响应。在6个测试数据集上进行的大量实验表明，Prompt-IML具有优异的性能、更好的泛化能力和更高的鲁棒性。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DH-GAN:Image manipulation localization via a dual homology-aware generative adversarial network</title>
      <link href="/DH-GAN/"/>
      <url>/DH-GAN/</url>
      
        <content type="html"><![CDATA[<p>DH-GAN: Image manipulation localization via a dual homology-awaregenerative adversarial network<br/><spanclass="math inline">\(\text{Weihuang Liu}^1,\text{XiaodongCun}^1,\text{Chi-Man Pun}^*\)</span><br/>澳门大学科技学院计算机与信息科学系，澳门凼仔</p><h1 id="摘要">摘要</h1><p>图像操作定位是一种二值分割任务，对被篡改的伪影敏感，而不是物体的对象。因此，传统的方法和基于学习的方法都高度依赖于手工制作的特征。然而，这些特定定义的特征限制了网络对一般场景的能力。</p><p>为了解决这一问题，我们提出了一个双同源感知生成对抗网络（DH-GAN, dualhomology-aware generative adversarialnetwork），这是一种新的基于gan的框架来定位被操纵的区域。首先，我们通过使用选择性金字塔生成器重新校准多尺度编码特征来定位伪造区域。然后，我们在鉴别器中进行同源性识别。所提出的同源识别鉴别器包含一堆掩码卷积（MConv,maskedconvolution）层，并学习以硬门控的方式识别预测/目标掩蔽图像上的分割像素的真实/虚假。总的来说，这些网络是在一个标准的GAN下进行优化的。实验表明，该方法在四种流行的图像处理数据集上都优于其他最先进的算法。</p><p>我们的主要贡献总结如下：</p><ul><li>我们提出了一种双同源感知生成对抗网络（DH-GAN），这是一种新的基于gan的框架，用于由两个同源感知的鉴别器进行图像操作定位。</li><li>在生成器中，我们遵循编码器-解码器结构，设计了选择性金字塔（SAP），它使用融合的多阶段动作机制来选择和重新校准多尺度特征。</li><li>在每个同源感知鉴别器中，我们提出了一个基于mconv的网络，利用预测掩模与地面真实掩模来识别分割像素的同源性。</li><li>我们的方法在几个图像伪造检测基准上取得了最先进的性能。</li></ul><figure><img src="../postimages/DH-GAN/image-20240824153543322.png"alt="image-20240824153543322" /><figcaption aria-hidden="true">image-20240824153543322</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153751891.png"alt="image-20240824153751891" /><figcaption aria-hidden="true">image-20240824153751891</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153820459.png"alt="image-20240824153820459" /><figcaption aria-hidden="true">image-20240824153820459</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IAD</title>
      <link href="/IAD/"/>
      <url>/IAD/</url>
      
        <content type="html"><![CDATA[<h1 id="cv-surveys">2024-CV-Surveys</h1><h2 id="industrial-anomaly-detection工业缺陷检测">Industrial AnomalyDetection(工业缺陷检测)</h2><ul><li><a href="https://arxiv.org/abs/2401.15448">A Systematic Review ofAvailable Datasets in Additive Manufacturing</a><br/> [2024-01-30]</li><li><a href="https://arxiv.org/abs/2406.07880">A Comprehensive Survey onMachine Learning Driven Material Defect Detection: Challenges,Solutions, and Future Prospects</a><br/> [2024-06-13]</li><li><a href="https://arxiv.org/abs/2406.07694">A PRISMA DrivenSystematic Review of Publicly Available Datasets for Benchmark and ModelDevelopments for Industrial Defect Detection</a><br/> [2024-06-13]</li><li>VAD<br/> - <a href="https://arxiv.org/abs/2401.16402">A Survey onVisual Anomaly Detection: Challenge, Approach, and Prospect</a><br/>[2024-01-30]</li><li>点云的工业系统 3D 缺陷检测和分类<br/> - <ahref="https://arxiv.org/abs/2402.12923">Advancements in PointCloud-Based 3D Defect Detection and Classification for IndustrialSystems: A Comprehensive Survey</a><br/> [2024-02-21]</li></ul><h1 id="cv-surveys-1">2023-CV-Surveys</h1><h2 id="anomaly-detection">Anomaly Detection</h2><ul><li><a href="https://arxiv.org/abs/2301.11514">Deep Industrial ImageAnomaly Detection: A Survey</a><br/> [2023-01-30]<br/> ⭐<ahref="https://github.com/M-3LAB/awesome-industrial-anomaly-detection">code</a></li></ul><h1 id="cv-surveys-2">2022-CV-Surveys</h1><h2 id="工业异常检测">工业异常检测</h2><ul><li><a href="https://arxiv.org/abs/2204.11161">A Survey on UnsupervisedIndustrial Anomaly Detection Algorithms</a><br/> [2022-04-26]</li></ul><h1id="promptad-learning-prompts-with-only-normal-samples-for-few-shot-anomaly-detection">PromptAD:Learning Prompts with only Normal Samples for Few-Shot AnomalyDetection</h1><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h2 id="摘要">摘要</h2><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h2 id="引言">引言</h2><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/IAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h2 id="前期准备工作">前期准备工作</h2><h3 id="clip和提示学习">CLIP和提示学习</h3><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h3 id="clip-surgery">CLIP Surgery</h3><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h2 id="方法论">方法论</h2><h3 id="概观">概观</h3><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><img src="./../postimages/IAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h3 id="语义连接">语义连接</h3><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h2 id="显式异常边缘">显式异常边缘</h2><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h3 id="异常检测">异常检测</h3><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h2 id="实验">实验</h2><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h3 id="数据集">数据集</h3><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h3 id="评估指标">评估指标</h3><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h3 id="实施细节">实施细节</h3><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p><h1 id="open-set-supervised-anomaly-detection">Open-set SupervisedAnomaly Detection</h1><p>Anomaly Heterogeneity Learning for Open-set Supervised AnomalyDetection</p><figure><img src="./../postimages/IAD/640.png" alt="图片" /><figcaption aria-hidden="true">图片</figcaption></figure><p>-这篇文章探讨了一种新兴的异常检测领域——开放集监督异常检测（Open-setSupervised Anomaly Detection,OSAD）。OSAD的目标是利用训练过程中所见异常类别的少量样本，来检测未见过的异常（即来自开放集异常类别的样本），同时有效地识别已见过的异常。现有的OSAD方法虽然能够通过所见异常的先验知识大幅减少误报错误，但它们通常在封闭集设置下训练，并且将异常样本视为来自同质分布，这限制了它们对来自任何分布的未见过异常的泛化能力。</p><p>-为了解决这一问题，文章提出了一种名为异常异质性学习（AnomalyHeterogeneity Learning,AHL）的新方法。AHL通过模拟多样化的异常分布，并利用这些分布来学习统一的异常模型，以在替代的开放集环境中进行学习。AHL是一个通用框架，现有的OSAD模型可以轻松地插入并使用，以增强它们的异常建模能力。通过在九个真实世界的异常检测数据集上的广泛实验，AHL不仅在检测已见和未见过的异常方面显著增强了不同的最新OSAD模型，而且能够有效地泛化到新领域中的未见过异常。</p><p>-文章首先介绍了异常检测的背景和挑战，然后详细阐述了AHL的框架和方法。AHL包括两个主要组件：异质异常分布生成（HeterogeneousAnomaly Distribution Generation,HADG）和异常异质性的协同可微学习（Collaborative Differentiable Learning,CDL）。HADG组件通过将正常样本的不同簇与随机选择的异常样本相结合，模拟并生成了多样化的异常分布数据集。CDL组件则设计为使用T个基础模型学习这些异常分布，并通过迭代验证和调整模型来优化统一的异常检测模型。</p><p>-此外，文章还提出了一种自监督的泛化性估计方法，以适应性地调整模型训练过程中每个学习到的异常分布的重要性。通过这种方式，AHL能够动态地评估基础模型的泛化能力，并据此调整它们在统一模型更新中的权重。</p><p>-在实验部分，作者展示了AHL在多个数据集上的性能，并与其他最新技术进行了比较。结果表明，AHL在检测同一领域和跨领域设置中的未见过异常方面，都取得了显著的性能提升。文章最后对AHL进行了深入的分析，包括对AHL组件的效用、少数样本的实用性以及超参数的敏感性进行了研究，并得出了有价值的结论。</p><p>-总的来说，这篇文章为开放集监督异常检测领域提供了一种新的视角和强大的工具，通过学习异常的异质性，显著提高了模型对未知异常的检测能力和泛化性。</p><h3 id="摘要-1">摘要</h3><p>开放集监督异常检测（OSAD）是一个最近出现的异常检测领域，其目的是利用训练过程中看到的一些异常类的样本来检测不可见的异常（即来自开放集异常类的样本），同时有效地识别可见的异常。得益于所见异常所说明的先验知识，目前的OSAD方法往往可以大大减少假阳性误差。然而，这些方法是在封闭集设置中训练的，并将异常例子视为齐次分布，使得它们在推广到可以从任何分布中得出的看不见的异常时效果较差。本文提出利用有限异常实例学习异质异常分布来解决这一问题。为此，我们引入了一种新的方法，即异常异质性学习（AHL），它模拟了一组不同的异构异常分布，然后利用它们在替代开放集环境中学习一个统一的异构异常模型。此外，AHL是一个通用的框架，现有的OSAD模型可以即插即用，以增强其异常建模。在9个真实世界异常检测数据集上的广泛实验表明，AHL可以1)显著增强不同的最先进的OSAD模型来检测可见和不可见的异常，2)有效地推广到新领域的不可见异常。</p><h3 id="引言-1">引言</h3><p>开放集监督AD（OSAD）是一个新兴的领域，旨在利用这些有限的训练异常数据学习广义模型来检测看不见的异常（即来自开放集异常类的样本），同时有效地识别那些可见的异常（即类似于训练异常例子的异常）。针对这个OSAD问题[1,15,24,32,68]，已经引入了许多方法。得益于由所看到的异常情况所说明的先验知识，当前的OSAD通常可以极大地减少假阳性误差。</p><figure><img src="./../postimages/IAD/QQ_1721787242975.png"alt="QQ_1721787242975" /><figcaption aria-hidden="true">QQ_1721787242975</figcaption></figure><p>目前的OSAD方法的一个问题是，它们将异常例子视为均匀分布，如图1(a)所示，这在很大程度上限制了它们在检测看不见异常方面的性能。这是因为异常可以由广泛的条件产生，并且天生是无界的，从而导致非均匀的异常分布（即，异常可以从非常不同的分布中得出)。例如，肿瘤图像可以根据肿瘤的性质，在外观、形状、大小、位置等方面显示出不同的特征。目前的OSAD方法忽略了这些异常的异质性，如果它们来自于与所看到的异常不同的数据分布，则往往无法检测到异常。</p><p>为了解决这个问题，我们建议用有限的训练异常例子来学习异构异常分布。这些异常只是可见异常类的例子，它们并不能说明所有可能的异常类的分布，例如，那些看不见的异常类，这使得在有限的异常信息下学习潜在的异构异常分布具有挑战性。这项工作引入了一个新的框架，即异常异质性学习（AHL），来解决这一挑战。如图1(b)所示，它首先通过将正态样本的细粒度分布与随机选择的异常样本关联起来，来模拟各种非均匀异常分布。然后AHL执行协作可微学习，综合所有这些异常分布，以学习异构异常模型。进一步，生成的异常数据使我们的模型的训练代理开放环境中，其中异常分布的一部分用于模型训练而其他作为看不见的数据来验证和调整模型，导致更好的广义模型比当前方法训练在一个封闭的设置。此外，模拟的异常分布通常具有不同的质量。因此，在AHL中设计了一种自监督泛化估计，以自适应地调整模型训练过程中每个学习到的异常分布的重要性。</p><p>AHL的另一种简单的替代方法是，在模拟的异构数据分布上，基于同构/异构OSAD模型的简单集成来建立一个集成模型。然而，这样的集合没有考虑到在基础模型中捕获的异常异质性的共性和差异，导致了对异质性的次优学习(Sec。4.5.2).</p><p>因此，本文做出了四个主要贡献。</p><p><strong>框架。</strong>我们提出了异常异质性学习（AHL，AnomalyHeterogeneityLearning），一个新的OSAD框架。与目前将训练异常例子视为均匀分布的方法不同，AHL通过这些有限的例子来学习异构异常分布，从而能够对不可见的异常进行更广义的检测。</p><p><strong>新的模型。</strong>我们进一步将AHL框架实例化为一个新的OSAD模型。该模型使用一组不同的模拟异构异常分布对异常异质性进行协同可微学习，促进了在替代开放集环境中对模型的迭代验证和调优。这使得比简单的集成方法更最优的异常异质性学习。</p><p><strong>通用的。</strong>我们的模型是通用的，其中来自不同OSAD模型的特性和损失函数可以即插即用，并获得显著提高的检测性能。</p><p><strong>具有较强的泛化能力。</strong>在9个真实世界的AD数据集上进行的实验表明，AHL在检测同域和跨域设置中看不见的异常方面大大优于最先进的模型。</p><h3 id="异常异质性学习">异常异质性学习</h3><h4 id="问题陈述">问题陈述：</h4><p>我们假设有一组训练图像和注释<spanclass="math inline">\(\left\{\omega_i,y_i\right\}_{i=1}\)</span>，其中$<em>i^{HWC}<span class="math inline">\(表示图像RGB通道和\)</span>y</em>{i}{0,1}<span class="math inline">\(表示一个图像级类标签，\)</span>y_{i}=1<spanclass="math inline">\(时\)</span><em>{i}<spanclass="math inline">\(为异常，\)</span>y</em>{i}=0<spanclass="math inline">\(时相反。由于异常的粗糙性，标记数据通常主要由正常数据表示。给定现有的AD模型f（·），可以用来提取低维图像特征来构造训练特征集\)</span>={_i,y_i}<spanclass="math inline">\(，其中\)</span>_i<sub>=</sub>f(_i)<sub></sub><spanclass="math inline">\(表示对应的第i个图像特征，\)</span>_n ={_1,_2,...,_N}<span class="math inline">\(和\)</span>_a = {_1,_2,...,_M}(N M)<spanclass="math inline">\(分别表示正常和异常图像的特征集，那么我们提出的AHL框架的目标是学习一个异常检测函数\)</span>g:<spanclass="math inline">\(，它能够为来自不同分布的异常图像分配更高的异常分数。请注意，在OSAD中，训练异常Xa来自于可见的异常类S，它只是C的一个子集，在推理过程中可以包含一个更大的异常类集，例如，\)</span>$。</p><h4 id="我们的方法概述">我们的方法概述</h4><p>我们的AHL框架的关键思想是通过对嵌入在不同模拟异常分布中的异常的协作可微学习，来学习一个统一的异常异质性模型。</p><figure><img src="./../postimages/IAD/QQ_1721788581713.png"alt="QQ_1721788581713" /><figcaption aria-hidden="true">QQ_1721788581713</figcaption></figure><p>图2。我们的方法AHL的概述。它的HADG组件首先从训练集<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中生成T个异构异常分布数据集，每个训练集都包含一个支持集和开放集查询集，即<spanclass="math inline">\(\mathcal{D}_i=\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>。然后利用它们在模拟的开放集环境中学习T个异构AD模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，并通过协作微分学习（CDL）将这些异构异常模型合成为一个统一的AD模型g（·）。不同的ϕi学习不同质量的异常分布，因此我们还设计了一个模型ψ（·），为每个ϕi分配一个重要性评分，以增强CDL成分。</p><p>如图2所示，AHL由两个主要组成部分组成：非均匀异常分布产生（HADG,Heterogeneous Anomaly DistributionGeneration）和异常异质性的协同可微分学习（CDL, CollaborativeDifferentiable Learning）。</p><p>具体来说，HADG组件从训练集<span class="math inline">\(\mathcal{T} =\{\mathcal{D}_i\}_{i=1}^T\)</span>模拟并生成T个异构分布数据集，每个Di包含正态数据子集和随机采样异常例子的混合。每个Di都是以一种代表不同于其他异常分布的方式生成的。然后设计CDL学习一个统一的异构异常检测模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>，该模型合成了一组T基模型，记为<spanclass="math inline">\(\left\{\phi_i\big(\mathcal{D}_i;\theta_i\big)\right\}_{i=1}^T\)</span>，其中<spanclass="math inline">\(\theta_{g}\)</span>和<spanclass="math inline">\(\theta_{i}\)</span>分别表示统一模型g和基模型<spanclass="math inline">\(\phi_{i}\)</span>的可学习权值参数，每个<spanclass="math inline">\(\phi_i:\mathcal{D}_i\to\mathbb{R}\)</span>从一个异常分布中学习进行异常评分。权重参数<spanclass="math inline">\(\theta_{g}\)</span>基于基础模型权重<spanclass="math inline">\(\{\theta_i\}_{i=1}^T\)</span>协同更新。此外，单个基模型的有效性差异很大，因此如果估计相应的基模型ϕi具有较小的泛化误差，则在CDL中添加一个模块ψ，以增加θi在协同权重更新中的重要性。在推理过程中，仅使用统一的异构异常模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>进行异常检测。</p><p>AHL是一个通用的框架，其中可以轻松地插入现成的OSAD模型来实例化ϕi，并获得显著提高的性能。</p><h4 id="非均匀异常分布产生hadg">非均匀异常分布产生HADG</h4><p>学习潜在的复杂异常的一个主要挑战是缺乏说明不同可能的异常分布的训练数据。我们的HADG组件是为了解决这一挑战，我们将正常范例划分为不同的簇，并将每个正常范例与随机抽样的异常示例关联起来，以创建不同的异常分布。由此产生的分布在正常模式和/或异常模式方面彼此不同。具体来说，HADG生成T个训练异常分布数据集，<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>，每个<spanclass="math inline">\(\mathcal{D}_i = \mathcal{X}_{n,i} \cup\mathcal{X}_{a,i}\)</span>，其中<spanclass="math inline">\(\mathcal{X}_{n,i}\subset\mathcal{X}_n\)</span>和<spanclass="math inline">\(\mathcal{X}_{a,i}\subset\mathcal{X}_a\)</span>。为了模拟高质量的异常分布，我应该代表一个主要的正态模式。为此，HADG采用聚类方法将Xn划分为C聚类，然后随机抽取这些C正常聚类中的一个为<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>。另一方面，为了保证每个Di、Xa中异常的多样性，我们从Xa和常用的异常生成方法[22,60,63]生成的伪异常中随机提取了<spanclass="math inline">\(\mathcal{X}_{a,i}\)</span>。</p><p>此外，HADG利用这些训练数据来创建开放集的检测和验证数据集，以便在代理OSAD环境中对我们的模型进行训练。特别是，对于每个Di，HADG将它分成两个不相交的子集，即<spanclass="math inline">\(\mathcal{D}_i =\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>，分别对应支持集和查询集，支持集<spanclass="math inline">\(\mathcal{D}_i^s=\mathcal{X}_{n,i}^s\cup\mathcal{X}_{a,i}^s\)</span>用来训练我们的基本模型ϕi，查询集<spanclass="math inline">\(\mathcal{D}_i^q=\mathcal{X}_{n,i}^q\cup\mathcal{X}_{a,i}^q\)</span>用于验证其开放集性能。保证开放的验证/查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>，我们执行抽样的方式，以确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>是两个不同的正常集群，同时确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>不相互重叠，例如，$<em>{a,i}^s</em>{a,i}^q=$。</p><h4id="异常异质性的协同可微分学习cdl">异常异质性的协同可微分学习CDL</h4><p>我们的CDL组件的目标是首先使用T个基模型ϕi学习隐藏在<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异构异常分布，然后利用这些模型以端到的方式协同优化统一检测模型g。CDL的详细介绍如下。</p><h5 id="学习t个异构异常分布">学习T个异构异常分布。</h5><p>我们首先训练T个基模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，分别捕获<spanclass="math inline">\(\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异质异常分布，每个ϕi使用以下损失进行优化：<spanclass="math display">\[\mathcal{L}_{\phi_i}=\sum_{j=1}^{|\mathcal{D}_i^s|}\ell_{dev}\left(\phi_i(\mathbf{x}_j;\theta_i),y_j\right),\]</span>其中<spanclass="math inline">\(\ell_{dev}\)</span>由偏差损失[32]指定，遵循之前的OSAD方法DRA[15]和DevNet[32]，<spanclass="math inline">\(\mathcal{D}_i^s\)</span>是<spanclass="math inline">\(\mathcal{D}_i\)</span>中的支持集。虽然在训练阶段只有有限的可见异常，但每个Di中的正常样本和异常样本的混合差异很大，使得每个ϕi可以学习不同的异常评分的异常分布。</p><h5 id="协作性可微分学习">协作性可微分学习。</h5><p>每个ϕi只捕获了潜在的异常异质性的全貌的一部分。因此，我们然后执行一个协作的可微学习，利用来自T个基模型的损失来学习统一的AD模型g，以捕获更丰富的异常异质性。关键的见解是，g经过了优化，可以很好地处理各种可能的异常分布，减轻对特定异常分布的潜在过拟合。此外，g的优化是基于在等式1中训练基本模型时没有看到的查询集上的损失，即在一个代理开放环境下进行优化，这有助于训练一个更广义的OSAD模型g。具体来说，g被指定为与基于模型ϕi具有完全相同的网络架构，其在t+ 1阶段的权重参数θg根据所有基础模型在t阶段的损失进行优化： <spanclass="math display">\[\theta_g^t\longleftarrow\theta_g^{t\boldsymbol{-}1}-\alpha\nabla\mathcal{L}_{cdl},\]</span>其中，α是一个学习速率，<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>是对查询集上的T个基模型的聚合损失：<spanclass="math display">\[\mathcal{L}_{cdl}=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right).\]</span>在下一个训练阶段，所有基础模型的<spanclass="math inline">\(\theta_i^{t+1}\)</span>设置为<spanclass="math inline">\(\theta_g^t\)</span>作为新的权重参数。然后，我们使用等式1优化基本模型ϕi，然后使用等式2在查询集上优化统一模型g。这种替代基础模型和统一模型学习用于获得日益捕获更丰富的异常异质性。</p><h5id="学习个体异常分布的重要性得分">学习个体异常分布的重要性得分。</h5><p>模拟异常分布数据Di的质量变化很大，导致基本模型的有效性存在较大差异。此外，在一个轮次效率较低的基础模型可以在另一个轮次变得更有效。因此，在整个优化动态过程中，平均考虑每一个基本模型可能会因为性能不佳的基础模型会影响统一模型g的整体性能从而导致劣等优化。为了解决这个问题，我们提出了一个自监督顺序建模模块来动态估计每个基模型在每个轮次的重要性。这就细化了<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>的损失如下： <spanclass="math display">\[\mathcal{L}_{cdl}^+=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}w_i^t\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right),\]</span>其中，<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>表示其基模型ϕi在t轮次的重要性得分。下面我们将介绍我们是如何通过ψ来学习<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>的。</p><p>我们顺序建模的基于动态重要性分数的估计是建立在直觉，如果一个基础模型ϕi有良好的泛化能力，其预测异常分数为不同的输入数据应该一致和准确的在不同的训练阶段，各种异常异质性逐渐出现随着训练的展开。为此，我们训练了一个序列模型ψ来捕获所有基本模型产生的异常分数的一致性和准确性。这是通过训练ψ使用基础模型之前的输出异常分数来预测它们的下一个轮次的异常分数来实现的。具体来说，给定一个训练样本xj和利用基础模型<spanclass="math inline">\(\left\{\phi_i\right\}_{i=1}^T\)</span>得到的一组异常评分预测<spanclass="math inline">\(\mathbf{s}_j=\begin{Bmatrix}s_{ji}\end{Bmatrix}_{i=1}^T\)</span>，结果在轮次t之前产生了一系列的分数预测，<spanclass="math inline">\(\mathbf{S}_j^t =[\mathbf{s}_j^{t-K},\cdots,\mathbf{s}_j^{t-2},\mathbf{s}_j^{t-1}]\)</span>记录到K个之前的步骤，然后<spanclass="math inline">\(\psi:\mathbf{S}\to\mathbb{R}^T\)</span>旨在预测所有T个基础模型在轮次t的预测得分。在我们的实现中，ψ由一个由θψ参数化的序列神经网络指定，并使用以下下一个序列预测损失进行优化：<spanclass="math display">\[\mathcal{L}_{seq}=\sum_{\mathbf{x}_j\in\mathcal{D}}\mathcal{L}_{mse}(\hat{\mathbf{s}}_j^t,\mathbf{s}_j^t),\]</span>其中，<spanclass="math inline">\(\hat{\mathbf{s}}_j^t=\psi(\mathbf{S}_j^t;\theta_\psi)\)</span>和<spanclass="math inline">\(\mathbf{s}_j^t\)</span>分别为在轮次t的基模型中xj的预测和实际异常得分，<spanclass="math inline">\(\mathcal{L}_{seq}\)</span>为均方误差函数。模型ψ不是使用监督损失，而是使用等式5中的自监督损失函数进行训练，以保留groundtruth标签，避免对标记数据的过拟合，有效地评价基础模型的泛化能力。</p><p>然后利用预测的异常得分<span class="math inline">\(\hat{s}_{ji}^t\)</span>与真实标签<spanclass="math inline">\(y_{j}\)</span>之间的差值来定义基本模型ϕi的泛化误差<spanclass="math inline">\(r_i^t\)</span>，如下： <spanclass="math display">\[r_i^t=\frac{1}{|\mathcal{D}^{\prime}|}\sum_{\mathbf{x}_j\in\mathcal{D}^{\prime}}c_j\mathcal{L}_{mse}(\hat{s}_{ji}^t,y_j),\]</span>其中，<spanclass="math inline">\(\mathcal{D}^{\prime}=\mathcal{D}\setminus\mathcal{X}_{n,i}\)</span>和<spanclass="math inline">\(c_{j}\)</span>是与每个范例<spanclass="math inline">\(x_{j}\)</span>关联的预定义的类别权重。换句话说，<spanclass="math inline">\(r_i^t\)</span>测量ϕi来预测<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}\)</span>和所有其他未看到的正常和异常训练例子中可见异常的异常得分时的检测误差，不包括已看到的正常例子<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>（与ϕi相关）。如果xj是一个看不见的异常，则分配一个较大的cj，以突出检测看不见的异常的重要性；否则，将为其他例子分配相同的值。</p><p>由于较大的<spanclass="math inline">\(r_i^t\)</span>意味着基模型ϕi在轮次t时的泛化能力较差，因此在更新统一模型g时应较少注意它。因此，将ϕi的重要性得分定义为其泛化误差的倒数如下：<spanclass="math display">\[w_i^t=\frac{\exp(-r_i^t)}{\sum_i^T\exp(-r_i^t)}.\]</span></p><h3 id="实验-1">实验</h3><h4 id="实验设置">实验设置</h4><h5 id="数据集-1">数据集</h5><p>在之前的OSAD研究[15,32]之后，我们对9个真实世界的异常检测数据集进行了广泛的实验，包括5个工业缺陷检测数据集MVTecAD [5]，AITEX [42]，SDD [44]，ELPV[13]和光学[50]，一个行星探测数据集（Mastcam [20]）和3个医疗数据集HeadCT[40]，BrainMRI [40]和Hyper-Kvasir[7]。根据我们如何对所看到的异常示例进行采样，我们使用两种协议来评估检测性能，一般设置和硬设置[15]。一般设置假设异常例子是从异常类中随机抽样的，而硬设置提出了一个更具挑战性的情况，即异常例子只从一个类中抽样，以评估对新的或看不见的异常类的泛化能力。与[15]一样，我们还将异常例子的数量分别设置为M= 10和M = 1来评估性能。关于这些数据集的更多细节请见附录A。</p><h5 id="比较的方法和评价指标">比较的方法和评价指标</h5><p>将AHL与五种密切相关的最先进的（SOTA）方法进行了比较，包括MLEP[24]、SAOE [22,30,45]、FLOS [23]、DevNet [32]和DRA[15]。MLEP、DevNet和DRA都是专门为OSAD而设计的。SAOE是一种增强了合成异常和异常值曝光的监督检测器，而FLOS是一种基于焦点损失的不平衡分类器。对于评价指标，我们采用广泛使用的ROC曲线下面积（AUC）来衡量所有方法和设置的性能。所有报告的结果都是三次独立运行的平均结果，另有说明。</p><h5 id="实施细节-1">实施细节</h5><p>为了生成一组不同的异常分布，我们提出的方法使用了随机选择的正常簇和标记的异常簇来创建每个单独的异常分布数据Di。具体来说，首先使用k-means聚类将正常样本划分为三个正常聚类（即使用k=3）。然后选择两个随机选择的聚类，结合可见异常，构造Di，选择一个正常的集群和50%的异常集作为支持集<spanclass="math inline">\(\mathcal{D}_i^s\)</span>，而其余的样本用作查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>（根据只有一个可见的异常例子的协议，该示例都包含在这两个集合中）。这有助于有效地模拟具有部分观察到的异常分布的开放集环境。为了进一步增加异常分布数据集内部和之间的异质性，我们随机选择三种流行的异常生成技术中的一种，包括CutMix[60]、CutPaste [22]和DRAEM Mask[63]，来生成伪异常并注入Di的支持和查询集。以保证开放集相关的伪异常检测、<spanclass="math inline">\(\mathcal{D}_i^s\)</span>和<spanclass="math inline">\(\mathcal{D}_i^q\)</span>中的伪异常都是由两种不同的异常生成方法生成的。对于每个数据集，都使用T=6来生成单个的异常分布数据。当xj表示看不见的异常样本时，Cj设置为1.0，当xj表示可见的异常或看不见的正常样本时，Cj设置为0.5。</p><p>AHL是一个通用框架，在该框架下，现有OSAD模型的特性和损耗函数可以很容易地作为基本特性和基本损耗插入。特别是，从其中一个OSAD模型（如DRA）中提取图像特征，然后使用我们提出的基于基础损失的损失函数来训练AHL(见等式4).DRA [15]、DevNet [32]和BGAD[58]是目前OSAD使用的SOTA模型，但BGAD使用的与其他两个数据集非常不同的基准数据集。我们的实验严格遵循DRA[15]和DevNet [32]中使用的开创性的OSAD评估协议和基准，并选择DRA[15]和DevNet[32]分别插入AHL，表示为AHL（DRA）和AHL（DevNet）。Adam被用作优化器。学习异构T基模型的初始学习率设置为0.0002，而统一AD模型g的初始学习率设置为0.002。在自监督重要性评分估计器中，采用两层双向LSTM[67]作为骨干，隐藏维数设置为6。在预测层之前，后面是一个有12个隐藏节点的全连接层。该组件的初始学习率被设置为0.002。</p><p>上述设置默认用于所有数据集的AHL报告结果。MLEP、SAOE和FLOS的结果取自[15]。DevNet和DRA的结果使用他们的官方代码进行复制，以获得AHL中使用的特性，这意味着DevNet和AHL（DevNet）使用相同的特性集，这也适用于DRA和AHL（DRA）（更多的实现细节请参见附录B）。</p><h5 id="在一般设置下的性能">在一般设置下的性能</h5><p>表1显示了在一般设置下的比较结果，其中模型使用一个或10个随机抽样的异常例子进行训练。</p><figure><img src="./../postimages/IAD/QQ_1721792278221.png"alt="QQ_1721792278221" /><figcaption aria-hidden="true">QQ_1721792278221</figcaption></figure><p>MVTecAD上的结果在其16个数据子集上取平均值（关于这些子集的详细结果见附录C）。总的来说，我们的方法AHL在三个应用场景的所有数据集的10个镜头和一次性设置协议中都为各自的DRA和DevNet带来了持续的实质性改进。由于DRA是一个比DevNet更强的基础模型，因此AHL（DRA）通常比AHL（DevNet）获得更好的性能。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PromptAD</title>
      <link href="/PromptAD/"/>
      <url>/PromptAD/</url>
      
        <content type="html"><![CDATA[<p>PromptAD: Learning Prompts with only Normal Samples for Few-ShotAnomaly Detection</p><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h1 id="摘要">摘要</h1><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h1 id="引言">引言</h1><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/PromptAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h1 id="前期准备工作">前期准备工作</h1><h2 id="clip和提示学习">CLIP和提示学习</h2><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h2 id="clip-surgery">CLIP Surgery</h2><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h1 id="方法论">方法论</h1><h2 id="概观">概观</h2><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><imgsrc="./../postimages/PromptAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h2 id="语义连接">语义连接</h2><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h1 id="显式异常边缘">显式异常边缘</h1><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h2 id="异常检测">异常检测</h2><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h1 id="实验">实验</h1><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h2 id="数据集">数据集</h2><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h2 id="评估指标">评估指标</h2><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h2 id="实施细节">实施细节</h2><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DeepClustering</title>
      <link href="/DeepClustering/"/>
      <url>/DeepClustering/</url>
      
        <content type="html"><![CDATA[<h1 id="聚类算法">聚类算法</h1><p>​聚类算法是一类无监督学习算法，用于将数据分组成具有相似性的簇或群体。</p><h2 id="传统算法">传统算法</h2><h3 id="k均值聚类k-means-clustering">K均值聚类（K-MeansClustering）</h3><ul><li>优点：<br/> - 简单易懂，容易实现。<br/> - 适用于大规模数据。<br/> -速度较快，适用于许多应用。</li><li>缺点：<br/> - 需要预先指定簇的数量K。<br/> -对初始簇中心的选择敏感。<br/> - 对异常值和噪声敏感。<br/> -适用于凸形簇。</li></ul><h3 id="层次聚类hierarchical-clustering">层次聚类（HierarchicalClustering）</h3><ul><li>优点：<br/> - 不需要预先指定簇的数量。<br/> -可以生成层次化的簇结构。<br/> - 适用于不规则形状的簇。</li><li>缺点：<br/> - 计算复杂性较高，不适用于大规模数据。<br/> -结果的可解释性较差。</li></ul><h3 id="密度聚类density-based-clustering">密度聚类（Density-BasedClustering）</h3><ul><li>优点：<br/> - 能够发现任意形状的簇。<br/> -对噪声和异常值相对稳健。<br/> - 不需要预先指定簇的数量。</li><li>缺点：<br/> - 对参数的选择敏感。<br/> -不适用于数据密度差异很大的情况。</li></ul><h3 id="谱聚类spectral-clustering">谱聚类（Spectral Clustering）</h3><ul><li>优点：<br/> - 能够发现任意形状的簇。<br/> -适用于不规则形状的簇。<br/> - 不受初始簇中心的选择影响。</li><li>缺点：<br/> - 计算复杂性较高，对于大规模数据不适用。<br/> -需要谨慎选择相似度矩阵和簇数。</li></ul><h3id="dbscandensity-based-spatial-clustering-of-applications-with-noise">DBSCAN（Density-BasedSpatial Clustering of Applications with Noise）</h3><ul><li>优点：<br/> - 能够自动发现任意形状的簇。<br/> -对噪声和异常值相对稳健。<br/> - 不需要预先指定簇的数量。</li><li>缺点：<br/> - 对于高维数据，需要特别注意参数的选择。<br/> -可能在数据密度差异较大时效果不佳。</li></ul><h3id="em聚类expectation-maximization-clustering">EM聚类（Expectation-MaximizationClustering）</h3><ul><li><p>优点：<br/> - 适用于混合模型，可以发现概率分布簇。<br/> -适用于数据有缺失值的情况。</p></li><li><p>缺点：<br/> - 对初始参数的选择敏感。<br/> -对于高维数据，需要特别注意参数的选择。</p></li><li><p>模糊聚类（Fuzzy Clustering）</p></li><li><p>优点：<br/> -能够为每个数据点分配到多个簇，考虑数据的不确定性。<br/> -适用于模糊分类问题。</p></li><li><p>缺点：<br/> - 计算复杂性较高。<br/> -结果的可解释性较差。</p></li></ul><p>选择适当的聚类方法通常取决于数据的性质、问题的要求以及计算资源的可用性。聚类算法可以用于数据探索、模式发现、异常检测等多种应用，但需要根据具体情况进行选择和调整。</p><h2 id="基于网络的算法">基于网络的算法</h2><h3id="自编码聚类算法--dec-deep-embedded-clustering">自编码聚类算法--DEC(Deep Embedded Clustering)</h3><h3 id="软分配">1.软分配</h3><p>​  使用t-SNE算法的t-分布作为核来衡量嵌入点<spanclass="math inline">\(z_i\)</span>和质心<span class="math inline">\(\mu_j\)</span>之间的相似度： <spanclass="math display">\[q_{ij}=\frac{(1+\|z_i-\mu_j\|^2/\alpha)^{-\frac{\alpha+1}2}}{\sum_{j&#39;}(1+\|z_i-\mu_{j&#39;}\|^2/\alpha)^{-\frac{\alpha+1}2}}\]</span>​  其中<span class="math inline">\(z_i =f_{\theta}(x_i)\inZ\)</span>对应于嵌入后的<span class="math inline">\(x_i\inX\)</span>，其中<spanclass="math inline">\(\alpha\)</span>是t-SNE算法t-分布的自由度，而<spanclass="math inline">\(q_{ij}\)</span>可解释为将样本<spanclass="math inline">\(i\)</span>分配给聚类<spanclass="math inline">\(j\)</span>的概率（即软分配）。由于我们无法在无监督的设置中对验证集上的<spanclass="math inline">\(\alpha\)</span>进行交叉验证，并且得知它是多余的，因此对于所有实验，我们让<spanclass="math inline">\(\alpha=1\)</span>。</p><h3 id="kl分流最小化">2.KL分流最小化</h3><p>​  使用辅助分布用来衡量样本属于某个聚类的分布，在辅助目标分布的帮助下，通过从集群的高可信度分配中学习来迭代地优化集群。具体来说，通过将软分配与目标分布匹配来训练我们的模型。为此，我们将目标定义为软分配qi和辅助分布pi之间的KL散度损失，如下所示：<spanclass="math display">\[L=\mathrm{KL}(P\|Q)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}.\]</span></p><p>​  目标分布P的选择对于DEC的性能至关重要。一种方法是将每个pi设置为高于置信度阈值的数据点的delta分布（至最接近的质心），并忽略其余部分。但是，由于qi是软分配，因此使用较软的概率目标更为自然和灵活。<br/>​  具体来说，我们希望我们的目标分布具有以下属性：<br/>​  （1）加强预测（即提高簇纯度），<br/>​  （2）更加注重以高置信度分配的数据点<br/>​  （3）归一化每个质心的损耗贡献，以防止大型聚类扭曲隐藏的特征空间。<br/>​  在我们的实验中，我们通过先将qi升至第二次幂，然后通过每个群集的频率进行归一化来计算pi：<spanclass="math display">\[p_{ij}=\frac{q_{ij}^2/f_j}{\sum_{j&#39;}q_{ij&#39;}^2/f_{j&#39;}}\\f_{j}= \sum_{i}q_{ij}\]</span> ​  <spanclass="math inline">\(f_j\)</span>为软簇频率。培训策略可以看作是一种自我培训的形式。与自训练中一样，我们采用初始分类器和未标记的数据集，然后使用分类器标记数据集，以便对其自身的高置信度预测进行训练。实际上，在实验中，我们观察到DEC通过学习高置信度预测来提高每次迭代中的初始估计，从而有助于改善低置信度预测。</p><h3 id="损失的优化过程">3.损失的优化过程</h3><p>​  我们使用带有动量的随机梯度下降（SGD）联合优化聚类中心<spanclass="math inline">\(\mu_j\)</span>和DNN参数<spanclass="math inline">\(\theta\)</span>。关于每个数据点zi和每个聚类质心<spanclass="math inline">\(\mu_j\)</span>的特征空间嵌入的L梯度计算如下：<span class="math display">\[\begin{aligned}\frac{\partial L}{\partialz_{i}}&amp;=\quad\frac{\alpha+1}{\alpha}\sum_{j}(1+\frac{\|z_{i}-\mu_{j}\|^{2}}{\alpha})^{-1}\times(p_{ij}-q_{ij})(z_{i}-\mu_{j}),\\\frac{\partialL}{\partial\mu_{j}}&amp;=\quad-\frac{\alpha+1}{\alpha}\sum_{i}(1+\frac{\|z_{i}-\mu_{j}\|^{2}}{\alpha})^{-1}\times(p_{ij}-q_{ij})(z_{i}-\mu_{j}).\end{aligned}\]</span>​  然后将梯度∂L/∂zi向下传递到DNN，并在标准反向传播中用于计算DNN的参数梯度∂L/∂θ。在两次连续的迭代中，当有少于tol%的点会更改聚类簇的时候，停止执行该过程。<br/>​  第一个公式是优化AE中的Encoder参数，第二个公式是优化聚类中心。也就是说作者同时优化了聚类和DNN的相关参数。</p><p>作者设计的网络概念图如下:</p><figure><imgsrc="../postimages/DeepClustering/377271-20181021223727573-1521567551.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  DEC算法由两部分组成，第一部分会预训练一个AE模型；第二部分选取AE模型中的Encoder部分，加入聚类层，使用KL散度进行训练聚类。</p><details open><br/><summary>深度聚类</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/Deep-Clustering-Survey/">Deep Clustering: A ComprehensiveSurvey</a> <ahref="https://ieeexplore.ieee.org/abstract/document/10585323"><imgsrc="https://img.shields.io/badge/TNNLS-2024-yellow"alt="TNNLS" /></a></label></li><li><label><input type="checkbox" />A Survey on Deep Clustering: Fromthe Prior Perspective <a href="https://arxiv.org/abs/2406.19602"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Image Clustering with ExternalGuidance <a href="https://arxiv.org/abs/2310.11989"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Scaling Up Deep Clustering MethodsBeyond ImageNet-1K <a href="https://arxiv.org/abs/2406.01203"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" />Unsupervised Learning of VisualFeatures by Contrasting Cluster Assignments <ahref="https://arxiv.org/abs/2006.09882"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li></ul></details><h1 id="deep-clustering">Deep Clustering</h1><p>(来自于<strong>Twin Contrastive Learning for OnlineClustering</strong>)</p><p>​  有效的聚类策略和判别特征都是实现良好聚类的关键。由于深度神经网络强大的代表性性，深度聚类方法最近引起越来越多的关注（Asano等，2019；卡隆等，2018；郭等，2017；李等，2020,2021a；彭等人，2016；谢等人，2016；Yang等人，2016）。例如，JULE（Jangetal.，2016）迭代地学习数据表示并执行分层聚类。深度聚类（Caronetal.，2018）使用先验表示对数据进行聚类，并使用每个样本的聚类分配作为分类目标来学习新的表示。虽然表示学习和聚类可以在一定程度上相互引导，但这种两阶段的方法可能会出现在交替过程中积累的错误。这些方法的另一个缺点是，它们不能应用于在线场景，即数据以流的形式呈现，并且一次只能访问一批样本。具体来说，JULE需要全局相似性来决定应该合并哪些子集群，而Deep集群和SL（Asanoetal.，2019）需要执行离线k-means或解决全局最优传输问题来获得集群分配。为了克服离线的局限性，提出了一些在线深度聚类方法（Dang等，2021；黄等，2020；吉等，2019；李等，2021b；钟等，2020）。例如，IIC（Ji等人，2019）通过最大化数据对的集群分配之间的互信息来发现集群。PICA（Huangetal.，2020）通过最大化聚类解决方案的分区置信度来学习语义上最可信的数据分离。最近，一些研究（Niu&amp; Wang，202120；Park等人，2020；VanGansbeke等人，2020）使用初步聚类生成的伪标签，即自标记，以多阶段方式进一步提高聚类性能。<br/>​  与上述大多数在多个阶段执行表示学习和聚类的工作不同，我们的方法将这两个任务统一到双对比学习框架中。与之前只进行即时对比学习的研究相比，这种单阶段学习范式有助于模型学习更多有利于聚类的表征（Niu&amp; Wang，2021；Van Gansbeke etal.，2020）。在推进阶段，尽管基于早期提取的特征修正了聚类分配（Niu &amp;Wang，2021），但我们也可以通过单阶段学习范式，微调实例级对比学习，以减轻假阴性对的影响。</p><h1 id="deep-robust-clustering-by-contrastive-learning">Deep RobustClustering by Contrastive Learning</h1><h2 id="摘要">摘要</h2><p>​  最近，许多无监督的深度学习方法被提出来学习与无标记数据的聚类。通过引入数据增强，大多数最新的方法从原始图像及其转换应该共享相似的语义聚类分配的角度来进行深度聚类。然而，由于softmax函数只对最大值敏感，因此，即使分配给同一集群的表示特征也可能完全不同。这可能导致表示特征空间的高类内多样性，从而导致局部最优不稳定，从而损害聚类性能。为了解决这个缺点，我们提出了深度鲁棒聚类（DRC，<strong>D</strong>eep<strong>R</strong>obust<strong>C</strong>lustering）。与现有的方法不同，DRC从语义聚类分配和表示特征两个角度进行深度聚类，可以同时增加类间多样性，减少类内多样性。与现有的方法不同，DRC从语义聚类分配和表示特征两个角度进行深度聚类，可以同时增加类间多样性，减少类内多样性。此外，我们总结了一个一般的框架，通过研究互信息和对比学习之间的内部关系，可以将任何最大化的互信息转化为最小化对比损失。我们成功地将其应用于DRC，学习不变特征和鲁棒聚类。在6个广泛采用的深度聚类基准上的广泛实验表明，DRC在稳定性和准确性方面具有优越性。例如，在CIFAR-10上达到了71.6%的平均准确率，比最先进的结果高出7.1%。</p><h2 id="方法">方法</h2><h3 id="问题定义">问题定义</h3><p>​  给定一组来自K个不同语义类的未标记图像I = {I1，...，IN}。深度聚类的目的是通过卷积神经网络（CNN）模型将图像分离为K个不同的聚类，从而将具有相同语义标签的图像简化为相同的聚类。在这里，我们旨在学习一个基于参数为θ的映射函数Φ的深度CNN网络，然后每个图像Ii都可以映射到一个k维分配特征<spanclass="math inline">\(z_{i}=\Phi_{\theta}(I_{i})\)</span>。在此基础上，可以通过softmax函数得到分配概率向量pi，该函数可由<span class="math display">\[p_{i j}=\frac{e^{z_{ij}}}{\sum_{t=1}^{K}e^{z_{i t}}},j=1,...,K\]</span>​  然后可以用最大似然法预测聚类分配： <spanclass="math display">\[\ell_{i}=\arg\operatorname*{max}_{j}(p_{ij}),j=1,\dots,K,i=1,\dots,N\]</span></p><h3 id="网络架构">网络架构</h3><p>​  为了解决上述问题，我们引入了一种新的端到端深度聚类框架，同时利用分配概率和分配特征。</p><figure><img src="../postimages/DeepClustering/image-20241218212302013.png"alt="image-20241218212302013" /><figcaption aria-hidden="true">image-20241218212302013</figcaption></figure><p>​  如图2所示，我们首先采用深度卷积神经网络（CNN）来生成K维的分配特征和分配概率。然后，利用基于分配概率的对比损失来保持原始图像及其增强图像的分配一致性，这有助于增加类间的方差，形成分离良好的聚类。利用基于分配特征的对比损失来捕获原始图像及其增强图像之间的表示一致性，有助于减少类内方差，实现更鲁棒的聚类。</p><h3 id="互信息与对比学习">互信息与对比学习</h3><p>​  对比学习已被证明在无监督学习和自我监督学习中是强大的，这有助于在许多任务中实现最先进的结果。而对比损失也与互信息密切相关。设X= {x1，x2，...，xN }是一个给定空间中的N个样本。X的变换由X 0 = {x 01，x02，...，x 0N}定义。因为我们不知道X的ground-truth，我们所知道的是，xi’都可以被视为xi的正样本，对于任何的i=1,2，...，N。换句话说，<spanclass="math inline">\(p({x_{i}^{\prime}}|x_{i})\)</span>应该比<spanclass="math inline">\(p({x_{j}^{\prime}}|x_{i}),j \neqi\)</span>。一个非常自然的想法是最大限度地保留X和X'之间的互信息，定义为<span class="math display">\[M I({\bf X},{\bfX}^{\prime})=\sum_{i=1}^{N}\sum_{j=1}^{N}p(x_{i},x_{j}^{&#39;})l og{\frac{p(x_{j}^{&#39;}|x_{i})}{p(x_{j}^{&#39;})}}\]</span>​  如果我们假设 <spanclass="math display">\[\frac{p(x_{j}^{&#39;}|x_{i})}{p(x_{j}^{&#39;})}\proptof(x_{i},x_{j}^{&#39;})\]</span>​  其中f是一个在不同的情况下可能会有不同的函数，那么我们有以下定理。<br/><strong>定理1</strong>假设存在一个常数c0，使得<spanclass="math inline">\(p({x_{i}^{\prime}}|x_{i})&gt;0\)</span>对所有的i =1,2，...，N都成立，那么 <span class="math display">\[M I({\bf x},{\bfx}^{&#39;})\geq\logN+\frac{c_{0}}{N}\sum_{i=1}^{N}\log\frac{f(x_{i},x_{i}^{&#39;})}{\sum_{t=1}^{N}f(x_{i},x_{t}^{&#39;})}\]</span>​  定义 <spanclass="math display">\[\mathcal{L}_{c}=\sum_{i=1}^{N}\log\frac{f(x_{i},x_{i}^{\prime})}{\sum_{t=1}^{N}f(x_{i},x_{t}^{\prime})},\]</span>​  因此，最小化对比损失Lc等于最大化互信息MI（X，X 0）的下界。</p><h3 id="损失函数">损失函数</h3><p>​  我们的损失函数由三个部分组成：1。一种基于分配特征的对比损失，在特征水平上保留互信息。2.一种基于分配概率的对比损失，使原始图像的预测标签与转换图像的预测标签之间的互信息最大化。3. 聚类正则化损失是为了避免平凡的解。</p><h1 id="deep-clustering-1">Deep Clustering</h1><p>(来自于<strong>Twin Contrastive Learning for OnlineClustering</strong>)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸识别与深度鉴伪研究进展</title>
      <link href="/20240627%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240627%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<p><ahref="https://www.bilibili.com/video/BV1F4421D7DL/">【在线课程】人脸识别与深度鉴伪研究进展（CSIG图像视频通信专委会青年学者沙龙第七期）_哔哩哔哩_bilibili</a></p><p>深度伪造反制技术需求迫切</p><figure><img src="./../postimages/0627/image-20240627201621844.png"alt="image-20240627201621844" /><figcaption aria-hidden="true">image-20240627201621844</figcaption></figure><figure><img src="./../postimages/0627/image-20240627201913441.png"alt="image-20240627201913441" /><figcaption aria-hidden="true">image-20240627201913441</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202139680.png"alt="image-20240627202139680" /><figcaption aria-hidden="true">image-20240627202139680</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202304130.png"alt="image-20240627202304130" /><figcaption aria-hidden="true">image-20240627202304130</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202334148.png"alt="image-20240627202334148" /><figcaption aria-hidden="true">image-20240627202334148</figcaption></figure><p>总结<br/>遮挡人脸识别<br/>√渐进式学习-----兼顾非口罩人脸识别性能<br/>√无标签样本助力-----适应真实口罩遮挡<br/>√遮挡预测与身份特征耦合学习-----应对多样性遮挡</p><p>伪造人脸检测及溯源<br/>√隐身份驱动-----解释伪造人脸检测<br/>√身份解耦溯源-----追溯目标人脸</p>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAA+</title>
      <link href="/SAA/"/>
      <url>/SAA/</url>
      
        <content type="html"><![CDATA[<p>Segment Any Anomaly without Training via Hybrid PromptRegularization</p><p>中国华中科技大学数字制造设备与技术国家重点实验室</p><h1 id="摘要">摘要</h1><p>​  我们提出了一个新的框架，即分段任意异常+（SAA+, Segment Any Anomaly+），用于混合快速正则化的零快速异常分割，以提高现代基础模型的适应性。现有的异常分割模型通常依赖于特定领域的微调，限制了它们在无数异常模式上的泛化。在这项工作中，受到基础模型强大的零镜头泛化能力的启发，我们首先探索它们的组装，以利用不同的多模态先验知识进行异常定位。对于非参数基础模型对异常分割的自适应，我们进一步引入了来自领域专家知识和目标图像上下文的混合提示作为正则化。我们提出的SAA+模型在几个异常分割基准测试上取得了最先进的性能，包括VisA、MVTec-AD、MTD和KSDD2。code:https://github.com/caoyunkang/Segment-Any-Anomaly</p><h1 id="介绍">介绍</h1><p>​  异常分割模型[1,2,3]在工业质量控制[4,5]和医学诊断[6]等各个领域都引起了人们极大的研究兴趣。可靠异常分割的关键是区分异常数据的分布。具体来说，本文考虑了图像上的零样本异常分割（ZSAS,zero-shot anomalysegmentation），这是一种很有前途但未探索的设置，在训练过程中没有为目标类别提供正常和异常图像。</p><p>​  由于缺乏用于训练的异常样本，许多工作都致力于无监督或自监督的异常分割，其目标是在训练过程中学习正常样本的表示。然后，通过计算测试样本与学习到的正态分布之间的差异，可以对异常进行分割。具体来说，这些模型，包括基于自动编码器的重建[7,8,9,10,11,12]、一类分类[13,14,15]和基于记忆的正态分布[3,2,16,17,18]方法，通常需要对特定的有限的类别训练单独的模型。然而，在现实场景中，有数以百万计的工业产品，为单个对象收集大型训练集没有成本效益，这阻碍了在需要高效部署的情况下，例如生产的初始阶段，它们的部署。</p><p>​  最近，基础模型，如SAM [19]和CLIP[20]，通过提示[21,22]检索存储在这些模型中的先验知识，显示出了巨大的零镜头视觉感知能力。在这项工作中，我们想探索如何适应基础模型来实现异常分割下的零样本迁移能力。</p><p><img src="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA）。然而，SAA显示了严重的误警报问题，它错误地检测到所有的“灯芯”，而不是ground-truth异常区域（“过长的灯芯”）。因此，我们在改进的模型（SAA+)中进一步加强了混合提示的正则化，成功地帮助识别了异常区域。</p><p>​  为此，如图1所示，我们首先通过级联提示引导的目标检测[23]和分割基础模型[19]，构造一个普通的基础模型，即分别作为异常区域生成器和异常区域细化器（SAA）。根据解锁基础模型知识[24,25]的实践，使用朴素语言提示，如“缺陷”或“异常”，来分割目标图像所期望的异常。具体来说，语言提示用于提示异常区域生成器为所需的异常区域生成提示条件的框级区域。然后在异常区域细化器中对这些区域进行细化，以产生最终的预测，即用于异常分割的掩模。</p><p>​  然而，如图1所示，普通的基础模型装配（SAA）往往会导致严重的误报，例如，SAA错误地将所有的灯芯称为异常，而只有超长的灯芯是真正的异常，我们将其归因于幼稚的语言提示带来的歧义。首先，当面对基础模型的预训练数据分布与下游数据集之间的域转换时，传统的语言提示可能会变得无效。其次，目标的“异常”程度取决于对象上下文，这对于朴素的粗粒度语言提示，例如“异常区域”，很难准确地表达。</p><p>​  因此，超越幼稚的语言提示，我们将领域专家知识和目标图像纳入我们改进的框架上下文，即分段任何异常+（SAA+）。一方面，专家知识提供了在开放世界场景中与目标相关的异常情况的详细描述。我们利用更具体的描述作为上下文提示，有效地对齐预训练和目标数据集中的图像内容。另一方面，我们利用目标图像上下文来可靠地识别和自适应地校准异常分割预测[26,27]。通过利用目标图像中丰富的上下文信息，我们可以准确地将对象上下文与最终的异常预测联系起来。</p><p>​  从技术上讲，除了单纯的类不可知的提示外，我们还利用领域专家知识来构建面向目标的异常语言提示，即特定于类的语言表达式。此外，由于语言不能准确地检索具有特定对象特征的区域，如数量、大小和位置，精确地[28,29]，我们以阈值过滤器的形式引入对象属性提示。这些提示有助于识别和删除不满足所需属性的候选区域。此外，为了充分利用目标图像的上下文，我们建议利用图像显著性和区域置信度排序作为提示，通过考虑一个区域与图像内其他区域之间的相似性，如欧氏距离，来建模一个区域的异常程度。最后，我们进行了彻底的实验，以确认我们的混合提示在适应基础模型的零镜头异常分割的有效性。具体来说，我们最终的模型（SAA+）在零镜头设置下，在各种异常分割数据集上获得了新的最新性能。总之，我们的主要贡献是：</p><p>​  我们提出了异常分割的SAA框架，允许在不需要训练的情况下协同组装不同的基础模型。</p><p>​  我们引入混合提示作为一种正则化技术，利用领域专家知识和目标图像上下文来适应基础模型进行异常分割。这导致了SAA+的开发，这是我们框架的一个增强版本。</p><p>​  我们的方法在几个基准数据集上实现了最先进的零镜头异常分割，包括VisA、MVTec-AD、KSDD2、MTD的性能。值得注意的是，SAA/SAA+在不需要任何注释的情况下检测与纹理相关的异常方面显示出了显著的能力。</p><h1 id="saa针对zsas的基础模型组装">SAA：针对ZSAS的基础模型组装</h1><h2 id="问题定义zsas">问题定义：ZSAS</h2><p>零样本异常分割（ZSAS， Zero-shot Anomaly Segmentation）</p><p>​  ZSAS的目标是对新对象进行异常分割，而不需要任何相应的对象训练数据。ZSAS试图创建一个基于空训练集∅的异常映射<spanclass="math inline">\(\mathbf{A}\in[0,1]^{h\timesw\times1}\)</span>，以识别包含新对象的图像<spanclass="math inline">\(\mathbf{I}\in\mathbb{R}^{h\timesw\times3}\)</span>中单个像素的异常程度。ZSAS任务有可能显著减少对培训数据的需求，并降低与实际检查部署相关的成本。</p><h2 id="基线模型saa">基线模型：SAA</h2><p>分段任何异常（SAA，Segment Any Anomaly）</p><p>​  对于ZSAS，我们首先构建一个普通的基础模型组件，即分段任何异常（SAA），如图1所示。<br/><imgsrc="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA)。</p><p>具体来说，给定一个特定的异常分割查询图像，我们首先使用语言作为初始提示，通过一个异常区域生成器粗略地检索粗糙的异常区域建议，即GroundingDINO[23]。然后，使用异常区域细化器将异常区域建议细化为像素级高质量的分割掩模，其中使用提示驱动的分割基础模型，即SAM[19]。</p><h3 id="异常区域发生">异常区域发生</h3><p>​  随着语言视觉模型的蓬勃发展，一些基础模型[24,23,46]逐渐获得了通过语言提示检索图像中对象的能力。给定描述要检测区域的语言提示T，例如“异常”，基础模型可以为查询图像i生成所需区域i。在那里，我们将区域检测器的结构基于文本引导的开集对象检测结构，用于视觉grounding。具体来说，我们采用了一个已经在[41]上预先训练过的大规模语言视觉数据集的GroundingDINO[23]架构。该网络首先通过文本编码器和视觉编码器分别提取语言提示符和查询图像的特征。然后用交叉模态解码器以边界框的形式生成粗糙的对象区域。给定边界盒级区域集RB及其对应的置信度评分集S，异常区域生成器（生成器）的模块可以表示为：<spanclass="math display">\[\mathcal{R}^{B},\mathcal{S}:=\mathrm{Generator}(\mathbf{I},\mathcal{T})\]</span></p><h3 id="异常区域细化">异常区域细化</h3><p>​  为了生成像素级的异常分割结果，我们提出了异常区域细化器，将边界盒级的异常区域候选区域细化为异常分割掩模集。为此，我们使用了一个复杂的基础模型来进行开放世界的视觉分割，即SAM[19]。该模型主要包括一个基于vit的[56]主干和一个提示条件掩码解码器。具体来说，该模型是在一个具有10亿个细粒度掩模的大规模图像分割数据集[19]上进行训练的，这使得在开放集分割下能够具有高质量的掩模生成能力。有提示条件的掩码解码器接受各种类型的提示作为输入。我们将边界框候选RB视为提示，得到像素级分割掩模r。异常区域细化器（Refiner）的模块可以表述如下：<spanclass="math display">\[\mathcal{R}:=\operatorname{Refiner}(\mathbf{I},\mathcal{R}^B)\]</span>​  在此之前，我们以具有相应置信度分数s的高质量分割掩模R的形式获得了区域集。综上所述，我们总结了框架（SAA）如下：<spanclass="math display">\[\mathcal{R},\mathcal{S}:=\text{SAA}(\mathbf{I},\mathcal{T}_n)\]</span>​  其中Tn是一个朴素的类不可知的语言提示，例如”异常“，在SAA中使用。</p><h3 id="基线模型组件的zsas性能分析">基线模型组件的ZSAS性能分析</h3><p>​  我们提出了一些初步的实验来评估基础模型组装对ZSAS的有效性。尽管解决方案的简单和直观，我们观察到一个语言歧义的问题。具体来说，某些语言提示，如“异常”，可能无法检测到所需的异常区域。例如，如图1所示，所有的“灯芯”都被SAA用“异常”提示符错误地识别为异常。</p><p>​  我们将这种语言歧义归因于训练前的语言-视觉数据集和目标ZSAS数据集之间的领域差距，这意味着一些语言提示可能具有不同的含义，并在不同的数据集中与不同的图像内容相关联。此外，在这些大规模的数据集中几乎没有像“异常”这样的形容词表达，这使得这种快速的设计很难理解什么是异常区域。此外，确切的“异常”是特定于对象的，并且会因对象而变化。例如，它表示皮革上的划痕或榛子上的裂缝。语言歧义问题导致ZSAS数据集中严重的误警报。我们建议引入由领域专家知识和目标图像上下文生成的混合提示，以减少语言歧义，从而实现更好的ZSAS性能。</p><h1id="saa通过混合提示正则化的自适应基础模型">SAA+：通过混合提示正则化的自适应基础模型</h1><p>​  为了解决SAA中的语言歧义并提高其在ZSAS上的能力，我们提出了一个名为SAA+的升级版本，它包含了混合提示，如图2所示。除了利用从预先训练过的基础模型中获得的知识外，SAA+还利用领域专家知识和目标图像上下文来生成更准确的异常区域掩模。我们将在下面提供关于这些混合提示的进一步细节。</p><h2 id="从领域专家知识中生成的提示">从领域专家知识中生成的提示</h2><p>​  根据提示学习[48,54]的趋势，我们以语言的形式初始化提示，以解锁基础模型的知识。然而，当只使用朴素的语言提示“异常”时，由领域差距引起的语言歧义问题尤为严重。为了解决这个问题，我们利用了包含关于目标异常区域的有用的先验信息的领域专家知识。具体来说，尽管专家可能没有为新产品提供潜在开放世界异常的全面列表，但他们可以根据他们过去使用类似产品的经验来确定一些候选产品。领域专家知识使我们能够将朴素的“异常”提示细化为更具体的提示，以更详细地描述异常状态。除了语言提示之外，我们还引入了属性提示，以补充现有基础模型[28]中对“count”和“area”[28]等特定属性的认识不足。</p><h3 id="异常的语言表达式作为提示">异常的语言表达式作为提示</h3><p>​  为了描述潜在的开放世界异常情况，我们建议设计更精确的语言提示。这些提示可分为两种类型：类无关的提示和类特定的提示。</p><p>​  <strong>类别不可知论提示（Ta）</strong>是描述非特定于任何特定类别的异常情况的通用提示，例如，“异常”和“缺陷”。尽管预先训练的数据集和目标ZSAS数据集之间存在领域差距，但我们的实证分析（5.3）表明，这些通用提示提供了令人鼓舞的初始性能。</p><p>​  <strong>类别特定提示（Ts）</strong>是基于对类似产品的异常模式的专家知识而设计的，以补充更具体的异常细节。我们使用预先训练的视觉语言数据集中已经使用的提示，例如“黑洞”和“白色气泡”，来查询所需的区域。这种方法重新定义了寻找异常区域的任务，以定位具有特定异常状态表达式的对象，这比利用基础模型在对象上下文中识别“异常”更简单。</p><p>​  通过使用来自领域专家知识的异常语言提示<spanclass="math inline">\(\mathcal{P}^L=\{\mathcal{T}_\mathrm{a},\mathcal{T}_\mathrm{s}\}\)</span>提示SAA，我们生成了更精细的异常区域候选项R和相应的置信分数S。</p><h3 id="异常对象属性作为提示">异常对象属性作为提示</h3><p>​  目前的基础模型[23,57]在查询具有特定属性描述的对象时存在局限性，比如大小或位置，这些对于描述异常很重要，比如“电缆左边的小黑洞”。为了整合这一关键的专家知识，我们建议使用作为规则而不是语言来表述的异常属性提示。具体来说，我们考虑了异常的位置和面积。</p><p>​  <strong>异常定位。</strong>异常的准确定位在区分真实异常和假阳性中起着关键作用。通常，在推理过程中，异常被期望位于感兴趣的对象内。然而，由于背景上下文的影响，异常可能偶尔会出现在被检查的物体之外。为了解决这一挑战，我们利用基础模型的开放世界检测能力来确定被检查对象的位置。随后，我们计算了潜在异常区域和被检查对象之间的并集的交集（IoU）。通过应用expert-derived的IoU阈值，表示为<spanclass="math inline">\(θ_{IoU}\)</span>，我们过滤出了IoU值低于该阈值的异常候选值。此过程确保保留的异常候选项更有可能表示位于被检查对象内的真实异常。</p><p>​  <strong>异常区域。</strong>由其面积所反映的异常现象的大小，也是一种可以提供有用信息的特性。一般来说，异常应小于被检查物体的大小。专家可以为所考虑的特定类型的异常提供一个合适的阈值<spanclass="math inline">\(θ_{area}\)</span>。与<spanclass="math inline">\(θ_{area}\)</span>目标区域不匹配的候选区域可以被过滤掉。</p><p>​  通过结合两个属性提示<spanclass="math inline">\(\mathcal{P}^P=\left\{\theta_{area},\theta_{IoU}\right\}\)</span>，我们可以通过过滤候选区域的过滤函数（Filter）R，得到具有相应置信分数<spanclass="math inline">\(S^P\)</span>的候选<spanclass="math inline">\(R^P\)</span>的子集， <spanclass="math display">\[\mathcal{R}^P,\mathcal{S}^P:=\mathrm{Filter}(\mathcal{R},\mathcal{P}^P)\]</span></p><h2 id="来自目标图像上下文的提示">来自目标图像上下文的提示</h2><p>​  除了结合领域专家知识外，我们还可以利用输入图像本身提供的信息来提高异常区域检测的准确性。在这方面，我们提出了两个由图像上下文引起的提示。</p><h3 id="异常显著为提示">异常显著为提示</h3><p>​  由于预先训练的语言视觉数据集[41]和目标异常分割数据集[4,58]之间的领域差距，由[23]等基础模型生成的预测可能是不可靠的。为了校准个体预测的置信度得分，我们提出了模拟人类直觉的异常显著性提示法。具体来说，人类可以通过与周围区域[40]的差异来识别异常区域，即视觉显著性包含了指示异常程度的有价值的信息。因此，我们通过计算相应的像素特征(f)与其N个最近邻之间的平均距离，来计算输入图像的显著性映射(s)，<span class="math display">\[\mathbf{s}_{ij}:=\frac1N\sum_{\mathbf{f}\inN_p(\mathbf{f}_{ij})}(1-\langle\mathbf{f}_{ij},\mathbf{f}\rangle)\]</span>​  式中，<span class="math inline">\((i,j)\)</span>表示像素位置，<spanclass="math inline">\(N_p(\mathbf{f}_{ij})\)</span>表示对应像素的N个最近邻，<spanclass="math inline">\(\langle\cdot,\cdot\rangle\)</span>表示余弦相似度。我们使用来自大规模图像数据集[59]的预先训练好的cnn来提取图像特征，以确保特征的描述性。显著性地图表示一个区域与其他区域的不同程度。显著性提示PS定义为相应区域掩模内的指数显著性平均值，<spanclass="math display">\[\mathcal{P}^S:=\left\{\exp(\frac{\sum_{ij}\mathbf{r}_{ij}\mathbf{s}_{ij}}{\sum_{ij}\mathbf{r}_{ij}})\quad|\quad\mathbf{r}\in\mathcal{R}^P\right\}\]</span>​  显著性提示提供了异常区域置信度的可靠指示。这些提示是用来重新校准基础模型生成的信心分数，产生新的调整分数<spanclass="math inline">\(S^S\)</span>基于异常显著性提示<spanclass="math inline">\(P^S\)</span>。这些调整分数提供一个综合措施，考虑到信心来自基础模型和地区候选人的显著性。该流程的表述如下：<span class="math display">\[\mathcal{S}^S:=\begin{Bmatrix}p\cdots&amp;|&amp;p\in\mathcal{P}^S,s\in\mathcal{S}^P\end{Bmatrix}\]</span></p><h3 id="异常置信为提示">异常置信为提示</h3><p>​  通常，一个被检查对象中的异常区域的数量是有限的。因此，我们提出异常置信度提示<spanclass="math inline">\(P^C\)</span>根据图像内容识别出置信度得分最高的K个候选对象，并使用它们的平均值进行最终的异常区域检测。这是通过根据其对应的置信度得分选择前K个候选区域来实现的，如下所示，<spanclass="math display">\[\mathcal{R}^C,\mathcal{S}^C:=\mathrm{Top}_K(\mathcal{R}^P,\mathcal{S}^S)\]</span>​  将单个区域及其对应的得分表示为<spanclass="math inline">\(r^C\)</span>和<spanclass="math inline">\(s^C\)</span>，然后我们使用这些K个候选区域来估计最终的异常图，<spanclass="math display">\[\mathbf{A}_{ij}:=\frac{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C\cdots^C}{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C}\]</span>​  通过提出的混合提示<spanclass="math inline">\((\mathcal{P}^L,\mathcal{P}^P,\mathcal{P}^S,\text{and}\mathcal{P}^C)\)</span>，SAA在我们最终的框架中进行了正则化，即分段任何异常+（SAA+），从而做出了更可靠的异常预测。</p><h1 id="实验">实验</h1><p>​  在本节中，我们首先评估SAA/SAA+在几个异常分割基准上的性能。然后，我们广泛地研究了个体混合提示的有效性。</p><h2 id="实验设置">实验设置</h2><p>​  <strong>数据集。</strong>我们利用了四个带有像素级注释的数据集。：VisA [58]、MVTec-AD [4]、KSDD2 [60]和MTD[61]。VisA和MVTec-AD由多种对象子集组成，如电路板，而KSDD2和MTD则由纹理异常组成。总之，我们将所有这些数据集的子集分类为通常在单个图像中显示相似模式的纹理（如地毯），以及包括更多样化分布的对象（如蜡烛）。</p><p>​  <strong>评估指标。</strong>ZSAS性能的评估基于两个指标： (I)<strong>max-F1-pixel</strong>（Fp）[25]，它测量在最优阈值下的像素分割的F1分数；（II）<strong>max-F1-region</strong>（Fr），本文提出，以减轻最大f1像素[4]观察到的大缺陷的偏差。具体来说，我们在最优阈值下计算区域分割的f1分数，如果重叠值超过0.6，考虑预测为正。</p><p>​  <strong>实施细节。</strong>我们采用了GroundingDINO和分段任何模型2的官方实现来构建基线（SAA）。关于来自领域专家知识的提示的细节在补充材料中有解释。对于由图像内容诱导的显著性提示，我们使用WideResNet50[62]网络，在ImageNet [59]上进行预训练，并根据之前的研究[40]设置N =400。对于异常置信度提示，我们将超参数K默认设置为5。输入图像的分辨率固定为400×400进行评估。</p><h2 id="主要结果">主要结果</h2><p>​  <strong>比较方法。</strong>我们比较了我们最终的模型，即分段任何异常+（SAA+）与几种并发的最先进的方法，包括WinClip[25]，UTAD [40]，ClipSeg[24]，和我们的香草基线（SAA）。对于WinClip，我们报告其在VisA和MVTec-AD上的官方结果。对于其他三种方法，我们使用官方实现，并使它们适应于ZSAS任务。值得注意的是，由于所有的方法都不需要训练过程，它们的性能是稳定的，方差为±0.00。</p><p>​  <strong>定量结果：</strong>如表1所示，SAA+方法在Fp和Fr方面均显著优于其他方法。虽然WinClip[25]、ClipSeg[24]和SAA也使用基础模型，但SAA+更好地释放了基础模型的能力，并调整它们来解决ZSAS问题。SAA+的显著性能满足了不经训练就能分割任何异常现象的期望。</p><figure><img src="./../postimages/SAA/image-20240621164328885.png"alt="image-20240621164328885" /><figcaption aria-hidden="true">image-20240621164328885</figcaption></figure><p>​  <strong>定性结果：</strong>图3为SAA+与以往竞争方法的定性比较，其中SAA+取得了更好的性能。此外，可视化显示SAA+能够检测纹理异常，如皮革上的小划痕。</p><figure><img src="./../postimages/SAA/image-20240621164520983.png"alt="image-20240621164520983" /><figcaption aria-hidden="true">image-20240621164520983</figcaption></figure><h1 id="消融研究">消融研究</h1><p>​  在表2中，我们执行组件级分析，以消除框架中特定的提示设计。</p><figure><img src="./../postimages/SAA/image-20240621164657482.png"alt="image-20240621164657482" /><figcaption aria-hidden="true">image-20240621164657482</figcaption></figure><p>​  <strong>语言提示符<spanclass="math inline">\((\mathcal{P}^L)\)</span>。</strong>表2验证了来自领域专家知识的语言提示的有效性（Fp中+3.90%，Fr+4.90%）。然后，我们深入研究了Ta和Ts的有效性，这清楚地表明，一般描述和专门设计的异常描述都可以达到合理的性能。此外，它们的组合可以产生协同作用，提高异常分割性能。<spanclass="math inline">\(\mathcal{P}^L\)</span>的改进有助于解锁当前基础模型[23,19]的语言驱动区域检测能力。</p><p>​  <strong>属性提示符<spanclass="math inline">\((\mathcal{P}^P)\)</span>。</strong>除了改善整体性能，属性提示带来显著的改善（从21.83%到53.79%）纹理类别，由于过滤机制过滤掉大量的错误检测异常区域候选人通过高级特征，例如，目标图像的位置和面积。</p><p>​  <strong>显著性提示符<spanclass="math inline">\((\mathcal{P}^S)\)</span>。</strong>表2提供了<spanclass="math inline">\(\mathcal{P}^S\)</span>在异常分割的有效性的明确证据。这是因为区域显著性可以准确地描述一个区域与周围环境的偏离程度。</p><figure><img src="./../postimages/SAA/image-20240621165328601.png"alt="image-20240621165328601" /><figcaption aria-hidden="true">image-20240621165328601</figcaption></figure><p>​  在图4中，我们展示了<spanclass="math inline">\(\mathcal{P}^S\)</span>对异常分割的定性影响，说明了视觉显著性图可以帮助突出异常区域，即与其他区域相比更高的显著性值。通过结合<spanclass="math inline">\(\mathcal{P}^S\)</span>来校准置信度分数，可以获得更精确的分割结果。例如，<spanclass="math inline">\(\mathcal{P}^S\)</span>的使用可以有效地定位榛子的裂缝区域和蜡烛上的过长的灯芯。</p><p>​  <strong>置信度提示符<spanclass="math inline">\((\mathcal{P}^C)\)</span>。</strong>通过加入异常置信度提示，我们限制了异常区域的数量，这有效地减少了假阳性，导致所有类别的Fp平均提高0.72%，如表2所示。</p><figure><img src="./../postimages/SAA/image-20240621165812635.png"alt="image-20240621165812635" /><figcaption aria-hidden="true">image-20240621165812635</figcaption></figure><p>​  超参数K在PC中的影响如图5所示。从图中可以看出，随着K的提高，异常区域检测准确。然而，当K超过一定的阈值（约为K= 5）时，随着更多的区域被错误地识别为异常，性能略有下降。在K =5左右时获得最佳结果，所有类别的平均Fp为34.85%。</p><h1 id="结论">结论</h1><p>​  在这项工作中，我们探索如何在没有任何进一步训练的情况下，通过释放现代基础模型的全部力量来分割任何异常现象。基础模型装配的调整归功于快速设计，这是控制非基础模型功能的关键。因此，我们提出了一个新的框架，即分段任何异常+，利用来自专家知识和目标图像上下文的混合提示来规范无需训练的基础模型。最后，我们成功地采用了多个基础模型来解决零镜头异常分割问题，并在几个基准上获得了新的SoTA结果。我们希望我们的工作能够阐明对异常分割的无标签模型自适应的设计。</p><p>​  <strong>限制。</strong>由于计算的限制，我们目前没有在更大尺度的基础模型上测试我们的方法。我们已经用具有代表性的基础模型完成了对我们的方法的探索，并将在未来探讨这些模型的尺度效应。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning for Image Super-Resolution</title>
      <link href="/Image_Super-Resolution/"/>
      <url>/Image_Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p><h1 id="摘要">摘要：</h1><p>​  图像超分辨率（SR）是在计算机视觉中提高图像和视频分辨率的一类重要的图像处理技术。近年来，利用深度学习技术实现的图像超分辨率取得了显著进展。本文旨在对利用深度学习方法的图像超分辨率的最新进展进行一个全面的调查。一般的来说，我们可以将现有的SR技术的研究大致分为三大类：有监督SR、无监督SR和领域特异性SR。此外，我们还讨论了一些重要的问题，如公开可用的基准数据集和性能评估指标。最后，我们通过强调几个未来的方向和社区应该进一步解决的问题来结束这项调查。</p><h1 id="介绍">介绍</h1><p>​  图像超分辨率（SR）是指从低分辨率（LR）图像中恢复高分辨率（HR）图像的过程，是计算机视觉和图像处理中的一类重要的图像处理技术。它享有广泛的现实世界的应用，如医学成像[1]，[2]，[3]，监视和安全[4]，[5])等。除了提高图像感知质量外，它还有助于改善其他计算机视觉任务[6]、[7]、[8]、[9]。一般来说，这个问题是非常具有挑战性的，并且具有固有的不适定性，因为总是有多个HR图像对应于单个LR图像。在文献中，已经提出了各种经典的SR方法，包括基于预测的方法[10]，[11]，[12]，基于边缘的方法[13]，[14]，统计方法[15]，[16]，基于补丁的方法[13]，[17]，[18]，[19]和稀疏表示方法[20]，[21]，等。</p><p>​  近年来深度学习技术的快速发展，基于深度学习的SR模型已经积极探索，经常实现最先进的性能的各种基准的各种深度学习方法被应用于解决SR任务，从早期基于卷积神经网络（CNN）的方法（例如，SRCNN[22][23]）最近有前途的SR方法使用生成对抗网（GAN）[24]（如SRGAN[25]）。一般来说，使用深度学习技术的SR算法家族在以下主要方面有所不同：不同类型的网络架构[26]、[27]、[28]、不同类型的损失函数[8]、[29]、[30]、不同类型的学习原则和策略[8]、[31]、[32]等。</p><p>​  在本文中，我们全面概述了图像超分辨率的最新进展。虽然有一些现有的SR调查文献，我们的工作不同，我们专注在深度学习SR技术，而大多数早期作品[33]，[34]，[35]，[36]旨在调查传统SR算法或一些研究主要集中在提供定量评估基于全参考指标或人类视觉感知[37]，[38]。与现有的调查不同，本调查采用了一个独特的基于深度学习的视角，以系统和全面的方式回顾了SR技术的最新进展。</p><p>​  这次调查的主要贡献有三方面：<br/>&gt; 1.我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。<br/>&gt;1.我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。<br/>&gt;1.我们讨论挑战和开放的问题，并确定新的趋势和未来的方向，为社区提供深刻的指导。</p><p>​  在下面的章节中，我们将介绍在深度学习中图像超分辨率的最新进展的各个方面。图1显示了本次调查中将以层次结构的方式覆盖的图像SR的分类。第2节给出了问题的定义，并回顾了主流数据集和评估指标。第3节模块化地分析了监督SR的主要成分。第4节简要介绍无监督的SR方法。第5节介绍了一些流行的特定于领域的SR应用程序，第6节还讨论了未来的发展方向和开放的问题。</p><h1 id="问题设置和术语">问题设置和术语</h1><h2 id="问题定义">问题定义</h2><p>​  图像超分辨率的目的是从LR图像中恢复相应的HR图像。通常，LR图像Ix被建模为以下退化的输出：<span class="math display">\[I_x=\mathcal D(I_y;\delta),\]</span>​  式中，D为退化映射函数，Iy为对应的HR图像， $ $为退化过程的参数（如缩放因子或噪声）。</p><p>​  一般来说，退化过程（即D和 $ $）是未知的，只提供LR图像。在这种情况下，也称为盲SR，需要研究人员通过从LR图像Ix中恢复地面真实HR图像^的HR近似，如下：<span class="math display">\[\hat{I}_y=\mathcal{F}(I_x;\theta),\]</span>​  其中，F为超分辨率模型， $ $ 为F的参数。</p><p>​  虽然退化过程是未知的，并且可能受到各种因素的影响（如压缩伪影、各向异性退化、传感器噪声和散斑噪声），但研究人员正试图对退化映射进行建模。大多数工作都直接将退化建模为一个单一的降采样操作，如下所示：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y)\downarrow_s,\{s\}\subset\delta,\]</span>​  其中， $ <em>s $是一个具有缩放因子s的降采样操作。事实上，大多数通用SR的数据集都是基于这种模式构建的，而最常用的降采样操作是带有抗锯齿的双边插值。然而，[39]还有其他一些工作，将退化建模为几种操作的组合：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y\otimes\kappa)\downarrow_s+n_\varsigma,\{\kappa,s,\varsigma\}\subset\delta,\]</span>​  其中 $ I_y$ 表示模糊核与HR图像Iy之间的卷积， $ n</em>$ 是带有标准差 $$的加性高斯白噪声。与等式的朴素定义相比3、等式的组合降解模式4更接近真实世界的情况，并已被证明对SR[39]更有利。为此目的，SR的目标如下： <spanclass="math display">\[\hat{\theta}=\arg\min_\theta\mathcal{L}(\hat{I}_y,I_y)+\lambda\Phi(\theta),\]</span>​  其中， $ (_y,I_y) $ 表示生成的HR图像 $ _y $与地面真实图像Iy之间的损失函数，$ ()$为正则化项，为权衡参数。虽然SR最流行的损失函数是像素级均方误差（即像素损失），但更强大的模型倾向于使用多个损失函数的组合，这将在第3.4.1节中介绍。</p><h2 id="图像质量评估">图像质量评估</h2><p>​  图像质量是指图像的视觉属性，侧重于对观众的感知评估。一般来说，图像质量评估（IQA）方法包括基于人类感知的主观方法（即图像看起来的真实程度）和客观的计算方法。前者更符合我们的需求，但往往是耗时和昂贵的，因此后者是目前的主流。然而，这些方法之间不一定一致，因为客观方法往往不能非常准确地捕捉人类的视觉感知，这可能导致IQA结果[25]，[58]的很大差异。</p><p>​  此外，客观IQA方法进一步分为三种类型的[58]：使用参考图像进行评估的全参考方法，基于提取特征比较的简化参考方法，以及无任何参考图像的无参考方法（即盲IQA）。接下来，我们将介绍几种最常用的IQA方法，包括主观方法和客观方法。</p><h3 id="峰值信噪比">峰值信噪比</h3><p>​  峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）是有损变换（如图像压缩、图像嵌入绘制）中最常用的重建质量测量方法之一。对于图像的超分辨率，PSNR是通过图像之间的最大像素值（记为L）和均方误差（MSE）来定义的。给定N个像素的groundtruth图像I和重建I，PSNR定义如下：<spanclass="math display">\[\mathrm{PSNR}=10\cdot\log_{10}\left(\frac{L^2}{\frac{1}{N}\sum_{i=1}^N\left(I(i)-\hat{I}(i)\right)^2}\right),\]</span>​  其中，L等于255，在一般情况下使用8位表示。由于PSNR只与像素级MSE相关，只关注对应像素之间的差异而不是视觉感知，这往往导致在真实场景中表示重建质量的表现不佳，而我们通常更关注人类的感知。然而，由于需要与文献作品进行比较，且缺乏完全准确的感知指标，PSNR仍然是目前SR模型中使用最广泛的评价标准。</p><h2 id="操作通道">操作通道</h2><p>​  除了常用的RGB颜色空间外，YCbCr颜色空间也被广泛用于SR。在这个空间中，图像分别用Y、Cb、Cr通道表示，分别表示亮度、蓝差和红差的色度分量。虽然目前还没有公认的最佳实践来执行或评估超分辨率，但早期的模型倾向于在YCbCr空间[26]、[43]、[78]、[79]的Y通道上运行，而最近的模型倾向于在RGB通道[28]、[31]、[57]、[70]上运行。值得注意的是，在不同颜色的空间或通道上进行操作（培训或评估）可以使评估结果差异很大（高达4dB）[23]。</p><h2 id="超分辨率挑战">超分辨率挑战</h2><p>​  在本节中，我们将简要介绍图像SR的两个最流行的挑战，NTIRE [80]和PIRM[47]，[81]。</p><h3 id="ntire的挑战">NTIRE的挑战</h3><p>​  图像恢复和增强（NTIRE, The New Trends in Image RestorationandEnhancement）的新趋势挑战[80]与CVPR相结合，包括多个任务，如SR，去噪和着色。对于图像SR，NTIRE挑战是建立在DIV2K[42]数据集上，由双边降缩放轨迹和具有现实未知退化的盲轨迹组成。这些轨道在降解和比例因子上有所不同，旨在促进在理想条件和现实世界的不利情况下的SR研究。</p><h3 id="pirm挑战">PIRM挑战</h3><p>​  感知图像恢复和操作（PIRM, The Perceptual Image Restoration andManipulation）挑战与ECCV相结合，还包括多个任务。与NTIRE相比，PIRM的一个子挑战[47]侧重于生成准确性和感知质量之间的权衡，而另一个[81]侧重于智能手机上的SR。正如众所周知的[77]一样，针对失真的模型经常产生视觉上不愉快的结果，而针对感知质量的模型在信息上表现较差保真度。具体来说，PIRM根据均方根误差（RMSE）的阈值将感知扭曲平面划分为三个区域。在每个区域，获胜的算法是获得最佳感知质量的[77]，由NIQE[76]和Ma[66]评估。而在另一个子挑战[81]，智能手机上的SR，参与者被要求使用有限的智能手机硬件（包括CPU、GPU、RAM等）执行SR，评价指标包括PSNR、MS-SSIM和MOS测试。通过这种方式，PIRM鼓励对感知-失真的权衡进行高级研究，并在智能手机上驱动轻量级和高效的图像增强。</p><h1 id="监督超分辨率">监督超分辨率</h1><p>​  目前，研究人员已经提出了各种具有深度学习的超分辨率模型。这些模型侧重于有监督的SR，即同时用LR图像和相应的HR图像进行训练。虽然这些模型之间的差异非常大，但它们本质上是一组组件的一些组合，如模型框架、上采样方法、网络设计和学习策略。从这个角度来看，研究人员结合这些组件来建立一个集成的SR模型，以拟合特定的目的。在本节中，我们将集中精力模块化地分析基本组件（如图1所示），而不是孤立地介绍每个模型，并总结它们的优点和局限性。</p><h2 id="超分辨率框架">超分辨率框架</h2><p>​  由于图像超分辨率是一个不适定问题，如何进行上采样（即从LR输入生成HR输出）是关键问题。尽管现有模型的架构差异很大，但基于所采用的上采样操作及其在模型中的位置，它们可以归因于四个模型框架（如图2所示）。</p><h3 id="预上采样超分辨率">预上采样超分辨率</h3><p>​  由于直接学习从低维空间到高维空间的映射的困难，利用传统的上采样算法获得高分辨率的图像，然后利用深度神经网络进行细化是一个简单的解决方案。因此，Dong等人[22]，[23]首先采用预上采样SR框架（如图2a所示），并提出SRCNN来学习从插值的LR图像到HR图像的端到端映射。具体来说，使用传统方法（如双边插值）将LR图像上采样到具有所需大小的粗糙HR图像，然后在这些图像上应用深度cnn来重建高质量的细节。由于最困难的上采样操作已经完成，cnn只需要对粗糙的图像进行细化，这大大降低了学习难度。此外，这些模型可以以任意大小和缩放因子的插值图像作为输入，并给出与单尺度SR模型[26]性能相当的细化结果。因此，它逐渐成为[55]、[56]、[82]、[83]中最流行的框架之一，这些模型之间的主要区别是后验模型设计（第3.3节）和学习策略（第3.4节）。然而，预定义的上采样往往会引入副作用（如噪声放大和模糊），由于大多数操作是在高维空间进行的，时间和空间的成本比其他框架[43]，[84]高得多。</p><h3 id="后上采样超分辨率">后上采样超分辨率</h3><p>​  为了提高计算效率，充分利用深度学习技术自动提高分辨率，研究人员提出在低维空间中用端到端可学习层替换预定义的计算。在该框架的先驱作品[43]，[84]中，即如图2b所示的上采样后SR，LR输入图像在不提高分辨率的情况下输入深度cnn，在网络末端应用端到端可学习的上采样层。</p><p>​  由于计算成本较大的特征提取过程只发生在低维空间中，而分辨率最终只会提高，因此大大降低了计算复杂度和空间复杂度。因此，这种框架也已成为最主流的框架之一，[25]，[31]，[79]，[85]。这些模型的不同主要在于可学习的上采样层（第3.2节）、前CNN结构（第3.3节）和学习策略（第3.4节）等。</p><h3 id="逐步上采样超分辨率">逐步上采样超分辨率</h3><p>​  虽然上采样后的SR框架极大地降低了计算成本，但它仍存在一些缺点。一方面，上采样只进行了一步，这大大增加了对大尺度因子（如4,8）的学习差异。另一方面，每个比例因子都需要训练一个单独的SR模型，这无法应对多尺度SR的需要。为了解决这些缺点，拉普拉斯金字塔SR网络（LapSRN）[27]采用了渐进式上采样框架，如图2c所示。具体来说，该框架下的模型是基于cnn的级联，并逐步重建更高分辨率的图像。在每个阶段，图像被上采样到更高的分辨率，并通过cnn进行细化。</p><p>​  其他的工作，如MS-LapSRN[65]和渐进式SR（ProSR）[32]也采用了这个框架，并实现了相对较高的性能。与LapSRN和MSLapSRN使用中间重建图像作为后续模块的“基础图像”相比，ProSR保留主要信息流，并通过单个头部重建中间分辨率图像。</p><p>​  该框架下的模型将困难任务分解为简单任务，大大降低了学习难度，特别是在因素较大的情况下，并在不引入过多空间和时间成本的情况下应对多尺度SR。此外，一些具体的学习策略，如课程学习（第3.4.3节）和多监督（第3.4.4节），进一步降低学习难度，提高最终成绩。然而，这些模型也遇到了一些问题，如多阶段模型设计复杂和训练稳定性高，需要更多的建模指导和更先进的训练策略。</p><h3 id="迭代上下采样超分辨率">迭代上下采样超分辨率</h3><p>​  为了更好地捕捉LR-HR图像对的相互依赖关系，在SR[44]中加入了一种有效的反投影[12]迭代过程。该SR框架，即迭代上下采样SR（如图2d所示），尝试迭代应用反投影细化，即计算重建误差，然后将其重新融合，调整HR图像强度。具体来说，Haris等人[57]利用迭代上下采样层提出DBPN，将上采样和下采样层交替连接，并使用所有中间重建重建最终的HR结果。类似地，SRFBN[86]采用了一个迭代的上下采样反馈块，具有更密集的跳跃连接，并学习更好的表示。而用于视频超分辨率的RBPN[87]从连续的视频帧中提取上下文，并将这些上下文结合起来，通过一个反向投影模块产生循环输出帧。</p><p>​  该框架下的模型可以更好地挖掘LR-HR图像对之间的深层关系，从而提供更高质量的重建结果。然而，反投影模块的设计标准仍然不清楚。</p><p>​  由于该机制刚刚被引入到基于深度学习的SR中，因此该框架具有巨大的潜力，需要进一步的探索。</p><h2 id="上采样方法">上采样方法</h2><p>​  除了模型中的上采样位置外，如何进行上采样也非常重要。虽然有各种传统的上采样方法[20]、[21]、[88]、[89]，但利用cnn学习端到端上采样已逐渐成为一种趋势。在本节中，我们将介绍一些传统的基于插值的算法和基于深度学习的上采样层。</p><h3 id="基于插值的上采样">基于插值的上采样</h3><p>​  图像插值，a.k.a.图像缩放，是指调整数字图像的大小，并被广泛应用于与图像相关的应用程序中。传统的插值方法包括最近邻插值、双线性和双边插值、Sinc和兰氏重采样等。由于这些方法易于解释和易于实现，其中一些方法仍被广泛应用于基于cnn的SR模型中。</p><h4 id="最近邻插值">最近邻插值</h4><p>最近邻插值是一种简单、直观的算法。它为每个要被插值的位置选择最近的像素的值，而不考虑任何其他像素。因此，这种方法速度非常快，但通常会产生低质量的块状结果。</p><h4 id="双线性插值">双线性插值</h4><p>双线性插值（BLI）首先在图像的一个轴上进行线性插值，然后在另一个轴上进行，如图3所示。由于它导致了一个接受场大小为22的二次插值，因此在保持相对较快的速度的同时，它显示出了比近邻域插值更好的性能。</p><h4 id="二进制插值">二进制插值</h4><p>同样，双边插值（BCI）[10]在两个轴上分别进行三次插值，如图3所示。与BLI相比，BCI考虑了44个像素，并导致更流畅的结果，更少的伪影，但速度更低。事实上，具有抗锯齿的BCI是构建SR数据集的主流方法（即将HR图像降解为LR图像），也广泛应用于预上采样SR框架（第3.1.1节）。</p><p>​  事实上，基于插值的上采样方法仅基于其自身的图像信号来提高图像的分辨率，而没有带来更多的信息。相反，它们经常会引入一些副作用，如计算复杂性、噪声放大、模糊的结果。因此，目前的趋势是用可学习的上采样层取代基于插值的方法。</p><h3 id="基于学习的上采样">基于学习的上采样</h3><p>​  为了克服基于插值的方法的不足，以端到端学习上采样，在SR场中引入了转置卷积层和亚像素层。</p><h4 id="转置卷积层">转置卷积层</h4><p>转置卷积层，a.k.a.反卷积层[90]，[91]，试图执行与正常卷积相反的变换，即，基于类似于卷积输出大小的特征图来预测可能的输入。具体来说，它通过插入零和进行卷积来展开图像来提高图像的分辨率。以33 核的2SR为例（如图4所示），首先将输入的大小扩展原来的两倍，其中添加的像素值设置为0（图4b）。然后对核大小为33、步幅1和填充1进行卷积（图4c）。通过这种方式，输入被上采样了2倍，在这种情况下，接受域最多为22倍。由于转置卷积在保持与普通卷积兼容的连接模式的同时，使图像大小以端到端方式放大，因此在SR模型[57]、[78]、[79]、[85]中被广泛用作上采样层。然而，这一层很容易在每个轴[92]上造成“不均匀的重叠”，并且在两个轴上相乘的结果进一步创建了一个不同大小的棋盘状模式，从而损害了SR性能。</p><h4 id="亚像素层">亚像素层</h4><p>亚像素层[84]是另一个端到可学习的上采样层，通过卷积生成多个信道，然后进行上采样，如图5所示。在这一层中，首先应用卷积来产生具有s2倍通道的输出，其中s是比例因子（图5b）。假设输入大小为hw c，输出大小将为h w s2c。之后，进行整形操作(a.k.a.执行shuffle[84])来产生大小为sh sw c的输出（图5c）。在这种情况下，接受野最高可达33。由于端到端上采样方式，该层也被广泛应用于SR模型[25]、[28]、[39]、[93]。与转置卷积层相比，亚像素层具有更大的接受域，提供了更多的上下文信息，帮助生成更真实的细节。然而，由于感受野的分布是不均匀的，块状区域实际上共享相同的感受野，它可能会在不同块的边界附近产生一些伪影。另一方面，独立预测块状区域中的相邻像素可能会导致输出不平滑输出。因此，Gao等人[94]提出了PixelTCL，它将独立预测替换为相互依赖的序列预测，并产生更平滑、更一致的结果。</p><h4 id="meta上采样模块">Meta上采样模块</h4><p>以往的方法需要对缩放因子进行预细化，即针对不同的因子训练不同的上采样模块，但效率低，不符合实际需求。因此，Hu等人[95]提出了Meta上采样模块（如图6所示），首先基于元学习解决了任意比例因子的SR。具体来说，对于HR图像上的每个目标位置，该模块将其投影到LR特征图上的一个小补丁（即kk cin），根据投影偏移和比例因子预测卷积权值（即k k cincout），并进行卷积。这样，Meta上采样模块就可以通过单一模型的任意因素连续放大。由于大量的训练数据（同时训练多个因素），该模块在固定因素上可以表现出类似甚至更好的性能。虽然该模块在推理过程中需要预测权重，但上采样模块的执行时间只占特征提取[95]时间的1%左右。然而，该方法基于独立于图像内容的几个值来预测每个目标像素的大量卷积权值，因此在面对较大的放大倍数下，预测结果可能不稳定，效率较低。</p><p>​  目前，这些基于学习的层已经成为应用最广泛的上采样方法。特别是在上采样后框架（第3.1.2节）中，这些层通常在最终上采样阶段使用，基于低维空间提取的高级表示重建HR图像，从而在避免高维空间中压倒性操作的同时实现端到端SR。</p><h2 id="网络设计">网络设计</h2><p>​  网络设计是深度学习的重要组成部分之一。在超分辨率领域，研究人员在四种SR框架之上（第3.1节）应用各种网络设计策略来构建最终的网络。在本节中，我们将这些网络分解为网络设计的基本原则或策略，介绍它们，并逐一分析其优点和局限性。</p><h3 id="残差学习">残差学习</h3><p>​  在He等人[96]提出ResNet来学习残差而不是进行彻底的映射之前，残差学习已被SR模型[48]、[88]、[97]广泛使用，如图7a所示。其中，剩余学习策略大致可分为全局残差学习和局部残差学习。</p><h4 id="全局残差余学习">全局残差余学习</h4><p>由于图像SR是一种图像-图像转换任务，输入图像与目标图像高度相关，研究者尝试只学习它们之间的残差，即全局残差学习。在这种情况下，它避免了学习从一个完整图像到另一个完整图像的复杂转换，而是只需要学习一个残差映射来恢复缺失的高频细节。由于大多数区域的残差接近于零，模型的复杂性和学习差异大大降低。因此，它被广泛应用于SR模型[26]、[55]、[56]、[98]。</p><h4 id="局部残差学习">局部残差学习</h4><p>局部残差学习类似于ResNet[96]中的残差学习，用于缓解由于网络深度不断增加而导致的[96]退化问题，降低训练难度，提高学习能力。它也被广泛用于SR[70]、[78]、[85]、[99]。</p><p>​  在实际应用中，上述方法都是通过快捷连接（通常按一个小常量缩放）和元素加法实现的，不同之处在于前者直接连接输入和输出图像，而后者通常在网络内部不同深度的层之间添加多个快捷方式。</p><h3 id="递归学习">递归学习</h3><p>​  为了在不引入压倒性参数的情况下学习更高层次的特征，我们在SR字段中引入了递归学习，即以递归的方式多次应用相同的模块，如图7所示。</p><p>​  其中，16递归DRCN [82]采用单一卷积层作为递归单元，达到4141，远远大于SRCNN [22]的13 13，没有过多参数。DRRN [56]使用一个ResBlock[96]作为25次递归的递归单元，并且获得了比17-ResBlock基线更好的性能。后来Tai等人[55]提出了基于内存块的MemNet，该块由6个递归的重新块组成，每个递归的输出被连接起来，并经过额外的11个卷积进行记忆和遗忘。级联剩余网络（CARN）[28]也采用了类似的递归单元，包括几个重新块。最近，Li等人[86]采用了迭代上下采样SR框架，提出了一种基于递归学习的反馈网络，其中整个网络的权值在所有递归中共享。</p><p>​  此外，研究人员还在不同的部分使用了不同的递归模块。具体来说，Han等人[85]提出了双状态递归网络（DSRN）来在LR和HR状态之间交换信号。在每个时间步长（即递归），每个分支的表示都被更新和交换，以更好地探索LR-HR关系。</p><p>​  类似地，Laiet al.[65]使用嵌入和上采样模块作为递归单元，因此以性能损失很小为代价，大大减少了模型的大小。</p><p>​  一般来说，递归学习确实可以学习更高级的表示，而不引入过多的参数，但仍然不能避免高昂的计算成本。它本质上带来了消失或爆炸的梯度问题，因此一些技术，如残差学习（第3.3.1节）和多监督（第3.4.4节）经常与递归学习集成，以缓解这些问题[55]，[56]，[82]，[85]。</p><h3 id="多路径学习">多路径学习</h3><p>​  多路径学习是指将特征通过多条路径，这些路径执行不同的操作，并将它们融合回来以提供更好的建模能力。具体来说，它可以分为全局、局部和特定规模的多路径学习，如下所述。</p><h4 id="全局多路径学习">全局多路径学习</h4><p>全局多路径学习是指利用多条路径来提取图像的不同方面的特征。这些路径在传播过程中可以相互交叉，从而大大提高了学习能力。具体来说，LapSRN[27]包括一个以粗到细的方式预测子带残差的特征提取路径和另一个基于来自两条路径的信号重建HR图像的路径。同样，DSRN[85]利用两条路径分别在低维和高维空间中提取信息，并不断交换信息以进一步改进学习能力。像素递归超分辨率[64]采用条件反射路径来捕获图像的全局结构，并采用先验路径来捕获生成的像素的串行依赖性。相比之下，Ren等人[100]在模型的末端采用多条具有不平衡结构的路径进行上采样和融合。</p><h4 id="局部多路径学习">局部多路径学习</h4><p>在初始模块[101]的激励下，MSRN[99]采用了一个新的块来进行多尺度特征提取，如图7e所示。在这个块中，采用两个核大小为33 和5 5的卷积层同时提取特征，然后将输出连接并再次进行相同的操作，最后应用额外的11个卷积。快捷方式通过元素添加连接输入和输出。通过这种局部多路径学习，SR模型可以更好地从多个尺度中提取图像特征，进一步提高性能。</p><h4 id="特定尺寸的多路径学习">特定尺寸的多路径学习</h4><p>考虑到不同尺度的SR模型需要进行相似的特征提取，Lim等人[31]提出了特定尺度的多路径学习来应对单一网络的多尺度SR。具体地说，它们共享模型的主成分（即特征提取的中间层），并分别在网络的开始和结束时附加了特定尺度的预处理路径和上采样路径（如图7f所示）。在训练期间，只启用和更新与所选比例对应的路径。通过这种方式，所提出的MDSR[31]通过共享不同尺度的大部分参数，大大减少了模型的大小，并表现出与单尺度模型相当的性能。CARN[28]和ProSR [32]也采用了类似的尺度特异性多路径学习。</p><h3 id="密集连接">密集连接</h3><p>​  由于Huang等人[102]提出了基于密集块的DenseNet，密集连接在视觉任务中越来越流行。对于密集块中的每一层，前面所有层的特征图都被用作输入，其自己的特征图被用作所有后续层的输入，从而导致l层密集块中的（l*(l-1)/2）的连接。密集连接不仅有助于缓解梯度消失，增强信号传播，鼓励特征重用，而且还通过使用小增长率（即密集块中的通道数量）和连接所有输入特征图后压缩通道，大大减少模型大小。</p><p>​  为了融合低层次和高层次的特征，为重构高质量的细节提供更丰富的信息，在SR域中引入了密集的连接，如图7d所示。唐等[79]不仅采用密集块构造一个69层SRDenseNet，还插入密集连接不同密集块，也就是说，对于每一个密集块，所有之前的特征映射块被用作输入，和自己的特性映射被用作输入到所有后续块。MemNet[55]、CARN [28]、RDN [93]和ESRGAN[103]也采用了这些层级和块级的密集连接。DBPN[57]也广泛地采用了密集连接，但它们的密集连接位于所有的上采样单元之间，下采样单元也是如此。</p><h3 id="注意力机制">注意力机制</h3><h4 id="通道注意力">通道注意力</h4><p>考虑到不同通道之间特征表示的相互依赖和相互作用，Hu等人[104]提出了一个“挤压和激励”块，通过明确建模通道相互依赖来提高学习能力，如图7c所示。在这个块中，使用全局平均池化（GAP）将每个输入信道压缩到一个信道描述符（即一个常数）中，然后将这些描述符输入到两个密集的层中，为输入信道生成信道缩放因子。最近，Zhang等人[70]将通道注意机制与SR结合起来，提出了RCAN，显著提高了模型的表示能力和SR性能。为了更好地学习特征相关性，Dai等人的[105]进一步提出了一个二阶信道注意（SOCA）模块。SOCA通过使用二阶特征统计而不是GAP自适应地调整信道特征，并能够提取更多信息性和区别性的表示。</p><h4 id="非本地注意力">非本地注意力</h4><p>大多数现有的SR模型的局部接受域非常有限。然而，一些遥远的对象或纹理可能对局部补丁的生成非常重要。因此，Zhang等人[106]提出了局部和非局部注意块来提取捕获像素之间的长期依赖关系的特征。具体地说，他们提出了一个用于提取特征的主干分支，以及一个（非）局部掩码分支，用于自适应地重新调整主干分支的特征。其中，局部分支采用编码器-解码器结构来学习局部注意，而非局部分支采用嵌入式高斯函数来评估特征图中每两个位置指标之间的成对关系，以预测尺度权值。通过这种机制，该方法很好地捕捉了空间注意力，并进一步提高了表示能力。同样，Dai等人[105]也采用了非局部注意机制来捕获长距离空间背景信息。</p><h3 id="先进的卷积">先进的卷积</h3><p>​  由于卷积操作是深度神经网络的基础，研究人员也试图改进卷积操作，以提高性能或提高效率。</p><h4 id="膨胀卷积">膨胀卷积</h4><p>众所周知，上下文信息有助于生成SR生成现实细节。因此，Zhang等人[107]在SR模型中用扩张卷积来取代常见的卷积，增加了两次以上，获得了更好的性能。</p><h4 id="集团卷积">集团卷积</h4><p>受轻量级CNNs的最新进展的推动，[108]，[109]，Hui等人[98]和Ahn等人[28]分别提出了IDN和CARN-M，用组卷积代替香草卷积。正如之前的一些工作所证明的那样，组卷积大大减少了参数和操作的数量，而牺牲了一点性能损失[28]，[98]。</p><h4 id="深度可分离卷积">深度可分离卷积</h4><p>自从Howard等人[110]提出深度可分离卷积以实现有效的卷积以来，它已经被扩展到各个领域。具体地说，它由一个因子分解的深度卷积和一个点态卷积（即11个卷积）组成，因此只在很小的情况下减少了大量的参数和操作降低精度的[110]。最近，Nie等人的[81]采用了深度可分离卷积，并大大加速了SR体系结构。</p><h3 id="区域递归学习">区域递归学习</h3><h3 id="金字塔池化">金字塔池化</h3><h3 id="小波变换">小波变换</h3><h3 id="desubpixel">Desubpixel</h3><h3 id="xunit">xUnit</h3><h2 id="学习策略">学习策略</h2><h3 id="损失函数">损失函数</h3><h4 id="pixel-loss">Pixel Loss</h4><p>像素损失测量两个图像之间的像素级差异，主要包括L1损失（即平均绝对误差）和L2损失（即均方误差）：<spanclass="math display">\[\mathcal{L}_{\mathrm{pixel}\perp1}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}|\hat{I}_{i,j,k}-I_{i,j,k}|\\\mathcal{L}_{\mathrm{pixel}\perp2}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}(\hat{I}_{i,j,k}-I_{i,j,k})^{2},\]</span></p><h4 id="content-loss">Content Loss</h4><p>为了评价图像的感知质量，将内容损失引入SR[29]，[127]。具体来说，它使用预先训练好的图像分类网络来测量图像之间的语义差异。将该网络表示为f，提取的第1层高级表示表示为fðlÞðIÞ，内容损失表示为两幅图像的高级表示之间的欧氏距离，如下：<spanclass="math display">\[\mathcal{L}_{\mathrm{content}}(\hat{I},I;\phi,l)=\frac{1}{h_lw_lc_l}\sqrt{\sum_{i,j,k}(\phi_{i,j,k}^{(l)}(\hat{I})-\phi_{i,j,k}^{(l)}(I))^2},\]</span></p><h3 id="批量规范化">批量规范化</h3><h3 id="课程学习">课程学习</h3><h3 id="多重监督">多重监督</h3><h3 id="其他改进">其他改进</h3><h4 id="上下文网络融合">上下文网络融合</h4><h4 id="数据增强">数据增强</h4><h4 id="多任务学习">多任务学习</h4><h4 id="网络插值">网络插值</h4><h4 id="自我整合">自我整合</h4><h2 id="最先进的超分辨率模型">最先进的超分辨率模型</h2><p>​  近年来，基于深度学习的图像超分辨率模型受到了越来越多的关注，并取得了最先进的性能。在前面的章节中，我们将SR模型分解为特定的组件，包括模型框架（第3.1节）、上采样方法（第3.2节）、网络设计（第3.3节）和学习策略（第3.4节），对这些组件进行分层分析，并确定它们的优点和局限性。事实上，今天大多数最先进的SR模型基本上都可以归因于我们在上面总结的多种策略的组合。例如，RCAN[70]最大的贡献来自于通道注意机制（第3.3.5节），它还采用了其他策略，如亚像素上采样（第3.3.2.2节）、残差学习（第3.3.1节）、像素L1损失（第3.4.1节）和自集成（第3.5.5节）。以类似的方式，我们总结了一些具有代表性的模型及其关键策略，如表2所示。</p><figure><imgsrc="./../postimages/Image_Super-Resolution/image-20240619184059043.png"alt="image-20240619184059043" /><figcaption aria-hidden="true">image-20240619184059043</figcaption></figure><p>在上面，“Fw", "Rec.", "Res"，"Dense","Att."分别表示SR框架、上采样方法、递归学习、残差学习、密集连接、注意机制</p><p>​  除了SR精度外，效率是另一个非常重要的方面，不同的策略对效率有或多或少的影响。因此，在前面几节中，我们不仅分析了所提出策略的准确性，而且还指出了对效率影响较大的策略的具体影响，如后上采样（3.1.2节）、递归学习（3.3.3.2节）、密集连接（3.3.3.4节）、xUnit（3.3.11节）。我们还对一些具有代表性的SR模型的SR精度（即PSNR）、模型大小（即参数数）和计算成本（即多加数）等方面的SR模型进行了基准测试，如图8所示。精度是通过在4个基准数据集（即Set5[48]，Set14 [49]，B100 [40]和Urban100[50]）上的PSNR的平均值来测量的。模型大小和计算成本用PyTorch-光学传感器[157]计算，其中输出分辨率为720p（即1080720）。所有的统计数据都是根据原始论文或根据官方模型计算得出的，比例因子为2。为了更好地查看和比较，我们还提供了一个交互式的在线版本1。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</title>
      <link href="/UnionFormer/"/>
      <url>/UnionFormer/</url>
      
        <content type="html"><![CDATA[<center>UnionFormer: Unified-Learning Transformer with Multi-View Representationfor Image Manipulation Detection and Localization <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a></center><center><span class="math inline">\(\text{Shuaibo Li}^{1,2}\quad\text{WeiMa}^{1\dagger}\quad\text{Jianwei Guo}^2\quad\text{ShibiaoXu}^3\quad\text{Benchong Li}^1\quad\text{Xiaopeng Zhang}^2\)</span></center><center>北京理工大学1、MAIS(中国科学院自动化研究所)2、北京邮电大学3</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/UnionFormer/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  我们提出了一个新的框架，通过统一学习集成了三个视图上的篡改线索，用于图像操作检测和定位。特别地，我们构建了一个BSFI-Net，从RGB和噪声视图中提取篡改特征，在调节不同尺度上的空间一致性的同时，增强了对边界伪影的响应性。此外，为了探索对象之间的不一致性作为一种新的线索视角，我们将对象一致性建模与篡改检测和定位结合成一个三任务统一的学习过程，使它们能够相互促进和改进。</p><p>​  因此，我们在多尺度监督下获得了一个统一的操作鉴别表示，从三个角度整合信息。这种集成便于高效的并行检测和定位篡改。我们在不同的数据集上进行了大量的实验，结果表明，该方法在篡改检测和定位方面优于最先进的方法。</p><h1 id="引言">1. 引言</h1><p>​  数字图像篡改可分为三大类[19]：拼接，即将区域从一幅图像复制到另一幅图像；复制-移动，包括复制或移动同一图像中的元素；移除，删除图像部分和创建视觉一致的内容以掩盖改变的过程。这些操作在被篡改区域和周围环境之间留下痕迹，造成真实区域和伪造区域之间的不一致。与传统的强调高级语义信息的传统检测或分割任务不同，图像篡改检测优先考虑局部语义无关的线索，以区分真实性，而不是语义内容。因此，篡改检测的关键挑战是学习结合不同层次信息并捕获真实和篡改区域之间多尺度不一致的通用特征。以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层[23,27,40,71]的特征，不能充分表示篡改痕迹。受[9,12,67]的启发，我们设计了一个专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork），并将其作为特征编码器集成到我们的框架中。BSFI-Net是一个并行的cnn-Transformer结构，它可以加强边缘响应，同时有效地在局部特征和全局表示之间进行交互，以探索不同尺度上图像内部的一致性。<br/>​  另一方面，许多在RGB视图中难以察觉的篡改伪影在噪声视图中变得明显明显。使用固定的[18]或可学习的高通滤波器[6,35,66]将RGB图像转换为噪声图，可以抑制内容，并突出显示低级的伪造线索。因此，开发一种同时建模RGB和噪声维度的多视图策略对于检测细微的篡改痕迹至关重要。我们的框架采用了一个双流架构来独立地构建RGB和噪声视图的表示，随后合并它们以提高鉴别能力和泛化性。此外，我们还结合了对比监督，以改善这两种观点之间的协作。<br/>​  此外，为了创建空间相干和语义一致的图像，篡改操作总是改变整个对象来隐藏证据，即执行对象级操作。目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息。相反，我们认为图像操作检测应该不仅仅是识别分布外的像素或补丁，以捕获由操作导致的对象一致性和分布的异常。由于扩散模型[4,5,20,30,44,65,69]生成的超真实的篡改图像，利用对象视图信息变得特别重要。基于扩散的模型[4,30,44]反复更新了整个图像的初始噪声，增强了空间连续性，留下了更少的RGB和噪声痕迹。此外，与真实的图像源不同，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。最近的扩散模型[20,29,55,64]试图通过采用以对象为中心的方法来解决这个问题，强调了使用对象视图线索进行篡改检测的必要性和可行性。然而，创建和集成这样的新视图与其他视图，以篡改伪影表示是一个重大的挑战，需要新的架构和学习策略。<br/>​  考虑到上述要点，我们引入了UnionFormer，一个用于图像操作检测和定位的多视图表示的统一学习transformer框架，如图1所示。</p><p><img src="../postimages/UnionFormer/image-20240617100605323.png"alt="image-20240617100605323" /><br/>图1.UnionFormer的组成概述。我们通过整合来自三个视图表示的篡改线索来实现同时的篡改检测和定位，每个视图由不同的颜色背景表示。我们通过BSFI-Net获得了RGB和噪声视图下的表示，并在统一学习中构建了基于两者的对象视图表示。同时，将三个视图的信息交互融合成统一的操作判别表示（UMDR,unified manipulation discriminative representation）进行检测和定位</p><p>​  首先，我们使用BSFI-Net作为特征编码器，获得在RGB和噪声视图下的通用化特征，并将其进行组合。然后，我们利用融合的特征进行一个单一化的学习过程，其中包括三个子任务：对象一致性建模、伪造检测和伪造定位。在统一学习中，我们的模型建立了对象视图表示，并将三个视图信息集成到一个统一的操作鉴别表示（UMDR,unified manipulation discriminativerepresentation）中，同时完成伪造检测和定位。综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一种新的图像取证transformer框架，UnionFormer。通过多尺度监督的统一学习，整合三个视角的信息，同时执行图像操作检测和定位。</li><li>我们引入了BSFI-Net，一种用于高级人工表示学习的混合网络结构，它增强了边界响应，同时揭示了不同层次的局部不一致性。</li><li>通过对UMDR的统一学习，我们构建了一种创新的对象视图表示方法，能够从三个视图中捕获对象之间的不一致性和聚合信息，用于伪造检测。</li><li>我们通过各种基准进行了全面的实验，证明了我们的方法在检测和定位任务中都获得了最先进的结果。</li></ul><h1 id="方法">2. 方法</h1><p>​  在本节中，我们首先提供对工会成员的概述和对每个组件的详细介绍。我们的目标是充分利用来自三个视图的丰富工件来同时进行篡改检测和定位。我们通过在多尺度监督下的统一学习过程来实现这一目标。<br/>​  如图1所示，首先使用受约束的CNN[7]将输入的RGB图像X转换为噪声视图表示N = C(X)，可以显示低级的篡改。<br/>​  然后，将X和N分别输入边界敏感特征交互网络（BSFI-Net）进行特征编码。高频边缘特征(H)与X或N一起作为BSFI-Net的输入，以提高边缘响应性。这使得我们能够在RGB和噪声视图下获得可推广的和可鉴别的特征，构造两个特征金字塔$ f_r = _1(X,H), f_n = _2(N,H) $。<br/>​  随后，我们使用区域建议网络（RPN）[51]从特征fr中获得一组感兴趣的区域（RoIs），用pi表示。从fr和fn中提取RoI信息，然后扁平得到建议的嵌入表示，记为ri，ni。将每个方案的RGB特征ri和噪声特征ni连接起来，生成融合的方案特征di，并将其输入到I变压器编码器层。<br/>​  在统一学习阶段，我们处理了三个子任务：建模对象的一致性、真实性的二进制分类和篡改区域定位。在转换器编码器之后，将伪造-判别查询嵌入DI输入到统一操作判别表示部分，对三个子任务生成三个预测。如图1所示，我们对三个子任务采用了具有统一形式的多尺度监督，包括Lcls、Locm和Lloc。</p><figure><img src="../postimages/UnionFormer/image-20240618124653610.png"alt="image-20240618124653610" /><figcaption aria-hidden="true">image-20240618124653610</figcaption></figure><h2 id="特征交互编码">2.1 特征交互编码</h2><h3 id="rgb和噪声视图表示">2.1.1 RGB和噪声视图表示。</h3><p>​  在特征编码阶段，我们利用一个双流结构来利用来自RGB和噪声视图的线索。RGB流被设计为捕获视觉上明显的篡改伪影，而噪声流旨在探索被篡改区域和真实区域之间的分布不一致性。我们利用[7]中提出的可学习约束卷积层将RGB图像转换为噪声视图。如第2节所述，被篡改区域及其周围环境的边缘表现出更明显的篡改线索。因此，我们增强了两个流中的高频边缘信息，将网络的响应集中在被篡改的区域。具体来说，我们利用离散余弦变换（DCT）将图像数据X转换为频域，然后应用高通滤波器得到高频分量。然后，我们将高频分量转换回空间域，以促进特征交互和保持局部一致性。因此，我们得到的边缘增强信息H如下：<spanclass="math display">\[H=\mathcal{T}_d^{-1}\left(\mathcal{F}_h\left(\mathcal{T}_d(X),\beta\right)\right)\]</span>​  其中Td表示DCT，Fh表示高通滤波器，β为阈值。我们将X和N分别输入到BSFI-Net中，以及H来进行特征编码，如图2所示。</p><h3 id="边界敏感特征交互网络">2.1.2 边界敏感特征交互网络。</h3><p>​  除了增强边界响应外，集成局部特征和全局表示对图像伪造检测也至关重要。这就要求进行全面分析在不同尺度上的图像内部的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-Transformer并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><br/>图2.BSFI-Net的概述。FCU表示特征耦合单元，BOB表示边界向块。</p><p>​  如图2所示，CNN分支作为主分支，以一个RGB或噪声图像作为输入，对局部信息进行编码。变压器分支以输入作为边缘增强信息H，引导CNN分支聚焦于被篡改的区域，并将图像补丁之间的长距离不一致传输给它。我们使用[48]提出的特征耦合单元（FCU）来消除来自CNN分支的特征映射和来自transformer分支的补丁嵌入之间的错位。此外，我们还设计了一个面向边界的块（BOB），以方便将高级补丁一致性和边界信息从变压器分支传输到CNN分支，从而指导CNN分支。<br/>​  CNN分支由5个卷积块组成，类似于ResNet构造[24]。与[16,48]一样，transformer分支由5个重复的transformer块组成，由一个多头自注意模块和一个MLP块组成。采用与ViT[16]相同的令牌化操作。在FCU中，在添加补丁嵌入和CNN特征之前，使用1×1的卷积和重新采样来对齐通道和空间维度。在BOB中，CNN分支的特征映射被输入1×1卷积层、批归一化层、s型层，并通过双线性插值上采样到高分辨率。然后，将来自CNN分支的特征与长距离判别权值进行元素级乘法。我们将BSFI-Net作为特征编码器进行预训练，生成RGB和噪声视图表示，特征金字塔网络[38]基于中间特征映射{C2、C3、C4、C5}生成两个特征金字塔fr，fn。培训细节详见第4.1节。</p><h2 id="特征对比性协作">2.2 特征对比性协作</h2><p>​  在特征协作阶段，受[51,56]的启发，我们首先使用一个基于RGB特征金字塔fr的区域建议网络（RPN）来生成一组感兴趣的区域（RoIs）。然后，我们利用RoIAlign[25]从两个流的特征金字塔fr和fn中提取RoIs的信息。除了特征连接之外，我们还采用对比监督来促进两个视图之间的协作。我们将来自不同流的被篡改的建议视为积极建议，被篡改的建议和真实建议被指定为负对。在InfoNCE损失[47,67]之后，对比度损失被定义为：<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{con}}=-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{1})}-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{2})}\end{aligned}\]</span>​  式中，s0表示正对之间的相似性，s1表示RGB篡改嵌入与噪声真实嵌入之间的相似性，s2表示RGB真实嵌入与噪声篡改嵌入之间的相似性。对比损失Lcon引入统一学习监督，将在第3.3节进行讨论。</p><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><h2 id="具有多尺度监督下的统一学习">2.3 具有多尺度监督下的统一学习</h2><p>​  <strong>Transformer编码器。</strong>我们的统一学习模块是一个仅限编码器的transformer架构，它处理融合的提议嵌入二，以及它们的特定位置编码作为输入。在转换器编码器的每一层中，自我注意机制通过不同的建议嵌入来聚合信息，并捕获它们的长距离依赖关系，这意味着对象的一致性。详细地说，我们使用了一个变压器解码器，具有六层，宽度为512，和8个注意头。变压器内的前馈网络（FFN）的隐藏大小为2048。在转换器编码器之后，我们生成判别查询嵌入DI，并输入统一操作判别表示（UMDR）部分，以生成三个子任务的预测，即。对象一致性建模、图像操作检测和定位。</p><p>​  <strong>统一伪造判别表示</strong>。在转换器编码器之后，DI中的每个篡改判别查询都表示对应建议的三个视图中的篡改线索。图3显示了三个子任务的学习过程。</p><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure><p>图3。多尺度监督下的UMDR学习。图像内部在不同尺度上的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-变压器并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p>​  UMDR是在真实性分类、对象一致性建模和操作定位分支的监督下学习的。与DETR[9]和SOLQ[12]一样，分类分支是一个完全连接的（FC）层，用来预测真实性可信度Pˆc。目标一致性建模分支是一个多层感知（MLP），隐藏大小为256，用于预测目标空间信息Pˆo。操作定位分支也是一个隐藏大小为1024的多层感知来预测定位掩码向量Pˆm。对前两个分支机构的监管类似于DETR[9]。在第三个分支中，我们利用对地面真实掩码进行编码得到的掩模向量作为监督信息。在推理过程中，将压缩后的编码过程应用于Pˆm来重构定位掩码。在压缩编码中，我们利用主成分分析（PCA）将二维空间二值掩模转换为一维掩模向量。</p><p>​  <strong>损失函数。</strong>UnionFormer监督的总体损失职能可表示为：<spanclass="math display">\[\mathcal{L}_{union}=\lambda_{cls}\cdot\mathcal{L}_{cls}+\mathcal{L}_{ocm}+\lambda_{loc}\cdot\mathcal{L}_{loc}+\beta\cdot\mathcal{L}_{con},\]</span>​  其中Lcls表示分类的focal损失[39]。Lloc表示定位掩码向量监督的L1损失。Lcon是在第3.2节中引入的对比性学习损失。λcls、λloc、β是相应的调制系数。Locm是对象一致性建模的损失，其定义为：<spanclass="math display">\[\mathcal{L}_{\mathrm{ocm}}=\lambda_{L_1}\cdot\mathcal{L}_{L_1}+\lambda_{gious}\cdot\mathcal{L}_{gious}\]</span>​  其中LL1和Lleam为L1损失和广义IoU损失[52]，与DETR相同。λL1和λgious是对应的系数。在[12]之后，Lloc不包括在二部匹配过程中。</p><h1 id="实验">3. 实验</h1><h2 id="实验设置">3.1 实验设置</h2><p>​  <strong>训练</strong>。我们使用了一个大规模的训练数据集，包括各种类型的篡改和真实的图像。它分为五个部分：1) CASIA v2 [14]，2)Fantastic Reality[32]，3)Tampered COCO,，来自COCO2017数据集[37]，4)Tampered RAISE，基于RAISE数据集[11]构建，5)从COCO2017和RAISE数据集中选择的原始图像。我们在合成数据中随机添加高斯噪声或应用JPEG压缩来模拟现实场景中的视觉质量和篡改轨迹。在训练过程中，我们依次分三个阶段对BSFI-Net、RPN和UnionFormer进行训练。</p><p>​  <strong>测试。</strong>为了全面评估和比较我们的模型与各种最先进的方法，我们使用了6个公开可用的测试数据集和另一个由混合扩散模型[4]创建的超真实篡改图像数据集。具体来说，我们使用了CASIAv1 [14]、Columbia[26]、Coverage[61]、NIST16 [22]、IMD20 [46]和CocoGlide[23]。然后，我们构建了BDNIE，包括512张由先进的混合扩散模型生成的超真实的假图像，用于文本驱动的自然图像编辑。训练和测试数据的细节载于补充资料。</p><p>​  <strong>评价指标。</strong>我们评估了该方法在图像篡改检测和定位任务中的性能。对于定位图像操作的任务，我们报告了像素级的曲线下面积（AUC）和F1分数，同时使用最佳的和固定的0.5阈值。对于[23]之后的检测任务，我们采用图像级AUC和平衡精度，同时考虑假报警和遗漏检测，在这种情况下，阈值设置为0.5。为了保证比较的公平性和准确性，我们从文献[23,59]中取出了其他方法的一些结果值。</p><p>​  <strong>实施细节。</strong>BSFI-Net采用AdamW优化器[41]进行了100个周期的交叉熵损失训练，批处理大小为512，权重衰减为0.05。初始学习速率被设置为0.001，并在余弦时间表中衰减。</p><p>​  在与Lunion一起训练完整的UnionFormer时，受[56,63]的启发，我们采用36周期（3×）计划来训练UnionFormer进行2.7×105次迭代，批大小为16。在这个阶段还使用了一个AdamW优化器。学习速率在开始时被设置为10−4，并在1.8×105和2.4×105迭代时乘以0.1。</p><h2 id="与最先进的技术相比较">3.2 与最先进的技术相比较</h2><p>​  <strong>Baseline。</strong>为了确保公平和准确的比较，我们只选择了最先进的方法，其中作者提供了预训练的模型，发布的源代码，或在通用标准[27,40,59]下进行评估。为了减少偏差，我们只考虑了在不与测试数据集重叠的数据集上训练的方法或版本。详细地说，我们包括了7种最先进的方法：MantraNet[62]，SPAN[27]，PSCC-Net[40]，MVSS-Net[13]，CAT-Netv2[34]，ObjectFormer[59]，和TruFor[23]。</p><p>​  <strong>定位结果。</strong>表2和表1分别显示了基于像素级AUC和F1评分指标的图像篡改定位结果。排名最高的方法用粗体表示，一条水平线表示排名第二的方法，在表4和表3中也采用了相同的注释。</p><figure><img src="../postimages/UnionFormer/image-20240617115640307.png"alt="image-20240617115640307" /><figcaption aria-hidden="true">image-20240617115640307</figcaption></figure><p>​  我们的方法在所有数据集上展示了像素级AUC评估的最佳性能。</p><figure><img src="../postimages/UnionFormer/image-20240617115601335.png"alt="image-20240617115601335" /><figcaption aria-hidden="true">image-20240617115601335</figcaption></figure><p>​  对于f1评估，我们的方法在所有数据集上排名最好或第二。平均而言，无论是否使用最优或固定的阈值，我们都获得了显著的优势。事实上，在包含基于扩散的局部操作的相对新颖的CocoGlide数据集上，我们在两个阈值上分别比排名第二的TruOfor高出2.2%和1.3%。这是由于联合前体构建的对象视图伪影表达式，它可以揭示由扩散模型生成的区域和真实区域之间的不一致性。这些比较表明，我们的方法具有较强的泛化和捕获篡改伪的能力。</p><p>​  <strong>检测结果。</strong>表4为篡改检测的比较结果。</p><figure><img src="../postimages/UnionFormer/image-20240617115836976.png"alt="image-20240617115836976" /><figcaption aria-hidden="true">image-20240617115836976</figcaption></figure><p>​  在[23]之后，我们使用定位映射的最大值作为未明确为检测任务设计的方法的检测统计量。UnionFormer在除Columbia外的所有数据集上都取得了最佳的性能，并在平均结果上显示了显著的优势，无论是通过AUC还是平衡精度测量。正如[13,23]中提到的，精度对阈值选择很敏感，如果没有良好校准的数据集，很难确定。然而，我们的方法和次要的TruFor在这个要求很高的场景中取得了值得称赞的结果。我们在平均AUC和精度上分别保持了2.5%和2%的领先优势。这一优势主要归因于我们的框架的统一学习过程。统一学习通常会促进对定位和检测任务的相互增强。通过统一的操作鉴别表示，掌握了两个子任务，进一步提高了模型的性能。</p><p>​  <strong>鲁棒性评估。</strong>我们通过对NIST16数据集图像应用图像失真，验证了UnionFormer的鲁棒性。在[40,59]之后，我们包括了四种类型的畸变：1)将图像的大小改变到不同的尺度；2)应用核大小为k的高斯模糊；3)添加以标准偏差σ为特征的高斯噪声；4)对图像进行JPEG压缩，使用质量因子q。我们比较了像素级AUC与其他方法的性能。表3显示，我们的方法对各种失真操作表现出鲁棒性，优于其他方法。</p><figure><img src="../postimages/UnionFormer/image-20240617120221607.png"alt="image-20240617120221607" /><figcaption aria-hidden="true">image-20240617120221607</figcaption></figure><h1 id="可视化结果">4. 可视化结果</h1><h2 id="定性比较">4.1 定性比较</h2><p>​  <img src="../postimages/UnionFormer/image-20240618103731575.png"alt="image-20240618103731575" /></p><p>​  图4显示了跨不同数据集的定位结果。我们的方法可以准确地定位被篡改的区域，预测更详细和清晰的边界。这是由于我们的多视图特征捕获和BSFI-Net，其中频率信息增强了边缘响应，而分支之间的交互作用增强了特征的泛化和识别。由于对对象视图线索的建模和统一的学习框架，我们的方法在具有挑战性的BDNIE数据集上取得了令人满意的结果，而其他方法都失败了。</p><h2 id="不同视图表示法的可视化">4.2 不同视图表示法的可视化</h2><p>​  在图5中，我们可视化了BSFI-Net中变压器分支的噪声特征和边缘引导特征。</p><figure><img src="../postimages/UnionFormer/image-20240618103938959.png"alt="image-20240618103938959" /><figcaption aria-hidden="true">image-20240618103938959</figcaption></figure><p>如列1到4所示，一些图像在RGB视图中可能看起来很自然，但它们被篡改/真实的部分很容易在频域或噪声视图中被容易区分出来。第5列和第6列显示了由一个CNN分支和BSFI-Net的双分支生成的RGB特性。与只使用CNN分支相比，BSFI-Net更准确地激活了被篡改的区域，这得益于变压器分支提供的边缘引导和长距离线索。</p><p>​  此外，我们还定量地分析了对象视图，如图6所示。</p><figure><img src="../postimages/UnionFormer/image-20240618104100130.png"alt="image-20240618104100130" /><figcaption aria-hidden="true">image-20240618104100130</figcaption></figure><p>​  在统一学习阶段，我们从transformer编码器中推导出亲和矩阵Ai。基于Ai，我们随机选择提案嵌入的一个子集，计算它们与其他建议的平均亲和力，记为ei。然后将ei归一化到范围[0,1]，并作为一个颜色系数来可视化建议，较浅的颜色表示较低的亲和力。结果表明，使用伪造物体的提案与其他区域的平均亲和力较低，这表明UMDR能够捕捉真实物体和虚假物体之间的不一致性。</p><h1 id="消融研究">5. 消融研究</h1><p>​  我们进行了消融研究，以评估我们的方法中关键成分的影响。定量结果见表5。</p><figure><img src="../postimages/UnionFormer/image-20240618104307449.png"alt="image-20240618104307449" /><figcaption aria-hidden="true">image-20240618104307449</figcaption></figure><p>​  我们可以观察到，通过在第一个基线模型上添加噪声流，CASIAv1的AUC得分增加8.7%，NIST 16增加8.3%，同时进一步增加对象视图表示，CASIAv1继续增加10.7%，NIST16继续增加7.4%。这证明了噪声和对象视图表示的有效性。此外，当缺乏对比监督，或BSFI-Net被ResNet-50[24]取代时，模型的性能会显著下降。这突出了两个流之间的交互的有效性和BSFI-Net在描述伪造制品方面的特殊能力。</p><p>​  BSFI-Net中的BOB和FCU模块改善了其两个分支之间的交互作用，并有效地消除了它们之间的特征失调。当单独去除BOB或FCU时，整体模型在NIST16数据集上的定位AUC得分分别下降了4.8%和6.3%。</p><p>​  我们进一步进行了实验，研究了UMDR中几个关键因素的影响。λloc，Locm，掩码向量维度nv，以及压缩编码的类型。</p><figure><img src="../postimages/UnionFormer/image-20240618104459423.png"alt="image-20240618104459423" /><figcaption aria-hidden="true">image-20240618104459423</figcaption></figure><p>​  我们比较了三种压缩编码方法：稀疏编码[15]、离散余弦变换（DCT）[2]和主成分分析（PCA）[1]。如表6所示，当设置对比损失时，以PCA为编码类型，并将λloc和Locm分别设置为1和256时，该模型在NIST16数据集上表现最好。</p><h1 id="结论">6. 结论</h1><p>​  在本文中，<br/>​  我们介绍了UnionFormer，一个联合学习transformer框架，它利用来自三个不同视图的线索来进行图像操作检测和定位。UnionFormer使用BSFI-Net作为特  征编码器，在RGB和噪声视图下提取具有高度区分性的特征。然后，通过三个任务的统一学习过程，UnionFormer建模了对象之间的不连续性，即对象视图表示，并学习统一的判别表示。从三种观点整合信息的统一表示具有较强的通用性和区分性。它可以准确地识别各种图像操作，无论是传统的手动编辑还是基于扩散模型的自然语言驱动的篡改。此外，统一的学习框架使子任务的相互增强，实现了高精度的检测和定位。在不同的数据集上进行的综合实验证明了该方法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告1</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/</url>
      
        <content type="html"><![CDATA[<h1 id="图像超分辨率的深度学习综述">图像超分辨率的深度学习：综述</h1><p><ahref="%5BDeep%20Learning%20for%20Image%20Super-Resolution:%20A%20Survey%20%7C%20IEEE%20Journals%20&amp;%20Magazine%20%7C%20IEEE%20Xplore%5D(https://ieeexplore.ieee.org/abstract/document/9044873)">TPAMI2020</a></p><p>本综述的主要贡献有三个方面：</p><p>​  1)我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。</p><p>​  2)我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。</p><p>​  3)我们讨论了这些挑战和开放的问题，并确定了新的趋势和未来的发展方向，为社区提供了一个深刻的指导。</p><p><strong><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240606164746361.png"alt="image-20240606164746361" /></strong></p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094212179.png"alt="image-20240607094212179" /><figcaption aria-hidden="true">image-20240607094212179</figcaption></figure><h1 id="选取网络">选取网络</h1><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/SR.drawio.png"alt="SR.drawio" /><figcaption aria-hidden="true">SR.drawio</figcaption></figure><p>​  SRCNN</p><p>​  SRResNet</p><p>​  VDSR</p><p>​  CARN</p><p>​  MemNet</p><h1 id="最新图像超分辨率">最新图像超分辨率</h1><p><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28194">AAAI2024</a></p><p>AdaFormer: Efficient Transformer with Adaptive Token Sparsificationfor Image Super-resolution</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094111731.png"alt="image-20240607094111731" /><figcaption aria-hidden="true">image-20240607094111731</figcaption></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<p>《基于深度学习的图像增强算法》——图像超分辨</p><p>基本要求：</p><ul><li><p>训练集T91-train,测试集Set5-test，不能更改。</p></li><li><p>需要分析网络架构不同所引起的性能变化，并在提交报告中对比分析。（可以是模型不同，也可以只是层数不同）</p></li><li><p>报告内容必须包含数据处理部分、模型部分、训练部分和测试部分。</p></li><li><p>超分辨任务b可以只选择放大倍数为4倍（横纵各4倍，图像大16倍）。</p></li><li><p>老师使用的模型：</p><ul><li><p>SRCNN:</p><ul><li><p>结构：</p><ul><li><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529181952910.png"alt="image-20240529181952910" /><figcaption aria-hidden="true">image-20240529181952910</figcaption></figure></li></ul></li><li><p>损失：nn.MSELoss</p></li><li><p>优化器：optim.Adam</p></li><li><p>训练批次：200轮<br/> <br/> - 评价指标：Peak Signal-to-Noise Ratio(PSNR)<br/> <br/> -<code>&lt;br/&gt;                def calc_psnr(img1, img2):&lt;br/&gt;                    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))&lt;br/&gt;</code></p></li></ul></li></ul></li><li><p>可以比较的模型：（完成意味着代码写完了，可以跑起来，但是效果需要针对训练集进行微调，延续老师代码的损失、优化器和学习率）</p><ul><li><p>初始模型SRCNN<br/> - 输入为16*1*76*76<br/> -输出为16*1*76*76<br/> - 来源于ResNet的模型SRResNet(完成，训练中)<br/> -输入为16*3*19*19<br/> - 输出为16*3*76*76<br/> - best epoch: 166, psnr:24.29<br/> - 来源于DenseNet的模型SRDenseNet <br/> - 基于递归的方法<br/>- DRCN<br/> - DRRN<br/> - CARN<br/> -VDSR(完成，但是在第三个epoch，损失就不在降低了，效果也不如SRCNN，所以需要针对训练集进行微调)</p></li><li><p>基于像素的方法：<br/> - 生成式对抗性网络（GANs）：<br/> -SRGAN<br/> - ESRGAN<br/> - 基于光流的方法</p></li></ul></li><li><p>可以使用的对比评价指标</p><ul><li><p>峰值信噪比（PSNR）：峰值信噪比（PSNR）是评价SISR重建质量最广泛使用的技术之一。它表示SR图像ˆy与实际图像y之间的最大像素值L与均方误差（MSE）之比。</p></li><li><p>结构相似度指数（SSIM）：SSIM和PSNR一样，是一种流行的评价方法，侧重于图像之间结构特征的差异。它通过比较亮度、对比度和结构来独立地捕获结构上的相似性。SSIM估计一个图像y的亮度µy为强度的平均值，而它估计对比度σy为其标准差。</p></li><li><p>平均意见评分（MOS）：MOS是一种主观的测量方法，利用人类的感知质量来评估生成的SR图像。人类观众会看到SR图像，并要求他们进行质量评分，然后映射到数值，然后取平均值。通常，这些范围从1（坏）到5（好），但可能有不同的[15]。虽然这种方法是对人类感知的直接评估，但与客观指标相比，进行它更耗时和麻烦。此外，由于这个度量标准的高度主观性，它很容易受到偏见的影响。</p></li></ul></li></ul><p>后面是学习一些综述</p><h1 id="超分辨率评价指标">超分辨率评价指标：</h1><p>​  图像质量评估（IQA）</p><p>​  许多特性与优秀的图像质量有关，如锐度，对比度，或没有噪声。因此，对SR模型的公平评价具有挑战性。本节展示了属于图像质量评估（IQA）范畴的不同评估方法。广义上说，IQA指的是任何基于对人类观众的感知评估的度量，即应用SR方法后图像的真实程度。IQA可以是主观的（例如，人类评分者）或客观的（例如，正式的指标）。</p><p>​  1)平均意见得分（MOS, Mean OpinionScore）：数字图像最终是为人类观看的。因此，评估图像的最合适的方法是主观评价[12]，[13]。一种常用的主观IQA方法是平均意见评分（MOS）。人类观众给有质量分数的图像打分，通常是1（差）到5（好）。MOS是所有评分的算术平均值。尽管具有可靠性，但调动人力资源是耗时和麻烦的，特别是对于大型数据集。</p><p>​  2)峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）：由于近年来产生的大量图像和主观测量的弱点，客观评估质量具有无可争辩的重要性。一种流行的目标质量测量方法是峰值信噪比（PSNR）。它是可能的最大像素值L（8位表示为255）与参考图像的均方误差（MSE）之间的比率。给定近似值$ y $ 和地面真实值y，PSNR是一个使用分贝尺度[dB]的对数量： <spanclass="math display">\[\mathrm{PSNR}\left(\mathbf{y},\mathbf{\hat{y}}\right)=10\cdot\log_{10}\frac{L^2}{\frac{1}{N_{\mathbf{y}}}\sum_{p\in\Omega_{\mathbf{y}}}\left[\mathbf{y}_p-\mathbf{\hat{y}}_p\right]^2}\]</span>​  虽然它被广泛用作SR模型的评价标准，但在真实场景中往往导致平庸的结果。它关注像素水平的差异，而不是哺乳动物的视觉感知，后者更吸引结构[14]。随后，它与主观感知质量的相关性较差。像素的轻微变化（例如，移动）可能会导致一个显著的PSNR降低，而人类几乎不知道这种差异。因此，新的指标关注于图像中更多的结构性特征。</p><h1 id="srnet">SRnet</h1><h2 id="简单的网络">简单的网络</h2><p>​  简单的网络是一种主要应用卷积链的体系结构。它们很容易理解，并且由于它们的大小，通常只使用最少的计算资源。大多数这些体系结构都可以在基于dl的SR的早期找到，因为它们的性能低于最先进的水平。此外，DL的“越深越好”的范式并不能很好地适用于简单的网络，因为正在消失/爆炸的梯度[98]。</p><p>​  图6显示了不同的简单网络设计。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182208483.png"alt="image-20240529182208483" /><figcaption aria-hidden="true">image-20240529182208483</figcaption></figure><p>​  第一个引入SR数据集的CNN是由Dong等人[39]提出的SRCNN（2014）。它使用双边缘预上采样来匹配地面真实空间大小（见第7.1节）。随后，它由三个卷积层组成，这遵循了图像恢复中流行的策略：补丁提取、非线性映射和重建。SRCNN的作者声称，应用更多的图层会损害性能，这与DL范式“越深越好”的[99]相矛盾。</p><p>​  如下所示，这个观察结果是错误的，需要更高级的构建块才能正确工作，例如，像VDSR[98]中那样的残差连接。</p><p>​  在他们的后续论文中，作者探索了各种加速SRCNN的方法，导致FSRCNN（2016）[33]利用了三个主要技巧：</p><p>​    首先，他们减少了卷积层内核的大小。</p><p>​    其次，他们使用了一个1x1的卷积层来增强和减少在使用3x3卷积的特征处理之前和之后的通道维度。</p><p>​    第三，他们采用了带有换位卷积的后上采样，这是提高速度的主要原因（见第7.1节）。</p><p>​  令人惊讶的是，它们在获得更快的同时优于SRCNN。</p><p>​  一年后，LapSRN（2017）[44]被提出，其关键贡献是一个拉普拉斯金字塔结构[100]，可以实现逐步上采样（见第7.1节）。它以粗分辨率的特征图作为输入，并预测高频残差，逐步细化每个金字塔层的SR重建。为此，在一个前馈通道中预测多尺度图像是可行的，从而促进了资源感知的应用程序。</p><p>​  简单的网络架构设计主要出现在基于dl的SR的早期，因为由于它们的大小，它们学习复杂结构的能力有限。最近，研究人员关注的是更有深度的网络，无论是残差网络，还是基于循环网络的合成深度。下面的部分将介绍这两种可能性。</p><h2 id="残差网络">残差网络</h2><p>​  残差的网络使用跳过连接来跳过图层。增加跳过连接的主要原因有两个：为了避免梯度的消失和降低精度饱和问题[99]。对于SR来说，引入跳过连接开启了深度构建模型的世界。其主要优点是深度架构用大的接受域替代卷积，这对于捕获重要特征至关重要。SRCNN的作者指出，“越深越好”的范式并不支持SR。相比之下，Kim等人用VDSR（2015）[98]驳斥了这一说法，并表明非常深的网络可以显著改善SR，如图7所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182809721.png"alt="image-20240529182809721" /><figcaption aria-hidden="true">image-20240529182809721</figcaption></figure><p>​  他们使用了来自其他DL方法的两种见解：首先，他们应用了一个著名的架构VGG-19[24]作为特征提取块。其次，他们使用了从插值层到最后一层的剩余连接。因此，VGG-19特征提取块在插值中添加了高频细节，导致目标分布呈正态分布，极大地降低了学习难度，如图8所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182849749.png"alt="image-20240529182849749" /><figcaption aria-hidden="true">image-20240529182849749</figcaption></figure><p>​  此外，由于插值中高频的稀疏表示，它减少了消失/爆炸梯度。这一优点产生了跟踪残余网络的趋势，从而增加了使用的残差的数量。</p><p>​  一个例子是RED-Net（2016）[101]，它将U-Net[102]架构适应SR。它结合了一个下采样编码器和一个上采样解码器网络，它对给定的输出进行下采样以提取特征，然后将特征映射上采样到目标空间大小。在此过程中，RED-Net利用在整个下采样过程中获得的剩余信息扩展了上采样操作，从而减少了消失的梯度。因此，它在几个缩放因子上都优于SRCNN。</p><p>​  另一个例子采用了SRGAN（2016）[13]的出版物中，即由多个残差单元组成的RenseNet[104]，它将残差信息发送给所有后来出现的卷积操作。此外，作者还比较了这些架构应用于像素和对抗性损失（见第3节）。SRResNet由多个堆叠的剩余单元组成，允许高级特征提取通过大量的求和操作访问低级特征信息。因此，它通过提供一个简单的反向传播路径来简化优化。与SRResNet一样，SRDenseNet[40]应用密集的残余块，它利用更多的残余连接来允许直接路径到更早的层。相比之下，SRResNet的性能大大优于SRCNN、DRCN和ESPCN。对SRDenseNet的一个扩展是在2018年提出的剩余密集网络[42]，它在密集块上包含了一个额外的剩余连接。</p><p>​  密集残差拉普拉斯网络（DRLN）[6]是SRDenseNet的扩展，是一种基于后上采样、通道注意的残差网络，并取得了最先进的竞争结果。每个密集块后面都有一个基于拉普拉斯金字塔注意的模块，它学习特征映射之间的层间和层内依赖关系。它在每个DRLM中逐步加权子频带特征，类似于HAN（连接不同深度的各种特征图）。</p><p>​  残差块的另一种变体是信息蒸馏网络（IDN）[43]。它使用剩余连接将特征映射的一部分积累到以后的层。给定六个卷积层，它将特征映射在中间分成两个部分。然后，其中一部分被最后三层进一步处理，并添加到输入部分和另一部分的连接中。简而言之，利用剩余连接的网络是最先进的。它们有效地传播信息的能力有助于对抗消失/爆炸的梯度，从而产生出色的性能。有时，剩余块的使用会与其他体系结构相结合，例如基于循环的网络。</p><h2 id="基于递归的网络">基于递归的网络</h2><p>​  人工深度可以通过重复来完成，其中接受野对于获取重要信息至关重要，通过重复相同的操作而扩大。此外，递归性减少了参数的数量，这有助于对抗小型设备的过拟合和内存消耗。它是通过不引入新的参数而多次应用卷积层来实现的。</p><p>​  Kim等人[105]通过DRCN（2015）引入了第一个基于循环的SR网络。它使用相同的卷积层多达16次，随后的重建层考虑所有递归输出进行最终估计。然而，他们观察到，他们的深度递归网络很难训练，但通过跳过连接和递归监督来缓解它，本质上是辅助训练。图9显示了DRCN。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529190806493.png"alt="image-20240529190806493" /><figcaption aria-hidden="true">image-20240529190806493</figcaption></figure><p>​  结合了DRCN [105]和VDSR[98]的核心思想，DRRN（2017）[65]在一个递归块结构中使用了几个堆叠的残差单元。此外，它使用的参数分别比VDSR和DRCN少6倍和14倍，同时获得了更好的结果。与DRCN相反，DRRN在剩余单元之间共享权值集，而不是在所有递归应用的卷积层中共享一个权值。通过强调多路径，DRNN比DRCN（总共52个）训练得更稳定，递归程度更深。</p><p>​  受DRCN的启发，Tai等人介绍了MemNet（2017）[92]。主要贡献是由递归单元和门单元组成的内存块，以挖掘持久内存。该递归单元被应用了多次，类似于DRCN。输出被连接并发送到一个栅极单元，这是一个简单的1x1卷积层。自适应门单元控制先前信息的量和保留的当前状态。图9显示MemNet。引入门对序列到序列任务（如LSTM[106]）的影响是开创性的，但深入了解其对SR任务的影响标志着一个开放的研究问题。</p><p>​  受DRRN的启发，DSRN（2018）[97]的作者探索了一种具有多路径网络的双态设计。它介绍了两种状态，一种在HR操作，另一种在LR空间，它们共同利用LR和HR信号。通过延迟反馈[107]，信号在lr到hr和hr到lr这两个空间之间反复交换。从lr到hr，它使用了一个转置的卷积层来进行上采样。HR-to-LR是通过分层卷积来执行的。最终的近似值使用了在人力资源空间中所做的所有估计值的平均值。因此，它应用了迭代上下上采样的扩展公式（见第7.1节）。这两种状态使用的参数多于DRRN，但小于DRCN。然而，适当地开发双态设计在未来需要进行更多的探索。</p><p>​  超分辨率反馈网络（SRFBN，2019）[64]也在使用反馈[108]。最基本的贡献是反馈块（FB）作为一个实际的循环细胞。FB使用多个具有密集跳跃连接的迭代上下采样来产生高水平的判别特征。SRFBN为每次迭代生成一个SR图像，并且FB块接收前一次迭代的输出。它尝试在每次迭代中为单个退化任务生成相同的SR图像。对于更复杂的案例，它通过课程学习通过每次迭代返回更好、更好质量的图像（见第6.1节）。与其他框架相比，SRFBN已经显示出了显著的改进，但在未来还需要更多的研究。</p><p>​  Liu等人提出了NLRN（2018）[109]，它提供了一个非局部模块来产生自相似性的特征相关性。图像中的每个位置测量其邻近区域的每个位置的特征相关性。NRLN利用特征相关消息之间的相邻循环阶段。事实上，NLRN的表现也略好于DRCN、DRCN和MemNet。</p><p>​  然而，最近对SR中rnn的主要研究是针对MISR进行的，如视频SR[110]或元学习[111]相关任务。一般来说，基于循环的网络在保存参数方面很有趣，但其主要缺点是通过重复应用相同的操作来实现其计算开销。此外，由于时间依赖性，它们不能并行化。替代方案是轻量级架构，接下来将介绍它们。</p><h2 id="轻量级网络">轻量级网络</h2><p>​  到目前为止，我们已经引入了能够提高SR图像质量的模型，以及一些尝试去做同样的事情，但计算量较少的模型。例如，FSRCNN[33]利用更小的内核大小、后采样和1x1卷积层来增强/减少通道维度，从而比SRCNN[39]更快（见7.2节）。本工作中的另一个例子是基于循环的网络，它减少了第7.4节中所述的冗余参数。这些精益递归网络的缺点是，参数的减少是以增加操作和推理时间为代价的，这是现实世界场景的一个基本方面。例如，移动设备上的SR受到电池容量的限制，这取决于所需的计算功率。因此，轻量级体系结构明确地同时关注执行速度和内存使用情况。补充材料，在线提供，包括参数比较，和执行速度的公平比较是受欢迎的需求。</p><p>​  MDSR（2017）[38]使用多路径方法来学习具有共享参数的多个缩放因子。它有三个不相同的路径作为预处理步骤和三个路径作为上采样。对于给定的比例因子s∈{2,3,4}，MDSR在三条路径之间选择确定性的。大尺度的路径比低尺度因子的路径建立得更深。在预处理和上采样步骤之间是一个由多个剩余块组成的共享模块。该特征提取块经过训练，常用于所有的缩放因子，如图10所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191245073.png"alt="image-20240529191245073" /><figcaption aria-hidden="true">image-20240529191245073</figcaption></figure><p>​  其主要优点是，一个模型就足以在多个尺度上进行训练，从而节省了参数和内存。相比之下，其他SR模型必须在不同的尺度上独立训练，并独立保存用于多尺度应用。然而，添加一个新的缩放因子需要从头开始进行训练。其他轻量级体系结构也采用了这一想法，以实现参数高效的多尺度训练，如CARN/CARNM（2018）[112]。</p><p>​  此外，它还在残差网络[103]上实现了一种级联机制。CARN由多个级联块（见图11）和它们之间的1x1卷积组成。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191409578.png"alt="image-20240529191409578" /><figcaption aria-hidden="true">image-20240529191409578</figcaption></figure><p>​  级联块的输出将被发送到所有后续的1x1卷积中，就像在级联块本身中一样。因此，局部级联几乎与全局级联完全相同。它允许多层次的表示和稳定的训练，如残余网络。最终，它在三条路径中进行选择，通过类似于MDSR的高效亚像素层，将特征映射上采样到2倍、3倍或4倍的缩放因子。受MobileNet[113]的启发，CARN还在每个残差块组件中使用分组卷积。这允许配置模型的效率，因为选择不同的组大小和由此产生的性能是在一种权衡关系中。具有组卷积的残差块根据组卷积的大小，最多可减少计算14倍。他们测试了CARN的一种变体，它设置了组的大小，从而使计算减少最大化，并将其称为CARN-Mobile（CARN-M）。此外，他们通过允许在每个级联块中的残余块的权重共享（与非共享块相比减少了3倍），进一步减少了CARN-M的参数。</p><p>​  受IDN和IMDB[115]的启发，RFDN（2020）[114]通过使用RFDB块重新考虑了IMDB体系结构，如图12所示。RFDB块由特征蒸馏连接组成，它们将1x1的卷积级联到最后一层。此外，它使用浅层残差块（SRBs），其中只有一个3x3的卷积，来进一步处理给定的输入。最后一层是一个1x1的卷积层，它结合了所有的中间结果。最后，它应用了专门为轻量级模型设计的增强型空间注意力[114]。RFDN架构包括后续的RFDB块，并使用具有最终亚像素层的后上采样框架。</p><p>​  XLSR（2021）[116]是一个非常具有硬件感知能力和量化友好性的网络。它应用多路径来减轻卷积操作的负担，并使用1x1卷积来按像素级进行组合。每个卷积层都有一个较小的滤波器尺寸（8、16、27)。在组合之后，它将分割特征贴图，并再次应用多条路径。XLSR的一个核心方面是末端激活层，它利用了量化的好处。量化是有用的，因为它可以通过使用更多的微型位表示[117]来保存参数。不幸的是，许多移动设备都支持8位数据。因此，对在浮32或浮16中表现良好的SR模型应用uint8量化不起作用。裁剪的ReLU（限制为最大值1）作为最后一个激活层而不是典型的ReLU可以消除这个问题。然而，作者建议通过进一步的实验来寻找其他的最大值。</p><p>​  一般来说，有很多想法可以让SR模型轻量级有待发现。它们包括对现有架构的简化、量化和修剪。此外，利用SR的资源有限的设备和应用是一个日益感兴趣的领域。</p><h2 id="小波变换网络">小波变换网络</h2><p>​  不同的图像表示可以带来一些好处，比如提高计算速度。小波理论为表示和存储多分辨率图像提供了稳定的数学基础，描述了上下文和纹理信息[119]。离散小波变换（DWT）将一幅图像分解为一系列小波系数。在SR中最常见的小波是Haar小波，通过二维快速小波变换计算出来。通过对每个输出系数进行迭代重复分解，计算出小波系数。它捕获四个子波段的图像细节：平均（LL）、垂直（HL）、水平（LH）和对角线（HH）信息。DWSR（2017）[120]是第一个使用小波预测的网络之一。它使用了一个简单的网络架构来细化在一个预上采样框架中的LR和HR图像小波分解之间的差异。首先，计算放大（采用双边插值）LR图像的小波系数。然后对小波系数进行卷积层处理。然后，加入初始计算的小波系数，并采用残差连接进行波面处理。因此，卷积层学习了系数的额外细节。最后，采用二维-DWT的反过程得到SR图像，如图13所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191746837.png"alt="image-20240529191746837" /><figcaption aria-hidden="true">image-20240529191746837</figcaption></figure><p>​  利用WIDN（2019）[121]提出了另一种方法，它使用平稳小波变换代替DWT来实现性能更好。与DWSR使用小波-srnet（2017）[122]的同时，我们提出了一个更复杂的模型。为了从LR图像中生成特征映射，它提供了一个由残余块组成的嵌入网络。然后，它多次应用小波变换，并利用多个小波预测网络。最后，它应用反向过程，并使用转置卷积进行上采样。该系数用于小波损失函数，而SR图像用于传统的纹理和MSE损失函数。因此，他们的网络适用于不同放大倍数的不同输入分辨率，并对MS-SR的未知高斯模糊、姿态和遮挡显示出鲁棒性。多级小波CNNS的思想也可以在以后的出版物中找到，即MWCNN（2018）[123]。</p><p>​  下面的工作应用了一种混合的方法，通过混合小波变换与其他著名的SR方法。即Zhang等人提出了一个基于小波的SRGAN（2019）框架[124]，它融合了SRGAN和小波分解的优点。生成器使用嵌入网络将输入处理到特征映射中，类似于小波-srnet。接下来，它使用一个小波预测网络来细化系数，类似于DWSR。2020年，Xue等人[125]将小波与包含通道注意和空间注意模块的残差注意块（混合注意，见下文第5节）相结合，并将其称为网络称为WRAN。在过去的几年里，小波的应用也应用于视频SR[126]。</p><p>​  一般来说，小波变换可以有效地表示图像。因此，使用这种策略的SR模型通常会降低总体模型的大小和计算成本，同时达到与最先进的架构相似的性能。然而，这一研究领域还需要更多的探索。例如，由于高频子带和低高频子带的分布，合适的归一化技术存在显著差异，或者由于高频子带的稀疏表示可能不合适，因此可以替代卷积操作。</p><h1 id="无监督超分辨率">无监督超分辨率</h1><p>​  监督SR的惊人性能归因于它们主要从许多LR-HR图像对学习自然图像的能力，大多是已知的退化映射，这在实践中通常是未知的。因此，经过监督训练的SR模型对于切实可行的用例有时是不可靠的。例如，当训练数据集生成LR图像（保留高频），然后SR模型在该数据集上进行的训练不太适合用于使用抗锯齿生成的真实LR图像（平滑图像）。使用抗混叠方法生成的LR图像（平滑图像）。此外，一些专门的应用领域缺乏LR-HR图像对数据集。因此，人们对无监督SR越来越感兴趣。我们简要地研究了这个领域，为了进一步阅读基于流的方法（退化核的密度估计），我们参考Liu等人[127]的调查。</p><h2 id="弱监督方法">弱监督方法</h2><p>​  弱监督方法使用未配对的LR和HR图像，如WESPE（2018）[128]。WESPE由两个发生器和两个鉴别器组成。第一个生成器获取一个LR图像并对其进行超级解析。第一发生器的输出构成一个SR图像，但也与电视损失[59]正则化。第二个生成器接受对第一个生成器的预测，并执行逆映射。第二个生成器的结果通过内容丢失[12]与原始输入的LR图像进行优化。这两个鉴别器取第一个生成器的SR图像，并被训练来区分预测和原始HR图像。第一鉴别器根据图像颜色将输入分类为SR或HR图像。第二个鉴别器使用图像纹理[61]来进行分类。</p><p>​  一个类似的方法是一个被称为CinCGAN（2018）[52]的周期中循环SR框架，基于CycleGAN[129]。它总共使用了四个发生器和两个鉴别器。第一个生成器取一个有噪声的LR图像，并将其映射到干净的版本。第一个鉴别器被训练来区分来自数据集的干净LR图像和预测的干净图像。第二台发电机训练逆函数。因此，它从预测的干净版本中生成有噪声图像，从而关闭了一个半周期周期的第一个周期。第三个生成器特别有趣，因为它是实际的SR模型，它将LR图像上采样到HR。第二个鉴别器被训练来区分预测的和数据集的HR图像。最后一个生成器将预测的HR图像映射到有噪声的LR图像，从而关闭CycleGAN的第二个周期。除了其有希望的结果和类似的方法[130]外，它还需要进一步的研究来降低学习难度和计算成本。</p><h2 id="零次学习">零次学习</h2><p>​  零次学习或一次学习与对物体的训练和对从未观察到的完全不同的物体的测试有关。理想情况下，如果将“斑马看起来像条纹马”转换为[132]马，那么用马训练的分类器应该识别斑马。关于SR的零次学习的第一个出版物是ZSSR（2017）[87]。我们的目标是只训练手头的一张图像，一张独一无二的图像。ZSSR对LR图像进行下采样，并训练CNN以反转退化映射。训练后的CNN最终直接用于LR图像。令人惊讶的是，该方法的效果优于SRCNN，与VDSR比较接近。</p><p>​  在此基础上，提出了一种基于深度信息[134]的退化仿真网络（DSN，2020）[133]，以避免预定义的退化内核。它使用双循环训练来同时学习未知的退化核和SR图像的重建。MZSR[135]将ZSSR设置与元学习合并，并使用一个外部数据集来学习不同的模糊内核，这被称为元学习领域中的任务分布。然后将SR模型在类似于ZSSR的降采样图像上进行训练，并从元测试阶段返回模糊核。这种方法的好处是，它使SR模型更快地学习特定信息，比纯ZSSR性能更好。SR的零镜头学习标志着进一步研究的一个令人兴奋的领域，因为它非常实用，特别是对于特定于应用程序的数据集很少或不存在的应用程序。</p><h2 id="深度图像先验">深度图像先验</h2><p>​  Ulyanov等人[131]提出了深度图像先验（DIP），这与在大数据集上训练CNN的传统范式相矛盾。它使用一个CNN来预测降采样时的LR图像，给定一些随机的噪声，而不是一个实际的图像。因此，它遵循了ZSSR的策略，只使用LR图像。然而，它将输入固定为随机噪声，并对预测采用固定的降采样方法进行修正。此外，它还优化了降采样预测与LR图像之间的差异。然后，CNN在不使用固定降采样方法的情况下生成SR图像。因此，它利用噪声生成一个SR图像，而不是转换一个原始图像。ZSSR和DIP之间的差异见图14。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529192743577.png"alt="image-20240529192743577" /><figcaption aria-hidden="true">image-20240529192743577</figcaption></figure><p>图14。零镜头超分辨率（ZSSR）[87]和深度图像先验（DIP）[131]。ZSSR使用LR图像进行降采样，SR模型学习反向降采样。对于LR图像的SR图像的最终预测，它直接应用于LR图像。DIP使用固定噪声作为输入，预测SR图像，并进行降采样，以优化降采样图像与给定LR图像之间的差异。最终的预测使用SR模型来预测SR图像，但跳过了退化映射。</p><p>​  令人惊讶的是，研究结果与LapSRN[136]很接近。不幸的是，它是一篇关于图像先验的理论出版物，而且正如作者自己所说的那样，这种方法太慢了，对大多数实际应用都不太有用。然而，它并不排除未来可以提高DIP关于更好的图像重建质量，特别是运行时的实用性的想法。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM1</title>
      <link href="/SAM1/"/>
      <url>/SAM1/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything Model for Medical Images?</p><p>发表于MICCAI 2024</p><p>Testing pipeline of SAM</p><figure><img src="./../postimages/SAM1/image-20240528220811746.png"alt="image-20240528220811746" /><figcaption aria-hidden="true">image-20240528220811746</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/yuhoo0302/Segment-Anything-Model-for-Medical-Images</span><br><span class="line">任务是医学图片的分割</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">优化器和损失函数设计：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"># Set up the optimizer, hyperparameter tuning will improve performance here</span><br><span class="line">optimizer = torch.optim.AdamW(sam_model.mask_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction=&#x27;mean&#x27;)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">为一个训练过程中的代码：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">outputs = []</span><br><span class="line"># do not compute gradients for image encoder and prompt encoder</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    none_grad_features = &#123;&quot;sparse&quot;: &#123;&#125;, &quot;dense&quot;: &#123;&#125;&#125;</span><br><span class="line">    for idx, image_record in enumerate(batched_input):</span><br><span class="line">        sparse_embeddings, dense_embeddings = model.prompt_encoder(</span><br><span class="line">                    points=None,</span><br><span class="line">                    boxes=image_record[&quot;box&quot;].to(device),</span><br><span class="line">                    masks=None,</span><br><span class="line">                )</span><br><span class="line">        none_grad_features[&quot;sparse&quot;][idx] = sparse_embeddings</span><br><span class="line">        none_grad_features[&quot;dense&quot;][idx] = dense_embeddings </span><br><span class="line"></span><br><span class="line">batched_loss = 0</span><br><span class="line">for id, im_record in enumerate(batched_input):</span><br><span class="line">    # low_res_masks.shape == (B, M, 256, 256) M is set to 1</span><br><span class="line">    low_res_masks, iou_predictions = model.mask_decoder(</span><br><span class="line">        image_embeddings=im_record[&quot;img_embed&quot;].unsqueeze(0).to(device), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        image_pe=model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        sparse_prompt_embeddings=none_grad_features[&quot;sparse&quot;][id], # (B, 2, 256) !!B = target num instead of batch size</span><br><span class="line">        dense_prompt_embeddings=none_grad_features[&quot;dense&quot;][id], # (B, 256, 64, 64) !!B = target num instead of batch size</span><br><span class="line">        multimask_output=False,</span><br><span class="line">    )</span><br><span class="line">    # upscale + eliminate padding + restore to ori size</span><br><span class="line">    masks = model.postprocess_masks(</span><br><span class="line">        low_res_masks,</span><br><span class="line">        input_size=tuple(im_record[&quot;size_before_pad&quot;]),</span><br><span class="line">        original_size=tuple(im_record[&quot;image_ori_size&quot;]),</span><br><span class="line">    )</span><br><span class="line">    outputs.append(&#123;</span><br><span class="line">        &quot;masks&quot;: masks,</span><br><span class="line">        &quot;iou_predictions&quot;: iou_predictions,</span><br><span class="line">        &quot;low_res_logits&quot;: low_res_masks,</span><br><span class="line">        &quot;gt2D&quot;: im_record[&quot;gt2D&quot;].to(device)</span><br><span class="line">    &#125;)</span><br><span class="line">    # first ele: 1, B, ori_H, ori_W</span><br><span class="line">    # second ele: 1, B, ori_H, ori_W</span><br><span class="line">    # considering the multi-object situation</span><br><span class="line">    batched_loss += criterion(masks.squeeze(1).unsqueeze(0), im_record[&quot;gt2D&quot;].to(device).unsqueeze(0)) </span><br><span class="line">loss = batched_loss / len(batched_input)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">epoch_loss += loss.item()</span><br></pre></td></tr></table></figure><p>Segment Anything in High Quality</p><p>发表于NeurIPS 2023</p><figure><img src="./../postimages/SAM1/image-20240528221000632.png"alt="image-20240528221000632" /><figcaption aria-hidden="true">image-20240528221000632</figcaption></figure><p>图3:HQ-SAM将HQ输出令牌和全局局部特征融合引入SAM，用于高质量掩模预测。为了保持SAM的零样本能力，轻量级HQ-Output-Token重用SAM的掩码解码器，并生成新的MLP层，用于执行具有融合HQ-Features的点向产品。在训练过程中，当我们固定预先训练的SAM的模型参数时，HQ-SAM中只有少数可学习的参数是可训练的。为了清晰起见，此处省略了提示编码器。误差校正简单地用作推理期间SAM的输出令牌和HQ输出令牌的预测logits之间的直接元素和。</p><p>损失函数设置</p><p>We supervise mask prediction of the new HQ-Output token with acombination of both BCE Loss and Dice Loss.</p><p>我们用BCE损失和Dice损失组合的联合损失监督新HQ输出token的掩码预测。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box、point、noise_mask</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/SysCV/SAM-HQ</span><br><span class="line">任务是SAM的高质量掩模预测问题</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">from utils.loss_mask import loss_masks</span><br><span class="line"></span><br><span class="line">net = MaskDecoderHQ(args.model_type)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">if input_type == &#x27;box&#x27;:</span><br><span class="line">dict_input[&#x27;boxes&#x27;] = labels_box[b_i:b_i+1]</span><br><span class="line">elif input_type == &#x27;point&#x27;:</span><br><span class="line">point_coords = labels_points[b_i:b_i+1]</span><br><span class="line">    dict_input[&#x27;point_coords&#x27;] = point_coords</span><br><span class="line">    dict_input[&#x27;point_labels&#x27;] = torch.ones(point_coords.shape[1], device=point_coords.device)[None,:]</span><br><span class="line">elif input_type == &#x27;noise_mask&#x27;:</span><br><span class="line">    dict_input[&#x27;mask_inputs&#x27;] = labels_noisemask[b_i:b_i+1]</span><br><span class="line">else:</span><br><span class="line">    raise NotImplementedError</span><br><span class="line">dict_input[&#x27;original_size&#x27;] = imgs[b_i].shape[:2]</span><br><span class="line">batched_input.append(dict_input)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">batched_output, interm_embeddings = sam(batched_input, multimask_output=False)</span><br><span class="line"></span><br><span class="line">batch_len = len(batched_output)</span><br><span class="line">encoder_embedding = torch.cat([batched_output[i_l][&#x27;encoder_embedding&#x27;] for i_l in range(batch_len)], dim=0)</span><br><span class="line">image_pe = [batched_output[i_l][&#x27;image_pe&#x27;] for i_l in range(batch_len)]</span><br><span class="line">sparse_embeddings = [batched_output[i_l][&#x27;sparse_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">dense_embeddings = [batched_output[i_l][&#x27;dense_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">masks_hq = net(</span><br><span class="line">    image_embeddings=encoder_embedding,</span><br><span class="line">    image_pe=image_pe,</span><br><span class="line">    sparse_prompt_embeddings=sparse_embeddings,</span><br><span class="line">    dense_prompt_embeddings=dense_embeddings,</span><br><span class="line">    multimask_output=False,</span><br><span class="line">    hq_token_only=True,</span><br><span class="line">    interm_embeddings=interm_embeddings,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_mask, loss_dice = loss_masks(masks_hq, labels/255.0, len(masks_hq))</span><br><span class="line">loss = loss_mask + loss_dice</span><br><span class="line"></span><br><span class="line">loss_dict = &#123;&quot;loss_mask&quot;: loss_mask, &quot;loss_dice&quot;:loss_dice&#125;</span><br><span class="line"></span><br><span class="line"># reduce losses over all GPUs for logging purposes</span><br><span class="line">loss_dict_reduced = misc.reduce_dict(loss_dict)</span><br><span class="line">losses_reduced_scaled = sum(loss_dict_reduced.values())</span><br><span class="line">loss_value = losses_reduced_scaled.item()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line">metric_logger.update(training_loss=loss_value, **loss_dict_reduced)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">loss_masks：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">def loss_masks(src_masks, target_masks, num_masks, oversample_ratio=3.0):</span><br><span class="line">    &quot;&quot;&quot;Compute the losses related to the masks: the focal loss and the dice loss.</span><br><span class="line">    targets dicts must contain the key &quot;masks&quot; containing a tensor of dim [nb_target_boxes, h, w]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # No need to upsample predictions as we are using normalized coordinates :)</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # sample point_coords</span><br><span class="line">        point_coords = get_uncertain_point_coords_with_randomness(</span><br><span class="line">            src_masks,</span><br><span class="line">            lambda logits: calculate_uncertainty(logits),</span><br><span class="line">            112 * 112,</span><br><span class="line">            oversample_ratio,</span><br><span class="line">            0.75,</span><br><span class="line">        )</span><br><span class="line">        # get gt labels</span><br><span class="line">        point_labels = point_sample(</span><br><span class="line">            target_masks,</span><br><span class="line">            point_coords,</span><br><span class="line">            align_corners=False,</span><br><span class="line">        ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    point_logits = point_sample(</span><br><span class="line">        src_masks,</span><br><span class="line">        point_coords,</span><br><span class="line">        align_corners=False,</span><br><span class="line">    ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    loss_mask = sigmoid_ce_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line">    loss_dice = dice_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line"></span><br><span class="line">    del src_masks</span><br><span class="line">    del target_masks</span><br><span class="line">    return loss_mask, loss_dice</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">get_uncertain_point_coords_with_randomness：</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">    Sample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The unceratinties</span><br><span class="line">        are calculated for each point using &#x27;uncertainty_func&#x27; function that takes point&#x27;s logit</span><br><span class="line">        prediction as input.</span><br><span class="line">    See PointRend paper for details.</span><br><span class="line">    Args:</span><br><span class="line">        coarse_logits (Tensor): A tensor of shape (N, C, Hmask, Wmask) or (N, 1, Hmask, Wmask) for</span><br><span class="line">            class-specific or class-agnostic prediction.</span><br><span class="line">        uncertainty_func: A function that takes a Tensor of shape (N, C, P) or (N, 1, P) that</span><br><span class="line">            contains logit predictions for P points and returns their uncertainties as a Tensor of</span><br><span class="line">            shape (N, 1, P).</span><br><span class="line">        num_points (int): The number of points P to sample.</span><br><span class="line">        oversample_ratio (int): Oversampling parameter.</span><br><span class="line">        importance_sample_ratio (float): Ratio of points that are sampled via importnace sampling.</span><br><span class="line">    Returns:</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) that contains the coordinates of P</span><br><span class="line">            sampled points.</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">point_sample：</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.</span><br><span class="line">    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside</span><br><span class="line">    [0, 1] x [0, 1] square.</span><br><span class="line">    Args:</span><br><span class="line">        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains</span><br><span class="line">        [0, 1] x [0, 1] normalized point coordinates.</span><br><span class="line">    Returns:</span><br><span class="line">        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains</span><br><span class="line">            features for points in `point_coords`. The features are obtained via bilinear</span><br><span class="line">            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">sigmoid_ce_loss_jit = torch.jit.script(</span><br><span class="line">    sigmoid_ce_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def sigmoid_ce_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    Returns:</span><br><span class="line">        Loss tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&quot;none&quot;)</span><br><span class="line"></span><br><span class="line">    return loss.mean(1).sum() / num_masks</span><br><span class="line">    </span><br><span class="line">dice_loss_jit = torch.jit.script(</span><br><span class="line">    dice_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def dice_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the DICE loss, similar to generalized IOU for masks</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    inputs = inputs.sigmoid()</span><br><span class="line">    inputs = inputs.flatten(1)</span><br><span class="line">    numerator = 2 * (inputs * targets).sum(-1)</span><br><span class="line">    denominator = inputs.sum(-1) + targets.sum(-1)</span><br><span class="line">    loss = 1 - (numerator + 1) / (denominator + 1)</span><br><span class="line">    return loss.sum() / num_masks</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCIG大会—1</title>
      <link href="/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/"/>
      <url>/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/</url>
      
        <content type="html"><![CDATA[<h1 id="csig-年度学科发展报告论坛">CSIG 年度学科发展报告论坛</h1><h2 id="报告题目分割一切模型综述">报告题目：分割一切模型综述</h2><p><strong>报告嘉宾：</strong>张军平，复旦大学计算机科学技术学院教授、博士生导师，中国自动化学会普及工作委员会主任。主要研究方向包括人工智能、机器学习、图像处理、生物认证、智能交通及气象预测。至今发表论文100 余篇，连续两年（2022、2023）入选全球前2%顶尖科学家榜单终身科学影响力排行榜。著有《人工智能极简史》《爱犯错的智能体》《高质量读研》，主编《人机混合增强智能》，译著《统计学习要素》（第二版）。</p><p><strong>报告摘要：</strong>Meta 公司提出的“分割一切模型”(SegmentAnything Model，简称SAM)于 2023 年在图像分割领域获得了优异的性能。在 SAM开源后不久，科研</p><p>人员提出了一系列改进的方法和应用。为了能全面深入了解分割一切模型的发展脉络，优势与不足，本报告将对SAM 的研究进展进行综述。我将先介绍分割一</p><p>切模型的背景和核心框架。在此基础上，综述相关改进方法，并探讨 SAM在图像处理、视频处理以及其他领域的应用。最后，对 SAM未来的发展方向和潜在</p><p>应用前景进行分析和讨论。</p><h3 id="sam背景">SAM背景</h3><ul><li>分割一切项目<br/> - 任务:提出新的提示分割任务范式。<br/> -模型:图像编码器、提示编码器、轻量级掩码解码器。<br/> -数据:提出新的数据引擎构建了SA-1B数据集。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528163704285.png"alt="image-20240528163704285" /><figcaption aria-hidden="true">image-20240528163704285</figcaption></figure><h3 id="sam的应用">SAM的应用</h3><h4 id="视频超分辨率">视频超分辨率</h4><ul><li>SEEM模块可以利用语义信息增强模型的特征对齐和融合能力。<br/> -具体来说，通过利用注意力机制和特征映射操作实现将SAM的表示与当前输入帧的特征相结合，然后生成语义感知的特征。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164011993.png"alt="image-20240528164011993" /><figcaption aria-hidden="true">image-20240528164011993</figcaption></figure><p>基于滑动窗口的超分辨率方法，引入SEEM改进了三个步骤：即对齐、融合和重建</p><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164206753.png"alt="image-20240528164206753" /><figcaption aria-hidden="true">image-20240528164206753</figcaption></figure><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164239992.png"alt="image-20240528164239992" /><figcaption aria-hidden="true">image-20240528164239992</figcaption></figure><p>在基于循环结构的超分辨率方法，我们将SEEM应用于基于双向递归结构的方法。“F”和“B”是向前和向后传播表示串联操作。</p><h4 id="视频目标追踪">视频目标追踪</h4><ul><li>TAM结合了分割模型SAM和高级视频对象分割模型多重记忆模型，这两个模型以交互的方式集成在一起。</li></ul><h4 id="总结">总结</h4><p>SAM的应用形式主要大致分为四类:</p><ol type="1"><li>在<strong>特定领域</strong>对SAM进行<strong>微调</strong><br/>2.使用<strong>SAM辅助其他领域</strong>原有的模型<br/>3.利用SAM构建其他特定领域的数据集<br/>4.使用生成提示模型自动生成提示来辅助SAM</li></ol><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/SAM(CCIG).drawio.png"alt="SAM(CCIG).drawio" /><figcaption aria-hidden="true">SAM(CCIG).drawio</figcaption></figure><h3 id="未来研究方法">未来研究方法</h3><ul><li>模块化</li><li>弱监督语义分割</li><li>多模态融合图像分割</li><li>对SAM进行高效率微调</li><li>格式塔心理学的整体认知观加强SAM的对抗鲁棒性</li></ul><p>综述论文</p><p><ahref="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=202311030000002">分割一切模型SAM的潜力与展望：综述-Thepotential and prospects of segement anything model: a survey(cjig.cn)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="第三章-经典参数估计">第三章—-经典参数估计</h1><p>如何评价估计器的好坏？</p><h2 id="两个重要的无偏一致估计">两个重要的无偏一致估计：</h2><h3 id="均值">均值：</h3><p><span class="math display">\[\hat \mu = \frac 1 N \sum _{i=1} ^Nx_i\]</span></p><h3 id="协方差矩阵">协方差矩阵：</h3><p>已知均值 <span class="math display">\[\hat R = \frac 1 N \sum _{i=1}^N (x_i -  \mu _x)(x_i -  \mu _x)^H\]</span> 未知均值 <spanclass="math display">\[\hat R = \frac 1 {N-1} \sum _{i=1} ^N (x_i-  \hat \mu _x)(x_i -  \hat \mu _x)^H\]</span></p><h2 id="均方误差mean-squared-error-mse">均方误差(mean squared error, MSE)</h2><p><span class="math display">\[E[||\hat A -A||^2]=E[e^He]=trE[e^He]=trM\]</span></p><p>$ M=E[ee^H] $ 称为均方误差矩阵，可对其进行进一步分解</p><p>令 $ b = _e $ : <span class="math display">\[R_e = E[(e - \mu _e)(e -\mu _e)^H] = M - bb^H\]</span> 因此 $ M = R_e + bb^H $，可视为协方差和偏差的加权，当估计器为无偏估计时，</p><p>最小方差估计器不一定是无偏的，因此存在<strong>最小均方误差估计</strong>和<strong>最小方差无偏估计</strong>两种不同的准则</p><h2 id="最小方差无偏估计mvdr">最小方差无偏估计MVDR</h2><p>对于$ y=Hx+n $ <em>现有问题</em>：</p><p><spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\\\\\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\\\\\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span></p><p>以下是例题：</p><p>例： $ y=Hx+n $，对于上述线性模型，在满足线性运算的条件下寻找最小方差无偏估计，结果称为minimumvariance distortionless response (MVDR) ，也称作best linear unbiasedestimator (BLUE)：</p><p>解：</p><p>假设线性条件： $ x = W^H y $</p><p>则无偏性条件： <span class="math display">\[E[\hat x] = E[W^H y]=E[W^H(Hx+n)]= E[W^HHx+W^Hn]= W^HHx+W^HE(n)=W^HHx=x\\W^HH=I\]</span>最小化方差： <span class="math display">\[E[||\hat x - x||^2]=trE[(\hatx - x)(\hat x - x)^H]\\=trR_{\hat x \hatx}=tr\{W^HR_{yy}W\}=tr\{W^HR_{nn}W\}\]</span>则原问题变成了MVDR优化<em>现有问题</em>： <spanclass="math display">\[\underset{W}{min} \ tr\{ W^HR_{nn}W \}\\s.t.W^HH=I\]</span> 首先构造拉格朗日函数： <spanclass="math display">\[\mathcal{L}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\sum_{i,j}\lambda_{ij}\left[\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]_{ji}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\mathrm{tr}\left[\mathbf{\Lambda}\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]\]</span>对W求偏导 <spanclass="math display">\[\begin{aligned}&amp;\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{R}_{mn}\mathbf{W})}{\partial\mathbf{W}}=2\mathbf{R}_m\mathbf{W}\\&amp;\frac{\partial(\mathbf{\Lambda}\mathbf{W}^T\mathbf{H})}{\partial\mathbf{W}}=\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{H}\mathbf{\Lambda})}{\partial\mathbf{W}}=\mathbf{H}\mathbf{\Lambda}\end{aligned}\quad\Rightarrow\quad\frac{\partial\mathcal{L}}{\partial\mathbf{W}}=2\mathbf{R}_{mn}\mathbf{W}-\mathbf{H}\mathbf{\Lambda}=0\quad\Rightarrow\quad\mathbf{W}=\frac12\mathbf{R}_{mn}^{-1}\mathbf{H}\mathbf{\Lambda}\]</span>由约束条件得： <spanclass="math display">\[\mathbf{W}^{T}\mathbf{H}=\frac{1}{2}\mathbf{\Lambda}^{T}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H}=\mathbf{I}\Rightarrow\mathbf{\Lambda}^{T}=2(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\]</span>解得 <spanclass="math display">\[\mathbf{W}^{T}=(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\]</span>对于复数情形 <spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\]</span>因此，MVDR估计为： <spanclass="math display">\[\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\]</span>MVDR估计的协方差矩阵 <spanclass="math display">\[\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span>例： $ y=sA+n $ ，其中s为已知实信号，A为待估计幅度，n为噪声，其协方差矩阵为 $ R_{nn} $，试设计MVDR估计并优化s,在满足总功率限制的条件下使得的估计误差最小化</p><p>解：首先将信号模型写成矩阵的形式 <spanclass="math display">\[y=sA+n\quad\Rightarrow\quady=Hx+n\\H=s,A=x\]</span>计算权值 <spanclass="math display">\[\mathbf{W}^T=(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{R}_{nn}^{-1}\\=(\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s})^{-1}\mathbf{s}^T\mathbf{R}_{nn}^{-1}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}\]</span>由此可得MVDR估计 <spanclass="math display">\[\hat{\mathbf{A}}=\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}y\]</span>要使得估计误差最小化，即最小化方差，即最小化MSE <spanclass="math display">\[\mathrm{tr}(\mathbf{R}_{\mathrm{xx}})=\mathrm{tr}\left[\left(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H}\right)^{-1}\right]=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\]</span>最小化方差等同于最大化分母 ，考虑 $ R_{nn} $ 的EVD分解 <spanclass="math display">\[\mathbf{R}_{\mathrm{nn}}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^T\]</span>同时将s表示为 $ R_{nn} $ 特征向量的加权 <spanclass="math display">\[s=V\alpha\]</span> 所以： <spanclass="math display">\[\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\Lambda}^{-1}\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\Lambda}^{-1}\mathbf{\alpha}=\sum_{i\operatorname{=}1}^N\frac{|\alpha_i|^2}{\lambda_i}\]</span>考虑功率约束 <spanclass="math display">\[\mathcal{E}=\sum_{k=1}^Ns_k^2=\parallel\mathbf{s}\parallel^2=\mathbf{s}^T\mathbf{s}\]</span>由于 $ s=V$ : <spanclass="math display">\[\mathbf{s}^T\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\alpha}=\parallel\mathbf{\alpha}\parallel^2=\sum_{i=1}^N\alpha_i^2\]</span>为最大化 $ <sup>T<em>{nn}^{-1} = </em>{i1}</sup>N $，应将能量分配到较小的 $ <em>i $ 上，因此最佳方案为<br /><span class="math display">\[\alpha_i^2=\begin{cases} 0,&amp;i=1,...,N-1 ,\\ \mathcal{E},&amp;i=N,\end{cases}\]</span> 此时 $ =</em>{}$，对应于 $ R_{nn} $ 最小特征根对应的特征向量</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理作业</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<h1id="在超密集网络中基于连接的定位crlb理论方差和mle">在超密集网络中基于连接的定位：CRLB，理论方差，和MLE</h1><h1 id="摘要">摘要</h1><p>​  超密集网络（UDNs, ultra-densenetworks）中基于连接的地理定位的性能分析是一项非常重要的任务。虽然已经对无距离定位进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。在本文中，我们首先推导了对无范围定位的性能评估的Cramer-Rao下界（CRLB）。文献中关于无距离定位的所有当前性能分析都用于评估给定算法的实际性能，而所提出的CRLB提供了评估任何无偏无距离定位算法性能的基准，并确定了无偏估计器的方差小于界的物理不可能性。</p><p>​  据我们所知，这是文献中第一次推导出用于无距离定位的CRLB。其次，推导了任意节点分布下基于质心定位的理论方差。与均匀节点分布下CL的现有理论方差相比，所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还给出了所提出的CRLB的特性和理论方差。最后，为了提高定位精度，提出了一种基于最大似然估计器的最优估计器。由于我们的算法有效地利用了空间节点分布的先验信息和连通性，因此所提出的方法比CL方法性能更好，并且可以渐近地获得CRLB。</p><h1 id="引言">引言</h1><p>​  随着网络设备数量的增加，超密集网络（UDN）系统中盲节点（BN）的位置估计近年来引起了[1]的广泛关注。无线位置作为一个重要的公共安全功能，在未来的无线通信系统中创造了许多潜在的应用，如位置敏感的计费、欺诈保护、人员/资产跟踪、车队管理、移动黄页、无线网络设计、无线电资源管理和智能交通系统[2]、[3]。</p><p>​  虽然全球导航卫星系统（GNSSs），如全球定位系统（GPSs），可以提供较高定位精度的定位服务，但其局限性，包括高功率消耗，在室外丰富散射场景和城市峡谷的性能下降，阻止GNSSs应用于复杂的城市和室内环境。</p><p>从广义上讲，无线通信系统中的定位技术可以分为两类：</p><p>​  (1)基于范围的定位方法</p><p>​  (2)无范围的定位方法（也称为基于连接的定位）。</p><p>​  几种基于距离的定位技术，包括到达时间（TOA）、到达时差（TDOA）、到达角度（AOA）、基于接收信号强度（RSS）的方法和混合定位方法，被用于无线定位。基于范围的定位首先利用承载角和绝对或相对距离建立BN和参考节点（RNs）之间的显式几何关系，这些距离是由AOA、TOA、RSS和TDOA测量值估计的。然后，根据几何模型可以得到BN的位置。由于其定位精度高，基于范围的定位方法已经在文献[4]-[25]中得到了广泛的研究。研究了视线（LOS）环境[4]-[9]下的封闭解和迭代算法。对于非视线（NLOS）传播，开发了几何约束条件[10]-[12]和机器学习理论[13]-[16]来减轻NLOS误差。这些研究大多是基于单一的测量路径。为了进一步提高定位精度，在[17]，[18]中提出了基于多天线阵列的多路径传播环境的AOA定位算法。此外，在文献[19]-[25]中，还推导出了许多精度的几何稀释度和Cramer-Rao下界（CRLBs）。</p><p>​  本文从CRLB和理论方差的角度分析了无范围定位的性能。本文还提出了基于最大似然估计量（MLE）的最优估计量。本文的主要贡献如下：</p><p>​  (1)本文推导了一个具有随机分布RNs的UDN中无范围定位的CRLB。虽然已经对无距离定位[26]、[27]、[31]进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。所有当前的性能分析[26]，[27]，[31]无距离定位用于评估给定算法的实际性能，而提出CRLB提供了一个基准来评估任何无偏的位置算法的性能和确定物理不可能的方差无偏的估计器小于绑定。据我们所知，这是文献中首次推导出无范围定位的CRLB。</p><p>​  (2)推导了具有任意节点分布的CL(centroid-basedlocalization)方法的理论方差。需要注意的是，[26]中CL方法的理论方差是针对均匀节点分布推导出的。所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还提供了所提出的CRLB的特征和理论方差。</p><p>​  (3)提出了一种基于MLE的最优估计器来提高定位精度。由于该算法有效地利用了空间节点分布的先验信息和连通性，因此该方法优于CL方法，并能渐近得到CRLB。</p><p>​  本文的组织结构如下。第二节给出了信号模型和一些基本的符号。在第三节中，本文首先推导了一个在随机分布的UDN中无距离定位的CRLB。然后，导出了任意节点分布下CL方法的理论方差。在本节的最后给出了所提出的CRLB和理论方差的一些特征。第四节提出了一种基于MLE的基于连通性信息和RN分布的迭代方法。第五节给出了所提出的CRLB的性能评价、理论方差和位置方法。本文的结论见第六节。</p><h1 id="系统模型">系统模型</h1><p>​  无距离定位的目的是利用BN和RNs之间的连接信息来定位BN。通常，CL算法包括两个阶段：监听和定位。在监听阶段，BN尝试监听并与RNs沟通。当接收到的信号功率超过检测阈值时，建立通信链路。在定位阶段，BN的位置近似为在其传输范围内所有RN的位置（RN的质心）位置的平均值。显然，CL的性能受到许多因素的影响，如节点密度和随机性、无线信道环境和位置方案。</p><p>​  无线信道环境对定位系统的性能起着非常重要的作用。这个通道环境决定了可以检测到多少和哪些RNs用于定位。传播模型通常是用来描述无线信道的情况，并预测在距离发射机的给定距离下的平均接收信号强度。虽然有几种传播模型[32]，[33]，本文选择了路径损失法向阴影模型，因为它被广泛应用于通信和定位应用，并已通过现场测量[32]得到证实。</p><p>​  假设（x，y）是待估计的BN的位置，并且N个RNs系统中第i个RN的已知坐标为（xi，yi），如果不失一般性，可以将BN的位置设为（0,0）。第i个RN和BN之间的真实距离可以建模为：<span class="math display">\[r_i = \sqrt{(x_i - x)^2 + (y_i -y)^2}\]</span>​  基于路径损失正态阴影模型，测量的RNi（dBm）的接收功率Pi可以视为对数正态变量[32]。因此，Pi和ri之间的关系变为：<span class="math display">\[P_i=P_0-10 \beta log_{10}(\frac {r_i}{r_0}) + n_i\]</span>​  式中，β为路径损失指数，表示路径损失随距离增加的速率；ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）；P0为参考距离r0处的参考功率，它取决于传输功率。一般来说，r0=1米。为简单起见，本文将r0设为1m。本文利用路径损失法向阴影模型推导出了无距离定位的CRLB和MLE。</p><p>​  节点的随机性是影响该定位方法性能的另一个主要因素。在文献中，基于不同的假设，提出了不同的节点分布。文献中首先提出了均匀分布，建立了一个节点分布模型，假设传感器节点均匀分布在半径为R[26]，[27]，[34]的圆盘中。然而，最近，人们已经认识到，均匀分布节点的假设对于实际部署的无线网络[35]，[36]是相当不可信的。事实上，节点的空间分布依赖于许多因素，如部署方法、节点的周围环境、节点的运动，甚至是通信协议。根据中心极限定理，实际节点位置将遵循高斯分布[35]，[36]。在这个模型中，根据二维高斯空间分布，协方差矩阵σp2i。一个RN位于（xi，yi）的概率可以用概率密度函数（PDF）[36]来描述：<span class="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span>​  需要注意的是，(3)是基于笛卡尔坐标系的。对于极坐标，PDF (3)可以写成：<span class="math display">\[f(r_i) = \frac {1} {\sigma _p ^2}exp(-\frac {r _i ^2} {2 \sigma _p ^2}) r_i\\f(\phi _i)= \frac {1} {2 \pi}\]</span> ​  其中ri是第i个RN和BN之间的范围。 $ _i = acos((x−x_i)/r_i) $是RNi相对于BN的方位角。式(4)表明，RN将在不同方向上以相同的概率出现，而f（xi，yi）只取决于RN与BN之间的距离。这也意味着靠近BN的RN可能比在更大距离的RN有更高的概率。</p><p>​  CL(centroid-based localization,基于质心的定位)是最简单的无范围定位方法，它只需要BN和相邻rN之间的二进制连接信息。CL算法基于以下假设[28]：</p><p>​  (1)有完美的球形无线电传播</p><p>​  (2)所有无线电具有相同的传输范围（功率）</p><p>​  (3)RN对称地分布在一个BN周围。</p><p>​  (4) CL仅基于从相邻RNs收集的连通性信息（单跳假设）。</p><p>​  假设（1-3）保证了CL算法是一个无偏估计量，第四个假设简化了定位过程。由于仿真和实验结果都证明了该模型在整洁环境[28]下非常符合户外无线电传播，因此本文也遵循了这些假设。BN定位于与RNs集合的连通性区域相交重合的区域，该区域由RNs[28]的质心定义： <span class="math display">\[(\hat x , \hat y)=(\frac 1M \sum _{i=1} ^ M x_i , \frac 1 M \sum _{i=1} ^ M y_i )\]</span>​  其中M≤N是实际参与定位过程的RNs的数量。从(5)可以看出，CL算法只对RN的坐标进行平均，用等权值估计BN的位置。为了进一步提高定位精度，文献中提出了几种加权CL（WCL）算法[37]-[48]。[37]中早期的WCL算法使用链路质量指示作为权值，并应用于基于zigbee的传感器网络中<span class="math display">\[(\hat x , \hat y)=(\frac {\sum _{i=1} ^ Mw_i x_i} {\sum _{i=1} ^ M w_i}  , \frac {\sum _{i=1} ^ M w_i y_i} {\sum_{i=1} ^ M w_i} )\]</span> ​  其中，权重wi是RSSPi的函数。由于现有的WCL算法需要RSS测量，这些都是基于范围的定位技术，超出了我们的研究范围。本文主要研究了仅使用单跳连接信息的无范围定位技术。</p><h1id="基于连接性的定位技术的理论分析">基于连接性的定位技术的理论分析</h1><p>​  本节推导出CRLB和理论方差来研究无距离定位技术的性能。CRLB对于参数估计非常重要，因为它提供了一个基准来评估任何无偏估计器的性能，而理论方差被用来评估给定算法的真实性能。</p><h2 id="crlb">CRLB</h2><p>​  对于无距离定位技术，BN的位置使用被检测的位置估计RN的位置。因此，测量向量为s= [x1，y1，···，xM，yM ] T，待估计的参数向量θ为[x，y] T。</p><p>​  假设PDF满足“规律性”条件： <spanclass="math display">\[E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right]=0\text{for all }\boldsymbol{\theta}\]</span>​  然后，将CRLB矩阵定义为Fisher信息矩阵（FIM）Jθ的逆： <spanclass="math display">\[E((\hat{\theta}-\theta)(\hat{\theta}-\theta)^T)\geq\mathbf{J}_\theta^{-1}\]</span>​  Fisher信息矩阵的确定为[49]： <spanclass="math display">\[\mathbf{J}_{\theta}=\begin{bmatrix}J_{xx}&amp;&amp;J_{xy}\\J_{xy}&amp;&amp;J_{yy}\end{bmatrix}=E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\left(\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right)^T\right]\]</span>​  其中 <spanclass="math display">\[f\left(\mathbf{s};\theta\right)=\prod_{i=1}^{M}f\left(x_{i},y_{i};\theta\right)\\f\left(x_{i},y_{i};\theta\right)=\frac{\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)}{\gamma}\]</span>​  f（xi，yi）描述了节点的空间分布概率，定义为(3)。 <spanclass="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span> $ (r_i) $ 为检测概率，表示在RNi处接收到的信号功率超过检测阈值Pth的概率。</p><p>γ是一个归一化常数： <spanclass="math display">\[\gamma=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)dx_{i}dy_{i}=\int\limits_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\]</span></p><p>因为 $ P_i=P_0-10 log_{10}( {r_0}) + n_i $,其中接收功率Pi、β为路径损失指数，表示路径损失随距离增加的速率、ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）、P0为参考距离r0处的参考功率，它取决于传输功率。<spanclass="math display">\[f\left(P_i\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_i-P_0+10\beta\log_{10}\left(r_i\right)\right)^2}{2\sigma^2}\right)\]</span>所以开始， $ (r_i) $ 可计算为： <spanclass="math display">\[\begin{aligned}&amp;\Phi\left(r_{i}\right)\\&amp;=p\left(P_{i}\geqP_{th}\right)=\int_{P_{th}}^{+\infty}f\left(P_{i}\right)dP_{i}\\&amp;=\int_{P_{th}}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_{i}-P_{0}+10\beta\log_{10}\left(r_{i}\right)\right)^{2}}{2\sigma^{2}}\right)dP_{i}\end{aligned}\]</span>使用替代方法，（14）可简化为： <spanclass="math display">\[\Phi\left(r_i\right)=\int_{\varphi\left(r_i\right)}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\nu^2}{2}\right)d\nu\]</span> 其中 <spanclass="math display">\[\varphi\left(r_i\right)=\begin{pmatrix}P_{th}-P_0+10\beta\log_{10}\left(r_i\right)\end{pmatrix}/\sigma\]</span> 和 $ (r_i) $ 可以使用MATLAB函数“erfc”来计算： <spanclass="math display">\[\Phi\left(r_i\right)=0.5erfc\left(\frac{\varphi\left(r_i\right)}{\sqrt{2}}\right)\]</span>方程（17）表明， $ (r_i) $只依赖于一个给定系统的距离ri。将（10）-（17）替换为(9)，如附录所示，基于连接的定位技术的CRLB为：<spanclass="math display">\[CRLB=tr\left\{\mathbf{J}_\theta^{-1}\right\}=\frac{4}{\bar{\psi}^2M}\]</span>其中 $ {}^2 $ ： <spanclass="math display">\[\begin{aligned}\bar{\psi}^{2}&amp; =\frac{1}{\gamma}\int_{0}^{+\infty}\left(\frac{1}{\Phi\left(r\right)}\frac{b}{r}\exp\left(-\frac{\varphi\left(r\right)^{2}}{2}\right)+\frac{r}{\sigma_{p}^{2}}\right)^{2}\times\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\\&amp; b = 10\beta/\left(\ln10\sqrt{2\pi}\sigma\right)\end{aligned}\]</span> 由于M是检测BN的RNs的数量，并由RNsN的总数和信道传输模型(2)决定，因此在每个定位过程中可能会发生变化。为了评估平均性能，将平均CRLB定义为<span class="math display">\[CRLB_{average}=E[CRLB]=E\left[\frac{4}{\bar{\psi}^{2}M}\right]=\frac{4}{\bar{\psi}^{2}}E\left[\frac{1}{M}\right]\]</span>显然，数值计算可以直接用于计算E[1/M]。为了进行进一步的性能分析，本文提出了一种分析方法。当M足够大时，期望均值可以替换为样本均值[34]，平均CRLB近似为：<spanclass="math display">\[CRLB_{average}\approx\frac{4}{\bar{\psi}^{2}}\frac{1}{\bar{M}}\]</span>其中 <span class="math display">\[\begin{aligned}\bar{M}&amp; =E [M]=E[N\Phi (r)]\\&amp;=NE\left[\Phi\left(r\right)\right]=N\int_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\\&amp;=N\int_{0}^{+\infty}\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\end{aligned}\]</span></p><h2id="具有任意节点分布的质心定位的理论方差">具有任意节点分布的质心定位的理论方差</h2><p>​  类似于CRLB，它提供了一个基准来评估任何无偏定位算法的性能，理论方差对于性能分析非常重要，因为它被用来评估给定算法的真实性能。虽然对于均匀节点分布[26]已经提出了CL算法的理论方差，但任意节点分布的情况仍然是一个有待解决的问题。本小节推导了具有任意节点分布的CL算法的理论方差。理论方差的定义为：<spanclass="math display">\[\begin{aligned}\operatorname{cov}(\theta)&amp;=tr\left\{E\left(\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)^{T}\right)\right\}\\&amp;=E\left(\left(\widehat{x}-x\right)^{2}+\left(\widehat{y}-y\right)^{2}\right)\end{aligned}\]</span>​  将CL估计值(5)替换入（24），得到： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}+\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)\]</span>​  上式中的第一项可以写成： <spanclass="math display">\[\begin{aligned}&amp;E\left(\left({\frac{1}{M}}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)\\&amp;=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}\left(x_{i}-x\right)\right)^{2}\right)\\&amp;\left.=E\left(\frac{1}{M^{2}}\left(\sum_{i=1}^{M}(x_{i}-x)^{2}+\sum_{i=1}^{M}\sum_{i=1}^{M}(x_{i}-x)\left(x_{j}-x\right)\right)\right)\right)\end{aligned}\]</span>​  注意，在i 不等于j的情况下，xi和xj是独立的，它可以从RN节点分布(3)中得到E（xi−x）=0。因此，（26）可以简化为： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(x_{i}-x\right)^{2}\right)\]</span>​  同理： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(y_{i}-y\right)^{2}\right)\]</span>​  将（27）和（28）代入（25），CL算法的理论方差可计算为： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}r_{i}^{2}\right)=E\left(\frac{1}{M}\right)\bar{r}^{2}\approx\frac{\bar{r}^{2}}{\bar{M}}\]</span>​  类似于（22），E [1/M]可以使用数值方法或近似方法（23）来求解。</p><p>​  ¯r2可计算为： <spanclass="math display">\[\bar{r}^{2}=\frac{1}{\gamma}\int_{0}^{+\infty}r^{2}\Phi\left(r\right)f\left(r\right)dr\]</span>​  其中f (r)为rN和BN之间范围的PDF。使用不同的f(r)，可以使用（29）和（30）计算具有任意节点分布的CL的理论方差。</p><p>​  对于高斯节点分布，f (r)可以从(4)得到： <spanclass="math display">\[f\left(r\right)=\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)r\quad0&lt;r\]</span>​  对于均匀的节点分布，f (r)为： <spanclass="math display">\[f\left(r\right)=\frac{2r}{R^{2}}\quad0&lt;r&lt;R\]</span>​  其中，R为均匀节点分布的分布半径。</p><h2 id="所提出的crlb的特征和理论方差">所提出的CRLB的特征和理论方差</h2><p>所提出的CRLB的特征和理论方差。</p><p>​  命题1：对于高接收功率，所提出的CRLB可以近似为： <spanclass="math display">\[CRLB_H\approx\frac{2\sigma_p^2}N\]</span>​  备注1：命题1表明，在高信噪比（SNR）情况下，无距离方法的性能主要取决于信N的节点分布，而不是信道环境。这种现象的发生是因为所有的rn都可以连接到一个具有良好信道环境的BN。</p><p>​  此外，下面还提出了密度λ对所提出的CRLB的影响</p><p>​  命题2：在高接收功率下，所提出的CRLB可以用密度λ近似为： <spanclass="math display">\[CRLB_H\approx\frac{1}{5.95\pi\lambda}\]</span>​  命题2表明，CRLB与λ呈负相关。这意味着一个更密集的网络将导致更高的定位精度。因此，无范围的方法是UDN的首选解决方案。以下命题提供了CL的实际性能与所提出的CRLB之间的关系。</p><p>​  命题3：在高斯节点分布和高接收功率的情况下，基于质心的定位的理论方差等于CRLB：<spanclass="math display">\[\operatorname{cov}(\theta)_H=CRLB_H=\frac{2\sigma_p^2}{N}\]</span>​  备注2：由于理论方差代表了CL方法的实际性能，因此从命题3中可以确定，在高信噪比的情况下，CL方法可以达到CRLB。这可以解释为这样一个事实，即在高信噪比的情况下，几乎所有的rN都可以与BN通信。因此，对于8(r)→1和γ→1，联合PDF（11）简化为高斯节点分布(3)。对于高斯PDF(3)，质心估计(5)是一个MLE。众所周知，MLE是渐近无偏的，并且可以渐近地获得具有明显的大测量值的CRLB。它是渐近有效的和最优的[49]。因此，CL在高信噪比的情况下具有最佳的性能。命题3也证明了所提出的CRLB的有效性。对于一个更实用的信噪比变化的信道，下一节提出了一种基于MLE的新的定位方法来提高性能。</p><p>基于</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20240522讲座</title>
      <link href="/20240522%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240522%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>记录5月22日讲座~</p><h1id="题目面对aigc的多功能数字水印与版权保护研究">题目：面对AIGC的多功能数字水印与版权保护研究</h1><p>讲师：张建</p><h2 id="图像重建">图像重建</h2><ol type="1"><li>Zero-Shot Image Restoration Using Denoising Diffusion Null-SpaceModel</li></ol><p>​  大多数现有的图像恢复（IR）模型都是特定于任务的，不能推广到不同的退化算子。在这项工作中，我们提出了去噪扩散零空间模型（DDNM,Denoising Diffusion Null-SpaceModel），这是一种新的零样本框架，用于解决任意线性IR问题，包括但不限于图像超分辨率、着色、修复、压缩感知和去模糊。DDNM只需要一个预先训练的离架扩散模型作为生成先验，而不需要任何额外的训练或网络修改。通过在反向扩散过程中仅细化零空间内容，我们可以产生满足数据一致性和真实性的不同结果。我们进一步提出了一个增强和稳健的版本，称为DDNM+，以支持噪声恢复并提高硬任务的恢复质量。我们在几个红外任务上的实验表明，DDNM优于其他最先进的零样本红外方法。我们还证明了DDNM+可以解决复杂的现实世界应用，例如旧照片恢复。</p><h2 id="图像条件生成">图像条件生成</h2><ol start="2" type="1"><li>FreeDoM: Training-Free Energy-Guided Conditional DiffusionModel</li></ol><p>​  最近，条件扩散模型由于其卓越的生成能力而在许多应用中受到欢迎。然而，许多现有的方法都是需要培训的。他们需要训练一个与时间相关的分类器或与条件相关的分数估计器，这增加了构建条件扩散模型的成本，并且不方便在不同条件下转移。目前的一些工作旨在通过提出无训练的解决方案来克服这一限制，但大多数只能应用于特定类别的任务，而不能应用于更一般的条件。在这项工作中，我们提出了一种用于各种条件的训练自由条件扩散模型（FreeDoM）。具体来说，我们利用现成的预训练网络，如人脸检测模型，来构建与时间无关的能量函数，该函数在不需要训练的情况下指导生成过程。此外，由于能量函数的构造非常灵活，能够适应各种条件，因此我们提出的FreeDoM比现有的无训练方法具有更广泛的应用范围。FreeDoM的优势在于其简单、有效和低成本。实验表明，FreeDoM在各种条件下都是有效的，适用于不同数据域的扩散模型，包括图像域和潜在代码域。</p><h2 id="图像精准控制生成">图像精准控制生成</h2><ol start="3" type="1"><li>T2I-Adapter: Learning Adapters to Dig Out More Controllable Abilityfor Text-to-Image Diffusion Models</li></ol><p>​  大规模文本到图像（T2I,text-to-image）模型令人难以置信的生成能力已经证明了学习复杂结构和有意义语义的强大能力。然而，仅仅依靠文本提示并不能充分利用模型所学到的知识，尤其是在需要灵活准确的控制（如结构和颜色）时。在本文中，我们的目标是“挖掘”T2I模型隐式学习的能力，然后显式地使用它们来更细粒度地控制生成。具体而言，我们建议学习低成本的T2I适配器，以使T2I模型中的内部知识与外部控制信号相一致，同时冻结原始的大型T2I模型。这样，我们可以根据不同的条件训练各种适配器，从而在生成结果的颜色和结构上实现丰富的控制和编辑效果。此外，所提出的T2I适配器具有可组合性和泛化能力等有吸引力的实用价值。大量实验表明，我们的T2I转换器具有良好的生成质量和广泛的应用。我们的代码可在https://github.com/TencentARC/T2I-Adapter.</p><ol start="4" type="1"><li>DragonDiffusion: Enabling Drag-style Manipulation on DiffusionModels</li></ol><p>​  尽管现有的大规模文本到图像（T2I）模型能够从详细的文本描述中生成高质量的图像，但它们往往缺乏精确编辑生成的或真实图像的能力。在本文中，我们提出了一种新的图像编辑方法，DragonDiffusion，可以在Diffusion模型上进行Drag风格的操作。具体来说，我们基于扩散模型中中间特征的强对应性来构建分类器引导。它可以通过特征对应损失将编辑信号转换为梯度，以修改扩散模型的中间表示。基于这种制导策略，我们还构建了一个多尺度制导，同时考虑语义和几何对齐。此外，增加了跨分支的自关注，以保持原始图像和编辑结果之间的一致性。我们的方法通过高效的设计，实现了对生成或真实图像的各种编辑模式，如对象移动、对象大小调整、对象外观替换和内容拖动。值得注意的是，所有编辑和内容保存信号都来自图像本身，并且该模型不需要微调或附加模块。我们的源代码将在这个httpsURL上提供。</p><ol start="5" type="1"><li>DiffEditor: Boosting Accuracy and Flexibility on Diffusion-basedImage Editing</li></ol><p>图像视频隐写</p><ol type="1"><li>Robust Invertible Image Steganography</li></ol><p>​  图像隐写术旨在将秘密图像隐藏到容器图像中，在容器图像中隐藏秘密，并在必要时进行恢复。以前的图像隐写方法在隐藏能力和鲁棒性方面受到限制，通常容易受到容器图像失真的影响，如高斯噪声、泊松噪声和有损压缩。本文提出了一种新的基于流的鲁棒可逆图像隐写框架，称为RIIS。我们引入了条件归一化流，以容器图像为条件对冗余高频分量的分布进行建模。此外，精心设计的容器增强模块（CEM）也有助于稳健的重建。为了调节不同失真水平的网络参数，我们提出了一种基于流的块上的失真引导调制（DGM），使其成为一个一刀切的模型。在干净和失真图像隐写方面，大量实验表明，所提出的RIIS有效地提高了鲁棒性，同时保持了不可见性和容量。据我们所知，我们是文献中第一个增强图像隐写术鲁棒性的基于学习的方案。隐写术鲁棒性的保证大大拓宽了隐写术在现实应用中的应用。</p><ol start="2" type="1"><li>Large-Capacity and Flexible Video Steganography via InvertibleNeural Network</li></ol><p>​  视频隐写术是一种在封面视频中不引人注目地隐藏秘密数据，然后在接收器端通过解码协议恢复秘密数据的技术。尽管已经进行了几次尝试，但大多数都局限于低容量和固定的隐写术。为了弥补这些不足，本文提出了一种大容量、灵活的视频隐写网络（LF-VSN）。对于大容量，我们提出了一种可逆管道，通过单个可逆神经网络（INN）来执行多个视频的隐藏和恢复。我们的方法可以在1个封面视频中隐藏/恢复7个秘密视频，性能良好。为了灵活性，我们提出了一种密钥可控方案，使不同的接收器能够通过特定的密钥从同一封面视频中恢复特定的秘密视频。此外，我们通过提出一种可扩展的多视频隐藏策略，进一步提高了灵活性，该策略可以用单个模型和单个训练会话在封面视频中隐藏可变数量的秘密视频。大量实验表明，随着视频隐写性能的显著提高，我们提出的LF-VSN具有高安全性、大隐藏容量和灵活性。源代码位于https://github.com/MC-E/LF-VSN.</p><h2 id="定制化溯源水印">定制化溯源水印</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM</title>
      <link href="/SAM/"/>
      <url>/SAM/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything <ahref="https://github.com/facebookresearch/segment-anything"><imgsrc="https://img.shields.io/github/stars/facebookresearch/segment-anything?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><div class="row">    <embed src="/postpdfs/SAM/2304.02643v1.pdf" width="100%" height="550" type="application/pdf"></div></details><h1 id="摘要">摘要</h1><p>​  我们介绍了分段任意事物（SA, SegmentAnything）项目：一个新的图像分割任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们建立了迄今为止（迄今为止）最大的分割数据集，在11M许可和尊重隐私的图像上有超过10亿个面具。该模型的设计和训练是及时的，因此它可以转移零镜头到新的图像分布和任务。我们评估了它在许多任务上的能力，发现它的零样本性能令人印象深刻——通常与之前的完全监督结果竞争，甚至更好。</p><h1 id="segment-anything-model">Segment Anything Model</h1><p>​  接下来，我们将描述用于快速分割的分段任何东西模型（SAM, SegmentAnything Model）。</p><figure><img src="./../postimages/SAM/image-20240519201116761.png"alt="image-20240519201116761" /><figcaption aria-hidden="true">image-20240519201116761</figcaption></figure><p><img src="./../postimages/SAM/image-20240519194006526.png"alt="image-20240519194006526" />图4：分段任何东西模型（SAM)概述。重量级图像编码器输出图像嵌入，然后可以通过各种输入提示有效地查询，以平摊的实时速度产生对象掩模。对于对应于多个对象的模糊提示，SAM可以输出多个有效的掩码和相关的置信度分数。</p><p>​  SAM有三个组件，如图4所示：图像编码器、灵活的提示编码器和快速掩码解码器。我们建立在转换视觉模型[14,33,20,62]上，对（摊销）实时性能进行特定的权衡。我们在这里高级描述这些组件，在a中详细说明。</p><h2 id="图像编码器">图像编码器</h2><p>​  一般来说，图像编码器可以是任何输出C×H×W图像嵌入的网络。基于可伸缩性和强大的预训练，我们使用MAE[47]预训练视觉transformer（ViT）[33]，具有最小的适应来处理高分辨率输入，特别是ViT-H/16，有14×14窗口注意和4个等间隔的[62]块。图像编码器的输出是输入图像的16×缩小嵌入。由于我们的运行时目标是实时处理每个提示，因此我们可以提供大量的图像编码器片段，因为它们每幅图像只计算一次，而不是每个提示计算一次。根据标准的实践（例如，[40]），我们使用了1024×1024的输入分辨率，这是通过重新缩放图像和填充较短的边而获得的。因此，图像嵌入值为64×64。为了减少信道维度，在[62]之后，我们使用1×1卷积得到256个通道，然后使用3×3卷积得到256个通道。每个卷积之后都是一个层的归一化[4]。</p><h2 id="提示编码器">提示编码器</h2><p>稀疏提示被映射到256维的向量嵌入如下。</p><p>​  一个点被表示为该点的位置的位置编码[95]和两个学习嵌入之一的总和，这表明该点是在前景中还是在背景中。</p><p>​  盒子由嵌入对表示：(1)其左上角的位置编码与表示“左上角”的学习嵌入求和，(2)相同的结构，但使用学习嵌入表示“右下角”。最后，为了表示自由形式的文本，我们使用CLIP[82]的文本编码器（任何文本编码器都是可能的）。我们将在本部分的其余部分中关注几何提示，并在D.5中深入讨论文本提示。</p><p>​  密集的提示（即掩码）与图像具有空间对应关系。我们以比输入图像低4×的分辨率输入掩模，然后使用两个2×2，步幅-2卷积分别与输出通道4和16缩小额外的4×。最后的1×1卷积将通道维度映射到256。每一层通过GELU激活[50]和层归一化分开。然后，将按元素的方式添加图像嵌入和掩码。如果没有掩码提示，则在每个图像嵌入位置添加一个表示“无掩码”的学习嵌入。</p><h2 id="轻量级掩码器">轻量级掩码器</h2><p>​  该模块有效地将图像嵌入和一组提示嵌入映射到一个输出掩码。为了结合这些输入，我们从transformer分割模型[14,20]中获得灵感，并修改了一个标准的transformer解码器[103]。在应用我们的解码器之前，我们首先在提示嵌入集中嵌入一个学习到的输出tokens嵌入，该嵌入将用于解码器的输出，类似于[33]中的[类]tokens。为简单起见，我们将这些嵌入（不包括图像嵌入）统称为“标记”。</p><p>​  我们的解码器设计如图14所示。</p><figure><img src="./../postimages/SAM/image-20240519195617684.png"alt="image-20240519195617684" /><figcaption aria-hidden="true">image-20240519195617684</figcaption></figure><p>图14：轻量级掩码解码器的细节。一个两层解码器通过交叉注意来更新图像嵌入和提示标记。然后对图像嵌入进行升级，利用更新后的输出标记来动态预测掩模。（为了图形清晰度，没有说明：在每个注意层，位置编码被添加到图像嵌入中，整个原始提示tokens（包括位置编码）被重新添加到tokens查询和键中。）</p><p>​  每个解码器层执行4个步骤：(1)对标记的自我注意，(2)从标记（作为查询）交叉注意到图像嵌入，(3)点级MLP更新每个标记，以及(4)从图像嵌入（作为查询）交叉注意到标记。这最后一步将使用提示信息更新图像嵌入。在交叉注意过程中，将图像嵌入视为一组64225个6维向量。每个自我/交叉注意和MLP都有一个残差连接[49]、层归一化和退出[93]为0.1。下一个解码器层从上一层中获取更新的tokens和更新的图像嵌入。我们使用了一个两层解码器。</p><p>​  为了确保解码器能够访问关键的几何信息，当位置编码参与注意层时，它们将被添加到图像嵌入中。此外，整个原始提示标记（包括它们的位置编码）都会被重新添加到更新后的标记中。这允许强烈地依赖于提示tokens的几何位置和类型。</p><p>​  在运行解码器后，我们用两个转置卷积对更新后的图像嵌入上采样，即4×（现在它相对于输入图像缩小了4×）。然后，tokens再次关注图像嵌入，我们将更新后的输出tokens嵌入传递给一个小的3层MLP，该MLP输出一个与升级图像嵌入的通道维数相匹配的向量。最后，我们预测了一个在升级的图像嵌入和MLP的输出之间具有空间点级乘积的掩模。</p><p>​  该transformer使用的嵌入尺寸为256。TransformerMLP块有一个很大的内部尺寸为2048，但MLP只应用于有相对较少（很少大于20）的提示tokens。在交叉注意层中，我们有一个64×64的图像嵌入，为了提高计算效率，我们将查询、键和值的通道维数降低两倍到128。所有的注意力层都使用8个头。用于升级输出图像嵌入的转置卷积为2×2，步幅为2，输出通道尺寸分别为64和32，并具有GELU激活。它们被层归一化分开。</p><h1 id="sam-adapter">SAM-Adapter</h1><p>​  《SAM Fails to Segment Anything? – SAM-Adapter: Adapting SAM inUnderperformed Scenes: Camouflage, Shadow, Medical ImageSegmentation,and More》<ahref="https://tianrun-chen.github.io/SAM-Adaptor/"><imgsrc="https://img.shields.io/badge/project-blue" alt="project" /></a> <ahref="https://arxiv.org/abs/2304.09148"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/tianrun-chen/SAM-Adapter-PyTorch"><imgsrc="https://img.shields.io/github/stars/tianrun-chen/SAM-Adapter-PyTorch?style=flat"alt="GitHub" /></a></p><p>​  该文指出号称可以“分割一切”的SAM模型虽然在自然图像的通用分割任务中取得了优异的效果，但在许多特殊图像的特定分割任务上表现差强人意，如水下目标分割、阴影分割、伪装对象分割等。作者认为这是由于SAM主要在常见的自然图像中进行训练，其特征提取器不能很好的适应特殊图像。因此作者提出一种轻量化的适配器模块（Adaptor），对SAM的编码器得到的特征图进行适应性调整。编码器的输入为特定任务信息，该文采用了图块嵌入特征和高频成分特征，将两种特征相加后经过两个MLP层得到适配器模块的输出，并将该输出与对应SAM编码器的Transformer层输出相加，并传递至下一层。训练过程中SAM编码器的参数保持不变，解码器部分使用SAM的参数进行初始化，然后利用特定数据集进行微调。</p><figure><img src="./../postimages/SAM/image-20240519202804593.png"alt="image-20240519202804593" /><figcaption aria-hidden="true">image-20240519202804593</figcaption></figure><p>​  如上图所示，该模型使用了SAM的图像编码器和掩模解码器，其中图像编码器冻结了参数，解码器是参与梯度回传的。这样可以有效利用SAM已经预训练好的分割能力，同时解码器更新参数以改装下游任务。另外引入了Adaptor模块，用于引入特殊任务的知识，辅助适配器模型。Adaptor的网络结构由两层MLP层构成，其输入的知识可以是微处理器的，对于文中的任务，其输入可以是纹理信息或者是频率信息等。各种信息用下面的权重来均衡。</p><blockquote><p>该文提出的Adaptor模块包括所使用的两个特定任务信息——图块嵌入特征和高频成分特征，都是来源于另一篇论文《ExplicitVisual Prompting for Low-Level StructureSegmentations》（EVP）。图块嵌入特征就是将图片划分成若干个图块，利用ViT将其映射为一个$ C_{seg} $维的特征；高频成分特征，则是将图片进行快速傅里叶变换，并保留其中的高频成分，再进行反变换得到高频成分对应的时域图，最后经过一个线性映射层得到一个特征向量。</p></blockquote><p>​  实验表明，在多个任务中SAM-Adapter均取得了远超SAM的表现，甚至由于各自领域的其他优秀算法，作为SAM的一种改进思路还是有值得借鉴和学习的地方。然而，整篇论文的思路几乎完全照搬了EVP，只是将模型从SegFormer换成了SAM，其他并没有明显改变。但在实验章节的算法效果对比中却回避了EVP，尤其是有些结果还不如EVP，这就很让人质疑其原创性和先进性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CMX:_Cross-Modal_Fusion_for_RGB-X_Semantic_Segmentation_With_Transformers</title>
      <link href="/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/"/>
      <url>/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/</url>
      
        <content type="html"><![CDATA[<p>CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation WithTransformers(TruFor使用了这个方法)</p><figure><imgsrc="../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170446816.png"alt="image-20240516170446816" /><figcaption aria-hidden="true">image-20240516170446816</figcaption></figure><p>1、原任务是分割任务，论文提出了一种将RGB图与其他图特征充分融合的方法RGB-X，可以从RGB图与X图提取特征。</p><p>2、RGB-X主要由两个部分组成：CM-FRM、FFM。</p><p>CM-FRM用于提取图片特征，其可以纠正关于另一个特性的一个特性，反之亦然，将属于同一层次的特征融合成一个单一的特征图。</p><p>FFM参考自注意力机制，设计了一种将特征融合的方法最后通过融合特征，完成分割任务。</p><p>TruFor使用了cmx来提取融合特征</p><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170520387.png"alt="image-20240516170520387" /><figcaption aria-hidden="true">image-20240516170520387</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170527564.png"alt="image-20240516170527564" /><figcaption aria-hidden="true">image-20240516170527564</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170535748.png"alt="image-20240516170535748" /><figcaption aria-hidden="true">image-20240516170535748</figcaption></figure><p>两阶段的特征融合模块（FFM）来增强信息的交互和组合。</p><p>在信息交换阶段（阶段1），两个分支仍然保持不变，并设计了一种交叉注意机制，在两个分支之间进行全局信息交换。</p><p>在融合阶段（阶段2），通过混合嵌入通道将连接的特征转换为原始大小。</p>]]></content>
      
      
      <categories>
          
          <category> 损失函数 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Uncertainty-Uncertainty_Learning_for_Improving_Image_Manipulation_Detection</title>
      <link href="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/"/>
      <url>/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<p>Uncertainty-guided Learning for Improving Image ManipulationDetection</p><h1 id="摘要">摘要</h1><p>​图像操纵检测（IMD）至关重要，因为伪造图像和传播错误信息可能是恶意的，会危害我们的日常生活。IMD是解决这些问题的核心技术，并在两个主要方面提出了挑战：（1）数据不确定性，即被操纵的工件通常很难被人类辨别，并导致噪声标签，这可能会干扰模型训练；（2）模型不确定性，即由于操纵操作，同一对象可能包含不同的类别（篡改或未篡改），这可能会混淆模型训练并导致不可靠的结果。以往的工作主要集中在通过设计细致的特征和网络来解决模型的不确定性问题，但很少考虑数据的不确定性。在本文中，我们通过引入一个不确定性引导的学习框架来解决这两个问题，该框架通过一个新的不确定性估计网络（UEN）来测量数据和模型的不确定性。UEN在动态监督下进行训练，并输出估计的不确定性图来细化操纵检测结果，这显著缓解了学习困难。据我们所知，这是第一项将不确定性建模嵌入IMD的工作。在各种数据集上进行的大量实验证明了最先进的性能，验证了我们方法的有效性和可推广性。</p><p>​  将不确定性引入图片篡改检测：Model不确定性（同一对象因不同的模型标记不同）与 Data不确定性（误标签与漏标签）</p><p>​  Model不确定性由U^p测定，p_t是第 t次采样中的估计操纵得分图；U^GT是真实值 y 和 μ̂之间的差异，可以测定Model不确定性和Data不确定性，可以用动态不确定性监督Lu，让U^GT专注于Data不确定性</p><p>​  其主干网络是HRNetV2 （特征提取网络）</p><p>​  模型基于数据不确定性可以增强复杂边缘的篡改检测，精细化粗略输出的边缘，得到更好的结果</p><p>​  该模型是在7张A100上训练，可见训练难度大，不易收敛</p><figure><imgsrc="../postimages/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/image-20240516170144843.png"alt="image-20240516170144843" /><figcaption aria-hidden="true">image-20240516170144843</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pre-training-free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning</title>
      <link href="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/"/>
      <url>/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<center>Pre-training-free Image Manipulation Localization through Non-MutuallyExclusive Contrastive Learning <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-orange" alt="ICCV" /></a></center><center><span class="math inline">\(\text{Jizhe Zhou}^{1,4},\text{XiaochenMa}^{1,4},\text{Xia Du}^{2,4},\text{AhmedY}.\text{Alhammadi}^{3,4},\text{Wentao Feng}^{1,4*}\)</span></center><center>1四川大学计算机科学学院</center><center>2厦门理工大学计算机与信息工程学院</center><center>3 Mohamed Bin Zayed大学战略事务办公室</center><center>4中国教育部机器学习与产业智能工程研究中心</center><h1 id="摘要">摘要</h1><p>​  深度图像操作定位（IML）模型存在训练数据不足，严重依赖于预训练。我们认为对比学习更适合于解决IML的数据不足问题。形成相互排斥的正性与负性是对比学习的先决条件。然而，当在IML中采用对比学习时，我们遇到了三类图像补丁：篡改、真实和轮廓补丁。篡改和真实的补丁自然是相互排斥的，但是包含篡改和真实像素的轮廓补丁对它们不是相互排斥的。<br/>​  简单地取消这些轮廓补丁会导致巨大的性能损失，因为轮廓补丁对学习结果是决定性的。因此，我们提出了非互斥对比学习（NCL，Nonmutually exclusive ContrastiveLearning）框架来从上述困境中拯救传统的对比学习。在NCL中，为了应对非互斥性，我们首先建立一个具有双分支的枢轴结构，在训练时不断地在正和负之间切换轮廓补丁的作用。然后，我们设计了一个枢轴一致的损失，以避免由角色转换过程造成的空间损坏。<br/>​  通过这种方式，NCL既继承了自监督的优点来解决数据不足，又保留了较高的操作定位精度。大量的实验证明，我们的NCL在没有任何预训练的情况下，在所有五个基准测试上都达到了最先进的性能，并且在看不见的真实样本上更鲁棒。</p><h1 id="引言">1. 引言</h1><p>​  媒体技术的惊人进步使我们更容易地获得操作图像。图像处理定位（IML）是防御性信息取证中必不可少的一部分，并得到了信息安全行业的大量投资。今天，数据不足是构建深度IML模型中最突出的问题。由于用于篡改识别的密集注释和专业知识过高，IML的公共数据集都很小（有几百到几千张图像），严重不足以训练深度cnn。因此，主要的深度IML方法在额外的大规模数据集上进行预训练。<br/>​  一般来说，IML模型的预训练依赖于综合数据集。一方面，合成数据集消除了较高的标记成本，对合成数据集的预训练避免了过拟合。另一方面，使用综合数据集进行预训练会阻碍模型之间的公平比较，甚至危及模型的通用性。预训练对模型的性能至关重要，为了公平比较，同一任务的模型通常在同一数据集上进行预训练。然而，IML模型的合成预训练数据集在注释数量和质量上存在显著差异。例如，ManTra-Net[34]基于一个自收集的、像素标记的数据集102,028张图像和385种操作类型进行预训练；RGBN[38]采用了超过42000张图像的随机合成数据集；星网[33]包含100,000张复制移动图像的合成数据集用于预训练；MVSS[9]采用84000张图像的合成数据集。对在不同的合成数据集上预先训练的模型进行忠实的评估变得不可能了。此外，与真实的篡改图像不同，这些天真合成的图像严重缺乏复杂的后处理来覆盖它们的操作痕迹或伪影[5,29,9]。换句话说，合成数据集的采样过程偏向于人工构建数据集[36,37]的采样过程。在这样一个具有抽样偏差的数据集上学习的模型在通用性上很短，并且在很小的、非同源的基准上测量这种模式不能完全揭示其在真实情况下的糟糕性能。</p><p>​  总之，我们的主要贡献是四重的：</p><ul><li><strong>没有额外的数据。</strong>据我们所知，我们是第一个在IML中引入对比学习来解决训练数据的不足和训练前造成的缺陷的工作。</li><li><strong>非相互排斥的对比。</strong>据我们所知，我们也是第一个通过对比学习处理非互排他的三边关系。我们的非互斥对比学习（NCL，Non-mutuallyexclusive ContrastiveLearning）框架可以服务于其他任务，如语义分割或目标细粒度检测。</li><li><strong>最高的基准性能。</strong>我们的方法使用较少和较差的训练数据，但在所有五个公共基准上取得了最先进的性能以及最高的模型泛化能力。</li><li><strong>插件价值。</strong>我们的方法功能可以基于CNN风格和Transformer风格的骨干网络。主干选择不会破坏NCL的完整性。</li></ul><h1 id="方法">3. 方法</h1><h2 id="基本的编码器-解码器结构">3.1 基本的编码器-解码器结构</h2><p>​  我们采用DeepLabV3+[4]作为我们的IML模型的基本编码-解码器结构，因为它已经被许多其他IML模型作为基线[13,9]。请注意，基本模式选择或主干模式选择会影响我们的NCL的有效性。因此，图2中的编码器主干是ResNet101[15]块，在最后几个块中存在空洞卷积。同样应用了空间空间金字塔池（ASPP）块。然后，所编码的大小特征（64×64）被传递给解码器。该解码器采用了两个上采样模块。编码器的输出被两次上采样到4倍。简而言之，我们的基本编码器-解码器应用了与DeepLabV3+模型相同的网络结构和训练设置。</p><h2 id="非互斥对比学习">3.2 非互斥对比学习</h2><h3 id="问题公式">3.2.1 问题公式</h3><p>​  对于传统的对比学习，将问题域定义为通用集<spanclass="math inline">\(\mathbb{U}\)</span>。如图1中传统的对比学习部分所示，我们有集合的正类<spanclass="math inline">\(\mathbb{P}\)</span>，集合的负类<spanclass="math inline">\(\mathbb{N}\)</span>，其中： <spanclass="math display">\[\begin{aligned}\mathbb{P}\cup\mathbb{N}&amp;=\mathbb{U}\\\mathbb{P}\cap\mathbb{N}&amp;=\emptyset\end{aligned}\]</span>​  <spanclass="math inline">\(\emptyset\)</span>表示正类与负类的互斥性。将<spanclass="math inline">\(p\)</span>标记为一个被篡改的图像补丁，是<spanclass="math inline">\(\mathbb{P}\)</span>的一个元素。对于<spanclass="math inline">\(\forallp\in\mathbb{P}\)</span>，我们进一步表示<spanclass="math inline">\(p_{j}\in\mathbb{P},p_{j}\neq p\)</span>；和<spanclass="math inline">\(n_i\in\mathbb{N}\)</span>。那么，传统的对比学习目标是：<span class="math display">\[\arg\max_{f}\{\sum_{ii}\phi(f(p),f(n_{i}))-\phi(f(p),f(p_{j}))\}\]</span> ​  <spanclass="math inline">\(f(\cdot)\)</span>是一个图像patch的学习特征表示。<spanclass="math inline">\(f(p_{j})\)</span>和<spanclass="math inline">\(f(n_{j})\)</span>为图2中IML特征图中的红色和蓝色立方体，<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>表示两个特征向量之间的测量距离，即相似度。本文的符号是统一的，其中图像块集用大写字母表示，图像块用小写字母表示，<spanclass="math inline">\(f(\cdot)\)</span>函数是图像块的学习特征表示。</p><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240919233419057.png"alt="image-20240919233419057" /><figcaption aria-hidden="true">image-20240919233419057</figcaption></figure><p>​  然而，对于图1所示的NCL，我们有： <spanclass="math display">\[\begin{aligned}&amp;\mathbb{N}\cup\mathbb{P}\cup\mathbb{C}=\mathbb{U}\\&amp;\mathbb{P}\cap\mathbb{N}=\emptyset;\mathbb{C}\cap\mathbb{N}=\mathbb{C}^-;\mathbb{C}\cap\mathbb{P}=\mathbb{C}^+\end{aligned}\]</span>​  <spanclass="math inline">\(\mathbb{C}\)</span>是所有轮廓补丁的集合。<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>表示为<spanclass="math inline">\(\mathbb{C}\)</span>与正集和负集的交点。这意味着正像素和负像素混合在轮廓补丁中。对于对比学习，通过在同一集合中找到另一个元素，可以很容易地形成正对。根据(1)和(2)，空的交点意味着如何形成重要的负对。因此，我们首先将(3)与(1)修改为完全相同的格式。用一些小技巧，我们就可以有：<spanclass="math display">\[\begin{aligned}&amp;\mathbb{N}\cup\mathbb{P}\cup\mathbb{C}=\mathbb{U}\\&amp;\mathbb{P}\cap\mathbb{N}=\mathbb{C}^+\cap\mathbb{N}=\mathbb{C}^-\cap\mathbb{P}=\emptyset\end{aligned}\]</span>​  然后，根据(1)，我们现在可以将(3)中所写的非互斥对比转换为（<spanclass="math inline">\(\mathbb{P}\cap\mathbb{N}\)</span>）、（<spanclass="math inline">\(\mathbb{C}^+\cap\mathbb{N}\)</span>）和（<spanclass="math inline">\(\mathbb{C}^-\cap\mathbb{P}\)</span>）之间的三个二进制对比。为了进行三对比较，我们首先需要找出(3)中定义的<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>。<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>也是补丁片段或像素。基本的编码器网络不能产生补丁片段的特征。因此，我们设计了枢轴网络，直接使用轮廓块作为输入，并生成C+和C−的特征表示。也就是说，枢轴网络通过学习（<spanclass="math inline">\(\mathbb{C}\cap\mathbb{C}^+\)</span>）和（<spanclass="math inline">\(\mathbb{C}\cap\mathbb{C}^-\)</span>）之间的两个映射函数来切换轮廓斑块的作用。自然地，枢轴网络应该拥有两个具有相同输入的相似分支。</p><h3 id="枢轴网络">3.2.2 枢轴网络</h3><p>​  在构建支点网络的详细布局之前，我们需要进一步考虑支点网络的输入。训练枢轴网络也需要足够的轮廓补丁。但是，如果我们选择一个较小的斑块大小来生成更多的轮廓斑块。小的补丁大小导致一个图像补丁中的少量像素。那么，<spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>中的一些元素可能包含少量的像素，不适合训练枢轴网络。因此，在单个图像中，我们将所有的轮廓块特征连接到一个完整的嵌入<spanclass="math inline">\(\mathfrak{p}\)</span>中，并发送<spanclass="math inline">\(\mathfrak{p}\)</span>作为枢轴网络的输入，以确保学习结果足够显著地进行比较。</p><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920091847744.png"alt="image-20240920091847744" /><figcaption aria-hidden="true">image-20240920091847744</figcaption></figure><p>图2。 (a): 我们的NCL框架的一般网络结构。 (b):详细的透视图网络结构。绿色箭头表示通过枢轴网络进行非互斥对比，然后产生非互斥对比学习（NCL）损失。赭色的箭头表示产生枢轴一致（PC）损失的流量。第一个编码器块输出的特征图根据地面真相，按点方向分为篡改（红色）、真实（蓝色）和等高线（紫色）特征。黄色矩形的伪造面具是不同大小的地面真相。功能尺寸用方括号表示。</p><p>​  在图2中，这个连接将紫色的立方体组装成一条大小的条带（<spanclass="math inline">\(k\times C\times W\times H\)</span>）。<spanclass="math inline">\(k=card(\mathbb{C})\)</span>。C、W、H是一个轮廓特征的通道、高度和宽度。<spanclass="math inline">\(card()\)</span>表示基数或集<spanclass="math inline">\(\mathbb{C}\)</span>中的元素数。一方面，利用<spanclass="math inline">\(k=card(\mathbb{C})\)</span>，我们将轮廓块特征连接成一个向量（<spanclass="math inline">\(k\times C\times W\timesH\)</span>）。该向量将整个图像中的轮廓块特征聚集在整个图像中，以解决当存在少数轮廓块时的模型效率低下的问题。另一方面，枢轴网络将这个（<spanclass="math inline">\(k\times C\times W\timesH\)</span>）向量平化为一个固定大小的（<spanclass="math inline">\(1\times C\times W\timesH\)</span>）向量。这进一步有助于处理特征处理中k的不同大小。<br/>​  枢轴网络的详细结构如图2(b)所示，通过粉红色的矩形和绿色的箭头表示。<br/>​  然后，我们为我们的枢轴网络设计了两个对称的分支。这些分支共享相同的输入和具有相同的结构。<spanclass="math inline">\(\mathfrak{p}\)</span>是由（1×1）卷积产生的第一个过程。这个（1×1）卷积核使<spanclass="math inline">\(\mathfrak{p}\)</span>扁平化为（<spanclass="math inline">\(1\times C\times W\timesH\)</span>）的形状。此外，这个（1×1）核将p投射到一个潜在的Hilbert空间<spanclass="math inline">\(\mathcal{H}:\mathbb{R}^{C\times W\timesH}\)</span>，其中<span class="math inline">\(f(p_{j})\)</span>和<spanclass="math inline">\(f(n_{j})\)</span>确定，特征之间的相似性可以用<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>均匀地度量。BN和ReLU是批处理规范化层和ReLU激活层。<br/>​  枢轴网络在输入集<spanclass="math inline">\(\mathbb{C}\)</span>（<spanclass="math inline">\(c\in\mathbb{C}\)</span>）和输出集<spanclass="math inline">\(\mathbb{C}^+\)</span>（<spanclass="math inline">\(c\in\mathbb{C}^+\)</span>）和<spanclass="math inline">\(\mathbb{C}^-\)</span>（<spanclass="math inline">\(c\in\mathbb{C}^-\)</span>）之间构造反射f（·）。因此，<spanclass="math inline">\(f(\cdot)\)</span>应满足：<br/>​   (1). <spanclass="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>有利于IML的精度；<br/>​   (2).<span class="math inline">\(\mathbb{C}^+\)</span>和<spanclass="math inline">\(\mathbb{C}^-\)</span>是平滑的流形，以保证NCL损失的反向传播。由于<spanclass="math inline">\(\mathbb{C}\)</span>是一个光滑的流形（受限于Euclidean空间)，所以<spanclass="math inline">\(f(\cdot)\)</span>应该是一个双射体；<br/>​   (3).反射后无信息丢失。这意味着我们可以通过一些二进制操作<spanclass="math inline">\((\cdot)\)</span>将<spanclass="math inline">\({c}^+\)</span>和<spanclass="math inline">\({c}^-\)</span>组合回<spanclass="math inline">\({c}\)</span>；<span class="math inline">\(c^+\cdotc^-=c,c^+\cdot c=c,c^-\cdotc=c.\)</span>。<br/>​  因此，我们可以有一个组<spanclass="math inline">\((G,\cdot)\)</span>，其中<spanclass="math inline">\(G=\mathbb{C}^+\cup\mathbb{C}^-\)</span>。G是一个李群，因为：<br/>​  根据(2)，组逆<span class="math inline">\(G\toG\)</span>是平滑的。<br/>​   根据(3)，组乘<spanclass="math inline">\(G\times G\toG\)</span>是平滑的。<br/>​  因此，枢轴网络（<spanclass="math inline">\({c}^+\)</span>和<spanclass="math inline">\({c}^-\)</span>）的输出是李群元素。然后，我们将枢轴网络作为一个光滑的映射函数，并从李群中借用<spanclass="math inline">\(\mathfrak{se}\)</span>符号。我们将这两个分支的输出写成<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>。<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>只是表示由枢轴网络学习到的特征变换函数；我们不能保证它们是微分流形。<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>是在图2(b)中得到的浅红色和浅蓝色的立方体。<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>的集合是所期望的<spanclass="math inline">\(\mathbb{PI}^{+}\)</span>和<spanclass="math inline">\(\mathbb{PI}^{-}\)</span>。对<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>的直观解释是，它们是由枢轴网络生成的特征<spanclass="math inline">\(\mathfrak{p}\)</span>压缩出来的特殊的正负特征；而常见的正负特征是由主干网络根据物理上存在的图像补丁产生的。从这个角度来看，枢轴网络就像钟摆一样在正与负之间摆动枢轴的作用。<br/>​  基于H中的<spanclass="math inline">\(f(\cdot)\)</span>和<spanclass="math inline">\(\phi(\cdot,\cdot)\)</span>、<spanclass="math inline">\(\mathfrak{se}^+(\cdot)\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\cdot)\)</span>，我们将NCL的学习目标表述为：<span class="math display">\[&amp;\arg\max_{f,s\mathbf{c}^{+},\mathbf{s}^{-}}\{\sum_{i,j}\phi(f(p),f(n_{i}))-\phi(f(p),f(p_{j}))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}))-\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),f(p_{j}))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{s}\mathfrak{c}^{+}(\mathfrak{p}),\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}))-\phi(\mathfrak{s}\mathfrak{c}^{-}(\mathfrak{p}),f(n_{i}))\}\]</span></p><h3 id="非互斥的对比度损失">3.2.3 非互斥的对比度损失</h3><p>​  我们确实可以根据(5)构造NCL损失函数。但是，由于轴网络为每个被操纵的图像产生一个<spanclass="math inline">\(\mathfrak{se}^+(\mathfrak{p})\)</span>和<spanclass="math inline">\(\mathfrak{se}^-(\mathfrak{p})\)</span>，<spanclass="math inline">\(\phi(\mathfrak{se}^+(\mathfrak{p}),\mathfrak{se}^-(\mathfrak{p}))\)</span>独立于和参数<spanclass="math inline">\(i,j\)</span>，并在损失积累过程中成为一个常数。这样的常数破坏了对比对的多样性。因此，我们在正对的构建中做了一些小的替换，并进一步细化(5)为：<spanclass="math display">\[&amp;\arg\max_{f,\mathfrak{sc}^+,\mathfrak{sc}^-}\{\sum_{i,j}\phi(f(p),f(n_i))-\phi(f(p),f(p_j))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{sc}^+(\mathfrak{p}),f(n_i))-\phi(\mathfrak{sc}^+(\mathfrak{p}),f(p_j))\}+\\&amp;\{\sum_{i,j}\phi(\mathfrak{sc}^-(\mathfrak{p}),f(p_j))-\phi(\mathfrak{sc}^-(\mathfrak{p}),f(n_i))\}\]</span>​  通过我们的枢轴网络，在(6)中，NCL将三边图像补丁之间的非互斥关系改革为三个由“+”连接的互斥、成对的二值比较。这是由图2中的NCL监督绘制的。为了简化，我们让<spanclass="math inline">\(p = p_m\)</span>分配一个下标；标记$e_{x}^{y} =(f(x),f(y))/<span class="math inline">\(，\)</span>e_{x}^{-} =(<sup>{-}(),f(x))/<span class="math inline">\(和\)</span>e_{x}</sup>{+}= (^{+}(),f(x))/<span class="math inline">\(，其中\)</span><spanclass="math inline">\(是温度参数。参考(6)，NCL损失函数为：\)</span><spanclass="math inline">\(L_{NCL}=\frac1{m\timesj}\sum_m\sum_j\log\frac{e_{p_m}^{p_j}}{e_{p_m}^{p_j}+\sum_ie_{p_m}^{n_i}}+\frac1j\sum_j\log\frac{e_{p_j}^+}{e_{p_j}^++\sum_ie_{n_i}^+}+\frac1i\sum_i\log\frac{e_{n_i}^-}{e_{n_i}^-+\sum_je_{p_j}^-}\)</span>$​  最后但并非最不重要的是，我们探索了实施支点网络的确切位置。之前的一些工作[3]截断了不同层的深度cnn，揭示了早期截断的网络为伪造检测提供了更好的特征。此外，早期截断的网络布局浅，接收场小，大特征图，理想地满足NCL小块尺寸的要求。然后，我们将ResNet101划分为卷积块，如在他们的论文[15]中所述，并探索由每个ResNet101块产生的特征图。如预期的那样，实验结果验证了第一个块后的特征图是最合适的。在实验部分，我们提供了更多关于NCL图像补丁大小选择的详细信息。</p><h2 id="枢轴一致性损失">3.3 枢轴一致性损失</h2><p>​  枢轴网络对连接的轮廓块进行卷积，破坏了轮廓块内部和之间的空间相关性。[16]已经表明，空间信息在IML中是至关重要的。因此，我们在解码器侧开发了一个枢轴一致性（PC，PivotConsistent）损失，以确保轮廓斑块的空间相关性在枢轴网络后仍然存在。PC损失在基本的像素级BCE损失中为轮廓像素分配额外的权重<spanclass="math inline">\(\mu\)</span>，以加强轮廓像素之间的空间连接。然而，轮廓像素的数量远远少于操纵或真实的像素。为了避免过拟合，如图2(a)中解码器侧的赭色箭头所示，我们使用辅助分类器[7]，在每个上采样过程中逐步累积PC损失。每次上采样后，我们将groundtruth缩小到与特征图相同的大小；然后，可以通过缩小的伪造掩模进行像素级的IML监督。我们在这里略滥用小写字母的符号。<spanclass="math inline">\(t\)</span>表示图像中的像素，<spanclass="math inline">\(\hat t\)</span>表示轮廓像素，<spanclass="math inline">\(\mu\)</span>表示额外的权重。<spanclass="math inline">\(\gamma(\cdot)\)</span>是一个像素的地面真实标签，<spanclass="math inline">\(\theta(\cdot)\)</span>是我们的网络对一个像素的预测标签。<spanclass="math inline">\(\gamma(\cdot)\)</span>和<spanclass="math inline">\(\theta(\cdot)\)</span>提供二进制值作为输出。那么，我们的PC损失是：<spanclass="math display">\[L_{PC}=\frac{\mu}{\hat{t}}\sum_{\hat{t}}(\gamma(\hat{t})\log(\theta(\hat{t}))+(1-\gamma(\hat{t}))\log(1-\theta(\hat{t})))+\frac{(1-\mu)}t\sum_t(\gamma(t)\log(\theta(t))+(1-\gamma(t))\log(1-\theta(t)))\]</span>​  我们发现较大的<spanclass="math inline">\(\mu\)</span>有利于最终的IML精度。对<spanclass="math inline">\(\mu\)</span>的评估详见实验部分。</p><h2 id="总损失函数">3.4 总损失函数</h2><p>​  综上所述，IML的NCL的混合总损失为： <spanclass="math display">\[L_{total}=\omega\times L_{NCL}+L_{PC}\]</span>​  <spanclass="math inline">\(\omega\)</span>是在浅层编码器层上进行的非互斥对比学习的权值参数。更多的<spanclass="math inline">\(\omega\)</span>可以在实验部分找到。</p><h1 id="实验和讨论">4. 实验和讨论</h1><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095639419.png"alt="image-20240920095639419" /><figcaption aria-hidden="true">image-20240920095639419</figcaption></figure><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095705963.png"alt="image-20240920095705963" /><figcaption aria-hidden="true">image-20240920095705963</figcaption></figure><figure><imgsrc="../postimages/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/image-20240920095718686.png"alt="image-20240920095718686" /><figcaption aria-hidden="true">image-20240920095718686</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ANOMALYCLIP</title>
      <link href="/ANOMALYCLIP/"/>
      <url>/ANOMALYCLIP/</url>
      
        <content type="html"><![CDATA[<p>ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING FOR ZERO-SHOT ANOMALYDETECTION <a href="https://arxiv.org/abs/2310.18961"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/zqhang/AnomalyCLIP"><imgsrc="https://img.shields.io/github/stars/zqhang/AnomalyCLIP?style=flat"alt="GitHub" /></a></p><p><strong>Qihang Zhou</strong>1<em>∗</em> <strong>, GuansongPang</strong>2<em>∗</em> <strong>, Yu Tian</strong>3 <strong>, ShiboHe</strong>1<em>†</em> <strong>, Jiming Chen</strong>1<em>†</em></p><p>1浙江大学2新加坡管理大学3哈佛大学</p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/ANOMALYCLIP/2310.18961v8.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  零样本异常检测（ZSAD, Zero-shot anomaly detection）需要使用辅助数据进行训练的检测模型，以便在目标数据集中没有任何训练样本的情况下检测异常。这是一个至关重要的任务时，当训练数据无法访问由于各种问题，例如，数据隐私，但它是具有挑战性的，因为模型需要推广异常在不同领域前景对象的外观，异常区域和背景特性，如缺陷/肿瘤在不同的产品/器官，可以显著不同。最近，大型的预先训练的视觉语言模型（VLMs），如CLIP，在包括异常检测在内的各种视觉任务中显示出了很强的零样本识别能力。然而，它们的ZSAD性能较弱，因为vlm更关注于前景对象的类语义建模，而不是图像中的异常/正常性。</p><p>​  在本文中，我们介绍了一种新的方法，即AnomalyCLIP，使CLIP在不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习与对象无关的文本提示，这些提示捕获图像中的一般正常性和异常性，而不管其前景对象如何。这使得我们的模型能够关注异常的图像区域，而不是对象的语义，从而能够对不同类型的对象概括归纳正常和异常识别。在17个真实异常检测数据集上进行的大规模实验表明，AnomalyCLIP在不同缺陷检测和医学成像领域的高度多样性类语义数据集的异常检测和分割方面具有优越的零镜头性能。</p><h1 id="引用">引用</h1><p>本文的主要贡献如下。</p><p>  1.我们首次揭示了学习对象不可知的文本提示的正常和异常是一种简单而有效的准确的ZSAD方法。与目前主要为对象语义对齐而设计的文本提示方法(Jeong et al., 2023; Zhou et al.,2022b)相比，我们的文本提示嵌入模型语义的一般异常和正常，允许对象无关，广义ZSAD性能。<br/>  1.然后，我们引入了一种新的ZSAD方法，称为AnomalyCLIP，其中我们利用一个对象不可知的提示模板和一个g局部异常损失函数（即全局和局部损失函数的组合）来学习通用异常和正常提示。在此过程中，AnomalyCLIP在很大程度上简化了提示设计，并可以有效地应用于不同的领域，而不需要更改其学习到的两个提示，这与WinCLIP等现有的方法不同，后者的有效性很大程度上依赖于对数百个手动定义提示的广泛工程。<br/>  1.对来自不同工业和医学领域的17个数据集进行的综合实验表明，AnomalyCLIP在检测和分割来自缺陷检查和医学成像领域的高度多样性类语义数据集的异常方面具有优越的ZSAD性能。</p><h1 id="方法">方法</h1><h2 id="与对象无关的提示学习">与对象无关的提示学习</h2><h3 id="方法概述">方法概述</h3><p>​  在本文中，我们提出了一种催化CLIP通过对象不可知的提示学习使CLIP适应ZSAD。</p><figure><img src="./../postimages/anomalyClip/image-20240519164440456.png"alt="image-20240519164440456" /><figcaption aria-hidden="true">image-20240519164440456</figcaption></figure><p>图2：AnomalyCLIP的概述。为了使CLIP适应于ZSAD，AnomalyCLIP引入了与对象无关的文本提示模板来捕获一般的正常性和异常性，而不管对象的语义如何。然后，我们引入了全局上下文优化，将全局和细粒度的异常语义纳入到对象不可知的文本提示学习中。最后，使用文本提示调优和DPAM，在CLIP的文本和局部视觉空间中实现提示学习。</p><p>​  如图2所示，AnomalyCLIP首先引入了对象不可知的文本提示模板，其中我们设计了$ g_n $ 和 $ g_a $的两个通用的对象不可知的文本提示模板，分别学习正态类和异常类的广义嵌入(见Sec.3.2)。为了学习这种通用的文本提示模板，我们引入了全局和局部上下文优化，将全局和细粒度的异常语义合并到与对象无关的文本嵌入学习中。此外，文本提示调优和DPAM还被用于支持在CLIP的文本和局部视觉空间中的学习。最后，我们整合了多个中间层，以提供更多的局部视觉细节。在训练过程中，所有模块通过全局和局部上下文优化进行联合优化。在推理过程中，我们量化了文本和全局/局部视觉嵌入的错位，以分别获得异常得分和异常得分图(见Sec.3.3)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dual_Defense</title>
      <link href="/Dual-Defense/"/>
      <url>/Dual-Defense/</url>
      
        <content type="html"><![CDATA[<p>Dual Defense: Adversarial, Traceable, and Invisible RobustWatermarking Against Face Swapping</p><h1 id="摘要">摘要</h1><p>​深度人脸交换技术的恶意应用构成了虚假信息传播和身份欺诈等安全威胁。一些研究提出了利用鲁棒水印方法来跟踪人脸图像的版权，促进伪造后的身份归属。然而，这些方法并不能从根本上防止或消除换脸的不利影响。为了解决这个问题，我们提出了双重防御，这是一种基于<strong>鲁棒对抗性水印</strong>的创新框架。它通过一次嵌入鲁棒对抗性水印，同时跟踪图像版权并破坏人脸交换模型。</p><p>​ 具体而言，我们提出了一种原域特征冒充攻击（OFEA, Original-domainFeature EmulationAttack）方法，该方法通过专门设计的原始域对抗性损失，使可跟踪水印更具攻击性。此外，我们将小波域图像结构信息补偿损失与通道注意力机制相结合，以联合平衡水印的不可见性、对抗性和可追溯性。此外，我们设计了一种更全面、更合理的评估方法来全面评估对抗性攻击对人脸交换模型的有效性。大量实验表明，双重防御表现出非凡的跨任务通用性和数据集泛化能力。它在原始和稳健的环境中都保持了令人印象深刻的对抗性和可追溯性，超过了目前仅拥有其中一种功能的伪造防御方法。</p><figure><img src="./../postimages/Dual-Defense/image-20240513111747304.png"alt="image-20240513111747304" /><figcaption aria-hidden="true">image-20240513111747304</figcaption></figure><p>图1。深度伪造主动防御场景的说明。基于水印的(a)主动防御。能够追踪伪造图像的来源，但不能防止伪造，并消除其在来源上的不利影响。基于对抗性例子的(b)积极防御。它可以破坏伪造文件的生成，但不支持可追溯性，在攻击失败时不提供可追溯性基础。(c)，我们的双重防御，主动防御。在跟踪面部图像版权的同时，它可以在确保水印完整性的同时破坏FaceSwap模型。此外，它还提供了在攻击失败时的辅助可跟踪性。</p><h1 id="引言">引言</h1><p>​ 我们的贡献可以总结为：</p><ol type="1"><li><p>我们提出了一种新型的可追溯性对抗性水印网络，这是第一种结合了对抗性和可追溯性的针对人脸交换模型的双效应主动防御方法。它具有优异的鲁棒性、跨任务通用性和数据集泛化能力。</p></li><li><p>我们创新性地提出了OFEA方法，通过将可追溯的水印嵌入到载体的鲁棒对抗性特征中，使其具有对抗性。同时，我们通过结合一个专门设计的小波域结构信息补偿损失来解决水印多目标学习中的优化冲突。</p></li><li><p>我们专门设计了一种更合理、更全面的评估方法来充分评估对脸交换的逆性。结合传统的评估指标，我们已经证明了双重防御在三个大数据集上的源跟踪和对抗性攻击中的双重有效性。</p></li></ol><h1 id="网络">网络</h1><p>Dual Defense整体算法流程如图所示：</p><figure><img src="./../postimages/Dual-Defense/image-20240513113840540.png"alt="image-20240513113840540" /><figcaption aria-hidden="true">image-20240513113840540</figcaption></figure><p>图3。双重防御的整个管道。双防御系统通过端到端训练来优化水印模型。该过程首先将目标图像$X_t <span class="math inline">\(和用户定义的水印\)</span> W_{ID}$输入到编码器中，以生成水印图像。随后，水印图像进行FaceSwap进行原始域特征冒充攻击（OFEA），计算原始域对抗损失。受干扰的图像和水印图像都通过水印解码器进行解码器优化。</p><p>​ 。</p><h1 id="实验">实验</h1><h2 id="定量结果">定量结果</h2><p>​本文首先在CASIA-WebFace、VGGFace2和LFW三个大型人脸数据集上，从不可见性、对抗性以及可溯源性三个方面的多个指标全面评估DualDefense的性能。实验表明DualDefense在保证不可见性的同时实现了出色的对抗性以及水印恢复精度。此外，通过跨图像身份及跨数据集测试表明，DualDefense具有显著的身份通用性和数据集泛化性</p><p>Dual Defense在原始设置下的定量结果。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114244119.png"alt="image-20240513114244119" /></p><p>​此外，本文与典型的基于对抗攻击的和基于深度水印的深度伪造主动防御方法分别在原始场景和各种不同的鲁棒场景下进行了对比。实验表明，DualDefense在对抗性及可溯源性方面都几乎保持最优的性能。尤其在对图像进行后处理后，对抗攻击方法的对抗性显著降低，而DualDefense依旧保持显著的对抗性能。</p><p>Dual Defense与其他主动防御方法的比较。N/A表示该方法无对应性能。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114456325.png"alt="image-20240513114456325" /></p><p>​在真实社交网络传输信道中，图像通常经历各种后处理操作。因此，本文评估了DualDefense在四种常见图像后处理操作下对 FaceSwap的对抗性和可溯源性，实验表明当水印图像经过各种处理操作时，DualDefense始终保持出色的性能，从而验证了本文方法在实际场景中的可行性。</p><figure><img src="./../postimages/Dual-Defense/image-20240513114605619.png"alt="image-20240513114605619" /><figcaption aria-hidden="true">image-20240513114605619</figcaption></figure><p><img src="./../postimages/Dual-Defense/image-20240513114617035.png"alt="image-20240513114617035" /><imgsrc="./../postimages/Dual-Defense/image-20240513114636009.png"alt="image-20240513114636009" /></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning</title>
      <link href="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/"/>
      <url>/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Generic Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</p><p>University at Buffalo</p><p><ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-blue" alt="ICCV" /></a><ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a><ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></p><h1 id="摘要">摘要</h1><p>​随着先进的图像处理技术的出现，检测操作变得越来越重要。尽管最近基于学习的图像操作检测方法取得了成功，但它们通常需要昂贵的像素级注释来进行训练，同时在与训练图像相比被不同操作的图像上进行测试时表现出较差的性能。为了解决这些局限性，我们提出了<strong>弱监督图像篡改检测</strong>，使得训练目的只需要二进制图像级别的标签（真实或篡改）。这种弱监督设置可以利用更多的训练图像，并有可能快速适应新的操作技术。为了提高泛化能力，我们提出了弱监督自一致性学习（WSCL）来利用弱注释图像。具体来说，学习了两个一致性属性：多源一致性（MSC,multi-source consistency）和补丁间一致性（IPC, inter-patch consistency）。MSC利用不同的内容无关信息，并通过在线伪标签生成和细化过程实现跨源学习。IPC执行全局成对补丁关系推理，以发现完整的操作区域。大量实验验证了我们的WSCL，即使是弱监督的，在分布内和分布外评估下，与完全监督的WSCL相比，也表现出竞争性能，以及合理的操纵定位能力。</p><p>单流概述：</p><figure><imgsrc="../postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171025018.png"alt="image-20240510171025018" /><figcaption aria-hidden="true">image-20240510171025018</figcaption></figure><p>图2:单流概述。给定输入图像，基线方法（上图）预测操作图。预测图由基于自适应池的分类损失$ L_{A-CLS} $ 和多源一致性学习损失 $ L_{MSC} $监督。同时，补丁间一致性分支（底部）学习测量补丁相似性的一致性体积。一致性卷由补丁间一致性丢失$ L_{IPC} $ 来监督。</p><p>多流概述：</p><figure><imgsrc="../postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171253974.png"alt="image-20240510171253974" /><figcaption aria-hidden="true">image-20240510171253974</figcaption></figure><p>图3。多源一致性学习的概述。分别在RGB图像、SRM噪声图和Bayar噪声图上训练三个并行流。它们的加权平均预测被用作伪地面实况，以监督每个流。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>损失函数合集</title>
      <link href="/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉熵损失crossentropyloss">交叉熵损失CrossEntropyLoss</h1><p>交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight = imbalance_weight)</span><br><span class="line">loss = criterion(pred, tar.long().detach()) </span><br></pre></td></tr></table></figure><p>最优化的目标是找到一组模型参数 θ，使得交叉熵损失在训练数据上最小化：<span class="math display">\[\operatorname*{min}_{\theta}L_{CE}(\theta)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{i=1}^{C}y_{i}^{(n)}\log(p_{i}^{(n)}(\theta))\]</span>其中 NNN 是训练样本数。</p><h1 id="块对比损失patchcontrastloss">块对比损失PatchContrastLoss</h1><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><p>  我们首先将 $ F∈R^{256×H×W} $ 在空间上划分为k×k个块，从而得到 $f_iR^{256hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个 $ f_i $都变成了 $ R^{256} $的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到$ m_iR^{hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $ 。为了得到每个$ m_i $的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为$ m_i $ 的值。</p><p>  然后，我们有了像素嵌入 $ f_i $ 和每个嵌入 $ m_i $的相应标签。我们现在得到监督对比损失： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中， $ A_i $表示与 $ f_i $ 具有相同标签的所有其他像素嵌入 $ k^+ $ 的集合。类似地， $k^− $ 是所有与 $ f_i $ 有不同标签的负像素嵌入。损失函数中的所有嵌入都是$ L_2 $归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span></p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/CFL-Net-contrast-loss.drawio.png"alt="CFL-Net-contrast-loss.drawio" /><figcaption aria-hidden="true">CFL-Net-contrast-loss.drawio</figcaption></figure><h1 id="infonce对比损失">InfoNCE对比损失</h1><p>​  FOCAL 的训练过程如图3 (b)所示：</p><figure><img src="/postimages/FOCAL/image-20240518144850862.png"alt="image-20240518144850862" /><figcaption aria-hidden="true">image-20240518144850862</figcaption></figure><p>​  一旦我们从给定的输入X中提取高级特征F，我们就通过像素级对比学习直接监督F。地面真实伪造掩模Y自然为我们提供了正和消极类别的索引，使有效的像素级对比学习。正如很快就会更清晰的那样，焦点的对比学习以逐图像的方式进行监督，这与现有的对整个正向小批执行监督的算法[19,6,15,54,56]有很大的不同。</p><p>​  具体来说，我们采用了一种改进的InfoNCE损失[16,35]来实现焦点中的对比学习。</p><p>​  我们首先通过执行一个扁平化操作来构造一个字典 $ f( \cdot ) : {\mathbb {R}}^{ \hat{H} \times \hat{W} \times \hat{C}} \rightarrow {\mathbb{R}}^{ \hat{H} \hat{W} \times \hat{C}} $ <spanclass="math display">\[f(F) \rightarrow \{ q , k^+_1 , k^+_2 , ...,k^+_J , k^-_1 , k^-_2 , ..., k^-_K \}\]</span> ​  其中， $ { q , k^+_1 ,k^+_2 , ..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K } $被定义为字典，q是一个编码查询。我们让 $ { q , k^+_1 , k^+_2 , ..., k^+_J} $ 表示属于原始区域的特征（以Y中的0为索引），而 $ { k^-_1 , k^-_2 ,..., k^-_K } $ 表示伪造区域的特征（以Y中的1索引）。</p><p>​  在图像伪造检测任务中，伪造或原始区域通常覆盖超过1像素（特征）的区域，这意味着字典中正键J的数量也远远大于1。然后，根据图像伪造任务而定制的改进的信息损失可以计算为<span class="math display">\[{\cal {L}}_{InfoNCE++}=-log \frac { \frac 1J \sum _{ j \in [1,J] } exp(q \cdot k^+_J / \tau ) } { \sum _{ i \in[1,K] } exp(q \cdot k^+_i / \tau ) }\]</span> ​  其中， $ $是一个温度超参数[51]。注意，在原始的InfoNCE loss[16,35]中，字典中只有一个q匹配的正键。在我们改进的InfoNCE损失(2)中，我们通过取q的$ { k^+_j } $的期望，在每个损失计算中涉及所有的正键。这将促进优化过程。</p><p>​  需要强调的是，训练阶段的监督是直接在地面真实伪造掩模Y和提取的特征F之间进行的，而没有生成预测的伪造掩模。</p><p>​  此外，对于前向小批量中的每一幅图像， $ { \cal {L}} _ { InfoNCE++ }$以逐图像的方式（one-by-one）计算，而不是对整个批量进行计算，然后求和计算总体损失。更具体地说，给定一个小批特征$ {F_1、F_2、···、F_B} $ ，总体对比损失 $ { \cal {L}} _ { ct } $ : <spanclass="math display">\[{\cal {L}}_{ct}=\frac {1} {B} \sum _{b=1} ^{B}({\cal {L}}_{InfoNCE++}(F_b))\]</span>​  请注意，在上述（3）式中，没有合并小批特征来计算整体的 $ { \cal{L}}_{InfoNCE++} $，避免了训练数据的交叉图像影响。在伪造/原始像素的相对定义的指导下设计的总损失与[5,15,16,32]中的损失有很大的不同，[5,15,16,32]中的损失计算是在批处理级别进行的。</p><p>​  为了进一步证明(3)的合理性，我们在图4中绘制了传统的基于批处理和我们的逐图像图像的对比损失曲线。<br/><imgsrc="/postimages/FOCAL/image-20240518153103103.png"alt="image-20240518153103103" /></p><p>​  可以清楚地看到，损失函数（橙色线）的逐图像设计不仅使收敛速度更快，而且使优化更加稳定。特别是，在蓝线中检测到的高振幅脉冲表明相关的图像中可能存在严重的冲突，例如，类似于图2(a)和(b)的情况，其中出现冲突的标签。</p><figure><imgsrc="/postimages/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20240519102823673.png"alt="image-20240519102823673" /><figcaption aria-hidden="true">image-20240519102823673</figcaption></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAIG课代表</title>
      <link href="/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/"/>
      <url>/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>b站up主 <strong>AAIG课代表</strong></p><table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 10%" /><col style="width: 39%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th style="text-align: left;">关键词</th><th>title</th><th>description</th></tr></thead><tbody><tr class="odd"><td>1</td><td>BV1qr421u7vJ</td><td style="text-align: left;">安全大模型</td><td>揭秘阿里安全审核领域的“专家”｜集内容安全/舆情分析/代码漏洞修复为一身的AI安全大模型长什么样子？</td><td></td></tr><tr class="even"><td>2</td><td>BV1aF4m1w7RB</td><td style="text-align: left;">机器语言大模型ML</td><td>《追AI的人》第38期直播清华大学网络研究院副院长张超教授分享《机器语言大模型MLM：面向软件安全分析》</td><td></td></tr><tr class="odd"><td>3</td><td>BV1dt421j7Fy</td><td style="text-align: left;"></td><td>揭秘阿里AI的全生命期风险治理实践！</td><td></td></tr><tr class="even"><td>4</td><td>BV1DM4m1Q7Kt</td><td style="text-align: left;">内容安全</td><td>防范看不见的直播“陷阱”！解密内容安全背后的四大挑战！电商平台五大要素是？分别有哪些风险？</td><td></td></tr><tr class="odd"><td>5</td><td>BV1KJ4m157Sn</td><td style="text-align: left;">阿里安全大模型</td><td>《追AI的人》第37期阿里巴巴安全部御风大模型算法能力负责人秦鹏达分享《如何用安全大模型锻造企业“盾牌”？浅谈阿里安全大模型的实践应用！》</td><td></td></tr><tr class="even"><td>6</td><td>BV1YH4y1H7zZ</td><td style="text-align: left;">生成式AI</td><td>文字变电影?!揭秘生成式AI的五大构成要素！</td><td></td></tr><tr class="odd"><td>7</td><td>BV11x4y1m7Aa</td><td style="text-align: left;">Sora</td><td>6分钟Get生成式内容检测技术,让你拥有辨别Sora的"火眼金睛"！</td><td></td></tr><tr class="even"><td>8</td><td>BV18i421Z7vH</td><td style="text-align: left;">Sora</td><td>2分钟解密全网爆火的Sora！Sora是什么？它的主要特点是？</td><td></td></tr><tr class="odd"><td>9</td><td>BV1Wz421X7GM</td><td style="text-align: left;">Sora</td><td>《追AI的人》第36期阿里安全刘佳睿分享《解密全网爆火的Sora：如何区分真实与AI生成内容？》</td><td></td></tr><tr class="even"><td>10</td><td>BV1gU421o7e4</td><td style="text-align: left;">生成式AI</td><td>AI生成图与电影画面傻傻分不清？揭秘AI绘画成功背后的技术支持</td><td></td></tr><tr class="odd"><td>11</td><td>BV1i44y1F7o5</td><td style="text-align: left;"></td><td>《追AI的人》第35期中国社会科学院大学互联网法治研究中心执行主任刘晓春分享《人工智能发展的数据流通制度基础》</td><td></td></tr><tr class="even"><td>12</td><td>BV1sC4y1C7wk</td><td style="text-align: left;"></td><td>人工智能"上位"成功？揭秘ChatGPT成功背后的技术突破！</td><td></td></tr><tr class="odd"><td>13</td><td>BV1sw411g7Ui</td><td style="text-align: left;"></td><td>《追AI的人》第34期直播回放复旦大学张谧教授分享《当“巨兽”成为“宠物”：复旦白泽带你领略大模型安全伦理风险与治理》》</td><td></td></tr><tr class="even"><td>14</td><td>BV1Tb4y1j7iU</td><td style="text-align: left;"></td><td>生成式AI能减小人与人之间的差异？生成式AI互动机制如何能够更有效地产生结果？</td><td></td></tr><tr class="odd"><td>15</td><td>BV1Tg4y1f7gf</td><td style="text-align: left;"></td><td>《追AI的人》第33期直播回放中山大学网络空间安全学院院长操晓春教授分享《“病态的”的计算机视觉算法》</td><td></td></tr><tr class="even"><td>16</td><td>BV1yu411F7E5</td><td style="text-align: left;"></td><td>为什么生成式AI更容易取代白领员工,对体力工作者的影响却微乎其微?</td><td></td></tr><tr class="odd"><td>17</td><td>BV1fN41137zv</td><td style="text-align: left;">数字水印</td><td>《追AI的人》第32期直播回放中国科学技术大学张卫明教授分享《人工智能背景下的数字水印》</td><td></td></tr><tr class="even"><td>18</td><td>BV1Wy4y1c7Dm</td><td style="text-align: left;"></td><td>《追AI的人》第31期直播回放清华大学经济管理学院领导力与组织管理系主任李宁教授分享《人机协同、效率与创新：AI时代的组织模式探索》</td><td></td></tr><tr class="odd"><td>19</td><td>BV1Rh4y1i75x</td><td style="text-align: left;"></td><td>《追AI的人》第30期直播上海交通大学张拳石教授分享《较真地追问，神经网络是否可以被严谨地彻底地解释清楚？》</td><td></td></tr><tr class="even"><td>20</td><td>BV1XH4y1o7Nh</td><td style="text-align: left;">多模态信号融合</td><td>从生物心理学角度看多模态大模型发展史！多模态信号如何融合？</td><td></td></tr><tr class="odd"><td>21</td><td>BV1nm4y1N7Q7</td><td style="text-align: left;">安全伦理</td><td>《追AI的人》第29期直播回放复旦大学桂韬老师分享《大模型有何安全伦理风险问题？看MOSS-RLHF如何实现人类与AI的价值观对齐》</td><td></td></tr><tr class="even"><td>22</td><td>BV1M94y1s76K</td><td style="text-align: left;">深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td><td></td></tr><tr class="odd"><td>23</td><td>BV178411v7xy</td><td style="text-align: left;"></td><td>《追AI的人》第13期直播回放IEEE亚太区执委、人道主义科技活动委员会主席董晶分享《AI前沿技术对抗中的”天使”与“恶魔”》</td><td></td></tr><tr class="even"><td>24</td><td>BV1m34y1P7oY</td><td style="text-align: left;"></td><td>《追AI的人》第14期直播回放中国政法大学兼职教授、盈理律师事务所资深顾问赵军分享《《对人工智能产业发展四大要素的保护——数据与知识产权的挑战与实践》》</td><td></td></tr><tr class="odd"><td>25</td><td>BV1QF411k7LA</td><td style="text-align: left;"></td><td>3分钟Get多模态是什么？不许在脑海中想象一头粉红色的大象，你想的是什么？</td><td></td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td style="text-align: left;">图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td><td></td></tr><tr class="odd"><td>27</td><td>BV1dG411o7fn</td><td style="text-align: left;"></td><td>《追AI的人》第27期直播回放AAIG图片视觉大模型与视觉AIGC安全算法负责人洪海文分享《多模态大模型的发展与攻防</td><td></td></tr><tr class="even"><td>28</td><td>BV1DV4y1v7VS</td><td style="text-align: left;"></td><td>以ChatGPT为例3分钟解密生成式语言模型的训练过程！</td><td></td></tr><tr class="odd"><td>29</td><td>BV1nu4y1S7eX</td><td style="text-align: left;"></td><td>教你分清生成式AI、大模型、AIGC！Bert、T5、ChatGPT三者有什么区别？</td><td></td></tr><tr class="even"><td>30</td><td>BV1qP411C7KT</td><td style="text-align: left;"></td><td>“吃“书狂魔是怎么炼成的？解密ChatGPT数据集之谜！</td><td></td></tr><tr class="odd"><td>31</td><td>BV14j411S7ys</td><td style="text-align: left;"></td><td>从感知机到大模型,3分钟读懂AIGC的前世今生！</td><td></td></tr><tr class="even"><td>32</td><td>BV1na4y1A7LK</td><td style="text-align: left;"></td><td>《追AI的人》第26期直播回放阿里巴巴人工智能治理与可持续发展研究中心AAIG人工智能安全实验室主任张荣分享《我们给生成式模型做一个体检》》</td><td></td></tr><tr class="odd"><td>33</td><td>BV1Vo4y1N7Lg</td><td style="text-align: left;"></td><td>《追AI的人》第25期直播回放橙盾科技-塔玑虚拟模特产品算法负责人郎一宁分享《从灵魂画师到光速写手，挖一挖大模型背后的知识结构》</td><td></td></tr><tr class="even"><td>34</td><td>BV1oc411G7qb</td><td style="text-align: left;"></td><td>震惊!白领职业将被替代?如何和AI合作共赢?工作的毁灭和创造的时代来了!</td><td></td></tr><tr class="odd"><td>35</td><td>BV1Da4y1u7bg</td><td style="text-align: left;"></td><td>ChatGPT来抢饭碗了!盘点20个最有可能被取代的职业!</td><td></td></tr><tr class="even"><td>36</td><td>BV1cz4y1a7or</td><td style="text-align: left;"></td><td>《追AI的人》第24期直播回放北京大学国家发展研究院助理教授胡佳胤分享《生成式AI的时代，我们的工作会变成什么样子？》</td><td></td></tr><tr class="odd"><td>37</td><td>BV1bz4y1Y7ss</td><td style="text-align: left;"></td><td>用户、网店、骑手、监管部门如何看待算法?算法是否能完全透明?</td><td></td></tr><tr class="even"><td>38</td><td>BV12c411n7Q1</td><td style="text-align: left;"></td><td>大数据杀熟现象:商业惯例还是不道德行为？如何定义？</td><td></td></tr><tr class="odd"><td>39</td><td>BV15g4y1T7ey</td><td style="text-align: left;"></td><td>什么是三近一反原则?流量为王的时代,教你如何“破圈”创作!</td><td></td></tr><tr class="even"><td>40</td><td>BV1PN411A7FF</td><td style="text-align: left;"></td><td>《追AI的人》第23期直播回放中国广告协会法律与道德工作委员会常务委员杜东为分享《算法治理拉开帷幕，知识需不断碰撞融合》</td><td></td></tr><tr class="odd"><td>41</td><td>BV1sv4y1V7zN</td><td style="text-align: left;"></td><td>《追AI的人》第22期直播回放清华大学副教授眭亚楠老师分享《AI助力瘫痪患者恢复站立和行走》</td><td></td></tr><tr class="even"><td>42</td><td>BV1qg4y1t7xR</td><td style="text-align: left;"></td><td>《追AI的人》第21期直播回放阿里巴巴人工智能治理与可持续发展研究中心算法专家许皓天分享《ChatGPT的前世今生与风险治理》</td><td></td></tr><tr class="odd"><td>43</td><td>BV1yY4y1y77L</td><td style="text-align: left;"></td><td>这份安全收快递指南请收好！这份安全快递指南请收好！如何减少隐私泄漏的风险？如何自我保护？</td><td></td></tr><tr class="even"><td>44</td><td>BV1g54y1c7i3</td><td style="text-align: left;"></td><td>《追AI的人》第20期直播回放南开大学陈兵老师分享《以友好型算法治理为中心，推进可信人工智能健康发展》</td><td></td></tr><tr class="odd"><td>45</td><td>BV1mP4y1k7aM</td><td style="text-align: left;"></td><td>《追AI的人》直播第18期对外经贸许可老师分享《中国民众如何看待算法：经验与规范》</td><td>我们已然生活在“算法社会”中，无处不在的算法无时不刻不在改变着我们的生活和工作。为了更好地了解我国民众对算法应用真实的感受和评价，完善我国算法治理，我们开展了“算法应用用户感知”大规模实证调查，并且由此得出了富含意蕴的启示。</td></tr><tr class="even"><td>46</td><td>BV1p3411d7eC</td><td style="text-align: left;"></td><td>《追AI的人》直播第19期清华梁正老师分享《人工智能治理亟待标准落地》</td><td>过去两年来，世界范围内人工智能治理加速推进，欧盟、美国、英国相继出台了相关法律、政策与战略文件，有专家总结人工智能领域的“治理竞争”已经开始。国内来看，随着数安法、个保法、以及跨境数据流动、数据治理、算法治理、伦理治理等领域重要法规和文件的颁布实施，我国在数字经济乃至人工智能领域的治理架构已基本成型。当前人工智能治理面临的一个突出矛盾是如何将相关合规要求落实为可以具体操作实施的举措，而在这方面，作为治理基本指引和行业最佳实践的标准恰恰可以发挥重要的作用。标准作为“软法”在人工智能治理中如何发挥作用？国</td></tr><tr class="odd"><td>47</td><td>BV1Bd4y1572B</td><td style="text-align: left;"></td><td>大数据是如何精准猜透你的心（下）如何减少推荐用户不喜欢的内容？如何增加推荐系统的多样性？</td><td></td></tr><tr class="even"><td>48</td><td>BV1EG4y1L7eg</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（下）</td><td></td></tr><tr class="odd"><td>49</td><td>BV1qe4y1V7NV</td><td style="text-align: left;"></td><td>信息茧房和马太效应是什么?大数据如何精准猜透你的心?</td><td>在日常生活中，你有没有遇到过这种情况：只要你点赞或收藏了某个短视频，就会持续收到同类型的内容？不仅仅是短视频，还有听音乐的每日推荐，是不是也都是你经常听的类型或者歌手？其实，这些都属于个性化推荐！</td></tr><tr class="even"><td>50</td><td>BV1g44y1R7bK</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（中）</td><td></td></tr><tr class="odd"><td>51</td><td>BV1uD4y1V7hF</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享(上)</td><td></td></tr><tr class="even"><td>52</td><td>BV16P411u7ea</td><td style="text-align: left;"></td><td>《追AI的人》第16期直播重庆邮电大学校长高新波老师分享《人工智能的未来发展趋势分析》</td><td>近年来，人工智能获得了迅猛的发展，面向特定领域取得了诸多单点突破。但是，从本质上看今天人工智能的成功更多来自“勤能补拙”，距离真正的智慧还有较大的距离。为了使人工智能聪明更可靠。《追AI的人》第16期直播首先介绍了目前人工智能发展所遇到的瓶颈问题，然后提出了未来人工智能发展的六个方向，即绿色低碳更灵巧的人工智能、知识数据双驱动的人工智能、人机物融合的混合人工智能、可信可靠可解释的人工智能、非深度神经网络的人工智能、开放环境自适应的人工智能。最后，一起展望未来人工智能的发展前景。</td></tr><tr class="odd"><td>53</td><td>BV1wW4y1W764</td><td style="text-align: left;"></td><td>什么是大数据杀熟？揭秘“杀熟”误区！为什么老用户看到的价格比新用户更高？</td><td>什么是大数据杀熟？揭秘“杀熟”误区！《如何获取消费者对电商平台价格和用户权益的信任》为什么老用户看到的价格比新用户更高？为什么不少用户会产生“被杀熟”的想法？阿里针对不同用户，可能在获得的优惠结果上不一致的情况下，如何实施遵循原则以保障用户的权益公平性</td></tr><tr class="even"><td>54</td><td>BV1Pv4y1D7Za</td><td style="text-align: left;"></td><td>《追AI的人》直播第15期AAIG自然语言理解实验室EMNLP专场《机器=冰冷？看机器如何捕捉你的小情绪》《文本如药，如何精确提炼“有效成分”？》》</td><td>Part 1: 《机器=冰冷?看机器如何捕捉你的小情绪》人工智能领域日新月异，人们已经不满足于让机器完成指定的任务，从如今大火的各类语音助手和数字人可以看出，“拟人化”是人工智能应用的重大需求。为了让人机对话过程更加丝滑，理解用户的情绪是关键一步。计算机该如何理解我们的情绪呢？构建有人情味的人工智能应用有哪些困难？第一部分将简要对此进行介绍。1、对话中的情感分析问题的定义、应用和当前的困难2、基于有监督原型对比学习的对话情感分析方法3、对话情感分析的未来发展方向 Part 2: 《文本如药,如何</td></tr><tr class="odd"><td>55</td><td>BV1WP411E7Xk</td><td style="text-align: left;"></td><td>如何识别美颜照片的真实性？眼见一定为实吗？虚拟idol也是深度合成吗？什么是深度合成呢？阿里针对深度合成做了哪</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>56</td><td>BV1ae411u7cY</td><td style="text-align: left;"></td><td>算法透明=公开源代码？源代码开放后会带来什么影响和后果呢？为什么算法透明是算法治理的核心要求？</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="odd"><td>57</td><td>BV1Zd4y1d7aa</td><td style="text-align: left;"></td><td>科幻小说中的人类为什么害怕人工智能？如何构建人与人工智能的伦理关系?</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>58</td><td>BV1SU4y1v7VL</td><td style="text-align: left;"></td><td>人工智能的生成物能否获得版权的保护？获得著作权保护需要满足哪些条件呢?|《追AI的人》1分钟AI科普短视频</td><td>《追AI的人》系列短视频是一档由阿里巴巴人工智能治理与可持续发展研究中心(AAIG)联合高校和产业界发起的AI治理交互栏目《追AI的人》的衍生短视频。用浅显易懂的词汇把人工智能新技术、AI治理新观点、可持续发展新风向在短短1分钟左右传达给大家，让AI与我们的生活离得更近。</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位视频合集</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 13%" /><col style="width: 37%" /><col style="width: 37%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>题目</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>30</td><td>BV1xN411Z7Tt</td><td>智能信息伪装</td><td>【图图Seminar30】张卫明：智能信息伪装</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”前沿论坛报告人：中国科学技术大学 张卫明</td></tr><tr class="even"><td>30</td><td>BV1j64y1y7XS</td><td>神经网络水印</td><td>【图图Seminar30】张新鹏：神经网络水印</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”论坛报告人：复旦大学 张新鹏</td></tr><tr class="odd"><td>76</td><td>BV18Y4y1r7tb</td><td>视频身份识别技术</td><td>【图图Seminar76】“视频身份识别技术”主编论坛——图图名师讲堂</td><td>论坛致辞：中山大学 赖剑煌 报告专家：厦门大学 纪荣嵘 清华大学 王生进南方科技大学 于仕琪 国防科技大学 蓝龙 武汉科技大学 王晓论坛主持：山东科技大学 张鹏</td></tr><tr class="even"><td>77</td><td>BV1Kr4y147rU</td><td>智能图像安全</td><td>【图图Seminar77】“智能图像安全”主编论坛——图图名师讲堂</td><td>论坛致辞：中国科学技术大学 张卫明 报告专家：华南理工大学 胡永健上海理工大学 秦川 中国科学技术大学 储琪 论坛主持：复旦大学 钱振兴</td></tr><tr class="odd"><td>125</td><td>BV1ry4y1F7vX</td><td>视频身份识别</td><td>【图图Seminar125】视频身份识别前沿论坛——图图专刊优秀成果分享会</td><td>主持人：于仕琪 副教授（南方科技大学） 报告人：王生进教授（清华大学）；马丙鹏 教授（中国科学院大学）；张权博士生（中山大学）；许文正 硕士生（山东大学）</td></tr><tr class="even"><td>131</td><td>BV14w41187rS</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar131】数字媒体深度伪造与对抗论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共11位报告人，论坛主席：卢伟 教授、钱振兴 教授，评审专家：李晓龙 教授、张卫明 教授、王员根 教授、秦川 教授。</td></tr><tr class="odd"><td>148</td><td>BV1Bt421A7gP</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar148】“数字媒体深度伪造与对抗”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：生成式人工智能与可证明安全隐写 报告人：陈可江中国科学技术大学特任副研究员报告二：联合多重对抗与通道注意力的高安全性图像隐写 报告人：马宾齐鲁工业大学教授 报告三：基于语义解耦的AI生成图像取证技术 报告人：丁峰南昌大学校聘教授</td></tr></tbody></table><table><colgroup><col style="width: 4%" /><col style="width: 13%" /><col style="width: 17%" /><col style="width: 65%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>title</th></tr></thead><tbody><tr class="odd"><td>22</td><td>BV1M94y1s76K</td><td>深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td>图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards Effective Image Manipulation Detection with Proposal Contrastive Learning</title>
      <link href="/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/"/>
      <url>/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Effective Image Manipulation Detection with ProposalContrastive Learning</p><h1 id="摘要">摘要：</h1><p>​深度模型在图像操作检测中得到了广泛而成功的应用，旨在对篡改图像进行分类和定位篡改区域。现有的方法大多集中于从被篡改图像中提取全局特征，而忽略了单个被篡改图像中被篡改区域与真实区域之间的局部特征的关系。为了利用这种空间关系，我们提出了建议对比学习（PCL）来进行有效的图像操作检测。我们的PCL由一个双流架构组成，通过分别从RGB和噪声视图中提取两种类型的全局特征。为了进一步提高鉴别能力，我们通过吸引/排斥基于命题的代理命题对比学习任务来利用局部特征的关系。此外，我们还证明了我们的PCL在实践中可以很容易地适应未标记的数据，这可以降低人工标记的成本，并促进更一般化的特性。在几个标准数据集中进行的大量实验表明，我们的PCL可以作为一个通用的模块来获得一致的改进。</p><figure><imgsrc="../postimages/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/image-20250107105329585.png"alt="image-20250107105329585" /><figcaption aria-hidden="true">image-20250107105329585</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CatmullRom Splines-Based Regression for Image Forgery Localization</title>
      <link href="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/"/>
      <url>/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/</url>
      
        <content type="html"><![CDATA[<center>CatmullRom Splines-Based Regression for Image Forgery Localization<ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\textbf{Li Zhang}^{1,2},\textbf{MingliangXu}^{2},\textbf{Dong Li}^{2},\textbf{JianmingDu}^{1,\dagger},\textbf{Rujing Wang}^{1,2\dagger}\)</span></center><center>中国科学院合肥物理科学研究所、中国科技大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/28548.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>  IFL（Image ForgeryLocation）有助于确保数字媒体取证的安全。然而，许多方法存在错误的检测（即FPs）和不准确的边界。在本文中，我们提出了基于CatmullRom样条的回归网络（CSR-Net,CatmullRom Splines-based RegressionNetwork），它从回归的角度重新考虑IFL任务来处理这个问题。</p><p>  具体来说，我们提出了一种自适应的CutmullRom样条变换方案，用于被篡改区域的粗定位。然后，对于假阳性例子，我们开发了一种新的重新评分机制，旨在筛选出不能在分类分支和实例分支上都有响应的样本。随后，为了进一步限制边界，我们设计了一个可学习的纹理提取模块，该模块通过解耦水平和垂直伪造特征来参考和增强轮廓表示，从而提取出更鲁棒的轮廓表示，从而抑制FPs。与基于分割的方法相比，由于不需要后处理，我们的方法简单而有效。大量的实验表明，CSR-Net优于现有的先进方法，不仅在标准的自然图像数据集上，而且在社交媒体数据集上。</p><h1 id="引言">引言</h1><p>  第一个问题是<strong>假阳性（FPs）</strong>。假阳性是指测试结果表明存在一个令人满意的目标区域，而实际上它并不令人信服。传统的基于分割的方法往往会出现这种情况（如图2所示）。</p><p><img src=" ../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503164842397.png" alt="image-20240503164842397 " style="zoom:40%;" /></p><p>  二值化是这些方法中不可或缺的决定性策略，它是一种直接决定前景区域块数量的阈值敏感任务。在传统的分割方法中，一个不合理的阈值往往会导致出现意外的区域（即假阳性的情况）。然而，许多方法在关注潜在的被篡改区域时，通常会忽略误报率。这对数字内容的传播有负面影响，影响了相关新闻来源的可获得性，这限制了分析结果向更有令人信服的方向发展。</p><p>  第二个问题是<strong>不准确的边界</strong>。传统的基于分割的方法存在连续解码器层之间的掩模预测不一致，导致优化目标不一致和特征空间的弱耦合（如图1a所示）。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503165143372.png" alt="image-20240503165143372" style="zoom:67%;" /></p><p>  另一方面，当直接引入一般回归方法来处理任务时，定位效果也不令人满意，因为所使用的边界盒只能以四边形的方式定位目标区域，而且通常目标区域大多出现在不规则曲线中（图1(b)显示了旋转检测的定位效果（Li et al.2022），其中我们使用掩蔽区域的最小边界四边形作为GroundTruth）。越来越复杂的被篡改图像提出了更大的挑战，因为大多数方法不能很好地约束或明确地建模伪造的区域边界，这很容易导致在检测结果中混合其他目标或不兼容的背景。</p><p>  最近，一些基于回归的策略在目标检测领域的假阳性判定方面取得了显著进展(You et al. 2022; Li and Kosecka 2022; Chen et al.2023)。与目标检测任务不同，IFL是一个像素级的任务，这意味着方法迁移的直接性将带来性能下降。为此，需要引入一些定制的方法和处理方法，可以有效地连接这两个任务。</p><p>  首先，对于标记为GT的掩模，我们引入了CatmullRom样条曲线来将其转换为多边形帧，从而使回归策略能够应用于像素级的任务，如IFL。同时，在训练和推理过程中，为了使多边形标记更接近真实标记，提出了自适应参数CatmullRom样条方法，该方法可以最小化预测区域与地面真实值之间的相似性差距和目标区域的曲率。其次，为了进一步、明确地抑制定位结果中的假阳性，我们提出了一种有效的重评分机制：我们通过两个独立的预测分支，直接拒绝在两个分支中都没有接收到响应的假阳性，每个预测分支都有一个区域分类得分和一个实例得分。此外，为了得到更准确的边界，我们通过解耦水平纹理特征和垂直纹理特征，进一步细化预测区域的轮廓，以建模锻造区域边界，减少它们与其他掩模之间的重叠。</p><p>  我们的贡献可以概括为三类：</p><p>  1)我们制作了一个基于CatmullRom样条的回归网络（CSR-Net,<strong>C</strong>atmullRom <strong>S</strong>plines-based<strong>R</strong>egressionNetwork），首次尝试将回归方法引入像素级任务。</p><p>  2)为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><p>  3)在多个公共数据集（包括自然图像数据集和社交媒体数据集）上进行的大量实验表明，我们的方法与IFL中最先进的方法相比具有优越性。</p><h1 id="方法">方法</h1><h2 id="概括">概括</h2><p>  图3是对我们的框架的概述。输入图像表示为 $ X R^{H×W×3} $。首先，我们使用嵌入ResNet-50的FPN作为骨干网络进行Catmull样条检测。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>  更具体的是，我们的方法利用一种基于CatmullRom样条的参数化方法来自适应地识别目标分割区域（橙色部分）。跟随（Chenet al. 2017），我们采用空间空间金字塔池（ASPP, atrous spatial pyramidpooling）和ResNet-50来捕获长期上下文信息和多尺度特征。<br/><br/>  这种无锚定的卷积神经网络大大简化了我们的任务的检测，也允许我们获得一个粗糙的特征图。稍后，我们使用一个重新评分机制（CRA,re-scoringmechanism）来筛选出在粗特征图（蓝色部分）上突出显示的可疑区域的假阳性样本。最后，我们同时从水平和垂直方向的区域进行纹理提取（byVTP），以预期获得更准确的边界（绿色部分）。请注意，保留的每个篡改区域将由VTP独立处理。</p><h2 id="catmullrom样条检测">CatmullRom样条检测</h2><p>  大多数传统方法在IFL中使用基于分割的思想。<br/><br/>  然而，在处理此类面具或基于多边形的数据集时，基于回归的方法往往是一种更有效的方法，例如（Liu等2019；Pranav，正刚等2020；Zhang等2022）。<br/><br/>  一般来说，主流的基于回归的方法需要复杂的处理来适应实例的边界，这导致了实践中的不可靠和不稳定性。近年来，样条曲线被用于计算机图形应用中生成各种形状的曲线。例如，自动驾驶车道线(Ma等2019年；余和陈2017年）、文本检测（刘等2020年；唐等2022年；阮等2021年）、故障检测(Park等2011年；郭和王2005年）等。其中，CatmullRom样条函数是一种经典的插值样条，由于其变换效应和推理代价，适用于被篡改区域的参数化（钱德拉2020；Li2022）。<br/><br/>  具体地说，CatmullRom样条是一组三次插值样条，因此每个点上的切线使用样条上的上一个点和下一个点计算。在给定的控制点下，卡特穆尔罗姆样条函数可以适应任何形状（李和陈2016；李，刘，刘2022）。此外，构造三次小矩阵样条函数只涉及整数系数，与其他样条函数相比，它降低了实现成本。上面提到的所有这些属性都有助于提高更快的推理速度和更低的计算消耗（Flops）。<br/><br/>  数学上，数学样条被定义为等式1: <spanclass="math display">\[c_i(t)=\sum_{j=0}^3b_j(t)\boldsymbol{p}_{i+j},\quadi=0,1,\ldots,n-3\]</span></p><p>  其中，0≤t≤1，<spanclass="math inline">\(p_{i}(i=0,1,\ldots,n-3;n\geq3)\)</span>为控制点，<spanclass="math inline">\(b_{j}(t)\)</span>为基。例如，它可以用等式2来表示当函数<spanclass="math inline">\(b_{j}(t)\)</span>中t的最大幂为3时：</p><p><span class="math display">\[c_i(t)=\frac{1}{2}\cdot[1\quad t\quadt^2\quadt^3]\cdot\begin{bmatrix}0&amp;2&amp;0&amp;0\\-\tau&amp;0&amp;\tau&amp;0\\2\tau&amp;\tau-6&amp;-2(\tau-3)&amp;-\tau\\-\tau&amp;4-\tau&amp;\tau-4&amp;\tau\end{bmatrix}\cdot\begin{bmatrix}p_i\\p_{i+1}\\p_{i+2}\\p_{i+3}\end{bmatrix}\]</span></p><p>  为了协调被篡改区域的任意形状与CatmullRom样条曲线，我们从现有的数据集和真实的图像中深入研究了定向或弯曲的被篡改区域。在卡氏样条曲线中，<spanclass="math inline">\(\tau\)</span>（张力因子）是控制样条线紧性的一个重要参数。张力因子的值越高，曲线在控制点之间弯曲越紧密，从而在移动过程中更接近给定的数据点。相反，较低的张力系数值会使控制点之间的曲线更平滑。直观地看，传统的卡莫样条（参数<spanclass="math inline">\(\tau=1\)</span>）对IFL任务很差，因此我们试图通过调整τ在匹配精度和曲线平滑度之间找到正确的平衡。消融实验（在消融分析部分）表明，当<spanclass="math inline">\(\tau\)</span>设置为16时，CatmullRom样条曲线对该任务是可靠的。它还允许学习到的控制点更接近前景（篡改）区域。</p><h2 id="catmullrom-ground-truth-生成"><strong>CatmullRom Ground Truth生成</strong></h2><p>  在IFL中，许多基准测试使用基于掩码或多边形的数据集作为公共数据集(Dong, Wang, and Tan 2013; Hsu and Chang 2006; Alibaba2021/2022)。给定曲线边界的注释点 $ { p_i } _{i=1} ^n $ ，其中 $ p_i $表示第 $ i $ 个注释点，主要目标是根据等式一获得CatmullRom样条 $ c(t) $的最优参数。为了实现这一点，我们可以简单地应用标准最小二乘法，如等式3所示 :</p><p><spanclass="math display">\[\begin{bmatrix}p_{03t_0}&amp;\cdots&amp;p_{33t_0}\\p_{03t_1}&amp;\cdots&amp;p_{33t_1}\\\vdots&amp;\ddots&amp;\vdots\\p_{03t_m}&amp;\cdots&amp;p_{33t_m}\end{bmatrix}\begin{bmatrix}c_{x_0}&amp;c_{y_0}\\c_{x_1}&amp;c_{y_1}\\c_{x_2}&amp;c_{y_2}\\c_{x_3}&amp;c_{y_3}\end{bmatrix}=\begin{bmatrix}\mathscr{P_{x_0}}&amp;\mathscr{P_{y_0}}\\\mathscr{P_{x_1}}&amp;\mathscr{P_{y_1}}\\\mathscr{P_{x_2}}&amp;\mathscr{P_{y_2}}\\\mathscr{P_{x_3}}&amp;\mathscr{P_{y_3}}\end{bmatrix}\]</span></p><p>  其中，m表示一个曲线边界的标注点的数量。而t是通过使用累积长度与多段线的周长的比值来计算的。$ p_{ij} $ 可以从等式中引用1，我们使用 $ _{i} $表示变换后的新坐标点。根据等式1和等式3，我们将原始的掩码注释转换为一个参数化的CatmullRom样条曲线。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240505171016728.png" alt="image-20240505171016728" style="zoom:50%;" /></p><p>图4：立方CatmullRom样条曲线的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。</p><h2 id="综合再评分算法cra">综合再评分算法CRA</h2><p>  MaskR-CNN背后的基本机制是将所得到的边界框的分类一致性视为分数，然后使用一个预先确定的阈值来筛选出背景框。然而，尽管有了一些进展，当边界框包含一个明显不兼容的区域实例时，它伴随着大量的背景信息，并且MaskRCNN经常显示出如此低分数的真阳性，而它保留了一些相对较高可信度的FPs。因此，我们为每个区域实例重新分配分数。具体来说，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。数学上，给定预测的n类分数$~ \mathrm{CLS}=\{s_{ij}^{cls}\mid j\in[0,\cdots,n-1]\} $ 和 $~\mathrm{INS}=\{s_{ij}^{ins}\mid j\in[0,\cdots,n-1]\} $通过等式4计算第i个提出建议的综合得分： <spanclass="math display">\[s_{ij}=\frac{e^{s_{ij}^{cls}+s_{ij}^{ins}}}{\sum_{l=0}^{n-1}e^{s_{il}^{cls}+s_{il}^{ins}}}\]</span>  在我们的工作中，我们采用了n =2，其中这两个类再现了图4：三次组合样条的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。发送篡改（前景）和真实（背景）区域。因此，我们只需要计算前景类的分数。<br/><br/>  CLS由一个类似于MaskR-CNN的分类分支直接获得，INS是区域实例在全局区域分割图上的激活值。具体来说，它被投影到每个区域实例的篡改区域分割地图上，包含<spanclass="math inline">\(P_{i} = \{p_{i}^{1},p_{i}^{2}\ldots p_{i}^{n}\}\)</span>，并且区域实例区域中Pi的平均值可以表示为： <spanclass="math display">\[s_{i1}^{ins}=\frac{\sum_jp_i^j}N\]</span>  其中，Pi是区域分割地图上第i个区域实例的像素值的集合。将分类得分与实例得分有机结合，得到综合得分，在实践中可以降低FP的可信度。这是因为FPs往往比分割图上的区域有更弱的响应。<br/><br/>  下面的实验结果表明，我们的设计对图片剪切情况更友好，因为剪接情况通常在分割图上享有更强的响应，较高的实例分数将弥补较低的分类分数。</p><h2 id="垂直纹理-交互式感知vtp">垂直纹理-交互式感知VTP</h2><p>  传统的边缘检测操作符（例如，索贝尔、罗伯茨、普雷威特等）有助于提取自然图像处理任务中手工制作的特征，而最大的缺点是它们不能根据任务的特殊性进行动态学习。受（Holla和Lee2022）的启发，我们在一个被称为Sobel层的可学习模块中采用了一个边缘检测算子，见图5。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>图5：Sobel层图，用于VTP，用于增强边缘相关模式和操作边缘检测。来自第i个块首先通过Sobel单元（SU），然后通过一个边缘剩余单元（ERU）。为了进行训练和优化的原因，引入了一种残差学习策略。</p><p>  此外，为了更好地建模被篡改的区域边界，我们在我们的网络中引入了垂直纹理交互式感知（VTP,Vertical Texture interactivePerception）。在VTP中，被篡改的区域用一组等高线点表示，这些包含强纹理特征的点可以精确地定位具有任意形状的被篡改区域。<br/><br/>  看到它们：有两个核心并行分支在VTP分支，在顶部，我们引入一个卷积内核大小1×k滑动功能地图模型局部纹理信息在水平方向，只关注k-range地区的纹理特征。这个巧妙的技巧通过我们的预实验证明是简单且有效的。此外，它几乎是免资源开销的同时保持竞争效率。通过类似的范例，将底部分支通过大小为k×1的卷积核在垂直方向上对纹理特征进行建模。k是控制纹理特征接受场大小的超参数。在实际实验中，我们取k=3。最后，涉及两个独立的s型层，将两个方向上的热图归一化为[0,1]。这样，就可以在两个正交方向上检测到篡改区域，并在两个不同的热图中用等高线点表示，其中任何一个热图都只响应特定方向上的纹理特征。<br/><br/>  由于在两个正交方向上考虑响应值可以有效地抑制假阳性预测，因此通过点重评分算法进一步处理来自VTP的两个热图。具体地说，通过NMS对不同热图中的点进行直接处理，以实现紧密的表示。然后，为了抑制具有强单向或弱正交响应的预测，我们只选择在两个热图中具有不同响应的点作为候选点。最后，篡改区域可以用由这些高质量轮廓点组成的多边形表示。</p><h2 id="最优化">最优化</h2><p>  如上所述，我们的网络包括多任务任务。因此，我们计算了以下组件的损失函数：<span class="math display">\[L=L_{rpn}+\lambda_{1}\cdotL_{cls}+\lambda_{2}\cdot L_{mask}+\lambda_{3}\cdotL_{qts}+\lambda_{4}\cdot L_{CR}\]</span>  其中，Lrpn、Lcls和Lmask是来自MaskR-CNN的标准损失。Lgts用于优化篡改区域检测，定义为： <spanclass="math display">\[L_{gts}=\frac{1}{N}\sum_i-\log\left(\frac{e^{p_i}}{\sum_je^{p_j}}\right)\]</span>  Lgts是Softmax损失，其中p是网络的输出预测。<br/><br/>  LCR用于优化CatmullRom样条检测的ft，定义为：<span class="math display">\[L_{CR}=L_{ctr}+L_{bias}\]</span>  Lctr和Lbias均为FCOS损失（Tian等人，2019年）。前者用于优化从CatmullRom控制点中心的距离损失，而这些控制点到中心的偏移距离受后者的限制。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><h3 id="预训练数据"><strong>预训练数据</strong></h3><p>我们创建了一个规模可观的图像篡改数据集，并使用它来预训练我们的模型。<br/>该数据集包括三类：1）拼接，2）复制移动，和3）删除。</p><h3 id="测试数据集">测试数据集</h3><p>如下（Wang等2022；胡等2020），我们在 CASIA (Dong, Wang, and Tan2013), Columbia (Hsu and Chang 2006), NIST16(Guan et al. 2019), COVER(Wen et al. 2016)上评估我们的模型。</p><h3 id="评价指标">评价指标</h3><p>为了量化定位性能，根据之前的工作（Hu et al.2020），我们在操作掩模上使用像素级的曲线下面积（AUC）和F1分数。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h3 id="实施细节">实施细节</h3><p>输入图像的大小被调整为512×512。在这项工作中，骨干网络是ResNet-50，在ImageNet上进行了预训练。由PyTorch实现，我们的模型使用GeForce GTX3090进行训练，使用Adam作为优化器。</p><h2 id="与sota方法的比较">与SOTA方法的比较</h2><p>遵循经典方法（Hu et al. 2020；Wang et al.2022），我们的模型与其他最先进的篡改定位方法在两种设置下进行了比较：1)训练合成数据集和评估完整的测试数据集，2)调整预训练模型对测试数据集的训练分割和评估它们的测试分割。预先训练的模型将演示每种方法的通用性，fne调整模型将演示一旦域差异显著减少，每种方法在局部的表现如何。</p><h3 id="预训练模型">预训练模型</h3><p>表1显示了不同SOTA方法的预训练模型在像素级AUC下的fve数据集上的定位性能。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823163003537.png"alt="image-20240823163003537" /><figcaption aria-hidden="true">image-20240823163003537</figcaption></figure><p>我们的CSR-Net在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia排名第二。特别是在复制-移动数据集(COVER)上达到了94.4%，其图像伪造区域与背景难以区分。这验证了我们的模型具有抑制FPs和生成更准确的边缘的优越能力。然而，我们未能在Columbia上取得最好的表现，AUC得分比PSCCNet低1.4%。我们推测，原因可能是他们（PSCCNet）合成的训练数据的分布与Columbia数据集非常相似。表2中的结果进一步支持了这一点，这说明CSR-Net在AUC和F1得分上都优于PSCCNet。此外，值得指出的是，我们在较少的预训练数据下取得了不错的结果。</p><h3 id="微调模型">微调模型</h3><p>预训练模型的网络权值用于启动调优模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练部分上进行训练。我们在表2中评估了不同方法的微调模型。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210155557.png"alt="image-20240823210155557" /><figcaption aria-hidden="true">image-20240823210155557</figcaption></figure><p>对于AUC和F1，我们的模型取得了显著的性能提高。这表明CRA模块有效地抑制了假阳性错误实例，提高了VTP预测区域位置和边界的准确性。</p><p>在综合表1和表2中的数据后，我们的方法证明了为像素级任务引入回归方法是有效的，这是在引言中提到的。</p><h3 id="消融分析">消融分析</h3><p>  在本节中，我们将进行实验来证明我们所提出的CSR-Net方法的有效性。与传统的回归方法相比，正式引入了基于CatmullRom样条的回归（CSR）来更好地描述被篡改的区域。综合评分算法（CRA）的目标是选择分类得分高、实例得分高的期望区域，而垂直纹理交互感知（VTP）则采用水平和垂直两种方式对纹理特征进行建模，以参考目标区域。为了进一步评估CSR、CRA和VTP的有效性，我们分别删除它们，并验证它们在CASIA和NIST16数据集上的伪造定位性能。表3显示定量结果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210814928.png"alt="image-20240823210814928" /><figcaption aria-hidden="true">image-20240823210814928</figcaption></figure><p>  基线(I)表示我们只使用传统的回归方法（Li et al.2022）。在接下来的消融实验中，我们可以推断，当不涉及VTP时，CASIA的F1评分下降了1.9%，NIST16的评分下降了1.7%。而在没有CRA时，AUC评分比（IV）下降更多。然而，当CRA不可用时，在（II）中可以观察到明显的性能下降，即AUC为12.3%，F1为11.2%。<br/><br/>  在图7中，我们展示了CatmullRomGround Truth生成中参数<spanclass="math inline">\(\tau\)</span>的不同值，以验证对自然图像数据集（即CASIA）和社交媒体数据集（即RIFL21）的各自预测效果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211034561.png"alt="image-20240823211034561" /><figcaption aria-hidden="true">image-20240823211034561</figcaption></figure><p>  直观地看，随着<spanclass="math inline">\(\tau\)</span>的逐渐增加，不在不同数据集中，拟合的CatmullRom控制点与具有掩模水平的地面真值之间的欧几里德距离逐渐减小，显示出更好的拟合。然而，当<spanclass="math inline">\(\tau\)</span>超过16时，欧氏距离反而呈现出扩张的趋势，这意味着拟合效果可能会减少。显然，<spanclass="math inline">\(\tau =16\)</span>是生成基于CatmullRom的最佳GroundTruth的最好选择。</p><h2 id="可视化结果">可视化结果</h2><h3 id="定性结果">定性结果</h3><p>  我们在图6中提供了不同方法的预测伪造掩模。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211647860.png"alt="image-20240823211647860" /><figcaption aria-hidden="true">image-20240823211647860</figcaption></figure><p>图6：通过不同的方法可视化预测的操作掩模。从左到右，我们展示了伪造的图像，对ManTra-Net、SPAN、PSCCNet、TruFor、我们的预测和GT掩模。</p><p>  由于 ObjectFormer（Wang et al.2022）的源代码不可用，因此他们的预测不可用。与最先进的方法相比，我们的CSR-Net在抑制假阳性方面取得了更好的性能，无论是在抑制还是在更准确的篡改区域边界方面。我们有理由相信，改善受益于CRA和VTP。CRA能够更全面地考虑每个可能的区域，确定被篡改区域和真实区域之间的细微差别，而VTP同时通过两种正交方法建模纹理边界，以准确描述目标区域。</p><h3 id="不同的基于样条曲线的回归">不同的基于样条曲线的回归</h3><p>  有许多类型的插值函数，经典的CatmullRom样条和贝塞尔曲线，前者是一个插值样条函数，精确插值一组已知数据点通过使用一系列的节点，而后者是一个近似样条函数，近似一组数据点使用节点。IFL的数据集来自自然图像和社交媒体，被篡改的区域共享不同的形状。通过比较实验，我们发现CutmullRom样条更适合于具有不同曲率的数据集（如IFL），而基于贝塞尔曲线的方法有时容易受到其他目标的干扰。详情请参见图8。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211913581.png"alt="image-20240823211913581" /><figcaption aria-hidden="true">image-20240823211913581</figcaption></figure><p>图8：通过不同的回归可视化结果。从左到右，我们展示了伪造的图像，不同的基于样条曲线的回归结果（左边显示了信任分数，而右边是预测的操作掩模），GT掩模。由于空间限制，请放大以更好地可视化。</p><h1 id="结论">结论</h1><p>  在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），以分类评分和实例评分来区分精确的篡改区域。此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（优秀代码鉴赏）(施工中)</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>2024华为软件精英挑战赛决赛代码（初赛粤港澳赛区冠军，复赛粤港澳赛区冠军，全球总决赛第4 名）)<ahref="https://github.com/OrangeQi-CQ/HuaweiCodeCraft2024-Final"><imgsrc="https://img.shields.io/github/stars/OrangeQi-CQ/HuaweiCodeCraft2024-Final?style=flat"alt="GitHub" /></a></p><p>这是 2024 华为软件精英挑战赛<strong>“适可而止矣，涓埃之事，亦央原神”</strong> 队的决赛代码。</p><p>我们属于<strong>粤港澳赛区</strong>，三名队员（<ahref="https://github.com/OrangeQi-CQ">cq</a>、<ahref="https://github.com/yhf4aspe">yhf</a>、<ahref="https://github.com/zzwtx">xsf</a>）都是来自<strong>华南理工大学</strong>的本科生。在2024华为软件精英挑战赛中成绩如下：</p><ul><li><p>初赛：<strong>粤港澳赛区冠军</strong></p></li><li><p>复赛：<strong>粤港澳赛区冠军、全国第 2 名</strong></p></li><li><p>决赛：<strong>全球总决赛第 4 名（季军/三等奖）</strong></p></li></ul><hr /><p>成功之处：</p><ul><li>我们的代码实现能力比较强，能够高效准确地将想法落地并测试效果。有很多想法预期效果很好但实际徒劳无功甚至负作用，而个别想法看似普通却会有很惊喜的效果。我认为将idea 快速落地并测试是在华为软挑取得好成绩的关键。</li><li>有一点点算法基本功（三人都有 icpc/ccpc银），但相比其他一些队伍并不亮眼。</li><li>虽然没有单元测试，但编写了很多集成测试，帮助我们迅速定位没有正常达到目标的模块。</li><li>临时抱佛脚学习了 git（之前只会用 zip压缩+微信传代码）、cmake、clang-format 等工具，并写了一些 python 和shell 脚本。利用工具提升效率。</li></ul><p>不足之处：</p><ul><li>三人都没有大厂实习经验，缺乏项目合作开发的流程。例如没有需求和交付的流程和文档，git分支混乱，git 流程不规范，缺乏设计模式的使用等。</li><li>在决赛中，策略过于保守（意图避免出大错，也不算是一件坏事）。</li></ul><hr /><p>main.cpp</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifdef USE_MFMC</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE USE_MFMC&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef AVOID_SWING</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE AVOID_SWING&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef RAND</span><br><span class="line">    unsigned rand_seed = static_cast&lt;unsigned&gt;(time(nullptr));</span><br><span class="line">    srand(rand_seed);</span><br><span class="line">    std::cerr &lt;&lt; &quot;srand = &quot; &lt;&lt; rand_seed &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef LOCAL</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE LOCAL&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef DEBUG</span><br><span class="line">    fprintf(stderr, &quot;DEFIND DEBUG\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#if !defined(_WIN32) &amp;&amp; !defined(_WIN64)</span><br><span class="line">    // 将 main 线程绑定到 cpu 0 上</span><br><span class="line">    pthread_t main_thread_id = pthread_self();</span><br><span class="line">    cpu_set_t cpu_set;</span><br><span class="line">    CPU_ZERO(&amp;cpu_set);</span><br><span class="line">    CPU_SET(0, &amp;cpu_set);</span><br><span class="line">    pthread_setaffinity_np(main_thread_id, sizeof(cpu_set), &amp;cpu_set);</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;Set CPU!\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    Init();     // 初始化</span><br><span class="line">    Control();  // 控制所有帧</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFL-Net:Image Forgery Localization Using Contrastive Learning</title>
      <link href="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/"/>
      <url>/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>CFL-Net: Image Forgery Localization Using Contrastive Learning</p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/2210.02182v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><a href="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></p><h2 id="摘要">摘要</h2><p>  传统的伪造定位方法的缺点是过度拟合和只关注少数特定的伪造痕迹。我们需要一种更通用的图像伪造定位方法，能够很好地适应各种伪造条件。底层伪造区域定位的一个关键假设是，无论伪造类型如何，每个伪造图像样本中未被篡改和被篡改区域的特征分布都存在差异。在本文中，我们的目标是利用这种差异特征分布来帮助图像伪造定位。</p><p>  具体来说，我们使用对比损耗来学习映射到一个特征空间，在该空间中，每个图像的未篡改区域和被篡改区域之间的特征被很好地分离。此外，该方法不需要对伪造类型进行任何先验知识或假设，就可以对伪造区域进行局部定位。我们证明，我们的工作优于几个现有的方法在三个基准的图像处理数据集。</p><h2 id="引言">引言</h2><p>  交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><p>  考虑到这些局限性，我们在最近提出的对比损失的基础上，提出了一种新的伪造定位方法，称为对比伪造定位网络CFL-Net。我们的方法依赖于底层伪造区域定位的一般假设，即无论伪造类型如何，未被篡改区域和被篡改区域之间的特征统计量仍存在差异，即颜色、强度、噪声等。在本文中，我们着重于利用特征空间中的这种差异，通过对比损失来帮助图像伪造定位。具体来说，我们的模型学习映射到一个特征空间，在这个空间中，每个图像中未被篡改和被篡改区域之间的特征被很好地分离和分散。因此，我们的方法并不专注于特定的伪造线索。此外，我们还计算了每个样品的对比损失。因此，我们的方法对每个样本的伪造线索进行了不同的处理，这有助于归纳。</p><p>  因此，我们的方法对每个样本的伪造线索处理不同，有助于泛化。我们的主要贡献总结如下：</p><p>  1.提出了一种新的图像伪造定位方法，称为CFL-Net。利用了每个图像样本中未被篡改和被篡改区域之间特征分布的差异，而不关注特定的伪造足迹。因此，我们的方法更适合于检测真实生活中的伪造。</p><p>  2.解决了在无任何约束的情况下使用交叉熵损失进行通用图像伪造定位的问题。我们将对比损失纳入其中，并针对这个问题进行调整。</p><p>  3.我们在基准操作数据集上进行了大量的实验，表明我们的方法优于现有的几种图像伪造定位方法。</p><h2 id="网络框架">网络框架</h2><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/archnew.png"alt="CFL-Net" /><figcaption aria-hidden="true">CFL-Net</figcaption></figure><p>图3：对比学习模块：为了便于可视化，图中的投影头输出了一个形状为256×8×8的特征图F。然后将特征图分成4个×4个补丁。然后，对每个补丁中的4个空间向量进行平均，得到大小为4×4的嵌入（图中表示为‘k×k带标签的嵌入’）。groundtruth掩码也被划分为4×4个补丁，每个补丁中计数出出现的最大像素标签，得到输出的4×4掩码（图中表示为“k×k掩码”）。</p><p>  我们使用了两个流编码器，一个用于RGB输入图像，另一个用于SRM滤波图像。由编码器产生的特性被融合并传递到ASPP模块中。来自ASPP块的输出特征然后通过分割头和Projection头，其中第一个产生最终的预测掩模，后者产生进入对比学习模块的特征。</p><p>  我们使用SRM过滤器，并使用它作为其他流的输入。SRM滤波器是一种高通滤波器，它增强了输入图像的高频信息，从而更突出边缘信息，有利于篡改的定位。我们使用ResNet作为这两个流的Backbone。然后，我们通过将特性通道连接起来，融合两个流中的特性。融合特征图采用ASPP模块，提取多尺度信息。全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过在不同尺度上提取信息来帮助实现这一点，比如全局上下文以及更细粒度的像素级上下文信息变得可用。</p><p>  然后我们使用分割头和Projection头，将ASPP模块提取的上采样多尺度特征作为输入。我们选择一个DeepLab风格的分割头，输出大小为H×W的最终分割图。投影图由Conv-BatchNorm-Conv层构成，该层将特征图投影到<spanclass="math inline">\(F∈R^{256×H×W}\)</span>​​,256为嵌入维数。将嵌入的特征图F传递给对比学习模块。评估时不使用投影头。</p><h3 id="aspp模块">ASPP模块</h3><p>  在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><h3 id="对比学习模块">对比学习模块</h3><figure><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410174105358.png"alt="image-20240410174105358" /><figcaption aria-hidden="true">image-20240410174105358</figcaption></figure><p>  由于我们的嵌入式特征图在空间上的大小是H×W，并且我们有相应的、大小相似的真实掩模M，所以我们知道每个像素嵌入的标签。因此，我们可以使用有监督的对比学习。对于每个查询像素嵌入<spanclass="math inline">\(z_i\)</span>​，该嵌入的对比损失变为： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(z_i\cdot k^+/ \tau)}{exp(z_i\cdot k^+/\tau)+\sum_{k^-}exp(z_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(k^+\)</span>是一个嵌入与查询<spanclass="math inline">\(z_i\)</span>具有相同标签的像素。<spanclass="math inline">\(A_i\)</span>表示投影头输出特征图<spanclass="math inline">\(F\)</span>中所有<spanclass="math inline">\(k_+\)</span>的集合。同样，<spanclass="math inline">\(k_-\)</span>是<spanclass="math inline">\(F\)</span>中与<spanclass="math inline">\(z_i\)</span>不同标签的像素嵌入。</p><p>  然而，用这种方式计算<spanclass="math inline">\(L_i\)</span>有一些主要的局限性。首先，基于单像素嵌入的对比损失没有考虑相邻嵌入的上下文信息。此外，为了计算损失，需要存储大小为HW×HW的点积矩阵，这很消耗内存。因此，为了在上下文和细粒度轨迹之间找到平衡，我们选择将F划分为局部区域。</p><p>  我们首先将<spanclass="math inline">\(F∈R^{256×H×W}\)</span>在空间上划分为k×k个块，从而得到<spanclass="math inline">\(f_i\in R^{256\times h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个<spanclass="math inline">\(f_i\)</span>都变成了<spanclass="math inline">\(R^{256}\)</span>的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到<spanclass="math inline">\(m_i\in R^{h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。为了得到每个<spanclass="math inline">\(m_i\)</span>的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为<spanclass="math inline">\(m_i\)</span>的值。</p><p>  然后，我们有了像素嵌入<spanclass="math inline">\(f_i\)</span>和每个嵌入<spanclass="math inline">\(m_i\)</span>​​​的相应标签。我们现在得到监督对比损失：<span class="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(A_i\)</span>表示与<spanclass="math inline">\(f_i\)</span>具有相同标签的所有其他像素嵌入<spanclass="math inline">\(k^+\)</span>的集合。类似地，<spanclass="math inline">\(k^−\)</span>是所有与<spanclass="math inline">\(f_i\)</span>有不同标签的负像素嵌入。损失函数中的所有嵌入都是<spanclass="math inline">\(L_2\)</span>归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span>   最后要优化的损失是： <spanclass="math display">\[L=L_{CE}+L_{CON}\]</span>   其中<spanclass="math inline">\(L_{CE}\)</span>是交叉熵损失。</p><h2 id="实验">实验</h2><p>  在本节中，我们将描述在三个不同的操作数据集上进行的实验，以探索CFL-Net的有效性。这些数据集是包含几种操作类型的通用操作数据集，并不只特定于一种操作类型。我们使用的评估度量是像素级的曲线下面积（AUC）评分。</p><p>  我们使用ResNet-50作为这两个流的编码器。我们用Adam优化器训练CFL-Net，学习率为1e-4。每过了20个epochs，我们就会将学习率降低20%。我们将输入图像的大小调整为256×256。我们将F划分为总共64个×64个块。温度系数<spanclass="math inline">\(\tau\)</span>​设置为0.1。对交叉熵损失进行加权，使被篡改类的权重增加十倍。我们将批处理大小设置为4，并在NVIDIARTX Titan GPU上训练模型超过100个epochs。</p><h3 id="与各种baseline模型的比较">与各种baseline模型的比较：</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410194714118.png" alt="image-20240410194714118 " style="zoom:50%;" /></p><p>  我们在表1中报告了我们的方法和基线模型的AUC评分（%）。需要注意的是，这里陈述的RGB-N和SPAN的结果是它们在各自的论文中报道的精细结果。JLSTM和Transforensics不进行任何预训练。虽然ManTraNet在合成操作数据集上对它们的模型进行了预训练，但它们并没有对特定的数据集进行微调。从表格中可以看出，CFLNet在基线模型中的所有数据集上都取得了最好的定位性能。特别是，CFLNet在IMD-20数据集上的性能大大优于所有的基线模型，而IMD-20数据集是一个具有各种伪造类型的真实操作数据集。具体来说，CFL-Net在IMD-20数据集上获得了89.9%的AUC分数，比性能第二良好的模型Transforensics提高了5.1%。</p><p>  因此，它验证了我们的主张，即cfll-net非常适合于本地化真实生活中的伪造品。我们的模型在其他数据集上也优于基线模型——Casia和Nist。此外，值得指出的是，CFLNet在没有对合成操作数据进行预训练的情况下就实现了这些结果。</p><h3 id="实验有无对比损失">实验（有无对比损失）</h3><p><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195126470.png"alt="image-20240410195126470" /><br/>表2：最左边的一列显示了所训练的数据集模型。后面的列是评估模型的数据集。“w/o”训练没有对比损失，“w”训练有对比损失。结果以%AUC表示。</p><p>  表2显示了结果。很明显，用对比损失训练的CFL-Net在跨数据集的推广方面表现得非常好。在所有情况下，该模型比没有对比损失的模型表现得更好。当在IMD-20上进行训练并在NIST的测试集上进行评估时，我们提出的模型甚至优于ManTraNet的AUC分数。当在IMD-20数据集上进行训练时，可以看到的性能提高最高。IMD-20是真实的图像操作数据集，因此在这个数据集上进行训练有助于模型学习最一般化的特征。因此，我们提出的在IMD-20上训练并在其他数据集上进行评估的模型，与在没有对比损失的情况下训练的模型相比，产生了最大的性能改进。<br/>  还需要注意的是，在NIST上训练和在其他数据集上评估的两种模型的表现都很差，因为NIST拥有的图像很少，即数据集中有584张图像。因此，很难使用NIST来推广到其他数据集。尽管如此，我们提出的模型还是比训练后的没有对比损失的模型表现得更好。</p><h3 id="定性分析">定性分析</h3><p>  为了证明我们的对比损失通过避免同一类特征的聚类来保持特征的变化，我们通过t-SNE将从图5中分割头获得的类特征可视化。</p><p><imgsrc="../postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195906151.png"alt="image-20240410195906151" /><br/>图5：左栏显示了当仅使用交叉熵损失训练CFL-Net时，IMD-20和CASIA测试集上的平均特征的t-SNE图。右列对应于使用交叉熵损失和对比损失训练的CFL-Net。绿色=未篡改特征，红色=篡改特征。</p><p>  左列显示了当仅使用CFL-Net训练交叉熵损失时，IMD-20和CASIA测试集上每个图像样本的平均特征向量。很明显，未被篡改（图中绿色）和被篡改（图中红色）区域对应的特征在这里是堵塞的。与此同时，右栏显示了同时使用交叉熵和对比损失进行CFL-Net训练时的平均特征。这两个区域对应的特征更加分散。<br/>  因此，不同的操作足迹更容易可分离。实验表明，传统的交叉熵损失由于类别内不变性而减少了图像伪造定位的泛化，而我们提出的方法可以通过分散特征分布来提高泛化效果。</p><h3 id="消融实验损失函数变化">消融实验（损失函数变化）</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410200342409.png" alt="image-20240410200342409 " style="zoom:50%;" /></p><p>  在表3中，我们报告了结果。从表中可以清楚地看出，添加对比损失确实有助于定位。这种改进在真实的图像处理数据集IMD-20上更为突出。对比损失有助于提高AUC评分4.7%。需要注意的是，在没有对比损失的情况下，我们的方法已经取得了很好的结果。</p><h2 id="结论">结论</h2><p>  在本文中，我们从一个新的角度来研究通用的图像伪造定位问题。我们发现了现有方法的一个主要缺点，该方法关注特定的伪造足迹，并使用没有任何约束的交叉熵损失来定位伪造。为了解决这一缺点，我们补充了交叉熵损失和对比损失，并提出了一种新的图像伪造定位方法，即对比伪造定位网络CFL-Net。我们在三个基准图像处理数据集上进行了实验，并将实验结果与近年来的主要伪造定位方法进行了比较。CFL-Net在AUC度量方面优于所有方法。此外，在现实生活中的图像处理数据集IMD-2020上的改进更为突出。在未来的工作中，可以考虑一个更复杂的融合机制来融合来自RGB和SRM流的特征映射。例如，注意模块或最近提出的视觉变压器可以被用作一种融合机制。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文合集</title>
      <link href="/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/"/>
      <url>/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<hr /><p><ahref="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/">Attentiveand Contrastive Image Manipulation Localization With BoundaryGuidance</a>(TIFS24)</p><p><em>现有问题</em>：在被操纵的图像中，被篡改区域的边界是分离被操纵和未被操纵像素的关键位置，在定位被操纵区域时应特别注意并明确利用这一点。然而，如何利用这些边界信息来提高检测被操纵图像区域的性能仍有待探索。。</p><p><em>解决方案</em>：提出了一种新的边界引导图像操纵定位模型，该模型通过精心设计的注意力和对比学习机制充分利用被篡改区域的边界信息，在框架的解码器中引入了一个边界感知注意模块，旨在指导模型通过提取被操纵区域的边界来强调图像操作的非自然混合，我们提出了一种边界引导的篡改对比损失，鼓励模型将样本的边缘从篡改和非篡改区域扩大到最大的程度。</p><details close><br/><summary>具体情况</summary><p>其网路架构如下：</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png"alt="image-20240910222829501" /><figcaption aria-hidden="true">image-20240910222829501</figcaption></figure><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure></details><hr /><p><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image ManipulationDetection</a>(AAAI24)<br/><em>现有问题</em>：</p><ul><li>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。</li><li>基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</li></ul><p><em>解决方案</em>：包含RGB和频率特征的双分支架构，能够检测双压缩伪影的压缩伪影学习模型。</p><details close><br/><summary>具体情况</summary><blockquote><p>RGB和频率特征的双分支架构</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><blockquote><p>双压缩伪影的压缩伪影学习模型</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /><figcaption aria-hidden="true">image-20240326170313123</figcaption></figure></details><hr /><p><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -所有现有的IMD主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p><em>解决方案</em>：一种基于掩码引导查询的转换器框架（MGQFormer），该框架使用基本事实掩码来引导可学习查询令牌（LQT）识别伪造区域。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png"alt="image-20240401164940206" /><figcaption aria-hidden="true">image-20240401164940206</figcaption></figure><p>  利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征，过空间和通道注意模块（SCAM,spatialand channel attentionmodule）对多模态特征进行融合。其特征提取器如下:</p><p><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822164324909.png"alt="image-20240822164324909" /><br/>  我们设计了两个可学习的查询token来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。其解码器如下:</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><p>  具体来说，我们将噪声的GT掩模输入MGQFrorer，以获得引导查询token（GQT)和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。</p><p><em>解决方案</em>：通过关注噪声域内的操纵痕迹来检测和定位图像伪造，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><p>一阶段：</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><br/>为了明确地分离出这两个区域（真实的和伪造的)的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $ 的噪声。</p><figure><imgsrc="./../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822220206200.png"alt="image-20240822220206200" /><figcaption aria-hidden="true">image-20240822220206200</figcaption></figure><p>式中， $ _a $ 、 $ _f $ 为 $ N_a $ 和 $ N_f $ 的标准差， $ _a $ 、 $_f $ 为 $ N_a $ 和 $ N_f $ 的平均值。</p><p><spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>二阶段：</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>利用两个分支来处理RGB和噪声信息，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。</p></details><hr /><p><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a>(AAAI24)</p><p><em>现有问题</em>： 假阳性（FPs）和不准确的边界。</p><p><em>解决方案</em>：基于CatmullRom样条的回归网络（CSR-Net, CatmullRomSplines-based RegressionNetwork），首次尝试将回归方法引入像素级任务。为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。</p><p>详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），我们为每个区域实例重新分配分数，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。</p><p>此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p></details><hr /><p><ahref="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/">Multi-viewFeature Extraction via Tunable Prompts is Enough for Image ManipulationLocalization</a>(ACMMM24)</p><p><em>现有问题</em>：IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p><em>解决方案</em>：提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。</p><details close><br/><summary>具体情况</summary><blockquote><p>通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><blockquote><p>特征对齐和融合的FAF模块</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824220942200.png"alt="image-20240824220942200" /><figcaption aria-hidden="true">image-20240824220942200</figcaption></figure></details><hr /><p><a href="/UnionFormer/">UnionFormer: Unified-Learning Transformerwith Multi-View Representation for Image Manipulation Detection andLocalization</a>(CVPR24)</p><p><em>现有问题</em>：以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层的特征，不能充分表示篡改痕迹；目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。</p><p><em>解决方案</em>：设计了专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork）设计了用于图像操作检测和定位的多视图表示的统一学习transformer框架</p><details close><br/><summary>具体情况</summary><blockquote><p>cnn-Transformer并发网络BSFI-Net，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><figcaption aria-hidden="true">image-20240617110632461</figcaption></figure><blockquote><p>采用对比监督来促进两个视图之间的协作</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><blockquote><p>统一伪造判别表示，每个篡改判别查询都表示对应建议的三个视图中的篡改线索</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>：篡改痕迹主要来源于真实区域和伪造区域的噪声分布则不一致性。</p><p><em>解决方案</em>：使用两阶段训练方法：第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><blockquote><p>第一阶段，训练一个特征提取器，使用的是DNE，一个盲去噪网络，使用JS散度来约束Gd，将Gd划分为真实区域Na和伪造区域Nf，将其视为两个高斯分布</p></blockquote><p><span class="math display">\[JSD=\\log\\frac{\\sqrt{\\sigma_{a}^{2}+\\sigma_{f}^{2}}}{2}-\\frac{\\log\\sigma_{a}+\\log\\sigma_{f}}{2}+\\frac{(\\mu_{a}-\\mu_{f})^{2}}{\\sigma_{a}^{2}+\\sigma_{f}^{2}}+\\frac{1}{2}\]</span></p><p><span class="math display">\[\\mathbf{L_{n}}=\\lambda\\left(1-JSD\\right)+\\left(1-\\lambda\\right)\\mathcal{L}\\left(Y,G_{c}\\right)\]</span></p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><blockquote><p>第二阶段，使用一阶段训练好的特征提取器DNE，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。</p></blockquote><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure></details><hr /><p>CVPR '23:</p><p><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a></p><p>ICCV '23:</p><p><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>WACV' 23:</p><p><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> (<em>WACV'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2210.02182"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/niloy193/CFLNet"><strong>Code</strong></a><strong>]</strong></p><p>TPAMI '22:</p><p><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a></p><details close><br/><summary>CLIP</summary><p><br/>原论文：<br/><ahref="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/">LearningTransferable Visual Models From Natural Language Supervision</a></p>Coop：<br/><a href="/Prompt-Learning-for-Vision-Language-Models/">PromptLearning for Vision Language Models</a><br/></details><details close><br/><summary>HRnet</summary><p><br/>原论文：<br/><a href="/HRnet/">Deep High-ResolutionRepresentation Learning for Human Pose Estimation</a></p></details><details close><br/><summary>自监督学习——对比学习</summary><p><a href="">SimCLR</a></p><p>https://proceedings.mlr.press/v119/chen20j.html</p><p>https://github.com/google-research/simclr</p><p>https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning</p><p>本文介绍了SimCLR：一个用于视觉表征对比学习的简单框架。我们简化了最近提出的对比自监督学习算法，而不需要专门的架构或内存库。为了理解是什么使对比预测任务能够学习有用的表征，我们系统地研究了我们框架的主要组成部分。我们表明：（1）数据增强的组成在定义有效的预测任务中起着关键作用；（2）在表征和对比损失之间引入可学习的非线性变换，大大提高了学习表征的质量；（3）与监督学习相比，对比学习受益于更大的批量和更多的训练步骤。通过结合这些发现，我们能够在ImageNet上大大优于以前的自监督和半监督学习方法。在SimCLR学习的自监督表示上训练的线性分类器实现了76.5%的top-1准确率，这比以前的最先进技术提高了7%，与监督ResNet-50的性能相匹配。当只对1%的标签进行微调时，我们实现了85.8%的前五名准确率，比AlexNet少了100倍的标签。</p><p><a href="">Matrix Information Theory for Self-SupervisedLearning</a></p><p>https://paperswithcode.com/paper/kernel-ssl-kernel-kl-divergence-for-self</p></details><details close><summary>监督对比学习</summary><blockquote><p><ahref="https://paperswithcode.com/paper/supervised-contrastive-learning">SupervisedContrastive Learning | Papers With Code</a><br/>&gt; <br/>&gt; <ahref="https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html">SupervisedContrastive Learning NeurIPS 2020</a></p></blockquote></details><details close><summary>arXiv</summary><p><br/><a href="/FOCAL/">Rethinking Image Forgery Detection viaContrastive Learning and Unsupervised Clustering</a></p><p>图像伪造检测旨在检测和定位图像中的伪造区域。大多数现有的伪造检测算法都提出了将像素分类为伪造或原始的分类问题。然而，伪造像素和原始像素的定义仅在单个图像内是相对的，例如，图像a中的伪造区域实际上是其源图像B中的原始区域（拼接伪造）。现有的方法严重忽视了这种相对定义，不必要地将不同图像中的伪造（原始）区域混合到同一类别中。为了解决这一困境，我们提出了模糊控制聚类（FOCAL）方法，这是一种新的、简单但非常有效的基于对比学习和无监督聚类的图像伪造检测方法。</p>具体而言，FOCAL1）利用像素级对比学习，以逐图像的方式监督高级取证特征提取，明确地反映了上述相对定义；2）采用动态无监督聚类算法（而不是经过训练的算法）将学习到的特征聚类为伪造/原始类别，进一步抑制了训练数据对跨图像的影响；以及3）允许在不需要重新训练的情况下通过简单的特征级级联来进一步提高检测性能。<br/></details><details close><summary>论文（c）</summary><p><br/>MMM '24：<br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection andLocalization</a></p><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p></details>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HRnet</title>
      <link href="/HRnet/"/>
      <url>/HRnet/</url>
      
        <content type="html"><![CDATA[<p>Deep High-Resolution Representation Learning for Human PoseEstimation<a href="https://arxiv.org/abs/1902.09212"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><ahref="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"><imgsrc="https://img.shields.io/github/stars/leoxiaobin/deep-high-resolution-net.pytorch?style=flat"alt="GitHub" /></a></p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/hrnet.png"alt="Illustrating the architecture of the proposed HRNet" /><figcaption aria-hidden="true">Illustrating the architecture of theproposed HRNet</figcaption></figure><p>算法流程如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205309290.png"alt="architecture" /><figcaption aria-hidden="true">architecture</figcaption></figure><p>其中Bottleneck的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205400578.png"alt="Bottleneck" /><figcaption aria-hidden="true">Bottleneck</figcaption></figure><p>stage：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205432768.png"alt="stage2" /><figcaption aria-hidden="true">stage2</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205452973.png"alt="stage3" /><figcaption aria-hidden="true">stage3</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205529301.png"alt="stage4" /><figcaption aria-hidden="true">stage4</figcaption></figure><p>其中BasicBlock的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205604131.png"alt="BasicBlock" /><figcaption aria-hidden="true">BasicBlock</figcaption></figure><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details>]]></content>
      
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> hrnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rethinking Image Forgery Detection via Contrastive Learning and Unsupervised Clustering</title>
      <link href="/FOCAL/"/>
      <url>/FOCAL/</url>
      
        <content type="html"><![CDATA[<p>Rethinking Image Forgery Detection via Contrastive Learning andUnsupervised Clustering <ahref="https://github.com/HighwayWu/FOCAL"><imgsrc="https://img.shields.io/github/stars/HighwayWu/FOCAL?style=flat"alt="GitHub" /></a><a href="https://arxiv.org/abs/2308.09307"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/FOCAL/2308.09307v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  图像伪造检测的目的是检测和定位图像中的伪造区域。大多数现有的伪造检测算法制定了分类问题，以分类像素为伪造或原始。然而，伪造像素和原始像素的定义只在一张图像中是相对的，例如，图像A中的伪造区域实际上是其源图像B中的原始区域（拼接伪造）。这种相对的定义被现有的方法严重忽视了，这些方法不必要地将不同图像中的锻造（原始）区域混合到同一个类别中。为了解决这一难题，我们提出了法医对比聚类（FOCAL）方法，这是一种新的、简单而又非常有效的基于对比学习和无监督聚类的图像伪造检测范式。</p><p>​  具体来说，FOCAL：</p><p>​    1)利用像素级对比学习，利用像素级对比学习以图像-图像的方式监督高级取证特征提取，明确反映上述相对定义；</p><p>​    2)采用动态无监督聚类算法（而不是训练的算法）将学习到的特征聚为伪造/原始类别，进一步抑制训练数据的交叉图像影响；</p><p>​    3)允许通过简单的特征级连接进一步提高检测性能，而不需要再训练。</p><p>​  广泛的实验结果在六个公共测试数据集表明，我们提出的FOCAL明显优于先进的竞争算法：Coverage+24.3%，<em>Columbia</em>+18.6%，FF+++17.5%，MISD+14.2%，CASIA+13.5%，IoU+10.3%。FOCAL的范式可以带来新的见解，并为图像伪造检测任务提供一个新的基准。该代码可以在https://github.com/HighwayWu/FOCAL上找到。</p><h1 id="引言">引言</h1><p>​  一般来说，现有的基于学习的图像伪造检测方法提出了两类分类问题，将像素分类为伪造或原始。需要指出的是，伪造像素和原始像素的定义只是相对于一幅图像的。</p><figure><img src="../postimages/FOCAL/image-20240518122357716.png"alt="image-20240518122357716" /><figcaption aria-hidden="true">image-20240518122357716</figcaption></figure><p>​  例如，图2 (a)中与两个人相关联的像素是原始的，而图2(b).中相同的像素是伪造的。不幸的是，这种相对的定义被现有的基于分类的伪造检测方法严重忽视了，这些方法不必要地将不同图像中的伪造（原始）区域混合到同一类别中。事实上，图2中的α1、α2和α3区域并不一定具有相似的法医特征，尽管它们属于相同的原始类别（与β1和β2相似）。因此，当看到同一组像素被标记为伪造和原始的时，分类器可能会被误导，导致训练不稳定和检测性能较差。</p><p>​  重新思考伪造像素和原始像素的相对定义，促使我们重新制定先前流行的分类问题，形成一个具有对比学习和无监督聚类的新范式。具体地说，我们在这项工作中提出了法医对比聚类（FOCAL,FOrensic ContrAstivecLustering）方法，一种新颖、简单而有效的图像伪造检测范例。FOCAL利用像素级对比学习，以逐幅图像的方式监督高级法医特征提取，明确地利用上述相对定义。ground-truth的伪造掩模自然地提供了积极和消极类别的像素级区分，使我们的像素级对比学习。此外，我们的对比学习的另一个独特特征是逐图像监督，这可以有效地避免一批不同图像间特征的相互影响。</p><p>​  此外，FOCAL采用动态无监督聚类算法将学习到的特征聚类为伪造/原始类别，进一步避免了训练数据的交叉图像干扰。请注意，这里所采用的聚类模块不涉及任何可训练的参数，因此不参与训练过程。研究还表明，通过直接的特征级融合，可以进一步提高性能，而不需要再训练。</p><p>​  广泛的实验结果在六个公共测试数据集表明，我们提出的焦点显著优于先进的竞争算法[8,25,28,50,15]大利润率：Coverage+24.3%，Columbia+18.6%，FF+++17.5%，MISD+14.2%，CASIA+13.5%，+NIST10.3%。FOCAL的范式可以带来新的见解，并为图像伪造检测任务提供一个新的基准。我们的主要贡献可以总结如下：</p><p>  1.我们从伪造/原始像素的相对定义度的角度，重新思考了基于分类的图像伪造检测范式的固有局限性。<br/>  1.我们设计了一种新颖的、简单而有效的基于对比学习和无监督聚类的范式用于图像伪造检测。<br/>  1.在6个（跨域）数据集上，所提出的焦点算法的性能显著优于几种最先进的图像伪造检测方法，IoU的平均增益为19.6%，F1的平均增益为10.4%。</p><h1 id="focal-对比学习"><strong>FOCAL 对比学习</strong></h1><p>​  FOCAL 的训练过程如图3 (b)所示：</p><figure><img src="../postimages/FOCAL/image-20240518144850862.png"alt="image-20240518144850862" /><figcaption aria-hidden="true">image-20240518144850862</figcaption></figure><p>​  一旦我们从给定的输入X中提取高级特征F，我们就通过像素级对比学习直接监督F。地面真实伪造掩模Y自然为我们提供了正和消极类别的索引，使有效的像素级对比学习。正如很快就会更清晰的那样，焦点的对比学习以逐图像的方式进行监督，这与现有的对整个正向小批执行监督的算法[19,6,15,54,56]有很大的不同。</p><p>​  具体来说，我们采用了一种改进的InfoNCE损失[16,35]来实现焦点中的对比学习。</p><p>​  我们首先通过执行一个扁平化操作来构造一个字典 $ f() : {}^{ } ^{ }$<span class="math display">\[f(F) \rightarrow \{ q , k^+_1 , k^+_2 ,..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K \}\]</span> ​  其中， $ { q ,k^+_1 , k^+_2 , ..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K } $被定义为字典，q是一个编码查询。我们让 $ { q , k^+_1 , k^+_2 , ..., k^+_J} $表示属于原始区域的特征（以Y中的0为索引），而 $ { k^-_1 , k^-_2 , ...,k^-_K } $表示伪造区域的特征（以Y中的1索引）。</p><p>​  在图像伪造检测任务中，伪造或原始区域通常覆盖超过1像素（特征）的区域，这意味着字典中正键J的数量也远远大于1。然后，根据图像伪造任务而定制的改进的信息损失可以计算为<span class="math display">\[{\cal {L}}_{InfoNCE++}=-log \frac { \frac 1J \sum _{ j \in [1,J] } exp(q \cdot k^+_J / \tau ) } { \sum _{ i \in[1,K] } exp(q \cdot k^+_i / \tau ) }\]</span> ​  其中， $ $是一个温度超参数[51]。注意，在原始的InfoNCE loss[16,35]中，字典中只有一个q匹配的正键。在我们改进的InfoNCE损失(2)中，通过取q与正类${ k^+_j } $点积的期望，在每个损失计算中涉及所有的正键。这将促进优化过程。</p><p>​  需要强调的是，训练阶段的监督是直接在地面真实伪造掩模Y和提取的特征F之间进行的，而没有生成预测的伪造掩模。</p><p>​  此外，对于前向小批量中的每一幅图像， $ {}<em>{InfoNCE++} $以逐图像的方式（one-by-one）计算，而不是对整个批量进行计算，然后求和计算总体损失。更具体地说，给定一个小批特征$ {F_1、F_2、···、F_B} $ ，总体对比损失 $ {}</em>{ct} $ ： <spanclass="math display">\[{\cal {L}}_{ct}=\frac {1} {B} \sum _{b=1} ^{B}({\cal {L}}_{InfoNCE++}(F_b))\]</span>​  请注意，在上述（3）式中，没有合并小批特征来计算整体的 $ {}_{InfoNCE++}$，避免了训练数据的交叉图像影响。在伪造/原始像素的相对定义的指导下设计的总损失与[5,15,16,32]中的损失有很大的不同，[5,15,16,32]中的损失计算是在批处理级别进行的。</p><p>​  为了进一步证明(3)的合理性，我们在图4中绘制了传统的基于批处理和我们的逐图像图像的对比损失曲线。<br/><imgsrc="../postimages/FOCAL/image-20240518153103103.png"alt="image-20240518153103103" /></p><p>​  可以清楚地看到，损失函数（橙色线）的逐图像设计不仅使收敛速度更快，而且使优化更加稳定。特别是，在蓝线中检测到的高振幅脉冲表明相关的图像中可能存在严重的冲突，例如，类似于图2(a)和(b)的情况，其中出现冲突的标签。</p><p>​  最终，训练有素的提取器将被用于FOCAL测试阶段。正如预期的那样，并将通过实验验证，我们的像素级对比学习与逐幅图像的整体损失设计，显著提高了图像伪造检测性能。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>特征融合、特征采样方法合集</title>
      <link href="/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/"/>
      <url>/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="swim-transformer">Swim Transformer:</h1><p>PatchMerging类 PatchMerging 类是 Swin Transformer架构中用于降低特征图分辨率的层。这个过程通过合并相邻的patch来减少序列长度，同时增加通道数，以保持信息的密度。</p><p>每执行一个stage后，都会执行一个一个下采样操作，也就是PatchMerging类的前向传播。所谓的下采样操作，主要是把x切片成4个，<spanclass="math inline">\(x_0\)</span>、<spanclass="math inline">\(x_1\)</span>、<spanclass="math inline">\(x_2\)</span>、<spanclass="math inline">\(x_3\)</span>这四个是按照长宽间隔去选的：</p><figure><imgsrc="../postimages/特征融合、特征采样方法合集/f34d8f6a311047b18d5e32e49a827f8b.png"alt="Swim Transformer" /><figcaption aria-hidden="true">Swim Transformer</figcaption></figure><p>x原来是8 ∗ 8，取完后变成了4个4 ∗4的，再把4个做一个拼接，拼接完成后再连接一个全连接，使用全连接进行降维。</p><p>构造函数：</p><ul><li>input_resolution ，dim ：输入特征的分辨率和通道数</li><li>reduction，初始化一个线性变换，用于将相邻四个patch的特征合并成一个patch</li></ul><p>前向传播：</p><p>原始输入：</p><ol type="1"><li>torch.Size([4, 3136, 96])<br/>2. H, W =56<em>56，输入特征长宽<br/>3. B, L, C =4</em>3136*96，batch，序列长度，特征维度<br/>4. x： torch.Size([4, 56,56, 96])，将输入特征重塑为四维张量，准备进行patch合并操作<br/>5. x0：torch.Size([4, 28, 28, 96])、x1： torch.Size([4, 28, 28, 96])、x2：torch.Size([4, 28, 28, 96])、x3： torch.Size([4, 28, 28,96])，提取四个相邻patch的特征，每个patch分别来自原始特征图的不同子区域<br/>6.x： torch.Size([4, 28, 28,384])，将四个patch的特征在通道维度上合并<br/>7. x： torch.Size([4, 784,384])，将合并后的特征图重塑，准备进行线性变换<br/>8. x： torch.Size([4,784, 384])，层归一化，维度不变<br/>9. x： torch.Size([4, 784,192])，通过线性变换降低合并后特征的维度，减少通道数</li></ol><details close><br/><summary>Swim Transformer code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.reduction(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br/></details><h1 id="hrnet">hrnet：</h1><p>详情<ahref="/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/">DeepHigh-Resolution Representation Learning for Human PoseEstimation</a></p><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details><h1 id="aspp">ASPP:</h1><p><a href="https://arxiv.org/abs/1706.05587">Rethinking atrousconvolution for semantic image segmentation</a></p><p>​在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><p>其中nn.Conv2d中的dilation参数含义如下：<br/>dilation = 1：</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>dilation=2:</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16-1712826246932-3.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><details close><br/><summary>ASPP code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, kernel_size, padding, dilation, BatchNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(_ASPPModule, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,</span><br><span class="line">                                            stride=<span class="number">1</span>, padding=padding, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn = BatchNorm(planes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.atrous_conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_stride == <span class="number">16</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>]</span><br><span class="line">        <span class="keyword">elif</span> output_stride == <span class="number">8</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">36</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.aspp1 = _ASPPModule(inplanes, outplanes, <span class="number">1</span>, padding=<span class="number">0</span>, dilation=dilations[<span class="number">0</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp2 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">1</span>], dilation=dilations[<span class="number">1</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp3 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">2</span>], dilation=dilations[<span class="number">2</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp4 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">3</span>], dilation=dilations[<span class="number">3</span>], BatchNorm=BatchNorm)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                             nn.Conv2d(inplanes, outplanes, <span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                             BatchNorm(outplanes),</span><br><span class="line">                                             nn.ReLU())</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(outplanes*<span class="number">5</span>, outplanes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = BatchNorm(outplanes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = <span class="variable language_">self</span>.aspp1(x)</span><br><span class="line">        x2 = <span class="variable language_">self</span>.aspp2(x)</span><br><span class="line">        x3 = <span class="variable language_">self</span>.aspp3(x)</span><br><span class="line">        x4 = <span class="variable language_">self</span>.aspp4(x)</span><br><span class="line">        x5 = <span class="variable language_">self</span>.global_avg_pool(x)</span><br><span class="line">        x5 = F.interpolate(x5, size=x4.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat((x1, x2, x3, x4, x5), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                <span class="comment"># n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels</span></span><br><span class="line">                <span class="comment"># m.weight.data.normal_(0, math.sqrt(2. / n))</span></span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_aspp</span>(<span class="params">inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">    <span class="keyword">return</span> ASPP(inplanes, outplanes, output_stride, BatchNorm)</span><br></pre></td></tr></table></figure></details><h1 id="srm-滤波器">SRM 滤波器:</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/6197267">RichModels for Steganalysis of Digital Images</a></p><p>SRM滤波器可以捕捉高频篡改痕迹，使用SRM过滤器从RGB图像中提取局部噪声特征</p><p>我们的设置中，噪声是通过像素值与仅通过内插相邻像素的值而产生的该像素值的估计之间的残差来建模的。从30个基本滤波器开始，再加上非线性运算（例如，滤波后附近输出的最大值和最小值），SRM功能将收集基本噪声特征。SRM量化并截断这些滤波器的输出，并提取附近的共现信息作为最终特征。从该过程获得的特征可以被视为局部噪声描述符。我们选择3个内核，其权重如下所示，并将其直接输入经过3通道输入训练的预训练网络中。我们将噪声流中SRM滤波器层的内核大小定义为5×5×3。SRM层的输出通道大小为3。</p><p><span class="math display">\[\frac {1} {4} \begin{bmatrix}0 &amp; 0&amp; 0 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 2&amp; -4 &amp; 2 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 0&amp; 0 &amp; 0 &amp; 0\end{bmatrix}\\\frac {1} {12} \begin{bmatrix}-1&amp; 2 &amp; -2 &amp; 2 &amp; -1\\2 &amp; -6 &amp; 8 &amp; -6 &amp;2\\-2 &amp; 8 &amp; -12 &amp; 8 &amp; -2\\2 &amp; -6 &amp; 8 &amp; -6&amp; 2\\-1 &amp; 2 &amp; -2 &amp; 2 &amp; -1\end{bmatrix}\\\frac {1}{2} \begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp;0\end{bmatrix}\]</span></p><details close><br/><summary>SRM 滤波器 code 1 from CFL-Net</summary><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def setup_srm_weights(input_channels: int = 3, output_channel=1) -&gt; torch.Tensor:</span><br><span class="line">    &quot;&quot;&quot;Creates the SRM kernels for noise analysis.</span><br><span class="line">    note: values taken from Zhou et al., &quot;Learning Rich Features for Image Manipulation Detection&quot;, CVPR2018</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional):  Defaults to 3.</span><br><span class="line">        output_channel (int, optional): Defaults to 1.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.Tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    srm_kernel = torch.from_numpy(</span><br><span class="line">        np.array([</span><br><span class="line">            [  # srm 1/2 horiz</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 1., -2., 1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/4</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 2., -4., 2., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/12</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-2., 8., -12., 8., -2.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">            ]</span><br><span class="line">        ])).float()</span><br><span class="line">    srm_kernel[0] /= 2</span><br><span class="line">    srm_kernel[1] /= 4</span><br><span class="line">    srm_kernel[2] /= 12</span><br><span class="line">    return srm_kernel.view(3, 1, 5, 5).repeat(output_channel, input_channels, 1, 1)</span><br><span class="line">    </span><br><span class="line">def setup_srm_layer(input_channels: int = 3, output_channel=None) -&gt; torch.nn.Module:</span><br><span class="line">    &quot;&quot;&quot;Creates a SRM convolution layer for noise analysis.</span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional): [description]. Defaults to 3.</span><br><span class="line">        output_channel ([type], optional): [description]. Defaults to None.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.nn.Module: [description]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if output_channel == None:</span><br><span class="line">        weights = setup_srm_weights(input_channels)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)</span><br><span class="line">    else:</span><br><span class="line">        weights = setup_srm_weights(input_channels, output_channel)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels,</span><br><span class="line">                               out_channels=output_channel,</span><br><span class="line">                               kernel_size=5,</span><br><span class="line">                               stride=1,</span><br><span class="line">                               padding=2,</span><br><span class="line">                               bias=False)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        conv.weight = torch.nn.Parameter(weights, requires_grad=False)</span><br><span class="line">    return conv</span><br><span class="line">    </span><br></pre></td></tr></table></figure></details><details close><br/><summary>SRM 滤波器 code 2 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>SRM滤波器[14，66]使用预定义的核来学习中心像素的相邻像素之间不同类型的噪声残差，然后进行线性或非线性的最大/最小运算。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SRMConv2d(nn.Module):</span><br><span class="line">    def __init__(self, stride: int = 1, padding: int = 2, clip: float = 2):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.clip = clip</span><br><span class="line">        self.conv = self._get_srm_filter()</span><br><span class="line"></span><br><span class="line">    def _get_srm_filter(self):</span><br><span class="line">        filter1 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 2, -4, 2, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        filter2 = [</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-2, 8, -12, 8, -2],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">        ]</span><br><span class="line">        filter3 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 1, -2, 1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        q = [4.0, 12.0, 2.0]</span><br><span class="line">        filter1 = np.asarray(filter1, dtype=float) / q[0]</span><br><span class="line">        filter2 = np.asarray(filter2, dtype=float) / q[1]</span><br><span class="line">        filter3 = np.asarray(filter3, dtype=float) / q[2]</span><br><span class="line">        filters = [</span><br><span class="line">            [filter1, filter1, filter1],</span><br><span class="line">            [filter2, filter2, filter2],</span><br><span class="line">            [filter3, filter3, filter3],</span><br><span class="line">        ]</span><br><span class="line">        filters = torch.tensor(filters).float()</span><br><span class="line">        conv2d = nn.Conv2d(</span><br><span class="line">            3,</span><br><span class="line">            3,</span><br><span class="line">            kernel_size=5,</span><br><span class="line">            stride=self.stride,</span><br><span class="line">            padding=self.padding,</span><br><span class="line">            padding_mode=&quot;zeros&quot;,</span><br><span class="line">        )</span><br><span class="line">        conv2d.weight = nn.Parameter(filters, requires_grad=False)</span><br><span class="line">        conv2d.bias = nn.Parameter(torch.zeros_like(conv2d.bias), requires_grad=False)</span><br><span class="line">        return conv2d</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        if self.clip != 0.0:</span><br><span class="line">            x = x.clamp(-self.clip, self.clip)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    srm = SRMConv2d()</span><br><span class="line">    x = torch.rand((63, 3, 64, 64))</span><br><span class="line">    x = srm(x)</span><br></pre></td></tr></table></figure></details><h1 id="bayarconv">BayarConv:</h1><p><ahref="https://ieeexplore.ieee.org/abstract/document/8335799">BelhassenBayar and Matthew C Stamm. Constrained convolutional neural networks: Anew approach towards general purpose image manipulationdetection</a></p><p>Bayar卷积滤波器[2]通过使用可学习权重来改进SRM滤波器，约束条件是相邻像素的加权和等于中心像素的权重的负。</p><details close><br/><summary>BayarConv code 1 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from einops import rearrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BayarConv2d(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channles: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        kernel_size: int = 5,</span><br><span class="line">        stride: int = 1,</span><br><span class="line">        padding: int = 0,</span><br><span class="line">        magnitude: float = 1.0,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line">        assert kernel_size &gt; 1, &quot;Bayar conv kernel size must be greater than 1&quot;</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channles</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.magnitude = magnitude</span><br><span class="line"></span><br><span class="line">        self.center_weight = nn.Parameter(</span><br><span class="line">            torch.ones(self.in_channels, self.out_channels, 1) * -1.0 * magnitude,</span><br><span class="line">            requires_grad=False,</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight = nn.Parameter(</span><br><span class="line">            torch.rand((self.in_channels, self.out_channels, kernel_size**2 - 1)),</span><br><span class="line">            requires_grad=True,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def _constraint_weight(self):</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(2, 0, 1)</span><br><span class="line">        self.kernel_weight.data = torch.div(</span><br><span class="line">            self.kernel_weight.data, self.kernel_weight.data.sum(0)</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(1, 2, 0) * self.magnitude</span><br><span class="line">        center_idx = self.kernel_size**2 // 2</span><br><span class="line">        full_kernel = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                self.kernel_weight[:, :, :center_idx],</span><br><span class="line">                self.center_weight,</span><br><span class="line">                self.kernel_weight[:, :, center_idx:],</span><br><span class="line">            ],</span><br><span class="line">            dim=2,</span><br><span class="line">        )</span><br><span class="line">        full_kernel = rearrange(</span><br><span class="line">            full_kernel, &quot;ci co (kw kh) -&gt; ci co kw kh&quot;, kw=self.kernel_size</span><br><span class="line">        )</span><br><span class="line">        return full_kernel</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = nn.functional.conv2d(</span><br><span class="line">            x, self._constraint_weight(), stride=self.stride, padding=self.padding</span><br><span class="line">        )</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    device = &quot;cuda&quot;</span><br><span class="line">    bayer_conv2d = BayarConv2d(3, 3, 3, magnitude=1).to(device)</span><br><span class="line">    bayer_conv2d._constraint_weight()</span><br><span class="line">    i = torch.rand(16, 3, 16, 16).to(device)</span><br><span class="line">    o = bayer_conv2d(i)</span><br></pre></td></tr></table></figure></details><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% 读取图像</span><br><span class="line">grayImage = imread(&#x27;image.jpg&#x27;);</span><br><span class="line"></span><br><span class="line">% 检查图像是否为灰度图像，如果不是，转换为灰度图像</span><br><span class="line">if size(grayImage, 3) == 3</span><br><span class="line">    grayImage = rgb2gray(grayImage);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% 显示灰度图像</span><br><span class="line">imshow(grayImage);</span><br><span class="line">title(&#x27;显示灰度图像&#x27;);</span><br><span class="line"></span><br><span class="line">% 保存灰度图像</span><br><span class="line">imwrite(grayImage, &#x27;grayImage.png&#x27;);</span><br></pre></td></tr></table></figure><h1 id="cbam-convolutional-block-attention-module">CBAM: ConvolutionalBlock Attention Module:</h1><p>本文出自2018ECCV会议</p><p>论文地址：<ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.06521">https://arxiv.org/abs/1807.06521</a></p><h2 id="一计算机视觉中的注意力机制">一、计算机视觉中的注意力机制</h2><p>​  在计算机视觉中能够能够把注意力聚集在图像重要区域而丢弃掉不相关的方法被称作是<strong>注意力机制</strong>(<strong>AttentionMechanisms</strong>)。在人类视觉大脑皮层中，使用注意力机制能够更快捷和高效地分析复杂场景信息。这种机制后来被研究人员引入到计算机视觉中来提高性能。  注意力机制可以看做是对图像输入重要信息的动态选择过程，这个过程是由对于特征自适应权重实现的。注意力机制极大提升了很多计算机视觉任务性能水平，比如在分类，目标检测，语义分割，人脸识别，动作识别，小样本检测，医疗影像处理，图像生成，姿态估计，超分辨率，3D视觉以及多模态中等任务中发挥着重要作用。<br/>​  一般来说，注意力机制通常被分为以下基本四大类：</p><ul><li><strong>通道注意力 Channel Attention</strong>，告诉网络 what to payattention to</li><li><strong>空间注意力机制 Spatial Attention</strong>，告诉网络 where topay attention to</li><li><strong>时间注意力机制 Temporal Attention</strong>，告诉网络 when topay attention</li><li><strong>分支注意力机制 Branch Attention</strong>，告诉网络 which topay attention to</li></ul><p>​  最后还有两种混合注意力机制：</p><p>​    <strong>通道&amp;空间注意力机制</strong> 和<strong>空间&amp;时间注意力机制</strong>。</p><p>​  不同种类的注意力机制在不同的视觉任务中的效果是不同的。</p><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-021feae50e271854cedcfd078189bed7_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-14bb2947cc066e31a3ab653b2a8ab99e_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-fb04ed7d7293bafda456ae921af6deeb_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="二cbam-convolutional-block-attention-module">二、CBAM:Convolutional Block Attention Module</h2><p>​  本文提出的<strong>Convolutional Block AttentionModule(CBAM)</strong>就是上面所提到的两种混合注意力中<strong>通道&amp;空间注意力</strong>的一种。在给定一张特征图，CBAM模块能够序列化地在<strong>通道</strong>和<strong>空间</strong>两个维度上产生注意力特征图信息，然后两种特征图信息在与之前原输入特征图进行相乘进行自适应特征修正，产生最后的特征图。CBAM是一种轻量级的模块，可以嵌入到任何主干网络中以提高性能。</p><h3 id="设计启发">1.设计启发</h3><p>​  CNN网络由于其强大的特征表示能力极大地提高了计算机视觉任务的水平，为了进一步增强这种特征表达能力，研究者在三个主要方面对其进行了更深入地研究，分别是<strong>网络的深度</strong>，<strong>网络的宽度</strong>，<strong>网络的维度</strong>。<br/>​  在深度研究层面，像是我们熟知的最早先的网络<strong>LeNet</strong>，到深度逐渐加深的采用1x1卷积和3x3卷积不断堆叠的<strong>VGGNet</strong>，再到采用了残差设计的<strong>ResNet</strong>，证实了网络可以堆叠到几百层甚至上千层。<strong>GoogLeNet</strong>设计了<strong>Inception</strong>模块可以对网络的宽度进行研究并证明了其效果。<strong>Xception</strong>和<strong>ResNeXt</strong>提出了网络另一个维度Cardinality，证实了这个维度不仅可以节约大量参数而且展示出了比深度和宽度有更强特征表达能力的效果。<br/>​  所以在除了在以上维度对CNN的特征表达能力进行研究之外，作者对网络结构的另外一个层面进行了探索，那就是注意力机制。早前的注意力机制研究已经进行的如火如荼了。注意力机制的主要目的就是：聚焦图像的重要特征，抑制不必要的区域响应，通过在对<strong>通道维度</strong>和<strong>空间维度</strong>组合分析研究，提出了CBAM模块，并证实了网络的性能的提升来自于精确的注意力机制和对无相关噪声信息的抑制。</p><h3 id="cbam总体流程">2.CBAM总体流程</h3><p>对于网络主干生成的特征图： <span class="math display">\[F\inR^{C\times H\times W}\]</span>CBAM分别产生<strong>1D通道注意力特征图</strong>、<strong>2D空间注意力特征图</strong>：<span class="math display">\[M_c\in R^{C\times 1\times 1}\\M_s\inR^{1\times H\times W}\]</span> 这个过程我们可以描述为以下公式： <spanclass="math display">\[F^{&#39;}=M_{c}(F)\otimesF\\F^{&#39;&#39;}=M_{s}(F^{&#39;})\otimes F^{&#39;}\]</span> <spanclass="math inline">\(\otimes\)</span>表示元素级相乘，中间采用广播机制进行维度变换和匹配。</p><h3 id="channel-attention-module">3.Channel Attention Module</h3><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-4d30e7347da8e92cab00fcd45aa65a5d_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  先看通道注意力机制，通过特征内部之间的关系来通道注意力机制。特征图的每个通道都用来被视作一个特征检测器，所以通道特征聚焦的是图像中有用的信息是"什么"（what）。<br/>​  为了更高效地计算通道注意力特征，要做的就是压缩特征图的空间维度，之前采用的是平均池化方法，这个方法可以学习到目标物体的程度信息，作者这里研究了最大池化也能够学习到物体的判别性特征。所以在通道注意力模块同时采用了这两种方法，在后面的实验中也证实了，同时使用两种方法要比单独使用一种方法效果要好。<br/>​  在经过这种方法之后产生了两种不同的空间上下文信息：<spanclass="math inline">\(F_{avg}^c\)</span> 和 <spanclass="math inline">\(F_{max}^c\)</span>分别代表平均池化特征和最大池化特征。<br/>​  然后再将该特征送入到一个共享的多层感知机(MLP)网络中产生最终的通道注意力特征图<span class="math inline">\(M_c\in R^{C\times 1\times1}\)</span><br/>​  为了降低计算参数，在MLP中采用了一个降维系数r， <spanclass="math inline">\(M_c\in R^{C/r\times 1\times1}\)</span><br/>​  综上通道注意力计算公式总结为： <spanclass="math display">\[\begin{gathered}M_{c}(F)=\sigma(MLP(AvgPool(F))+MLP(MaxPool(F)))\\=\sigma(W_{1}(W_{0}(F_{avg}^{c}))+W_{1}(W_{0}(F_{max}^{c})))\end{gathered}\]</span></p><h3 id="spatial-attention-module">4.Spatial Attention Module</h3><figure><imgsrc="../postimages/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/v2-3ffbd68a6064eaf2fae6de939c92c584_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​  通过对特征图空间内部的关系来产生空间注意力特征图。不同于通道注意力，空间注意力聚焦于特征图上的有效信息在"哪里"（where）。为了计算空间注意力，首先在通道维度平均池化和最大池化，然后将他们产生的特征图进行拼接起来（concat）。然后在拼接后的特征图上，使用卷积操作来产生最终的空间注意力特征图：<span class="math display">\[M_s(F)\in R^{1\times H\times W}\]</span>​  同上，在通道维度使用两种池化方法产生2D特征图：</p><p><span class="math display">\[F_{avg}^c \in R^{1\times H\timesW}\\F_{max}^c \in R^{1\times H\times W}\]</span>​  最终这个过程的公式如下： <spanclass="math display">\[\begin{aligned}M_{s}(F)&amp;=\sigma(f^{7\times7}([AvgPool(F);MaxPool(F)]))\\&amp;=\sigma(f^{7\times7}(F_{avg}^{s};F_{max}^{s}))\end{aligned}\]</span></p><h2 id="三.代码实现">三.代码实现</h2><p>​  经过上面的分析，我们可以看到CBAM主要包括两个部分，Channel AttentionModule 和 Spatial AttentionModule，其实际的代码也是非常通俗易懂。<br/>​  下面是pytorch版本的代码。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#通道注意力</span><br><span class="line">class ChannelAttention(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, ratio=16):</span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(1)</span><br><span class="line">        </span><br><span class="line">        #MLP  除以16是降维系数</span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) #kernel_size=1</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        #结果相加</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        return self.sigmoid(out)</span><br><span class="line"></span><br><span class="line">#空间注意力</span><br><span class="line">class SpatialAttention(nn.Module):</span><br><span class="line">    def __init__(self, kernel_size=7):</span><br><span class="line">        super(SpatialAttention, self).__init__()</span><br><span class="line">        #声明卷积核为 3 或 7</span><br><span class="line">        assert kernel_size in (3, 7), &#x27;kernel size must be 3 or 7&#x27;</span><br><span class="line">        #进行相应的same padding填充</span><br><span class="line">        padding = 3 if kernel_size == 7 else 1</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        avg_out = torch.mean(x, dim=1, keepdim=True)  #平均池化</span><br><span class="line">        max_out, _ = torch.max(x, dim=1, keepdim=True) #最大池化</span><br><span class="line">        #拼接操作</span><br><span class="line">        x = torch.cat([avg_out, max_out], dim=1)</span><br><span class="line">        x = self.conv1(x) #7x7卷积填充为3，输入通道为2，输出通道为1</span><br><span class="line">        return self.sigmoid(x)</span><br></pre></td></tr></table></figure><p>CBAM的整理逻辑还是很简单很清晰的，如果向更深入的了解一些细节信息，可以去下载论文仔细研究研究。注意力机制系统比较庞大，最近大火的Tranformer系列又是SelfAttention的一种设计。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</title>
      <link href="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/"/>
      <url>/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<center>Learning Discriminative Noise Guidance for Image Forgery Detection andLocalization</center><p><span class="math inline">\(\text{Jiaying Zhu}^*,\text{DongLi}^*,\text{Xueyang Fu}^\dagger,\text{Gang Yang, Jie Huang, Aiping Liu,Zheng-Jun Zha}\)</span></p><center>中国科学技术大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/28608.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange"alt="AAAI" /></a></p><h1 id="摘要">1. 摘要</h1><p>​  随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。<br/>​  本研究介绍了一种新的图像伪造检测和定位方法，该方法通过关注<strong>噪声域内的操纵痕迹</strong>来检测和定位图像伪造。我们假设RGB图像中几乎看不见带有篡改痕迹的噪声，有助于识别和定位伪造品。然而，篡改技术的进步使噪声用于伪造检测的直接应用变得复杂，因为伪造区域和真实区域之间的噪声不一致性没有得到充分利用。为了解决这一问题，我们开发了一种两步判别噪声引导的方法，以明确地增强一致性中噪声的表示和使用，从而充分利用噪声信息来提高伪造检测的准确性和鲁棒性。<br/>​  具体而言，我们使用去噪网络和基于统计量的约束，增强了伪造区域与真实区域之间的噪声可识别性。然后，我们将模型驱动的引导学习机制与数据驱动的注意力机制相结合，创建了一个可学习且可微分的噪声引导学习机制。这种复杂的滤波器使我们能够从噪音中学习到的伪造区域的边缘。在多个数据集上的综合实验表明，我们的方法可以可靠地检测和定位伪造品，超过了现有的最先进的方法。</p><h1 id="引言">2. 引言</h1><p>​  真实区域和伪造区域的噪声分布则不一致，导致噪声域中的操纵痕迹。许多研究人员已经利用这种噪音信息来图像伪造信息的检测与定位，实现了显著的结果。这些方法直接构建噪声特征到掩模的端到端映射，并采用融合策略集成RGB和噪声信息，以提高伪造检测精度。然而，随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不再是隐含的关系。<br/>​  鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。因此，我们引入了一种新的两步噪声引导方案。<br/>​  第一阶段，训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。我们使用去噪网络，然后使用Bayar卷积来构建噪声提取器，并使用基于统计的约束进行优化。使用去噪器的基本原理源于一个合适的去噪网络可以放大两个区域间的噪声分布差异。<br/>​  如图1所示，标准差为25的去噪网络在噪声域内最大化了真实区域和伪造区域之间的差异，而直接提取的噪声不能达到相同的效果。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502202145227.png" alt="image-20240502202145227 " style="zoom:70%;" /></p><p>图1：我们使用不同标准差的去噪网络（CBDNet (Guoet al.2019），用标准差为15、25、50）对伪造图像进行处理，然后分别提取噪声。对于该图像，25个标准差的去噪器可以帮助获得鉴别噪声。<br/>  为了自适应地调整去噪网络，我们基于高斯分布的Jensen-Shannon（JS）散度对处理后的图像噪声施加自定义约束。</p><p>​  第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。与之前的融合策略不同，我们明确地利用噪声来指导RGB分支，显著提高了有效性。我们将手工制作的引导变换（He，Sun，和Tang2012）和数据驱动的注意力机制（Wang et al.2018）合并，创建了基于交叉注意力的引导滤波器（CAGF，theCross-Attention-Based GuidedFilter）。由于引导变换的局部线性和边缘保存，CAGF不仅充分集成了RGB和噪声域的互补信息，而且确保了结构信息从噪声域到RGB域的传输。<br/>​  本质上，我们的方法明确地学习了第一阶段的噪声不一致，并在第二阶段利用了这种表示。</p><p>​  我们的贡献如下：<br/>​  1.我们提出了一种新的判别噪声引导方案，该方案明确增强了噪声不一致性的表示和利用。<br/>​  2.我们开发了一种方法来突出伪造区域中的噪声不一致性，使用去噪网络来处理图像，并使用基于统计的约束来优化噪声提取。<br/>​  3.我们设计了一种基于交叉注意力的引导滤波器，它结合了模型驱动和数据驱动技术，充分利用伪造信息的噪声表示，显著增强了噪声不一致对RGB分支的引导效果。</p><p>​  在几个有代表性的基准上进行的广泛实验表明，我们的方法优于最先进的方法，特别是在真实数据集IMD20(Novozamsky,Mahdian, and Saic 2020)上。</p><h1 id="方法">3. 方法</h1><p>​  源图像和目标图像之间的噪声特征不太可能匹配（Zhou et al.2018），篡改操作破坏了自然噪声分布（Wang et al.2022a）。此外，使用噪声可以抑制内容信息，这有利于提取IFDL的语义不可知的特征（Chenet al.2021）。然而，随着篡改和后处理技术的发展，噪声的不一致性并不明显，不再是隐含的关系。我们认为，明确地挖掘和利用噪声不一致性可以进一步提高精度和鲁棒性。<br/>​  因此，我们提出了一种两步策略，来充分利用IFDL的噪声不一致性，包括噪声表示学习和噪声引导网络。<br/>​  首先，我们提出了一种使用去噪网络和自定义约束的学习方案。输入图像用$ X R^{H×W×3} $表示，其中H和W表示图像的高度和宽度。将X输入去噪网络得到图像 $ X^{'} $，然后用BayarConv (Wu, AbdAlmageed, and Natarajan 2019)提取噪声 $ G_dR^{H×W×3} $。去噪网络的选择不是本工作的重点，所以我们使用广泛使用的CBDNet（Guo etal. 2019）来权衡性能和计算量。我们对 $ G_d $施加了基于统计的约束，以确保这两个区域的噪声分布被分开。<br/>​  其次，我们训练噪声引导网络（NGNet,noise-guidednetwork）显式地应用噪声不一致性。NGNet是一个双分支网络，它包含多个基于交叉注意力引导的信号，逐步引导具有噪声不一致的RGB信息，以获得更准确的IFDL。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png"alt="image-20240822204529012" /><figcaption aria-hidden="true">image-20240822204529012</figcaption></figure><h2 id="噪声表示学习">3.1 噪声表示学习</h2><p>​  为了明确地表示噪声的不一致性，我们设计了一个噪声表示学习网络（NRLNet,noise representation learningnetwork），如图2a所示，它使用一个去噪网络来处理图像和统计损失来优化网络。考虑到伪造图像的噪声分布未知，我们使用盲去噪网络CBDNet（Guoet al. 2019)，并加载盲去噪的权值作为初始化。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><p>​  具体的体系结构和培训策略如下所述。<br/>​  我们使用一个基于CBDNet的网络来处理输入的图像X。具体地说，我们预测了一个噪声水平图$~ \hat{l} \in R^{H×W×3} $ ，它可以看作是与噪声分布相关的权值。然后将 $$ 和输入X一起输入编码解码器结构，得到图像 $~ X^{'} \in R^{H×W×3} $，表示为： <spanclass="math display">\[\hat{l}=\mathrm{NE}\left(X\right)\\X^{&#39;}=\mathrm{D}\left(\mathrm{Concat}\left(X,\hat{l}\right)\right)\]</span></p><p>​  其中，Concat表示连接操作。NE是由5层全卷积网络实现的噪声估计模块，卷积核大小为3×3。D是一种U-Net架构，它可以获得具有鉴别噪声的图像。<br/>​  然后，跟随（Chenet al. 2021），我们采用BayarConv从 $ X^{'} $ 中提取噪声 $~ G_d \inR^{H×W×3} $。此外，为了使学习到的噪声更有利于IFDL，我们将噪声输入Res-CNN，以预测粗定位结果$~ G_c \in R^{H×W×1} $。Res-CNN包含10个res-块，其中一个块由两个3×3卷积和ReLU函数组成。<br/>​  <strong>最优化</strong>。为了明确地分离出这两个区域（真实的和伪造的）的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $的噪声。图像中的平稳扰动可以建模为高斯分布（Guo et al. 2019）， $ N_a $和 $ N_f $都可以视为噪声的采样值。因此，我们利用连续高斯分布的JS散度来测量两个区域的噪声分布之间的距离：<span class="math display">\[JSD(P_a||P_f)= \frac {1}{2} KL(P_a||M) +\frac {1}{2} KL(P_f||M)\]</span> ​  式中， $ P_a $ 和 $ P_f $ 分别为 $N_a $ 和 $ N_f $ 的分布，M为 $ 2 $ 。两个高斯分布的KL散度计算如下：<span class="math display">\[KL(P_1||P_2)=log \sigma _2 − log \sigma _1+ \frac { \sigma _1 ^2 + {( \mu _1 - \mu _2 )}^2} {2 \sigma _2 ^2} -\frac {1} {2}\]</span> ​  式中， $~ \sigma_1 $ 、 $~ \sigma_2 $ 为 $ P_1$ 和 $ P_2 $ 的标准差， $~ \mu_1 $ 、 $~ \mu_2 $ 为 $ P_1 $ 和 $ P_2 $的平均值。然后，JSD的计算方法如下：</p><p><spanclass="math display">\[JSD=\log\frac{\sqrt{\sigma_{a}^{2}+\sigma_{f}^{2}}}{2}-\frac{\log\sigma_{a}+\log\sigma_{f}}{2}+\frac{(\mu_{a}-\mu_{f})^{2}}{\sigma_{a}^{2}+\sigma_{f}^{2}}+\frac{1}{2}\]</span></p><p>​  式中， $~ \sigma_a $ 、 $~ \sigma_f $ 为 $ N_a $ 和 $ N_f $的标准差， $~ \mu_a $ 、 $~ \mu_f $ 为 $ N_a $ 和 $ N_f $的平均值。此外，如果仅使用JS散度作为损失函数，网络的优化过程就会发生振荡。因此，我们采用伪造损失定位来辅助网络的优化，这也更有利于神经图像的伪造定位。  我们结合了辅助损失和JS散度组成噪声表示学习的损失函数，可以写成：<spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>​  其中L表示Dice损失（Chen et al. 2021）， $~ Y \in R^{H×W×1} $为groundtruth， $ $ 为平衡两项的超参数，设置为0.80。</p><h2 id="噪声引导网络">3.2 噪声引导网络</h2><p>​  如图2b所示，在NRLNet收敛后，我们将训练好的判别噪声提取器（去噪器和BayarConv）嵌入到噪声引导网络（NGNet）中。与以前的工作不同，我们以显式指导的形式应用噪声。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>​  NGNet的网络架构、基于交叉注意的引导滤波（CAGF）和优化具体如下。<br/>​  <strong>网络架构。</strong>我们利用两个分支来处理RGB和噪声信息。利用训练好的判别噪声提取器获取噪声分支的输入，以更好地提取伪造痕迹。我们使用在ImageNet上预训练的ResNet-50(Deng et al.2009)作为骨干网络。然后，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。最后，我们将提取的具有平卷积层和双线性上采样的特征转换为最后预测掩模$~ G_{out} \in R^{H×W×1} $。  <strong>基于交叉注意力引导的滤波。</strong>现有的IFDL方法直接使用融合策略，不能明确保证噪声域内的篡改伪影得到充分利用。引导滤波器可以保证结构信息在从引导图像到目标图像的传输过程中具有保持边缘的特性(He, Sun, and Tang2012)。受此启发，我们从引导的角度探讨了噪声和RGB信息的融合，从而提出了CAGF，如算法1所示。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502212532274.png"alt="image-20240502212532274" /><figcaption aria-hidden="true">image-20240502212532274</figcaption></figure><p>​  我们在实践中使用了三个CAGF块。传统的引导滤波是由局部线性模型导出的。它通过考虑引导来产生滤波输出结果。  然而，传统的引导滤波是一种不可训练的算法，没有考虑引导与目标之间的相互依赖性，这不适用于IFDL。由于噪声和RGB之间的信息差距很大，简单地将结构信息从噪声传输到RGB就会产生各种伪影。因此，在传统算法的基础上，我们利用注意力机制来计算方差和协方差，并使用卷积层代替平均频率。具体而言，来自噪声流和RGB流的输入特征分别为$~ G_n \in R^{H_s×W_s×C_s} $ 和 $~ G_r \in R^{H_s×W_s×C_s} $ ，我们以 $G_n $ 作为引导图像，以 $ G_r $作为输入图像。首先，我们设计了新的跨模态注意（CMA, cross-modalattention)来获得协方差和方差。以协方差的计算为例，CMA的输入量分别为 $G_n $ 和 $ G_r $ 。CMA利用图3中描述的计算块将它们转换为 $ cov_{nr}R^{H_s×W_s×C_s} $</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171039698.png"alt="image-20240401171039698" /><figcaption aria-hidden="true">image-20240401171039698</figcaption></figure><p>​  具体计算公式如下：</p><p><spanclass="math display">\[\begin{aligned}cov_{nr}&amp;=CMA(G_n,G_r)\\&amp;=Res(C(C(G_n)^T  \otimes C(G_r)) \otimes C(C(G_n) \odotC(G_r)))\end{aligned}\]</span> ​  式中， $ $ 为矩阵乘法， $ $为Hadamard乘积，Res表示包含两个3×3卷积和ReLU函数的Res-块，C表示1×1卷积。我们执行$ G_n $ 和 $ G_r $ 的矩阵乘法获得 $ C R^{N×N} $ （ $ N=H_s×W_s $）而不是传统的引导滤波，并计算两者的Hadamard乘积来代替传统算法的 $mean_n ∗ mean_r $ (He, Sun, and Tang 2012)。在 $ G_n $ 和 $ G_r $的矩阵乘法之前，将两者转换为 $ C_s/r×N $，其中r是一个标量，它降低信道维数以提高计算效率。同样，当CMA的两个输入都是噪声特征$ G_n $ 时，我们可以得到方差 $ var_n R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[var_n = CMA(G_n, G_n)\]</span> ​  res块用于获得系数 $ a R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[a = Res (Concat (cov_{nr}, var_n))\]</span>​  接下来，我们按照算法1中的公式来计算 $ b R^{H_s×W_s×C_s} $ 。受（Wu etal. 2018）的启发，我们使用卷积运算（MFConv）代替平均滤波： <spanclass="math display">\[mean_M = MFConv (M) = \frac {Conv (M)} {Conv (J)}\]</span>​  其中M为MFConv的输入，J为与M大小相同的全一矩阵，Conv为3×3卷积。</p><p>​  最后，我们根据局部线性关系得到CAGF的输出 $ q R^{H_s×W_s×C_s} $ ：<span class="math display">\[q = mean_a \odot G_r + mean_b\]</span>​  其中 $ mean_a $ 和 $ mean_b $ 是a和b经过MFConv后的结果，大小为 $H_s×W_s×C_s $ 。</p><p>​  <strong>检测器。</strong>对于检测器，我们采用了MVSS-Net++ (Dong etal. 2023)提出的ConvGeM，它可以将定位结果 $ G_{out} $ 转换为检测预测 $D_{out} $。ConvGeM通过一个衰减的跳过连接，在检测和定位之间取得了很好的平衡。因此，我们使用ConvGem来获得一个更准确的检测结果：<span class="math display">\[D_{out} = ConvGeM(G_{out})\]</span>​  <strong>最优化。</strong>根据大多数研究 (Chen et al. 2021;Wang et al.2022c)，我们也利用了边缘监督。但是，这并不是这项工作的重点，所以我们使用了一些常见的方  法。跟随（Chenet al. 2021），我们使用Sobel层和残差块，以浅到深的方式获得边缘预测 $ G_eR^{H_e×W_e×1} $ 。对于边缘损失，ground-truth边缘 $ E R^{H×W×1} $被降采样到一个更小的尺寸 $ E^{'} R^{H×W×1} $ 以匹配 $ G_e $。该策略在计算成本和性能方面优于上采样 $ G_e $ 。NGNet的损失可以写成：<span class="math display">\[L_N = \alpha L_1 (Y, G_{out}) + \beta L_2(y, D_{out})+(1 − \alpha − \beta ) L_3 (E^{&#39;},G_e)\]</span> ​  式中 $L_1 $ 和 $ L_3 $ 表示Dice损失（Chen et al. 2021）， $ L_2 $为BCE损失，y表示图像真实性的标签， $ $ ， $ $是平衡损失函数的超参数。实际上， $ $ 设置为0.60， $ $设置为0.2。请注意，真实的图像只用于计算 $ L_2 $ 。</p><h1 id="实验">4. 实验</h1><h2 id="实验设置">4.1 实验设置</h2><p>​  <strong>预训练数据。</strong>我们构建了一个大量的图像篡改数据集，并将其用于预训练我们的模型。该数据集包括三个类别：1）拼接、2）复制移动和3）删除。<br/>​  <strong>测试数据集。</strong>在CASIA (Dong, Wang, and Tan 2013)、Coverage (Wen et al. 2016)、Columbia(Hsu and Chang 2006)、Nist Nimble 2016 (NIST16) (Guan et al.2019)和IMD20(Novozamsky, Mahdian, and Saic2020)上评估我们的模型。我们使用 (Hu et al. 2020; Wang et al.2022b)一样的方法来划分训练，以便进行公平比较。<br/>​  <strong>评估指标。</strong>为了量化定位性能，根据之前的工作（Huet al. 2020；Wang et al.2022b），我们使用像素级曲线下面积（AUC）和F1分数。为了评估检测性能，我们使用了图像级的AUC和F1评分。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h2 id="图像伪造定位">4.2 图像伪造定位</h2><p>​  在两种情况下，我们的模型与其他最先进的篡改定位方法进行了比较：1)在合成数据集上进行训练和对完整的测试数据集进行评估；2)对测试数据集的训练分割和对它们的测试分割进行评估。<br/>​  <strong>预训练模型。</strong>表1a显示了在像素级AUC下的fve数据集上，不同方法的预训练模型在五个数据集上的定位性能。我们将我们的模型NGNet与MantraNet(Wu, AbdAlmageed, and Natarajan 2019), SPAN (Hu et al. 2020), PSCCNet(Liu et al. 2022),ObjectFormer (Wang et al. 2022b), TANet (Shi, Chen,and Zhang 2023) 和HiFi-Net (Guo et al.2023)进行比较。预训练的NGNet在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia上排名第二。特别是NGNet在复制-移动数据集Coverage上达到了94.1%，该数据集图像伪造区域与背景难以区分。这验证了我们的模型具有在噪声域中捕获篡改痕迹的优越能力。我们未能在Columbia上取得最好的表现，在AUC下落后TANet0.2%。我们认为，可能的解释是他们合成的训练数据的分布与Columbia数据集非常相似。表1b的结果进一步支持了这一点，结果显示NGNet在AUC和F1分数方面都优于TANet。此外，值得指出的是，NGNet在较少的训练前数据下取得了不错的结果。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171128950.png"alt="image-20240401171128950" /><figcaption aria-hidden="true">image-20240401171128950</figcaption></figure><p>表1：图像伪造检测和定位结果。(a)训练前模型的定位性能。(b)调整模型的(b)定位性能。(c)对CASIA-D数据集的检测性能。（粗体表示最好，下划线表示第二好）。<br/>  <strong>微调模型。</strong>预训练模型的网络权值用于启动调整模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练分割上进行训练。我们在表1b中评估了不同方法的神经调优模型。对于AUC和F1，我们的模型取得了显著的性能提高。这验证了我们的方法可以通过识别噪声表示学习和基于交叉注意的引导转换来精确捕获细微的篡改痕迹。</p><h2 id="图像伪造检测">4.3 图像伪造检测</h2><p>​  为了避免误警报，我们还考虑了伪造检测任务。根据ObjectFormer（Wang etal. 2022b），我们对（Liu et al.2022）引入的CASIA-D数据集进行了实验比较。如表1c所示，我们的方法具有良好的检测性能，即AUC为99.81%，F1为98.72%。我们的方法明确地建模和利用噪声的不一致，从而准确地区分伪造的图像和真实的图像。</p><h2 id="鲁棒性评估">4.4 鲁棒性评估</h2><p>​  为了分析我们的定位模型的鲁棒性，我们遵循（Wang et al.2022b）中的失真设置来降解NIST16中的伪造图像。这些失真类型包括将图像调整到不同的尺度，应用核大小k的高斯模糊，添加标准偏差σ的高斯噪声，以及使用质量因子q执行JPEG压缩。我们将我们预训练模型的伪造定位性能（AUC分数）与SPAN和ObjectFormer做比较，并报告的结果在表2中。我们的模型对各种失真技术证明了更好的鲁棒性。值得注意的是，JPEG压缩通常是在向社交媒体上载图片时执行的。我们的模型在压缩图像上表现得明显更好。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171143021.png"alt="image-20240401171143021" /><figcaption aria-hidden="true">image-20240401171143021</figcaption></figure><p>表2：NIST16数据集在各种畸变情况下的性能。报告AUC分数（%），（模糊：高斯模糊：噪声：高斯噪声，压缩：JPEG压缩。）</p><h2 id="消融实验">4.5 消融实验</h2><p>​  在本节中，我们进行了实验来证明我们的方法的有效性。噪声表示学习（NRL,noise representationlearning）的设计是为了明确地扩大两个区域之间的噪声分布的差异（真实和伪造）。基于交叉注意力的引导滤波包含跨模态注意力（CMA,cross-modal attention）和引导滤波机制（GF, guided flteringmechanism）。CMA充分集成了RGB和噪声分支中所包含的互补信息，而GF则保证了结构信息从噪声到RGB的传输，并具有保持边缘的特性。为了评估NRL、CMA和GF的有效性，我们将它们从我们的方法中分离出来，并评估它们在CASIA和NIST16上的伪造定位性能。<br/>​  表3显示了定量结果。基线表示我们只使用ResNet-50。可以看出，没有GF，CASIA和NIST16的AUC评分下降了4.5%，而没有CMA，CASIA和NIST16的AUC评分下降了10.9%。此外，当丢弃NRL时，表3的性能严重下降，即AUC为9.5%，F1为20.9%。</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171152148.png"alt="image-20240401171152148" /><br/>  由于不同的去噪器可能产生不同的性能，我们对去噪器的选择进行了消融研究。我们选择DnCNN(Zhanget al. 2017)、FFDNet (Zhang, Zuo, and Zhang 2018)、RIDNet(Anwar andBarnes 2019) 和DRUNet (Zhang et al. 2021)等盲去噪网络进行比较。如表4所示，CBDNet权衡了性能和计算复杂度。<br/><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171207701.png"alt="image-20240401171207701" /></p><h2 id="可视化结果">4.6 可视化结果</h2><p>​  <strong>定性的结果。</strong>如图4所示，我们提供了各种方法的预测掩模。结果表明，该方法不仅能准确地定位篡改区域，而且能开发出清晰的边界。它受益于我们的模型能够明确地扩大两个区域之间的噪声差异，并保持边缘。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222847414.png"alt="image-20240502222847414" /><figcaption aria-hidden="true">image-20240502222847414</figcaption></figure><p>图4：通过不同的方法可视化预测的操作掩码。从上到下，我们展示了伪造的图像、GT面具、ManTraNet、SPAN、PSCC-Net、HiFi-Net和我们的预测。<br/>  <strong>噪声表示学习的可视化。</strong>我们在图5中展示了有和没有NRL的特征的变化。可见，NRL有助于对伪造特征的学习，并获得更准确的伪造区域轮廓。这是因为NRL帮助网络捕获噪声域中的篡改痕迹。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222929983.png"alt="image-20240502222929983" /><figcaption aria-hidden="true">image-20240502222929983</figcaption></figure><p>图5：噪声表示学习和基于交叉注意的引导飞行器的可视化。从左到右，我们显示了没有（w/o）NRL和没有CAGF以及两者都有（NGNet）的特征地图的伪造图像、掩模（Selvaraju等人2017），以及预测。<br/>  <strong>基于交叉注意力的引导滤波器的可视化。</strong>为了验证CAGF的效果，我们在图5中展示了闪烁前后特征的变化。可见，CAGF可以提高伪造定位的精度。没有CAGF的网络将会对类似于伪造文件的物体做出错误的判断。<br/>  <strong>判别噪声表示的可视化。</strong>为了进一步验证我们的方法的动机和有效性，我们分别在图6中展示了提取的没有和有噪声表示学习（NRL）的噪声。可以看出，NRL获得了一个更有鉴别性的噪声表示，这是伪造信息的。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502223135944.png" alt="image-20240502223135944 " style="zoom:67%;" /></p><h1 id="结论">5. 结论</h1><p>​  本文提出了一种包含噪声表示学习和噪声引导网络的两步噪声引导方案。第一步是明确地强调在真实区域和伪造区域之间的噪声分布的可辨别性。在第二步中，设计了一个定制的基于交叉注意力的引导滤波架构，结合了模型驱动和数据分割技术，以增强噪声不一致对RGB分支的引导效果，充分利用伪造噪声表示。我们的工作为解决模糊提取微妙的伪造痕迹的问题提供了一个新的研究策略。在几个基准上的大量实验结果证明了该方案的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> BayarConv </tag>
            
            <tag> JS散度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技巧合集</title>
      <link href="/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p><a href="/MarkDown（一）/">MarkDown（一）：基本语法</a></p><p><a href="/MarkDown（二）/">MarkDown（二）：图表流程图</a></p><p><a href="/MarkDown（三）/">MarkDown（三）：公式语法</a></p><p><a href="/Hexo技巧/">Hexo技巧</a></p><p><a href="/typora技巧/">typora技巧</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization</title>
      <link href="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/"/>
      <url>/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>MGQFormer: Mask-Guided Query-Based Transformer for Image ManipulationLocalization <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://dml.fudan.edu.cn/d1/65/c35285a643429/page.htm"><imgsrc="https://img.shields.io/badge/News-4096ff.svg" alt="new" /></a></center><center><spanclass="math inline">\(\text{KunlunZeng}^*,\text{RiCheng}^*,\text{WeiminTan}^\dagger,\text{BoYan}^\dagger\)</span></center><center>复旦大学智能信息处理上海关键实验室计算机科学学院</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/MGQFormer.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  基于深度学习的模型在图像篡改定位方面取得了巨大的进展，其目标是区分被篡改和真实区域。然而，这些模型在训练效率上存在问题。这是因为它们主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p>  为了解决这个问题，我们提出了一个基于<strong>掩码引导查询</strong>的Transformer框架（MGQProtrer），它使用GroundTruth掩码来指导<strong>可学习查询token</strong>（LQT,learnable query token）识别伪造区域。</p><p>  具体地说，提取GroundTruth掩码的特征嵌入作为<strong>指导查询token</strong>（GQT,guiding querytoken），并将GQT和LQT分别输入到MGQFrorter中来估计篡改区域。然后，我们提出了一种掩模引导损失的位置和形状信息，以减少掩模在GQT和LQT之间的特征距离。我们还观察到，这种掩模引导的训练策略对MGQprort训练的收敛速度有显著影响。在多个基准测试上的大量实验表明，我们的方法显著地改进了最先进的方法。</p><h1 id="引言">引言</h1><p>  在训练过程中，我们使用多分支特征提取器从RGB输入图像中提取空间信道感知特征。它使用两个不同的转换器编码器分别从RGB输入图像及其噪声图中提取特征。然后，利用空间注意和通道注意来融合不同分布和域的RGB图像和噪声图特征。最后，将融合的特征输入到我们提出的基于查询的transformer解码器中，以输出伪造区域在图像中的位置。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164911763.png" alt="image-20240401164911763 " style="zoom:80%;" /></p><p>图1：以前的方法和我们的方法之间的区别。我们的方法使用了一个有效的和可解释的基于查询的Transformer。Token相似度是指图像特征与查询Token之间的标量乘积的softmax结果。此外，我们使用GroundTruth掩码来指导可学习的查询token（LQT）来识别真实的和伪造的区域。</p><p>  为了迫使LQT集中于伪造区域，提取GroundTruth掩模特征作为真实的伪造引导查询token（GQT），并将它们输入解码器，以估计伪造区域的位置。由于GQT来自于地面真实掩模，这是预测掩模的目标，因此GQT将包含伪造区域的空间位置和形状细节。因此，提出了一种掩模引导损耗来减小GQT和LQT之间的特征距离。模型经过训练后，LQT也使网络关注锻造区域的位置和形状。因此，我们只在推理过程中使用LQT来定位基于查询的transformer解码器中被篡改的区域。</p><p>  我们介绍了基于掩模引导的基于查询的Transformer，它包含一个基于查询的Transformer解码器，利用可学习的查询token（LQT）来定位被篡改的区域。</p><p>  我们提出了一种掩模引导训练方法，将从GT掩模中提取的引导查询token（GQT）作为参考LQT。此外，我们设计了掩模引导的损失，以迫使GQT引导LQT集中于被篡改区域的空间位置和形状细节。</p><p>  我们在多个基准测试上进行了广泛的实验，并证明了我们的方法在多个数据集上达到了最先进的性能。</p><h1 id="方法">方法</h1><p>  我们的方法旨在使用基于掩码引导的基于查询的Transformer（MGQprorter）来识别可疑图像中被篡改的区域。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png" alt="image-20240401164940206" style="zoom:50%;" /></p><p>图2：提出的框架MGQprorter的概述，由双分支transformer编码器、融合模块和掩模引导transformer解码器组成。在训练过程中，输入是一个可疑图像（H×W×3）和一个地面真实掩模，输出包括一个预测掩模和一个辅助掩模（H×W×1），这两者都涉及损失计算（Lloc和Laux）。请注意，红线部分是由掩模引导的训练，在推理过程中不需要进行训练。</p><p>  我们将输入图像表示为 $~ X \in R^{H×W×3} $，其中H和W分别为图像的高度和宽度。我们利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征。然后，通过空间和通道注意模块（SCAM,spatialand channel attention module）对多模态特征进行融合。</p><p>  我们设计了两个可学习的查询token（LQT）来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。</p><p>  具体来说，我们将噪声的GT掩模输入MGQFormer，以获得引导查询token（GQT）和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p><h2 id="多分支特征提取器">多分支特征提取器</h2><p>  图像处理定位通常包含复杂的后处理，使得检测微小的差异和伪造痕迹对RGB域具有挑战性。因此，我们采用了一个双分支transformer编码器来完全利用来自两个域的信息。</p><p>  BayarConv 提取噪声特征 $~ X_n \in R^{H×W×3} $。然后将输入的图像和噪声图发送到Transformer编码器。具体地说，我们将X和 $X_n $ 划分为大小为P的补丁，并将补丁重塑为嵌入 $~ X_p \in R^{N×D} $，其中 $ N=HW/P^2 $ 为补丁的数量，D是嵌入的维数。将可学习的位置嵌入 $~pos \in R^{N×D} $ 添加到图像嵌入中，生成序列token $ Z = X_p + pos $，然后通过Transformer层L进行处理。对噪声分支也进行了上述相同的结算。在Transformer编码器之后，两个分支的输出被连接起来，我们得到$~ Z_c \in R^{N×2D} $，用于后续的融合。<br/><br/>  来自不同分支transformer编码器的token具有不同的域和不同的分布。因此，我们使用空间和通道注意模块（SCAM）来完成融合任务。<br/><br/>  我们首先重塑标记$ Z_c $ ，并使用一个卷积层得到 $~ Z_m \in R^{h×w×c} $ ，其中 $ h = H/P,w= W/P,c = D $ 。<br/><br/>  接下来，我们将 $ Z_m $ 和 $ Z_m $的转置分别定义为： <span class="math display">\[V=proj(Z_m) \inR^{hw×c}, K=proj(Z_m) \in R^{hw×c}, Q=transpose(proj(Z_m)) \inR^{c×hw}\]</span>   其中 $ proj $是一个独特的投影层，包括1×1卷积和重塑操作。然后，通道注意模块如下：<span class="math display">\[CAM(Z_m)=proj(V(softmax(QK)))\]</span>  同时，我们继续计算空间注意力，除了转置的Q和K，几乎与通道注意相同。随后，我们可以得到的token如下：<spanclass="math display">\[SAM(Z_m)=proj(softmax(Q^TK^T)V)\\Z_f=CAM(Z_m)+SAM(Z_m)+Z_m\]</span>  然后将图像特征令牌 $~ Z_f \in R^{N×D} $​发送到基于查询的Transformer解码器。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822164324909.png"alt="image-20240822164324909" /><figcaption aria-hidden="true">image-20240822164324909</figcaption></figure><h2 id="掩模transformer解码器">掩模Transformer解码器</h2><p>  我们首先介绍在引用阶段的解码器。</p><p>  对于所提出的基于查询的transformer解码器，我们使用了真实的和伪造的可学习查询令牌$~ LQT \in R^{2×D} $。这些查询被随机初始化，并表示伪造的和真实的特性。具体地说，图像特征token$ Z_f $和LQT由由n个基于Transformer层组成的解码器同时处理。在注意机制过程中，LQT与特征token$ Z_f $ 相互作用，提取丰富的伪造信息。然后，我们得到了图像特征 $ Z_f^∗ $和 $ LQT^∗ $ 。掩码的计算方法如下： <spanclass="math display">\[M^∗=norm(proj(Z_f^∗))∗(norm(proj(LQT^∗))^T\]</span>  其中proj是一个线性层，norm表示 $ L_2 $归一化，我们通过在参考图像特征和可学习查询标记之间执行标量乘积得到 $~M^∗ \in R^{N×2} $ 。<br/><br/>  为了得到最终的掩模，我们将序列重塑为掩模$~ M^{∗∗} \in R^{h×w×2} $ ，并以类的维度应用一个softmax层： <spanclass="math display">\[M=upsample(norm(softmax(M^{∗∗})))\]</span> 其中，$~ M \in R^{H×W} $为预测的掩模，upsample为双线性上采样操作，将掩模的大小调整为与输入图像相同的大小。</p><p>综上所述，我们基于查询的方法利用真实和伪造的LQT来选择与自身高度相似的区域，这使得预测伪造区域的过程更容易解释和有效。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><h2 id="掩码引导的训练过程">掩码引导的训练过程</h2><p>  基于查询的模型在相应的任务中取得了很大的成功。然而，这些模型已经被证明存在查询回复的效率较低的问题。先前的方法已经提出了诸如去噪（Li等人2022a）和掩蔽注意（Cheng等人2022a）等方法。<br/><br/>  我们指出，以往的方法缺乏对伪造区域的位置和形状细节的LQT的直接监督，导致训练无效。这些方法主要利用交叉熵损失的地面真实掩模，优先考虑每像素的精度。<br/><br/>  为了解决这个问题，我们提出了一种掩模引导的训练策略，该策略使用引导查询令牌（GQT）来迫使LQT关注伪造区域的位置和形状。GQT通过提取噪声地面真掩模的特征得到，并利用辅助损失使GQT包含伪造区域的空间和形状信息。从而提高了MGQ前体训练的收敛速度。<br/><br/>  特别地，我们首先会向GroundTruth掩模添加噪声。这一步是因为从原始GroundTruth掩码预测辅助掩码对于transformer解码器和延迟训练来说可能太简单了。我们将点噪声应用于掩模中，类似于DN-DETR（Liet al.2022a）用于盒子去噪训练，以获得更鲁棒的模型。我们随机选择掩模内的点，并倒置原始值来表示不同的区域。此外，我们使用一个调优的参数µ来表示面积的噪声百分比，因此噪声点的数量为µ·HW。<br/><br/>  在噪声掩模的情况下，我们通过卷积网络将掩模转换为GQT，以保持掩模中的空间信息，并将GroundTruth$~ G \in R^{H×W} $ 转换为 $~ GQT \in R^{2×N} $ 。然后，GQT连同图像特征 $Z_f $和LQT一起被发送到Transformer解码器。在解码器中，GroundTruth信息GQT作为与其他查询交互的引导，并帮助解码器重构LQT。<br/><br/>  在Transformer解码器之后，我们得到了由GroundTruthtoken GQT引导的图像特征 $ Z_f^∗ $ 和查询令牌 $ LQT^∗ $ 和 $ GQT^∗ $。通过使用与掩模Transformer解码器部分相同的过程，对 $ Z_f^∗ $ 和 $ GQT^∗$ 进行标量乘积，进一步计算了辅助掩模 $~ M_{aux} \in R^{H×W} $。然后我们让 $ M_{aux} $ 参与到损失的计算中来。</p><h2 id="辅助损失">辅助损失</h2><p>  由于我们使用卷积网络将GroundTruth掩码转换为查询token，并且对掩模加噪声以保持鲁棒性，为了使辅助掩码更加精确，需要对卷积网络进行监督。因此，为了使GQT包含锻造区域的空间和形状信息，我们使用像素级交叉熵损失如下：</p><p><span class="math display">\[L_{aux}=−\sum_{i=1}^{HW}G_i\cdotlog(M_{auc},i)\]</span></p><p>  其中 $~ G \in R^{H×W} $​是GroundTruth掩模。注意，为了得到模型预测的精确掩模，我们使用没有噪声的原始GT掩模G来计算辅助损失。</p><h2 id="掩模引导的损失">掩模引导的损失</h2><p>  GQT的目的是引导LQT，并对两者进行相同的处理，生成预测的掩模M和辅助掩模<spanclass="math inline">\(M_{aux}\)</span>​。因此，我们期望LQT与GQT相似，从而使预测更加精确。我们采用余弦相似度损失来减少两个查询的距离，可以表述为：<span class="math display">\[L_{guide}=1-cos(LQT^*,GQT^*)\]</span>  其中cos为余弦相似度。</p><h2 id="损失函数">损失函数</h2><p>  总损失函数L包括三个部分：使<spanclass="math inline">\(M_{aux}\)</span>精确的辅助损失，使$ LQT^∗ $ 和 $GQT^∗ $ 更接近的掩模引导损失，以及预测掩模M的定位损失<spanclass="math inline">\(L_{loc}\)</span>，其中<spanclass="math inline">\(L_{loc}\)</span>​采用了和辅助损失相同的交叉熵损失：<span class="math display">\[L=L_{loc}+L_{aux}+ \lambdaL_{guide}\]</span></p><p>  其中，$ $是一个权重参数，在训练期间设置为0.5。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>  <strong>测试数据集。</strong>我们首先使用PSCC-Net合成的数据集对我们的模型进行预训练（Liu等人，2022年）。然后，我们在CASIA数据集（Dong、Wang和Tan2013）、Columbia数据集（Hsu和Chang 2006）、NIST16数据集（Guan et al.2019）和IMD20数据集（诺沃扎姆斯基、马赫迪安和Saic2020）上评估我们的模型。特别地，CASIA提供拼接和复制移动图像，这广泛出现在图像伪造场中。Columbia数据集由180张拼接图像组成，它们未压缩，没有经过后处理。NIST16是一个具有挑战性的数据集，它有564张眼睛很难识别的高分辨率图像。IMD20收集了由不同的相机模型捕获的35,000张真实图像，并由不同的内部绘制方法生成的不同类型的操作组成。</p><p>  <strong>评估指标。</strong>为了评估所提出的MGQFrotar的定位性能，在PSCCNet（Liuet al.2022）之后，我们报告了图像级F1评分和曲线下面积（AUC）作为评价度量。我们采用fxed阈值对预测的掩模进行二值化，这是计算f1分数所必需的。</p><p>  <strong>实施细节。</strong>MGQFormer在一个NVIDIA GTX 1080 TiGPU上实现的。所有输入图像的大小都被调整为384×384。我们使用Adam作为优化器，学习率从2.5e-7衰减到1.5e-8，批处理大小为2。特征提取器使用ImageNet预训练的ViT模型权值（Steineret al.2021）初始化，共12层，补丁大小为16，而解码器使用来自6层的截断正态分布的随机权值初始化。</p><h2 id="与最先进的方法的比较">与最先进的方法的比较</h2><p>  我们在两种设置下，将我们的模型与其他最先进的方法进行了比较：</p><p>  1)在合成数据集上进行训练和在完整的测试数据集上进行评估。</p><p>  2)对测试数据集的训练分割和评估其训练分割的预训练模型进行微调。</p><p>  对于预先训练的模型，我们评估的方法有：ManTraNet (Wu, AbdAlmageed,and Natarajan 2019), SPAN (Hu et al. 2020), ObjectFormer (Wang et al.2022), 和ERMPC (Li et al. 2023)，同时进一步比较的方法有： RGB-N (Zhou etal. 2018) 和PSCCNet (Liu et al. 2022)。</p><p>  <strong>预先训练的模型。</strong>表1报告了使用预先训练过的模型获得的最佳定位AUC（%）分数。我们可以观察到，MGQFormer在Columbia、CASIA、IMD20和所有数据集的平均AUC（%）上取得了最高的性能，并在NIST16上获得了具有竞争力的效果。特别是，MGQFormer在现实世界的IMD20数据集上达到88.3%，比ERMPC高2.7%。这验证了我们的方法具有捕获篡改痕迹和泛化到高质量数据集的突出能力。在NIST16数据集上，我们未能达到最佳的性能。我们认为，Transformer网络的性能受到了训练决策的影响。如果测试时的分辨率接近训练，就可以完全实现高性能。然而，NIST16是一个高分辨率的数据集，它大大超过了我们的训练数据集。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185721944.png" alt="image-20240502185721944 " style="zoom:50%;" /></p><p>  <strong>微调模型。</strong>为了补偿合成数据集与标准数据集之间的视觉质量差异，使用预训练模型的网络权值来启动调整模型，该模型将在CASIA数据集的训练分割上进行训练。如表2所示，我们将AUC和F1结果（%）与其他方法进行了比较，我们的模型获得了最好的性能，表明MGQFormer可以通过查询有效地捕获细微的篡改伪。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185818907.png" alt="image-20240502185818907 " style="zoom:40%;" /></p><h2 id="鲁棒性评估">鲁棒性评估</h2><p>  我们对Columbia数据集的原始图像应用不同的图像失真方法，并评估我们的MGQFormer的鲁棒性。失真类型包括：1)用不同的尺度调整图像的大小，2)用核大小k的高斯模糊，3)用质量因子q的JPEG压缩。我们比较了预训练模型在原始数据集和损坏数据上的操作定位性能（AUC分数），并报告了表3中的结果。与以往的方法相比，MGQFormer对所有失真具有最好的鲁棒性。特别是当面对调整大小和JPEG压缩时，我们的方法的性能略有下降，表示补丁MGQFormer对低质量图像具有鲁棒性。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165650581.png"alt="image-20240401165650581" /><figcaption aria-hidden="true">image-20240401165650581</figcaption></figure><h2 id="消融实验">消融实验</h2><p>  MGQFormer的设计包含多分支特征提取器和掩模引导训练。该多分支特征提取器采用了一个额外的BayarConv分支来利用噪声信息，并利用SCAM融合这两个域。利用掩模引导的训练来添加基础GroundTruth信息，引导LQT专注于目标区域，提高查询回复的效率。</p><p>  <strong>噪声分支的消融研究。</strong>定量结果见表4。基线表示我们只使用单个编码器和基于查询的转换器解码器。为了评估噪声分支的有效性，我们使用了一个RGB分支并去除SCAM。我们可以观察到，如果没有噪声分支，哥伦比亚大学的AUC评分下降了1.1%，CASIA大学的AUC评分下降了2.3%。性能提升验证了多分支特征提取器的使用有效地提高了模型的性能。</p><p>  <strong>掩模引导训练的消融研究。</strong>为了证明掩模引导训练的影响，我们在带有图像特征的transformer解码器中只留下LQT，并在训练过程中取出GroundTruth掩模的输入。如表4所示，在没有面具引导训练的情况下，哥伦比亚大学组的AUC评分下降了2.8%，CASIA组下降了3.6%。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165731530.png"alt="image-20240401165731530" /><figcaption aria-hidden="true">image-20240401165731530</figcaption></figure><p>  除了促进定位外，掩模引导训练进一步提高了收敛速度。为了评估这种效果，我们比较了不同时期训练策略的存在和不存在的结果。如图3所示，我们在训练期间显示了合成数据集的验证分割上的AUC（%）分数。事实证明，MGQFrorer在开始时显著促进了训练，在frst时期比没有面具引导训练的模型多了12.7%，并显著加快了收敛速度。这表明，GQT当然有助于transformer解码器提高重构LQT的效率。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165639502.png"alt="image-20240401165639502" /><figcaption aria-hidden="true">image-20240401165639502</figcaption></figure><p>  <strong>GT引导掩膜对应用噪声与否的消融研究。</strong>在图4中，我们展示了参数µ的不同值，表示噪声点的百分比，以验证其对哥伦比亚和IMD20的影响。随着地面真实掩模的增加，有更多的噪声点，得到更鲁棒和广义的模型；然而，较大的值可能会对空间信息造成损害，误导网络。相比之下，较小的µ值提供了一个更准确的地面真实掩模，但模型可能太容易预测辅助掩模和延迟训练。从比较中可以看出，设置为0.01是最优解。点噪声的使用达到了0.9%/1.2%的AUC增益，如表4所示。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165703374.png"alt="image-20240401165703374" /><figcaption aria-hidden="true">image-20240401165703374</figcaption></figure><h1 id="可视化结果">可视化结果</h1><p>  <strong>定性的结果。</strong>如图5所示，我们提供了各种方法的预测伪造掩模。可以观察到PSCC-Net和ManTraNet要么输出错误的区域，要么做出不明确的预测。对可视化结果的比较表明，该方法不仅可以更准确地定位篡改区域，而且还可以输出清晰的区域。它受益于多模态信息和基于查询的transformer解码器，它使用全局注意来生成掩码。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191008224.png"alt="image-20240502191008224" /><figcaption aria-hidden="true">image-20240502191008224</figcaption></figure><p>  <strong>掩膜引导训练的可视化。</strong>为了验证掩模引导训练的有效性，我们展示了MGQFormer预测的掩模，未掩模引导训练生成的掩模，以及图6中的辅助掩模。很明显，MGQFrorer利用地面真实掩模关注伪造区域，从预测掩模与辅助掩模之间的相似性可以看出。具体来说，没有掩模引导训练的网络会对相对较小的物体做出错误的判断。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191142614.png" alt="image-20240502191142614 " style="zoom:70%;" /></p><p>  在图7中，我们进一步展示了表示MGQFormertransformer解码器伪造的LQT注意图与未经掩模引导训练的注意图之间的差异。很明显，在掩膜引导训练中，由于GQT的引导，LQT可以准确地聚焦于目标区域。相比之下，没有掩模引导训练的LQT不能很好地检测伪造，甚至被分配到代表真实位置的完全相反的区域。这一比较表明，所提出的包含来自GT掩模的空间和形状信息的GQT可以迫使LQT集中于我们分配给LQT的正确区域类型。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191302621.png" alt="image-20240502191302621 " style="zoom:67%;" /></p><h1 id="结论">结论</h1><p>  在本文中，我们提出了一种新的基于掩模引导的Transformer框架（MGQFormer）。具体来说，第一步，提取RGB和噪声特征，并进一步融合它们。第二步，将噪声GroundTruth掩码转换为引导查询token（GQT），并将GQT和LQT输入MGQFormer分别估计篡改区域。我们进一步提出了辅助损失和掩模引导损失来指导LQT的重建。可视化结果表明，所提出的掩模引导训练策略对MGQ训练的收敛速度和定位性能有显著影响。在几个基准上的大量实验结果证明了我们的算法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> BayarConv </tag>
            
            <tag> Mask-Guided </tag>
            
            <tag> Query-Based </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/"/>
      <url>/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/</url>
      
        <content type="html"><![CDATA[<p>Learning Transferable Visual Models From Natural LanguageSupervision<a href="https://arxiv.org/abs//2103.00020"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/OpenAI/CLIP"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></p><p><strong>官方解读博客：</strong></p><p><a href="https://openai.com/research/clip">CLIP: Connecting text andimages (openai.com)</a></p><h2 id="clip-论文解读"><strong>1 CLIP 论文解读：</strong></h2><h3 id="背景和动机"><strong>1.1 背景和动机</strong></h3><p><strong>[借助文本的监督方法属于有监督和无监督的一个中间地带]</strong>借助文本的监督方法属于：”借助有限的标注数据进行有监督训练” 和“借助几乎无限量的原始文本进行无监督训练”二者之间的中间地带。相同的是，这两种方式都使用静态的 Softmax分类器来执行预测，缺乏动态输出的机制。这严重限制了它们的灵活性和“Zero-Shot” 能力。</p><p><strong>[CLIP 方法及其结果]</strong>在本文中作者研究了借助大规模自然语言监督训练图像分类器。互联网上存在大量公开可用的无标注文本数据集，作者创建了一个包含4亿对(图像，文本) 的新数据集，并通过对比语言-图像预训练的方式训练了 CLIP模型，是一种从自然语言监督中学习视觉模型的有效新方法。作者发现 CLIP类似于 GPT家族，在预训练期间学习执行一系列任务，包括动作识别，OCR，地理定位，ImageNet-1K图像分类，细粒度图像分类任务等。作者通过在30多个现有数据集上对 CLIP 的“Zero-Shot” 迁移学习性能进行测试，并发现 CLIP可以与有监督训练得到的模型性能相当。比如，CLIP 在 ImageNet-1K上的性能与专门有监督训练的 ResNet-50 相当，但是却没有使用 1.28M 的ImageNet-1K 训练数据集。</p><h3 id="自然语言的监督"><strong>1.2 自然语言的监督</strong></h3><p>本文方法的核心是从自然语言的监督中获得感知能力。只要是你的方法具备这一特点，都可以称之为“接受了自然语言的监督”。那这种方法有哪些优势呢？其一就是可扩展性。因为它不需要经典机器学习方法中大量的有标签数据。</p><h3 id="clip-的数据集"><strong>1.3 CLIP 的数据集</strong></h3><p>本文的一个主要特点是想利用互联网上大量公开可用的数据。由于现有的数据集(MS-COCO 约100,000张，YFCC100M 高质量的仅仅约 15M 张，和 ImageNet-1K大小相似) 不够大，可能会低估这一研究领域的潜力。</p><p>为了解决这个问题，作者构建了一个新的数据集，其中包含4亿对(图像，文本)对，这些数据来自互联网上各种公开可用的资源。而且这个数据清理得非常好，质量是非常高的，这也可能是CLIP 这么强大的主要原因之一。结果数据集的总字数与用于训练 GPT-2 的WebText 数据集相似，因此作者将此数据集称为 WebImageText (WIT)。</p><h3 id="clip-的预训练方法"><strong>1.4 CLIP 的预训练方法</strong></h3><p>本文采取基于对比学习的高效预训练方法。作者的思路是这样的：一开始的方法是联合训练了一个处理图像的CNN 和一个处理文本的 Transformer 模型，来预测图像的caption。这个实验结果如下图1的蓝色曲线所示，可以看到其 Scalability是很差的。橘红色曲线是预测文本的词袋，其效率是蓝色曲线的3倍。这两种方法都有一个关键的相似性，即试图去预测每幅图片对应的文字的确切单词是什么。但我们知道这可不是一件容易的事，因为与同一幅图像对应的描述、注释和相关文本种类繁多。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-25eb6d56491be12552c88613e2a0a3d1_720w.png"alt="img" /></p><p>图1：不同方法的 Zero-Shot ImageNet-1K 精度</p><p>基于最近的图像对比表征学习方面的研究，可以仅预测整个文本与哪个图像配对，而不是该文本的确切单词，实验结果如下图1的绿色曲线所示，其效率是橘红色曲线的4倍。具体的做法是：</p><p><strong>对比学习阶段：</strong><br/>如下图2所示，给定一个 Batch 的N个(图片，文本) 对，图片输入给 Image Encoder 得到表征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，文本输入给 Text Encoder得到表征<span class="math inline">\(I_1\)</span>,<spanclass="math inline">\(I_2\)</span>,…,<spanclass="math inline">\(I_N\)</span>，作者认为<spanclass="math inline">\((I_j,T_j)\)</span>属于是正样本，<spanclass="math inline">\((I_i,T_j)\)</span>属于负样本。最大化N个正样本的Cosine 相似度，最小化<spanclass="math inline">\(N^2-N\)</span>个负样本的 Cosine 相似度。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-65e1dbb935aa7804189dc100783a4940_720w.png"alt="img" />]</p><p>图2：CLIP 的对比学习阶段</p><p>作者从头开始训练 CLIP，不使用 ImageNet-1K 权重初始化 ImageEncoder，也不使用预先训练的权重初始化 Text Encoder。同时使用线性投影(权重为<span class="math inline">\(W_i,W_t\)</span>)将每个编码器的表征映射到多模态的嵌入空间。数据增强只使用随机裁剪，温度系数<spanclass="math inline">\(\tau\)</span>的对数形式随整个模型一起训练。</p><p><strong>Zero-Shot Transfer：</strong>如下图4所示，这个阶段是使用 CLIP的预训练好的 Image Encoder 和 Text Encoder 来做 Zero-ShotTransfer。比如来一张 ImageNet-1K 验证集的图片，我们希望 CLIP预训练好的模型能完成这个分类的任务。但是你想想看，这个 Image Encoder是没有分类头 (最后的 Classifier)的，也就是说它没法直接去做分类任务，所以说呢 CLIP 采用了下面的 PromptTemplate 模式：</p><p>比如来一张 ImageNet-1K 验证集的图片，作者把它喂入 CLIP 预训练好的Image Encoder，得到特征 <spanclass="math inline">\(I_1\)</span>，接下来把所有类别的词汇 “cat”, “dog”等，做成一个 prompt：”A photo of a {object}”，并将这个 prompt 喂入 CLIP预训练好的 Text Encoder，依次得到特征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，最后看<strong>哪个的余弦相似度和<spanclass="math inline">\(I_1\)</span>最高</strong>，就代表<strong>该图片是哪个类别的</strong>。</p><p>那我们就可以注意到貌似这个 prompt 的加入很关键，正好弥补了 ImageEncoder 没有分类头的问题，又正好用上了 CLIP 训练好的 Text Encoder。</p><p>而且重要的是，CLIP 的这种推理的方法摆脱了类别的限制，比如一张“三轮车” 的图片，假设 ImageNet 里面没有 “三轮车” 这个类，那么基于ImageNet 所训练的任何模型都无法正确地讲这个图片分类为 “三轮车” ，但是CLIP 的范式是可以做到的，只需要去做成一个 prompt：”A photo of a{tricycle}”。</p><p>那么我们不禁要问：<strong>其他任务可以像这样使用 prompt吗？或者什么样的 prompt 可以带来 Zero-Shot的性能提升？</strong>作者做了实验发现：</p><ul><li><p>对于细粒度图像分类任务，比如 Oxford-IIIT Pets 数据集，prompt就可以设置为：”A photo of a {label}, a type of pet.”。比如 Food101数据集，prompt 就可以设置为：”A photo of a {label}, a type offood.”。比如 FGVC Aircraft 数据集，prompt 就可以设置为：”A photo of a{label}, a type of aircraft.”</p></li><li><p>对于 OCR 任务，加上一些文本或者数字的引号可以提升性能。</p></li><li><p>对于卫星图像分类数据集，prompt 就可以设置为：”a satellite photoof a {label}.”。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2e0561dbbcf5ae00c5053824d2148c25_720w.png"alt="img" />]</p><p>图4：CLIP 的 Zero-Shot Transfer</p></li></ul><p>作者还开脑洞尝试了通过使用多个上下文的 prompt 来 Ensemble 多个Zero-Shot 分类器，比如一个 prompt 是 ‘A photo of a big {label}”，另一个prompt 是 ‘A photo of a small{label}”。作者观察到这样可以可靠地提高性能。在 ImageNet 上，作者集成了80 个不同的上下文提示，这比上面讨论的单个默认提示提高了 3.5%的性能。当一起考虑时，如下图5所示是 Prompt 工程和 Ensemble策略如何改变一组 CLIP 模型的性能，可以看到 Prompt 工程和 Ensemble 策略将ImageNet 精度提高了近 5%，其中蓝色的线代表直接嵌入类名的结果。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-12b0f33645fcb5ffb3fdcea97a2bc0b6_720w.png"alt="img" />]</p><p>图5：prompt 工程和 Ensemble 对 Zero-Shot 性能的影响</p><h3 id="clip-的模型选择"><strong>1.5 CLIP 的模型选择</strong></h3><p>对于 Image Encoder，作者尝试了改进版的 ResNet-50 和 ViT，对于 TextEncoder，作者使用改进版的 Transformer，作者使用了一个带有8个注意头的 63M参数的12层512宽 Transformer 模型，其输入是一个大小为49152的词汇表的BPE[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_2">2]</a>小写表征。为了计算效率，最大序列长度为76。文本序列用[SOS] 和 [EOS] 令牌括起来，[EOS] 处 Transformer末层的输出被视为文本的特征，然后通过 LN，后接 Linear层投影到多模态空间中。</p><p>至于模型缩放的问题，作者发现对于图像编码器ResNet，同时缩放其深度，宽度，和输入分辨率的效果是最优的。而对于文本编码器Transformer，作者只缩放模型的宽度，使其与 ResNet宽度的计算增量成正比，而无需缩放深度，因为作者发现 CLIP的性能对文本编码器的容量不太敏感。</p><h3 id="零样本迁移-zero-shot-transfer-实验结果"><strong>1.6 零样本迁移(Zero-Shot Transfer) 实验结果</strong></h3><p>本节中的 Zero-Shot是指研究对未见过的数据集的泛化性能，也就是说一个模型训练号以后，在它从未见到过的新数据集上的性能如何。</p><p>作者进一步探索 CLIP 的 Zero-Shot 性能。为了说明这一点，作者比较了CLIP 与基于 ResNet-50完全监督的、正则化的逻辑回归分类器的性能。实验结果如下图7所示，在一共对比的27个数据集中，Zero-ShotCLIP 在16个数据集上面战胜了全监督的 ResNet-50 模型。</p><p>在细粒度分类任务上，可以观察到性能上的广泛差异。在其中两个数据集(Stanford Cars 和 Food101) 上，Zero-Shot CLIP 在 ResNet-50特征上的表现比逻辑回归好 20% 以上，而在另外两个数据集 (Flowers102 和FGVCAircraft) 上，Zero-Shot CLIP 的表现比逻辑回归差 10% 以上。在OxfordPets 和 Birdsnap 上，二者的表现更为接近。</p><p>在 ImageNet、CIFAR10/100、STL10 和 PascalVOC2007 等 “更广义的”分类数据集上，二者的性能相对相似，在所有情况下，Zero-Shot CLIP都有轻微的优势。在 STL10 上，CLIP 在不使用任何训练样本的情况下达到了99.3% 的精度。在 Kinetics70 上，CLIP的表现比ResNet-50高出14.5%，在UCF101 上，Zero-Shot CLIP 的性能也比 ResNet-50 的性能高出7.7%。作者推测这估计是因为与 ImageNet中以名词为中心的对象监督相比，自然语言对涉及动词的视觉概念提供了更广泛的监督。</p><p>也可以看到，Zero-Shot CLIP在一些专业、复杂或抽象的任务上相当弱，如卫星图像分类 (EuroSAT和RESISC45)、淋巴结肿瘤检测 (PatchCamelyon)、合成场景中的物体计数(CLEVRCounts)、与自动驾驶相关的任务，如德国交通标志识别(GTSRB)、识别到最近汽车的距离 (KITTI distance)。这些结果突出了 Zero-ShotCLIP 在更复杂任务中的较差能力。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-9f59b5dfeae10e475d35e3a2b715b0ba_720w.png"alt="img" />]</p><p>图7：Zero-Shot CLIP 与完全监督的基线相比具有竞争力</p><blockquote><p><strong>CLIP 零样本迁移的 Data Efficiency</strong></p></blockquote><p>除此之外，作者还进行了一个有趣的实验，即探究 CLIP的零样本迁移的性能与其他模型的少样本学习性能的比较。这里的其他模型，作者使用的是ImageNet-21K 数据集上面预训练的 BiT-MResNet-152x2。如下图8所示的结果是零样本迁移 (Zero-Shot Transfer) 的 dataefficiency，即少样本学习 (Few-Shot Learning)在样本量为多少时的性能能够跟上 CLIP零样本迁移的性能。可以发现每个数据集的效率差异很大，从有的数据集不到一个标记到有的数据集需要184个标记。比如，Flowers102数据集可以在 1-shot 的情况下就能够跟上 CLIP 零样本迁移的性能，但是像FER2013 数据集在 184-shot 的情况下才能够做得到。平均估计数据效率为每个类20.8 个示例。对于 ImageNet 数据集，CLIP零样本迁移的结果与在相同特征空间上训练的 16-shot线性分类器的结果相当。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-f0b49427382cb9fbf87dbc6ae3eec46d_720w.png"alt="img" />]</p><p>图8：CLIP Zero-Shot Transfer 的 data efficiency</p><h3 id="表征学习-representation-learning-实验结果"><strong>1.7 表征学习(Representation Learning) 实验结果</strong></h3><p>为了更全面地评估 CLIP模型的效果，作者进一步评估了它的表征学习能力。关于表征学习的评估方法，有很多方法来评估某个表征的质量，以及一个“理想”的表征应该具有哪些属性。一种比较常见的方法是冻住模型的骨干部分，只训练最后的分类器，通过在某个数据集上的精度来衡量提取到的特性的好坏。</p><p>如下图9所示是本文关于表征学习研究结果。作者首先研究了[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_3">3]</a>论文的12个数据集，虽然像ResNet-50 和 ResNet-101 这样的小型 CLIP 模型比在 ImageNet-1K上训练的其他 ResNet 表现更好，但它们比在 ImageNet-21K (BiT-M) 上训练的ResNet 表现更差。这些小型 CLIP 模型在具有类似计算需求的情况下，也不如EfficientNet系列的模型。作者继续在27个更多的数据集上做了相关研究，在这个更广泛的评估套件上，CLIP的优势更加明显。所有 CLIP模型，无论规模如何，在计算效率方面都优于其他模型。最佳模型的平均分数的提高从2.6% 增加到 5%。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-04eacca100b96d628b95e2f7538bb332_720w.png"alt="img" />]</p><p>图9：CLIP 模型与最先进的计算机视觉模型 Linear Probe 性能的比较</p><p>作者还研究了 CLIP 的特征在各种各样的数据集上与最佳 ImageNet模型的特征的比较。最佳 ImageNet 模型的特征用的是 Noisy StudentEfficientNet-L2 的最佳模型的特征。结果发现在27个数据集上，CLIP取得了21个数据集的优势。CLIP 在需要 OCR (SST2，HatefulMemes)，地理定位和场景识别 (Country211, SUN397) 的任务上改进最多。此外，CLIP在细粒度的汽车和交通标志识别方面也做得更好 (Stanford Cars 和GTSRB)。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2faa222d0c791900c6e1551b568a3d6f_720w.webp"alt="img" />]</p><p>图10：CLIP 的特征在各种各样的数据集上优于最佳 ImageNet 模型的特征</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo技巧</title>
      <link href="/Hexo%E6%8A%80%E5%B7%A7/"/>
      <url>/Hexo%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="hexo-butterfly支持mermaid">hexo butterfly支持mermaid</h4><p><a href="https://mermaid.js.org/intro/">mermaid官方文档</a></p><p>如果主题本身自带了mermaid，只需要在config里改mermaid:true即可，以下方法针对没有mermaid的主题。</p><h5 id="安装hexo插件">安装hexo插件</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm i hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure><h5 id="配置">配置</h5><p>在config文件里加入以下代码</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># mermaid chart</span><br><span class="line">mermaid: ## mermaid url https://github.com/knsv/mermaid</span><br><span class="line">  enable: true  # default true</span><br><span class="line">  version: &quot;8.13.8&quot; # default v7.1.2</span><br><span class="line">  options:  # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js</span><br><span class="line">    #startOnload: true  // default true</span><br></pre></td></tr></table></figure><h5 id="主题配置">主题配置</h5><p>找到themes_partials.pug文件，加入这一行代码即可<br/>butterfly的路径为_modules-theme-butterfly.pug</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">script(src=&quot;https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js&quot;)</span><br></pre></td></tr></table></figure><h4 id="hexo-部署-github-错误解决方案">hexo 部署 github错误解决方案</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Failed to connect to github.com port 443 after 21074 ms: Couldn&#x27;t connect to server</span><br></pre></td></tr></table></figure><p>解决方案：</p><p>1.通过git配置文件查看是否使用代理：git config --globalhttp.proxy；</p><p>2.通过git取消代理：</p><ul><li>git config --global --unset http.proxy</li><li>git config --global --unset https.proxy</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>typora技巧</title>
      <link href="/typora%E6%8A%80%E5%B7%A7/"/>
      <url>/typora%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h5 id="图片保存路径">图片保存路径：</h5><p>文件</p><ul><li>自动保存</li></ul><p>图像<br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">../../source/postimages/$&#123;filename&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（三）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>转载自https://blog.csdn.net/jzj_c_love/article/details/122279703</p><p>代码都可以在<code>typora</code>中运行，给出的图片链接语法是<code>Ketax</code>，可能有少数的不适用，但基本可以。</p><h5 id="一基本公式">一、基本公式</h5><h6 id="上下标">1. 上下标</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">A_1^2</span><br><span class="line">\\</span><br><span class="line">B_&#123;12&#125;</span><br><span class="line">\\</span><br><span class="line">2^&#123;x^2+y&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A_1^2\\B_{12}\\2^{x^2+y}\]</span></p><h6 id="分数">2. 分数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;x&#125;&#123;1+x^2&#125;</span><br><span class="line">\\</span><br><span class="line">\frac&#123;\frac&#123;1&#125;&#123;2&#125;+x&#125;&#123;y&#125;</span><br><span class="line">\\</span><br><span class="line">\tfrac&#123;a&#125;&#123;b&#125;</span><br><span class="line">\frac&#123;a&#125;&#123;b&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\frac{x}{1+x^2}\\\frac{\frac{1}{2}+x}{y}\\\tfrac{a}{b}\frac{a}{b}\]</span></p><h6 id="开根号">3. 开根号</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sqrt&#123;x&#125;</span><br><span class="line">\sqrt[3]&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\sqrt{x}\sqrt[3]{x}\]</span></p><h6 id="组合数">4. 组合数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\binom&#123;n&#125;&#123;k&#125;</span><br><span class="line">\tbinom&#123;n&#125;&#123;k&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\binom{n}{k}\tbinom{n}{k}\]</span></p><h6 id="导数">5. 导数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">a&#x27;</span><br><span class="line">a&#x27;&#x27;</span><br><span class="line">a^&#123;\prime&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[a&#39;a&#39;&#39;a^{\prime}\]</span></p><h6 id="取模">6. 取模</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">x \pmod a</span><br><span class="line">\\</span><br><span class="line">2\mod&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[x \pmod a\\2\mod{x}\]</span></p><h6 id="积分">7. 积分</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\int_&#123;1&#125;^&#123;2&#125;</span><br><span class="line">\intop_&#123;2&#125;^&#123;1&#125;</span><br><span class="line">\oint</span><br><span class="line">\smallint</span><br><span class="line">\\</span><br><span class="line">\iint</span><br><span class="line">\oiint</span><br><span class="line">\iiint</span><br><span class="line">\oiiint</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\int_{1}^{2}\intop_{2}^{1}\oint\smallint\\\iint\oiint\iiint\oiiint\]</span></p><h6 id="微分">8.微分</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\nabla&emsp;&emsp;</span><br><span class="line">\partial x&emsp;&emsp;</span><br><span class="line">\mathrm&#123;d&#125;x</span><br><span class="line">\dot x&emsp;&emsp;</span><br><span class="line">\ddot y     </span><br><span class="line">\Delta</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\nabla&amp;emsp;&amp;emsp;\partialx&amp;emsp;&amp;emsp;  \mathrm{d}x \dot x&amp;emsp;&amp;emsp;\ddoty     \Delta\]</span></p><h6 id="累积累乘极限">9.累积/累乘/极限</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sum_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\prod_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\lim_&#123;k \to \infty&#125;</span><br><span class="line">\lim\limits_&#123;k \to \infty&#125;</span><br><span class="line">\lim\nolimits_&#123;k \to \infty&#125;]</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\sum_{i=1}^{k}\displaystyle\sum_{i=1}^n\textstyle\sum_{i=1}^n\\\prod_{i=1}^{k}\displaystyle\prod_{i=1}^n\textstyle\prod_{i=1}^n\\\lim_{k\to \infty}\lim\limits_{k \to \infty}\lim\nolimits_{k \to\infty}]\]</span></p><h5 id="二修饰符号">二、修饰符号</h5><h6 id="简单的帽子">1. 简单的帽子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\hat&#123;\theta&#125;</span><br><span class="line">\widehat&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;y&#125;</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\tilde&#123;a&#125;</span><br><span class="line">\widetilde&#123;ac&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;a&#125;</span><br><span class="line">\acute&#123;a&#125;</span><br><span class="line">\check&#123;a&#125;</span><br><span class="line">\grave&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\dot&#123;a&#125;</span><br><span class="line">\ddot&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\vec&#123;a&#125;</span><br><span class="line">\overline&#123;a&#125;</span><br><span class="line">\underline&#123;a&#125;</span><br><span class="line">\underset&#123;min&#125;&#123;a&#125;</span><br><span class="line">\hat&#123;a&#125;</span><br><span class="line">\tilde&#123;a&#125;</span><br><span class="line">\widehat&#123;a&#125;</span><br><span class="line">\widetilde&#123;a&#125;</span><br><span class="line">\dot&#123;a&#125;</span><br><span class="line">\ddot&#123;a&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\hat{\theta}\widehat{AB}\\\bar{y}\overline{AB}\\\tilde{a}\widetilde{ac}\\\bar{a}\acute{a}\check{a}\grave{a}\\\dot{a}\ddot{a}\\\vec{a}\overline{a}\underline{a}\underset{min}{a}\hat{a}\tilde{a}\widehat{a}\widetilde{a}\dot{a}\ddot{a}\]</span></p><h6 id="帽子和袜子">2. 帽子和袜子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overleftarrow&#123;AB&#125;</span><br><span class="line">\overrightarrow&#123;AB&#125;</span><br><span class="line">\overleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\underleftarrow&#123;AB&#125;</span><br><span class="line">\underrightarrow&#123;AB&#125;</span><br><span class="line">\underleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overbrace&#123;AB&#125;</span><br><span class="line">\underbrace&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\underline&#123;AB&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overleftarrow{AB}\overrightarrow{AB}\overleftrightarrow{AB}\\\underleftarrow{AB}\underrightarrow{AB}\underleftrightarrow{AB}\\\overbrace{AB}\underbrace{AB}\\\overline{AB}\underline{AB}\]</span></p><h6 id="盒子和帽子">3. 盒子和帽子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overbrace&#123;a+b+c&#125;^&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\underbrace&#123;a+b+c&#125;_&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\boxed&#123;\pi=3.14&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overbrace{a+b+c}^{\text{note}}\\\underbrace{a+b+c}_{\text{note}}\\\boxed{\pi=3.14}\]</span></p><h6 id="各种括号">4. 各种括号</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">(</span><br><span class="line">\big(</span><br><span class="line">\Big(</span><br><span class="line">\bigg(</span><br><span class="line">\Bigg(</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[(\big(\Big(\bigg(\Bigg(\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">[]</span><br><span class="line">&lt;&gt;</span><br><span class="line">|-2|</span><br><span class="line">\&#123;\&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[[]&lt;&gt;|-2|\{\}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\lgroup x \rgroup</span><br><span class="line">\lVert a \rVert</span><br><span class="line">\lceil 2.6 \rceil</span><br><span class="line">\lfloor 1.2 \rfloor</span><br><span class="line">\ulcorner</span><br><span class="line">\urcorner</span><br><span class="line">\llcorner</span><br><span class="line">\lrcorner</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\lgroup x \rgroup\lVert a \rVert\lceil2.6 \rceil\lfloor 1.2\rfloor\ulcorner\urcorner\llcorner\lrcorner\]</span></p><h5 id="三希腊字母">三、希腊字母</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\text&#123;字母&#125;</span><br><span class="line">\bf&#123;字母&#125;</span><br><span class="line">\mathit&#123;字母&#125;</span><br><span class="line">\pmb&#123;字母&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\text{R}\text{A}\text{C}\text{L}\\\bf{R}\bf{A}\bf{C}\bf{L}\\\mathit{R}\mathit{A}\mathit{C}\mathit{L}\\\pmb{R}\pmb{A}\pmb{C}\pmb{L}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\mathbb&#123;字母&#125;</span><br><span class="line">\mathtt&#123;字母&#125;</span><br><span class="line">\mathrm&#123;字母&#125;</span><br><span class="line">\mathsf&#123;字母&#125;</span><br><span class="line">\mathscr&#123;字母&#125;</span><br><span class="line">\mathfrak&#123;字母&#125;</span><br><span class="line">\cal&#123;字母&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\mathbb{R}\mathbb{A}\mathbb{C}\mathbb{L}\\\mathtt{R}\mathtt{A}\mathtt{C}\mathtt{L}\\\mathrm{R}\mathrm{A}\mathrm{C}\mathrm{L}\\\mathsf{R}\mathsf{A}\mathsf{C}\mathsf{L}\\\mathscr{R}\mathscr{A}\mathscr{C}\mathscr{L}\\\mathfrak{R}\mathfrak{A}\mathfrak{C}\mathfrak{L}\\\cal{R}\cal{A}\cal{C}\cal{L}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\tiny ABCabc</span><br><span class="line">\small ABCabc</span><br><span class="line">\normalsize ABCabc</span><br><span class="line">\large ABCabc</span><br><span class="line">\Large ABCabc</span><br><span class="line">\huge ABCabc</span><br><span class="line">\Huge ABCabc</span><br><span class="line">&#123;\tiny ABC&#125; &#123;\large ABC&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\tiny ABCabc\\\small ABCabc\\\normalsizeABCabc\\\large ABCabc\\\Large ABCabc\\\huge ABCabc\\\Huge ABCabc\\{\tinyABC} {\large ABC}\]</span></p><p><span class="math display">\[\alpha   \beta   \gamma  \Gamma\delta  \Delta \epsilon   \\\\ \zeta   \eta  \theta  \Theta\iota   \kappa    \\\\ \lambda  \Lambda \mu   \nu  \xi  \Xi\pi  \Pi  \\\\ \rho   \sigma  \Sigma \tau   \upsilon  \Upsilon\phi  \Phi  \\\\ \chi   \psi  \Psi \omega  \Omega\]</span></p><p><imgsrc="../postimages/MarkDown（三）/e6a627023fa735a129f1725b85da1fa3.png"alt="image" /><br/><imgsrc="../postimages/MarkDown（三）/99c3a73a6e705c382b6b2acc99920392.png"alt="image" /></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\boldsymbol&#123;\alpha&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\boldsymbol{\alpha}\]</span></p><h5 id="四算术运算符号">四、算术运算符号</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\times</span><br><span class="line">\div</span><br><span class="line">\cdot</span><br><span class="line">\%</span><br><span class="line">\circ</span><br><span class="line">\ast</span><br><span class="line">\star</span><br><span class="line">\otimes</span><br><span class="line">\oplus</span><br><span class="line">\odot</span><br><span class="line">\oslash</span><br><span class="line">\pm</span><br><span class="line">\mp</span><br><span class="line">\dotplus</span><br><span class="line">\divideontimes</span><br><span class="line">\textbackslash</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\times\div\cdot\%\circ\ast\star\otimes\oplus\odot\oslash\pm\mp\dotplus\divideontimes\textbackslash\]</span></p><h5 id="五比较运算符">五、比较运算符</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\=</span><br><span class="line">\equiv</span><br><span class="line">\approx</span><br><span class="line">\approxeq</span><br><span class="line">\cong</span><br><span class="line">\sim</span><br><span class="line">\neq</span><br><span class="line">\not=</span><br><span class="line">&lt;</span><br><span class="line">\&gt;</span><br><span class="line">\le</span><br><span class="line">\ge</span><br><span class="line">\gg</span><br><span class="line">\ll</span><br><span class="line">\curlyeqprec</span><br><span class="line">\curlyeqsucc</span><br><span class="line">\prec</span><br><span class="line">\succ</span><br><span class="line">\preceq</span><br><span class="line">\succeq</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\equiv\approx\approxeq\cong\sim\neq\not=&lt;\&gt;\le\ge\gg\ll\curlyeqprec\curlyeqsucc\prec\succ\preceq\succeq\]</span></p><h5 id="六集合运算符">六、集合运算符</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\in</span><br><span class="line">\owns \not</span><br><span class="line">\subset \not</span><br><span class="line">\supset</span><br><span class="line">\subseteq</span><br><span class="line">\supseteq</span><br><span class="line">\\</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\\</span><br><span class="line">\neg</span><br><span class="line">\emptyset</span><br><span class="line">\varnothing</span><br><span class="line">\\</span><br><span class="line">\because</span><br><span class="line">\forall</span><br><span class="line">\exists</span><br><span class="line">\therefore</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\sqcup</span><br><span class="line">\sqcap</span><br></pre></td></tr></table></figure><p><span class="math display">\[\in\owns \not\subset\not\supset\subseteq\supseteq\\\cap\cup\land\lor\\\neg\emptyset\varnothing\\\because\forall\exists\therefore\cap\cup\land\lor\sqcup\sqcap\]</span></p><h5 id="七各种箭头">七、各种箭头</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\gets</span><br><span class="line">\leftarrow</span><br><span class="line">\to</span><br><span class="line">\rightarrow</span><br><span class="line">\leftrightarrow</span><br><span class="line">\\</span><br><span class="line">\uparrow</span><br><span class="line">\downarrow</span><br><span class="line">\updownarrow</span><br><span class="line">\Leftarrow</span><br><span class="line">\Rightarrow</span><br><span class="line">\Leftrightarrow</span><br><span class="line">\iff</span><br><span class="line">\\</span><br><span class="line">\Uparrow</span><br><span class="line">\Downarrow</span><br><span class="line">\Updownarrow</span><br><span class="line">\nearrow</span><br><span class="line">\searrow</span><br><span class="line">\swarrow</span><br><span class="line">\nwarrow</span><br><span class="line">\longleftarrow</span><br><span class="line">\longrightarrow</span><br><span class="line">\longleftrightarrow</span><br><span class="line">\Longleftarrow</span><br><span class="line">\Longrightarrow</span><br><span class="line">\Longleftrightarrow</span><br><span class="line">\longmapsto</span><br><span class="line">\xrightarrow&#123;over&#125;</span><br><span class="line">\xrightarrow[over]&#123;&#125;</span><br><span class="line">\xrightarrow[under]&#123;over&#125;</span><br><span class="line">\xleftarrow[]&#123;over&#125;</span><br><span class="line">\xleftarrow[under]&#123;&#125;</span><br><span class="line">\xleftarrow[under]&#123;over&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\gets\leftarrow\to\rightarrow\leftrightarrow\\\uparrow\downarrow\updownarrow\Leftarrow\Rightarrow\Leftrightarrow\iff\\\Uparrow\Downarrow\Updownarrow\nearrow\searrow\swarrow\nwarrow\longleftarrow\longrightarrow\longleftrightarrow\Longleftarrow\Longrightarrow\Longleftrightarrow\longmapsto\xrightarrow{over}\xrightarrow[over]{}\xrightarrow[under]{over}\xleftarrow[]{over}\xleftarrow[under]{}\xleftarrow[under]{over}\]</span></p><h5 id="七空间间距">七、空间间距</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A\!B</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\\</span><br><span class="line">A\thinspace B</span><br><span class="line">\\</span><br><span class="line">A\:B</span><br><span class="line">\\</span><br><span class="line">A\ B</span><br><span class="line">\\</span><br><span class="line">A \enspace B</span><br><span class="line">\\</span><br><span class="line">A\quad B</span><br><span class="line">\\</span><br><span class="line">A\qquad B</span><br></pre></td></tr></table></figure><p><span class="math display">\[A\!B\\AB\\A\thinspace B\\A\:B\\A\ B\\A\enspace B\\A\quad B\\A\qquad B\]</span></p><h5 id="八矩阵">八、矩阵</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A=</span><br><span class="line">\begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b &amp; \cdots &amp; c  \\</span><br><span class="line">d &amp; e &amp; \cdots &amp; f  \\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\</span><br><span class="line">g &amp; h &amp; \cdots &amp; j</span><br><span class="line">\end&#123;pmatrix&#125;</span><br><span class="line">\tag&#123;5.1&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A=\begin{pmatrix}a &amp; b &amp; \cdots&amp; c  \\\\d &amp; e &amp; \cdots &amp; f  \\\\\vdots &amp; \vdots&amp; \ddots &amp; \vdots  \\\\g &amp; h &amp; \cdots &amp;j\end{pmatrix}\tag{5.1}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A = \begin&#123;matrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A = \begin{matrix}a &amp; b\\\\c &amp;d\end{matrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">B = \begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;pmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[B = \begin{pmatrix}a &amp; b\\\\c &amp;d\end{pmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C = \begin&#123;vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[C = \begin{vmatrix}a &amp; b\\\\c &amp;d\end{vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">D = \begin&#123;bmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[D = \begin{bmatrix}a &amp; b\\\\c &amp;d\end{bmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E = \begin&#123;Vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;Vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[E = \begin{Vmatrix}a &amp; b\\\\c &amp;d\end{Vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">f(x) &amp;= (x+1)^2\\</span><br><span class="line">&amp;= x^2 + 2x + 1</span><br><span class="line">\end&#123;aligned&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{aligned}f(x) &amp;=(x+1)^2\\\\&amp;= x^2 + 2x + 1\end{aligned}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">f(x) = \begin&#123;cases&#125;</span><br><span class="line">a &amp;\text&#123;if b&#125;\\</span><br><span class="line">b &amp;\text&#123;if a&#125;\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[f(x) = \begin{cases}a &amp;\text{ifb}\\\\b &amp;\text{if a}\\\\\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x + 2y &amp;= 1\\</span><br><span class="line">3x - y &amp;= 5</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{cases}\begin{aligned}x + 2y&amp;= 1\\\\3x - y &amp;= 5\end{aligned}\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">g(x,y)=\left\&#123;</span><br><span class="line">\begin&#123;array&#125;&#123;rcl&#125;</span><br><span class="line">\frac&#123;M_g - d&#125;&#123;M_f-b&#125;[f(x,y)-b]+d       &amp;      &amp; &#123;b      \leq  f(x,y)  \leq M_f&#125;\\</span><br><span class="line">F^*_L     &amp;      &amp; &#123;S_L \leq 0 &lt; S_M&#125;\\</span><br><span class="line">F^*_R     &amp;      &amp; &#123;S_M \leq 0 &lt; S_R&#125;\\</span><br><span class="line">F_R       &amp;      &amp; &#123;S_R \leq 0&#125;</span><br><span class="line">\end&#123;array&#125; \right.</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[g(x,y)=\begin{cases}\begin{array}{rcl}\frac{M_g -d}{M_f-b}[f(x,y)-b]+d       &amp;      &amp; {b      \leq  f(x,y)  \leqM_f}\\\\F^*_L     &amp;      &amp; {S_L \leq 0 &lt;S_M}\\\\F^*_R     &amp;      &amp; {S_M \leq 0 &lt;S_R}\\\\F_R       &amp;      &amp; {S_R \leq 0}\end{array}\end{cases}\]</span></p><p>九、修改字体大小</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AB</span><br><span class="line">\Huge AB</span><br><span class="line">\huge AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\LARGE AB</span><br><span class="line">\Large AB</span><br><span class="line">\large AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\small AB</span><br><span class="line">\tiny AB</span><br></pre></td></tr></table></figure><p><span class="math display">\[AB\Huge AB\huge AB\\AB\LARGE AB\LargeAB\large AB\\AB\small AB\tiny AB\]</span></p><h5 id="十划掉">十、划掉</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\cancel&#123;5&#125;</span><br><span class="line">\bcancel&#123;5&#125;</span><br><span class="line">\xcancel&#123;ABC&#125;</span><br><span class="line">\not =</span><br></pre></td></tr></table></figure><p><span class="math display">\[\cancel{5}\bcancel{5}\xcancel{ABC}\not=\]</span></p><p>十一、常见图形</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\Box</span><br><span class="line">\square</span><br><span class="line">\blacksquare</span><br><span class="line">\triangle</span><br><span class="line">\triangledown</span><br><span class="line">\blacktriangle</span><br><span class="line">\diamond</span><br><span class="line">\Diamond</span><br><span class="line">\star</span><br><span class="line">\bigstar</span><br><span class="line">\circ</span><br><span class="line">\bullet</span><br><span class="line">\bigcirc</span><br><span class="line">\bigodot</span><br><span class="line">\diamondsuit</span><br><span class="line">\clubsuit</span><br><span class="line">\heartsuit</span><br><span class="line">\spadesuit</span><br><span class="line">\angle</span><br><span class="line">\measuredangle</span><br><span class="line">\top</span><br><span class="line">\bot</span><br><span class="line">\infty</span><br><span class="line">\checkmark</span><br><span class="line">\dagger</span><br><span class="line">\ddagger</span><br><span class="line">\yen</span><br><span class="line">\$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\Box\square\blacksquare\triangle\triangledown\blacktriangle\diamond\Diamond\star\bigstar\circ\bullet\bigcirc\bigodot\diamondsuit\clubsuit\heartsuit\spadesuit\angle\measuredangle\top\bot\infty\checkmark\dagger\ddagger\yen\\]</span>$</p><h5 id="十二声明宏">十二、声明宏</h5><p>对于一些复杂但是只有少许不同的表达式，可以声明一个函数来调用，提高源码的可读性，减少出错</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\macroname#1#2&#123;</span><br><span class="line">your command</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>宏允许带任意数量的参数（也可以不带参），必须是#1,#2,……这样的命名格式，同时注意再定义宏的时候注意让#1与，否则会解析成#。再调用的时候格式为，可以参考一下的例子</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\Normal#1#2#3&#123;</span><br><span class="line">\frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;\ #3&#125;\exp&#123;[-\frac&#123;(#1 - #2)^2&#125;&#123;2\ #3^2&#125;]&#125;</span><br><span class="line">&#125;</span><br><span class="line">f(x)=\Normal&#123;x&#125;&#123;u_1&#125;&#123;\sigma_1&#125;\\</span><br><span class="line">f(y)=\Normal&#123;y&#125;&#123;u_2&#125;&#123;\sigma_2&#125;\\</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\Normal#1#2#3{\frac{1}{\sqrt{2\pi}\#3}\exp{[-\frac{(#1 - #2)^2}{2\ #3^2}]}}f(x)=\Normal{x}{u_1}{\sigma_1}\\f(y)=\Normal{y}{u_2}{\sigma_2}\\\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\EXP&#123;</span><br><span class="line">e^x = 1 + x + \frac&#123;1&#125;&#123;2!&#125;x^2 + \frac&#123;1&#125;&#123;3!&#125;x^3  + \cdots</span><br><span class="line">&#125;</span><br><span class="line">\EXP</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\EXP{e^x = 1 + x + \frac{1}{2!}x^2 +\frac{1}{3!}x^3  + \cdots}\EXP\]</span></p><hr /><p>当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax对数学公式进行渲染。如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 =  \begin&#123;vmatrix&#125; </span><br><span class="line">\mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\</span><br><span class="line">\end&#123;vmatrix&#125;</span><br><span class="line">$&#123;$tep1&#125;&#123;\style&#123;visibility:hidden&#125;&#123;(x+1)(x+1)&#125;&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>显示效果：</p>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（二）</title>
      <link href="/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="图表绘制"><strong>图表绘制</strong></h5><h6 id="横向流程图"><strong>横向流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><h6 id="竖向流程图">竖向流程图</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><h6 id="标准流程图"><strong>标准流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### 标准流程图（横向）</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### UML时序图：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><p><br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><br/>###### UML时序图源码复杂样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Title: 标题：复杂使用</span><br><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象B-&gt;小三: 你好吗</span><br><span class="line">小三--&gt;&gt;对象A: 对象B找我了</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br><span class="line">Note over 小三,对象B: 我们是朋友</span><br><span class="line">participant C</span><br><span class="line">Note right of C: 没人陪我玩</span><br></pre></td></tr></table></figure><p><br/>###### UML标准时序图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头</span><br><span class="line">  sequenceDiagram</span><br><span class="line">    participant 张三</span><br><span class="line">    participant 李四</span><br><span class="line">    张三-&gt;王五: 王五你好吗？</span><br><span class="line">    loop 健康检查</span><br><span class="line">        王五-&gt;王五: 与疾病战斗</span><br><span class="line">    end</span><br><span class="line">    Note right of 王五: 合理 食物 &lt;br/&gt;看医生...</span><br><span class="line">    李四--&gt;&gt;张三: 很好!</span><br><span class="line">    王五-&gt;李四: 你怎么样?</span><br><span class="line">    李四--&gt;王五: 很好!</span><br></pre></td></tr></table></figure><p><br/>###### 甘特图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 语法示例</span><br><span class="line">        gantt</span><br><span class="line">        dateFormat  YYYY-MM-DD</span><br><span class="line">        title 软件开发甘特图</span><br><span class="line">        section 设计</span><br><span class="line">        需求                      :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">        原型                      :active,  des2, 2014-01-09, 3d</span><br><span class="line">        UI设计                     :         des3, after des2, 5d</span><br><span class="line">    未来任务                     :         des4, after des3, 5d</span><br><span class="line">        section 开发</span><br><span class="line">        学习准备理解需求                      :crit, done, 2014-01-06,24h</span><br><span class="line">        设计框架                             :crit, done, after des2, 2d</span><br><span class="line">        开发                                 :crit, active, 3d</span><br><span class="line">        未来任务                              :crit, 5d</span><br><span class="line">        耍                                   :2d</span><br><span class="line">        section 测试</span><br><span class="line">        功能测试                              :active, a1, after des3, 3d</span><br><span class="line">        压力测试                               :after a1  , 20h</span><br><span class="line">        测试报告                               : 48h</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（一）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一编写标题">一、编写标题</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 一级标题(h1)</span><br><span class="line">## 二级标题(h2)</span><br><span class="line">### 三级标题(h3)</span><br><span class="line">#### 四级标题(h4)</span><br><span class="line">##### 五级标题(h5)</span><br><span class="line">###### 六级标题(h6)</span><br></pre></td></tr></table></figure><h5 id="二字体">二、<strong>字体</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*斜体文本*</span><br><span class="line">_斜体文本_</span><br><span class="line">**粗体文本**</span><br><span class="line">__粗体文本__</span><br><span class="line">***粗斜体文本***</span><br><span class="line">___粗斜体文本___</span><br></pre></td></tr></table></figure><p><em>斜体文本</em><br/><em>斜体文本</em><br/><strong>粗体文本</strong><br/><strong>粗体文本</strong><br/><strong><em>粗斜体文本</em></strong><br/><strong><em>粗斜体文本</em></strong></p><h5 id="三分割线">三、<strong>分割线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">***</span><br><span class="line"></span><br><span class="line">* * *</span><br><span class="line"></span><br><span class="line">*****</span><br><span class="line"></span><br><span class="line">- - -</span><br><span class="line"></span><br><span class="line">----------</span><br></pre></td></tr></table></figure><hr /><hr /><hr /><hr /><hr /><h5 id="四删除线">四、<strong>删除线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RUNOOB.COM</span><br><span class="line">GOOGLE.COM</span><br><span class="line">~~BAIDU.COM~~</span><br></pre></td></tr></table></figure><p>RUNOOB.COM<br/>GOOGLE.COM<br/><del>BAIDU.COM</del></p><h5 id="五下划线">五、<strong>下划线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;带下划线文本&lt;/u&gt;</span><br></pre></td></tr></table></figure><p><u>带下划线文本</u></p><h5 id="六脚注">六、<strong>脚注</strong></h5><p>脚注是对文本的补充说明。</p><p>Markdown 脚注的格式如下:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[^要注明的文本]</span><br></pre></td></tr></table></figure><p>以下实例演示了脚注的用法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建脚注格式类似这样 [^RUNOOB]。</span><br><span class="line"></span><br><span class="line">[^RUNOOB]: 菜鸟教程 -- 学的不仅是技术，更是梦想！！！</span><br></pre></td></tr></table></figure><h5 id="七markdown列表">七、<strong>Markdown列表</strong></h5><p>Markdown 支持有序列表和无序列表。</p><p>无序列表使用星号(*<strong>)、加号(+</strong>)或是减号(-**)作为列表标记：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* 第一项</span><br><span class="line">* 第二项</span><br><span class="line">* 第三项</span><br><span class="line"></span><br><span class="line">+ 第一项</span><br><span class="line">+ 第二项</span><br><span class="line">+ 第三项</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- 第一项</span><br><span class="line">- 第二项</span><br><span class="line">- 第三项</span><br></pre></td></tr></table></figure><ul><li><p>第一项<br/>* 第二项<br/>* 第三项</p></li><li><p>第一项<br/>+ 第二项<br/>+ 第三项</p></li><li><p>第一项</p></li><li><p>第二项</p></li><li><p>第三项</p></li></ul><h5 id="八列表嵌套">八、<strong>列表嵌套</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 第一项：</span><br><span class="line">    - 第一项嵌套的第一个元素</span><br><span class="line">    - 第一项嵌套的第二个元素</span><br><span class="line">2. 第二项：</span><br><span class="line">    - 第二项嵌套的第一个元素</span><br><span class="line">    - 第二项嵌套的第二个元素</span><br></pre></td></tr></table></figure><ol type="1"><li>第一项：<br/> - 第一项嵌套的第一个元素<br/> -第一项嵌套的第二个元素<br/>2. 第二项：<br/> -第二项嵌套的第一个元素<br/> - 第二项嵌套的第二个元素</li></ol><h5 id="九markdown区块">九、<strong>Markdown区块</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 区块引用</span><br><span class="line">&gt; 菜鸟教程</span><br><span class="line">&gt; 学的不仅是技术更是梦想</span><br></pre></td></tr></table></figure><blockquote><p>区块引用<br/>&gt; 菜鸟教程<br/>&gt; 学的不仅是技术更是梦想</p></blockquote><h5 id="十markdown代码">十、<strong>Markdown代码</strong></h5><p>如果是段落上的一个函数或片段的代码可以用反引号把它包起来（<strong>`</strong>），例如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`printf()` 函数</span><br></pre></td></tr></table></figure><p>演示效果如下：</p><p><code>printf()</code> 函数</p><h5 id="十一代码块">十一、<strong>代码块</strong></h5><p>代码区块使用 <strong>```</strong>包裹一段代码，并指定一种语言（也可以不指定）：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```javascript</span><br><span class="line">$(document).ready(function () &#123;</span><br><span class="line">    alert(&#x27;RUNOOB&#x27;);</span><br><span class="line">&#125;);</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">$(document).ready(function () &#123;</span></span><br><span class="line"><span class="string">    alert(&#x27;RUNOOB&#x27;);</span></span><br><span class="line"><span class="string">&#125;);</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br></pre></td></tr></table></figure><h5 id="十二markdown链接"><strong>十二、Markdown链接</strong></h5><p>链接使用方法如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[链接名称](链接地址)</span><br><span class="line">或者</span><br><span class="line">&lt;链接地址&gt;</span><br></pre></td></tr></table></figure><p>例如：</p><p>这是一个链接 <ahref="https://www.cnblogs.com/caoleiCoding/">云中志</a></p><p>https://www.cnblogs.com/caoleiCoding/</p><h5 id="十三高级链接">十三、<strong>高级链接</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">链接也可以用变量来代替，文档末尾附带变量地址：</span><br><span class="line">这个链接用 1 作为网址变量 [Google][1]</span><br><span class="line">这个链接用 runoob 作为网址变量 [Coding][Coding]</span><br><span class="line">然后在文档的结尾为变量赋值（网址）</span><br><span class="line"></span><br><span class="line">  [1]: http://www.google.com/</span><br><span class="line">  [Coding]: https://www.cnblogs.com/caoleiCoding/</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><p>这个链接用 1 作为网址变量<ahref="http://www.google.com/%3Cbr/%3E%5BCoding%5D:%20https://www.cnblogs.com/caoleiCoding/">Google</a></p><p>这个链接用 runoob 作为网址变量 [Coding][Coding]</p><h5 id="十四markdown图片">十四、<strong>Markdown图片</strong></h5><p>Markdown 图片语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![alt 属性文本](图片地址)</span><br><span class="line"></span><br><span class="line">![alt 属性文本](图片地址 &quot;可选标题&quot;)</span><br></pre></td></tr></table></figure><ul><li>开头一个感叹号 !</li><li>接着一个方括号，里面放上图片的替代文字</li><li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上选择性的‘title’ 属性的文字。</li></ul><p>使用示例：</p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><h5 id="十五markdown表格"><strong>十五、Markdown表格</strong></h5><p>Markdown 制作表格使用 <strong>|</strong> 来分隔不同的单元格，使用<strong>-</strong> 来分隔表头和其他行。</p><p>语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|  表头   | 表头  |</span><br><span class="line">|  ----  | ----  |</span><br><span class="line">| 单元格  | 单元格 |</span><br><span class="line">| 单元格  | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th>表头</th><th>表头</th></tr></thead><tbody><tr class="odd"><td>单元格</td><td>单元格</td></tr><tr class="even"><td>单元格</td><td>单元格</td></tr></tbody></table><h5 id="十六对齐方式">十六、<strong>对齐方式</strong></h5><p><strong>我们可以设置表格的对齐方式：</strong></p><ul><li><strong>-:</strong> 设置内容和标题栏居右对齐。</li><li><strong>:-</strong> 设置内容和标题栏居左对齐。</li><li><strong>:-:</strong> 设置内容和标题栏居中对齐。</li></ul><p>示例如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :-----| ----: | :----: |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th style="text-align: left;">左对齐</th><th style="text-align: right;">右对齐</th><th style="text-align: center;">居中对齐</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr><tr class="even"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr></tbody></table><h5 id="十七缩进">十七、<strong>缩进</strong></h5><p>总结一下缩进的方式：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;能缩进一个汉字，可叠加</span><br><span class="line">&amp;ensp;能缩进半个汉字，可叠加</span><br><span class="line">&amp;nbsp;能缩进四分之一，可叠加</span><br></pre></td></tr></table></figure><p>一般需要所缩进两个汉字，以下方式都可以缩进两个汉字</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;&amp;emsp;</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;</span><br><span class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br></pre></td></tr></table></figure><p> 能缩进一个汉字，可叠加</p><p> 能缩进半个汉字，可叠加</p><p> 能缩进四分之一，可叠加</p><p>  能缩进两个汉字，可叠加</p><h5id="十八markdown高级技巧">十八、<strong>Markdown高级技巧</strong></h5><h6 id="支持的-html-元素"><strong>支持的 HTML 元素</strong></h6><p>不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML撰写。</p><p>目前支持的 HTML 元素有：<code>&lt;kbd&gt;</code><code>&lt;b&gt;</code> <code>&lt;i&gt;</code> <code>&lt;em&gt;</code><code>&lt;sup&gt;</code> <code>&lt;sub&gt;</code><code>&lt;br&gt;</code>等 ，如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑</span><br></pre></td></tr></table></figure><p>使用 <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> 重启电脑</p><h6 id="转义"><strong>转义</strong></h6><p>Markdown使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown使用反斜杠转义特殊字符：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">**文本加粗** </span><br><span class="line">\*\* 正常显示星号 \*\*</span><br></pre></td></tr></table></figure><p>显示效果：</p><p><strong>文本加粗</strong> <br/>** 正常显示星号 **</p>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A New Benchmark and Model for Challenging Image Manipulation Detection</title>
      <link href="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/"/>
      <url>/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<center>A New Benchmark and Model for Challenging Image Manipulation Detection</center><center><spanclass="math inline">\(ZhenfeiZhang^1,MingyangLi^2,Ming-ChingChang^1\)</span></center><center>美国纽约州立大学奥尔巴尼大学计算机科学系</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/2311.14218.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></p><h1 id="摘要">摘要</h1><p>  <strong>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。此外，基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</strong><br/><br/>  为了研究在这些具有挑战性的条件下最先进的（SoTA）IMD方法，我们引入了一个新的具有挑战性的图像操作检测（CIMD）基准数据集，它由两个子集组成，分别用于评估基于编辑和基于压缩的IMD方法。数据集的图像是手工拍摄和篡改高质量的注释。<br/><br/>  此外，我们提出了一种新的基于HRNet的双分支网络模型，该模型可以在这些具有挑战性的条件下更好地检测图像编辑和压缩伪影。在CIMD基准上的大量实验表明，我们的模型在CIMD上显著优于SoTAIMD方法。<br/><br/>本文的贡献包括：</p><ul><li>我们提出了一种新的双分支架构，结合了RGB和频率特征，以实现具有挑战性的图像操纵检测。据我们所知，我们的模型是第一个专注于检测小的被篡改区域的方法。</li><li>我们引入了开创性的压缩伪影学习模型，能够检测双压缩伪影，无论量化因子（QFs）是不同的还是相同的。</li><li>我们引入了一个新的高质量的CIMD基准来评估SoTAIMD方法在具有挑战性的操作中的性能。我们将在书面接受后公开CIMD。</li><li>在CIMD上的大量实验表明，该方法在具有挑战性的图像操作检测方面显著优于SoTA。</li></ul><h1 id="数据集">数据集</h1><p>The Challenging Image Manipulation Detection Dataset(CIMD)</p><p>  在这项工作中，我们的目标是建立一个全面的验证数据集（CIMD），专门用于在压缩和未压缩场景下的小区域伪造（平均小于1.5%）。我们的数据集在数据集大小、图像质量、图像多样性和伪造策略方面都具有优势。<br/><br/>  引入了两个独立的子集来分别评估基于图像编辑和基于压缩的方法。收集我们使用佳能RP相机捕获原始图像，包括未压缩的TIFF和压缩的JPG伪造-原始图像对。这些捕捉是在高度多样的多季节拍摄的，特点是复杂和复杂的照明条件。我们的目的是在现实生活中提供一个公正和全面的模型评估。<br/><br/>  两个CIMD数据集。我们提供了两个子集：</p><blockquote><p>CIMD-Raw子集由成对的原始未压缩的TIFF图像组成，用于评估基于图像编辑的方法。<br/>&gt;CIMD-压缩子集包括拼接伪图像及其对应的原始JPEG图像，其统一量化因子（QFs）范围在50到100之间。这个子集评估了基于压缩的模型在相同的QF条件下检测伪造的能力。</p></blockquote><h3 id="cimd-raw子集cimd-r">CIMD-Raw子集(CIMD-R)</h3><p>  CIMD-R旨在提供一个对基于图像编辑的模型在检测未压缩图像上的小篡改复制移动、对象删除和拼接伪造方面的性能的全面评估。未压缩图像的使用消除了伪造区域上不希望的压缩伪影，否则可以被神经网络感知，使对检测的更真实的性能评估。CIMD-R由600张TIFF图像组成，分辨率为2048×1365。还提供了ground-truth标签。此外，CIMD-R采用了一种面向未来的方法，提供16bit的图像对，可以提供多达<spanclass="math inline">\(2^{48}\)</span>种（以万亿）颜色。<br/><br/>  对于复制-移动操作，将图像的一部分复制和粘贴到同一图像中，然后是五种后处理方法，即缩放、旋转、水平/曲线增加、光照变化和颜色再分配。<br/><br/>  对于删除伪造操作，通过ps中的内容感知填充来从图像中删除选定的区域。内容感知填充被广泛应用于多个数据集（Parketal.2018b；Dong，Wang，和Tan2013b），代表了PS根据周围区域绘制物体的最佳猜测。<br/><br/>  对于拼接伪造操作，将一个图像的区域复制粘贴到另一个图片。然后，采用复制-移动操作中相同的后处理方法，使锻造区域与周围环境相协调。</p><h3 id="cimd-compressed子集cimd-c">CIMD-Compressed子集(CIMD-C)</h3><p>  CIMD-C旨在评估基于压缩的模型在主压缩和二次压缩具有相同的QFs时检测双JEPG压缩伪影的能力。该数据集包含200张JPEG图像，分辨率为2048×1365，其中QF均匀分布为50≤QF&lt;100。<br/><br/>  伪造图像的生成类似于CIMD-R的拼接样本，区别在于伪造图像使用JPEG压缩算法保存，使用与原始图像相同的QF。原始图像由RAW文件生成，确保原始图像第一次被压缩，增强了数据集的可信度。在伪造的图像中，背景是双压缩的，而被篡改的区域是单压缩的。此外，该数据集还包括用于压缩的二进制掩码和QF值，从而增强了其对进一步研究不同QFs的影响的效用。</p><h1 id="提出的imd方法">提出的IMD方法</h1><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><p>  我们提出的双分支架构能够检测异常特征和压缩伪影，其灵感来自于（Kwonet al.2022）。此外，我们的模型可以有效地检测小的操作区域和识别双压缩轨迹，应用相同的量化矩阵（Q-矩阵）。为了实现我们的研究目标，我们采用HR-Net（Wanget al.2020）作为我们模型的支柱，基于其提供三倍收益的能力。首先，HR-Net中没有池化层，这确保了这些特性在整个过程中保持高分辨率。其次，该模型在处理不同尺度的特征的同时，也要处理有效的信息交换，这对于获取不同尺度的信息至关重要。最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。<br/><br/>  为了更精确地定位微小的篡改区域，我们应用面积空间金字塔池（ASPP）（Chen等2017；Yang等2021）和注意机制（Vaswani等2017；胡，沈，和孙2018）仔细设计了模型。<br/><br/>  对于RGB流，输入的图像被输入到一个完整的HR-Net，它从视觉内容中学习图像编辑跟踪。<br/><br/>  对于DCT流，我们向主干提供量化的DCT系数、q矩阵和新的多次重压缩残差DCT系数，以检测双压缩伪影。这种设计工作，不考虑QF是否相同。为了提高所提出的双分支模型的性能，我们在最后引入了一种自适应加权热图聚合设计，使用软选择来融合由两个分支生成的热图。<br/><br/>  其次，该模型在处理不同尺度的特征的同时，还能处理有效的信息交换，这对于获取不同尺度的信息至关重要。<br/><br/>  最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。我们对RGB流应用完整的HR-Net，而对于频率流，我们使用三分辨率的变体HR-Net，用图5中所示的压缩伪影学习模型代替第一阶段。</p><h2id="压缩伪影学习模型compression-artifacts-learning-model">压缩伪影学习模型CompressionArtifacts Learning Model</h2><p>  当使用相同的QF创建拼接图像时，被操纵的区域被单独压缩，而背景区域被双重压缩。因此，当图像被反复压缩时，不稳定的量化DCT系数逐渐集中在被篡改的区域上，而真实的区域则保持相对稳定。在此基础上，我们引入了一种新的残差DCT图来指导DCT特征，以更好地关注IMD的不稳定区域。</p><p>YCbCr 是一种用于压缩彩色图像的色彩空间，其中：</p><ul><li><strong>Y 通道</strong>表示亮度（Luminance），也即图像的灰度级信息。</li><li><strong>Cb 和 Cr 通道</strong>分别表示蓝色差异和红色差异（Chrominance），即色彩信息。</li></ul><p>  我们的方法只关注于y通道DCT图，因为它对人眼更敏感。给定一个JPEG图像，很容易从JPEG文件报头中得到y通道量化的DCT系数<spanclass="math inline">\(Q_0\)</span>及其相应的<spanclass="math inline">\(Q\)</span>矩阵。<br/>首先重复<spanclass="math inline">\(Q\)</span>矩阵具有与<spanclass="math inline">\(Q_0\)</span>相同的大小，我们将重复的<spanclass="math inline">\(Q\)</span>矩阵设为<spanclass="math inline">\(q\)</span>。</p><p>  然后，我们使用以下方程依次计算(k+1)次再压缩量化JPEG系数<spanclass="math inline">\(Q_{k+1}\)</span>：</p><p><spanclass="math display">\[\begin{cases}\begin{array}{l}D_k=Q_k\odotq\\B_k=IDCT(D_k)\\I_{k+1}=RT(B_k)\\Q_{k+1}=[DCT(I_{k+1})\oslashq]\end{array}\end{cases}\]</span>   其中<spanclass="math inline">\(\oslash\)</span>表示元素级划分，D、B、I、Q分别表示去量化的DCT系数、使用逆DCT进行反变换块的DCT系数、使用图像块进行反变换块的DCT系数和使用量化JPEG系数进行反变换块的DCT系数。上述方程中变量的下标表示重压缩的次数，我们实验设置了<spanclass="math inline">\(k=7\)</span>。<spanclass="math inline">\(RT(.)\)</span>是四舍五入和截断操作。<spanclass="math inline">\([.]\)</span>表示该舍入操作。</p><p>  然后，将k次重压缩后的残余去量化DCT系数R定义为：</p><p><spanclass="math display">\[R=\frac{1}{k}\sum_{i=1}^{k}(Q_i-Q_{i-1})\]</span>  对于原始的通道系数<spanclass="math inline">\(Q_0\)</span>​，在把它们转换成一个二进制卷之后，我们使用一个阈值<spanclass="math inline">\(T\)</span>​来执行一个剪切操作，将此二进制值转换表示为：</p><p><span class="math display">\[f:Q_o^{H\times W}\rightarrow\{0,1\}^{(T+1)\times H\times W}\]</span>   DCT系数<spanclass="math inline">\(Q_0\)</span>​被转换为二进制：</p><p><span class="math display">\[f(Q_0(i,j))=\begin{cases}1, &amp;if|clip(Q_0(i,j))|=t,t\in [0,T]\\\\0, &amp;otherwise\end{cases}\]</span></p><p>  利用<span class="math inline">\(clip(.)\)</span>提取<spanclass="math inline">\([−T,T]\)</span>中的直方图特征，这对GPU内存约束是必不可少的。我们实验中将T设为20。此外，我们应用绝对值运算作为DCT直方图显示的对称性。<br/><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /></p><p>  在频率流中，首先将图像输入到图5所示的<strong>压缩伪影学习模型</strong>中，提取各种DCT特征。随后，DCT特性被输入到HR-Net的一个变体中，该变体在三种不同的分辨率（1/8、1/16和1/32）下运行。</p><h2 id="注意力空间金字塔池aspp">注意力空间金字塔池ASPP</h2><p>  为了精确地定位小的篡改区域，我们使用图6(a)所示的注意力空间金字塔池（ASPP）仔细设计了我们的模型。ASPP通过不同的接受域捕获远程距离信息，并处理尺度变化。它由三个具有不同速率的扩张卷积层和一个全局平均池化（GAP）组成。得到的特征被连接并传递到1×1卷积。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095235566.png" alt="image-20240329095235566 " style="zoom:50%;" /></p><h2 id="注意力交互机制">注意力交互机制</h2><p>其中</p><ul><li>左边输入为四个分辨率的分支</li><li>CA：通道注意力ChannelAttention</li><li>UP：双线性上采样BilinearUp-sampling</li><li>SA：空间注意力SpatialAttention</li></ul><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329100333958.png" alt="image-20240329100333958 " style="zoom:45%;" /></p><p>接下来，我们将描述注意力如何在RGB流中交互式地工作，其中的过程实际上与频率流相同，具有不同数量的输出分辨率分支。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329141519802.png" alt="image-20240329141519802 " style="zoom:50%;" /></p><h3 id="通道注意力channelattention">通道注意力ChannelAttention</h3><p>左侧输入为HRnet不同分辨率下的输入：</p><p><span class="math display">\[I\in R^{H \times W \times3}\rightarrow\begin{cases}\begin{aligned}F_1 \in R^{H/4 \times W/4\times C_1} &amp;  &amp; ,C_1=48\\F_2 \in R^{H/8 \times W/8 \times C_2}&amp;  &amp; ,C_2=96\\F_3 \in R^{H/16 \times W/16 \times C_3}&amp;  &amp; ,C_3=192\\F_4 \in R^{H/32 \times W/32 \times C_4}&amp;  &amp; ,C_4=384\\\end{aligned}\end{cases}\]</span>自下而上的通道注意特征的计算使用如下： <span class="math display">\[F_n= C(F_{n+1})\odot F_n, n = 1,2,3\]</span> 其中，<spanclass="math inline">\(C(.)\)</span>表示通道注意块，如下图所示，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。由于<spanclass="math inline">\(F_4\)</span>包含了最高级别的语义信息，因此它在通道级别上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095324781.png" alt="image-20240329095324781 " style="zoom:50%;" /><span class="math display">\[C(F)=\sigma(E(GAP(Conv_{1\times1}(F))))\]</span></p><p>  其中1×1卷积，以减少信道，GAP（·）为全局平均池，激励过程 <spanclass="math inline">\(E(.)=C^{&#39;}\rightarrow C^{&#39;}/r \rightarrowC^{&#39;}，r=4\)</span>，<span class="math inline">\(\sigma(.)\)</span>为Sigmoid激活函数。</p><h3id="双线性上采样bilinearup-sampling">双线性上采样BilinearUp-sampling</h3><p>在应用自底而上的信道注意后，使用双线性上采样方法对特征图<spanclass="math inline">\(F_2\)</span>、<spanclass="math inline">\(F_3\)</span>和<spanclass="math inline">\(F_4\)</span>进行上采样，以匹配<spanclass="math inline">\(F_1\)</span>的分辨率。</p><h2 id="空间注意力spatialattention">空间注意力SpatialAttention</h2><p>应用自上而下路径的空间注意机制，由： <spanclass="math display">\[F_m=S(F_{m-1})\otimes F_m,m = 2, 3, 4,\]</span>其中，S(.)为空间注意力，如下图所示。由于<spanclass="math inline">\(F_1\)</span>包含了丰富的空间信息，因此在空间层面上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329142515201.png" alt="image-20240329142515201 " style="zoom:50%;" /></p><h2 id="热图聚合heatmapaggregation">热图聚合HeatmapAggregation</h2><p>  每个分支的特征图在经过上采样和交互注意后，具有相同的分辨率。然后将这些特征连接在一起，形成最终特征，用于推理阶段的自适应加权热图聚合。</p><p>  我们的模型生成了两个最终的热图，它们通过软选择进行聚合。具体来说，我们采用双线性特征上采样来升级频率流的热图，以匹配RGB流热图的分辨率。然后，我们将Softmax激活函数应用于热图，然后使用全局最大池化（GMP），记为GMP（·），来选择主热图及其相应的权重。这种选择是基于更高的值，这表明与其他热图相比，它具有更强的定位响应。</p><p>  我们使用<span class="math inline">\(h_m\)</span>和<spanclass="math inline">\(h_s\)</span>​​定义主热图和次热图。因此，加权聚合热图h可以表示为：</p><p><span class="math display">\[h = GMP(h_m)\cdot h_m+(1-GMP(h_m))\cdoth_s\]</span></p><p>  最后，我们在预测的二值掩模上应用一个不可训练的GMP来执行图像级检测，因为图像级检测与像素级预测高度相关。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p><strong>数据集。</strong>本研究中使用的训练数据集大多采用了（Kwonetal.2022），其中包括CASIAv2、FantasticReality、IMD2020，以及专门为使用不同QFs检测压缩伪影而设计的数据集。测试阶段需要使用CIMD-R和CIMD-C来分别评估基于图像编辑和基于压缩的方法的有效性。补充材料中提供了有关所使用的数据集的进一步细节，以及与选定的公开可访问的数据集进行的评价指标的比较分析。</p><p><strong>实施细节。</strong>我们的模型是使用PyTorch（Paszke等人，2019年）实现的，并在8个RTX2080GPUs，上进行训练，批处理大小为4。我们将初始学习率设置为0.001，并呈指数衰减。为了减轻不平衡数据集对模型训练的影响，我们从每批的每个数据集中随机选择相同数量的样本。训练过程总共250批。该模型被设计为接受各种图像格式，包括JPEG和非JPEG格式。对于非jpeg图像，该模型采用全1的Q矩阵对样本进行压缩，这相当于使用100的QF进行无损压缩。RGB流的主干使用ImageNet进行预训练（(Krizhevsky,Sutskever,andHinton2017），而DCT流使用（Parketal.2018a）引入的双压缩图像进行预训练的方法。为了提高模型检测小篡改区域的灵敏度，设计了训练目标来<strong>最小化像素级的二值交叉熵损失</strong>。</p><p><strong>比较网络的选择。</strong>为了保证公平的比较和评估之前使用新引入的CIMD的模型，我们选择了使用这两个标准的最先进的方法：(1)预训练模型是公开的，(2)我们使用的评估数据集不在它们的训练集中。根据这些标准，我们选择了RRU-Net、MantraNet、CR-CNN、SPAN、PSCC-Net、MVSS-Net、IF-OSN、CAT-Net、DJPEG和对照品。其中，DJPEG和Comprint被设计用于压缩伪影检测，而CATNet可以联合检测异常特征和压缩伪影。上述所有的研究均在相关的工作部分中被适当地引用。我们使用CIMD-R来评估基于图像编辑的方法，用CIMD-C来评估基于压缩的方法。</p><h2 id="在cimd-r上的评估结果">在CIMD-R上的评估结果</h2><p>  使用CIMD-R子集进行评估。表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213659139.png"alt="image-20240327213659139" /><figcaption aria-hidden="true">image-20240327213659139</figcaption></figure><p>表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。像素级f1评分使用每张图像的最佳f1阈值，并使用固定的f1阈值0.5。最佳分数用粗体突出显示。我们的方法在图像级和像素级的检测任务中都取得了最好的性能。值得注意的是，我们的方法在图像级和像素级评估方面都优于现有的SOTA方法，这表明了它在检测小篡改区域方面的优越性。</p><h2id="在cimd-c上评价基于压缩的方法的评估结果">在CIMD-C上评价基于压缩的方法的评估结果</h2><p>  表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213738541.png"alt="image-20240327213738541" /><figcaption aria-hidden="true">image-20240327213738541</figcaption></figure><p>表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。我们的方法在整体性能方面仍然是表现最好的，突出了我们的方法对于具有相同QF的双压缩图像的有效性。</p><h2 id="消融研究">消融研究</h2><p>  我们提供了一个如表3所示的简单的消融研究。请注意，我们的RGB流在压缩的数据和未压缩的数据中都是有效的。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327214430772.png"alt="image-20240327214430772" /><figcaption aria-hidden="true">image-20240327214430772</figcaption></figure><p>  值得注意的是，由于没有压缩伪影，频率流在CIMD-R中不能产生令人满意的结果。然而，当这两个分支协同工作时，模型的性能在定位和检测评估方面都有所提高。在补充材料中提供了额外的烧蚀研究和实验结果。</p><h1 id="结论">结论</h1><p>  本研究提出了一种新的具有挑战性的图像处理检测（CIMD）数据集，它包括两个子集，分别用于评估基于图像编辑和基于压缩的方法。这些数据集被手动获取和篡改，并提供了高质量的注释。此外，我们提出了一种双分支的方法，在使用CIMD数据集检测图像操作方面优于最先进的模型。我们已经发布了我们的数据集，以促进未来的研究。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> TruFor </tag>
            
            <tag> SRM卷积 </tag>
            
            <tag> BayarConv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization</title>
      <link href="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/"/>
      <url>/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<p>Exploring Multi-Modal Fusion for Image Manipulation Detection andLocalization <a href="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></p><p>希腊信息技术研究所，研究和技术研究中心，希腊塞萨洛尼基</p><details close><br/><summary>论文（arxiv）</summary><br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">post1</a><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/">post2</a><br/><div class="row">    <embed src="/postpdfs/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/2312.01790.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h2 id="摘要">摘要</h2><p>最近的图像操作定位和检测技术通常利用由噪声敏感滤波器产生的法医伪影和痕迹，如SRM和Bayar卷积。</p><p>在本文中，我们展示了在这种方法中常用的不同过滤器擅长于揭示不同类型的操作，并提供互补的法医痕迹。因此，我们探索了合并这些滤波器输出的方法，其目的是利用所产生的伪影的互补性来执行图像操作定位和检测（IMLD）。</p><p>我们提出了两种不同的方法：一种是从每个法医过滤器产生独立的特征，然后将它们融合（称为晚期融合），另一种是执行不同模态输出的早期混合并产生早期组合特征（这称为早期融合）。</p><p>我们证明了这两种方法在图像操作定位和检测方面都取得了具有竞争力的性能，在多个数据集上优于最先进的模型1。</p><h2 id="方法">方法</h2><h3 id="编码器解码器框架">编码器解码器框架</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/architecture.png"alt="Architecture" /><figcaption aria-hidden="true">Architecture</figcaption></figure><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p><h3 id="特征融合方法大模型">特征融合方法（大模型）：</h3><p>首先分别从NoisePrint++、SRM和bayar卷积中提取RGB图像x的辅助特征。然后将每个辅助特征与原始RGB一起输入到一个双分支CMX编码器中，生成4尺度的特征图如图所示：</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213343139.png"alt="image-20240328213343139" /><figcaption aria-hidden="true">image-20240328213343139</figcaption></figure><p>在每个尺度上，3个编码器的输出被连接起来，以产生编码器的最终输出f。我们使用与TruFor中相同的解码器架构来处理异常和置信解码器。</p><h3id="提出的另外一种特征融合方法小模型">提出的另外一种特征融合方法（小模型）：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213412073.png"alt="image-20240328213412073" /><figcaption aria-hidden="true">image-20240328213412073</figcaption></figure><p>再次提取了RGB图像x的辅助特征、rbayar。然后每个输入通过卷积块C，生成早期特征fmod。然后将这3组特征映射连接起来，生成完整的早期特征集fef。这些特征然后通过另一个卷积块C，产生混合特征f mf = C（fef）。混合特征fmf和RGB图像x被用作双分支CMX编码器[34]的输入，其方式与TruFor中的相同。</p><p>这是一种特别轻量级的方法来扩展TruFor架构以处理多个辅助模式，因为它不会显著增加参数的数量（与TruFor的68.7M相比，是68.9M参数）。</p><h2 id="实验">实验：</h2><h3 id="与其他方法在f1参数上的比较">与其他方法在F1参数上的比较：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213436800.png"alt="image-20240328213436800" /><figcaption aria-hidden="true">image-20240328213436800</figcaption></figure><p>其提出的两种特征融合方式除了在DSO-1上不如TruFor，剩下的都好于TruFor<br/>特别是对于只包含复制移动伪造的覆盖数据集，我们的最佳方法比之前的最佳方法TruFor高了6.3%。</p><p>DSO-1数据集主要用于检测包含人的拼接图像。比TruFor落后3%。</p><h3 id="专门与trufor比较结果">专门与TruFor比较结果：</h3><p><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213450185.png"alt="image-20240328213450185" />认为只是误差。</p><h3 id="与其他方法在auc参数上的比较">与其他方法在AUC参数上的比较：</h3><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213506766.png"alt="image-20240328213506766" /><figcaption aria-hidden="true">image-20240328213506766</figcaption></figure><p>可以发现</p><p>小模型显示出了卓越的性能，超过了最先进的平均水平。与之前的领先方法相比，AUC实现了近7%的显著改进，bAcc面实现了9%的显著改进。</p><p>大模型也表现出具有竞争力的AUC性能，但在bAcc方面略落后于TruFor模型。bAcc性能的这种差异可能归因于大模型的大小，这可能容易发生过拟合。进一步的研究和实验需要探索需要额外的正则化技术的可能性，以优化其性能的检测任务。</p><h2 id="消融实验">消融实验：</h2><p>多模态特征输入的消融：</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213528368.png"alt="image-20240328213528368" /><figcaption aria-hidden="true">image-20240328213528368</figcaption></figure><p>在本节中，为了对比各种过滤器（SRM，Bayarconv，NoisePrint++），采用了一个双分支CMX架构，其中每个过滤器作为RGB图像的辅助输入。</p><p>结果见表6。在这个训练过程中，bayar卷积层是可训练的，而SRM和NoisePrint则保持冻结。</p><p>我们可以看到，NoisePrint++基于编辑历史的训练有助于实现DSO-1的最佳性能，其中使用后处理操作覆盖操作，而SRM和bayar在编码和覆盖方面表现更好。</p><p>覆盖范围只包含复制移动操作，噪音打印的相机模型识别可能无法提供足够强大的法医痕迹，而编码道的操作是基于扩散的内画，可能导致不同于传统编辑历史的独特文物。因此，噪音打印在有效处理此类案件时遇到了困难。</p><h2 id="鲁棒性分析"><strong>鲁棒性分析：</strong></h2><p>观察在后处理程度逐渐加深下，模型性能的变化</p><p>在本节中，我们包括对不同质量下降的图像进行的实验，以证明我们的方法的鲁棒性。我们使用Casiav1+数据集，使用不同的内核大小进行高斯模糊，使用不同的质量因子进行JPEG压缩，并与TruFor进行比较。</p><figure><imgsrc="../postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213554181.png"alt="image-20240328213554181" /><figcaption aria-hidden="true">image-20240328213554181</figcaption></figure><p>图4中描述的结果表明，我们的两种融合方法在广泛的降解过程中都表现出良好的鲁棒性，在所使用的所有降解水平上保持了比TruFor的一致优势。</p>]]></content>
      
      
      <categories>
          
          <category> 图像篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/</url>
      
        <content type="html"><![CDATA[<p><a href="/论文总集/">已读论文汇总</a></p><blockquote><p>橘色为A类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/A类-年份-red"alt="A类" /></a><br/>&gt; 黄色为B类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/B类-年份-yellow"alt="B类" /></a><br/>&gt; 绿色为C类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/C类-年份-green" alt="C类" /></a></p></blockquote><h3 id="image-tampering">Image Tampering</h3><h4 id="image-editing">Image Editing</h4><details open><br/><summary>2024</summary><ul class="task-list"><li><label><input type="checkbox" checked="" /><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection and Localization</a><ahref="https://link.springer.com/chapter/10.1007/978-3-031-53311-2_15"><imgsrc="https://img.shields.io/badge/MMM-2024-green" alt="MMM" /></a> <ahref="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image Manipulation Detection</a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-red" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/UnionFormer/">UnionFormer: Unified-Learning Transformer withMulti-View Representation for Image Manipulation Detection andLocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-red"alt="CVPR" /></a></label></li><li><label><input type="checkbox" /><a href="/DH-GAN/">DH-GAN: Imagemanipulation localization via a dual homology-aware generativeadversarial network</a> <ahref="https://doi.org/10.1016/j.patcog.2024.110658"><imgsrc="https://img.shields.io/badge/PR-2024-yellow"alt="PR" /></a></label></li></ul></details><details open><br/><summary>2023</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2023-red" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2212.10957"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/grip-unina/TruFor"><imgsrc="https://img.shields.io/github/stars/grip-unina/TruFor?style=flat"alt="GitHub" /></a> <a href="https://grip-unina.github.io/TruFor/"><imgsrc="https://img.shields.io/badge/project-blue"alt="project" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/WACV2023/html/Niloy_CFL-Net_Image_Forgery_Localization_Using_Contrastive_Learning_WACV_2023_paper.html"><imgsrc="https://img.shields.io/badge/WACV-2023-yellow" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a> <ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></label></li><li><label><input type="checkbox" /><ahref="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/">Pre-training-freeImage Manipulation Localization through Non-Mutually ExclusiveContrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/Knightzjz/NCL-IML"><imgsrc="https://img.shields.io/github/stars/Knightzjz/NCL-IML?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/">Uncertainty-UncertaintyLearning for Improving Image Manipulation Detection</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Uncertainty-guided_Learning_for_Improving_Image_Manipulation_Detection_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red"alt="ICCV" /></a></label></li></ul></details><details close><br/><summary>2022</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/PSCC-Net-Progressive-Spatio-Channel-Correlation-Network-for-Image-Manipulation-Detection-and-Localization/">PSCC-Net:Progressive Spatio-Channel Correlation Network for Image ManipulationDetection and Localization</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903/"><imgsrc="https://img.shields.io/badge/TCSVT-2022-yellow" alt="TCSVT" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2103.10596"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/proteus1991/PSCC-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" /><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903"><imgsrc="https://img.shields.io/badge/TPAMI-2022-red" alt="TPAMI" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2112.08935"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/dong03/MVSS-Net"><strong>Code</strong></a><strong>]</strong></label></li></ul></details><h3 id="tamper-text-in-detection">Tamper Text in Detection</h3><p>图像中的<strong>文本篡改检测</strong>问题 (parts of)</p><ul class="task-list"><li><label><input type="checkbox" />Image-based Freeform HandwritingAuthentication with Energy-oriented Self-Supervised Learning <ahref="https://arxiv.org/abs/2408.09676"><imgsrc="https://img.shields.io/badge/TMM_&#39;24-dc3545"alt="paper" /></a></label></li><li><label><input type="checkbox" />Generalized Tampered Scene TextDetection in the era of Generative AI <ahref="https://arxiv.org/abs/2407.21422"><imgsrc="https://img.shields.io/badge/arXiv_&#39;24-6c757d"alt="paper" /></a></label></li><li><label><input type="checkbox" />A Two-Stage Dual-Path Framework forText Tampering Detection and Recognition <em>(arXiv '24)</em><strong>[<ahref="https://arxiv.org/abs/2402.13545">Paper</a>]</strong></label></li><li><label><input type="checkbox" />CTP-Net: Character TexturePerception Network for Document Image Forgery Localization (<em>arXiv'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2308.02158v1"><strong>Paper</strong></a><strong>]</strong><a href="https://github.com/FCTMdataset/FCTM"><imgsrc="https://img.shields.io/github/stars/FCTMdataset/FCTM?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" />Toward Real Text ManipulationDetection: New Dataset and New Solution <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2312.06934">Paper</a>]</strong> <strong>[<ahref="https://github.com/DrLuo/RTM">Code</a>]</strong></label></li><li><label><input type="checkbox" />Towards Robust Tampered TextDetection in Document Image: New dataset and New Solution (<em>CVPR'23</em>) <strong>[</strong><ahref="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf"><strong>Paper</strong></a><strong>]</strong><strong>[<ahref="https://github.com/qcf-568/DocTamper">Code</a>]</strong></label></li><li><label><input type="checkbox" />Progressive Supervision forTampering Localization in Document Images (<em>ICONIP '23</em>)<strong>[<ahref="https://link.springer.com/chapter/10.1007/978-981-99-8184-7_11">Paper</a>]</strong></label></li><li><label><input type="checkbox" />SigScatNet: A Siamese + Scatteringbased Deep Learning Approach for Signature Forgery Detection andSimilarity Assessment <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/pdf/2311.05579.pdf">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Image Generation and LearningStrategy for Deep Document Forgery Detection <em>(arXiv '23)</em><strong>[<ahref="https://arxiv.org/abs/2311.03650">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Forgery-free signature verificationwith stroke-aware cycle-consistent generative adversarial network<em>(Neurocomputing '22)</em> <strong>[<ahref="https://doi.org/10.1016/j.neucom.2022.08.017">Paper</a>]</strong><strong>[<ahref="https://github.com/KAKAFEI123/Stroke-cCycleGAN">Code</a>]</strong></label></li><li><label><input type="checkbox" />Document Forgery Detection in theContext of Double JPEG Compression <em>(ICPR '22)</em> <strong>[<ahref="https://link.springer.com/chapter/10.1007/978-3-031-37745-7_5">Paper</a>]</strong></label></li></ul><p>datasets 下载:</p><ul><li><p><ahref="https://github.com/namtpham/casia2groundtruth">Casiav2</a></p></li><li><p><ahref="https://github.com/mjkwon2021/CAT-Net#1-downloading-tampcoco--compraise">tampCOCO</a></p></li><li><p><ahref="http://staff.utia.cas.cz/novozada/db/">IMD2020</a></p></li><li><p><ahref="http://zefirus.org/articles/9f78c1e9-8652-4392-9199-df1b6a6c1a3d/">FantasticReality</a></p></li><li><p><ahref="https://github.com/namtpham/casia1groundtruth">Casiav1</a>(创建Casiav1+数据集需要corel数据集)</p></li><li><p><ahref="https://www.kaggle.com/datasets/elkamel/corel-images">corel</a></p></li><li><p><ahref="https://github.com/grip-unina/TruFor#cocoglide-dataset">CocoGlide</a></p></li><li><p><ahref="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/">Columbia</a></p></li><li><p><a href="https://github.com/wenbihan/coverage">COVER</a></p></li><li><p><ahref="https://recodbr.wordpress.com/code-n-data/#dso1_dsi1">DSO-1</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（二）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一stderr-log打印">一、stderr log打印</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sys.stderr = <span class="built_in">open</span>(<span class="string">&quot;errors.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eprint</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(*args, file=sys.stderr, **kwargs)</span><br></pre></td></tr></table></figure><p>这样代码中使用eprint()方法输出的信息会保存在根目录errors.txt文件中</p><h5 id="二机器人状态控制">二、机器人状态控制</h5><p>机器人的状态由变量self.gooding控制，其中参数与其对应的状态如下：</p><table><colgroup><col style="width: 12%" /><col style="width: 60%" /><col style="width: 17%" /><col style="width: 10%" /></colgroup><thead><tr class="header"><th>self.gooding</th><th>状态</th><th>判断条件</th><th>下一个状态</th></tr></thead><tbody><tr class="odd"><td>0</td><td>机器人处于空闲状态，可以通过self.getTarget()方法，得到要去往的goods目标</td><td>无</td><td>1</td></tr><tr class="even"><td>1</td><td>机器人已经找到目标goods，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>1.5</td></tr><tr class="odd"><td>1.5</td><td>机器人已经得到goods，判断是否get成功</td><td>self.goods == 0</td><td>0</td></tr><tr class="even"><td></td><td></td><td>self.goods == 1</td><td>2</td></tr><tr class="odd"><td>2</td><td>机器人处于取得goods状态，可以通过self.getBerthTarget()方法，得到要去往的berth目标</td><td>无</td><td>3</td></tr><tr class="even"><td>3</td><td>机器人已经找到目标berth，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>0</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（一）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="赛题背景">【赛题背景】</h5><ul><li><p>华为基于自身ICT基础设施能力，通过对港口场景的洞察和理解，以智慧、绿色、高效、安全为目标，数字化、智能化为手段，联合生态伙伴，助力世界一流港口建设。</p></li><li><p>本次赛题抽象自华为云智能港口真实业务难题，选手通过算法完成运输船只智能泊靠、运输机器人智能拣货装货等任务，以最大化提升港口物流效率。</p></li></ul><h5 id="数据集说明"><strong>【数据集说明】</strong></h5><ul><li>初赛练习赛每天使用1张地图进行判题。</li><li>初赛正式赛有3张地图，取3张图的总分作为当次判题成绩。</li><li>初赛练习赛共8张地图，地图数据完全公开给大家下载。练习阶段前8日，每日公开当日判题地图，方便大家本地调试。</li></ul><p><strong>【官方论坛】</strong></p><ul><li><h5id="华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云-huaweicloud.com"><strong><ahref="https://bbs.huaweicloud.com/forum/forum-0168144383617537003-1.html">华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云(huaweicloud.com)</a></strong></h5></li><li><strong><ahref="https://bbs.huaweicloud.com/forum/thread-0239145895671582004-1-1.html">初赛练习阶段赛题相关材料及配套软件（Windows版本）整合版_2024华为软件精英挑战赛_华为软件精英挑战赛_华为云论坛(huaweicloud.com)</a></strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
