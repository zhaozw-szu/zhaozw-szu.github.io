<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</title>
      <link href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/"/>
      <url>/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/</url>
      
        <content type="html"><![CDATA[<center>Attentive and Contrastive Image Manipulation Localization With BoundaryGuidance <a href="https://ieeexplore.ieee.org/document/10589438"><imgsrc="https://img.shields.io/badge/TIFS-2024-orange" alt="TIFS" /></a></center><center>Wenxi Liu , <em>Member, IEEE</em>, Hao Zhang , Xinyang Lin , Qing Zhang, Qi Li , Xiaoxiang Liu , Ying Cao</center><h1 id="摘要">摘要</h1><p>​  近年来，图像生成技术的快速发展，导致了对操纵图像的广泛滥用，导致了信任危机，影响了社会公平。因此，我们工作的目标是检测和定位图像中被篡改的区域。许多基于深度学习的方法已经被提出来解决这一问题，但它们很难处理手动微调到图像背景的篡改区域。通过观察缓和区域的边界对篡改和非篡改部分的分离至关重要，我们提出了一种新的边界引导的图像操作检测方法，它引入了利用篡改区域边界信息的固有偏差。我们的模型采用编译码器结构，采用多尺度定位掩码预测，并通过注意机制和对比学习来引导下利用先验边界知识。特别地，我们的模型因为如下原因是独特的，1)我们在网络解码器中提出了一个边界感知注意模块，该模块预测被篡改区域的边界，从而将其作为关键的上下文线索来促进定位；2)我们提出了一种多尺度的对比学习方案，具有新的边界引导采样策略，从而产生更多的区别定位特征。我们在几个公共基准上的最新表现证明了我们的模型相对于之前的作品的优越性。<br/>​  索引术语-图像操作检测/定位。</p><h1 id="引言">1. 引言</h1><p>​  我们的论文的目标是在像素级别上定位不同类型的图像操作（包括拼接、复制-移动和删除）。主要的挑战在于难以区分被篡改和未被篡改的区域，特别是被篡改的区域是从原始图像中复制的，它们被仔细地微调。因此，被篡改区域和未被篡改区域之间的差异变得很小。之前的方法旨在学习特定于任务的显著特征[6]，[7]，[8]，[9]，但要么它们只能处理特定的操作类型[10]，[11]，[12]，[13]，[14]，[15]，要么它们很容易被精心操作的图像混淆。<br/>​  在被操纵的图像中，被篡改区域的边界是分离被操纵和未被操纵像素的关键位置，在定位被操纵区域时应特别注意并明确利用这一点。然而，如何利用这些边界信息来提高检测被操纵图像区域的性能仍有待探索。<br/>​  在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。首先，为了进一步增强操作定位，我们鼓励该模型关注一个被篡改区域周围的边界，其中经常存在非自然的混合。其次，受对比学习[16]、[17]、[18]、[19]、[20]的启发，我们寻求学习一个特征空间，即篡改区域内的点远离篡改区域边界附近的非调和区域点，以获得更强大的特性来定位篡改区域。<br/>​  与之前天真地利用边界信息[21]的工作相比，如联合预测篡改边界和掩模（见图5），我们的专注和对比的方法提供了一种新颖的、更复杂的利用边界信息的方法，并被证明比之前的方法更有效。<br/>​  在注意方面，在我们的框架的解码层中，我们提出了一种新的基于交叉注意的边界感知模块，旨在提取图像中被篡改区域的边界，从而使模型进一步集中于被篡改区域的边界。特别是，边界感知注意模块利用跳连编码特征与前一层解码特征的相关性，提取被篡改区域的边界，进一步用于生成图像篡改定位的掩模。<br/>​  在特征学习方面，我们提出的模型是基于一个典型的编解码器架构及其特征学习监督由一个新颖的对比目标函数[16]，[22]，[23]，表示为边界引导篡改对比损失，为了推动分开特征采样的篡改和非篡改区域，从而学习更多的区别特征表示。为此，我们采用边界引导的采样策略来收集负训练对，其中我们在被篡改区域的边界周围采样负样本，而不是整个非被篡改区域。该采样方案不仅鼓励模型关注存在非自然混合的边界区域，而且减轻了未篡改区域内巨大变化引起的干扰（见图1中的可视化特征）。<br/>​  为了进行评估，我们在几个公共数据集上进行了实验，包括CASIA[24]、Conbyea[25]、Coverage[26]和NIST16[27]。通过将我们的方法与之前的方法进行比较，证明了我们提出的模型可以达到最先进的性能。总之，我们的工作贡献包括：</p><ul><li>我们提出了一种新的边界引导图像操纵定位模型，该模型通过精心设计的注意力和对比学习机制充分利用被篡改区域的边界信息，而不是以往工作中使用边界信息的简单策略。</li><li>我们在框架的解码器中引入了一个边界感知注意模块，旨在指导模型通过提取被操纵区域的边界来强调图像操作的非自然混合。</li><li>我们提出了一种边界引导的篡改对比损失，鼓励模型将样本的边缘从篡改和非篡改区域扩大到最大的程度。</li><li>我们在几个基准测试上对我们的方法与现有的方法进行了广泛的评估和比较，并表明我们的方法达到了最先进的性能。</li></ul><h1 id="相关工作">2. 相关工作</h1><p>​  在本节中，我们将介绍有关图像操作检测/定位、深度伪造检测和对比学习的相关文献。</p><h2 id="a.-图像操作检测定位">A. 图像操作检测/定位</h2><p>​  由于操作特定的图像区域不可避免地会在被篡改区域与其周围区域之间留下痕迹，因此有几种方法利用边界信息来有利于操作检测[5]、[21]、[35]。多任务全卷积网络（MFCN）[35]提出利用两个输出分支来定位剪接区域的边界。作为一个基于gan的模型，GSR-Net[21]通过合成现有数据集的操纵图像来学习检测图像操作，通过篡改区域及其边界共同监督。参考文献[5]提出了一个双分支网络MVSS-Net，它融合了噪声分布和通过Sobel提取的边缘信息来完成图像操作定位。我们的方法共享一个寻求利用操纵边界信息的高级思想，但探索了两种新的方法，利用对比学习和注意机制来纳入边界先验，这在之前的图像操纵检测的工作中没有研究过。</p><h2 id="b.-深度伪造检测">B. 深度伪造检测</h2><p>​  近年来，随着生成模型的发展，深度伪造检测的任务已经引起了[40]、[41]、[42]、[43]、[44]、[45]等研究者的关注。它的目的是识别人脸的表情甚至身份被操纵的图像。从本质上，深度伪造检测解决了一个图像级的二值分类问题。与深度伪造检测任务不同，我们的图像操作定位任务需要估计被操纵的图像区域的位置，这是一个像素级的预测任务。</p><h2 id="c.-对比学习">C. 对比学习</h2><p>​  无监督/自监督学习方法[20]、[22]、[46]、[47]一般包括借口任务和损失函数两个方面。它们都致力于更好地学习数据表示。近年来，对比学习损失在[16]、[22]、[23]、[48]等方面取得了显著进展。这些方法通过从正对中关闭样本并将样本从负对中推开来学习表征。参考[49]使用一个内存库来存储实例类表示向量，并通过区分不同的实例和特征表示来提出实例级对比学习。其他工作[50]，[51]探索选择一批中的阳性和阴性样本，而不是一个记忆库。MoCo[23]提出了无监督的视觉表示学习，它从比较学习的角度构建了一个带有队列和移动平均编码器的动态字典。对于图像操作，最近的一项工作，CFL-Net[52]，提出使用对比度学习来分离未被篡改和被操纵的补丁嵌入的分布。相比之下，我们提出了一种边界引导的抽样策略，以寻找更多信息的负对，使我们学习的特征更具鉴别性。</p><h1 id="我们的方法">3. 我们的方法</h1><h2 id="a.-网络概述">A. 网络概述</h2><p>​  我们工作的目标是在像素级上检测和定位被篡改的区域。我们所提出的模型的体系结构如图2所示。<br/><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /></p><p>图2。左图：我们的框架的概述。将输入篡改图像的编码特征通过一个混合注意模块和多个边界感知注意模块来预测被篡改区域和边界的多尺度掩模。每个尺度的解码器还通过边界引导的篡改对比损失进行监督。右图：边界引导的篡改对比损失。给定解码器的特征图，采用对比学习方法对属于同一区域（即篡改或未篡改区域）的点特征进行分组，同时分离不同区域的点特征。点特征的采样的篡改区域被限制在外部边界区域的篡改区域和硬对挖掘方法用于使模型关注的类型难以处理积极对两个遥远的样本（硬正对）和负对与两个相近的样本（硬负对）。</p><p>​  给定一个经过处理的图像<spanclass="math inline">\(I\)</span>作为输入，我们使用ResNet-50[53]作为骨干来提取多尺度的视觉特征，<span class="math inline">\(X_i(i =\{1, . . .,S\})\)</span>。然后，将特征输入到由通道注意块和空间注意块连续级联组成的混合注意模块中，以便转换特征，从而在传递到解码器之前预先定位潜在的被篡改区域。基于[54]、[55]、[56]和[57]，我们采用了混合注意模块，可以对最深的编码特征<spanclass="math inline">\(X_S\)</span>沿空间维度和通道的长期依赖关系进行建模。具体地说，它由通道注意块<spanclass="math inline">\(F_{ch}\)</span>和空间注意块<spanclass="math inline">\(F_{sp}\)</span>依次级联组成，分别通过沿信道和空间维度的自注意方案实现。之后，<spanclass="math inline">\(X_S\)</span>将与编码的特征结合，在通过解码器之前，获得<spanclass="math inline">\(\hat{X}_{S}\)</span>，即<spanclass="math inline">\(\hat{X}_{S}=\mathrm{Concat}(X_{S}, F_{ch}(X_{S}),F_{sp}(F_{ch}(X_{S})))\)</span>。<br/>​  在接下来的解码层中，特征不仅被上采样，还与跨尺度特征交互，通过边界感知注意模块定位被篡改区域的边界。每个尺度的边界感知注意模块将同时估计被操纵的区域掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>及其边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。另一方面，为了鼓励模型集中于篡改区域的边界，我们提出了一种基于边界引导采样策略的边界引导篡改对比损失，有利于区分篡改区域和非篡改区域。为了更直观地描述本文中使用的所有符号，我们在表一中列出了所有的符号及其内涵。在下面的章节中，我们将详细阐述我们的边界感知注意模块和边界引导的篡改对比损失。</p><h2 id="b.-边界感知注意力学习">B. 边界感知注意力学习</h2><p>​  操作检测和定位的关键是发生非自然混合的被篡改区域的边界。对被篡改区域边界的准确定位可以有效地帮助被篡改区域的定位。在我们的网络的解码器中，我们合并了所谓的边界感知注意模块<spanclass="math inline">\(F_{ba}\)</span>，专门旨在估计边界。<br/>​  为了定位边界，我们不仅需要来自前一层的特征，而且还需要具有语义和细节的特征。随着解码特征的空间维数的增加，需要更详细的信息。因此，受类似unet的网络结构[58]的启发，我们利用相同尺度上的编码特征<spanclass="math inline">\(X_i\)</span>，以及前一层的解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>，来促进边界定位。边界感知注意模块有两个输出：1)预测的边界<spanclass="math inline">\(\hat{C}_{i}\)</span>和2)将被传播到下一层的特征，并用于生成第<spanclass="math inline">\(i\)</span>个尺度的掩模，<spanclass="math inline">\(\hat{M}_{i}\)</span>。这个过程可以表示如下： <spanclass="math display">\[\begin{aligned}\ [\tilde{X}_{i},\hat{C}_{i}]&amp;=F_{ba}(\hat{X}_{i+1},X_{i}), \\\hat{X}_{i}&amp;=\mathrm{Concat}(\tilde{X}_{i},\mathrm{Upsample}(\hat{X}_{i+1})),\\\hat{M}_{i}&amp; =\mathrm{Conv}(\hat{X}_{i}).\end{aligned}\]</span><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png"alt="image-20240910222829501" /></p><p>​  在图3中，受[59]、[60]、[61]和[62]的启发，我们设计了边界感知注意模块的结构。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910112215485.png"alt="image-20240910112215485" /><figcaption aria-hidden="true">image-20240910112215485</figcaption></figure><p>图3。边界感知注意模块的说明。<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>中选择的像素级特征分别用深绿色和深红色表示。通过交叉注意模块，我们得到了增强的特征<spanclass="math inline">\(\hat{v}_{i}\)</span>，用蓝色表示，然后将其映射回<spanclass="math inline">\(X_i\)</span>，同时保持剩余的特征不变。</p><p>​  首先，我们对编码的特征<spanclass="math inline">\(X_i\)</span>进行降采样，以匹配来自前一层解码特征<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的维数，并将它们连接起来。为了提取特征的边界信息，我们通过平均池化、卷积和sigmoid算子的组合对特征进行平滑，并让原始特征减去平滑后的特征，得到与篡改边界相关的高频信息。该过程可以描述如下：<spanclass="math display">\[\begin{aligned}&amp;\tilde{H}_{i}=\mathrm{Concat}(\mathrm{Downsample}(X_{i}),\hat{X}_{i+1}),\\&amp;H_{i}=\tilde{H}_{i}-\tilde{H}_{i}\odot\mathrm{Sigmoid}(\mathrm{Conv}(\mathrm{AvgPool}(\tilde{H}_{i}))),\end{aligned}\]</span>​  其中，<span class="math inline">\(\hat{H}_{i}\)</span>是<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>的组合特性。<spanclass="math inline">\(\odot\)</span>表示元素级的乘法。利用所获得的特征<spanclass="math inline">\(H_i\)</span>来预测边界，即<spanclass="math inline">\(\hat{C}_{i}=\mathrm{Conv}(H_{i})\)</span>。<br/>​  接下来，需要使用这些特征来生成篡改掩码<spanclass="math inline">\(\hat{M}_{i}\)</span>并传递到下一层，因此应该利用与篡改最相关的信息。在这里，我们使用一个特征选择块<spanclass="math inline">\(F_{fs}\)</span>，从预测的边界图<spanclass="math inline">\(\hat{C}_{i}\)</span>中找到前K个高置信像素的索引。然后，这些索引引导模型对<spanclass="math inline">\(X_i\)</span>和<spanclass="math inline">\(\hat{X}_{i+1}\)</span>对应的像素级特征进行采样，分别记为<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>。 <spanclass="math display">\[\mathbf{v}_{i}=F_{fs}(X_{i},\mathrm{TopK}(\hat{C}_{i})),\mathbf{\hat{v}}_{i+1}=F_{fs}(\hat{X}_{i+1},\mathrm{TopK}(\hat{C}_{i})),\]</span>​  其中K实际上设为32。由于<span class="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>是定位边界的最关键的特征，因此我们采用了一个交叉注意模块，其中<spanclass="math inline">\(v_i\)</span>作为查询，<spanclass="math inline">\(\hat{v}_{i+1}\)</span>作为键/值，如下所示： <spanclass="math display">\[\hat{\mathbf{v}}_i=\mathbf{v}_i+\text{Softmax}(\frac{\mathbf{v}_i\hat{\mathbf{v}}_{i+1}^T}{\sqrt{d_k}})\hat{\mathbf{v}}_{i+1},\]</span>​  其中<span class="math inline">\(d_k\)</span>表示<spanclass="math inline">\(v_i\)</span>和<spanclass="math inline">\(\hat{v}_{i+1}\)</span>的维数。最后，我们根据所选择的索引将<spanclass="math inline">\(\hat{v}_{i}\)</span>分散到<spanclass="math inline">\(X_i\)</span>中，并保持未被选择的位置不变。<br/>​  损失函数：我们应用多尺度损失来学习具有代表性的多尺度特征，以进行更精确的预测。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png"alt="image-20240910221211400" /><figcaption aria-hidden="true">image-20240910221211400</figcaption></figure><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png"alt="image-20240910110200920" /><figcaption aria-hidden="true">image-20240910110200920</figcaption></figure><p>​  在实践中，如图2所示，所有四个尺度都生成了操作掩模，而仅对两个尺度的中间边界掩模进行预测。对于具有最深特征的预测掩模，即<spanclass="math inline">\(\hat{M}_{S}\)</span>，我们应用二进制交叉熵（BCE）损失和IoU损失进行监督。此外，我们采用加权二值交叉熵损失[63]和加权IoU损失[63]来监督预测的掩模<spanclass="math inline">\(\hat{M}_{i}=(i\neq S)\)</span>和边界<spanclass="math inline">\(\hat{C}_{i}\)</span>。边界的groundtruth值是通过从膨胀图像中减去二值地面真实掩模的侵蚀而得到的。具体来说，我们应用核大小为5×5、步幅为1的最大池化操作来进行图像扩张和侵蚀。</p><h2 id="c.-边界引导下的篡改对比学习">C. 边界引导下的篡改对比学习</h2><p>​  一旦一幅图像被篡改，其被篡改的区域可能会显示出与未被篡改的区域略有不同的视觉统计数据，例如，不自然的照明信息或不一致的噪声分布。扩大学习特征空间中篡改区域和非篡改区域之间的差异，可以有效地提高学习特征的鉴别能力，有利于篡改区域的定位。<br/>​  基于此，我们采用对比学习，目的是学习区分特征表示，可以区分篡改和非篡改部分。我们选择对每个尺度，对直接用于预测最终掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>的特征图进行对比学习，我们根据经验发现它的效果很好。特别是，在训练过程中，我们以点的方式对特征图进行空间采样，从篡改和未篡改区域收集样本。在这里，一个样本指的是在特征映射的特定位置上的一个特征向量。然后，我们最小化一个对比损失函数，以减少同一区域内样本之间的距离（即正对），同时增加不同区域内样本之间的距离（即负对）。为了进一步提高学习特征的鲁棒性和可鉴别性，我们引入了一种边界引导的采样策略来构造信息更丰富的负对。<br/>​  1)边界引导篡改对比损失：从解码器的每个尺度上，我们采用一个对比损失，它由两项组成，即<spanclass="math inline">\(\mathcal{L}^{TC}=\mathcal{L}^{+}+\mathcal{L}^{-}\)</span>，其中<spanclass="math inline">\(\mathcal{L}^{+}\)</span>和<spanclass="math inline">\(\mathcal{L}^{-}\)</span>分别是正对损失和负对损失。在形式上，<spanclass="math inline">\(\mathcal{L}^{+}\)</span>被写为： <spanclass="math display">\[\begin{aligned}\mathcal{L}^{+}&amp;=\frac{1}{|\mathcal{H}_{t}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{t}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n}))\\&amp;+\frac{1}{|\mathcal{H}_{n}^{+}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}_{n}^{+}}-\log(\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\end{aligned}\]</span>​  其中，<spanclass="math inline">\(\mathrm{Sim}(\cdot,\cdot)\)</span>表示两个特征向量之间的余弦距离，<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>是来自被篡改区域的一组正对，而<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>包含从未被篡改区域提取的正对。<spanclass="math inline">\((\mathbf{u}_{m},\mathbf{u}_{n})\)</span>表示被采样的特征向量对。<spanclass="math inline">\(\mathcal{L}^{+}\)</span>作为一种力，将特征空间中同一区域内的样本拉在一起。为了获得样本的正对，我们使用硬对挖掘策略，将在同一区域内但彼此远离的样本聚集在一起。具体来说，在来自被篡改或未被篡改区域的所有可能的样本对中，我们保留具有前L个最大距离的样本对，构成<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>。在实践中，L被设置为来自同一区域的所有可能样本对数量的一半。我们对<spanclass="math inline">\(\mathcal{L}^{-}\)</span>的定义如下： <spanclass="math display">\[\mathcal{L}^{-}=\frac{1}{|\mathcal{H}^{-}|}\sum_{(\mathbf{u}_{m},\mathbf{u}_{n})\in\mathcal{H}^{-}}-\log(1-\mathrm{Sim}(\mathbf{u}_{m},\mathbf{u}_{n})),\]</span>​  其中<spanclass="math inline">\(\mathcal{H}^{-}\)</span>是一个负对集，其中每对都由来自被篡改区域的查询样本和来自未被篡改区域的负样本组成。<spanclass="math inline">\(\mathcal{L}^{-}\)</span>旨在将来自不同地区的样本分开。<br/>​  2)边界引导采样策略：为了构建<spanclass="math inline">\(\mathcal{H}^{-}\)</span>，一种简单的方法是从一个未被篡改的区域随机抽取负样本。然而，在一个未被篡改的区域内，通常存在很大的差异。例如，未被篡改的区域可能占据图像的很大一部分，因此它可能包含大量分散各种物体或杂乱的背景。因此，绘制负样本的原生方法往往会降低模型的性能。因此，我们提出了一种新的采样策略，即我们在一个被篡改区域的边界附近绘制负样本，而不是从整个非调和区域（见图4中的示例）。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910140100074.png"alt="image-20240910140100074" /><figcaption aria-hidden="true">image-20240910140100074</figcaption></figure><p>图4。边界引导的抽样策略。蓝色点表示未篡改区域的采样特征，红色的表示被篡改区域的采样特征。</p><p>​  这种采样方法具有两个方面的优势：第一，约束边界区域内的负样本倾向于减少未篡改区域内方差大而造成的干扰；第二，篡改区域的边界周围经常发生非自然的混合，因此从该区域采样鼓励模型专注于边界区域，这与我们框架的其他组件，如边界感知注意模块。特别地，给定一个地真二值篡改区域掩模<spanclass="math inline">\(M\)</span>，我们对其进行图像扩展，得到一个放大的掩模<spanclass="math inline">\(\hat{M}_{i}\)</span>，并将负样本限制为来自<spanclass="math inline">\(\hat{M}_{i}-M_i\)</span>指定的外部边界区域。最后，我们对<spanclass="math inline">\(\mathcal{H}_{t}^{+}\)</span>和<spanclass="math inline">\(\mathcal{H}_{n}^{+}\)</span>采用硬对挖掘方法，只保留距离最小的前L个负对来构造<spanclass="math inline">\(\mathcal{H}^{-}\)</span>。</p><h1 id="实验结果">4. 实验结果</h1><p>​  在本节中，我们将在几个公共基准上进行全面的实验，并将我们提出的方法与以前的最先进的方法进行比较。我们还分析了我们的模型的不同组件。</p><h2 id="a.-实施细节和数据集">A. 实施细节和数据集</h2><p>​  <strong>实施细节：</strong>我们使用Pytorch实现了我们的框架，并使用一个NVIDIA TitanxpGPU进行训练和测试。对于训练，所有输入的图像都被调整到512×512的分辨率，并通过随机的水平翻转、颜色抖动和裁剪来增强它们。我们采用ResNet50[53]作为骨干。在训练过程中，我们使用了Adam优化器[64]，其动量为0.9，重量衰减为5×10−4。我们将批处理大小设置为16，并使用多项式策略[65]调整学习率，基本学习率为1×10−5，幂次为0.9。为了进行测试，首先将输入图像的大小调整到512×512，用于网络推断，然后将输出映射的大小调整回输入图像的原始大小。<br/>​  <strong>数据集：</strong>为了评估模型的性能，我们在6个基准测试上进行了实验，包括CASIA[24]、NIST16[27]、Columbia[25]、Coverage[26]、Defacto[67]和一个真实世界的数据集IMD2020[68]。此外，我们还利用了[69]提出的合成数据集。这些数据集涵盖了不同类型的操作，包括拼接、复制-移动和删除。所有的数据集都提供了ground-truth的二进制掩码。</p><ul><li>CASIA[24]由两个子数据集组成，CASIAv1的数据为921张篡改图像和CASIAv2的篡改图像为5123张图像。篡改类型包括拼接和复制移动。裁剪后的区域通常是精心选择的，并应用后处理操作，使区域在视觉上逼真。</li><li>NIST16[27]包含564个被篡改的图像样本。所有三种操作类型都涉及到，它们还进行后处理以隐藏可见的痕迹。</li><li>Columbia[25]专注于拼接，它包含180张未压缩的图像。</li><li>Coverage[26]关注于包含100张图像的复制-移动操作。被操作的对象被手动裁剪以覆盖同一图像中的相似对象，并对它们进行后处理以去除可见的操作痕迹。</li><li>Defacto[67]是最近提出的一个大规模合成数据集，包含149k图像，这些图像从MS-COCO[70]中采样，并通过复制移动、拼接和绘制自动操作。</li><li>IMD2020[68]包含2010年真实的从互联网收集的真实操作图像，涉及所有三种操作类型。</li><li>合成训练数据集[69]包含了最初从[70]中收集到的大约100k张图像，涵盖了拼接、复制-移动和删除的操作类型。</li></ul><p>​  在以下小节的定性结果中，特征图显示在颜色图中，以较暖的颜色表示更多的关注，反之亦然。比较方法的预测篡改掩模在0到1的灰度图像中可视化。预测的掩模的每个像素都意味着被篡改的确定性。</p><h2 id="b.-与最先进的技术进行比较">B. 与最先进的技术进行比较</h2><h3 id="像素级篡改检测">像素级篡改检测</h3><p>​  在[5]和[34]之后，我们将我们提出的方法与两种实验设置下的几种最先进的方法进行了比较。<br/>​  在第一种设置中，在MVSS-Net[5]之后，所有的比较模型都是在真实数据上从头开始训练的，没有任何额外的合成数据集。我们采用CASIAv2[24]在其他公共基准上进行训练和测试，包括CASIAv1、NIST16、Columbia、Coverage、IMD20和DEFACTO。我们将我们的模型与之前的几种方法进行了比较，包括ManTra-Net[37]、HP-FCN [71]，CR-CNN [72]，GSR-Net [21]、SPAN [66]、CAT-Net[73]，MVSS-Net [5]和MVSS-Net++[74]。我们不与ObjectFormer[34]进行比较，因为它的代码是不公开的。表二显示了在[5]中报告的固定阈值为0.5的F1的测量结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910143404035.png"alt="image-20240910143404035" /><figcaption aria-hidden="true">image-20240910143404035</figcaption></figure><p>​  我们可以观察到，我们的模型在所有的数据集上都达到了最好的定位性能。我们注意到，GSR-Net还通过预测篡改的边界和掩码来利用其网络中的边界信息。我们从GSR-Net在所有基准上的显著改进表明，在利用边界信息方面，我们提出的注意和对比机制比简单的边界和掩模预测的优势。<br/>​  在第二种设置中，在[34]、[69]和[76]之后，我们首先在一个合成数据集上对每个模型进行预训练，然后在基准的训练集上进行微调，然后在基准的测试集上进行测试。比较方法包括RGB-N[38]、SPAN [66]、PSCC-Net [69]、ObjectFormer[34]、HiFi-Net [77]和ERMPC[76]，在各自的合成数据集上进行训练，在每个基准的训练集上进行微调，并在相应基准的测试集上进行测试。我们评估了所有的方法在CASIA，NIST16和Coverage。对于CASIA，来自CASIAv2的5123张图像用于微调，来自CASIAv1的921图像用于测试。对于NIST16，564张图像的NIST16，404张图像用于微调，160张图像用于测试。有100张图像的Coverage被分为75/25来进行微调和测试。对于这种设置，正如[34]中报道的，不同的方法使用自己的合成数据集，不同的内容和不同数量的图像从47k到100k不等。鉴于此，为了公平比较，我们从[69]的合成数据集中随机选择了60k张图像。关于三个基准测试的结果见表三。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910204820762.png"alt="image-20240910204820762" /><figcaption aria-hidden="true">image-20240910204820762</figcaption></figure><p>​  请注意，Columbia没有训练集，因此对它进行微调是不可能的。在CASIAv1和NIST16上，我们的方法大大优于所有比较方法，尽管我们的方法与SPAN和PSCCNet相比，该方法使用的数据要少得多。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205006572.png"alt="image-20240910205006572" /><figcaption aria-hidden="true">image-20240910205006572</figcaption></figure><p>​  通过深入研究Coverage上的失败情况（图12），我们发现我们的模型往往会在特定类型的图像上失败，在这些图像中，区域边界周围的操作痕迹被小心地擦去。对于图12中所示的所有失败案例，我们的模型的F1评分都相当低（小于35%）。通过从测试集中排除这三个图像，我们的方法的F1得分上升到78.1%。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205223378.png"alt="image-20240910205223378" /><figcaption aria-hidden="true">image-20240910205223378</figcaption></figure><p>​  图5显示了我们的方法与最先进的方法的定性比较。结果表明，我们的方法不仅可以更准确地定位被篡改的区域，而且还可以产生更清晰的边界（图5的前三行），这得益于边界感知的注意模块。此外，结果表明，我们的模型比其他模型对背景分心（图5的最后三行）更稳健，这是由于引入了抑制噪声的对比损失。</p><h3 id="图像级篡改检测">图像级篡改检测</h3><p>​  虽然我们的模型更关注像素级篡改定位任务，但它具有检测图像级篡改的能力。图像级篡改检测的目的是将输入图像分类为真实或篡改。我们遵循[5]的协议来运行一个图像级的篡改检测实验，其中我们的模型是在CASIAv2上进行训练的。考虑到CASIAv1和CASIAv2共享782张真实图像，我们从Corel[78]中随机抽取782张真实图像，以替换CASIAv1中的这些副本，从而得到数据集CASIAv1+。我们在表四中显示了三个基准的结果，CASIAv1+、Coverage和Columbia。在本实验中，我们将我们的模型与ManTraNet[37]、CR-CNN [72]、GSR-Net [21]、SPAN [66]和MVSS-Net[5]进行了比较。<br/>​  为了执行图像级篡改检测，我们修改了我们的模型，通过在特征<spanclass="math inline">\(\hat{X}_{s}\)</span>中添加一个图像分类头，以预测图像被操纵的概率。具体来说，图像分类头由一个CBR块、一个平均池化层和一个完全连接的层组成，其中CBR块是卷积、批处理归一化（BN）和ReLU的组合。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205812866.png"alt="image-20240910205812866" /><figcaption aria-hidden="true">image-20240910205812866</figcaption></figure><p>​  如表四所示，我们的方法在所有基准上都获得了最好的F1分数，我们的模型的AUC分数在所有方法中排名第二好，这表明我们的模型能够在像素级定位和图像级检测任务中都产生良好的性能。</p><h2 id="c.-对未篡改图像的定位">C. 对未篡改图像的定位</h2><p>​  我们还在未被篡改的图像上测试了我们的模型以进行像素级定位，并在图6中显示了定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910205954595.png"alt="image-20240910205954595" /><figcaption aria-hidden="true">image-20240910205954595</figcaption></figure><p>​  用我们的方法预测的未被篡改图像的掩模在图6的前三行上几乎为空白，而比较的方法错误地检测到一些位置被篡改了。对于图6的最后两行，所有的方法都出现了一些假阳性。然而，我们的模型倾向于将假阳性限制在小的局部区域，而其他方法则倾向于将它们分散在整个图像上。此外，从图6的最后一行，我们观察到，我们的模型可能会误解到真实图像中的可疑区域。相比之下，MVSS-Net[5]倾向于对误检测的置信度较低。这将导致我们的模型有更高的机会将真实的图像误分类为被篡改，这部分解释了为什么我们的模型在B节的图像级操作检测实验中与MVSS-Net相比的AUC评分较低。</p><h2 id="d.-模型组件分析">D. 模型组件分析</h2><p>​  为了阐明单个组件的影响，我们评估了在不同配置下提出的模型。所有报告的结果都来自于CASIA数据集。</p><h3 id="边界感知注意力模块">边界感知注意力模块</h3><p>​  边界感知注意模块可以部署在解码器的每个尺度上来预测边界，因此我们验证了表v中的设计。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210222453.png"alt="image-20240910210222453" /><figcaption aria-hidden="true">image-20240910210222453</figcaption></figure><p>​  作为参考，我们有一个没有任何BAMs的基线模型，它可以达到53.9%的f1和87.4%的AUC。首先，我们在最深的尺度上部署BAM（即i=3），它打算将最小的分辨率转换为边界。正如观察到，添加一个BAM会导致更好的性能，这可以从获得3%的F1分数中得到暗示。其次，我们在第二个尺度（即i=2）上附加了另一个BAM，因此有两个协作的BAM用于预测不同尺度的边界。通过编码特征带来的更详细的信息，该模型能够以60.0%的F1和88.6%的AUC达到最佳性能。最后，当我们在所有尺度上加入三个算法算法时，性能明显下降，因为最低尺度的特征引入的噪声会对边界预测产生负面影响。<br/>​  为了演示我们的边界感知注意模块如何通过检测边界来帮助定位被篡改的区域，我们在图7中展示了一个特征图的可视化。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210329644.png"alt="image-20240910210329644" /><figcaption aria-hidden="true">image-20240910210329644</figcaption></figure><p>​  为了进行比较，我们还将MVSS-Net[5]中最后一个Sobel层所产生的特征可视化，它提取了与边缘相关的特征。图7中的结果表明，由于我们的边界感知注意模块，我们的方法可以更准确地定位篡改区域。<br/>​  此外，我们还在图8中给出了一些关于BAM有效性的例子。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210454382.png"alt="image-20240910210454382" /><figcaption aria-hidden="true">image-20240910210454382</figcaption></figure><p>​  从我们可以观察到，在没有BAM的帮助的情况下，估计的掩模在边界区域往往是不完整的，因为像素可以很好地融合到背景中，而且它们很难区分。虽然BAM模型可能不能完美地预测边界，但它足以提供上下文线索来推断整个被篡改的掩模。<br/>​  如图9所示，我们还展示了在不同数据集上与MVSS-Net[5]进行比较的定性结果。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210642783.png"alt="image-20240910210642783" /><figcaption aria-hidden="true">image-20240910210642783</figcaption></figure><p>​  为了验证所提出的边界感知注意模块的有效性，我们在第三列和第四列中可视化最终的掩模和预测的边界掩模。在前两行观察到，虽然MVSS-Net可以粗略预测被篡改的区域，但其预测包含较大的灰色区域，表明确定性较低。相比之下，我们的预测结果不仅可以准确地定位边界更清晰的被篡改区域，而且对被篡改区域具有更高的确定性。同时，在第四和第五行，MVSS-Net可能对真实的真实区域有假阴性，而在我们的预测中，背景区域几乎是黑色的。计算结果表明了我们所提模型的优越性。</p><h3 id="边界导向的篡改对比损失">边界导向的篡改对比损失</h3><p>​  回想一下，我们的边界引导篡改对比损失的目标是将特征嵌入与相同区域的距离拉近，同时将特征与不同区域的距离分开，即篡改和非篡改区域的距离，使它们更具区分性。首先，我们评估了在表六中关于如何部署损失的差异。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910210828335.png"alt="image-20240910210828335" /><figcaption aria-hidden="true">image-20240910210828335</figcaption></figure><p>​  可以观察到，当我们在所有尺度上使用损失时，它通常会达到最优结果。相比之下，使用第二和第三等级（没有最高尺度）的AUC略有改善（88.9%），但F1略有下降（51.5%）。这是因为最高尺度的特征直接对应于最终的结果，因此它与操作检测的精度有关。如果不使用顶级规模的特性，一般的性能就会变得更糟。<br/>​  图10显示边界引导篡改对比损失对于篡改区域不明显的图像是有效的。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211059841.png"alt="image-20240910211059841" /><figcaption aria-hidden="true">image-20240910211059841</figcaption></figure><p>​  可以观察到的，在使用对比学习损失后，被篡改区域边界上的差异变大。此外，纹理图像的被篡改区域是某些物体的不完整部分。结果表明，我们的模型可以很好地推广到部分级篡改。<br/>​  此外，图11为应用多尺度对比学习损失后的不同尺度的特征图。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211157377.png"alt="image-20240910211157377" /><figcaption aria-hidden="true">image-20240910211157377</figcaption></figure><p>​  可以观察到，随着解码器规模的增加，对比学习损失以粗到细的方式定位篡改区域。在最好的尺度上，我们可以清楚地观察到被篡改区域周围边界的颜色变成蓝色，而被篡改区域的颜色变成红色，它们的大差异是由对比学习损失造成的。这种现象在第二尺度和第三尺度的特征上并不明显，因此第一尺度的特征为模型的改进提供了最大的增益。</p><h3 id="混合注意力模块">混合注意力模块</h3><p>​  我们还对我们的混合注意模块进行了消融研究，发现移除该模块会导致f1和AUC分别降低0.4%和2.8%。这说明了这个模块的重要性。</p><h3 id="边界引导的抽样策略">边界引导的抽样策略</h3><p>​  抽样策略在我们提出的对比损失中起着重要的作用。为了验证抽样策略的有效性，我们对表七中的CASIA数据集进行了实验。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211419997.png"alt="image-20240910211419997" /><figcaption aria-hidden="true">image-20240910211419997</figcaption></figure><p>​  最直接的方法是从被篡改区域和未被篡改区域中随机采样特征点。相比之下，我们对从篡改区域和非篡改区域随机抽取的点采用了硬对挖掘，从而提高了性能。然而，在非篡改区域内存在很大的差异，导致非篡改区域随机抽样的对比学习结果的鲁棒性。因此，我们将采样范围限制在篡改边界附近的未篡改区域内。可以观察到，在篡改边界附近的篡改和非篡改区域随机选择点，会导致负样本之间存在较大差异，导致网络性能下降。最后，我们将硬对挖掘策略与从边界区域随机抽取的样本相结合，获得了最优的性能。</p><h2 id="e.-超参数分析">E. 超参数分析</h2><p>​  我们从F1和AUC分析了边界引导篡改对比损失和CASIA数据集边界感知注意模块的超参数，如表八所示。这涉及到6个关键的超参数。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910211517337.png"alt="image-20240910211517337" /><figcaption aria-hidden="true">image-20240910211517337</figcaption></figure><p>​  1)输入图像大小：训练数据集有不同大小的图像，其中大多数图像的分辨率为384×256，其他图像有不同的分辨率，如640×480,336×638,500×375。因此，我们遵循[5]的方法，将不同输入图像大小的输入图像调整为512，并找到我们选择的512×512。我们的×512实验结果最好，如表八所示。<br/>​  2)选择指标数K：作为我们提出的边界感知注意模块的关键，所选指标对应于与边界预测最相关的特征。我们将所选指标的个数表示为K，并将其设为16、32和64。如表八所示，当K为32时，我们得到的性能最好。<br/>​  3)生成GroundTruth边界的扩张和侵蚀数：生成一个细化的GroundTruth边界对于监督第三节B小块中边界感知注意模块的预测边界掩模<spanclass="math inline">\(\hat{C}_{i}\)</span>是必要的。该过程包括通过从GroundTruth掩模的膨胀中减去GroundTruth掩模的侵蚀来导出GroundTruth边界。我们试图将膨胀和侵蚀的数量从1增加到3，这导致地面真实边界越来越宽，并发现更窄的地面真实边界会得到更好的结果。<br/>​  4)未篡改采样区域的扩张数：为了执行边界引导采样策略，我们从篡改区域的扩张中减去篡改区域的GroundTruth掩模，得到篡改区域周围的未篡改边界区域，从中对未篡改区域的点特征进行采样。我们用不同的膨胀次数进行实验，发现3次的膨胀效果最好。<br/>​  5)样本数Z：对于边界引导采样策略，在确定正负样本对的采样区域后，随机选择一些构建正对集和负对集。为了获得这些样本对，我们需要分别从篡改区域和未篡改区域抽取一定数量的以Z表示的样本。根据经验，我们将Z设为250和500。对于包含小于Z像素的被篡改区域，我们对所有像素的特征进行采样。我们在表八中显示了结果。正如所观察到的，一个小的Z意味着我们对这两个区域都采样不足，因此可能不能充分利用样本。<br/>​  6)Top-L硬对：对于边界引导采样策略，我们采用硬对挖掘方法获得正负样本对，其中我们对Top-L硬对进行采样，并将L设置为整个实验中所有对数量的一半。我们用L=1和L=来实验所有对的数量。如表八所示，我们的上半部分策略取得了最好的性能。top-1策略容易被极端样本对误导，而全对策略在样本间存在较大差异，从而引入无关信息来分离篡改区域和非篡改区域。</p><h2 id="f.-鲁棒性评价">F. 鲁棒性评价</h2><p>​  根据[5]、[34]和[69]中的失真设置，我们对我们的网络进行了鲁棒性分析。具体来说，对CASIA数据集中的操作图像应用了不同的失真操作。此外，我们结合了任意两种畸变，其中从区间[0.25、0.78]分别、[3,15]、[3,15]和[50,100]中随机选择调整尺度、核大小、标准差和质量因子。<br/>​  最后，我们将各种扭曲结合在一起，作为“<em>Mixed</em>”。我们应用f1和AUC来测量定位性能。对输入图像应用畸变将不可避免地导致边界信息被损坏，从而导致性能下降。然而，与MVSS-Net相比，我们的方法对失真显示了更鲁棒的性能，如表九所示。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212202152.png"alt="image-20240910212202152" /><figcaption aria-hidden="true">image-20240910212202152</figcaption></figure><p>​  我们还分析了在CASIAv1+上关于各种失真的图像级检测的鲁棒性。</p><figure><imgsrc="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910212225437.png"alt="image-20240910212225437" /><figcaption aria-hidden="true">image-20240910212225437</figcaption></figure><p>​  如表X所示，我们在调整大小和JPEG压缩失真方面的性能不如MVSS-Net。这可能是由于这些操作造成的边界信息被污染，严重影响了我们的边界感知注意模块。尽管如此，我们的模型仍然取得了合理的性能，并且我们的鲁棒性可以与MVSS-Net相当，甚至更优。</p><h2 id="g.-效率分析">G. 效率分析</h2><p>​  在计算复杂度方面，我们的方法实现了39.44个GFLOPs，明显低于MVSS-Net的163.57个GFLOPs。此外，我们使用具有24GBGPU内存的GeForce RTX3090来评估计算时间。使用相同的GPU，我们的完整模型需要0.014秒来处理一幅图像，比MVSS-Net快得多，后者需要0.038秒。</p><h1 id="结论和局限性">5. 结论和局限性</h1><p>​  在这项工作中，我们的目标是图像篡改定位问题。为此，我们提出了一种新的边界引导方法，其固有的倾向于通过注意机制和对比学习方案充分利用被篡改区域的边界信息。特别地，我们提出了一个具有边界感知能力的注意模块，它可以预测被篡改区域的边界，以迫使网络特别注意重要的边界区域。此外，我们引入了一种新的对比损失与边界引导抽样策略来学习更多的有区别的特征。我们证明，作为在CASIAv2上的训练，我们提出的模型在四个不同的基准上大大优于最先进的方法。此外，当在合成数据集上进行预训练时，我们的模型在现实基准上也显示了可比性或优越的通用性。<br/>​  我们在图12中显示了覆盖范围测试集上的失败情况。当被篡改边界的痕迹被仔细地擦去（图12的第一列）或类似的边界同时出现在被篡改的区域（图12的第二列）时，我们的模型可能难以区分被篡改区域的边界和未被篡改物体的边界。此外，如果被篡改的区域是两个相似对象的局部区域（图12的第二列和第三列），那么我们的方法可能会失败。在未来的工作中，我们将开发区分对象边界和被篡改区域边界的技术，旨在进一步提高我们的性能。<br/>​  此外，虽然这项工作的主要焦点是像素级的操作，但我们已经证明了我们的模型能够在图像级上检测操作。尽管通过简单地调整我们的模型来适应这种图像级的任务，已经取得了很好的性能，但肯定有一些进一步改进的空间。因此，未来研究的一个有趣的途径是专门为我们的模型定制图像级的检测问题，以优化性能，或探索使用我们的模型或它的一部分作为像素级和图像级任务的联合建模的主干。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning</title>
      <link href="/KaggleNet/"/>
      <url>/KaggleNet/</url>
      
        <content type="html"><![CDATA[<center>Towards Generalizable Deepfake Detection via Clustered and AdversarialForgery Learning</center><center>Yiming Chen, Haiwei Wu, Fengpeng Li, Kemou Li,</center><center>Zheng Li, Kahim Wong, Xiangyu Chen, Binbin Song,</center><center>Shuning Xu, Jun Liu, $ Jiantao Zhou ^ * $</center><center>September 2, 2024</center><h1 id="团队详细信息">1 团队详细信息</h1><p>团队名称：JTGroup<br/>组长：mortonchen（yimingchen98@gmail.com）<br/>团队成员人数（按字母顺序排列）：</p><ul><li>chxy95 (chxy95@gmail.com)</li><li>fengpengli (fengpeng.li@connect.umac.mo)</li><li>highwayw (823215616@qq.com)</li><li>kahimwong (jesonwong47@gmail.com)</li><li>kemoulee (kemoulee@gmail.com)</li><li>namecantbenull (doublebin111@gmail.com)</li><li>rebeccaee (rebeccaxu0418@gmail.com)</li><li>umlizheng (umlizheng@gmail.com)</li><li>yiyayoo (junlwind@163.com)</li></ul><p>隶属关系：澳门大学智能城市物联网国家重点实验室1<br/>团队主管：教授。JiantaoZhou2<br/>∗通讯作者<br/>1https://skliotsc.um.edu.mo/<br/>2https://www.fst.um.edu.mo/people/jtzhou/</p><h1 id="解决方案详细信息35">2解决方案详细信息（35%）</h1><p>​  本节首先介绍了解决方案设计背后的动机，然后概述了基本的工作流和其中每个阶段的细节。</p><h2 id="动机">2.1动机</h2><p>​  在获得官方的训练和验证数据后，我们发现即使是简单的小模型（如Efficient-b0[1]）也可以毫不费力地拟合训练集，并在验证集上实现显著的检测性能。这这表明训练集和验证集是同分布的，并且由相当同质的特征类型组成。虽然这种过拟合问题使小模型能够表现良好，但它不利于在实际场景中检测未知数据。在图1给出了一个说明性的例子，它通过t-SNE可视化了训练集和验证集的特征分布：</p><figure><img src="../postimages/KaggleNet/image-20240902200824430.png"alt="image-20240902200824430" /><figcaption aria-hidden="true">image-20240902200824430</figcaption></figure><p>​  图1：动机：训练集和验证集的官方分割中的特征可视化揭示了两个关键的见解：1)训练集和验证集之间分布的相似性使得评估模型在看不见的测试集上的性能具有挑战性；2)假数据的特征类型比真实数据更丰富。这一见解启发了我们最初使用无监督聚类来划分具有挑战性的验证集，然后基于对抗性学习原则增加真实和假数据的类型。这些见解促进了我们提出的泛化性的深度伪造检测方法。<br/>​  结果表明，在该数据分割方案下训练的模型在未知测试集上只能达到0.877的个AUC。<br/>​  基于这些观察结果，我们建议在训练集上进行特征级别（这在后门和噪声标签研究[3,4]中常用）的无监督聚类，然后重新分割训练集和验证集，以包括不同类型的伪造或真实数据。该方法旨在迫使模型更好地适应检测未知类型。此外，虽然深度伪造检测是一项分类任务，但我们在训练过程中在特征和对抗水平上引入了额外的优化目标（受[5]启发），以加强从不同角度对伪造的广义和鲁棒表示的学习。如图1的下部所示，训练集和验证集的重新聚类和重新分割在特征分布上表现出显著差异，使引导模型在未知测试集上的AUC为0.96。此外，通过使用专家混合策略来综合来自不同数据分割场景的知识，我们在未知测试集上实现了最大AUC为0.98。<br/>（[5]：<ahref="https://zhaozw-szu.github.io/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/">Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning| zhaozw后院(zhaozw-szu.github.io)</a>）<br/>​  下一节将深入介绍所提出的方法。</p><h2 id="提出的方法概述">2.2提出的方法概述</h2><p>​  该方法可分为三个阶段：1)数据准备、2)训练和3)推理，其基本过程如图2所示。</p><figure><img src="../postimages/KaggleNet/image-20240903113352343.png"alt="image-20240903113352343" /><figcaption aria-hidden="true">image-20240903113352343</figcaption></figure><p>​  在阶段I的第2.3节最初增加和丰富了所提供的官方数据，以提供更多样化的数据分布。随后，采用无监督聚类算法，基于特征对伪造或真实的数据进行聚类，便于训练集和验证集的重新分割。在数据分割之后，在阶段2的第2.4节介绍了学习鲁棒性和一般伪造痕迹的三个优化目标，解决特性级、对抗级和分类级的监督。最后在阶段3的第2.5节实现了特定的推理过程。<br/>​  我们的主要贡献可以总结如下：</p><ul><li>基于无监督聚类的数据类型重新分配设计缓解了由简单分区引起的过拟合问题，有助于模型学习更一般化的伪造痕迹。</li><li>在特征、对抗式和分类级别引入深度伪造检测的联合监督范式，增强了模型学习高度广义伪造表示的能力。</li><li>实验结果表明，该方法具有良好的泛化能力，特别是在检测未知类型的深度造假方面。</li></ul><h2 id="第一阶段数据准备">2.3第一阶段：数据准备</h2><p>​  我们所提出的方法的框架，如图3所示，包括两个主要阶段：数据准备和训练。</p><figure><img src="../postimages/KaggleNet/image-20240903142646600.png"alt="image-20240903142646600" /><figcaption aria-hidden="true">image-20240903142646600</figcaption></figure><p>​  在数据准备阶段，我们专注于通过图像编辑（Edit）和稳定扩散（SD，StableDiffusion）技术生成新的数据来增强现有的数据集。然后，我们进行聚类来重新组装数据集，旨在提高我们的方法的检测性能和鲁棒性。在训练阶段，我们引入了一系列的专家模型，并使用三种类型的损失对其进行优化：<span class="math inline">\(\mathcal{L}_{\mathrm{KL}}\)</span>、<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>和<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>。这种多损失方法确保了模型能够有效地区分真实图像和篡改图像。</p><h3 id="数据生成">2.3.1数据生成</h3><p>​  为了增强模型泛化和缓解过拟合，我们使用图像编辑和稳定扩散（SD）技术扩展了训练数据集。</p><figure><img src="../postimages/KaggleNet/image-20240903143630833.png"alt="image-20240903143630833" /><figcaption aria-hidden="true">image-20240903143630833</figcaption></figure><p>​  如图4所示，数据生成过程包括以下操作：<br/>​  <strong>图象编辑：</strong>我们使用的第一种技术是图像编辑，它包括修改原始图像的特定元素来创建新的变体。首先，我们应用人脸语义分割来分离人脸区域和背景。一旦分离出来，我们就会用不同的颜色（如紫色、绿色和蓝色）来修改背景，同时保留原始的面部特征。这一步引入了数据集的变化，模拟了模型在现实场景中可能遇到的不同环境和照明条件。<br/>​  <strong>稳定扩散：</strong>第二种技术利用稳定扩散（SD，StableDiffusion）模型从原始数据集生成新的图像。从原始图像<spanclass="math inline">\(I_0\)</span>开始，我们首先将其转换为中间潜在表示。然后，我们应用各种文本提示来指导SD模型生成不同版本的图像。例如，提示像“.. . old . . . ”, “. . . person. . . ”, 和“. . . young . . .”，分别被用来创造年老的、一般的和恢复活力的面孔。生成的图像反映了广泛的风格和特征，用真实数据和篡改数据的不同表示丰富了训练集。<br/>​  通过结合图像编辑和SD生成的结果，我们产生了一个增强的数据集<spanclass="math inline">\(\cal{D}\)</span>，它显著增强了训练数据的可变性。然后在随后的训练阶段中使用这个全面的数据集，确保模型暴露于广泛的特征变化，从而提高其泛化到看不见的数据的能力。</p><h3 id="数据聚类">2.3.2数据聚类</h3><p>​  在原始数据集中，训练集和验证集的分配是随机分配的。然而，在实际的伪造检测场景中，测试中遇到的伪造类型通常与训练中遇到的不同。当模型遇到新的、看不见的伪造类型时，这种差异会导致性能的显著下降。为了解决这个问题，我们提出了一种数据聚类方法来重新分配训练和验证数据集。其主要目标是根据伪造类型对数据集进行聚类，从而模拟实际的测试条件，其中模型暴露在更广泛的各种看不见的伪造。数据集群的详细过程包括以下三个步骤：<br/>​  <strong>分类器模型训练：</strong>我们首先使用一个EfficientNet-b0[1]（Eff-b0）模型来训练一个轻量级的伪造特征提取器。该模型在原始训练集上进行训练，仅使用交叉熵（CE）损失：<span class="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>： <spanclass="math display">\[\mathcal{L}_{\mathrm{CE}}=-\sum_{i=1}^N\sum_{l=1}^L\mathbf{y}_l^{(n)}\log(\hat{\mathbf{y}}_l^{(n)}),\]</span>​  式中，<spanclass="math inline">\(\mathbf{y}_l^{(n)}\)</span>表示第n个样本中伪造类型l的groundtruth软标签，<spanclass="math inline">\(\hat{\mathbf{y}}_l^{(n)}\)</span>为Effb0模型对应的预测概率输出。该模型学习了能够有效区分伪造类型的鉴别特征。<br/>​  <strong>聚类特征提取：</strong>一旦Eff-b0模型被训练，我们使用它来从整个训练集<spanclass="math inline">\(\mathcal{D}=\{(\mathbf{I}^{(n)},y^{(n)})\}_{n=1}^{N}\)</span>中提取特征。对于每幅图像，我们只保留最后一层的输出特征向量，对第n幅图像表示为$~ \mathbf{u}^{(n)}\in\mathbb{R}^{d} $。该过程得到了一个大小为N×d的特征矩阵，封装了整个训练数据集的伪造特征。<br/>​  <strong>基于聚类的重新拆分：</strong>利用提取的特征矩阵，我们应用无监督聚类算法，特别是K-means，将图像分成K个聚类。每个聚类<spanclass="math inline">\(\mathcal{C}_{k}\)</span>代表一组不同的伪造特征，并对应于在后续训练阶段中使用的一组原始图像<spanclass="math inline">\(\mathcal{D}_k\)</span>。聚类后，我们随机重组并将这些K个簇合并成<spanclass="math inline">\(I\)</span>个折叠<spanclass="math inline">\(\{\mathcal{F}_1,\mathcal{F}_2,\ldots,\mathcal{F}_I\}\)</span>。每个折叠包<spanclass="math inline">\(\mathcal{F}_{i}\)</span>含所有原始图像集<spanclass="math inline">\(\mathcal{D}_k\)</span>，但训练集<spanclass="math inline">\(\mathcal{F}_{i}^{\mathrm{train}}\)</span>和验证集<spanclass="math inline">\(\mathcal{F}_{i}^{\mathrm{val}}\)</span>分区不同。这种旋转为每个折叠中的数据分布提供了一个独特的视角，并确保每个集群作为验证集一次，通过模拟实际的测试场景来促进鲁棒的特征学习，其中验证集包含伪造类型的图像，而在相应的训练集中不存在。该设置测试了模型推广到不可见的伪造类型的能力，确保了在现实世界条件下对其性能进行全面的评估。<br/>​  图5描述了来自不同集群的真假数据的预览。</p><figure><img src="../postimages/KaggleNet/image-20240903145603867.png"alt="image-20240903145603867" /><figcaption aria-hidden="true">image-20240903145603867</figcaption></figure><p>​  随后，我们将深入研究基于这些聚类数据的训练的细节。</p><h2 id="第二阶段训练">2.4第二阶段：训练</h2><h3 id="网络架构">2.4.1网络架构</h3><p>​  训练阶段的网络体系结构旨在确保模型能够有效地推广到不同的数据分布中，同时重新维护对伪造和对抗性攻击[6]。如图3所示，该体系结构由几个完整的组件组成，每个组件都有影响整体模型性能的特定角色。</p><figure><img src="../postimages/KaggleNet/image-20240903142646600.png"alt="image-20240903142646600" /><figcaption aria-hidden="true">image-20240903142646600</figcaption></figure><p>​  <strong>专家模型：</strong>每个包<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中的原始图像<spanclass="math inline">\(I\)</span>被输入到可训练的专家模型<spanclass="math inline">\(\{M_1(\mathbf{I};\boldsymbol{\theta}_1),M_2(\mathbf{I};\boldsymbol{\theta}_2),\ldots,M_I(\mathbf{I};\boldsymbol{\theta}_I)\}\)</span>。每个专家模型<spanclass="math inline">\(M_i\)</span>，由<spanclass="math inline">\(\theta_i\)</span>参数化，专门从其各自的折叠<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中学习图像。每个专家的输出是一个高维特征向量<spanclass="math inline">\(\mathbf{v}_{i}=M_i(\mathbf{I};\boldsymbol{\theta}_i)\in\mathbb{R}^d\)</span>，它在随后的两个关键操作中使用：首先，用它来计算InfoNCE损失的[7]<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>；其次，它通过一个概率头来生成交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>的对数。<br/>​  <strong>概率头：</strong>将每个专家模型的高维特征向量<spanclass="math inline">\(\mathbf{v}_{i}\)</span>输入到相应的可训练概率头<spanclass="math inline">\(H_i(\mathbf{v}_i;\phi_i)\)</span>中，由<spanclass="math inline">\(\phi_i\)</span>参数化。概率头将特征向量转换为二维对数向量<spanclass="math inline">\(\mathbf{z}_{i} = H_{i}(\mathbf{v}_{i};\phi_{i})\in \mathbb{R}^{2}\)</span>。然后利用该对数向量计算交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{CE}}\)</span>，这直接影响了模型区分真假图像的能力。</p><h3 id="聚类目标">2.4.2聚类目标</h3><p>​  我们的框架中的聚类目标是将专家模型产生的高维特征<spanclass="math inline">\(\mathbf{v}_i\in\mathbb{R}^d\)</span>变成有语义意义的聚类。这一目标对于提高模型区分真实和虚假图像的能力至关重要，特别是在处理不同的和看不见的伪造类型时。<br/>​  在每个专家模型<spanclass="math inline">\(M_i(\mathbf{I};\boldsymbol{\theta}_i)\)</span>从其相应的折叠<spanclass="math inline">\(\mathcal{F}_{i}\)</span>中处理图像后，使用得到的特征向量<spanclass="math inline">\(\mathbf{v}_i\)</span>来计算InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span>。InfoNCE损失鼓励模型在同一聚类内最大化特征向量的相似性，同时最小化不同集群之间的相似性。该损失的定义如下：<spanclass="math display">\[\mathcal{L}_{\text{NCE}}=-\log\frac{\exp(\sin(\mathbf{v}_q,\mathbf{v}_p)/\tau)}{\sum_j\exp(\sin(\mathbf{v}_q,\mathbf{v}_j)/\tau)},\]</span>​  其中，<span class="math inline">\(\mathbf{v}_q\)</span>和<spanclass="math inline">\(\mathbf{v}_p\)</span>分别表示同一聚类内的查询向量和正样本向量，<spanclass="math inline">\(\mathbf{v}_j\)</span>表示来自其他聚类的特征向量。函数<spanclass="math inline">\(sin(\cdot)\)</span>通常指余弦相似度，<spanclass="math inline">\(\tau\)</span>是一个温度尺度因子，可以调整正负样本的相对重要性。<br/>​  在我们的实现中，我们专注于通过最大化不同伪造特征和真实特征之间的距离来增强模型的区分能力。具体来说，我们将真-真样本对视为正样本，真-假样本对视为负样本。该方法可以使模型有效地区分真实图像和伪造图像，从而提高检测精度。这种基于聚类的目标不仅增强了模型在已知伪造类型上的性能，而且通过关注特征表示的相似性，提高了模型处理未知伪造的能力。</p><h3 id="对抗性目标">2.4.3对抗性目标</h3><p>​  对抗性目标被嵌入到训练管道中，以增强模型对对抗性攻击的鲁棒性。这是通过使用kl-散度损失<spanclass="math inline">\(\mathcal{L}_{\mathrm{KL}}\)</span>来实现的，它策略性地应用于对抗性的例子（AEs，adversarialexamples）<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}\in\mathbb{R}^{H\timesW\times C}\)</span>，这些例子是通过在原始输入图像<spanclass="math inline">\(\mathbf{I}\)</span>中添加小扰动<spanclass="math inline">\(\delta\in\mathbb{R}^{H\times W\timesC}\)</span>而产生的。<br/>​  用 $~f_{i}=M_{i}\circ H_{i}\circ\sigma $表示第<spanclass="math inline">\(i\)</span>个专家模型的预测概率的函数，其中<spanclass="math inline">\(\sigma(\cdot)\)</span>为softmax函数。AE生成的优化目标是最大限度地提高<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}\)</span>与<spanclass="math inline">\(\mathbf{I}\)</span>之间的预测距离，如下： <spanclass="math display">\[\mathbf{I}_{\mathrm{adv}}^{(n,i)}=\underset{\mathbf{I}_{\mathrm{adv}}^{(n,i)}\in\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})}{\operatorname*{\arg\max}}D_{\mathrm{KL}}(f_{i}(\mathbf{I}_{\mathrm{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)})),\]</span>​  其中，<spanclass="math inline">\(\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})=\{\mathbf{I}_{\mathrm{adv}}:\|\mathbf{I}_{\mathrm{adv}}-\mathbf{I}^{(n)}\|_{\infty}\leq\epsilon\}\)</span>是半径为<spanclass="math inline">\(\epsilon\)</span>、以<spanclass="math inline">\(\mathbf{I}^{(n)}\)</span>为中心的<spanclass="math inline">\(\ell_{\infty}\)</span>球，<spanclass="math inline">\(D_{\mathrm{KL}}\)</span>是KL-散度。<br/>​  为了解决这一优化问题，我们通过每个专家模型的投影梯度上升迭代更新AE：<spanclass="math display">\[\mathbf{I}_{\mathrm{adv}}^{(n,i)}\leftarrow\Pi_{\mathcal{B}_{\epsilon}(\mathbf{I}^{(n)})}\left(\mathbf{I}_{\mathrm{adv}}^{(n,i)}+\alpha\cdot\mathrm{sign}\left(\nabla_{\mathbf{I}_{\mathrm{adv}}^{(n,i)}}D_{\mathrm{KL}}(f(\mathbf{I}_{\mathrm{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)}))\right)\right),\]</span>​  其中，<spanclass="math inline">\(\Pi(\cdot)\)</span>表示投影函数，<spanclass="math inline">\(\alpha\)</span>表示内部步长。<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}^{(n,i)}\)</span>是通过向<spanclass="math inline">\(\mathbf{I}^{(n)}\)</span>添加一个随机高斯噪声来初始化的。<br/>​  一旦生成了<spanclass="math inline">\(\mathbf{I}_{\mathrm{adv}}^{(n,i)}\)</span>，这些AEs就会被纳入模型训练过程中，以增强模型的鲁棒性。由于良性样本与AEs之间存在显著的分布偏移，因此在训练过程中直接包含AEs可能会降低模型在良性样本上的性能。为了实现准确性和鲁棒性之间更好的权衡，在对抗性的目标中同时考虑良性和对抗性的例子列举如下：<spanclass="math display">\[\mathcal{L}_{\text{KL}}=\sum_{n=1}^{N}\sum_{i=1}^{I}D_{\text{KL}}(f_{i}(\mathbf{I}_{\text{adv}}^{(n,i)})\|f(\mathbf{I}^{(n)})),\]</span>​  其中，N为训练样本的数量，<spanclass="math inline">\(I\)</span>为专家模型的数量，<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>为所有专家生成的所有AEs的kl-散度之和。<br/>​  这种对抗性训练策略确保模型在各种场景中很好地概括，包括那些可能遇到恶意改变的图像的场景。</p><h3 id="分类目标">2.4.4分类目标</h3><p>​  在交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>的驱动下，分类目标是模型准确地将图像分类为真假图像的核心。当使用由标签平滑[8]生成的软标签<spanclass="math inline">\(\mathbf{y}^{(n)}\in[0,1]^2\)</span>作为第n个样本的groundtruth时，每个专家模型<spanclass="math inline">\(M_i(\mathbf{I};\boldsymbol{\theta}_i)\)</span>的概率头<spanclass="math inline">\(H_i(\mathbf{v}_i;\phi_i)\)</span>输出一个二维对数向量<spanclass="math inline">\(\mathbf{z}_i^{(n)} = H(\mathbf{v}_i;\phi) \in\mathbb{R}^2\)</span>。通过对softmax函数进行对数分析，得到了通过第<spanclass="math inline">\(i\)</span>个专家和第<spanclass="math inline">\(n\)</span>个样本的头部的预测概率<spanclass="math inline">\(\hat{\mathbf{y}}_i^{(n)}\)</span>： <spanclass="math display">\[\hat{\mathbf{y}}_i^{(n)}=\sigma\left(\mathbf{z}_i^{(n)}\right).\]</span>​  然后，将带有软标签的CE损失计算为： <spanclass="math display">\[\mathcal{L}_{\mathsf{CE}}=-\sum_{n=1}^N\sum_{i=1}^I\sum_{c=0}^1\mathbf{y}_c^{(n)}\cdot\log(\hat{\mathbf{y}}_{i,c}^{(n)}),\]</span>​  其中，<span class="math inline">\(\mathbf{y}_c^{(n)}\)</span>为第<spanclass="math inline">\(n\)</span>个样本中<spanclass="math inline">\(c\)</span>类的软标签，<spanclass="math inline">\(\hat{\mathbf{y}}_{i,c}^{(n)}\)</span>为通过第<spanclass="math inline">\(i\)</span>个专家模型和分类头对应的预测概率。软标签的使用允许模型捕获更微妙的信息，特别是在类之间的区别不是严格的二进制的情况下，可能导致更鲁棒和更广义的学习结果。<br/>​  CE损失会惩罚错误分类的模型，迫使它产生与groundtruth非常匹配的输出。这确保了每个专家模型都有助于一个鲁棒和准确的最终预测，促进可靠地区分真实和被操纵的图像。</p><h3 id="整体优化">2.4.5整体优化</h3><p>​  总体优化策略将上述各种目标——InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>、对抗性训练损失<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>和交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>——协调为一个单一的连贯训练目标。总损失函数表示为：<spanclass="math display">\[\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{NCE}}+\lambda_{1}\mathcal{L}_{\mathrm{KL}}+\lambda_{2}\mathcal{L}_{\mathrm{CE}}.\]</span>​  在这里，<span class="math inline">\(\lambda_{1}\)</span>和<spanclass="math inline">\(\lambda_{2}\)</span>是调整每个组件损失的相对影响的加权参数。这种集成的方法确保了该模型不仅能够高精度地区分真实样本和虚假样本，而且对对抗性攻击具有鲁棒性，并能够跨不同的数据分布进行泛化。通过平衡这些目标，训练过程产生了一个既强大又通用的模型，使其非常适合于深度伪造检测的复杂任务。</p><h2 id="第三阶段推理">2.5第三阶段：推理</h2><p>​  推理过程利用了鲁棒的训练架构来提供精确和可靠的预测。当图像<spanclass="math inline">\(\mathbf{I}\in\mathbb{R}^{H\times W\timesC}\)</span>时，它被直接输入训练过的专家模型<spanclass="math inline">\(\{M_1(\mathbf{I};\phi_1),M_2(\mathbf{I};\phi_2),\ldots,M_I(\mathbf{I};\phi_I)\}\)</span>，每个对应于训练过程中形成的一个集群。每个专家模型生成一个高维特征向量<spanclass="math inline">\(\mathbf{v}_{i} = M_{i}(\mathbf{I};\phi_{i}) \in\mathbb{R}^{d}\)</span>，然后由相应的概率头处理，生成一个二维logit向量<spanclass="math inline">\(\mathbf{z}_{i}=H_{i}(\mathbf{v}_{i};\phi_{i}) \in\mathbb{R}^{2}\)</span>。最终的预测是通过平均所有专家模型的对数并应用sigmoid函数来确定的：<spanclass="math display">\[\hat{\mathbf{y}}=\sigma\left(\frac{1}{I}\sum_{i=1}^{I}\mathbf{z}_{i}\right).\]</span>​  为了进行分类预测，选择概率最高的类为： <spanclass="math display">\[\hat{y}=\arg\max_{c\in\{0,1\}}(\hat{\mathbf{y}}_c),\]</span>​  其中，c是这些类的索引。<br/>​  这种集成方法结合了所有专门模型的专业知识，确保了预测不仅准确，而且对变化和潜在的对抗性扰动具有鲁棒性。来自多个专家模型的输出的聚合减少了单独偏移的风险，从而更可靠地最终确定输入的真实性。</p><h1 id="多维人脸伪造检测分析35">3.多维人脸伪造检测分析（35%）</h1><h2 id="方法复杂性">3.1方法复杂性</h2><p>​  表1列举了所提出的方法的网络架构细节，包括7个专家编码器，由来自效率网[1]和ConvNeXt[9]的变体组成，促进了跨不同复杂度级别的联合特征学习。</p><figure><img src="../postimages/KaggleNet/image-20240903161617646.png"alt="image-20240903161617646" /><figcaption aria-hidden="true">image-20240903161617646</figcaption></figure><p>​  虽然基于transformer的编码器也被探索，但没有观察到显著的性能改善。这可能表明，在这个竞争的MultiFF数据集中，卷积网络架构具有优势。综上所述，该方法的可训练参数计数为187.123M，可操作的MACs为103.572G。对于512x512的单个图像大小，平均推理速度为0.201774秒，标准偏差为0.024823秒。这些指标是通过在一个H800GPU卡上进行10,000次重复测试而获得的。</p><h2 id="方法训练">3.2方法训练</h2><p>​  在模型训练过程中，我们主要采用正负样本的平衡抽样、余弦退火学习率调整策略和指数移动平均（EMA，exponentialmovingaverage）加权权值平滑。<br/>​  <strong>平衡抽样：</strong>具体来说，通过观察官方数据集中正负样本的比例为1：4，我们有意地采用了正负样本的平衡抽样。该方法有助于避免类别不平衡问题，提高了分类器对少数类样本的准确性，增加了模型学习不同类别之间决策边界的可能性，从而提高了其对不可见数据的泛化能力。这一行动还通过防止模型以牺牲少数类样本为代价，过度关注大多数类样本，从而减轻了过拟合的风险。<br/>​  <strong>退火调整：</strong>此外，余弦退火学习速率调整策略有助于在训练过程中平稳地调整学习速率，防止显著的波动，提高训练的稳定性。它还能更好地收敛到局部最优解，提高了模型在训练集上的性能。<br/>​  <strong>指数移动平均：</strong>为了进一步提高模型的稳定性，防止其陷入局部最优状态，我们在训练过程中加入了指数移动平均（EMA，exponentialmovingaverage）策略，以便在训练过程中平滑模型权值。EMA技术通过随时间平均模型参数，有助于稳定训练过程，从而减少噪声更新引入的方差，提高模型的整体性能。<br/>​  具体来说，在每个训练步骤t时，模型参数<spanclass="math inline">\(\theta\)</span>的EMA根据以下公式进行更新： <spanclass="math display">\[\boldsymbol{\theta}_\mathrm{EMA}^{(t)}=\gamma\boldsymbol{\theta}_\mathrm{EMA}^{(t-1)}+(1-\gamma)\boldsymbol{\theta}^{(t)},\]</span>​  式中，<spanclass="math inline">\(\boldsymbol{\theta}_\mathrm{EMA}^{(t)}\)</span>为步骤t处的平滑参数，<spanclass="math inline">\(\boldsymbol{\theta}^{(t)}\)</span>为当前模型参数，<spanclass="math inline">\(\gamma\)</span>为决定过去参数影响的衰减因子。在我们的实现中，我们将衰减因子<spanclass="math inline">\(\gamma\)</span>设置为0.995，这意味着在每一步中保留了99.5%的当前EMA值，剩下的0.5%由当前参数贡献。<br/>​  EMA的使用确保了模型参数的收敛更平稳，从而防止了可能导致次优解的急剧振荡。这种平滑效果也有助于减少过拟合的风险，因为它通过作为正则化的一种形式，有效地平均梯度更新中的噪声。<br/>​  我们的实验结果表明，将EMA策略集成到训练过程中，可以显著提高模型的性能。具体来说，我们观察到在曲线下面积（AUC）度量中增加了大约0.005，这表明模型的准确性和稳定性得到了提高。这一改进强调了EMA在改进模型的预测能力方面的有效性，使其在检测深度伪造方面更加鲁棒和可靠。</p><h2 id="方法概论">3.3方法概论</h2><p>​  在本节中，我们使用交叉熵损失和AUC评估度量在MultiFF数据集上评估所提方法的性能，如表2所示。<br/><imgsrc="../postimages/KaggleNet/image-20240903163802649.png"alt="image-20240903163802649" /></p><p>​  具体来说，我们引入了几个基线来进行比较。这些基线使用标准的官方训练/验证分割进行训练，并仅使用交叉熵损失进行优化。从表中可以看出，虽然基线在验证集上表现良好，达到AUC得分高于0.99，但它们并不能有效地推广到公共测试集。例如，使用Efficient-b0的基线在公共测试集上只达到了0.86397的AUC。<br/>​  相比之下，我们提出的框架在数据集级别引入了创新的聚类，使得相应的验证集更具挑战性。此外，通过结合聚类和对抗性优化目标，学习到的伪造特征表现出增强的泛化性。这种增强允许由我们的框架支持的Efficient-B0实现0.90767的AUC，超过相应的基线4.37%的AUC。最后，通过整合来自不同专家（网络）的信息，我们的方法在公共测试集上实现了0.98051的AUC，将其置于初步排行榜的前沿，并验证了我们提出的解决方案的有效性。</p><h2 id="方法鲁棒性">3.4方法鲁棒性</h2><p>​  虽然所提出的模型主要是为了增强跨域场景中的检测泛化，但我们也评估了其在更常见的退化场景中的鲁棒性，如添加噪声[10]、JPEG压缩、模糊和降采样。这种评估在现实应用中是至关重要的，因为这些类型的后处理操作通常被用来消除或隐藏伪造的伪造痕迹。为了实现这一点，我们将这些后处理操作应用于原始验证集，并在图6中报告定量结果。</p><figure><img src="../postimages/KaggleNet/image-20240903164202077.png"alt="image-20240903164202077" /><figcaption aria-hidden="true">image-20240903164202077</figcaption></figure><p>​  可以观察到，对于在给定的质量因子（QF）范围内添加高斯噪声或执行JPEG压缩的场景，检测性能几乎保持不变。对于高斯模糊和降采样，性能下降略有增加，约为3%左右。这些评估结果表明，我们提出的模型对这些常见的后处理操作表现出理想的鲁棒性。</p><h2 id="消融研究">3.5消融研究</h2><p>​  本小节分析了每个组件在第一阶段的重新分裂策略，以及第二阶段的聚类、对抗和分类目标方面，如何对我们提出的框架有贡献。消融结果列在表3中，其中第一行给出了基线框架的性能作为比较，而最后一行是经过各种消融研究后建立的最佳焦点框架。</p><figure><img src="../postimages/KaggleNet/image-20240903164320811.png"alt="image-20240903164320811" /><figcaption aria-hidden="true">image-20240903164320811</figcaption></figure><p>​  我们从第一阶段的数据集的重新分割开始了我们的消融研究，关键在于分割策略的选择。为此，我们探索了利用BIRCH[11]、DBSCAN[12]和K-means[13]进行特征聚类。从表3中第2行到第4行所描述的结果可以看出，很明显，使用重新分割策略可以有效地增强通用性。值得注意的是，与基线相比，使用K-means进行分割的AUC最大增加了2.43%。此外，我们还对第二阶段引入的优化目标进行了相应的消融实验和比较实验。特别是在特性层面，我们采用现有的损失，如Triplet[14]、DCL[15]和原始InfoNCE[7]。然而，并非所有这些损失函数都适用于伪造检测任务。例如，Triplet损失将相等的惩罚强度限制为每个查询正或查询负对的距离得分[16]，这将导致模型崩溃，如表的#5所示。虽然DCL和InfoNCE提供了一定的性能提升，但它们考虑负-负对的加权可能是不合适的，因此限制了它们的适用性。相比之下，我们提出的<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>，考虑到伪造数据的多样性，仅仅为了最大限度地提高其与真实数据的距离而施加限制，从而进一步提高了0.68%。对于对抗性目标，我们用现有的算法如FGSM[17]、PGD [18]和C&amp;W[19]，以及focal损失[20]进行了分类目标的实验。这些算法表现出相对具有可比性的性能。由于时间的限制，我们打算在未来进行更详细的实验比较。值得注意的是，表3中显示的损失和AUC可能并不能完全反映测试集的性能。</p><h1 id="全局方法描述25">4全局方法描述（25%）</h1><h2 id="预先训练好的模型">4.1预先训练好的模型</h2><p>​  在第一阶段，为了增加数据集和增加伪造类型的多样性，我们使用了预先训练的SD[21]模型，特别是来自Stability AI的Stable Diffusion 2.1 Base[22]。该模型以其生成高质量图像的能力而闻名，它被用来合成额外的伪造图像，从而能够创建一个更全面的数据集，其中包括更广泛的伪造模式。通过使用该模型进行数据增强，我们的目的是增强我们的检测算法对稳定扩散生成样本的鲁棒性。<br/>​  在第二阶段，我们使用了在ImageNet-1K[23]上预先训练过的专家模型，如EfficientNet[1]和ConvNext[9]。之所以选择这些模型，是因为它们具有提取深度视觉语义的固有能力，这可以提高我们的模型的检测性能和收敛速度。</p><h2 id="额外生成数据">4.2额外生成数据</h2><p>​  我们总共生成了101,413张编辑过的图像，其中包括51,258张来自训练集中假样本增强的图像和50,155张来自真实图像生成的图像。此外，我们使用稳定扩散生成了64451张图像，其中38584张使用提示“aClose Up of a face, barbie, doll-like style, highlydetailed”来创建娃娃脸和芭比娃娃一样的图像。这导致总共增加了165,864张图像。<br/>​  所有这些图像的大小都为512x512像素，总共占用了6.34GB的磁盘空间，平均图像大小约为40KB。这些增强的数据集主要用于训练阶段，以增强数据的多样性和多样性，从而提高模型检测各种类型伪造的能力。</p><h2 id="定性优势">4.3定性优势</h2><p>​  为了进一步评估我们提出的方法在检测不同或未知类型的伪造（如人脸交换、人脸再现、面部属性编辑、人脸合成等），我们在著名的数据集FaceForensics++[24]、DFDC[25]和DFD[26]上进行了额外的实验。以AUC为评价指标的实验结果如表4所示。本文介绍了最先进的方法，如SBI[27]、RECEE [28]和CFM[29]，以与我们的方法进行比较。为了确保公平的比较，我们在FaceForensics++训练集上重新训练了所有的方法。此外，还包括2019年DFDC比赛的冠军解决方案（DFDC-1st-place-[30]）作为参考。请注意，我们没有重新训练DFDC-1st-place的方法，因此，其结果仅供参考，不构成严格的比较。</p><figure><img src="../postimages/KaggleNet/image-20240903165509638.png"alt="image-20240903165509638" /><figcaption aria-hidden="true">image-20240903165509638</figcaption></figure><p>​  从表4可以明显看出，目前最先进的方法（SBI、RECEE和CFM）在域内测试集FaceForensic++上表现良好，AUC得分在0.99以上。然而，在跨域测试集DFDC和DFD上观察到显著的性能下降。例如，RECEE在DFDC上只获得了0.6690的AUC。这一现象强调，在现实世界的深度伪造检测任务中，未知类型的伪造会严重破坏法医算法的性能。因此，设计鲁棒的和可推广的学习策略是至关重要的。<br/>​  在我们提出的方法中，我们创新性地采用无监督聚类进行伪造类型分类，并引入了聚类和对抗性优化目标，旨在提高模型的鲁棒性和泛化性。结果表明，与基线相比，我们的方法在DFDC和DFD上的AUC评分可以分别提高4.29%和3.98%，达到了与最先进的方法相媲美的性能。接下来，我们将进一步探索针对不同类型和看不见的伪造攻击的技术，以确保深度伪造取证的可信度和可靠性。</p><h2 id="新颖性讨论">4.4新颖性讨论</h2><p>​  在我们提出的方法中，创新集中在以下关键方面：</p><ul><li>基于无监督聚类的特征重新分割：这种新的方法涉及到基于无监督聚类的特征的重新分割，有助于改进一般的伪造表示。</li><li>第二阶段的联合特征优化目标：优化引入了协同特征优化目标，增强了整体学习过程和模型泛化。</li></ul><p>​  这些创新的灵感来自于我们研究小组的学术出版物，这些出版物主要发表在信息法医领域的顶级期刊和会议上。具有代表性的出版物如下：</p><ul><li>H. W. Wu, J. T. Zhou, X. Y. Zhang, J. Y. Tian, and W. W. Sun,“Robust Camera Model Identification over Online Social Network SharedImages via Multi-Scenario Learning”, in TIFS’24 (CCF-A, JCR-Q1)[5].</li><li>S. R. Qi, Y. S. Zhang, C. Wang, J. T. Zhou, and X. C. Cao, “APrincipled Design of Image Representation: Towards Forensic Tasks”, inTPAMI’23 (CCF-A, JCR-Q1) [31].</li><li>H. W. Wu, J. T. Zhou, J. Y. Tian, J. Liu, and Y. Qiao, “Robust ImageForgery Detection against Transmission over Online Social Networks”, inTIFS’23 (CCF-A, JCR-Q1) [32]</li><li>F. P. Li, K. M. Li, J. Y. Tian and J. T. Zhou, “Regroup Median Lossfor Combating Label Noise”, in AAAI’24 (CCF-A, JCR-Q1) [4].</li><li>J. Liu, J. T. Zhou, J. D. Zeng, and J. Y. Tian, “DifAttack:Query-Efficient Black-Box Attack via Disentangled Feature Space”, inAAAI’24 (CCF-A, JCR-Q1) [6].</li><li>Y. M. Chen, J. Y. Tian, X. Y. Chen, and J. T. Zhou, “EffectiveAmbiguity Attack Against Passport-based DNN Intellectual PropertyProtection Schemes through Fully Connected Layer Substitution”, inCVPR’23(CCF-A, JCR-Q1) [33].</li><li>Y. M. Chen, H. W. Wu, and J. T. Zhou, “Progressive Poisoned DataIsolation for Training-time Backdoor Defense”, in AAAI’24 (CCF-A,JCR-Q1) [3].</li><li>B. B. Song, X. Y. Chen, S. N. Xu, and J. T. Zhou, “Under-DisplayCamera Image Restoration with Scattering Effect”, in ICCV’23 (CCF-A,JCR-Q1) [34].</li><li>H. W. Wu, J. T. Zhou, J. Y. Tian, and J. Liu, “Robust Image ForgeryDetection over Online Social Network Shared Images”, in CVPR’22 (CCFA,JCR-Q1) [35].</li><li>Y. M. Li, J. T. Zhou, X. W. Zheng, J. Y. Tian, and Y. Y. Tang,“Robust Subspace Clustering With Independent and Piecewise IdenticallyDistributed Noise Modeling”, in CVPR’19 (CCF-A, JCR-Q1) [10].</li></ul><h1 id="其他详细信息5">5其他详细信息（5%）</h1><h2 id="实施">5.1实施</h2><h3 id="系统配置">5.1.1系统配置</h3><p>​  我们的模型的实现是基于PyTorch[36]，这是一个被广泛使用的深度学习框架，以其灵活性和效率而闻名。我们的系统在Ubuntu22.04.3LTS上运行，确保开发和部署提供稳定和安全的环境。硬件配置包括800GB的DDR4内存，这为处理大型数据集和复杂的计算提供了足够的空间。此外，该设置利用了16NVIDIA V100和8 NVIDIAH800gpu，每个都配备了32GB的VRAM，以促进并行处理和加速训练时间。这种配置支持广泛的并行化，支持跨多个gpu分配计算任务，以提高性能和可伸缩性。</p><h3 id="数据的扩充和处理">5.1.2数据的扩充和处理</h3><p>​  我们使用来自Albumentations[37]库的图像增强函数来进一步增强我们的数据集。所使用的增强功能包括，但不限于，JPEG和WebP压缩、模糊、高斯噪声、随机亮度和对比度调整、随机伽马校正、音调变化、锐化和网格失真。这些增强被应用于引入可变性，并模拟现实世界伪造品中可能遇到的不同类型的失真和伪影。通过结合这些技术，我们的目标是使我们的模型更加鲁棒，并能够在广泛的条件下检测伪造物。</p><h3 id="超参数和优化设置">5.1.3超参数和优化设置</h3><p>​  我们精心选择了几个关键的超参数来优化模型的性能：<br/>​  <strong>聚类：</strong>无监督聚类算法的聚类数K设置为20。这一聚类步骤对于训练集和验证集的重新分割至关重要，以确保不同类型的伪造特征被有效地分组。<br/>​  <strong>专家模型：</strong>我们在集成中使用了<spanclass="math inline">\(I =7\)</span>个专家模型，每个模型都在通过聚类过程创建的不同折叠上进行训练。专家的这种多样性有助于捕获不同的视角，并提高模型的泛化能力。<br/>​  <strong>损失函数：</strong></p><blockquote><p>交叉熵损失<spanclass="math inline">\(\mathcal{L}_{\text{CE}}\)</span>：采用0.01的软标签平滑因子，通过放松模型对其预测的置信度来防止过拟合。<br/>&gt;对抗性训练损失<spanclass="math inline">\(\mathcal{L}_{\text{KL}}\)</span>：对半径为ϵ=0.4的ℓ∞球内产生扰动的对抗性例子，计算了kl-散度损失。对抗性步长α =0.1用于迭代更新。<br/>&gt; InfoNCE损失<spanclass="math inline">\(\mathcal{L}_{\text{NCE}}\)</span>：对此对比损失中使用的余弦相似度函数设置了温度尺度因子τ= 0.1，鼓励相似特征在分离不同特征的同时进行聚类。</p></blockquote><p>​  <strong>指数移动平均线：</strong>EMA的衰减因子γ设置为0.995。这保证了模型权值的有效平滑，减少了训练过程中噪声更新的方差，从而导致更稳定的收敛。<br/>​  <strong>整体损失加权：</strong>总损失函数Ltotal结合了三个主要的损失组成部分：LNCE、LKL和LCE。通过设置加权参数λ1 = 1和λ2 =10来平衡整体优化过程中对抗性训练损失和交叉熵损失的影响。</p><h2 id="人工努力">5.2人工努力</h2><p>​  人工努力主要用于使用SD生成合成图像。这个过程涉及到对超参数进行微调，如学习速率、批处理大小和迭代计数，以及精心制作精确的提示。在这个工作流程中，人工的努力是必要的，因为我们仔细评估生成的图像的质量。它们确保了合成数据是真实的和多样化的，这对于提高模型的检测伪造面孔的鲁棒性和准确性至关重要。</p><h2 id="耗时">5.3耗时</h2><p>​  在第一阶段的数据准备过程中，我们总共投入了30.5个小时。具体来说，大约花费了35分钟和30个小时来生成编辑和SD的增强图像。此外，分配40分钟用于训练聚类模型，13小时用于对训练数据（∼50万个样本）进行特征提取和无监督聚类。在第二阶段的培训过程中，平均要花费12个小时来培训一名专家编码器。在第三阶段的推理过程中，所提出的方法通常需要0.2秒来预测单个512×512图像。</p><h2 id="更大的影响">5.4更大的影响</h2><p>​  深度造假技术的出现对社会和个人都有着深远的影响。深度造假技术利用先进的人工智能技术制作超真实的虚假视频和图像，对隐私、民主和国家安全构成重大威胁。在社会层面上，深度造假可以被用来传播错误信息，操纵公众舆论，并破坏人们对媒体和机构的信任。对个人来说，这些风险包括身份盗窃、名誉损害和因未经授权使用他们的肖像而造成的心理痛苦。因此，发现和减轻深度造假影响的能力对于保护数字媒体的完整性以及保护个人和社会利益至关重要。<br/>​  这一挑战的组织在数字取证领域的推进中发挥着关键作用。这一挑战通过为研究人员提供了一个平台，以便在标准化条件下开发和测试新的检测算法，从而促进了创新。它还有助于创建反映真实世界场景的多样性和复杂性的综合数据集，从而提高检测方法的鲁棒性和通用性。此外，这些挑战提高了人们对深度造假的潜在危险的认识，并促进了学术界、工业界和政府机构之间的合作，以制定有效的对策。<br/>​  展望未来，一种新的多维面部伪造品的引入，预计将进一步增强我们对抗数字操作的能力。这一挑战可能会包括更广泛的伪造技术，不仅包括面部交换，还包括更微妙的变化，如表情变化和老化影响。通过解决这些额外的维度，研究人员可以开发出更复杂的检测模型，能够识别更广泛的操作范围。这将最终有助于建立一个更安全和更值得信赖的数字环境。</p><h1 id="致谢">致谢</h1><p>​  这项工作部分在SICC进行，并由澳门大学SKL-IOTSC支持。</p><h1 id="参考">参考</h1><p>[1] M. Tang and Q. Le, “Efficientnet: Rethinking model scaling forconvolutional neural networks,” in <strong>Proc. Int. Conf. Mach.Learn.</strong>, 2019, pp. 6105– 6114.</p><p>[2] L. V. der Maaten and G. Hinton, “Visualizing data using t-sne,”<strong>Journal of Mach. Learn. Res</strong>, vol. 9, no. 11, 2008.</p><p>[3] Y. Chen, H. Wu, and J. Zhou, “Progressive poisoned data isolationfor training-time backdoor defense,” <strong>Proceedings of the AAAIConference on Artificial Intelligence</strong>, vol. 38, no. 10, pp. 11425–11 433, Mar. 2024. [Online]. Available:https://ojs.aaai.org/index.php/AAAI/article/view/ 29023</p><p>[4] F. Li, K. Li, J. Tian, and J. Zhou, “Regroup median loss forcombating label noise,” <strong>Proceedings of the AAAI Conference onArtificial Intelligence</strong>, vol. 38, no. 12, pp. 13 474–13 482,Mar. 2024. [Online]. Available:https://ojs.aaai.org/index.php/AAAI/article/view/29250</p><p>[5] H. Wu, J. Zhou, X. Zhang, J. Tian, and W. Sun, “Robust cameramodel identification over online social network shared images viamultiscenario learning,” <strong>IEEE Transactions on InformationForensics and Security</strong>, vol. 19, pp. 148–162, 2024.</p><p>[6] J. Liu, J. Zhou, J. Zeng, and J. Tian, “Difattack:Query-efficient black-box adversarial attack via disentangled featurespace,” <strong>Proceedings of the AAAI Conference on ArtificialIntelligence</strong>, vol. 38, no. 4, pp. 3666–3674, Mar. 2024.[Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/28156</p><p>[7] A. Oord, Y. Li, and O. Vinyals, “Representation learning withcontrastive predictive coding,” <strong>arXivpreprint:1807.03748</strong>, 2018.</p><p>[8] R. Muller, S. Kornblith, and G. E. Hinton, “When does labelsmoothing help?” in <strong>Proc. Adv. Neural Inf. Process.Syst.</strong>, 2019, pp. 8748–8763.</p><p>[9] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S.Xie, “A convnet for the 2020s,” <strong>Proc. Comput. Vis. PatternRecogn.</strong>, 2022.</p><p>[10] Y. Li, J. Zhou, X. Zheng, J. Tian, and Y. Y. Tang, “Robustsubspace clustering with independent and piecewise identicallydistributed noise modeling,” in <strong>Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR)</strong>,June 2019.</p><p>[11] T. Zhang, R. Ramakrishnan, and M. Livny, “Birch: an efficientdata clustering method for very large databases,” in <strong>Proceedingsof</strong> <strong>the 1996 ACM SIGMOD International Conference onManagement of Data</strong>, ser. SIGMOD ’96. New York, NY, USA:Association for Computing Machinery, 1996, p. 103–114. [Online].Available: https://doi.org/10.1145/233269.233324</p><p>[12] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-basedalgorithm for discovering clusters in large spatial databases withnoise,” in <strong>Proceedings of the Second International Conference onKnowledge Discovery and Data Mining</strong>, ser. KDD’96. AAAI Press,1996, p. 226–231.</p><p>[13] J. MacQueen <strong>et al.</strong>, “Some methods forclassification and analysis of multivariate observations,” in<strong>Proceedings of the fifth Berkeley symposium on mathematicalstatistics and probability</strong>, vol. 1, no. 14. Oakland, CA, USA,1967, pp. 281–297.</p><p>[14] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk, “Learninglocal feature descriptors with triplets and shallow convolutional neuralnetworks,” in <strong>Proc. British Mach. Vis. Conf.</strong>, 2016, pp.1–11.</p><p>[15] C.-H. Yeh, C.-Y. Hong, Y.-C. Hsu, T.-L. Liu, Y. Chen, and Y.LeCun, “Decoupled contrastive learning,” in <strong>Computer Vision –ECCV 2022</strong>, S. Avidan, G. Brostow, M. Ciss´e, G. M. Farinella,and T. Hassner, Eds. Cham: Springer Nature Switzerland, 2022, pp.668–684.</p><p>[16] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y.Wei, “Circle loss: A unified perspective of pair similarityoptimization,” in <strong>Proc. Comput. Vis. Pattern Recogn.</strong>,2020, pp. 6398–6407.</p><p>[17] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining andharnessing adversarial examples,” in <strong>Proc. Int. Conf. Learn.Representat.</strong>, 2015, pp. 1–11.</p><p>[18] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,“Towards deep learning models resistant to adversarial attacks,” in<strong>Proc. Int. Conf. Learn. Representat.</strong>, 2018.</p><p>[19] N. Carlini and D. Wagner, “Towards evaluating the robustness ofneural networks,” in <strong>2017 IEEE Symposium on Security and Privacy(SP)</strong>, 2017, pp. 39–57.</p><p>[20] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal lossfor dense object detection,” in <strong>Proc. IEEE Int. Conf. Comput.Vis.</strong>, 2017, pp. 2980– 2988.</p><p>[21] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,“Highresolution image synthesis with latent diffusion models,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2022, pp. 10 684–10 695.</p><p>[22] “Stable-diffusion-v2-1,” https://huggingface.co/stabilityai/stable-diffusion-2-1-base.</p><p>[23] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei,“Imagenet: a large-scale hierarchical image database,” in <strong>Proc.Comput. Vis. Pattern</strong> <strong>Recogn.</strong>, 2009, pp.248–255.</p><p>[24] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, andM. Nießner, “Faceforensics++: learning to detect manipulated facialimages,” in <strong>Proc. IEEE Int. Conf. Comput. Vis.</strong>, 2019,pp. 1–11.</p><p>[25] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang,and C. Ferrer, “The deepfake detection challenge (dfdc) dataset,”<strong>arXiv preprint arXiv:2006.07397</strong>, 2020.</p><p>[26] Google ai blog: Contributing data to deepfake detectionresearch. Accessed on: May 5, 2024. [Online]. Available:https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html</p><p>[27] K. Shiohara and T. Yamasaki, “Detecting deepfakes withself-blended images,” in <strong>Proc. IEEE Conf. Comput. Vis. PatternRecogn.</strong>, 2022, pp. 18 720–18 729.</p><p>[28] J. Cao, C. Ma, T. Yao, S. Chen, S. Ding, and X. Yang,“End-to-end reconstruction-classification learning for face forgerydetection,” in <strong>Proc. IEEE Conf. Comput. Vis. PatternRecogn.</strong>, 2022, pp. 4113–4122.</p><p>[29] A. Luo, C. Kong, J. Huang, Y. Hu, X. Kang, and A. C. Kot,“Beyond the prior forgery knowledge: Mining critical clues for generalface forgery detection,” <strong>IEEE Trans. Inf. ForensicsSecur.</strong>, vol. 19, no. 1, pp. 1168–1182, 2023.</p><p>[30] Dfdc 1st place solution. Accessed on: Aug 25, 2024. [Online].Available: https://www.kaggle.com/competitions/deepfake-detection-challenge/discussion/145721</p><p>[31] S. Qi, Y. Zhang, C. Wang, J. Zhou, and X. Cao, “A principleddesign of image representation: Towards forensic tasks,” <strong>IEEETransactions on Pattern Analysis and Machine Intelligence</strong>, vol.45, no. 5, pp. 5337–5354, 2023.</p><p>[32] H. Wu, J. Zhou, J. Tian, J. Liu, and Y. Qiao, “Robust imageforgery detection against transmission over online social networks,”<strong>IEEE Trans. Inf. Forensics Secur.</strong>, vol. 17, no. 1, pp.443–456, 2022.</p><p>[33] Y. Chen, J. Tian, X. Chen, and J. Zhou, “Effective ambiguityattack against passport-based dnn intellectual property protectionschemes through fully connected layer substitution,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2023, pp. 8123–8132.</p><p>[34] B. Song, X. Chen, S. Xu, and J. Zhou, “Under-display cameraimage restoration with scattering effect,” in <strong>Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV)</strong>,October 2023, pp. 12 580–12 589.</p><p>[35] H. Wu, J. Zhou, J. Tian, and J. Liu, “Robust image forgerydetection over online social network shared images,” in<strong>Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)</strong>, June 2022, pp. 13 440–13 449.</p><p>[36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E.Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L.Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style,highperformance deep learning library,” in <strong>Proc. Adv. NeuralInf. Process. Syst.</strong>, 2019.</p><p>[37] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M.Druzhinin, and A. A. Kalinin, “Albumentations: Fast and flexible imageaugmentations,” <strong>Information</strong>, vol. 11, no. 2, 2020.[Online]. Available: https://www.mdpi.com/2078-2489/11/2/125</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ImageForensicsOSN</title>
      <link href="/ImageForensicsOSN/"/>
      <url>/ImageForensicsOSN/</url>
      
        <content type="html"><![CDATA[<center>Robust Image Forgery Detection over Online Social Network Shared Images</center><center>Haiwei Wu, Jiantao Zhou, Jinyu Tian, and Jun Liu</center><center>智慧城市物联网国家重点实验室</center><center>澳门大学计算机与信息科学系</center><center>{yc07912, jtzhou, yb77405, yc07453}<span class="citation"data-cites="um.edu.mo">@um.edu.mo</span></center><h1 id="摘要">摘要</h1><p>​  Photoshop和美图等图像编辑软件的滥用，导致数字图像的真实性受到质疑。与此同时，网络社交网络（OSNs）的广泛使用使其成为传输伪造图像、报道假新闻、传播谣言等的主要渠道。不幸的是，osn所采用的各种有损操作，如压缩和调整大小，给实现鲁棒的图像伪造检测带来了巨大的挑战。为了对抗OSN共享的伪造行为，本文提出了一种新的鲁棒训练方案。我们首先对osn引入的噪声进行了彻底的分析，并将其解耦为两部分，即可预测的噪声和看不见的噪声，它们分别建模。前者模拟了所公开的（已知）osn操作所引入的噪声，而后者的设计不仅完成前一个，而且还考虑了探测器本身的缺陷。然后，我们将建模的噪声合并到一个鲁棒的训练框架中，显著提高了图像伪造检测器的鲁棒性。大量的实验结果验证了该方案与几种最先进的竞争对手相比的优越性。最后，为了促进图像伪造检测的未来发展，我们基于四个现有数据集和三个最流行的osn建立了一个公共伪造数据集。所设计的探测器最近在一次证书伪造检测竞赛中排名第一1。源代码和数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上找到</p><h1 id="i.-引言">I. 引言</h1><p>​  很少有研究明确地解决在普遍存在的OSN平台上针对损耗操作的鲁棒伪造检测的设计。这样的主题非常重要，因为这些有损耗的操作会严重降低检测性能。如图1所示，最先进的算法[42]可以准确地检测到原始伪造文件中的伪造区域；但在处理通过脸书传输的伪造文件时，检测性能会严重下降。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904212142816.png"alt="image-20240904212142816" /><figcaption aria-hidden="true">image-20240904212142816</figcaption></figure><p>​  为了减轻OSN的负面影响，第一个关键问题是分析和建模由OSN有损信道引入的噪声。然而，这是一个相当不同的问题，主要是因为当前的平台没有披露操纵传输图像的过程。虽然[33,34]透露了osn采用的部分操作，但仍有许多未知的操作，例如Facebook中的增强过滤。更重要的是，osn经常调整它们的图像处理管道，这使得建模更具挑战性。<br/>​  针对上述挑战，本文设计了一种鲁棒的图像伪造检测方法，以击败osn中的有损操作。具体来说，为了处理OSN的退化，我们提出了一种噪声建模方案，并将模拟噪声集成到一个鲁棒的训练框架中。我们将OSN噪声解耦为两个组成部分：1)可预测的噪声和2)看不见的噪声。前者被设计用于模拟已知操作（如JPEG压缩）所带来的可预测损失，其建模依赖于带有残差学习的深度神经网络（DNN）和嵌入的可微JPEG层。而后者主要是针对osn所采取的不可知的行动和/或各种osn的训练和测试之间的差异。显然，从信号本身的角度来看，为看不见的噪声建立一个合适的模型是不现实的。为了解决这个困难，我们将我们的观察结果从噪声的角度转移到伪造检测器上，只关注可能导致检测性能下降的噪声。这种策略自然地孵化了一种新的算法，利用对抗噪声[35]的核心思想来建模看不见噪声，它本质上是难以察觉的扰动，会严重降低模型性能。结果表明，我们的鲁棒图像伪造检测方法具有优越的鲁棒性，性能优于几种先进的算法。我们的方案的检测结果的一个例子如图1所示。最后，我们基于四个现有的数据集[1,6,14,17]和三个OSN平台（脸书、微博和微信），构建了一个包含5000多个项目的公共伪造数据集。我们的主要贡献可以总结如下：</p><ul><li>我们提出了一种新的训练方案来对抗osn上传输的鲁棒图像伪造检测。该训练方案不仅对osn引入的可预测噪声进行建模，还结合了看不见噪声进一步提高鲁棒性。</li><li>与几种最先进的方法[12,27,37,42]相比，我们的模型取得了更好的检测性能，特别是在对抗osn传输的场景中。</li><li>我们基于四个现有的数据集[1,6,14,17]和三个平台（脸书、微博和微信）建立了一个公共伪造数据集。</li></ul><p>​  本文的其余部分组织如下。第2部分回顾一下相关的工作。第3部分通过提出的鲁棒训练策略，详细介绍了鲁棒图像伪造检测。实验结果见第4部分和第5部分。</p><h1 id="相关工作">2.相关工作</h1><h2 id="图像伪造检测">2.1.图像伪造检测</h2><p>​  许多取证方法（例如，[2、3、5、7、8、18-23、26、36、39、40]）已经被提出来验证数字图像的真实性。这些方法通过特定的伪影来检测锻造区域，如拼接[18,26]、复制移动[23,39]、中值滤波[8,20]、插入绘画[21,22,36]等。为了更好地适应实际需求，越来越多的方法被开发来解决检测一般（复合）类型的伪造[4,5,11,12,27,37,41,42]的问题，其中基于深度学习的方法是最成功的。沿着这条线，Wu等人[37]提出了一种一般的伪造检测网络MT-Net，该网络首先提取图像处理特征，然后识别异常区域。Mayer和Stamm最近，[27]引入了法医相似性，以确定两个图像补丁是否包含相同的法医痕迹。从相机指纹的角度来看，科佐利诺和维多里瓦·[12]设计了一种提取相机模型指纹的方法，称为噪声指纹，以揭示伪造的区域。为了学习通用伪造的痕迹，Zhuang等人[42]使用了使用ps脚本的训练数据生成策略。</p><h2id="在线社交网络osnonline-social-network">2.2.在线社交网络（OSN，OnlineSocial Network）</h2><p>​  Facebook、Wechat、Weibo等各种OSN平台的普及，大大简化了图片的传播和共享。然而，正如许多现有的作品[33,34]所表明的那样，几乎所有的osn都以一种有损的方式操作上传的图像。这些有损操作所带来的噪声会严重影响法医方法的有效性。以在[32–34]中发现的Facebook为例，这些操作主要包括三个阶段：调整大小、增强过滤和JPEG压缩。具体来说，如果图像的分辨率高于2048像素，则将应用调整大小。然后，对图像中一些选定的块进行高度自适应和复杂的增强滤波。正如在[33,34]中提到的，由于这些增强过滤操作的适应性，要精确地了解它们是非常具有挑战性的。最后，对图像进行一轮JPEG压缩，并根据图像内容自适应地确定一个质量因子（QF）。通过对[33]中提供的数据集的分析，Facebook使用的QF值范围从71到95。尽管在不同的OSN平台上的图像操作是不同的，主流osn进行的操作仍然有许多相似之处（例如，无处不在的JPEG压缩）[33]。<br/>​  一些现有的取证[9,24,38]被设计用来识别所涉及的传输操作。Liao等人[9,24]首先提出了一种基于盲信号分离的两种操作识别的特征解耦方法。为了进一步揭示一个长链，You等人的[38]提出了一个解决方案，通过创新性地将操作链检测表示为一个机器翻译问题。<br/>（[38]J. You, Y. Li, J. Zhou, Z. Hua, W. Sun, and X. Li. A transformer basedapproach for image manipulation chain detection. In <em>ACM Int. Conf.Multimedia</em>, pages 3510–3517. ACM, 2021. 3）</p><h1id="针对通过osn进行传输的鲁棒图像伪造检测">3.针对通过osn进行传输的鲁棒图像伪造检测</h1><p>​  在本节中，我们将详细介绍针对osn上传输的鲁棒图像伪造检测方案。导致成功的关键技术是适当地建模osn导致的退化，并将这些知识集成到一个鲁棒的训练框架中。从第2部分第二小节得知，OSN中的图像处理操作相当复杂；其中一些可以精确地知道，而另一些只能部分已知，甚至完全未知。因此，我们建议将OSN噪声分为两种类型：1)可预测噪声和2)不可见噪声。前一种类型对应于退化源被明确识别的情况。而后一种类型是各种噪声不确定性的组合，包括未知的建模/参数，训练和测试osn之间的差异，甚至是一些完全看不见的退化源。通过在训练阶段添加建模的OSN噪声，检测器有望学习到更广义的特征，能够在OSN传输中存活下来，从而显著提高整体伪造检测性能。<br/>​  在图2中，我们说明了我们的伪造检测鲁棒训练方案的框架，它包括四个阶段。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904213429555.png"alt="image-20240904213429555" /><figcaption aria-hidden="true">image-20240904213429555</figcaption></figure><p>​  粗略地说，阶段1和阶段2是专门通过一个可微的网络来模拟可预测的噪声。第三阶段处理通过对抗性噪声产生策略对看不见噪声的建模。最后，阶段4处理了图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的实际鲁棒训练。请注意，我们的鲁棒训练方案可以与任何基于深度学习的图像伪造检测器相结合。由于这项工作的重点更多的是在鲁棒的训练上，我们在下面将我们的注意力限制在1-3阶段，而留下<spanclass="math inline">\(f_{\theta}\)</span>的细节在第4部分第1小结陈述。<br/>​  形式上，设<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>分别表示可预测噪声和不可见噪声，因此在鲁棒训练阶段考虑的复合噪声变为<span class="math display">\[\delta=\tau+\xi.\]</span>​  对于每次训练迭代，我们首先采样两个原始的3通道（RGB）彩色图像<spanclass="math inline">\(\{\mathbf{p}_1,\mathbf{p}_2\} \in\mathbb{R}^{H\times W\times3}\)</span>和一个二进制掩码<spanclass="math inline">\(\mathbf{y}\in\{0,1\}^{H\times\dot{W}\times1}\)</span>其中，1代表伪造的区域，0代表其他地方。然后一个伪造的图像x可以被合成为<spanclass="math display">\[\mathbf{x}=\mathbf{p}_1\odot(1-\mathbf{y})+\mathbf{p}_2\odot\mathbf{y},\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级别的乘法。在拥有一对伪造的图像和相应的ground-truth掩膜后，我们可以创建一个数据集<spanclass="math inline">\(\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^N\)</span>用于训练，其中，<spanclass="math inline">\(i\)</span>是训练样本的索引。在复合噪声<spanclass="math inline">\(\delta\)</span>下，图像伪造检测器<spanclass="math inline">\(f_{\theta}\)</span>的鲁棒训练可以表述为： <spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\delta})}\Big\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\delta}),\mathbf{y}_{i})\Big\},\]</span>​  式中，<spanclass="math inline">\(P(\boldsymbol{\delta})\)</span>表示复合噪声<spanclass="math inline">\(\delta\)</span>的分布，N为训练样本数，<spanclass="math inline">\(\mathcal{L}_{b}\)</span>为二值交叉熵（BCE）损失。<br/>​  在我们的噪声模型中，我们考虑了一个相当一般的设置，即两个噪声分量<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>是相关的。然后，采用鲁棒训练方案的等式(3)可以进一步写成：<spanclass="math display">\[\arg\min_{\boldsymbol{\theta}}\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{P(\boldsymbol{\tau})}\Big\{\mathbb{E}_{P(\boldsymbol{\xi}|\boldsymbol{\tau})}\{\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}+\boldsymbol{\xi}),\mathbf{y}_{i})\}\Big\},\]</span>​  其中，<span class="math inline">\(P(\boldsymbol{\tau})\)</span>为<spanclass="math inline">\(\tau\)</span>的边缘分布，<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>为给定<spanclass="math inline">\(\tau\)</span>的<spanclass="math inline">\(\xi\)</span>的条件分布。从实现的角度来看，在有足够数量的噪声样本时，可以有效和准确地计算出这些期望值。为了进行等式(4)的稳健训练，一个关键的任务是建模边缘分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>和条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。</p><h2 id="建模分布pboldsymboltau">3.1.建模分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span></h2><p>​  我们现在对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模，其中的退化是由OSN平台的有损操作引起的。从第2部分第2小节，我们知道<spanclass="math inline">\(\tau\)</span>的主要衰退源头是应用的JPEG压缩，而后处理（如增强滤波）也部分地影响了<spanclass="math inline">\(\tau\)</span>。对于一个图像<spanclass="math inline">\(\mathbf{x}_i\)</span>和一个固定的OSN平台，所产生的噪声可以很容易地计算出来：<spanclass="math display">\[\tau_i=\mathrm{OSN}(\mathbf{x}_i)-\mathbf{x}_i,\]</span>​  其中函数<spanclass="math inline">\(OSN(\cdot)\)</span>反映了给定OSN平台进行的所有操作。请注意，<spanclass="math inline">\(\tau_i\)</span>依赖于<spanclass="math inline">\(\mathbf{x}_i\)</span>，即噪声是依赖于信号的。通过这种方式，我们似乎可以生成大量的噪声样本，这可以用来模拟<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>的分布。然而，在实践中，这种简单的建模方案是相当有问题的。处理后的图像<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\)</span>必须通过将<spanclass="math inline">\(\mathbf{x}_i\)</span>上传到特定的OSN平台，然后下载来获得。一方面，这种程序很耗时间；另一方面，许多osn平台不允许过多地上传/下载操作。例如，如果在短时间内观察到太多的上传操作，Weibo甚至会禁止该账户。这严重限制了所获得的噪声样本的数量，使得这种幼稚的方案在实践中非常无效。<br/>​  为了解决这一挑战，我们采用了另一种策略，以一种不明确的方式建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。我们提出使用一个替代的深度网络来模拟OSN的操作，以便方便地产生大量的噪声样本<spanclass="math inline">\(\tau_i\)</span>。具体来说，为了与OSN平台上的图像处理管道保持一致，我们训练了一个DNN模型，该模型显式地嵌入了一个可微层来描述JPEG压缩。对于输入图像<spanclass="math inline">\(\mathbf{x}_i\)</span>，我们的目标是学习一个映射<spanclass="math inline">\(g_{\boldsymbol{\phi}} : \mathbb{R}^{d} \to\mathbb{R}^{d}\)</span>，其中，<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>是一个具有可训练参数<spanclass="math inline">\(\phi\)</span>的网络，用于预测了OSN的输出。我们为了<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>使用了U-Net架构[28]，因为它本质上是一个图像到图像的映射。训练过程如图2的第一阶段所示，然后在第二阶段使用训练良好的<spanclass="math inline">\(g_{\boldsymbol{\phi^*}}\)</span>来建模<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>。在训练阶段，我们以离线方式收集输入图像<spanclass="math inline">\(\mathbf{x}_{i}\in\mathbb{R}^{d}\)</span>和OSN传输版本<spanclass="math inline">\(\mathrm{OSN}(\mathbf{x}_i)\in\mathbb{R}^d\)</span>。训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数可以表示为：<spanclass="math display">\[\min_{\boldsymbol{\phi}}\Big\{\mathcal{L}_r(g_{\boldsymbol{\phi}}(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\},\]</span>​  其中，<spanclass="math inline">\(\mathcal{L}_{r}(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_{2}\)</span>。<br/>​  由于我们更感兴趣的是学习由OSN传输产生的噪声，而不是图像内容本身，我们在设计<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>时采用了一个残差学习结构[16]。考虑到这一点，我们将目标函数更改为：<spanclass="math display">\[\min_\phi\Big\{\mathcal{L}_r(\mathbf{x}_i+g_\phi(\mathbf{x}_i),\mathrm{OSN}(\mathbf{x}_i))\Big\}.\]</span>​  残差学习有利于模型的优化，显著提高了建模性能。<br/>​  此外，我们明确地将一个特殊的JPEG层集成到模型中，以便更好地生成结构性的、类似于JPEG的工件，这反映了各种OSN平台中的真实情况。实现了等式(7)中目标函数的端到端优化，我们需要确保JPEG压缩的每一步都是可微的。很容易发现量化是唯一不可微的步骤，主要是因为所采用的舍入函数<spanclass="math inline">\(\left\lfloor\cdot\right\rceil\)</span>到处都有0的导数。为了处理它，我们用可微版本[31]来近似舍入函数：<span class="math display">\[\lfloor x\rceil_a=\lfloorx\rceil+(x-\lfloor x\rceil)^3.\]</span>​  一旦有了一个可微的JPEG层，训练<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的目标函数就变成了：<spanclass="math display">\[\min_\phi\mathcal{L}_r(\mathcal{J}_q(\mathbf{x}_i+g_\phi(\mathbf{x}_i)),\mathrm{OSN}(\mathbf{x}_i)),\]</span>​  其中，<span class="math inline">\(\mathcal{J}_q\)</span>表示具有给定QFq的可微JPEG层。在我们的训练中，q在Facebook采用的[71,95]范围内均匀采样。然后就可以直接推导出噪声<spanclass="math inline">\({\tau}_i\)</span>为 <spanclass="math display">\[\tau_i(q)=\mathcal{J}_q(\mathbf{x}_i+g_{\boldsymbol{\phi}^*}(\mathbf{x}_i))-\mathbf{x}_i,\]</span>​  其中，通过求解优化问题等式(9)得到<spanclass="math inline">\(\phi^{*}\)</span>，q是与JPEG压缩相关联的QF。然后实现蒙特卡罗（MC，MonteCarlo）采样方案，生成大量的噪声样本，用于对分布<spanclass="math inline">\(P(\boldsymbol{\tau})\)</span>进行建模。</p><h2 id="建模条件分布pboldsymbolxiboldsymboltau">3.2.建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span></h2><p>​  然后，我们解决条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>的建模问题，从而可以解决等式(4)中的优化问题。我们加入噪声术语<spanclass="math inline">\(\xi\)</span>的原因是，可预测的噪声<spanclass="math inline">\(\tau\)</span>不能完全捕获在实践中遇到的噪声行为。例如，不同的osn可能采用不同的过程，如，动态调整QF，自适应地调整大小，甚至引入完全未知的操作。<br/>​  现在的一个关键问题是如何为看不见的噪声<spanclass="math inline">\(\xi\)</span>建立一个适当的模型。显然，就像我们在第3部分第1小节中所做的那样，从信号本身的特征来建模<spanclass="math inline">\(\xi\)</span>是不现实的。为了解决这一挑战，我们通过研究噪声对检测性能的影响，将我们的位置从噪声方面转移到检测器<spanclass="math inline">\(f_{\theta}\)</span>方面。在各种潜在的不可见噪声<spanclass="math inline">\(\xi\)</span>中，我们实际上只需要注意那些降低检测性能的噪声，而忽略了那些对检测影响很小的噪声。这促使我们在建模<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>时使用一种对抗性噪声[35]。从本质上讲，对抗性噪声通常是人类感官难以感知，但同时能够引起严重的模型输出错误。与此同时，我们所关注的看不见的噪声<spanclass="math inline">\(\xi\)</span>是一个能够欺骗探测器的噪声，而且通常也很小（一个高度失真的图像会偏离伪造的目的）。这种与检测器<spanclass="math inline">\(f_{\theta}\)</span>效果的相似性使得对抗性噪声成为建模<spanclass="math inline">\(\xi\)</span>的合适候选噪声。<br/>​  从对抗性的角度来看，有各种方法来定义噪声<spanclass="math inline">\(\xi\)</span>，只要通过添加噪声<spanclass="math inline">\(\xi\)</span>来创建的对抗性例子，就会跨越决策边界。注意到噪声<spanclass="math inline">\(\xi\)</span>通常振幅较小，我们提出沿着相对于输入代价函数的梯度设置<spanclass="math inline">\(\xi\)</span>的方向，以使噪声能量最小化。因此，对于给定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>、可预测噪声<spanclass="math inline">\({\tau}_i\)</span>和目标输出<spanclass="math inline">\(\mathbf{y}_i\)</span>，将不可见噪声<spanclass="math inline">\(\xi_i\)</span>表示为 <spanclass="math display">\[\boldsymbol{\xi}_{i}=\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\(\mathcal{S}\)</span>返回梯度的符号。通过在训练过程中加入这些对抗性噪声，期望使学习到的模型不仅对特定的对抗性噪声，而且对更一般的不可见噪声具有鲁棒性。<br/>​  然而，由等式（11）计算出的噪声依赖于特定的输入<spanclass="math inline">\(\mathbf{x}_i\)</span>，而不是适用于训练集中所有示例和未知示例的一般输入。为了全面提高检测器的泛化能力，我们提出将对抗性噪声的方向调整为一个全局梯度方向。为此，我们采用了一种类似于随机梯度下降（SGD，StochasticGradientDescend）[30]的策略，通过从训练数据集的随机子集中随机选择的随机近似方法。更具体地说，对于第<spanclass="math inline">\((t+1)\)</span>个输入<spanclass="math inline">\(\mathbf{x}_{t+1}\)</span>，<spanclass="math inline">\({\xi}_{t+1}\)</span>（<spanclass="math inline">\(\tau\)</span>条件）可以设置为从第一个t输入计算出的平均梯度，即：<spanclass="math display">\[\xi_{t+1}=\frac{1}{t}\sum_{i=0}^{t}\mathcal{S}(\nabla_{\mathbf{x}_{i}}\mathcal{L}_{b}(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}+\boldsymbol{\tau}_{i}+\boldsymbol{\xi}_{i}),\mathbf{y}_{i})),\]</span>​  其中，<spanclass="math inline">\({\xi}_{0}\)</span>被初始化为0。虽然等式（12）可以用来估计平均梯度，但是它只反映了特定的已知数据（训练数据）的梯度。为了缓解上述问题，进一步提高鲁棒性，我们提出在小范围内扰动<spanclass="math inline">\({\xi}_{t+1}\)</span>。在这里，使用参数模型来描述平均梯度会更理想。为了找到一个合适的平均梯度模型，我们首先采用数据驱动的方法，分析从训练过程中随机选择的1000个<spanclass="math inline">\(\xi\)</span>的样本的统计数据。在图3中，我们使用t-SNE[13]在一个二维空间中可视化了这些1000个随机样本。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904222741508.png"alt="image-20240904222741508" /><figcaption aria-hidden="true">image-20240904222741508</figcaption></figure><p>​  可以看出，样本点集中在某一中心周围，当它们离开该中心时逐渐消失。这一现象建议我们使用高斯分布来建模平均梯度，即：<spanclass="math display">\[\xi_{t+1}|\tau\sim\mathcal{N}(\boldsymbol{u}_{t+1},\sigma^2\mathbf{I}),\]</span>​  其中，<spanclass="math inline">\(\sigma\)</span>是一个控制方差的经验集参数， <spanclass="math display">\[\boldsymbol{u}_{t+1}=\epsilon\cdot\frac1t\sum_{i=0}^t\mathcal{S}(\nabla_{\mathbf{x}_i}\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_i+\boldsymbol{\xi}_i),\mathbf{y}_i)),\]</span>​  而<spanclass="math inline">\(\epsilon\)</span>是一个用于约束扰动大小的参数，以避免不必要的模型退化。<br/>​  在等式(13)中使用参数化模型后，我们可以很容易地生成噪声样本来建模条件分布<spanclass="math inline">\(P(\boldsymbol{\xi}|\boldsymbol{\tau})\)</span>。因此等式(4)可以扩展为<spanclass="math display">\[\min_{\boldsymbol{\theta}}\sum_{i=1}^N\sum_{j=1}^m\sum_{k=1}^h\mathcal{L}_b(f_{\boldsymbol{\theta}}(\mathbf{x}_i+\boldsymbol{\tau}_j+\boldsymbol{\xi}_k),\mathbf{y}_i),\]</span>​  其中，关于的期望分别近似于m和hMC样本。有了这个可计算的损失函数，我们就能够执行鲁棒训练，如算法1所示。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904223247902.png"alt="image-20240904223247902" /><figcaption aria-hidden="true">image-20240904223247902</figcaption></figure><h1 id="实验结果">4.实验结果</h1><p>​  在本节中，我们给出了实验结果，以证明我们所提出的方法的优越的性能。由于空间的限制，在补充文件中给出了更多的结果。</p><h2 id="实验设置">4.1.实验设置</h2><h3 id="基线检测器">基线检测器</h3><p>​ 该检测器的目标是在像素级的精度上检测伪造区域。具体来说，检测器<spanclass="math inline">\(f_{\boldsymbol{\theta}}: \mathbb{R}^{H\timesW\times3}\to\mathbb{R}^{H\timesW\times1}\)</span>以分辨率为H×W的彩色图像作为输入，最终输出检测结果的二进制图。在我们的设置中，我们在基线检测器中采用了U-Net[28]架构。为了提高提取伪造相关特征的能力，我们通过合并空间信道“Squeeze-and-Excitation（SE）”机制[29]进一步增强了架构，产生了一个称为SE-U-Net的变体，而不是简单地使用传统的普通U-Net。</p><h3 id="训练验证数据集">训练/验证数据集</h3><p>​  对于OSN网络<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>的训练，我们采用了数据集<spanclass="math inline">\(\mathbf{WEI}\)</span>（记为<spanclass="math inline">\(\mathcal{D}_{1}\)</span>）[33]，该数据集包含了1300多张原始图像及其处理后的版本。需要注意的是，我们只使用来自Facebook的数据来培训<spanclass="math inline">\(g_{\boldsymbol{\phi}}\)</span>。而对于<spanclass="math inline">\(f_{\boldsymbol{\theta}}\)</span>的训练，我们使用<spanclass="math inline">\(\mathbf{Dresden}\)</span>[15]数据集作为原始图像的来源。然后，我们通过将原始图像与来自<spanclass="math inline">\(\mathbf{MS-COCO}\)</span>[25]数据集中的对象拼接来生成伪造的图像。这些伪造图像的数据集记为<spanclass="math inline">\(\mathcal{D}_{2}\)</span>。<spanclass="math inline">\(\mathcal{D}_{1}\)</span>和<spanclass="math inline">\(\mathcal{D}_{2}\)</span>随机分为训练集和验证集，比例为9：1。</p><h3 id="测试数据集">测试数据集</h3><p>​  我们通过采用四种广泛使用的数据集（<spanclass="math inline">\(\mathbf{DSO}\)</span>[6]，<spanclass="math inline">\(\mathbf{Columbia}\)</span>[17]，<spanclass="math inline">\(\mathbf{NIST}\)</span>[1]和<spanclass="math inline">\(\mathbf{CASIA}\)</span>[14]）创建测试数据集，并生成它们的OSN传输版本。更具体地说，我们通过三个最流行的OSNs（Facebook、Wechat和Weibo）手动上传和下载上述数据集，得到了5232个伪造的数据集和相应的掩码。这些收集到的数据集可以在https://github.com/HighwayWu/ImageForensicsOSN上获得我们希望这些数据集可以作为我们的研究社区的有用的基准，以对抗在osn上共享的伪造品。</p><h3 id="对比网络">对比网络</h3><p>​  我们将我们提出的方案与四种最先进的方法进行了比较：<spanclass="math inline">\(\mathbf{MT-Net}\)</span> [37]、<spanclass="math inline">\(\mathbf{NoiPri}\)</span> [12]、<spanclass="math inline">\(\mathbf{ForSim}\)</span> [27]和<spanclass="math inline">\(\mathbf{DFCN}\)</span> [42]。</p><h2 id="定量比较">4.2.定量比较</h2><p>​  在像素域上的AUC、F1和IoU（越高越好）的定量比较见表1。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904224250921.png"alt="image-20240904224250921" /><figcaption aria-hidden="true">image-20240904224250921</figcaption></figure><p>表1.以AUC、F1、IoU为标准的定量比较。对于同一OSN传输中的每一列，最高值为粗体，“-”表示不应用。</p><p>​  在这里，我们还报告了基线检测器的结果，以一种比较的方式证明了我们的鲁棒训练方案的改进。可以观察到，当伪造图片不通过OSN传输时，ForSim[27]、DFCN [42]和我们的检测方法获得了类似的结果，而MT-Net [37]和NoiPri[12]的表现略差。需要注意的是，由于NoiPri的分辨率小，不能用于检测CASIA中的伪造，而我们的方法没有这样的限制，在CASIA上比其他竞争对手表现更好。<br/>​  在伪造图片通过osn的情况下，所有现有方法的检测性能都显著下降。例如，在通过Facebook、Weibo和Wechat传播后，与没有OSN传输的情况相比，与MT-Net相关的IoU得分分别下降了10.1%、11.1%和9.4%，相比之下，由于<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>的适当的噪声建模，我们提出的方法对OSN传输表现出相当理想的鲁棒性，并且仍然能导致准确的伪造检测。以Facebook为例，IoU的降幅仅为0.9%。我们也可以注意到，Weibo和Wechat的伪造检测性能下降略大，分别下降了2.0%和4.5%。这主要是因为，与Facebook相比，Weibo和Wechat对上传的图片采用了更严格的压缩，导致了更多的证据丢失。另外，为了训练我们的方法，我们只使用Facebook数据，根本没有任何Weibo和Wechat数据。从表1，我们可以看到使用Facebook数据训练的方案可以很好地推广到Weibo和Wechat传输的伪造上。</p><h2 id="定性比较">4.3.定性比较</h2><p>​  除了定量比较外，图4还给出了两个具有代表性的例子（更多结果见补充文件）。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225133906.png"alt="image-20240904225133906" /><figcaption aria-hidden="true">image-20240904225133906</figcaption></figure><p>图4.检测OSN传输伪造品的定性比较。对于每一行，从左到右的图像都是伪造（输入）、ground-truth、由MT-Net[37]、NoiPri [12]、ForSim [27]、DFCN[42]和我们生成的检测结果（输出）。从上到下的伪造品分别是没有OSN传输，以及有Facebook、Weibo和Wechat传输的案例。</p><p>​  可以看出，在正常情况下（没有OSN传输），现有的检测方法表现得相对较好，例如，第一种情况下的MT-Net和ForSim，以及第二种情况下的NoiPri和DFCN。然而，这些方法在OSN传输版本的情况下并不能达到令人满意的检测性能。以第二种情况下的NoiPri为例。对于Facebook、Weibo和Wechat传输的图像，识别出的伪造区域分布在多个物体上，使得伪造检测效果降低。相比之下，我们提出的方法可以学习更鲁棒的伪造特征，从而在这些具有挑战性的情况下产生更精确的检测结果，这主要归功于复合噪声建模的鲁棒训练方案。</p><h2 id="消融研究">4.4.消融研究</h2><p>​  我们现在通过分析每个建模噪声（即可预测噪声<spanclass="math inline">\(\tau\)</span>和不可见噪声<spanclass="math inline">\(\xi\)</span>）如何对最终检测性能的贡献，对我们提出的训练方案进行消融研究。为此，我们首先禁止在方案中使用每个噪声，然后在适当的设置下评估不同的再训练检测器的性能。所得结果见表2。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904225545236.png"alt="image-20240904225545236" /><figcaption aria-hidden="true">image-20240904225545236</figcaption></figure><p>​  可以看出，在检测器（#2行）训练中引入可预测的噪声<spanclass="math inline">\(\tau\)</span>可以略微提高检测性能（F1增益1.2%），这在Facebook传输中更明显（F1增益4.6%）。然而，由于只采用<spanclass="math inline">\(\tau\)</span>是不完整的，如在第3部分第2小节中提到的，我们进一步加入设计的不可见噪声<spanclass="math inline">\(\xi\)</span>。第3行的结果表明，<spanclass="math inline">\(\xi\)</span>可以有效地提高检测器的鲁棒性，带来更显著的改进（例如，F1增加8.6%）。最后，第4行证明，当同时应用复合噪声<spanclass="math inline">\(\tau\)</span>和<spanclass="math inline">\(\xi\)</span>时，检测器对目标环境的稳定性更强，这对于OSN传输上的伪造检测任务至关重要（例如，F1的增益为15.7%）。此外，我们没有只使用SE-U-Net作为检测器，而是采用了另一个著名的架构，DPN[10]，以展示我们提出的训练方案的多功能性。如第5行和第6行所示，我们的鲁棒性训练方法也可以很好地增强DPN的鲁棒性。</p><h2 id="一些进一步的鲁棒性评估">4.5.一些进一步的鲁棒性评估</h2><p>​  虽然该方案主要是为了对抗osn进行的有损操作，但我们也希望评估其在一些更常用的退化场景下的鲁棒性，如噪声添加、裁剪、调整大小、模糊和独立的JPEG压缩。这种评估在现实情况中是非常重要的，因为这些类型的后处理操作经常被用来清除或隐藏伪造的伪影。为此，我们将这些后处理操作应用于原始测试集Columbia，并在图5中报告了定量比较。</p><figure><img src="../postimages/ImageForensicsOSN/image-20240904230059643.png"alt="image-20240904230059643" /><figcaption aria-hidden="true">image-20240904230059643</figcaption></figure><p>​  为了便于演示，我们使用一个统一的参数p来控制不同操作的大小。横轴的原点（p=0）对应于没有任何后处理的情况。可以观察到，对比网络[12,27,37,42]不能随着扰动强度的增加而表现一致，而我们的方法可以很好地推广到击败这些后处理操作。</p><h1 id="结论">5.结论</h1><p>​  在本文中，我们提出了一种新的训练方案来提高图像伪造检测对各种基于OSN的传输的鲁棒性。该方案的设计借助于建模一个可预测的噪声<spanclass="math inline">\(\tau\)</span>以及一个有意引入的看不见的噪声<spanclass="math inline">\(\xi\)</span>。实验结果表明，我们的方案与几种最先进的方法相比具有优越性。此外，我们为未来的法医研究社区建立了一个osn传输的伪造数据集。</p><h1 id="致谢">致谢</h1><p>​  澳门科技发展基金2021-2021-2023-0072/2020/20200/015/2019/AMJ，060/2019/A1和077/2012，澳门大学研究委员会2018-00029-FST和MYRG2019-00023-FST，中国自然科学基金61971476，阿里巴巴集团通过阿里巴巴创新研究计划。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>科研工具</title>
      <link href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/"/>
      <url>/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<h1id="t-snet-distributedstochasticneighborembedding">1.t-SNE(t-DistributedStochasticNeighborEmbedding)</h1><p>​  t-SNE是一种用于探索高维数据结构的非线性降维技术。它特别适用于高维数据的可视化，因为它能够在低维空间中保留原始高维数据的局部结构。由于这个特性，t-SNE在机器学习和数据分析领域越来越受到重视。</p><h2 id="算法解读">1算法解读：</h2><p>​  t-SNE的核心思想是在高维空间中为数据点之间定义一种<ahref="https://zhida.zhihu.com/search?q=概率分布&amp;zhida_source=entity&amp;is_preview=1">概率分布</a>，表示点与点之间的相似性，然后在低维空间中创建一个相似的概率分布。通过最小化这两个分布之间的差异（使用KL散度），算法将高维数据映射到低维空间，以便我们可以可视化。</p><h2 id="步骤和细节">2步骤和细节：</h2><h3 id="step1.计算高维空间中的相似度">Step1.计算高维空间中的相似度</h3><p>​  我们使用高斯分布（正态分布）来计算点之间的相似性。高斯分布是一种常见的概率分布，其形状呈钟型，由均值和方差（标准差的平方）决定。高斯分布有一个很好的性质：它的形状由均值（中心点）和方差（分布的宽度）决定。当我们围绕一个数据点x画一个高斯分布时，这个分布会给予附近的点较高的概率值，而离得远的点则会有较低的概率值。这与我们直觉上对“相似性”的理解相一致：靠近的点更相似，远离的点不相似。</p><p>​  对于每个数据点<spanclass="math inline">\(x_i\)</span>，我们计算所有其他点<spanclass="math inline">\(x_j\)</span>与其的条件概率<spanclass="math inline">\(p_{j|i}\)</span>。这个概率反映了点<spanclass="math inline">\(x_j\)</span>是点<spanclass="math inline">\(x_i\)</span>的近邻的可能性。计算公式为: <spanclass="math display">\[p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\neqi}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}\]</span> ​  这里，分子部分计算了<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>之间的欧氏距离的平方（即<spanclass="math inline">\(-\|x_i-x_j\|^2\)</span>），然后通过高斯分布转换成概率。分母部分是一个归一化因子，确保所有<spanclass="math inline">\(p_{j|i}\)</span>的和为1。<br/>​  <spanclass="math inline">\(\sigma_{i}\)</span>是高斯分布的方差，决定了近邻的范围。不同的点可能有不同的密度，因此<spanclass="math inline">\(\sigma_{i}\)</span>对于每个点<spanclass="math inline">\(x_i\)</span>可能是不同的，需要通过一种叫做“困惑度”的量来确定。<br/>​  最后，为了得到一个对称的相似度矩阵，我们取<spanclass="math inline">\(p_{j|i}\)</span>和<spanclass="math inline">\(p_{i|j}\)</span>的平均值得到<spanclass="math inline">\(p_{ij}\)</span>: <spanclass="math display">\[p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}\]</span>​  这样，我们就得到了一个对称的相似度矩阵，其中的每个元素<spanclass="math inline">\(p_{ij}\)</span>都反映了数据点<spanclass="math inline">\(x_i\)</span>和<spanclass="math inline">\(x_j\)</span>在高维空间中的相似性。通过这一步，我们成功地量化了高维空间中数据点之间的相似性，为后续的低维空间嵌入奠定了基础。</p><h3 id="step2.初始化低维空间的点">Step2.初始化低维空间的点</h3><p>​  这一步可以是随机初始化，只需保证初始化点的数量和原数据相同，维度更低即可。</p><h3id="step3.计算低维空间的点的相似度">Step3.计算低维空间的点的相似度</h3><p>​  在t-SNE算法中，高维空间的相似度是通过高斯（正态）分布计算的，而低维空间的相似度是通过t分布（具体来说是自由度为1的t分布，也叫做柯西分布）计算的。这种设计的目的是为了解决“拥挤问题”。<br/>​  当我们将高维空间中的数据点降维到低维空间时，数据点之间的距离会发生变化。特别是在低维空间中，点与点之间可用的空间更少，容易出现拥挤的情况。如果直接使用高斯分布来计算低维空间的相似度，那么低维空间中远离的点之间的相似度可能会被过高地估计，导致降维结果的可视化效果不佳。<br/>​  t分布（自由度为1）有一个重要的特性：它的尾部比高斯分布更“厚”（heavy-tailed）。这意味着，在低维空间中，即使两个点距离较远，它们之间的相似度（通过t分布计算）也不会迅速减小到0。这有助于缓解拥挤问题，因为低维空间中远离的点之间的相似度会被较低地估计。<br/>​  在低维空间中，我们计算点<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的相似度<spanclass="math inline">\(q_{ij}\)</span>如下: <spanclass="math display">\[q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\neql}(1+\|y_k-y_l\|^2)^{-1}}\]</span>​  这个公式来源于自由度为1的t分布。分子部分计算了<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(y_j\)</span>之间的欧氏距离的平方，并转换成了概率。分母部分是一个归一化因子，确保所有的<spanclass="math inline">\(q_{ij}\)</span>之和为1。通过这种方式，我们得到了低维空间中点之间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}。接下来，t-SNE算法会试图使高维空间的相似度矩阵{<spanclass="math inline">\(p_{ij}\)</span>}和低维空间的相似度矩阵{<spanclass="math inline">\(q_{ij}\)</span>}尽可能地一致，从而得到合适的低维空间表示。</p><h3 id="step4.优化低维空间的点的位置">Step4.优化低维空间的点的位置</h3><p>​  通过最小化Kullback-Leibler散度(KL散度)来优化低维空间中的点的位置。KL散度用于衡量高维空间和低维空间中的相似度分布之间的差异。<span class="math display">\[C=\sum_{i\neqj}p_{ij}\log\frac{p_{ij}}{q_{ij}}\]</span>​  使用梯度下降方法来最小化KL散度，更新低维空间中的点的位置。 <spanclass="math display">\[\frac{\delta C}{\deltay_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+\|y_i-y_j\|^2)^{-1}\]</span>​  在梯度下降的计算中，输入是低维空间中每个点的坐标{<spanclass="math inline">\(y_j\)</span>}。这些坐标是我们要优化的参数。输出是低维空间中点与点之间的相似度{<spanclass="math inline">\(q_{ij}\)</span>}。这些相似度是由当前的低维坐标{<spanclass="math inline">\(y_j\)</span>}计算出来的。标签是高维空间中点与点之间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}。这些相似度是已知的，因为它们是由原始高维数据计算得出的。我们的目标是通过调整低维空间中的点的坐标{<spanclass="math inline">\(y_j\)</span>}（即输入），使得由这些坐标计算出的相似度{<spanclass="math inline">\(q_{ij}\)</span>}（即输出）尽可能接近已知的高维空间的相似度{<spanclass="math inline">\(p_{ij}\)</span>}（即标签）。<br/>​  为了实现这个目标，我们计算损失函数（即KL散度）相对于每个低维坐标的梯度，并使用这个梯度来更新低维坐标。这个过程会重复进行，直到达到预定的迭代次数，或者低维坐标的变化小于某个阈值。</p><h2 id="代码实现">3. 代码实现</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 假设 features 是你的特征向量，形状为 (256, 256, 64)</span><br><span class="line">features = np.random.rand(256, 256, 64)</span><br><span class="line"></span><br><span class="line"># 假设 mask 是你的掩码数组，形状为 (256, 256)，由0和1组成</span><br><span class="line">mask = np.random.randint(0, 2, size=(256, 256))</span><br><span class="line"></span><br><span class="line"># 重塑特征向量为 (65536, 64)</span><br><span class="line">reshaped_features = features.reshape(-1, 64)  # 65536 = 256 * 256</span><br><span class="line"></span><br><span class="line"># 将掩码展平为1D数组</span><br><span class="line">flattened_mask = mask.flatten()</span><br><span class="line"></span><br><span class="line"># 对特征向量应用 t-SNE</span><br><span class="line">tsne = TSNE(n_components=2, random_state=42)</span><br><span class="line">features_2d = tsne.fit_transform(reshaped_features)</span><br><span class="line"></span><br><span class="line"># 根据掩码的值来选择颜色</span><br><span class="line">colors = np.where(flattened_mask == 1, &#x27;red&#x27;, &#x27;blue&#x27;)</span><br><span class="line"></span><br><span class="line"># 可视化点云，不同的掩码值对应不同的颜色</span><br><span class="line">plt.figure(figsize=(10, 10))</span><br><span class="line">plt.scatter(features_2d[:, 0], features_2d[:, 1], c=colors, s=1)</span><br><span class="line">plt.title(&#x27;t-SNE Visualization of Features with Mask Colors&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="kl散度-kullback-leibler-divergence">2.KL散度 (Kullback-Leiblerdivergence)</h1><h2 id="kl散度定义">1. KL散度定义</h2><ul><li>两个概率分布(probability distribution)间差异的非对称性度量；</li><li>参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗。</li><li>KL散度（Kullback-Leibler divergence），也称为相对熵（relativeentropy），是用来衡量两个概率分布之间差异的一种指标。在机器学习中，KL散度常常用于度量两个概率分布之间的相似度或差异性。</li></ul><p><span class="math display">\[KL[P(X)||Q(X)]=\sum_{x\inX}[P(x)log\frac{P(x)}{Q(x)}]=E_{x\simP(x)}[log\frac{P(x)}{Q(x)}]\]</span></p><p>​  在这种情况下，KL散度衡量了教师模型的输出分布q和学生模型的输出分布p之间的差异。通过最小化KL散度损失，学生模型被鼓励从教师模型中学习，并产生相似的输出分布。</p><p>​  此外，KL散度还经常用于变分自编码器（VAEs）中。VAEs是一种生成模型，它们学习数据的低维表示，可以用于生成新样本。在VAEs中，KL散度被用来鼓励学习到的潜在变量遵循先验分布，例如标准正态分布。这有助于正则化模型并防止过拟合。<br/>​  聚类：KL散度可以用于聚类，以度量两个聚类之间的差异。在这种情况下，KL散度可以用于评估聚类质量，并指导聚类算法的优化过程。</p><p>​  在上面的概率拟合应用场景下， <spanclass="math inline">\(KL[P||Q]\)</span> 也被称为前向KL散度（forwardKullback-Leibler Divergence），将 <spanclass="math inline">\(KL[Q||P]\)</span> 称为反向KL散度（reverseKullback-LeiblerDivergence）。<br/>​  这里需要注意的是，只有在概率拟合的应用场景下（也就是确定了真实分布和拟合分布两个角色之后），前向KL散度<span class="math inline">\(KL[P||Q]\)</span>和反向KL散度 <spanclass="math inline">\(KL[Q||P]\)</span>的定义才是有意义的，否则二者只是相同公式改变正负号、并交换P和 Q符号表示之后的平凡结果。</p><details close><br/><summary>具体情况</summary><ol type="1"><li>标准正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span></li></ol><p>​  正态分布 <spanclass="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span>的概率密度函数为: <spanclass="math display">\[p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\]</span>​  标准正态分布 <spanclass="math inline">\(\mathcal{N}(0,1)\)</span>的概率密度函数为: <spanclass="math display">\[q(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\]</span>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu,\sigma^{2})\|\mathcal{N}(0,1))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}}{\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}\log\biggl\{\frac{1}{\sqrt{\sigma^{2}}}\mathrm{exp}\biggl\{\frac{1}{2}\bigl[x^{2}-(x-\mu)^{2}/\sigma^{2}\bigr]\biggr\}\biggr\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx \\&amp;=\frac{1}{2}\intp(x)[-\log\sigma^{2}+x^{2}-(x-\mu)^{2}/\sigma^{2}]dx\end{aligned}\]</span></p><p>​  整个结果分为三项积分，第一项实际上就是<spanclass="math inline">\(-log \sigma ^2\)</span> 乘以概率密度的积分（也就是1），所以结果是 <span class="math inline">\(-log \sigma ^2\)</span>；第二项实际是正态分布的二阶矩，熟悉正态分布的朋友应该都清楚正态分布的二阶矩为<span class="math inline">\(\mu^{2}+\sigma^{2}\)</span>；而根据定义，第三项实际上就是“-方差除以方差=-1”。所以总结果就是: <spanclass="math display">\[KL(\mathcal{N}(\mu,\sigma^2)\|\mathcal{N}(0,1))=\frac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)\]</span></p><ol start="2" type="1"><li>正态分布KL散度计算： <spanclass="math inline">\(\mathcal{N}(\mu_1,\sigma_1^{2})\)</span> 与 <spanclass="math inline">\(\mathcal{N}(\mu_2,\sigma_2^{2})\)</span></li></ol><p>​  KL散度计算: <spanclass="math display">\[\begin{aligned}&amp;KL(\mathcal{N}(\mu_{1},\sigma_{1}^{2})\|\mathcal{N}(\mu_{2},\sigma_{2}^{2}))\\&amp;=\sum p(x)\log\frac{p(x)}{q(x)}\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}(\log\frac{\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}}{\frac{1}{\sqrt{2\pi\sigma_{2}^{2}}}e^{-(x-\mu_{2})^{2}/2\sigma_{2}^{2}}})dx\\&amp;=\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}\log\{\frac{\sqrt{\sigma_{2}^{2}}}{\sqrt{\sigma_{1}^{2}}}\mathrm{exp}\{\frac{1}{2}[\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]\}\}dx\\&amp;=\frac{1}{2}\int\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-(x-\mu_{1})^{2}/2\sigma_{1}^{2}}[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\\&amp;=\frac{1}{2}\intp(x)[\log\sigma_{2}^{2}-\log\sigma_{1}^{2}+\frac{(x-\mu_{2})^{2}}{\sigma_{2}^{2}}-\frac{(x-\mu_{1})^{2}}{\sigma_{1}^{2}}]dx\end{aligned}\]</span>​  整个结果分为四项积分，第一项实际上就是 <span class="math inline">\(log\sigma_2 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是<spanclass="math inline">\(log \sigma_2 ^2\)</span>；第二项实际上就是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>乘以概率密度的积分（也就是 1），所以结果是 <spanclass="math inline">\(-log \sigma_1 ^2\)</span>；第三项实际是异正态分布的二阶矩，熟悉正态分布的朋友应该都清楚异正态分布的二阶矩为$ $ ；而根据定义，第四项实际上就是“-方差除以方差=-1”。所以总结果就是:<spanclass="math display">\[KL(\mathcal{N}(\mu_1,\sigma_1^2)\|\mathcal{N}(\mu_2,\sigma_2^2))=\frac{1}{2}(\log\sigma_2^2-\log\sigma_1^2+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-1)\]</span></p></details><h2 id="两类kl散度">2、两类KL散度</h2><p>​  考虑到需要用人工设计的近似分布 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 来拟合真实分布 <spanclass="math inline">\(P(X)\)</span> ，这里下标 <spanclass="math inline">\(\theta\)</span> 强调 <spanclass="math inline">\(Q\)</span> 是一个受到参数 <spanclass="math inline">\(\theta\)</span> 控制的分布。<br/>例如： <spanclass="math inline">\(Q\)</span> 是正态分布<spanclass="math inline">\(N(\mu,\sigma^{2})\)</span>， <spanclass="math inline">\(P\)</span> 是正态分布 <spanclass="math inline">\(N(\mu_0,\sigma_0^{2})\)</span> ，现在希望用 <spanclass="math inline">\(Q\)</span> 来拟合 <spanclass="math inline">\(P\)</span> ，其中<spanclass="math inline">\(Q\)</span> 的均值和方差 <spanclass="math inline">\(\{\mu,\sigma^{2}\}\)</span>就是拟合过程中可以调整的参数<span class="math inline">\(\theta\)</span>。于是基于前向KL和反向KL代价的分布拟合问题分别转化为以下两个优化问题：</p><ul><li>命题1. 极小化前向KL：<span class="math inline">\(\arg min_\thetaKL(P||Q_\theta)\)</span> 等价于对参数 <spanclass="math inline">\(\theta\)</span> 的极大似然估计。</li><li>命题2. 极小化反向KL： <span class="math inline">\(\argmin_{\theta}KL(Q_{\theta}||P)\)</span> 相当于在要求 <spanclass="math inline">\(Q_{\theta}\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</li></ul><p>​  首先，证明命题一，过程如下： <spanclass="math display">\[\begin{aligned}&amp;arg min_{\theta} KL (P||Q)\\&amp;=arg min_{\theta}(E_{X\sim P}[-log Q_{\theta}(X)])+H(P(X))\\&amp;=arg min_{\theta} E_{X\sim P}[-log Q_{\theta}(X)] \\&amp;=argmax_{\theta} E_{X\sim P} [log Q_{\theta} (X)] \\&amp;\approx argmax_{\theta} E_{X\sim P_{data}} [log Q_{\theta}(X)]\end{aligned}\]</span> ​  其中 <spanclass="math inline">\(H(P(X))=-\sum_{x}[P(x)logP(x)]\)</span>，代表信息熵（Entropy）。上述推导的最终结果正好就是极大似然代价的定义式。<br/>​  推导过程分析：上面的推导过程中，第2行到第3行利用了<span class="math inline">\(H(P(X))\)</span> 是与优化自变量 <spanclass="math inline">\(\theta\)</span>无关的，故删除该项不会改变最优化问题的解，因此可以直接省略。第3行到第4行则是通过来将求最小值问题转化为求最大值问题消去负号。第4行到第5行利用了机器学习训练中一般假设特征在样本集上的分布可以被近似看作真实分布，即：<span class="math inline">\(P_{data} (X)\approx P(X)\)</span> 。</p><p>　　综上命题1成立。<br/>  其次，证明命题2，推导如下： <spanclass="math display">\[arg min_{\theta} KL(Q||P)=argmin_{\theta}(E_{X\sim Q_{\theta}}[-log P(X)]+H(Q_{\theta}(X)))\]</span>​  观察上面的等式右侧 <span class="math inline">\(argmin_{\theta}\)</span> 中的两项： <span class="math display">\[E_{X\simQ_{\theta}}[-log P(X)]+H(Q_{\theta}(X))\]</span>​  要想令上面两项之和最小，就意味着要找到参数<spanclass="math inline">\(\theta\)</span>的一个合适的取值，使得上面两项中的每一项 <spanclass="math inline">\(E_{X\sim Q_{\theta}}[-log P(X)])\)</span> 和 <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>都尽可能小。根据熵的性质可知，当 <spanclass="math inline">\(Q_{\theta}(X)\)</span> 越接近于均匀分布，第二项<span class="math inline">\(H(Q_{\theta}(X)\)</span>的值越大，反之当<span class="math inline">\(Q_{\theta}(X)\)</span>越去向于单一模态分布（可以通俗理解为单峰分布） <spanclass="math inline">\(H(Q_{\theta}(X)\)</span>的值越小。因此反向KL散度相当于在要求<span class="math inline">\(Q_{\theta}(X)\)</span> 在拟合 <spanclass="math inline">\(P\)</span> 的同时尽可能保持单一模态。</p><h1 id="js散度jensen-shannon-divergence">3. JS散度(Jensen-Shannondivergence)</h1><p>​  <strong>JS 散度</strong>（Jensen-Shannon Divergence，缩写JSD）是基于 KL散度（相对熵）的一种统计学度量，能够衡量两个概率分布之间的差异程度。<br/>​  设概率空间上有两个概率分布<span class="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>，<spanclass="math inline">\(M=\frac12(P+Q)\)</span>，为<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>的平均，则，<spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span> 的JS散度定义为 <spanclass="math display">\[JSD(P||Q)=\frac12D_{KL}(P||M)+\frac12D_{KL}(Q||M),\]</span> ​  其中，<span class="math inline">\(D_{KL}\)</span>表示KL散度。</p><p>​  KL散度的缺点：不是距离、不对称。因此引进JS散度的概念，其取值是0到1之间。由定义可以看出，JS散度是对称的，可以用于衡量两种不同分布之间的差异。JS散度用于生成对抗网络的数学推导上。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</title>
      <link href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/"/>
      <url>/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/</url>
      
        <content type="html"><![CDATA[<center>Robust Camera Model Identification Over Online Social Network SharedImages via Multi-Scenario Learning <ahref="https://ieeexplore.ieee.org/abstract/document/10262083"><imgsrc="https://img.shields.io/badge/TIFS-2023-orange" alt="TIFS" /></a></center><center>Haiwei Wu , Student Member, IEEE, Jiantao Zhou , Senior Member, IEEE,Xinyu Zhang ,</center><center>Jinyu Tian , Member, IEEE, and Weiwei Sun</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/Robust_Camera_Model_Identification_Over_Online_Social_Network_Shared_Images_via_Multi-Scenario_Learning.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>​  相机模型识别（CMI，Camera modelidentification）可广泛应用于图像取证的真实性鉴定、版权保护、伪造检测等领域。同时，随着互联网的蓬勃发展，在线社交网络（OSNs，online socialnetworks）已成为图像共享和传输的主导渠道。然而，在OSNs上不可避免的损耗操作，如压缩和后处理，给现有的CMI方案带来了巨大的挑战，因为它们严重破坏了被调查图像中留下的相机痕迹。在这项工作中，我们提出了一种新的CMI方法，它对各种OSN平台的有损操作具有鲁棒性。具体来说，可以观察到，一个相机跟踪提取器可以很容易地训练在一个单一的退化场景（例如，一个特定的OSN平台）；而在混合退化场景中（例如，多个OSN平台）要困难得多。受此启发，我们设计了一种新的多场景学习（MSL，multi-scenariolearning）策略，使我们能够在不同的osn中提取鲁棒的摄像机痕迹。此外，注意到图像平滑区域由OSN引起的失真更少，而图像信号本身的干扰更小，我们提出了一种平滑感知的痕迹提取器（STATE，SmooThness-AwareTraceExtractor），它可以根据输入图像的平滑度自适应地提取相机痕迹。通过与四种先进方法的比较实验，验证了该方法的优越性，特别是在各种OSN传输场景下。特别是在开放集摄像机模型验证任务中，我们在FODB数据集上的AUC大大超过第二名15.30%；而在闭集相机模型分类任务中，我们在SIHDR数据集的F1中显著领先第二名34.51%。我们所提出的方法的代码可在https://github.com/HighwayWu/CameraTraceOSN上找到。<br/>​  稿件于2022年11月24日收到；分别于2023年8月4日和2023年9月18日修订；2023年9月18日接受。出版日期为2023年9月25日；当前版本的日期为2023年11月20日。澳门科技发展基金2021-2023、0072/2020/AMJ、0022/2022/2/A1、0014/2022/AFJ；部分由澳门大学研究委员会MYRG2020-00101-FST和MYRG2022-00152-FST；中国自然科学基金61971476；部分由阿里巴巴集团通过阿里巴巴创新研究项目。协调审查这份手稿并批准其出版的副主编是Dr.Benedetta Tondi。（通讯作者：Jiantao Zhou）<br/>​  Haiwei Wu、 JiantaoZhou、XinyuZhang就职于智慧城市物联网国家重点实验室和澳门大学科技部计算机与信息科学系，中国澳门999078（电子邮件：yc07912@umac.mo；jtzhou@umac.mo；mc14958@umac.mo）。<br/>​  JinyuTian就职于澳门科技大学创新工程学院，中国澳门999078（电子邮件：jytian@must.edu.mo）。<br/>​  WeiweiSun在阿里巴巴集团工作，位于中国，杭州311100（电子邮件：sunweiwei.sww@alibaba-inc.com）。<br/>​  数字对象标识符10.1109/TIFS.2023.3318968<br/>​  索引术语：相机模型识别，在线社交网络，深度神经网络，鲁棒性。</p><h1 id="引言">引言</h1><p>我们的主要贡献如下：</p><ul><li>据我们所知，是我们首次将MSL策略用于CMI，并证明了该策略对OSN传输具有令人满意的鲁棒性。</li><li>我们提出了STATE根据区域平滑度，灵活有效地学习相机的痕迹。</li><li>与最先进的方法[5]，[6]，[10]，[11]方法相比，我们的方法获得了更好的鲁棒性性能，特别是在OSN传输的场景中。</li><li>我们基于现有的相机数据集FODB [12]和SIHDR[23]，构建了9个流行的osn（Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat）的OSN传输数据集，不仅可以评估CMI算法的鲁棒性，而且有利于不同的取证应用。</li></ul><h1 id="照相机模型识别的基线方案">照相机模型识别的基线方案</h1><p>​  在深入研究CMI的鲁棒设计之前，我们首先介绍了基于学习的基线方案的架构，该方案包括两个网络，即提取器和分类器，如图4所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902202942944.png"alt="image-20240902202942944" /><figcaption aria-hidden="true">image-20240902202942944</figcaption></figure><p>​  与现有的[6]、[10]、[13]方案类似，提取器的目的是提取摄像机的痕迹，而分类器则监督提取器的训练。在我们的基线方案中，提取器的特定体系结构采用了EfficientNet-b0。该选择是基于一个初步的实验，比较了不同候选架构的相机痕迹提取性能，包括ResNet[40]、VGG [41]、XceptionNet[42]、EfficientNet[43]、ViT[44]和SwinTransformer[45]等。对于分类器架构，我们简单地将其设计为线性层和SoftMax变换的组合。</p><p>​  一旦确定了提取器和分类器网络的体系结构，另一个关键问题是如何在测试阶段训练和使用它们。在图4中，我们说明了基线CMI方案的训练和测试过程。在训练阶段，给定一个由Y个不同相机模型捕获的图像组成的数据集<spanclass="math inline">\(\cal{D}\)</span>，<spanclass="math inline">\((\mathbf{X},y)\)</span>表示一对训练数据，其中<spanclass="math inline">\(\mathbf{X}\in\mathbb{R}^{H\times W\timesC}\)</span>为输入图像，<spanclass="math inline">\(y\in\{1,2,\cdots,Y\}\)</span>为相机的标签。具有可训练参数<spanclass="math inline">\(\theta\)</span>的提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>旨在提取能够表征相机痕迹的高级特征<spanclass="math inline">\(\mathbf{T}\)</span>，<spanclass="math inline">\(\mathbf{T}=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X})\)</span>。为了监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>将<spanclass="math inline">\(\mathbf{T}\)</span>转换为<spanclass="math inline">\(\mathbf{\hat{y}}=\mathcal{C}_{\phi}(\mathbf{T})\)</span>来计算损失<spanclass="math inline">\(\ell(\mathbf{\hat{y}},y)\)</span>，其中<spanclass="math inline">\(\ell\)</span>是广泛使用的交叉熵损失，即： <spanclass="math display">\[\ell(\hat{\mathbf{y}},y)=-\sum_{i=1}^Y\mathcal{I}[y=i]\cdot\log(\hat{\mathbf{y}}^{&lt;i&gt;}).\]</span>​  这里<spanclass="math inline">\(\mathbf{\hat{y}}^{&lt;i&gt;}\)</span>表示<spanclass="math inline">\(\mathbf{\hat{y}}\)</span>的第<spanclass="math inline">\(i\)</span>个条目，<spanclass="math inline">\(\mathcal{I}[y=i]\)</span>是一个二进制指标函数，如果是y= i，则取1，否则取0。<br/>​  经过训练，训练良好的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>可以根据[6]用于两个测试用例：1)开集验证，目的是推断两个测试图像是否被同一相机模型捕获；2)闭集分类，目的是在有限的相机模型池中识别测试图像的源相机模型。<br/>​  请注意，在这两种情况下，分类器<spanclass="math inline">\(\mathcal{C}_{\phi}\)</span>都被丢弃了。具体来说，在前一种情况下，给定两个图像<spanclass="math inline">\(\mathbf{X_1}\)</span>和<spanclass="math inline">\(\mathbf{X_2}\)</span>，它们的痕迹<spanclass="math inline">\(\mathbf{T_1}\)</span>和<spanclass="math inline">\(\mathbf{T_2}\)</span>首先由训练过的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取。然后，通过余弦相似度计算这些痕迹被同一相机拍摄的概率：<spanclass="math display">\[S(\mathbf{T}_1,\mathbf{T}_2)=\cos\Big(\frac{\mathbf{T}_1}{||\mathbf{T}_1||},\frac{\mathbf{T}_2}{||\mathbf{T}_2||}\Big).\]</span>​  对于闭集分类案例的测试过程，给出一组已知标签的<spanclass="math inline">\(y_i\in\{1,2,\cdots,Y\}\)</span>的图像，<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>首先提取它们的痕迹<spanclass="math inline">\(\mathbf{T_i}\)</span>。然后，平均相同相机模型的痕迹，可以形成一个痕迹池<spanclass="math inline">\(\{\mathbf{\bar{T}}_{i}\}_{i=1}^{Y}\)</span>。换句话说，池中的每个元素代表一个特定相机模型的平均痕迹。当一个测试图像<spanclass="math inline">\(\mathbf{X_t}\)</span>出现时，其预测的相机类型<spanclass="math inline">\(y_t\)</span>可以从痕迹池中搜索最大的相似性，即：<spanclass="math display">\[y_t=\underset{i}{\mathrm{argmax}}S(\mathbf{T}_t,\bar{\mathbf{T}}_i)\]</span>​  其中，<span class="math inline">\(\mathbf{T_t}\)</span>为通过<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的测试图像的痕迹。<br/>​  虽然我们的基线CMI可以用于提取摄像机痕迹，并最终在上述两个测试用例中被采用，但在有损传输上的性能，如各种OSN传输场景，可能会严重降低。这些场景所引入的扭曲很可能会破坏相机的痕迹，这在本质上是脆弱的。在表I中，我们简要展示了FODB[12]数据集上不同的osn造成的畸变，包括平均分辨率和文件大小的减少，以及平均采用的JPEG质量因子（QFs）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902205601762.png"alt="image-20240902205601762" /><figcaption aria-hidden="true">image-20240902205601762</figcaption></figure><p>由FODB[12]数据集上不同的OSNS造成的失真。在这里，“SCALE”和“SIZE”分别表示分辨率减少和文件大小减少的百分比。另外，“JPEGQF”表示qf值的平均值。</p><p>​  可以看出，最严重的畸变是由Dingding造成的，导致分辨率降采样84.60%，文件大小减少93.93%，从最低的平均QF值69.3也可以观察到。其中考虑的最友好的OSN平台是微博，导致分辨率降采样57.77%，文件大小减少60.19%。可以清晰地得到如下结论，这些OSN失真将严重影响CMI算法，因此设计一个鲁棒的CMI方案至关重要，能够可靠地提取OSN传输中摄像机的痕迹。</p><h1 id="鲁棒相机模型识别">鲁棒相机模型识别</h1><p>​  在有了基线之后，我们提出了一种新的方法来设计一个强大的CMI来对抗各种osn上的传输，其中关键的创新是双重的：MSL和STATE。正如预期的那样，并将通过实验验证，STATE为不同的输入提取了更多的自适应痕迹，而MSL策略更好地监督了状态的训练，共同有助于鲁棒提取摄像机痕迹的目标。<br/>​  所提出的鲁棒CMI的训练过程如图5所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png"alt="image-20240902210022345" /><figcaption aria-hidden="true">image-20240902210022345</figcaption></figure><p>图5。我们提出的鲁棒CMI的训练过程。STATE从给定的图像中提取摄像机的痕迹，而MSL策略利用一组分类器，在逐个场景的基础上监督STATE的训练。</p><p>​  具体来说，给定一个训练图像<spanclass="math inline">\(\mathbf{X}\)</span>，我们首先收集其在N个场景下的传输变体。为了达到令人满意的鲁棒性，在本工作中，我们定义了由NoTrans组成的场景。（原始），两种类型的OSN传输：Facebook和Whatsapp，这也被考虑在VISION数据集[17]。此外，考虑到训练和测试场景之间的差异，我们手工制作了一个增强场景来改进对未知（新的）osn的泛化，其中增强包括常用的后处理操作，如缩放、压缩、模糊和噪声添加。更多关于场景影响的分析，例如，不同数量的场景及其组合被推迟到第五章中的消融研究G.4。对于每个变体，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取相应的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>，其中嵌入的平滑注意模块引导<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更多地关注<spanclass="math inline">\(\mathbf{X}\)</span>中的平滑区域。根据观察I，使用N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>来监督<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的训练，其中每个分类器处理每个单独场景的训练。例如，<spanclass="math inline">\(\mathcal{C}_{\phi_1}\)</span>处理NoTrans场景，<spanclass="math inline">\(\mathcal{C}_{\phi_2}\)</span>处理Facebook场景等。接下来，将在<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>上生成的总损失<spanclass="math inline">\(\sum_{n=1}^{N}\mathcal{L}_{n}\)</span>进行反向传播，以更新与STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>相关的可学习参数<spanclass="math inline">\(\theta\)</span>和与N个分类器相关的<spanclass="math inline">\(\{\boldsymbol{\phi}_n\}_{n=1}^N\)</span>。</p><p>​  在测试阶段，该过程类似于基线CMI，其中只需要训练好的状态<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>，而N个分类器<spanclass="math inline">\(\{\mathcal{C}_{\phi_{n}}\}_{n=1}^{N}\)</span>被丢弃。在下面，我们将提供更多关于我们提出的MSL策略和STATE的更多细节。</p><h2 id="a.-多场景学习mslmulti-scenario-learning">A.多场景学习（MSL，Multi-Scenario Learning）</h2><p>​  如前所述，受观察I启发，我们的MSL策略使用了多个具有非共享权重的分类器来监督多个场景的训练。多分类器产生的一个隐式问题是训练过程可能不稳定。这是因为训练的复杂性因不同的场景不同，导致梯度方向上的冲突。为了弥补这一缺陷，我们建议在MSL的逆向过程中集成一个动量掩膜操作来减轻梯度冲突。我们现在详细介绍我们提出的MSL的前向和反向过程的细节。</p><h3 id="前向过程">1)前向过程</h3><p>​  设<span class="math inline">\(\mathbf{X}\)</span>为输入图像，<spanclass="math inline">\(\{\mathbf{X}_{n}\}_{n=1}^{N}\)</span>是其在N个场景下的N个变体，<spanclass="math inline">\(\{\mathbf{T}_{n}\}_{n=1}^{N}\)</span>是它们由STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取的痕迹。有关<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的细节将推迟到下一小节。同时，利用每个分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>，根据第n个场景中的训练数据来监督训练过程，损失如下：<spanclass="math display">\[\mathcal{L}_n=\ell(\mathcal{C}_{\boldsymbol{\phi}_n}(\mathbf{T}_n),y),\]</span>​  其中，<spanclass="math inline">\(\ell\)</span>为(1)中给出的交叉熵损失。<br/>​  值得注意的是，损失函数(4)和损失函数(1)之间的关键区别在于前者涉及多个非共享分类器，而后者只使用一个。另外，请注意，<spanclass="math inline">\(\mathbf{T}_{n}\)</span>和<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>应该有相同的下标n，以便逐场景实现MSL训练场景。</p><h3 id="反向过程">2)反向过程</h3><p>​  在反向过程中，分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>的参数更新为： <spanclass="math display">\[\phi_n=\phi_n-r\nabla_{\phi_n}\mathcal{L}_n,\]</span>​  其中，r是学习率。同样，提取器<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的反向更新可以表示为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\nabla_{\theta}\mathcal{L}_n.\]</span>​  然而，这里的<spanclass="math inline">\(\sum_{n=1}^N\nabla_{\boldsymbol{\theta}}\mathcal{L}_n\)</span>由N项组成，对应于N个场景，其中不一致的梯度方向可能导致冲突，导致<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>[18]的次优训练。此外，通过<spanclass="math inline">\(\mathbf{T}_n=\mathcal{N}_{\boldsymbol{\theta}}(\mathbf{X}_n)\)</span>和使用链规则，(6)可以重写为：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\partial\mathbf{T}_n/\partial\boldsymbol{\theta}\)</span>为<spanclass="math inline">\(\mathbf{T}_n\)</span>的雅可比矩阵。为了减轻梯度冲突对(6)或(7)中<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>更新的影响，一种解决方案是设计非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>作为<spanclass="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>的替代品。<br/>​  根据[20]，可以通过基于一致性水平逐元素掩膜<spanclass="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>来形成非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。具体来说， <spanclass="math display">\[\mathbf{L}_n=\mathbf{M}_n\odot\nabla_{\mathbf{T}_n}\mathcal{L}_n,\]</span>​  其中，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。这里的<spanclass="math inline">\(\mathbf{M}_n\)</span>是一个与<spanclass="math inline">\(\nabla_{\phi_n}\mathcal{L}_n\)</span>具有相同维数的二进制矩阵，定义为：<spanclass="math display">\[\begin{aligned}\mathbf{M}_{n}&amp;=\mathcal{I}[\mathbf{P}\succcurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\succcurlyeq\mathbf{0}]\\&amp;+\mathcal{I}[\mathbf{P}\preccurlyeq\mathbf{U}]\odot\mathcal{I}[\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\preccurlyeq\mathbf{0}],\end{aligned}\]</span>​  其中<spanclass="math inline">\(\mathcal{I}\)</span>为标准指标函数，适当尺寸的<spanclass="math inline">\(\mathbf{U}\)</span>表示一个从均匀分布<spanclass="math inline">\(U(0,1)\)</span>中抽样的随机矩阵，<spanclass="math inline">\(\geq(\preccurlyeq)\)</span>为元素不等式。此外，<spanclass="math inline">\(\mathbf{P}\)</span>测量了给定梯度中包含的正符号的纯度（一致性），其表述为：<spanclass="math display">\[\mathbf{P}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{G}_n}{\sum_n|\mathbf{G}_n|}\big),\]</span>​  其中<spanclass="math inline">\(\mathbf{G}_{n}=\mathrm{sign}(\mathbf{T}_{n})\odot\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>在批处理维度上合并了梯度贡献，所有的计算包括除值和绝对值操作都是逐元素进行。<br/>​  然而，由（10）计算的<spanclass="math inline">\(\mathbf{P}\)</span>依赖于单个反向的特定梯度<spanclass="math inline">\(\nabla_{\mathbf{T}_{n}}\mathcal{L}_{n}\)</span>（或输入<spanclass="math inline">\(\mathbf{X}\)</span>），限制了其在局部描述梯度一致性的能力。这可能导致不稳定的训练或在某些情况下较差的局部最小值，例如当批中的冲突相互抵消时。因此，我们建议通过动量平均[46]来考虑历史梯度，而不是只涉及当前反向的梯度。这样，<spanclass="math inline">\(\mathbf{P}\)</span>就可以全局计算不同场景的一致性，稳定了随机梯度下降（SGD，stochasticgradient descent）中的训练更新。<br/>​  为了将动量的概念应用于<spanclass="math inline">\(\mathbf{P}\)</span>的生成，我们重新定义了第t个反向过程中的纯度为：<spanclass="math display">\[\mathbf{P}^{(t)}=\frac{1}{2}\big(1+\frac{\sum_n\mathbf{g}_n^{(t)}}{\sum_n|\mathbf{g}_n^{(t)}|}\big),\]</span>​  其中 <spanclass="math display">\[\begin{aligned}\mathbf{g}_n^{(t)}&amp;=\mu\cdot\mathbf{g}_{n}^{(t-1)}+(1-\mu)\mathbf{G}_{n}^{(t)}\\&amp;=\mu^{t-1}\mathbf{g}_{n}^{(1)}+\sum_{i=1}^{t-2}\mu^{i}(1-\mu)\mathbf{G}^{(t-i)}+(1-\mu)\mathbf{G}_{n}^{(t)}\end{aligned}\]</span>​  在之前的t−1个历史反向过程上累积梯度，<spanclass="math inline">\(\mathbf{g}_n^{(1)}\)</span>被初始化为<spanclass="math inline">\(\mathbf{G}_{n}^{(1)}\)</span>。这里的<spanclass="math inline">\(\mu\)</span>是控制最近梯度的权重的衰减因子。在实践中，我们根据经验设置<spanclass="math inline">\(\mu=0.95\)</span>。显然，当<spanclass="math inline">\(\mu=0\)</span>时，动量<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>退化到原来的<spanclass="math inline">\(\mathbf{P}\)</span>。<br/>​  在得到动量纯度<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>后，可以通过用<spanclass="math inline">\(\mathbf{P^{(t)}}\)</span>代替<spanclass="math inline">\(\mathbf{P}\)</span>来相应地计算出(9)中的掩模<spanclass="math inline">\(\mathbf{M}_{n}\)</span>和(8)中的非冲突梯度<spanclass="math inline">\(\mathbf{L}_{n}\)</span>。最后，使用<spanclass="math inline">\(\mathbf{L}_{n}\)</span>通过以下方式更新<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>的参数：<spanclass="math display">\[\theta=\theta-r\sum_{n=1}^N\left(\frac{\partial\mathbf{T}_n}{\partial\boldsymbol{\theta}}\right)^T\mathbf{L}_n.\]</span></p><h3 id="msl训练算法">3) MSL训练算法</h3><p>​  我们总结了算法1中的整个MSL训练过程。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902220503880.png"alt="image-20240902220503880" /><figcaption aria-hidden="true">image-20240902220503880</figcaption></figure><p>​  更具体地说，前向过程在行5∼7中描述，而其余行专门用于反向过程。在第5行中，我们收集了N个场景中的输入变体，这也可以提前离线进行。然后，STATE<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>提取第6行中的摄像机痕迹，并利用分类器<spanclass="math inline">\(\mathcal{C}_{\phi_n}\)</span>来计算第7行中的损失。为了减轻损失中的梯度冲突，第8行∼20主要用于产生动量掩模<spanclass="math inline">\(\mathbf{M}_n\)</span>，然后在第21行中用于更新<spanclass="math inline">\(\theta\)</span>。最终，经过训练的<spanclass="math inline">\(\mathcal{N}_{\boldsymbol{\theta}}\)</span>在第24行产生。<br/>​  备注：一种简单的替代训练策略是在不同的场景下重新标记数据，并通过一个单一的分类器来计算预测。例如，一个包含29个摄像机和4个场景的数据集可以被表示为一个包含29个×4=116个类别的单一分类任务。一个潜在的关键问题是，这种替代方案假定不同场景之间有足够的可变性；否则，某些类别在某种程度上是无法区分的。然而，这个假设并不总是正确的，例如，对于NoTrans，在场景和裁剪场景中，摄像机的痕迹或多或少是相同的。换句话说，一个痕迹实际上可能对应于多个标签，这可能会导致训练过程中的不稳定。在我们提出的MSL策略中，这种困境可以通过N个分类器很自然地避免。<br/>​  我们现在将介绍STATE提取器的架构的细节。</p><h2 id="b.-清晰识别痕迹提取器statesmooth-aware-trace-extractor">B.清晰识别痕迹提取器（STATE，SmooTh-Aware Trace Extractor）</h2><p>​  STATE的目的是根据给定图像的局部平滑度进行专注的相机跟踪提取。在这项工作中，我们使用著名的香农熵[47]来表示一个图像块的平滑性。显然，越小的熵值表示越光滑的区域，活动越少，反之亦然。在我们提出的STATE下，我们利用交叉注意[48]层来实现注意提取，其中平滑矩阵被转换为一个注意映射并作为指导。我们想强调的是，我们提出的STATE明确地应用了根据观察II之前的平滑性，因此和交叉注意层的简单采用有显著的不同。<br/>​  STATE的过程如图5所示。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902221142472.png"alt="image-20240902221142472" /></p><p>​  具体地说，我们首先将大小为<span class="math inline">\(H\timesW\)</span>的输入图像分割为不重叠的块<spanclass="math inline">\(\{\mathbf{R}_{i}\}\)</span>，每一个块行长<spanclass="math inline">\(\frac{H}{\hat{H}}\)</span>、列长<spanclass="math inline">\(\frac{W}{\hat{W}}\)</span>，总共得到<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>个块。对于第<spanclass="math inline">\(i\)</span>个块<spanclass="math inline">\(\mathbf{R}_{i}\)</span>，我们计算相关的平滑度指标，即香农熵<spanclass="math inline">\(E_i\)</span>： <spanclass="math display">\[E_i=-\sum_{v=0}^{255}p_v\mathrm{log}(p_v),\]</span>​  其中 <spanclass="math display">\[p_{v}=\frac{\hat{H}\hat{W}}{HW}\sum_{h=0}^{H/\hat{H}}\sum_{w=0}^{W/\hat{W}}\mathcal{I}[\mathbf{R}_{i}^{&lt;h,w&gt;}=v]\]</span>​  为<spanclass="math inline">\(\mathbf{R}_{i}\)</span>内的像素值为v的概率。然后，通过将<spanclass="math inline">\(\{E_i\}\)</span>分组和重塑为维数<spanclass="math inline">\(\hat{H}\times\hat{W}\)</span>，可以得到平滑矩阵<spanclass="math inline">\(\textbf{S}\in\mathbb{R}^{\hat{H}\times\hat{W}}\)</span>。在实际实现中，我们对输入图像进行灰度处理以降低复杂性，并使用单热编码来进行高效的批处理。<br/>​  得到平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>后，我们首先通过基线提取器提取内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>，然后根据<spanclass="math inline">\(\textbf{S}\)</span>在<spanclass="math inline">\(\textbf{X}^f\)</span>上执行交叉注意力，特别注意的是，<spanclass="math inline">\(\textbf{X}^f\)</span>和<spanclass="math inline">\(\textbf{S}\)</span>分别通过压扁化得到<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>，其大小为<spanclass="math inline">\(\hat{H}\hat{W}\times\hat{C}\)</span>，其中<spanclass="math inline">\(\hat{C}\)</span>是标记化后的通道数。然后通过计算每个<spanclass="math inline">\(\hat{C}/K\)</span>通道的注意力，对展平的<spanclass="math inline">\(\mathbf{\hat{x}}^{f}\)</span>和<spanclass="math inline">\(\hat{\mathbf{s}}\)</span>计算K头注意力，结果是k头特征<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>，其中：<spanclass="math display">\[\hat{\mathbf{X}}_k^a=\text{Attention}(\hat{\mathbf{S}}\mathbf{Q}_k,\hat{\mathbf{X}}^f\mathbf{K}_k,\hat{\mathbf{X}}^f\mathbf{V}_k),k=1,\ldots,K,\]</span>​  <span class="math inline">\(\mathbf{Q}_k\)</span>、<spanclass="math inline">\(\mathbf{K}_k\)</span>、<spanclass="math inline">\(\mathbf{V}_k\in\mathbb{R}^{\hat{C}^{2}/K}\)</span>是交叉注意函数[48]的第k个投影的查询、键和值矩阵。这里，交叉注意力的执行是：<spanclass="math display">\[\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{SoftMax}(\frac{\mathbf{QK}^{T}}{\sqrt{\hat{C}/K}})\mathbf{V}.\]</span>​  接下来，所有头<spanclass="math inline">\(\{\hat{\mathbf{X}}_k^a\}_{k=1}^K\)</span>的拼接输出用来线性预测，得到展平的注意力<spanclass="math inline">\(\mathbf{\hat{X}}^{a}\)</span>： <spanclass="math display">\[\mathbf{X}^a=\mathrm{MLP}(\mathrm{Concat}(\hat{\mathbf{X}}_1^a,\hat{\mathbf{X}}_2^a,\ldots,\hat{\mathbf{X}}_K^a)),\]</span>​  其中<spanclass="math inline">\(\mathrm{MLP}(\cdot)\)</span>代表一个具有GELU激活[49]的MLP。最后，通过将其重塑为<spanclass="math inline">\(\hat{H}\times\hat{W}\times\hatC\)</span>分辨率，获得细化的注意力特征<spanclass="math inline">\(\mathbf{X}^{a}\)</span>。<br/>​  考虑到<spanclass="math inline">\(\mathbf{X}^{a}\)</span>中的维数冗余性，我们遵循传统的[6]，[10]，采用全局平均池化和线性映射将<spanclass="math inline">\(\mathbf{X}^{a}\)</span>编码为低维空间<spanclass="math inline">\(\mathbb{R}^d\)</span>，从而得到最终的相机痕迹<spanclass="math inline">\(\mathbf{T}\)</span>。</p><h1 id="实验结果">实验结果</h1><p>​  在本节中，我们将从开放集验证、闭集分类、对OSN传输的鲁棒性、后处理操作和重传输等方面全面评估所提出的CMI方法的性能。进一步，给出了系统的消融研究和分析。在介绍详细的结果之前，让我们先介绍一下实验设置。</p><h2 id="a.实验设置">A.实验设置</h2><h3 id="训练数据集">1)训练数据集：</h3><p>​  为了训练所提出的方法，类似于[5]，[6]，我们使用了包括29个不同的相机模型的VISION[17]数据集。在[5]、[6]、[11]和[10]之后，我们合并了来自不同设备但具有相同模型的图像，以避免歧义。需要注意的是，该数据集包含Facebook和Whatsapp传输的变体，可以方便地用作N个场景的训练数据（见图5）。</p><h3 id="测试数据集">2)测试数据集</h3><p>​  为了更好地模拟实际情况并评估泛化，我们采用FODB [12]和SIHDR[23]作为交叉测试数据集，与训练数据没有重叠。FODB数据集由25个模型和27个设备组成，而SIHDR数据集分别由21个模型和23个设备组成。在这两个数据集中，每个模型都有一个额外的设备。除非另有说明，我们将根据FODB（和SIHDR）数据集中对具有相同模型的不同设备的图像进行标记，类似于在VISION中进行的过程。</p><h3 id="在线社交网络">3)在线社交网络</h3><p>​  虽然FODB数据集本身包含5个OSN传输版本（Twitter、Telegram、Whatsapp、Instagram和Facebook），但我们进一步将FODB和SIHDR数据集扩展到9个流行的OSN传输场景，包括Twitter、Telegram、Whatsapp、Instagram、Facebook、Weibo、QQ、Dingding和WeChat。这使我们能够更广泛地评估CMI算法对当今实际OSN传输的鲁棒性。扩展的数据集和有关操作系统和OSN平台版本的详细信息可在https://github.com/HighwayWu/CameraTraceOSN上获得。</p><h3 id="比较方法">4)比较方法</h3><p>​  为了展示我们提出的CMI方法的优越性能，我们采用了四种最先进的算法作为竞争对手，即Kuzin[10]，ForSim [11]，NoiPri [5]和PCN [6]。</p><h3 id="实现细节">5)实现细节</h3><p>​  在培训期间，MSL策略中采用的场景数量设置为4，包括NoTrans、Facebook和Whatsapp（由VISION数据集本身提供），以及一个额外的手工增强场景。引入增强场景的原因是为了进一步提高网络对看不见的场景的泛化能力。更具体地说，增强是通过随机混合0∼50%的降采样，与QFs70∼100的JPEG压缩，与核3∼5的高斯模糊，与方差3∼10的高斯噪声相加形成的。<br/>​  我们使用PyTorch深度学习框架实现我们的方法，其中采用默认参数的Adam[50]作为优化器。学习速率初始化为1e-4，如果验证损失在5个时期内没有减少，则学习速率减半。在训练过程中，所有输入的图像被随机裁剪成512个×512补丁。内部特征<spanclass="math inline">\(\textbf{X}^f\)</span>、平滑矩阵<spanclass="math inline">\(\textbf{S}\)</span>和交叉注意特征<spanclass="math inline">\(\textbf{X}^a\)</span>具有32×32×320相同的特征维度。提取的痕迹<spanclass="math inline">\(\mathbf{T}\)</span>的尺寸d（参见第IV-B节）设置为256。基于观察到提高测试图像的分辨率将提高性能[6]，[12]，我们将测试大小设置为1536×1536。为了便于我们的结果，我们的代码可以在https://github.com/HighwayWu/CameraTraceOSN上找到。</p><h2 id="b.开集验证任务的评估">B.开集验证任务的评估</h2><p>​  开放集验证任务的目的是推断两个给定的图像是否被同一相机模型捕获。为此，我们从每个相机模型中随机选择25张图像，然后对每个测试数据集形成5000对正对和5000对负对。由于验证任务本质上是一个二元分类问题，我们采用广泛使用的接收机工作特征曲线下面积（AUC）作为评价性能的标准（越高越好）。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903093240759.png"alt="image-20240903093240759" /><figcaption aria-hidden="true">image-20240903093240759</figcaption></figure><p>​  从表二可以看出，当图像不通过OSN传输时，所有现有方法都表现良好，AUC从87.32%到89.38%，而我们的方法略好，AUC提高了1.50%。然而，当图像通过osn传输时，所有现有方法的验证性能都会显著下降。以Twitter为例，与NoTrans场景相比，Kuzin[10]、ForSim [11]、NoiPri [5]和PCN[6]的AUC值分别下降了10.83%、26.97%、18.24%和12.23%。这种严重的性能下降可能是因为osn执行的有损操作极大地消除了原有的相机痕迹，特别是那些不鲁棒的痕迹。相反，通过利用我们提出的MSL策略和平滑注意，我们提出的方法可以探索高度鲁棒的摄像机痕迹，在Twitter场景中仅减少1.89%的AUC。对于表二中的其他OSN传输场景，我们的方法仍然表现出令人满意的鲁棒性，在AUC上平均优于第二优的方法15.30%。<br/>​  对于表二下半部分的SIHDR数据集上的结果，我们可以观察到与FODB中的情况类似的现象。例如，对于ForSim，QQ平台的性能下降最为严重，AUC下降了33.62%，最友好的平台是Weibo，导致AUC下降了12.18%。相比之下，我们的方法具有非常理想的鲁棒性，平均超过第二名的AUC11.48%。还需要注意的是，这里考虑的所有方法在SIHDR上的表现都比FODB更好，这主要是因为SIHDR有更少的相机类别，而且类别之间的可变性更大。需要注意的是，在表二的每一列中，所有被选择的对都有两个图像通过相同的OSN。一个更具挑战性的实验，一对内的图像通过不同的osn传输，推迟到V-H节进一步阐述。</p><h2 id="c.-闭集分类任务的评价">C. 闭集分类任务的评价</h2><p>​  闭集分类任务的目标是预测有限的相机模型池中给定查询图像的来源，其中代表每个相机模型的“ground-truth”痕迹是从一组预定义的图像（即锚定集）[6]中提取的。通常，“ground-truth”的痕迹可以通过利用PRNU[1]算法，或者简单地通过平均深层特征来获得。与开放集评估类似，我们从每个相机类别中随机抽取45张图像，其中25张图像作为锚定集，其余的图像作为查询图像。对于每个查询图像，预测的相机标签将被授予池中与提取的痕迹相似性最高的一个。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094227527.png"alt="image-20240903094227527" /><figcaption aria-hidden="true">image-20240903094227527</figcaption></figure><p>图6。在SIHDR数据集上的闭集分类任务的混淆矩阵。对于每个矩阵，纵轴和横轴分别代表实际的标签和预测的标签。在这里，每一行（列）代表一个独特的相机模型，例如，第一行、第二行和最后一行分别对应于GioneeS55、Huaiwei-P8和iPhone5S。有关相机型号的具体信息，请参考我们的代码网站。</p><p>​  为了评估分类任务，我们在图6中显示了混淆矩阵，并在表3中显示了相应的精度（PRC）、召回率（RCL）和F1分数。在形式上，中国和RCL的定义为：<spanclass="math display">\[\mathrm{PRC}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FP}_{y}},\]</span></p><p><spanclass="math display">\[\mathrm{RCL}=\frac{1}{Y}\sum_{y=1}^{Y}\frac{\mathrm{TP}_{y}}{\mathrm{TP}_{y}+\mathrm{FN}_{y}},\]</span></p><p>​  其中，<span class="math inline">\(\mathrm{TP}_{y}\)</span>、<spanclass="math inline">\(\mathrm{FP}_{y}\)</span>和<spanclass="math inline">\(\mathrm{FN}_{y}\)</span>分别代表给定类y的真阳性、假阳性和假阴性。那么宏观平均的F1得分可以计算如下：</p><p><spanclass="math display">\[\mathrm{F1}=\frac{1}{y}\sum_{y=1}^{Y}\frac{2\times\mathrm{TP}_{y}}{2\times\mathrm{TP}_{y}+\mathrm{FP}_{y}+\mathrm{FN}_{y}}.\]</span><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903094839265.png"alt="image-20240903094839265" /></p><p>​  从表三可以看出，在NoTrans的情况下，我们的方法取得了相当令人满意的结果（96.38%F1），明显优于两个强竞争对手PCN [6]和NoiPri[5]算法（F1的差距分别为14.22%和20.51%）。当图像通过osn传输时，所有现有的方法都会受到严重的影响。例如，在QQ和WeChat上，PCN[6]的F1的降幅分别为32.83%和62.30%，这被认为是巨大的。这意味着在OSN干扰下，PCN很难提取有区别的摄像机痕迹。从图6中给出的混淆矩阵也很容易观察到这种现象（见第三行最后一列）。相比之下，我们的方法通常对所有的OSN传输都具有鲁棒性，导致平均F1得分为87.15%。与竞争算法Kuzin[10]、NoiPri [5]和PCN[6]相比，我们在F1方面分别获得了46.68%、37.18%和34.51%的压倒性优势。在这里，表三每列中的查询和锚图像都通过了相同的OSN。<br/>​  需要注意的是，我们在这里省略了ForSim[11]的结果。这是因为在ForSim中使用的相似性网络只能提取测量给定输入对的相似性的特征，而不能捕获特定于一种相机跟踪类型的特征。因此，ForSim无法为锚点集提取相应的特征。</p><h2 id="d.-开放式分类任务的评价">D. 开放式分类任务的评价</h2><p>​  除了之前的开集验证和闭集分类任务外，我们现在还考虑开集分类任务，它不仅涉及检测给定图像是否已知，而且还包括对其特定标签的进一步分类。具体来说，假设SIHDR数据集中的摄像机已知，每个摄像机使用25张图像作为锚定集。我们在SIHDR中为每个相机模型选择20张新图像，并确定它们是否可以被正确地分类为已知的和正确的相机模型。此外，我们在FODB中的每个相机模型中选择了20张图像，并评估它们是否可以被归类为未知的，即在SIHDR中没有一个已知的（可疑的）模型。为了避免歧义，我们排除了SIHDR和FODB数据集所共有的相机模型。显然，确定过程需要建立一个接受或拒绝一个测试示例的阈值。特别是，我们直接拒绝一个与已知相机模型的最大相似度Smax低于给定阈值δ，只有当Smax&gt;δ和正确预测时，才会接受。<br/>​  以精度和f1为标准，图7所示了阈值范围从0到1的实验结果。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903095334042.png"alt="image-20240903095334042" /></p><p>​  由于提取摄像机痕迹的方法不同，竞争的方法在不同的阈值下达到了各自的最佳性能。其中，Kuzin[10]的最佳准确率为79.26%，F1评分为68.87%，而NoiPri[5]的最佳准确率为72.30%，F1评分为65.58%。由于PCN倾向于对正对和负对产生相对较高的相似性得分，因此它对阈值变化变得不那么敏感，导致图7中没有峰值。其中，PCN获得的最高准确率和F1得分分别仅为50.07%和64.15%。这表明PCN并不很适合用于开放集分类任务。相比之下，我们提出的算法的准确率为91.63%，F1为91.40%，以准确率+12.37%、F1+22.53%的表现远远超过了第二好的算法。</p><h2 id="e.-后处理评价">E. 后处理评价</h2><p>​  虽然本文的重点是针对各种CMI方案的OSN传输场景的鲁棒设计，但我们的设计也自然地为常用的后处理带来了令人满意的鲁棒性。具体来说，考虑的后处理操作包括QFs范围为70到95的JPEG压缩，因子从10%到50%的线性调整，核大小为[3,5,7]的高斯模糊，以及方差为[3,5,7,9]的高斯噪声相加。总共有18种不同的后处理操作，JPEG压缩、调整大小、模糊和噪声添加分别有6、5、3和4个变体。这些操作被应用于SIHDR数据集，其中包含929张图像。因此，总共生成了18×929=16,722张图像。对于每个图像，只应用了一种后处理攻击类型。鉴于OSN已经包含了复合攻击场景，我们这里的重点主要是对抗单一的后处理攻击。比较结果如图8所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100039566.png"alt="image-20240903100039566" /><figcaption aria-hidden="true">image-20240903100039566</figcaption></figure><p>​  可以看出，尽管ForSim [11]和NoiPri[5]在NoTrans场景中表现良好，但当遇到高斯模糊和高斯噪声添加等强后处理时，他们正在努力保持稳定的性能。Kuzin[10]在JPEG压缩和调整大小下相对稳定；但在高斯模糊和噪声增加的情况下，性能迅速下降。相比之下，PCN[6]和我们的方法对这些后处理操作具有更好的鲁棒性；对于所有考虑的情况，我们的方法仍然优于PCN，特别是在大核大小的高斯模糊中。</p><h2 id="f.-再传输和交叉传输的评估">F. 再传输和交叉传输的评估</h2><p>​  在实践中，通过多个OSN平台进行再传输和/或交叉传输（即，图像被下载并重新上传到相同或不同的osn上）是非常常见的。现在我们简要地讨论了不同CMI算法在再传输和交叉传输情况下的鲁棒性评估问题。具体来说，我们考虑的是通过Facebook的传输，其次是Facebook/QQ，其次是Whatsapp/QQ。结果列于表四之中。<br/><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903100523776.png"alt="image-20240903100523776" /></p><p>​  可以观察到，第二轮传输，无论是再传播还是交叉传输，其影响远小于第一轮传播。我们的方法对两轮OSN传输都表现出相当强的鲁棒性，而竞争算法的性能要差得多。例如，例如，在通过Whatsapp场景之后再通过Whatsapp/QQ场景进行传输时，我们的方法只有2.21%/1.99%AUC下降，而PCN的性能下降更严重，达到10.26%/9.36% AUC损失。</p><h2 id="g.-消融研究">G. 消融研究</h2><p>​  在本小节中，我们通过分析每个组件如何有助于提取鲁棒的相机痕迹，对我们提出的方法进行消融研究。具体来说，我们首先禁止使用平滑注意和MSL策略，从而产生基线性能。然后，我们将注意力模块和MSL策略的不同变体纳入基线中，评估它们带来的额外性能收益。比较结果如表V所示，由于页面限制，它只包括在SIHDR[23]数据集下关于NoTrans、Facebook和Whatsapp场景的开放集验证结果。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101054119.png"alt="image-20240903101054119" /><figcaption aria-hidden="true">image-20240903101054119</figcaption></figure><h3 id="平滑注意力的采用">1)平滑注意力的采用：</h3><p>​  在表V的第二行中，我们给出了具有注意机制的结果。在这里，除了我们对平滑注意力的关注，我们还评估了变量注意力的性能，包括熵滤波器[11]或可靠性映射[22]。可以看出，通过引导基线提取器更加注意图像中的平滑区域，所有这三个注意模块都确实提高了性能。具体来说，可靠性映射[22]所带来的改进是有限的（仅有0.14%的收益），主要是因为两个因素：1)它估计给定的图像（局部）的可靠性，从全局角度缺乏交互性；2)它需要预先训练，因此很难端到端训练，这限制了提取器的优化。对于熵滤波器[11]方法，需要手动调整阈值来过滤高熵区域，这不可避免地丢弃了一些有价值的信息，在实践中很麻烦。因此，熵滤波器所带来的增量很小，只有0.82%。相比之下，我们的基于交叉注意的平滑注意模块不仅能够为不同的输入自动定制其平滑度，而且还可以很容易地纳入端到端训练中，从而获得2.79%的性能提高。在图9中，为了更直观的理解，我们还更直观地可视化了提取器注意。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903101452445.png"alt="image-20240903101452445" /><figcaption aria-hidden="true">image-20240903101452445</figcaption></figure><p>​  显然，我们的平滑注意模块可以有效地引导特征提取更加注意平滑区域。</p><h3 id="msl模块的采用">2)MSL模块的采用：</h3><p>​  为了分析不同的学习策略的贡献，我们在表V的第三行中给出了相应的消融结果。注意到，第一行的基线提取器使用单个分类器进行优化，而第三行的所有结果都采用多个分类器，属于MSL的类别。除了我们的MSL策略外，我们还包括了通过使用GradNorm[19]或GradDrop[20]来获得非冲突梯度的变体。由此可见，采用多分类器的MSL确实大大提高了整体性能；即使是一个简单的非加权策略也能带来7.63%的收益。GradNorm[19]或GradDrop[20]的加入可以实现更大的改进（分别为10.04%和10.77%）。由于动量掩蔽操作明确地利用了历史逆向过程产生的梯度，我们提出的MSL与基线相比获得了更好的性能增益，达到12.38%。最后，通过联合使用平滑注意和MSL策略，我们的鲁棒CMI可以大大优于基线，导致总性能提高了15.02%。</p><h3 id="基线提取器的选择">3)基线提取器的选择：</h3><p>​  正如第三节中提到的，我们提出的鲁棒CMI是通用的，其中基线提取器（EfficientNet-b0）可以灵活地被其他网络取代。为此，我们采用了另一种最先进的网络，MobileFormer[51]，作为基线，以证明通过应用我们的鲁棒设计，也可以大大提高提取器的鲁棒性。如表V的最后一行所示，MobileFormer基线的鲁棒性得到了很好的加强，例如，平均AUC增加了13.85%。</p><h3 id="msl中n个场景的影响">4)MSL中，N个场景的影响：</h3><p>​  在MSL策略的设计中，可能会出现一个有趣的问题，即需要多少个场景来实现所需的鲁棒性。因此，我们进行了额外的实验来分析在MSL中对N个场景的影响，结果见表6。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102058441.png"alt="image-20240903102058441" /><figcaption aria-hidden="true">image-20240903102058441</figcaption></figure><p>​  可以看出，当N=1时，提取器的鲁棒性远不能令人满意。其潜在原因有两方面：1)当N =1时，MSL只包含一个分类器，从而退化为正常的训练过程；2)场景的奇异性可能导致对特定场景的严重过拟合，导致泛化性能较差。当考虑的场景数量增加时，上述困境可以很好地缓解。即使我们只同时使用原始场景和Facebook场景（N=2），我们也观察到了显著的性能提高，平均实现了7.72%的AUC改进。此外，考虑到训练和测试过程之间的差异，我们引入了一个数据增强场景，以进一步增强提取器对未知（新的）osn的泛化。根据组合场景的结果，“NoTrans+Aug”，可以得出结论，这种增强场景是有效的，可能是因为这些增强操作在一定程度上与其他osn所使用的操作重叠。最后，最后两行的结果表明，进一步增加场景的数量到N= 3和N =4可以不断提高性能。我们也尝试了用更大的N进行更多的组合；但是额外的性能提高是非常边际的，代价是显著增加的复杂性。因此，在我们的方案中，我们采用了四种场景的组合，即“NoTrans+FB+WA+Aug”。</p><p><em>H.</em> 评估具有挑战性的交叉OSN场景</p><p>​  回想一下，在上述实验中，一对内的图像通过相同的OSN传输（一致的OSN场景）。在本小节中，我们进行了更具挑战性的实验，其中图像来自不同的osn（不一致的OSN场景），比如其中一张图像来自NoTrans，另一张来自Facebook（下面标记为“NT，FB”）。在这些场景中，每个算法的性能结果如表7所示。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903102551041.png"alt="image-20240903102551041" /><figcaption aria-hidden="true">image-20240903102551041</figcaption></figure><p>​  对于包含两个以上osn的列，一对中的图像来自两个随机选择的osn。很明显，我们提出的算法仍然大大优于现有的方法，平均AUC比排名第二的Kuzin[10]提高了8.24%。然而，不出所料，与一致的OSN场景相比，所有算法的性能都表现出了严重的下降。由于相机痕迹和OSN退化的混合，将相机痕迹分离出来，然后进行相机模型识别是相当具有挑战性的。事实上，在某些极端情况下，受OSN-A干扰影响的相机a和受OSN-B干扰影响的相机b可能具有相同的累积效应，从而误导了相机模型识别算法。一个可能的解决方案是进行盲分离，以区分相机的痕迹和OSN干扰。然而，盲分离本身就是一项极具挑战性的任务。在这项工作中，我们专注于在存在一致的OSN干扰下的相机痕迹的鲁棒提取。在未来，我们将继续探索和研究在更具挑战性的不一致OSN场景中的鲁棒摄像机模型识别算法。</p><h2 id="i.设备标识的评估">I.设备标识的评估</h2><p>​  虽然本文的主要重点是相机模型识别，但我们也尝试进行一些关于相机设备识别的实验，即评估算法识别同一模型但不同设备的图像的能力。实验结果如表8所示，其中“Same”指定的列表示AUC结果，所有正对的图像都来自同一设备。类似地，带有“Diff”的列对应于正对图像来自同一模型但不同设备的情况。</p><figure><imgsrc="../postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240903110000029.png"alt="image-20240903110000029" /><figcaption aria-hidden="true">image-20240903110000029</figcaption></figure><p>​  可以观察到，我们提出的方法仍然能够准确地分类来自不同设备的图像，在AUC度量上超过排名第二的ForSim[11]5.04%。此外，与“Diff”的情况下相比，在“Same”的情况下，算法的总体性能往往稍好一些。这些结果表明，同一相机型号的不同设备仍然具有一定的独特的相机痕迹。</p><h1 id="结论">结论</h1><p>​  在本文中，我们研究了在OSN共享图像上设计鲁棒CMI的问题。基于两个关键的观察结果，我们提出了一个鲁棒的CMI方案，明确地利用基于平滑的交叉注意和MSL策略。大量的比较实验与几种最先进的方法证明了我们的方法的优越性，特别是在各种OSN传输的场景中。我们的鲁棒设计也可以揭示一些法医取证问题，如耐OSN水印，鲁棒伪造检测等。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>experimet</title>
      <link href="/experimet/"/>
      <url>/experimet/</url>
      
        <content type="html"><![CDATA[<figure><img src="../postimages/experimet/image-20240902094509834.png"alt="image-20240902094509834" /><figcaption aria-hidden="true">image-20240902094509834</figcaption></figure><figure><img src="../postimages/experimet/image-20240902094320599.png"alt="image-20240902094320599" /><figcaption aria-hidden="true">image-20240902094320599</figcaption></figure><p>第一行是FOCAL论文效果<br/>第二行是复刻的最好效果<br/>第三四行是冻结encoder，训练decoder<br/>第五行是一起训练<br/>表格如下：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;"></td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="odd"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr></tbody></table><p>复刻与FOCAL论文效果相比：</p><table style="width:100%;"><colgroup><col style="width: 15%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 6%" /><col style="width: 7%" /><col style="width: 7%" /><col style="width: 4%" /><col style="width: 5%" /><col style="width: 5%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>FOCAL (HRNet)</td><td style="text-align: center;">9620</td><td style="text-align: center;">9290</td><td style="text-align: center;"></td><td style="text-align: center;">7690</td><td style="text-align: center;">5240</td><td style="text-align: center;"></td><td style="text-align: center;">8640</td><td style="text-align: center;">7060</td><td style="text-align: center;"></td><td style="text-align: center;">8570</td><td style="text-align: center;">6390</td><td style="text-align: center;"></td><td style="text-align: center;">7100</td><td style="text-align: center;">4030</td><td style="text-align: center;"></td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">+161</td><td style="text-align: center;">+292</td><td style="text-align: center;">9779</td><td style="text-align: center;">+309</td><td style="text-align: center;">+380</td><td style="text-align: center;">8425</td><td style="text-align: center;">-43</td><td style="text-align: center;">-102</td><td style="text-align: center;">8696</td><td style="text-align: center;">+166</td><td style="text-align: center;">+255</td><td style="text-align: center;">8426</td><td style="text-align: center;">-298</td><td style="text-align: center;">-555</td><td style="text-align: center;">7822</td></tr></tbody></table><p>decoder性能相比：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">9754</td><td style="text-align: center;">9516</td><td style="text-align: center;">9763</td><td style="text-align: center;">7914</td><td style="text-align: center;">5448</td><td style="text-align: center;">8364</td><td style="text-align: center;">8486</td><td style="text-align: center;">6842</td><td style="text-align: center;">8697</td><td style="text-align: center;">8763</td><td style="text-align: center;">6707</td><td style="text-align: center;">8478</td><td style="text-align: center;">6846</td><td style="text-align: center;">3531</td><td style="text-align: center;">7885</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">9766</td><td style="text-align: center;">9533</td><td style="text-align: center;">9773</td><td style="text-align: center;">7900</td><td style="text-align: center;">5460</td><td style="text-align: center;">8335</td><td style="text-align: center;">8505</td><td style="text-align: center;">6833</td><td style="text-align: center;">8687</td><td style="text-align: center;">8769</td><td style="text-align: center;">6720</td><td style="text-align: center;">8485</td><td style="text-align: center;">6848</td><td style="text-align: center;">3524</td><td style="text-align: center;">7880</td></tr><tr class="even"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">9719</td><td style="text-align: center;">9531</td><td style="text-align: center;">9732</td><td style="text-align: center;">7961</td><td style="text-align: center;">5516</td><td style="text-align: center;">8573</td><td style="text-align: center;">8456</td><td style="text-align: center;">6703</td><td style="text-align: center;">8729</td><td style="text-align: center;">8856</td><td style="text-align: center;">6914</td><td style="text-align: center;">8611</td><td style="text-align: center;">6792</td><td style="text-align: center;">3498</td><td style="text-align: center;">7934</td></tr><tr class="odd"><td>Log_v09061430/latest/</td><td style="text-align: center;">Log_v09061430/latest/</td><td style="text-align: center;">9718</td><td style="text-align: center;">9545</td><td style="text-align: center;">9734</td><td style="text-align: center;">8036</td><td style="text-align: center;">5693</td><td style="text-align: center;">8604</td><td style="text-align: center;">8284</td><td style="text-align: center;">6411</td><td style="text-align: center;">8547</td><td style="text-align: center;">8826</td><td style="text-align: center;">6851</td><td style="text-align: center;">8582</td><td style="text-align: center;">6767</td><td style="text-align: center;">3495</td><td style="text-align: center;">7930</td></tr></tbody></table><p>计算差值。以下是计算结果：</p><table><colgroup><col style="width: 13%" /><col style="width: 13%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 5%" /><col style="width: 6%" /><col style="width: 6%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 4%" /></colgroup><thead><tr class="header"><th></th><th style="text-align: center;">decoderweights</th><th style="text-align: center;">Columbia F1</th><th style="text-align: center;">Columbia IOU</th><th style="text-align: center;">Columbia AUC</th><th style="text-align: center;">COVERAGE F1</th><th style="text-align: center;">COVERAGE IOU</th><th style="text-align: center;">COVERAGE AUC</th><th style="text-align: center;">CASIA F1</th><th style="text-align: center;">CASIA IOU</th><th style="text-align: center;">CASIA AUC</th><th style="text-align: center;">MISD F1</th><th style="text-align: center;">MISD IOU</th><th style="text-align: center;">MISD AUC</th><th style="text-align: center;">NIST F1</th><th style="text-align: center;">NIST IOU</th><th style="text-align: center;">NIST AUC</th></tr></thead><tbody><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;"></td><td style="text-align: center;">9781</td><td style="text-align: center;">9582</td><td style="text-align: center;">9779</td><td style="text-align: center;">7999</td><td style="text-align: center;">5620</td><td style="text-align: center;">8425</td><td style="text-align: center;">8597</td><td style="text-align: center;">6958</td><td style="text-align: center;">8696</td><td style="text-align: center;">8736</td><td style="text-align: center;">6645</td><td style="text-align: center;">8426</td><td style="text-align: center;">6802</td><td style="text-align: center;">3475</td><td style="text-align: center;">7822</td></tr><tr class="even"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/latest/</td><td style="text-align: center;">-27</td><td style="text-align: center;">-66</td><td style="text-align: center;">-16</td><td style="text-align: center;">-85</td><td style="text-align: center;">-172</td><td style="text-align: center;">-61</td><td style="text-align: center;">-111</td><td style="text-align: center;">-116</td><td style="text-align: center;">+1</td><td style="text-align: center;">+27</td><td style="text-align: center;">+62</td><td style="text-align: center;">+52</td><td style="text-align: center;">+44</td><td style="text-align: center;">+56</td><td style="text-align: center;">+63</td></tr><tr class="odd"><td>Log_v09011445/Ep002_0.7782/</td><td style="text-align: center;">Log_v09041701/Ep001_0.6812/</td><td style="text-align: center;">-15</td><td style="text-align: center;">-49</td><td style="text-align: center;">-6</td><td style="text-align: center;">-99</td><td style="text-align: center;">-160</td><td style="text-align: center;">-90</td><td style="text-align: center;">-92</td><td style="text-align: center;">-125</td><td style="text-align: center;">-9</td><td style="text-align: center;">+33</td><td style="text-align: center;">+75</td><td style="text-align: center;">+59</td><td style="text-align: center;">+46</td><td style="text-align: center;">+49</td><td style="text-align: center;">+58</td></tr><tr class="even"><td>Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">Log_v09061430/Ep002_0.6672/</td><td style="text-align: center;">-62</td><td style="text-align: center;">-51</td><td style="text-align: center;">-47</td><td style="text-align: center;">-38</td><td style="text-align: center;">-104</td><td style="text-align: center;">+148</td><td style="text-align: center;">-141</td><td style="text-align: center;">-255</td><td style="text-align: center;">+33</td><td style="text-align: center;">+120</td><td style="text-align: center;">+269</td><td style="text-align: center;">+185</td><td style="text-align: center;">-10</td><td style="text-align: center;">+23</td><td style="text-align: center;">+112</td></tr><tr class="odd"><td>Log_v09061430/latest/</td><td style="text-align: center;">Log_v09061430/latest/</td><td style="text-align: center;">-63</td><td style="text-align: center;">-37</td><td style="text-align: center;">-45</td><td style="text-align: center;">+37</td><td style="text-align: center;">+73</td><td style="text-align: center;">+179</td><td style="text-align: center;">-313</td><td style="text-align: center;">-547</td><td style="text-align: center;">-149</td><td style="text-align: center;">+90</td><td style="text-align: center;">+206</td><td style="text-align: center;">+156</td><td style="text-align: center;">-35</td><td style="text-align: center;">+20</td><td style="text-align: center;">+108</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</title>
      <link href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/"/>
      <url>/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/</url>
      
        <content type="html"><![CDATA[<center>Towards Modern Image Manipulation Localization:A Large-Scale Dataset andNovel Methods <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a> <ahref="https://github.com/qcf-568/MIML"><imgsrc="https://img.shields.io/github/stars/qcf-568/MIML?style=flat"alt="GitHub" /></a></center><center><span class="math inline">\(\text{Chenfan Qu}^1,\text{YiwuZhong}^{2,*},\text{Chongyu Liu}^1,\text{Guitao Xu}^1,\text{DezhiPeng}^1,\text{Fengjun Guo}^3,\text{Lianwen Jin}^{1,4,*}\)</span></center><center>1华南理工大学，2威斯康星大学，3 INTSIG信息有限公司，4INTSIG-SCUT文件分析与识别联合实验室</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  近年来，图像操纵定位因其在保障社交媒体安全方面的关键作用而越来越受到人们的关注。然而，如何准确地识别伪造的区域仍然是一个开放的挑战。其中一个主要的瓶颈是，由于其昂贵的创建过程，严重缺乏高质量的数据。为了解决这一限制，我们提出了一种新的范式，称为CAAA，可以在像素级自动和精确地注释大量的人工伪造的图像。我们进一步提出了一种新的度量QES，以方便不可靠注释的自动过滤。利用CAAA和QES，我们构建了一个大规模、多样化、高质量的数据集，其中包括123,150张带有掩码注释的人工伪造图像。此外，我们开发了一种新的模型APSCNet，用于精确的图像篡改定位。根据大量的实验，我们的数据集显著地提高了在广泛使用的基准测试上的各种模型的性能，这些改进归因于我们提出的有效方法。这些数据集和代码可以在https://github.com/qcf-568/MIML上公开获得。</p><h1 id="引言">引言</h1><p>  我们提出了一种新的思想，利用训练有素的约束图像处理定位模型，自动获取这些未标记的伪造图像的掩模标注，从而大大缓解了图像处理定位的数据稀缺问题，如图1所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826151824078.png"alt="image-20240826151824078" /><figcaption aria-hidden="true">image-20240826151824078</figcaption></figure><p>图1.我们提出了一种新的约束图像处理定位范例，它分别处理SPG和SDG中的图像。我们还建议将它用于自动注释，并构建一个大规模、高质量的数据集，显著提高了图像篡改定位模型的泛化性。</p><p>  由于约束图像篡改定位方法利用相应的真实图像对伪造区域进行定位，可以大大降低任务的复杂性。<br/><br/>  然而，尽管在挑战性较小的数据方面取得了进展，但由于三个严重的障碍，以前的约束图像篡改定位方法不足以作为复杂的现代图像的合格自动注释器。首先，他们大多使用一个单一的基于相关性的模型来处理所有的输入数据[19,28]，我们认为这是一个次优的范式。一般情况下，根据操纵图像之间的共同部分是伪造区域还是真实区域，将伪造图像对及其原始图像可分为共享供体组（SDG，SharedDonor Group）和共享探针组（SPG， Shared ProbeGroup）两组，如图2所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826152216897.png"alt="image-20240826152216897" /><figcaption aria-hidden="true">image-20240826152216897</figcaption></figure><p>  虽然以往的基于相关的方法对于可持续发展目标是合理的，但它们还不够适合SPG，因为SPG中数据的实际公共部分大多是背景，而可持续发展目标中的实际公共部分大多是前景。与可持续发展目标中的共享前景相比，SPG中的共享背景具有更大的面积和更少的显著特征。同时对可持续发展目标和SPG数据上训练基于相关的模型会导致混淆，削弱其泛化能力。其次，从真实图像中减去伪造图像得到的差异图总是可以突出伪造区域，但这一重要线索被以往的约束图像篡改定位方法完全忽略了。最后，以往的工作对操作过程中大量重新缩放操作导致的语义错位关注不够重视，这混淆了模型并对其产生负面影响。<br/><br/>  为了解决这些问题，我们提出了一种新的范例，称为类别感知自动注释（CAAA，Category-AwareAuto-Annotation），它分别处理SDG和SPG中的图像对。所提出的CAAA范式由三个组成部分组成。首先，使用分类器来确定输入图像对是属于SDG还是SPG。该分类器可以通过使用无标记图像的自监督学习来有效地进行训练。其次，利用差异感知语义分割模型，利用图像对及其差异映射在SPG中进行精确的约束操作定位。此外，一个语义对齐相关匹配模型，通过更好的语义对齐提高了SDG的性能。实验表明，我们的方法在复杂场景下显著优于以往的约束图像篡改定位方法，并且足以进行自动标注。<br/><br/>  随后，我们从互联网上收集了大量手工伪造的图像，然后用提出的CAAA对其伪造的区域进行注释。该方法可以显著缓解图像处理定位中非合成数据的稀缺性，如图1所示。为了确保所有的注释都足够可靠，我们进一步提出了一个新的度量标准，称为质量评估评分（QES）。QES可以自动评估注释的质量并排除坏的注释，而不需要ground-truths来计算。实验表明，我们的数据集可以在广泛使用的基准测试上显著改进各种图像篡改定位模型。<br/><br/>  此外，为了更好地利用我们的MIML数据集，我们提出了一个新的模型，称为APSC-Net，它在各种基准测试上都优于以前的方法。<br/><br/>  综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一个新想法：从网络规模的图像中促进图像篡改定位的任务，以及从较少挑战性的任务，约束图像篡改定位中提取的自动注释。</li><li>我们提出了一种新的约束图像篡改定位范式，称为CAAA，它分别处理SPG和SDG。对于SPG，我们建议使用用语义信息去噪的图像差分。对于可持续发展目标，我们建议将语义与一个跨级别的特征相关框架对齐。</li><li>我们提出了一种新的有效度量QES，在数据集构建时自动过滤出不可靠的掩码注释。</li><li>基于上述技术，我们构建了一个大规模的、多样化的、高质量的数据集，称为MIML。它显著地解决了图像篡改定位的手工伪造数据的问题，从而大大提高了模型的泛化能力。</li></ul><p>  与图像处理定位相比，约束图像处理定位（CIML， constrained imagemanipulationlocalization）[32]在给定的真实图像的额外帮助下对伪造的图像区域进行定位。以往的工作大多是基于相关匹配，并对SDG和SPG中的图像对进行统一处理。Wu等人[32]提出了第一个深度相关模型DMVN，该模型计算相关映射来定位图像中的相似对象。Liu等人提出去除池化层，采用无卷积获得更丰富的空间信息。Liu等人的[18]采用了注意感知机制来获得更好的表现。Tan等人[28]提出在编码器和解码器中都执行相关性，以提取更好的特征。这些方法在挑战性较小的数据集上取得了重大进展（例如，合成COCO[32]）。然而，它们的性能在具有高分辨率、大变化度和大复杂度的现代图像中受到限制。</p><h1 id="分类感知自动注释模块">分类感知自动注释模块</h1><p>  对于受限的图像篡改定位，以往的工作没有考虑SPG和SDG图像对之间的差异，而是使用单一的相关性模型对其进行统一处理。我们认为这种范式是次优的，原因如下：</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826160604894.png"alt="image-20240826160604894" /><figcaption aria-hidden="true">image-20240826160604894</figcaption></figure><p>  首先，SDG 图像的相似区域是前景区域（例如，图3中SDG分支中的猫）。它们有特定的、相似的形状和独特的特征。相比之下，SPG图像的相似区域是背景。它们通常没有足够独特的特征来进行精确的相关匹配（例如，在图3的SPG分支图像中，一块背景中的雪与所有其他块背景中雪有很高的相似性）。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。因此，这些区域很可能会在基于相关性的模型中造成混淆，特别是在复杂的场景中。<br/><br/>  第二，配对的SPG图像之间的差异是一个重要的提示。SPG图像对中的大部分区域几乎相同，并在空间上对齐（例如，图3中SPG分支的图像对）。简单地在它们之间减去最终的差异图就可以突出被操纵的区域。然而，这些信息在以前的基于相关匹配的模型中难以利用，因此在以前的CIML工作中没有考虑到。<br/><br/>  基于这些观察结果，我们提出了一个新的CIML任务范式，类别感知自动注释CAAA。其关键思想是独立处理SPG和SDG图像，如图3所示。首先，利用第3.1节中提出的分类器，将输入的图像对分为SPG或SDG。对于SPG，图像对采用第3.2节中提出的差分感知语义分割进行处理。对于SDG，图像对通过第3.3节中提出的语义对齐相关匹配进行处理。更重要的是，用我们提出的范式训练的模型被进一步用于执行在大量手工伪造的图像上的自动注释。作为回报，收集的数据解决了用于图像篡改定位的非合成数据的严重短缺。</p><h2 id="自监督分类器">自监督分类器</h2><p>  为了实现SPG和SDG的分类，我们提出了通过对无标记图像的自监督学习来训练分类器。给定一个图像，我们对其进行随机的增强和操作，然后将伪造的图像和原始图像形成一个SPG图像对。为了构建一个SDG图像对，我们从原始图像中复制随机对象，调整它们的大小，并将它们粘贴到另一个图像中。利用所得到的图像对，我们可以有效地训练我们的分类器。每个输入对中的两个图像在被输入到分类器之前被连接在通道维数中。分类器只需要识别一个图像对中的两个图像是几乎相同（SPG）还是明显不同（SDG），而不考虑哪一个或哪里是假的。因此，这个分类任务非常简单，我们可以准确地将图像对分成两组。</p><h2 id="具有差异感知能力的语义分割">具有差异感知能力的语义分割</h2><p>  理想情况下，对于SPG中的图像对，真实图像和伪造图像之间的绝对差异实际上是伪造区域。然而，被操纵的图像在传输[31]过程中通常会发生退化，这使得无法利用绝对差异作为精确的注释。如图4所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826162503002.png"alt="image-20240826162503002" /><figcaption aria-hidden="true">image-20240826162503002</figcaption></figure><p>图4.由于篡改后的图像在传输过程中通常会经历一系列的退化，因此它们与真实图像之间的绝对差异不能准确地表示伪造区域。我们的方法通过使用语义信息来实现了充分的去噪，从而解决了这一问题。</p><p>  由于传输退化，图像差分图中几乎所有的区域都是非零的。即使是OTSU[24]算法二值化的差异图也在真实区域上突出，特别是在高频区域，如边缘。为了解决这个问题，我们建议利用图像中的语义信息来去噪差异映射。为了实现这一点，我们提出将真实图像、伪造图像及其差异映射图的信道维数连接输入到一个语义分割模型中。</p><h2 id="语义对齐的关联匹配">语义对齐的关联匹配</h2><p>  由于广泛的重缩放操作，语义失调成为对基于相关性的方法的有效性产生不利影响的关键因素。例如，在图3的SDG分支中，原始图像中的猫占据了一个很大的区域，而在伪造的图像中，同一只猫被限制在一个小得多的区域内。原始图像的猫特征大多在最高水平，而伪造图像的猫特征大多在最低水平。因此，两幅图像之间在同一编码水平上的视觉特征存在语义错位。然而，以往的工作只是迫使模型在相同的特征层次之间进行特征匹配，这混淆了模型，并对其泛化产生了负面影响。为此，我们提出通过实现更好的语义对齐来提高相关模型的性能。<br/><br/>  具体来说，给定从主干模型中提取的一组不同分辨率的特征映射，我们首先用平均池化的最高特征计算全局表示，然后用卷积层将它们与最高特征融合。随后，我们以一种类似于在FPN[15,34]中的自上而下的方式融合了这些特性映射。这样，低级特性就具有更多的语义，并准备与高级特性相匹配。然后，我们以跨层次的方式计算输入图像对特征之间的相关特征$ F_{corr} $，如式(1)，这不同于以前的方法[18,19,32]，它只计算与方程(2)相同水平的特征图之间的相关特征。<span class="math display">\[[Corr(F_{o,i},F_{m,j}) for i in (0{-}3) andfor j in (0{-}3)]\]</span></p><p><span class="math display">\[[Corr(F_{o,i},F_{m,i}) for i in (0-3)]\]</span></p><p>  在这些方程中，Corr表示之前工作中广泛使用的相关函数，[18,19,32]， $F_{o,i} $ 表示原始图像的第i层特征图， $ F_{m,j} $表示伪造图像的第j层特征图。我们的模型能够自适应地选择最优匹配路由，从而增强语义对齐。$ F_{corr} $ 随后被连接，信道减少并输入卷积解码器进行最终预测。</p><h1 id="miml数据集">MIML数据集</h1><p>  在本节中，我们提出了一个大规模的、多样化的、高质量的数据集，称为MIML。其关键思想是利用在现有数据集上训练的约束图像篡改定位模型，自动从网络中人工伪造的图像获得准确的掩码注释。为了确保数据集的高质量，我们还提出了一个新的度量标准来过滤掉不充分的注释。</p><h2 id="数据集构成">数据集构成</h2><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826163432173.png"alt="image-20240826163432173" /><figcaption aria-hidden="true">image-20240826163432173</figcaption></figure><p>如图6所示，我们构建MIML的步骤如下：</p><p>  <strong>图像收集。</strong>我们从imgur.com中收集图像对。在这个网站上，这些图片是由数百万人手工伪造的，因此有高质量、多样化的伪造区域。<br/><br/>  <strong>数据清理。</strong>我们清理收集到的数据，并排除与第6块中的评估数据集重叠的图像。<br/><br/>  <strong>分类。</strong>我们使用第3.1节中提出的分类器将清理后的图像对分类为SPG或SDG。实际的分类器是三个模型[8,20,21]的集合。<br/>  <strong>自动注释。</strong>我们利用第3.2节和第3.3节中提出的DASS和SACM，分别自动获取SPG和SDG中图像的掩码注释。<br/><br/>  <strong>质量评价。</strong>经过自动注释，SPG的注释已经有了高质量，而SDG的注释仍然不令人满意。为了保证整体质量，我们提出了一种新的度量标准，质量评估评分（QES），以进一步过滤掉不可靠的注释。QES的关键思想是，大多数高质量的预测都有非常高的置信度和尖锐的边缘，因此我们可以评估预测的质量，并通过检查预测的置信度和清晰度来排除不好的预测。具体来说，给定一个具有形状（H，W）和归一化概率的预测掩模，我们计算QES如下：<spanclass="math display">\[\textbf{QES=}\frac{\sum_{i,j}^{H,W}p_{i,j}&gt;(1-T_{h})}{\sum_{i,j}^{H,W}p_{i,j}&gt;T_{l}}\]</span>  其中， $~ \sum_{i,j}^{H,W}p_{i,j} &gt; (1 - T_{h}) $ 表示高置信度大于$ (1 - T_{h}) $ 的预测区域， $~ \sum_{i,j}^{H,W}p_{i,j}&gt;T_l $表示预测的总潜在操纵面积。我们将 $ T_{h} $ 和 $ T_{l} $ 设置为 $~\frac{1}{16} $，只保留QES&gt;0.5的样品。实验表明，我们的QES与IoU度量有很强的相关性，可以有效地帮助过滤出不可靠的掩码注释。</p><h2 id="数据集亮点">数据集亮点</h2><p>  我们在图5中给出了所建议的数据集的几个例子。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826164936525.png"alt="image-20240826164936525" /><figcaption aria-hidden="true">image-20240826164936525</figcaption></figure><p>  我们的数据集的主要亮点如下：</p><ul><li><strong>高质量。</strong>该数据集中的图像操作是由人类精心制作的。这些数据可以教模型在现实世界中发现伪造，而不仅仅是在合成数据中过度拟合几个简单的模式。</li><li><strong>大规模。</strong>如表1所示，所提出的数据集共有123,150张人工伪造的图像，比之前的手工IML数据集多几十倍（例如，≈比IMD20的60倍）。</li><li><strong>多样性。</strong>我们的数据集包括各种大小、各种样式和各种类型的操作（例如，复制-移动、拼接、删除）的图像。它们是由成千上万的人利用各种软件创建的。这些不同的数据可以大大提高深度IML模型的泛化能力。</li><li><strong>现代风格。</strong>我们的数据集有大量的现代图像，最近被捕获和伪造，跟上了现代数码摄影技术的步伐。相比之下，CASIA数据集[3]是在十多年前提出的，其中大多数图像的尺寸都很小，而且都很模糊。因此，我们的数据集可以更好地满足现代图像操作定位的要求。</li><li><strong>强大的可扩展性。</strong>网络上有许多越来越受欢迎的图像处理比赛，不断吸引数百万人来参加（例如1900万pas-Battles[9,25]的900万人），产生了大量新的手工伪造图像。我们的数据集构建方法已经准备好利用这些不断增长的廉价web数据。因此，我们的数据集可以很容易地进行扩展，显示出强大的可扩展性。</li></ul><h1 id="apsc-net">APSC-Net</h1><p>  在本节中，我们提出了一个新的模型，称为APSCNet，以实现精确的图像操作定位。如图7所示，它由特征提取器、自适应感知模块和自校准模块组成。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png"alt="image-20240826165320985" /><figcaption aria-hidden="true">image-20240826165320985</figcaption></figure><h2 id="自适应感知模块">自适应感知模块</h2><p>  在细致的图像取证分析过程中，人类经常会反复放大和缩小图像，选择一组最佳的观察结果来帮助他们的最终预测。为了模拟人类的感知方式，我们设计了一个自适应感知模块，以帮助模型比较不同的视图，并自适应地选择每个输入图像的最优组合。其关键思想是使用从全局表示计算出的自适应权重对当前和所有高级特征图进行加权。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170701354.png"alt="image-20240826170701354" /><figcaption aria-hidden="true">image-20240826170701354</figcaption></figure><p>  具体来说，给定从主干模型中提取的4个特征图，我们首先将它们映射到1×1转换层的通道上，得到4个特征图$ F_{i,0},F_{i,1},F_{i,2},F_{i,3} $ 。然后，我们通过全局平均池化从 $F_{i,3} $ 中得到全局图像表示，并使用1×1卷积层将它们与 $ F_{i,3} $融合，得到 $ F_{o,3} $。最后，对于（2，1，0）中的a和范围（a+1，3）中的b，我们遵循下面的公式(3)和(4)依次计算$ F_{o,a} $ ： <spanclass="math display">\[[w_{a,a},w_{a,b}]=\sigma(f_{a}(Cat([Avg(F_{i,a}),Avg(F_{o,b})])))\]</span></p><p><spanclass="math display">\[F_{o,a}=Conv(w_{a,a}*F_{i,a}+\sum_{b=a+1}^3w_{a,b}*F_{o,b})\]</span></p><p>  其中Avg表示全局平均池，Cat表示通道维连接， $ f_a $表示具有ReLU层的两个线性层， $ $表示Sigmoid激活函数，Conv表示3×3卷积层。</p><h2 id="自校准模块">自校准模块</h2><p>  当对操纵图像进行细致的定位时，人类倾向于通过比较预测的伪造区域周围的特征来确认他们的初始预测。此外，他们可能会根据他们对图像真实性的全局评估来修改他们的局部预测。为了模拟人类的感知方式，我们设计了一个自校准模块，以获得更好的性能。<br/><br/>  如图7所示，提出的自校准模块包括基于分割的自校准（SSC，Segmentation-basedSelf Calibration）和基于分类的自校准（CSC，Classification-based SelfCalibration）。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826170727546.png"alt="image-20240826170727546" /><figcaption aria-hidden="true">image-20240826170727546</figcaption></figure><p>  对于SSC，从自适应感知模块的末端获得初始预测，并将其输入一个由几个卷积层组成的小校准核映射模块。随后，我们得到一个校准核，并对其初始预测进行卷积运算。然后使用Min-Max方法对结果值进行归一化。我们将标准化结果乘以$ F_o $ ，即 $ F_{o,0},F_{o,1},F_{o,2},F_{o,3} $ 的串联，得到 $ F_{ref1}$ 。接下来，我们使用多个卷积层来细化 $ F_{ref1} $ ，并得到细化的特征 $F_{ref2} $ 。在此之后，我们将 $ F_{ref2} $ 与 $ F_o $连接起来，执行信道注意和信道减少的方法，用结果替换 $ F_o $ ，并重复利用$ F_o $ 和校准预测的过程，再次获得 $ F_{ref2} $ 两次，以获得 $ F_{ref2}$的改进版本。利用SSC，我们的模型可以根据其初始掩模预测大致自适应地关注最优区域，从而通过深入分析获得更高的性能。<br/><br/>  对于CSC，我们首先将改进后的特征$ F_{ref2} $输入到一个小分类头中，用于预测输入图像是否被篡改。如果图像被预测为真实，掩模预测很可能会有很多的假阳性（FP），所以我们增加二值化阈值来减少FP。另一方面，如果图像被预测为篡改，我们会降低二值化阈值以减少假阴性。给定输入图像被预测为篡改的概率P，CSC将预测掩模的二值化阈值从0.5调整到$~ min(max(1-P,\lambda),1-\lambda) $ ， $~ \lambda $ 设置为0.3。</p><h1 id="实验">实验</h1><h2id="受约束图像篡改定位ciml任务的实验">受约束图像篡改定位CIML任务的实验</h2><p>  图像操作自动标注的任务可以作为一个CIML任务的评估。考虑到IMD20数据集[23]中的图像与我们要标注的目标图像非常相似，我们使用其中一部分使用IoU和F1-score来评估模型的性能。</p><h3 id="实施细节">实施细节</h3><p>  我们将IMD20中伪造的伪造图像分为SPG或SDG，并将它们以大约3：1的比例随机分成训练集和测试集。CASIAv2[3]和大约100万张通过使用COCO数据集合成的图像[14]也被用于训练。输入图像的大小调整为512x512，并在所有方法中应用一致的训练配置以进行公平比较。</p><h3 id="消融实验">消融实验</h3><p>  对于SPG，伪造图像与其真实图像之间的图像差异可以粗略地表示伪造区域，图像本身可以提供语义信息，帮助模型去噪差异映射。我们在IMD20SPG的测试集上对所提出的差异感知语义分割进行了消融实验，如表4的右侧所示，这两种方法都可以提高模型的性能。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172424444.png"alt="image-20240826172424444" /><figcaption aria-hidden="true">image-20240826172424444</figcaption></figure><p>表4。在IMD20SPG上进行的CIML实验。左：我们的差异感知语义分割的比较研究。右：对应的消融实验。“DMVN*”表示同时使用SDG和SPG数据进行训练的DMVN，类似于“DMAC*”。“Nonzero”表示使用一对图像之间的差值的非零区域，“OTSU”表示用OTSU二值化的差值。'w.o.Difference‘表示语义分割模型的输入只包含图像对，’w.o.Images‘表示只使用图像对的差异映射作为输入。“Ours(VGG)”表示我们的模型与DMAC具有相同的VGG主干。“Ours(VAN)”表示我们的模型使用VAN主干。</p><p>  对于SDG，语义对齐可以减少训练过程中的混淆，帮助我们的模型实现更好的泛化。我们在IMD20SDG的测试集上对所提出的语义对齐相关匹配进行了消融实验，结果如表5的右侧所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826172752628.png"alt="image-20240826172752628" /><figcaption aria-hidden="true">image-20240826172752628</figcaption></figure><p>  显然，这两个提议的组件都有助于提高模型的更高性能。此外，所提出的质量评价评分（QES）允许自动过滤最令人满意的预测。由于IMD20的groundtruth中存在一些错误，我们的方法足以获得准确的自动注释。</p><h3 id="对qes的消融实验">对QES的消融实验</h3><p>  所提出的QES度量的目标是在数据集创建期间自动过滤掉糟糕的预测，其中groundtruth是不可用的。如表3所示，越高的QES阈值，精度就会越高。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173009052.png"alt="image-20240826173009052" /><figcaption aria-hidden="true">image-20240826173009052</figcaption></figure><p>  这是因为具有更大的高置信度和更清晰的边缘比率的预测大多更接近实际的groundtruth，而清晰度和置信度可以通过我们的QES很好地评估。因此，我们的QES显示了与IoU度量有很强的相关性。</p><h3 id="比较实验">比较实验</h3><p>  我们在与我们相同的数据上，使用它们的公共代码对DMVN [32]和DMAC[19]进行了重新训练，结果如表4和表5的左侧所示。显然，我们的方法明显优于这些以前的方法。值得注意的是，同时使用SPG和SDG数据训练的DMVN和DMAC在这两个任务上的表现都比只使用SPG或SDG数据训练的任务更差。</p><h2 id="图像篡改定位iml任务的实验">图像篡改定位IML任务的实验</h2><h3 id="实施细节-1">实施细节</h3><p>  我们采用ConvNeXt-Base[21]作为特征提取器，对模型进行160k次迭代的训练，批量大小为20，在按照之前的工作[6,12]进行训练时，输入大小设置为512x512。我们使用交叉熵损失和AdamW优化器[22]，学习速率从1e-4到1e-6。CASIAv2[3]和CAT-Net [12]中的合成数据集用于按照之前的工作[6,12]进行训练。</p><h3 id="对miml数据集的消融实验">对MIML数据集的消融实验</h3><p>  除了我们的APSCNet外，我们分别用PSCC-Net [17]和CAT-Net[12]的公共代码重新训练了提出的MIML数据集。当使用MIML数据集进行训练时，我们对原始合成数据和MIML采用近似1：1的采样比，所有实验中的总训练量都是固定的，以便进行公平比较。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826173603657.png"alt="image-20240826173603657" /><figcaption aria-hidden="true">image-20240826173603657</figcaption></figure><p>  如表2所示，MIML可以显著提高所有这些模型的性能，而在训练或测试过程中没有任何额外的负担。这是因为MIML可以大大缓解深度IML模型中人工伪造数据的严重短缺。为了进一步确认我们的MIML数据集的有效性，我们将IMD20数据集随机划分为10个12个样本的IMDP1和988个样本的IMDP2，用相同大小规模的IMDP1替换MIML数据集，用它们训练APSC-Net。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174020848.png"alt="image-20240826174020848" /><figcaption aria-hidden="true">image-20240826174020848</figcaption></figure><p>  如表6所示，尽管在训练中加入IMDP1减轻了领域差距，提高了模型在NIST16和IMDP2上的性能，但仍明显低于使用MIML训练的模型。显然，MIML可以通过其大量不同的手工伪造数据，显著提高深度模型的泛化能力。</p><h3 id="apsc-net的比较实验">APSC-Net的比较实验</h3><p>  我们在广泛使用的基准测试上比较了我们的APSC-Net与最先进的（SOTA）方法的性能。考虑到以前的方法执行不同的后处理，导致不公平（例如EVP[16]使用最佳阈值计算GT执行二值化），我们忽略了与他们所提出的方法无关的后处理和用一个固定的阈值0.5均匀地二值化预测，然后评估性能与普通IoU和F1-score指标。定量结果如表7所示，我们的APSC-Net在所有这些基准测试上都优于以前的最先进的方法。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174243317.png"alt="image-20240826174243317" /><figcaption aria-hidden="true">image-20240826174243317</figcaption></figure><p>  视觉比较的定性结果如图8所示。</p><figure><imgsrc="../postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826174324363.png"alt="image-20240826174324363" /><figcaption aria-hidden="true">image-20240826174324363</figcaption></figure><h1 id="结论">结论</h1><p>  在本文中，我们提出了一种新的约束图像处理定位（CIML）范例，称为CAAA，它分别处理共享探针组SPG和共享供体组SDG图像对。实验表明，该范式明显优于以往的CIML方法。在此范例下，训练后的模型被用于自动标注未标记的伪造图像，以进行图像操作定位。我们还提出了一种新的度量QES来自动排除错误的预测。因此，我们提出了一个大规模、多样化、高质量的数据集MIML，包括123,150张人工伪造的图像和像素级注释，这可以通过解决它们的数据稀缺问题来激发深度取证模型的潜力。此外，我们提出了一种新的有效模型APSC-Net用于图像操作定位。我们希望我们提出的CAAA范式、QES度量、MIML数据集和APSCNet能够为社区带来见解，并促进图像操作定位的现实应用。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</title>
      <link href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/"/>
      <url>/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>Multi-view Feature Extraction via Tunable Prompts is Enough for ImageManipulation Localization <ahref="https://openreview.net/forum?id=Ci5g2dnrMK"><imgsrc="https://img.shields.io/badge/ACMMM-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\text{Xuntao Liu},\text{YuzhouYang},\text{Haoyue Wang},\text{Qichao Ying},\\\text{ZhenxingQian}^*,\text{Xinpeng Zhang},\text{Sheng Li},\)</span></center><center>复旦大学计算机科学学院，NVIDIA上海</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/302_Multi_view_Feature_Extract.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  虚假图像可以通过社交网络服务迅速传播，构成重大风险。图像篡改定位（IML）的快速发展试图解决这个问题。然而，IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p>  为了应对这一挑战，我们提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。<br/>  具体来说，一组可调提示使冻结的预训练模型能够提取多视图特征，包括空间和高频特征。这种方法最大限度地减少了跨不同视图进行特征提取的冗余架构，从而降低了培训成本。<br/>  此外，我们开发了一个即插即用的特征对齐和融合模块，该模块无缝集成到预训练的模型中，无需进行额外的结构修改。所提出的模块通过交互式处理降低了特征中的噪声和不确定性。</p><p>  实验结果表明，我们提出的方法在6个测试数据集上取得了优异的性能，表现出卓越的鲁棒性。</p><h1 id="引言">1. 引言</h1><p>  我们观察到，分类、目标检测和语义分割等任务具有许多具有丰富的先验知识的预训练模型，如双变压器[20]。考虑利用这些预先训练过的模型来处理IML任务中的挑战是很自然的。然而，直接将它们应用到IML任务中被证明是低效的[22]。这种低效源于IML任务的独特性质，该任务侧重于从图像中提取非语义的视觉线索和低层次的不连续性。有两个关键方面说明了这种特殊性：</p><p>  1)高频信息：由不同的摄像机捕获的图像显示出不同的噪声模式[16]。这给伪造的图像带来不一致的噪声，而真实的区域来自不同的图像。此外，由不同网络生成的图像可能在频域[27]上存在差异。<br/>  2)边缘信息：图像编辑的级别可能会发生变化，导致在锻造区域的边界上的锯齿状和不光滑的边缘或颜色不一致的[37]。这些细节对于精确的操作定位化至关重要，但在许多任务中经常被忽略。</p><p>  IML-ViT[22]是在IML任务中使用基于普通ViT[6]体系结构的预训练模型的开创性尝试。它们还结合了边缘监督，将网络的注意力引向微妙的篡改伪影。然而，IML-ViT忽略了在以前的许多工作[4,14,15]中已经验证过的有效的高频信息。在IML任务中，处理多视图特征通常需要并行的主干架构[4,14]，这在参数增加的紧急情况下变得具有挑战性。此外，IML-ViT尽管利用了预先训练过的模型，但仍需要从头开始使用数据集来训练模型。这无疑对计算资源产生了巨大的需求，特别是在调优大型预训练模型方面。此外，之前的一些工作表明，在下游任务上调整大型预训练模型可能会损害模型[30]的性能，这在我们的比较实验中也可以观察到。</p><p>  在本文中，我们提出了Prompt-IML，如图1所示，旨在通过利用预训练模型的丰富的先验知识来解决IML任务中数据集的稀缺问题。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824185716412.png" alt="image-20240824185716412" style="zoom:67%;" /></p><p>图1：Prompt-IML利用一个具有冻结参数的单一预训练主干，通过可调谐提示来处理多视图特性。特性对齐和融合模块被设计为特性交互和增强的即插即用组件。</p><p>  具体来说，Prompt-IML遵循一个编码器-解码器架构。基于预训练模型的编码器负责特征提取，然后解码器对这些特征进行处理，以准确定位被操纵的区域。为了在不诉诸于复杂的并行架构的情况下处理有利于IML任务的多视图特性，我们建议使用可调提示集来利用预先训练好的模型作为编码器。在训练这些提示时，我们冻结了预先训练好的模型。它有三个主要的优势。首先，它允许预先训练好的模型用于处理每个视图中的特征。其次，处理后的特征保留了来自预训练模型的鲁棒性。最后，它有助于减少训练所需的计算资源。</p><p>  此外，考虑到多视图特征之间的变化，我们提出了一个特征对齐和融合（FAF）模块。该模块被设计为即插即用组件，可以无缝集成到编码器，而不需要额外的结构修改。在FAF模块中，针对不同的优点采用了多种注意机制。FAF模块减少了特征中的噪声和不确定性，同时也抑制了零星的正响应，以确保输出一致。</p><p>  为了公平地评估模型的能力，我们遵循了IML-ViT中概述的评估方案。它只涉及使用CASIA2数据集进行训练，然后对其他6个数据集进行测试。重要的是，我们确保了训练数据集和测试数据集之间的零数据重叠，使其成为一个跨数据集的评估。实验结果表明，所提出的Prompt-IML有效地利用了预训练模型中的先验知识，优于以往的先进方法，并表现出更强的鲁棒性。我们的贡献可以概括为三个方面：</p><p>  我们的贡献可以总结为三个方面：</p><ul><li>我们引入了Prompt-IML来应对IML数据集的稀缺所带来的挑战。我们的方法通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性。</li><li>我们精心设计了一个即插即用的特性对齐和融合（FAF）模块，它可以无缝地集成到主干网中。它有效地减少了特征中的噪声和不确定性，同时减轻了零星的积极响应的影响。</li><li>Prompt-IML在6个测试数据集上都优于最先进的方法。我们的广泛的实验证实了我们的方法的普遍性和鲁棒性，也验证了所提出的FAF模块的有效性。</li></ul><h1 id="方法">2. 方法</h1><h2 id="方法概述">2.1 方法概述</h2><p>  图2展示了所提出的Prompt-IML的管道设计，它遵循了通用的编码器-解码器框架。完整的传递途径包括特征提取和操作定位两个阶段。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><p>  在特征提取阶段，我们采用预训练好的双子变压器作为骨干，并在训练过程中保持其参数冻结。同时，我们利用多组可调提示来分别调整图像的空间特征和高频特征。因此，这种方法避免了使用冗余的模型体系结构来从其他视图中提取特征。考虑到多视图特征的差异，我们提出了一个特征对齐和融合（FAF）模块进行处理。FAF模块集成在主干层之间，有效地降低了每层提取特征内的噪声和不确定性。同时，它们有助于抑制零星的积极反应，导致更一致的输出。这些模块都是即插即用的，不需要对主干本身进行任何修改。在操作定位阶段，我们使用掩模2前作为解码器，其中包括一个像素解码器和一个变压器解码器。解码器处理从前一阶段获得的多尺度特征，并产生最终的预测。</p><h2 id="特征提取阶段">2.2 特征提取阶段</h2><p>  我们将输入图像表示为 $~\mathbf{X}\in\mathbb{R}^{\boldsymbol{h}\times\boldsymbol{w}\times3} $。为了获取空间特征的输入，我们将图像划分为指定大小的斑块：</p><p><spanclass="math display">\[\mathrm{F}_{0}^{RGB}=\mathrm{Norm}(\mathrm{Conv}(\mathrm{X}))+\mathrm{F}_{PE},\]</span>  其中 $~ \mathbf{F}_0^{RGB}\in\mathbb{R}^{H\times W\times C} $，Conv表示分区操作， $~ \mathrm{F}_{PE} $是一个可学习的位置嵌入。接下来，我们使用一组具有不同大小的内核的BayarConv来提取高频特征：<spanclass="math display">\[\mathbf{F}_{0}^{HFQ}=\mathrm{Concat}(\{\mathrm{BayarConv}_{\mathrm{i\timesi}}(\mathbf{X})\}), i\in\{3,5,7\},\]</span>   其中 $~\mathbf{F}_0^{HFQ}\in\mathbb{R}^{H\times W\times C} $，i表示内核大小。所获得的特征将被发送到骨干网中以进行进一步的处理。</p><h3 id="具有可调调提示的多视角特征处理">2.2.1具有可调调提示的多视角特征处理</h3><p>  我们采用预先训练的语义分割（SS, semanticsegmentation）任务中常用的Swin-Transformer作为主干，原因如下：<br/><br/>   1)Swin-Transformer包括一个与图像大小相比具有线性时间复杂度的窗口注意设计；<br/><br/>   2)补丁合并操作可以生成多尺度特征图，这在IML任务[4,11]中被证明是重要的。<br/><br/>   3)SS任务和IML任务有一些相似之处，因为它们本质上是像素级的分类任务。<br/><br/>  我们认为，用于SS任务的预训练模型，经过微调后，更有利于实现精确的像素级操作定位。<br/><br/>  Swin-Transformer包括4层，并具有特定分辨率的输出特征。我们将第i层的输出特征表示为$ F_i $ ： <spanclass="math display">\[\mathbf{F}_{i}=\mathrm{Layer}_{\mathrm{i}}\left(\mathbf{F}_{i-1}\right)\in\mathbb{R}^{(H_{i}\timesW_{i})\times C_{i}},i\in\{1,2,3,4\},\]</span></p><p>  其中， $~H_{i}=\frac{H}{2^{i-1}},W_{i}=\frac{W}{2^{i-1}},C_{i}=C*2^{i-1} $ ， ${Layer}_i $ 象征着 Swin-Transformer的第i层。</p><p>  我们采用了一种提示调优方法[12]，使一个单一的预训练模型能够同时处理空间和高频特征。具体来说，在训练过程中，我们在每一层利用两组提示分别处理空间特征和高频特征，同时冻结主干的参数。我们将第i层的输入特征表示为$~ F^{RGB}_{i-1} $ 和 $~ F^{HFQ}_{i-1} $ 。它们先重塑为 $~\mathbb{R}^{(H_{i-1}\times W_{i-1})\times C_{i-1}} $ ，然后分别加入提示$~ P^{RGB}_{i-1} $ 和 $~ P^{HFQ}_{i-1}\in\mathbb{R}^{n_{p}\timesC_{i-1}} $ 。因此，公式3的每一层过程已被变更为： <spanclass="math display">\[\mathbf{F}_{i}^{RGB}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{RGB},\mathbf{F}_{i-1}^{RGB}\right]\right),\\\mathbf{F}_{i}^{HFQ}=\mathrm{Layer}_{i}\left(\left[\mathbf{P}_{i-1}^{HFQ},\mathbf{F}_{i-1}^{HFQ}\right]\right),\]</span>  其中，[·]表示Concat操作。</p><h3 id="特征对齐和融合模块">2.2.2 特征对齐和融合模块</h3><p>  针对骨干处理的空间和高频特征，我们提出了一个特征对齐和融合的FAF模块。FAF模块集成在主干网的一些相邻层之间，如图2所示。FAF模块由对齐阶段[36]和融合阶段组成，详细的组成和过程如图3所示。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826110222896.png"alt="image-20240826110222896" /><figcaption aria-hidden="true">image-20240826110222896</figcaption></figure><p>图3：所提出的特性对齐和融合（FAF）模块的设计和可调提示的使用。通道，空间，可变形分别表示等式5，等式6和等式9的过程。</p><h4 id="特征对齐阶段">特征对齐阶段</h4><p>  在特征对齐阶段，我们同时利用通道注意和空间注意来研究特征的通道间和空间间的相关性，从而利用相应的信息增强特征。未经处理的特征从增强的特征中收集信息，减少了潜在的不确定性和噪声。</p><p><img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824221130411.png" alt="image-20240824221130411"  /></p><p>  具体来说，我们首先使用平均池化操作（用上划线表示）来聚合信息。然后，将它们连接到$ C_i $ 的维数上，即[·]表示，并输入到MLP层以生成信道注意向量 $~\mathbf{W}_{i}^{C_{RGB}} $ ， $~\mathbf{W}_{i}^{C_{HFQ}}\in\mathbb{R}^{1\times1\times C_{i}} $。上述过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{C_{RGB}},\mathbf{W}_{i}^{C_{HFQ}}&amp;=\mathrm{ChannelAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}(\mathrm{MLP}([\overline{\mathbf{F}_{i}^{RGB}},\overline{\mathbf{F}_{i}^{HFQ}}])),\end{aligned}\]</span></p><p>  其中，Split是Concat的反向操作。为了获得空间注意向量，我们利用两个1×1卷积与一个中间的ReLU层，用$ g() $ 表示，来聚合空间信息。获取空间注意向量 $~\mathbf{W}_{i}^{S_{RGB}} $ ， $~\mathbf{W}_{i}^{S_{HFQ}}\in\mathbb{R}^{H_i\times W_i\times1} $的过程公式化如下： <spanclass="math display">\[\begin{aligned}\mathbf{W}_{i}^{S_{RGB}},\mathbf{W}_{i}^{S_{HFQ}}&amp;=\mathrm{SpatialAttn}\left(\mathbf{F}_{i}^{RGB},\mathbf{F}_{i}^{HFQ}\right)\\&amp;=\mathrm{Split}\left(\mathrm{Conv}\left(g\left(\mathrm{Conv}\left(\left[\mathrm{F}_{i}^{RGB},\mathrm{F}_{i}^{HFQ}\right]\right)\right)\right)\right).\end{aligned}\]</span>  最后，我们通过应用交叉注意向量对来自不同分支的特征进行对齐，通过元素级添加为下一个主干层产生输入：<spanclass="math display">\[\begin{aligned}&amp;\mathbf{F}_{i}^{C_{RGB}}=\mathbf{W}_{i}^{C_{RGB}}\odot\mathbf{F}_{i}^{RGB},\quad\mathbf{F}_{i}^{S_{RGB}}=\mathbf{W}_{i}^{S_{RGB}}\odot\mathbf{F}_{i}^{RGB},\\&amp;\mathbf{F}_{i}^{C_{HFQ}}=\mathbf{W}_{i}^{C_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\quad\mathbf{F}_{i}^{S_{HFQ}}=\mathbf{W}_{i}^{S_{HFQ}}\odot\mathbf{F}_{i}^{HFQ},\\&amp;\mathbf{F}_{i}^{RGB}:=\mathbf{F}_{i}^{RGB}+\mathbf{F}_{i}^{C_{HFQ}}+\mathbf{F}_{i}^{S_{HFQ}},\\&amp;\mathbf{F}_{i}^{HFQ}:=\mathbf{F}_{i}^{HFQ}+\mathbf{F}_{i}^{C_{RGB}}+\mathbf{F}_{i}^{S_{RGB}}.\end{aligned}\]</span>#### 特征融合阶段</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112404970.png"alt="image-20240826112404970" /><figcaption aria-hidden="true">image-20240826112404970</figcaption></figure><p>  在特征融合阶段，我们首先利用不同膨胀率的扩张卷积DConv来处理特征图，增强斑块内的相互作用。具体来说，我们使用膨胀速率k∈{1,3,5}进行处理，然后在维度of$ C_i $上连接输出。对这些连接起来的特征进行处理，以整合信息和未经处理的特征：<spanclass="math display">\[\tilde{\mathrm{F}}_{i}=\mathrm{Conv}\left(\left[\mathrm{Conv}\left(\left[\mathrm{DConv}_{\mathrm{k\timesk}}\left(\mathbf{F}_{i}\right)\right]\right),\mathbf{F}_{i}\right]\right),k\in\{1,3,5\}.\]</span></p><p>  然后，我们应用可变形注意力机制来促进多视图块间的信息交互。可变形注意机制不仅通过可学习偏移量采样降低了计算复杂度，还有助于抑制特征图中的零星积极反应，这有助于定位，因为篡改操作通常影响像素的特定区域，而不是孤立的特定区域[4]。从上一步的$ ~\mathbf{\tilde{F}}_{i}^{RGB} $ 和 $~ \mathbf{\tilde{F}}_{i}^{HFQ} $处理得到特征：</p><p><spanclass="math display">\[\mathbf{attn}^{RGB}=\mathrm{DeformAttn}_{1}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{RGB},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{HFQ}\right),\\\mathbf{attn}^{HFQ}=\mathrm{DeformAttn}_{2}\left(\mathrm{Q}=\tilde{\mathbf{F}}_{i}^{HFQ},\mathrm{K\&amp;V}=\tilde{\mathbf{F}}_{i}^{RGB}\right),\\\mathbf{F}_{i}^{d}=\gamma_{1}\cdot\left(\tilde{\mathbf{F}}_{i}^{RGB}+\mathbf{attn}^{RGB}\right)+\gamma_{2}\cdot\left(\tilde{\mathbf{F}}_{i}^{HFQ}+\mathbf{attn}^{HFQ}\right),\]</span></p><p>  其中， $~ \gamma_{1} $ ， $~ \gamma_{2} $ 是可学习的参数。输出的 $~F^d_i $ 用于解码器。</p><h2 id="篡改定位阶段">2.3 篡改定位阶段</h2><p>  为了细化上一阶段获得的多尺度特征，我们使用Mask2Former[3]作为解码器，它包括两个关键组件：像素解码器和Transformer解码器。</p><figure><imgsrc="./../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240826112712079.png"alt="image-20240826112712079" /><figcaption aria-hidden="true">image-20240826112712079</figcaption></figure><p>  像素解码器负责逐步向上采样特征从低分辨率到高分辨率。Transformer解码器利用查询嵌入和多尺度特性进行定位。这种方法有几个优点。首先，利用多尺度特征有利于定位小的篡改区域。此外，掩膜注意力的查询嵌入的整合有助于限制交叉注意对被篡改区域的单独关注，从而增强了与篡改相关的特征提取。</p><h2 id="损失函数">2.4 损失函数</h2><p>  考虑到被篡改区域的边界可能表现出锯齿状、非平滑的边缘和颜色不一致，我们从IML-ViT[22]中汲取灵感，并引入了边缘监督。具体来说，我们使用形态学操作，如侵蚀和膨胀操作，来处理掩模M并生成相应的边缘掩模$ M^★ $。与利用网络生成边缘预测[4]的方法相比，该策略不仅包含了边缘信息，同时还消除了调整骨干的需要，增强了其灵活性。损失函数包括两个分量，每个分量对应于对预测结果和预测边缘的监督：<spanclass="math display">\[\mathcal{L}=\mathcal{L}_{seg}(M_{gt},M_{pred})+\lambda\mathcal{L}_{edge}(M_{gt}^{\star},M_{pred}^{\star})\]</span></p><h1 id="实验">3 实验</h1><h2 id="实验设置">3.1 实验设置</h2><h3 id="数据集">数据集</h3><p>  我们采用了一个关于IML任务的通用训练协议[2,22,37]，以促进模型性能的公平比较，并避免了私有合成数据集的影响。我们仅使用CASIA2[5]来训练Prompt-IML。6个公共测试数据集用于评估，包括CASIA1 [5]、NIST16[7]、COVERAGE[32]、Columbia[25]、IMD2020[26]和DEFACTO[23]。在MVSS-Net[2]之后，我们对来DEFACTO的抽样子数据集进行了测试，其中包含6000张真实图像和6000张经过处理的图像。评估构成了跨数据集分析，因为我们的训练集和测试数据集之间没有重叠。</p><h3 id="评估标准">评估标准</h3><p>  我们使用像素级的F1分数来评估我们的模型在测试数据集上的性能。以往的一些方法采用了以最优阈值优化F1分数的策略，为每幅图像选择不同的阈值。然而，最优阈值的决定需要地面真实数据，这在现实场景中是不可行的。因此，我们以固定的阈值报告f1分数，它独立于模型本身，并提供了一个公平的模型性能评估。</p><h3 id="实施细节">实施细节</h3><p>  我们在RTX 3090GPU上训练我们的模型 Prompt-IML80轮次，每个GPU的批处理大小为2。编码器和解码器都在COCO[17]上使用预先训练的权值进行初始化。除非另有说明，所有图像的大小都会被调整为1024×1024。在IML-ViT[22]之后，我们使用了简单和公共的数据增强技术，包括翻转、模糊、旋转、JPEG压缩、随机复制移动和在单个图像中矩形区域进行图像修复。我们使用基本学习速率为1×10−4的AdamW[21]优化器，并利用余弦衰减策略来调度学习速率。</p><h2 id="性能比较">3.2 性能比较</h2><p>  我们将我们的方法与其他8种最先进的方法进行了比较，以全面评估我们的方法，并在表1中报告了F1评分。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224337339.png"alt="image-20240824224337339" /><figcaption aria-hidden="true">image-20240824224337339</figcaption></figure><p>  我们可以观察到，我们的方法对每个改进的数据集的最佳基线分别为2.8%、4.6%、7.6%、8.1%和4.9%。与次优基线IML-ViT[22]相比，它平均提高了4.3%。这些都充分证明了我们的模型的优越性。然而，在COVER[32]数据集上，基于MVSS-net的方法[2,4]的性能优于所有其他方法。COVER是一个仅通过复制-移动技术创建的小型伪造图像数据集，大多数检测线索位于伪造区域的边界周围。因此，我们将这种现象归因于他们精心设计的边缘信息提取结构和数据增强技术。</p><p>  此外，图4显示了每个模型的预测定位结果，每个图像来自一个不同的数据集，在操纵区域有很大的变化。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224529342.png"alt="image-20240824224529342" /><figcaption aria-hidden="true">image-20240824224529342</figcaption></figure><p>  研究结果强调了该方法具有显著的泛化能力，表明该方法可以有效地利用嵌入在预训练模型中的先验知识来检测篡改痕迹。</p><h2 id="鲁棒性">3.3 鲁棒性</h2><p>  在本节中，我们利用6个测试数据集来全面评估Prompt-IML的鲁棒性。在IML-ViT[22]之后，我们应用两种常见的攻击方法，即JPEG压缩和高斯模糊，在不同的扰动级别上，来创建被攻击的图像。计算结果如图5所示。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824224656206.png"alt="image-20240824224656206" /><figcaption aria-hidden="true">image-20240824224656206</figcaption></figure><p>  在JPEG压缩测试中，提出的pt-IML在4个数据集上保持明显的优势。在COVER和NIST16上，我们的方法与领先的方法非常接近。在高斯模糊测试中，Prompt-IML在所有数据集上都显著优于其他方法。</p><p>  总的来说，与其他方法相比，Prompt-IML表现出了承受JPEG压缩和高斯模糊的显著能力，特别是针对后者。我们还注意到，IML-ViT比其他方法表现出更好的平均鲁棒性，因此我们将我们的方法的鲁棒性归因于更有效地利用大规模的预训练模型，因为这些模型可以学习更鲁棒的特征，因为它们存在于广泛的训练数据集。</p><p>  值得注意的是，与IML-ViT相比，该方法在抵抗高斯模糊攻击方面有了显著的性能提高。我们认为，这些优势源于高频特性和促进调优的使用，从而导致了以下推测。首先，IML-ViT对预先训练好的网络进行了完全的微调，这可能会由于灾难性遗忘[30]而损害其鲁棒性。此外，不同特性对各种攻击的抵抗力也各不相同，因此充分利用多视图特性可能有助于提高该方法的鲁棒性。</p><h2 id="消融研究">3.4 消融研究</h2><p>  我们按照表2中概述的设置进行了几个实验，以彻底评估我们的方法中各模块的有效性。我们报告了每个模型在COVER[32]、NIST16[7]和IMD20 [26]上的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225020324.png"alt="image-20240824225020324" /><figcaption aria-hidden="true">image-20240824225020324</figcaption></figure><h3 id="多视图特征的影响">多视图特征的影响</h3><p>  在设置2中，我们使用一个具有冻结参数的单一主干，同时从图像中提取空间和高频特征。与仅使用空间特征的设置1相比，我们观察到高频特征的利用率分别增加了2.5%、1.9%和2.7%的F1分数，有效地证明了利用预先训练好的模型来处理多视图特征的可行性。</p><h3 id="faf模块的影响">FAF模块的影响</h3><p>  所提出的FAF模块包括两个独立的阶段：对齐和融合。因此，我们使用设置3和设置4分别来验证每个阶段的有效性。在设置4中，我们跳过特征对齐阶段，并直接将特征传递到下一层。与设置5相比，我们注意到，当没有特征对齐阶段缺失时，所有三个数据集的F1得分都下降，分别下降了3.6%、2.9%和3.0%。在设置3中，我们跳过特征融合阶段，直接添加多视图特征作为融合特征。与设置5相比，特征融合阶段的缺失导致F1评分分别下降了3.0%、3.1%和7.1%。这些结果有效地证明了FAF模块通过特征之间的信息交互成功地增强了特征。</p><h2 id="预训练主干网络的选择">3.5 预训练主干网络的选择</h2><p>  我们研究了选择不同的预训练模型作为骨干的影响。我们使用CLIP[28]，MAE[9]，SAM[13]和Swin-Transformer[20]。CLIP和MAE都采用了普通ViT的架构，而SAM则类似于Swin-Transformer。考虑到全局自注意机制的计算需求，特别是对大图像，我们将所有图像的大小调整为512×512来进行比较。此外，由于普通ViT输出的特征图大小固定，我们在FAF模块的每个融合阶段结束时合并了几个卷积，以与解码器的输入需求对齐。我们在表3中报告了不同主干网络的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225456518.png"alt="image-20240824225456518" /><figcaption aria-hidden="true">image-20240824225456518</figcaption></figure><p>  在COCO[17]数据集上训练的语义分割任务的Swin-Transformer模型，提供了优越的结果。我们将这种成功主要归因于它在不同层的不同接受域，使它能够发现微妙的篡改痕迹。虽然CLIP是在一个大的数据集上进行预先训练的，但它强调文本和图像特征之间的对齐，因此单独使用图像编码器可能不是最佳的选择。此外，我们假设在SAM中的窗口注意机制的实现可能会限制其在低分辨率图像上的性能。因此，我们选择预先训练过的Swin-Transformer作为我们的主干网络。</p><h2 id="提示调优v.s.完全调优">3.6 提示调优v.s.完全调优</h2><p>  我们比较了两种方法，提示调优和完全调优，用于将预先训练好的模型适应IML任务，并评估它们对模型性能的影响。当使用完全调优方法时，作为单一主干网络在同时处理空间和高频特征方面面临限制，我们按照[2,14]的指导方针将主干调整为双分支架构。表4显示了不同调优方法的F1分数。</p><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824225735253.png"alt="image-20240824225735253" /><figcaption aria-hidden="true">image-20240824225735253</figcaption></figure><p>  完全调优并不会在大多数数据集上带来显著的性能提高，但在COVER数据集上显示出了19.1%的显著改进。我们将这种异常现象归因于COVER数据集的小规模及其使用篡改技术的单一。尽管完全调优显示出一定程度的性能改进，但提示调优相比，双分支结构引入了更多的可训练参数。为了进行更直接的比较，我们不计算FAF模块和解码器的可学习参数，因为它们都包含在这两种方法中。对于提示调优的可学习参数的大小为0.09M，而对于完全调优的双分支主干的大小为93.14M。因此，快速调优更有利于适应大型模型的开发和处理多视图特性。</p><h1 id="结论">4 结论</h1><p>  在本文中，我们探讨了利用现有的预训练模型来解决IML任务中公共可用数据集的稀缺性的潜力。我们提出了Prompt-IML，它利用单一的预先训练的网络通过可调提示提取多视图特征。采用专门设计的特征对齐融合（FAF）模块集成多视图特征，有效降低了特征的噪声和不确定性，抑制了零星的积极响应。在6个测试数据集上进行的大量实验表明，Prompt-IML具有优异的性能、更好的泛化能力和更高的鲁棒性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DH-GAN</title>
      <link href="/DH-GAN/"/>
      <url>/DH-GAN/</url>
      
        <content type="html"><![CDATA[<p>DH-GAN: Image manipulation localization via a dual homology-awaregenerative adversarial network<br/><spanclass="math inline">\(\text{Weihuang Liu}^1,\text{XiaodongCun}^1,\text{Chi-Man Pun}^*\)</span><br/>澳门大学科技学院计算机与信息科学系，澳门凼仔</p><h1 id="摘要">摘要</h1><p>图像操作定位是一种二值分割任务，对被篡改的伪影敏感，而不是物体的对象。因此，传统的方法和基于学习的方法都高度依赖于手工制作的特征。然而，这些特定定义的特征限制了网络对一般场景的能力。</p><p>为了解决这一问题，我们提出了一个双同源感知生成对抗网络（DH-GAN, dualhomology-aware generative adversarialnetwork），这是一种新的基于gan的框架来定位被操纵的区域。首先，我们通过使用选择性金字塔生成器重新校准多尺度编码特征来定位伪造区域。然后，我们在鉴别器中进行同源性识别。所提出的同源识别鉴别器包含一堆掩码卷积（MConv,maskedconvolution）层，并学习以硬门控的方式识别预测/目标掩蔽图像上的分割像素的真实/虚假。总的来说，这些网络是在一个标准的GAN下进行优化的。实验表明，该方法在四种流行的图像处理数据集上都优于其他最先进的算法。</p><p>我们的主要贡献总结如下：</p><ul><li>我们提出了一种双同源感知生成对抗网络（DH-GAN），这是一种新的基于gan的框架，用于由两个同源感知的鉴别器进行图像操作定位。</li><li>在生成器中，我们遵循编码器-解码器结构，设计了选择性金字塔（SAP），它使用融合的多阶段动作机制来选择和重新校准多尺度特征。</li><li>在每个同源感知鉴别器中，我们提出了一个基于mconv的网络，利用预测掩模与地面真实掩模来识别分割像素的同源性。</li><li>我们的方法在几个图像伪造检测基准上取得了最先进的性能。</li></ul><figure><img src="../postimages/DH-GAN/image-20240824153543322.png"alt="image-20240824153543322" /><figcaption aria-hidden="true">image-20240824153543322</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153751891.png"alt="image-20240824153751891" /><figcaption aria-hidden="true">image-20240824153751891</figcaption></figure><figure><img src="./../postimages/DH-GAN/image-20240824153820459.png"alt="image-20240824153820459" /><figcaption aria-hidden="true">image-20240824153820459</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>IAD</title>
      <link href="/IAD/"/>
      <url>/IAD/</url>
      
        <content type="html"><![CDATA[<h1 id="cv-surveys">2024-CV-Surveys</h1><h2 id="industrial-anomaly-detection工业缺陷检测">Industrial AnomalyDetection(工业缺陷检测)</h2><ul><li><a href="https://arxiv.org/abs/2401.15448">A Systematic Review ofAvailable Datasets in Additive Manufacturing</a><br/> [2024-01-30]</li><li><a href="https://arxiv.org/abs/2406.07880">A Comprehensive Survey onMachine Learning Driven Material Defect Detection: Challenges,Solutions, and Future Prospects</a><br/> [2024-06-13]</li><li><a href="https://arxiv.org/abs/2406.07694">A PRISMA DrivenSystematic Review of Publicly Available Datasets for Benchmark and ModelDevelopments for Industrial Defect Detection</a><br/> [2024-06-13]</li><li>VAD<br/> - <a href="https://arxiv.org/abs/2401.16402">A Survey onVisual Anomaly Detection: Challenge, Approach, and Prospect</a><br/>[2024-01-30]</li><li>点云的工业系统 3D 缺陷检测和分类<br/> - <ahref="https://arxiv.org/abs/2402.12923">Advancements in PointCloud-Based 3D Defect Detection and Classification for IndustrialSystems: A Comprehensive Survey</a><br/> [2024-02-21]</li></ul><h1 id="cv-surveys-1">2023-CV-Surveys</h1><h2 id="anomaly-detection">Anomaly Detection</h2><ul><li><a href="https://arxiv.org/abs/2301.11514">Deep Industrial ImageAnomaly Detection: A Survey</a><br/> [2023-01-30]<br/> ⭐<ahref="https://github.com/M-3LAB/awesome-industrial-anomaly-detection">code</a></li></ul><h1 id="cv-surveys-2">2022-CV-Surveys</h1><h2 id="工业异常检测">工业异常检测</h2><ul><li><a href="https://arxiv.org/abs/2204.11161">A Survey on UnsupervisedIndustrial Anomaly Detection Algorithms</a><br/> [2022-04-26]</li></ul><h1id="promptad-learning-prompts-with-only-normal-samples-for-few-shot-anomaly-detection">PromptAD:Learning Prompts with only Normal Samples for Few-Shot AnomalyDetection</h1><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h2 id="摘要">摘要</h2><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h2 id="引言">引言</h2><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/IAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h2 id="前期准备工作">前期准备工作</h2><h3 id="clip和提示学习">CLIP和提示学习</h3><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h3 id="clip-surgery">CLIP Surgery</h3><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h2 id="方法论">方法论</h2><h3 id="概观">概观</h3><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><img src="./../postimages/IAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h3 id="语义连接">语义连接</h3><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h2 id="显式异常边缘">显式异常边缘</h2><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h3 id="异常检测">异常检测</h3><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h2 id="实验">实验</h2><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h3 id="数据集">数据集</h3><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h3 id="评估指标">评估指标</h3><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h3 id="实施细节">实施细节</h3><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p><h1 id="open-set-supervised-anomaly-detection">Open-set SupervisedAnomaly Detection</h1><p>Anomaly Heterogeneity Learning for Open-set Supervised AnomalyDetection</p><figure><img src="./../postimages/IAD/640.png" alt="图片" /><figcaption aria-hidden="true">图片</figcaption></figure><p>-这篇文章探讨了一种新兴的异常检测领域——开放集监督异常检测（Open-setSupervised Anomaly Detection,OSAD）。OSAD的目标是利用训练过程中所见异常类别的少量样本，来检测未见过的异常（即来自开放集异常类别的样本），同时有效地识别已见过的异常。现有的OSAD方法虽然能够通过所见异常的先验知识大幅减少误报错误，但它们通常在封闭集设置下训练，并且将异常样本视为来自同质分布，这限制了它们对来自任何分布的未见过异常的泛化能力。</p><p>-为了解决这一问题，文章提出了一种名为异常异质性学习（AnomalyHeterogeneity Learning,AHL）的新方法。AHL通过模拟多样化的异常分布，并利用这些分布来学习统一的异常模型，以在替代的开放集环境中进行学习。AHL是一个通用框架，现有的OSAD模型可以轻松地插入并使用，以增强它们的异常建模能力。通过在九个真实世界的异常检测数据集上的广泛实验，AHL不仅在检测已见和未见过的异常方面显著增强了不同的最新OSAD模型，而且能够有效地泛化到新领域中的未见过异常。</p><p>-文章首先介绍了异常检测的背景和挑战，然后详细阐述了AHL的框架和方法。AHL包括两个主要组件：异质异常分布生成（HeterogeneousAnomaly Distribution Generation,HADG）和异常异质性的协同可微学习（Collaborative Differentiable Learning,CDL）。HADG组件通过将正常样本的不同簇与随机选择的异常样本相结合，模拟并生成了多样化的异常分布数据集。CDL组件则设计为使用T个基础模型学习这些异常分布，并通过迭代验证和调整模型来优化统一的异常检测模型。</p><p>-此外，文章还提出了一种自监督的泛化性估计方法，以适应性地调整模型训练过程中每个学习到的异常分布的重要性。通过这种方式，AHL能够动态地评估基础模型的泛化能力，并据此调整它们在统一模型更新中的权重。</p><p>-在实验部分，作者展示了AHL在多个数据集上的性能，并与其他最新技术进行了比较。结果表明，AHL在检测同一领域和跨领域设置中的未见过异常方面，都取得了显著的性能提升。文章最后对AHL进行了深入的分析，包括对AHL组件的效用、少数样本的实用性以及超参数的敏感性进行了研究，并得出了有价值的结论。</p><p>-总的来说，这篇文章为开放集监督异常检测领域提供了一种新的视角和强大的工具，通过学习异常的异质性，显著提高了模型对未知异常的检测能力和泛化性。</p><h3 id="摘要-1">摘要</h3><p>开放集监督异常检测（OSAD）是一个最近出现的异常检测领域，其目的是利用训练过程中看到的一些异常类的样本来检测不可见的异常（即来自开放集异常类的样本），同时有效地识别可见的异常。得益于所见异常所说明的先验知识，目前的OSAD方法往往可以大大减少假阳性误差。然而，这些方法是在封闭集设置中训练的，并将异常例子视为齐次分布，使得它们在推广到可以从任何分布中得出的看不见的异常时效果较差。本文提出利用有限异常实例学习异质异常分布来解决这一问题。为此，我们引入了一种新的方法，即异常异质性学习（AHL），它模拟了一组不同的异构异常分布，然后利用它们在替代开放集环境中学习一个统一的异构异常模型。此外，AHL是一个通用的框架，现有的OSAD模型可以即插即用，以增强其异常建模。在9个真实世界异常检测数据集上的广泛实验表明，AHL可以1)显著增强不同的最先进的OSAD模型来检测可见和不可见的异常，2)有效地推广到新领域的不可见异常。</p><h3 id="引言-1">引言</h3><p>开放集监督AD（OSAD）是一个新兴的领域，旨在利用这些有限的训练异常数据学习广义模型来检测看不见的异常（即来自开放集异常类的样本），同时有效地识别那些可见的异常（即类似于训练异常例子的异常）。针对这个OSAD问题[1,15,24,32,68]，已经引入了许多方法。得益于由所看到的异常情况所说明的先验知识，当前的OSAD通常可以极大地减少假阳性误差。</p><figure><img src="./../postimages/IAD/QQ_1721787242975.png"alt="QQ_1721787242975" /><figcaption aria-hidden="true">QQ_1721787242975</figcaption></figure><p>目前的OSAD方法的一个问题是，它们将异常例子视为均匀分布，如图1(a)所示，这在很大程度上限制了它们在检测看不见异常方面的性能。这是因为异常可以由广泛的条件产生，并且天生是无界的，从而导致非均匀的异常分布（即，异常可以从非常不同的分布中得出)。例如，肿瘤图像可以根据肿瘤的性质，在外观、形状、大小、位置等方面显示出不同的特征。目前的OSAD方法忽略了这些异常的异质性，如果它们来自于与所看到的异常不同的数据分布，则往往无法检测到异常。</p><p>为了解决这个问题，我们建议用有限的训练异常例子来学习异构异常分布。这些异常只是可见异常类的例子，它们并不能说明所有可能的异常类的分布，例如，那些看不见的异常类，这使得在有限的异常信息下学习潜在的异构异常分布具有挑战性。这项工作引入了一个新的框架，即异常异质性学习（AHL），来解决这一挑战。如图1(b)所示，它首先通过将正态样本的细粒度分布与随机选择的异常样本关联起来，来模拟各种非均匀异常分布。然后AHL执行协作可微学习，综合所有这些异常分布，以学习异构异常模型。进一步，生成的异常数据使我们的模型的训练代理开放环境中，其中异常分布的一部分用于模型训练而其他作为看不见的数据来验证和调整模型，导致更好的广义模型比当前方法训练在一个封闭的设置。此外，模拟的异常分布通常具有不同的质量。因此，在AHL中设计了一种自监督泛化估计，以自适应地调整模型训练过程中每个学习到的异常分布的重要性。</p><p>AHL的另一种简单的替代方法是，在模拟的异构数据分布上，基于同构/异构OSAD模型的简单集成来建立一个集成模型。然而，这样的集合没有考虑到在基础模型中捕获的异常异质性的共性和差异，导致了对异质性的次优学习(Sec。4.5.2).</p><p>因此，本文做出了四个主要贡献。</p><p><strong>框架。</strong>我们提出了异常异质性学习（AHL，AnomalyHeterogeneityLearning），一个新的OSAD框架。与目前将训练异常例子视为均匀分布的方法不同，AHL通过这些有限的例子来学习异构异常分布，从而能够对不可见的异常进行更广义的检测。</p><p><strong>新的模型。</strong>我们进一步将AHL框架实例化为一个新的OSAD模型。该模型使用一组不同的模拟异构异常分布对异常异质性进行协同可微学习，促进了在替代开放集环境中对模型的迭代验证和调优。这使得比简单的集成方法更最优的异常异质性学习。</p><p><strong>通用的。</strong>我们的模型是通用的，其中来自不同OSAD模型的特性和损失函数可以即插即用，并获得显著提高的检测性能。</p><p><strong>具有较强的泛化能力。</strong>在9个真实世界的AD数据集上进行的实验表明，AHL在检测同域和跨域设置中看不见的异常方面大大优于最先进的模型。</p><h3 id="异常异质性学习">异常异质性学习</h3><h4 id="问题陈述">问题陈述：</h4><p>我们假设有一组训练图像和注释<spanclass="math inline">\(\left\{\omega_i,y_i\right\}_{i=1}\)</span>，其中$<em>i^{HWC}<span class="math inline">\(表示图像RGB通道和\)</span>y</em>{i}{0,1}<span class="math inline">\(表示一个图像级类标签，\)</span>y_{i}=1<spanclass="math inline">\(时\)</span><em>{i}<spanclass="math inline">\(为异常，\)</span>y</em>{i}=0<spanclass="math inline">\(时相反。由于异常的粗糙性，标记数据通常主要由正常数据表示。给定现有的AD模型f（·），可以用来提取低维图像特征来构造训练特征集\)</span>={_i,y_i}<spanclass="math inline">\(，其中\)</span>_i<sub>=</sub>f(_i)<sub></sub><spanclass="math inline">\(表示对应的第i个图像特征，\)</span>_n ={_1,_2,...,_N}<span class="math inline">\(和\)</span>_a = {_1,_2,...,_M}(N M)<spanclass="math inline">\(分别表示正常和异常图像的特征集，那么我们提出的AHL框架的目标是学习一个异常检测函数\)</span>g:<spanclass="math inline">\(，它能够为来自不同分布的异常图像分配更高的异常分数。请注意，在OSAD中，训练异常Xa来自于可见的异常类S，它只是C的一个子集，在推理过程中可以包含一个更大的异常类集，例如，\)</span>$。</p><h4 id="我们的方法概述">我们的方法概述</h4><p>我们的AHL框架的关键思想是通过对嵌入在不同模拟异常分布中的异常的协作可微学习，来学习一个统一的异常异质性模型。</p><figure><img src="./../postimages/IAD/QQ_1721788581713.png"alt="QQ_1721788581713" /><figcaption aria-hidden="true">QQ_1721788581713</figcaption></figure><p>图2。我们的方法AHL的概述。它的HADG组件首先从训练集<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中生成T个异构异常分布数据集，每个训练集都包含一个支持集和开放集查询集，即<spanclass="math inline">\(\mathcal{D}_i=\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>。然后利用它们在模拟的开放集环境中学习T个异构AD模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，并通过协作微分学习（CDL）将这些异构异常模型合成为一个统一的AD模型g（·）。不同的ϕi学习不同质量的异常分布，因此我们还设计了一个模型ψ（·），为每个ϕi分配一个重要性评分，以增强CDL成分。</p><p>如图2所示，AHL由两个主要组成部分组成：非均匀异常分布产生（HADG,Heterogeneous Anomaly DistributionGeneration）和异常异质性的协同可微分学习（CDL, CollaborativeDifferentiable Learning）。</p><p>具体来说，HADG组件从训练集<span class="math inline">\(\mathcal{T} =\{\mathcal{D}_i\}_{i=1}^T\)</span>模拟并生成T个异构分布数据集，每个Di包含正态数据子集和随机采样异常例子的混合。每个Di都是以一种代表不同于其他异常分布的方式生成的。然后设计CDL学习一个统一的异构异常检测模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>，该模型合成了一组T基模型，记为<spanclass="math inline">\(\left\{\phi_i\big(\mathcal{D}_i;\theta_i\big)\right\}_{i=1}^T\)</span>，其中<spanclass="math inline">\(\theta_{g}\)</span>和<spanclass="math inline">\(\theta_{i}\)</span>分别表示统一模型g和基模型<spanclass="math inline">\(\phi_{i}\)</span>的可学习权值参数，每个<spanclass="math inline">\(\phi_i:\mathcal{D}_i\to\mathbb{R}\)</span>从一个异常分布中学习进行异常评分。权重参数<spanclass="math inline">\(\theta_{g}\)</span>基于基础模型权重<spanclass="math inline">\(\{\theta_i\}_{i=1}^T\)</span>协同更新。此外，单个基模型的有效性差异很大，因此如果估计相应的基模型ϕi具有较小的泛化误差，则在CDL中添加一个模块ψ，以增加θi在协同权重更新中的重要性。在推理过程中，仅使用统一的异构异常模型<spanclass="math inline">\(g\big(\mathcal{T};\theta_g\big)\)</span>进行异常检测。</p><p>AHL是一个通用的框架，其中可以轻松地插入现成的OSAD模型来实例化ϕi，并获得显著提高的性能。</p><h4 id="非均匀异常分布产生hadg">非均匀异常分布产生HADG</h4><p>学习潜在的复杂异常的一个主要挑战是缺乏说明不同可能的异常分布的训练数据。我们的HADG组件是为了解决这一挑战，我们将正常范例划分为不同的簇，并将每个正常范例与随机抽样的异常示例关联起来，以创建不同的异常分布。由此产生的分布在正常模式和/或异常模式方面彼此不同。具体来说，HADG生成T个训练异常分布数据集，<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>，每个<spanclass="math inline">\(\mathcal{D}_i = \mathcal{X}_{n,i} \cup\mathcal{X}_{a,i}\)</span>，其中<spanclass="math inline">\(\mathcal{X}_{n,i}\subset\mathcal{X}_n\)</span>和<spanclass="math inline">\(\mathcal{X}_{a,i}\subset\mathcal{X}_a\)</span>。为了模拟高质量的异常分布，我应该代表一个主要的正态模式。为此，HADG采用聚类方法将Xn划分为C聚类，然后随机抽取这些C正常聚类中的一个为<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>。另一方面，为了保证每个Di、Xa中异常的多样性，我们从Xa和常用的异常生成方法[22,60,63]生成的伪异常中随机提取了<spanclass="math inline">\(\mathcal{X}_{a,i}\)</span>。</p><p>此外，HADG利用这些训练数据来创建开放集的检测和验证数据集，以便在代理OSAD环境中对我们的模型进行训练。特别是，对于每个Di，HADG将它分成两个不相交的子集，即<spanclass="math inline">\(\mathcal{D}_i =\{\mathcal{D}_i^s,\mathcal{D}_i^q\}\)</span>，分别对应支持集和查询集，支持集<spanclass="math inline">\(\mathcal{D}_i^s=\mathcal{X}_{n,i}^s\cup\mathcal{X}_{a,i}^s\)</span>用来训练我们的基本模型ϕi，查询集<spanclass="math inline">\(\mathcal{D}_i^q=\mathcal{X}_{n,i}^q\cup\mathcal{X}_{a,i}^q\)</span>用于验证其开放集性能。保证开放的验证/查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>，我们执行抽样的方式，以确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{n},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>是两个不同的正常集群，同时确保<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{s}}\)</span>和<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}^{\boldsymbol{q}}\)</span>不相互重叠，例如，$<em>{a,i}^s</em>{a,i}^q=$。</p><h4id="异常异质性的协同可微分学习cdl">异常异质性的协同可微分学习CDL</h4><p>我们的CDL组件的目标是首先使用T个基模型ϕi学习隐藏在<spanclass="math inline">\(\mathcal{T}=\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异构异常分布，然后利用这些模型以端到的方式协同优化统一检测模型g。CDL的详细介绍如下。</p><h5 id="学习t个异构异常分布">学习T个异构异常分布。</h5><p>我们首先训练T个基模型<spanclass="math inline">\(\{\phi_i\}_{i=1}^T\)</span>，分别捕获<spanclass="math inline">\(\{\mathcal{D}_i\}_{i=1}^T\)</span>中的异质异常分布，每个ϕi使用以下损失进行优化：<spanclass="math display">\[\mathcal{L}_{\phi_i}=\sum_{j=1}^{|\mathcal{D}_i^s|}\ell_{dev}\left(\phi_i(\mathbf{x}_j;\theta_i),y_j\right),\]</span>其中<spanclass="math inline">\(\ell_{dev}\)</span>由偏差损失[32]指定，遵循之前的OSAD方法DRA[15]和DevNet[32]，<spanclass="math inline">\(\mathcal{D}_i^s\)</span>是<spanclass="math inline">\(\mathcal{D}_i\)</span>中的支持集。虽然在训练阶段只有有限的可见异常，但每个Di中的正常样本和异常样本的混合差异很大，使得每个ϕi可以学习不同的异常评分的异常分布。</p><h5 id="协作性可微分学习">协作性可微分学习。</h5><p>每个ϕi只捕获了潜在的异常异质性的全貌的一部分。因此，我们然后执行一个协作的可微学习，利用来自T个基模型的损失来学习统一的AD模型g，以捕获更丰富的异常异质性。关键的见解是，g经过了优化，可以很好地处理各种可能的异常分布，减轻对特定异常分布的潜在过拟合。此外，g的优化是基于在等式1中训练基本模型时没有看到的查询集上的损失，即在一个代理开放环境下进行优化，这有助于训练一个更广义的OSAD模型g。具体来说，g被指定为与基于模型ϕi具有完全相同的网络架构，其在t+ 1阶段的权重参数θg根据所有基础模型在t阶段的损失进行优化： <spanclass="math display">\[\theta_g^t\longleftarrow\theta_g^{t\boldsymbol{-}1}-\alpha\nabla\mathcal{L}_{cdl},\]</span>其中，α是一个学习速率，<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>是对查询集上的T个基模型的聚合损失：<spanclass="math display">\[\mathcal{L}_{cdl}=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right).\]</span>在下一个训练阶段，所有基础模型的<spanclass="math inline">\(\theta_i^{t+1}\)</span>设置为<spanclass="math inline">\(\theta_g^t\)</span>作为新的权重参数。然后，我们使用等式1优化基本模型ϕi，然后使用等式2在查询集上优化统一模型g。这种替代基础模型和统一模型学习用于获得日益捕获更丰富的异常异质性。</p><h5id="学习个体异常分布的重要性得分">学习个体异常分布的重要性得分。</h5><p>模拟异常分布数据Di的质量变化很大，导致基本模型的有效性存在较大差异。此外，在一个轮次效率较低的基础模型可以在另一个轮次变得更有效。因此，在整个优化动态过程中，平均考虑每一个基本模型可能会因为性能不佳的基础模型会影响统一模型g的整体性能从而导致劣等优化。为了解决这个问题，我们提出了一个自监督顺序建模模块来动态估计每个基模型在每个轮次的重要性。这就细化了<spanclass="math inline">\(\mathcal{L}_{cdl}\)</span>的损失如下： <spanclass="math display">\[\mathcal{L}_{cdl}^+=\sum_{i=1}^T\sum_{j=1}^{|\mathcal{D}_i^q|}w_i^t\mathcal{L}_{\phi_i}\left(\phi_i(\mathbf{x}_j;\theta_i^t),y_j\right),\]</span>其中，<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>表示其基模型ϕi在t轮次的重要性得分。下面我们将介绍我们是如何通过ψ来学习<spanclass="math inline">\(\boldsymbol{w}_i^t\)</span>的。</p><p>我们顺序建模的基于动态重要性分数的估计是建立在直觉，如果一个基础模型ϕi有良好的泛化能力，其预测异常分数为不同的输入数据应该一致和准确的在不同的训练阶段，各种异常异质性逐渐出现随着训练的展开。为此，我们训练了一个序列模型ψ来捕获所有基本模型产生的异常分数的一致性和准确性。这是通过训练ψ使用基础模型之前的输出异常分数来预测它们的下一个轮次的异常分数来实现的。具体来说，给定一个训练样本xj和利用基础模型<spanclass="math inline">\(\left\{\phi_i\right\}_{i=1}^T\)</span>得到的一组异常评分预测<spanclass="math inline">\(\mathbf{s}_j=\begin{Bmatrix}s_{ji}\end{Bmatrix}_{i=1}^T\)</span>，结果在轮次t之前产生了一系列的分数预测，<spanclass="math inline">\(\mathbf{S}_j^t =[\mathbf{s}_j^{t-K},\cdots,\mathbf{s}_j^{t-2},\mathbf{s}_j^{t-1}]\)</span>记录到K个之前的步骤，然后<spanclass="math inline">\(\psi:\mathbf{S}\to\mathbb{R}^T\)</span>旨在预测所有T个基础模型在轮次t的预测得分。在我们的实现中，ψ由一个由θψ参数化的序列神经网络指定，并使用以下下一个序列预测损失进行优化：<spanclass="math display">\[\mathcal{L}_{seq}=\sum_{\mathbf{x}_j\in\mathcal{D}}\mathcal{L}_{mse}(\hat{\mathbf{s}}_j^t,\mathbf{s}_j^t),\]</span>其中，<spanclass="math inline">\(\hat{\mathbf{s}}_j^t=\psi(\mathbf{S}_j^t;\theta_\psi)\)</span>和<spanclass="math inline">\(\mathbf{s}_j^t\)</span>分别为在轮次t的基模型中xj的预测和实际异常得分，<spanclass="math inline">\(\mathcal{L}_{seq}\)</span>为均方误差函数。模型ψ不是使用监督损失，而是使用等式5中的自监督损失函数进行训练，以保留groundtruth标签，避免对标记数据的过拟合，有效地评价基础模型的泛化能力。</p><p>然后利用预测的异常得分<span class="math inline">\(\hat{s}_{ji}^t\)</span>与真实标签<spanclass="math inline">\(y_{j}\)</span>之间的差值来定义基本模型ϕi的泛化误差<spanclass="math inline">\(r_i^t\)</span>，如下： <spanclass="math display">\[r_i^t=\frac{1}{|\mathcal{D}^{\prime}|}\sum_{\mathbf{x}_j\in\mathcal{D}^{\prime}}c_j\mathcal{L}_{mse}(\hat{s}_{ji}^t,y_j),\]</span>其中，<spanclass="math inline">\(\mathcal{D}^{\prime}=\mathcal{D}\setminus\mathcal{X}_{n,i}\)</span>和<spanclass="math inline">\(c_{j}\)</span>是与每个范例<spanclass="math inline">\(x_{j}\)</span>关联的预定义的类别权重。换句话说，<spanclass="math inline">\(r_i^t\)</span>测量ϕi来预测<spanclass="math inline">\(\mathcal{X}_{\boldsymbol{a},\boldsymbol{i}}\)</span>和所有其他未看到的正常和异常训练例子中可见异常的异常得分时的检测误差，不包括已看到的正常例子<spanclass="math inline">\(\mathcal{X}_{n,i}\)</span>（与ϕi相关）。如果xj是一个看不见的异常，则分配一个较大的cj，以突出检测看不见的异常的重要性；否则，将为其他例子分配相同的值。</p><p>由于较大的<spanclass="math inline">\(r_i^t\)</span>意味着基模型ϕi在轮次t时的泛化能力较差，因此在更新统一模型g时应较少注意它。因此，将ϕi的重要性得分定义为其泛化误差的倒数如下：<spanclass="math display">\[w_i^t=\frac{\exp(-r_i^t)}{\sum_i^T\exp(-r_i^t)}.\]</span></p><h3 id="实验-1">实验</h3><h4 id="实验设置">实验设置</h4><h5 id="数据集-1">数据集</h5><p>在之前的OSAD研究[15,32]之后，我们对9个真实世界的异常检测数据集进行了广泛的实验，包括5个工业缺陷检测数据集MVTecAD [5]，AITEX [42]，SDD [44]，ELPV[13]和光学[50]，一个行星探测数据集（Mastcam [20]）和3个医疗数据集HeadCT[40]，BrainMRI [40]和Hyper-Kvasir[7]。根据我们如何对所看到的异常示例进行采样，我们使用两种协议来评估检测性能，一般设置和硬设置[15]。一般设置假设异常例子是从异常类中随机抽样的，而硬设置提出了一个更具挑战性的情况，即异常例子只从一个类中抽样，以评估对新的或看不见的异常类的泛化能力。与[15]一样，我们还将异常例子的数量分别设置为M= 10和M = 1来评估性能。关于这些数据集的更多细节请见附录A。</p><h5 id="比较的方法和评价指标">比较的方法和评价指标</h5><p>将AHL与五种密切相关的最先进的（SOTA）方法进行了比较，包括MLEP[24]、SAOE [22,30,45]、FLOS [23]、DevNet [32]和DRA[15]。MLEP、DevNet和DRA都是专门为OSAD而设计的。SAOE是一种增强了合成异常和异常值曝光的监督检测器，而FLOS是一种基于焦点损失的不平衡分类器。对于评价指标，我们采用广泛使用的ROC曲线下面积（AUC）来衡量所有方法和设置的性能。所有报告的结果都是三次独立运行的平均结果，另有说明。</p><h5 id="实施细节-1">实施细节</h5><p>为了生成一组不同的异常分布，我们提出的方法使用了随机选择的正常簇和标记的异常簇来创建每个单独的异常分布数据Di。具体来说，首先使用k-means聚类将正常样本划分为三个正常聚类（即使用k=3）。然后选择两个随机选择的聚类，结合可见异常，构造Di，选择一个正常的集群和50%的异常集作为支持集<spanclass="math inline">\(\mathcal{D}_i^s\)</span>，而其余的样本用作查询集<spanclass="math inline">\(\mathcal{D}_i^q\)</span>（根据只有一个可见的异常例子的协议，该示例都包含在这两个集合中）。这有助于有效地模拟具有部分观察到的异常分布的开放集环境。为了进一步增加异常分布数据集内部和之间的异质性，我们随机选择三种流行的异常生成技术中的一种，包括CutMix[60]、CutPaste [22]和DRAEM Mask[63]，来生成伪异常并注入Di的支持和查询集。以保证开放集相关的伪异常检测、<spanclass="math inline">\(\mathcal{D}_i^s\)</span>和<spanclass="math inline">\(\mathcal{D}_i^q\)</span>中的伪异常都是由两种不同的异常生成方法生成的。对于每个数据集，都使用T=6来生成单个的异常分布数据。当xj表示看不见的异常样本时，Cj设置为1.0，当xj表示可见的异常或看不见的正常样本时，Cj设置为0.5。</p><p>AHL是一个通用框架，在该框架下，现有OSAD模型的特性和损耗函数可以很容易地作为基本特性和基本损耗插入。特别是，从其中一个OSAD模型（如DRA）中提取图像特征，然后使用我们提出的基于基础损失的损失函数来训练AHL(见等式4).DRA [15]、DevNet [32]和BGAD[58]是目前OSAD使用的SOTA模型，但BGAD使用的与其他两个数据集非常不同的基准数据集。我们的实验严格遵循DRA[15]和DevNet [32]中使用的开创性的OSAD评估协议和基准，并选择DRA[15]和DevNet[32]分别插入AHL，表示为AHL（DRA）和AHL（DevNet）。Adam被用作优化器。学习异构T基模型的初始学习率设置为0.0002，而统一AD模型g的初始学习率设置为0.002。在自监督重要性评分估计器中，采用两层双向LSTM[67]作为骨干，隐藏维数设置为6。在预测层之前，后面是一个有12个隐藏节点的全连接层。该组件的初始学习率被设置为0.002。</p><p>上述设置默认用于所有数据集的AHL报告结果。MLEP、SAOE和FLOS的结果取自[15]。DevNet和DRA的结果使用他们的官方代码进行复制，以获得AHL中使用的特性，这意味着DevNet和AHL（DevNet）使用相同的特性集，这也适用于DRA和AHL（DRA）（更多的实现细节请参见附录B）。</p><h5 id="在一般设置下的性能">在一般设置下的性能</h5><p>表1显示了在一般设置下的比较结果，其中模型使用一个或10个随机抽样的异常例子进行训练。</p><figure><img src="./../postimages/IAD/QQ_1721792278221.png"alt="QQ_1721792278221" /><figcaption aria-hidden="true">QQ_1721792278221</figcaption></figure><p>MVTecAD上的结果在其16个数据子集上取平均值（关于这些子集的详细结果见附录C）。总的来说，我们的方法AHL在三个应用场景的所有数据集的10个镜头和一次性设置协议中都为各自的DRA和DevNet带来了持续的实质性改进。由于DRA是一个比DevNet更强的基础模型，因此AHL（DRA）通常比AHL（DevNet）获得更好的性能。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>PromptAD</title>
      <link href="/PromptAD/"/>
      <url>/PromptAD/</url>
      
        <content type="html"><![CDATA[<p>PromptAD: Learning Prompts with only Normal Samples for Few-ShotAnomaly Detection</p><p>华东师范大学，上海，中国<br/>华东师范大学<br/>重庆学院，重庆，中国<br/>海军军医大学，上海，中国<br/>厦门大学，中国福建</p><h1 id="摘要">摘要</h1><p>​  视觉语言模型对few-shot工业异常检测有了很大的改进，通常需要通过快速工程设计数百个提示。对于自动化场景，我们首先使用传统的多类范式的提示学习作为自动学习提示，但发现它在单类异常检测中不能很好地工作。为了解决上述问题，本文提出了一种少镜头异常检测的一类提示学习方法PromptAD。首先，我们提出了语义连接方法，通过将正常提示与异常后缀连接，将正常提示转置为异常提示，从而构建了大量的负样本，用于指导单类设置中的提示学习。此外，为了缓解缺乏异常图像所带来的训练挑战，我们引入了显式异常边缘的概念，通过超参数显式地控制正常提示特征和异常提示特征之间的边缘。对于图像级/像素级异常检测，PromptAD在MVTec和VisA上的11/12few-shot设置中获得第一名。</p><h1 id="引言">引言</h1><p>​  在这个框架中，在训练期间只有正常的样本可用，但在测试阶段，该模型被期望识别异常的样本。由于工业异常检测通常为各种工业生产线定制一个模型，因此以很少的样本快速训练模型的能力在实际应用中具有重要的前景。</p><p>​  由于基础模型[27,36,38]具有较强的 zero-shot能力，WinCLIP[21]被提出作为第一个利用视觉语言基础模型（即CLIP[37]）来提高模型在少镜头设置下的异常检测性能的工作。为了更好地利用提示指导，WinCLIP引入了一种名为“提示集成”的提示工程师策略，该策略结合了足够数量的手动设计的提示。例如，一些手动提示（例如，裁剪后的照片、模糊的照片等）被作为正常的提示组合在一起。如图1（右）所示，随着提示数量的增加，WinCLIP的表现有所改善，在大约1000个提示时达到饱和点。其他方法如SAA+[7]和AnoVL[13]也采用即时工程来提高模型性能，这已经成为快速引导异常检测的仪式。及时工程涉及人工干预，需要仔细设计，不满足工业场景的自动化要求。</p><figure><img src="./../postimages/PromptAD/image-20240709213252747.png"alt="image-20240709213252747" /><figcaption aria-hidden="true">image-20240709213252747</figcaption></figure><p>图1.左：多类和单类设置下的提示学习。右图：WinCLIP使用不同的提示数量的提示引导结果，以及基线和我们的PromptAD的提示引导结果，以便一次性提示学习。所有的结果都在MVTec上。</p><p>​  在本文中，我们提出了一种只有正常样本的一类提示学习方法，称为PromptAD。为了解决上述的第一个挑战，我们提出了语义连接（SC）。直观地说，将一个提示符与反义文本连接起来可以改变它的语义。根据这个想法，如图1（左b.）所示，SC首先设计一个可学习的正常提示，如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.]\)</span>。对于正常样本，然后手动连接与异常相关的各种文本与正常提示，如<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][flaw]\)</span>。它被转换为异常提示，在提示学习过程中可以作为正常样本的负提示。为了扩大异常信息的丰富性，SC还设计了一个可学习的异常提示，通过将一个可学习标记的后缀与一个正常提示连接起来，例如<spanclass="math inline">\([\mathbf{P}_{1}][\mathbf{P}_{2}]\ldots[\mathbf{P}_{E_{N}}][obj.][\mathbf{A}_{1}][\mathbf{A}_{2}]\ldots[\mathbf{A}_{E_{A}}]\)</span>，其中<spanclass="math inline">\(\begin{bmatrix}\mathbf{A}_i\end{bmatrix}\)</span>是可学习的token。对可学习异常提示和手动异常提示的分布进行对齐，以确保可学习异常提示学习到更多正确的异常信息。</p><p>​  此外，在异常检测中，异常样本不可用，因此无法通过对比损失来明确控制正常和异常提示特征之间的边缘。为了解决第二个挑战，我们提出了显式异常边缘（EAM）的概念，其中引入了一个超参数，以确保正常特征与正常提示特征之间的距离小于正常特征与异常提示特征之间的距离。从而确保在正常提示和异常提示之间有足够的间隔。图1（右）说明了我们的巨大优势，可以看出，（与WinCLIP[21]和Baseline[59]相比）PromptAD仅通过10个∼20（↓∼980和↓0）提示就能达到91.3%（↑1.2%和↑9.8%）/92.5%（↑7.7%和↑的3.7%）图像级/像素级异常检测结果。</p><p>​  综上所述，本文的主要贡献是：</p><ol type="1"><li>我们探讨了提示学习在单类异常检测中的可行性，并提出了一种one-class提示学习方法称为PromptAD，它彻底击败了传统的多类提示学习。<br/>2.提出了语义连接（SC），它可以通过连接异常后缀来转换正常提示的语义，从而为正常样本构造足够的负提示。<br/>3.提出了显式异常边缘（EAM），它可以通过一个超参数显式地控制正常提示特征与异常提示特征之间的距离。<br/>4.对于图像级/像素级异常检测，PromptAD在MVTec [4]和VisA [61]的11/12few-shot设置中获得第一名。</li></ol><h1 id="前期准备工作">前期准备工作</h1><h2 id="clip和提示学习">CLIP和提示学习</h2><p>​  对比语言图像预训练称为CLIP[37]，是一种大规模的视觉语言模型，以其zero-shot分类能力而闻名。具体来说，给出一个未知的图像i，和K个文本提示<spanclass="math inline">\(\{\mathbf{s}_{1},\mathbf{s}_{2},...,\mathbf{s}_{K}\}\)</span>，CLIP可以预测i属于以下K个文本提示的分布：<spanclass="math display">\[p(\mathbf{y}|\mathbf{i})=\frac{\exp&lt;f(\mathbf{i}),g(\mathbf{s}_y)/\tau&gt;}{\sum_{i=1}^K\exp&lt;f(\mathbf{i}),g(\mathbf{s}_i)/\tau&gt;}\]</span>​  其中，f（·）和g（·）分别是视觉编码器和文本编码器。&lt;·，·&gt;表示余弦相似度，τ为温度超参数。用于CLIP零镜头分类的初始文本提示仍然很简单，例如[class]的照片等，比直接使用类的名称作为提示略好一些。</p><p>​  提示学习受自然语言处理（NLP）[24,46]中提示学习成功的启发，CoOp[59]将这种模式引入到few-shot分类中，旨在自动学习CLIP的高效提示。具体来说，在CoOp中使用的提示符不是冻结的文本描述，而是一组可训练的参数：<spanclass="math display">\[\mathbf{s}_k=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}][class_k]\]</span>​  其中<spanclass="math inline">\([\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_P}]\)</span>是可训练的标记，<spanclass="math inline">\([class_k]\)</span>是不可训练的第k类名。提示学习的目的是自动训练有效的提示，以提高下游分类任务的剪辑性能。</p><h2 id="clip-surgery">CLIP Surgery</h2><p>​  作为一种分类模型，CLIP在没有微调的提示引导图像定位任务中的适应性要差得多。为了找出为什么CLIP不能完成图像定位任务，一些CLIP可解释的工作[31,57]分析了CLIP提取视觉特征的机制。这些研究观察到，Q-K自注意[48]的全局特征提取影响了CLIP的定位能力，具体如下：<spanclass="math display">\[Attn(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\mathbf{Q}\cdot\mathbf{K}^\mathrm{T}\cdotscale)\cdot\mathbf{V}\]</span>​  为此，CLIP-Surgery[31]提出了一种V-V注意机制，在不破坏原始结构的情况下增强模型对局部特征的注意。如图2所示，特征提取过程描述如下：<spanclass="math display">\[\begin{gathered}\mathbf{Z}_{ori}^{l-1}=[\mathbf{t}_{cls};\mathbf{t}_{1};\mathbf{t}_{2},...;\mathbf{t}_{T}],\\\mathbf{Z}^{l-1}=[\mathbf{t}_{cls}^{\prime};\mathbf{t}_{1}^{\prime};\mathbf{t}_{2}^{\prime},...;\mathbf{t}_{T}^{\prime}],\\[\mathbf{Q}^{l},\mathbf{K}^{l},\mathbf{V}^{l}]=QKV_Proj.^{l}(\mathbf{Z}_{ori}^{l-1}),\\\mathbf{Z}^{l}=Proj.^{l}(Attn(\mathbf{V}^{l},\mathbf{V}^{l},\mathbf{V}^{l}))+\mathbf{Z}^{l-1},\end{gathered}\]</span> ​  其中<spanclass="math inline">\(\mathbf{Z}_{ori}^{l-1}\)</span>表示（l−1）层输出的原始剪辑视觉编码器和<spanclass="math inline">\(\mathbf{Z}^{l-1}\)</span>表示本地感知输出层l−1，QKVP roj.l和Projl表示QKV投影和输出投影，其参数由原始CLIP的视觉编码器参数初始化。最终的原始输出和局部感知输出为Zori和Z，CLS特征<spanclass="math inline">\(\mathbf{Z}_{ori}[0] \in\mathbb{R}^d\)</span>用于图像级异常检测，局部特征图<spanclass="math inline">\(\mathbf{Z}[1:]\in\mathbb{R}^{T\timesd}\)</span>用于像素级异常检测。在本文中，我们使用改进的CLIP作为主干，并将其称为VV-CLIP。</p><h1 id="方法论">方法论</h1><h2 id="概观">概观</h2><p>​  图2说明了我们建议的PromptAD的概述。PromptAD建立在VV-CLIP上，其视觉编码器用于提取全局和局部特征。所提出的语义连接（SC）用于设计提示。</p><p><strong><imgsrc="./../postimages/PromptAD/image-20240710104034694.png"alt="image-20240710104034694" /></strong></p><p>​  具体来说，将N个可学习正常前缀和目标名称连接得到正常提示（NPs），然后将N个正常提示分别与M个手动异常后缀和L个可学习异常后缀连接，得到N×手动异常提示（MAPs）和N×可学习异常提示（LAPs）。利用视觉特征和提示特征，通过对比损失和所提出的显式异常边缘（EMA）损失来完成提示学习。EMA可以通过一个超参数来控制正常提示特征和异常提示特征之间的显式边距。最后，利用提示学习获得的提示用于提示引导异常检测（PAD）。</p><p>​  除了PAD外，参考WinCLIP+[21]，我们还引入了视觉引导异常检测（VAD）。具体来说，如图2所示，在训练过程中，视觉编码器输出的第i层特征（没有CLS特征）被存储为正常的视觉记忆，记为<strong>R</strong>。在测试阶段，将查询图像的第i层特征图<spanclass="math inline">\(\mathbf{F}\in\mathbb{R}^{h\times w\timesd}\)</span>与<strong>R</strong>进行比较，得到异常得分图<spanclass="math inline">\(\textbf{M}\in\begin{bmatrix}1,0\end{bmatrix}^{h\timesw}\)</span>： <spanclass="math display">\[\mathbf{M}_{ij}=\min_{\mathbf{r}\in\mathbf{R}}\frac{1}{2}(1-&lt;\mathbf{F}_{ij},\mathbf{r}&gt;)\]</span>​  在实践中，我们使用两层的中间特征作为内存，为每个查询图像得到两个得分映射，然后对两个得分映射进行平均，得到最终的可视化得分映射Mv。</p><h2 id="语义连接">语义连接</h2><p>​  在异常检测训练过程中，只能获得正常的样本，这导致没有负的样本来引导快速学习，从而损害了其效果。我们发现，提示的语义可以通过连接来改变。例如，aphoto of cable具有正常语义，将其与后缀连接后，a photo of cable withflaw转换为异常语义。通过这种方法，我们提出了语义连接（SC），通过将正常提示与异常后缀连接，将正常提示转换为异常提示，从而基于可学习的正常提示构建足够的对比提示。具体来说，按照CoOp[59]的格式，可学习的正常提示符（NP）设计如下： <spanclass="math display">\[\mathbf{s}^n=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.]\]</span>​  其中，EN表示可学习的正规前缀和[obj.]的长度。表示正在被检测到的对象的名称。可学习的正常提示在与异常后缀连接后，可以转换为异常提示。特别是，我们从数据集[4,61]的异常标签中生成异常后缀，如[]with color stain， [] withcrack等，然后将这些文本与NP连接，获得手动异常提示（MAP）： <spanclass="math display">\[\mathbf{s}^m=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][with][color][stain]\]</span>​  其中，前缀为可训练的NP，后缀为手动异常文本。此外，我们将NP与一个可学习的标记后缀结合起来，设计了一个可学习的异常提示符（LAP）：<spanclass="math display">\[\mathbf{s}^l=[\mathbf{P}_1][\mathbf{P}_2]\ldots[\mathbf{P}_{E_N}][obj.][\mathbf{A}_1]\ldots[\mathbf{A}_{E_A}]\]</span>​  其中，EA表示可学习的异常后缀的长度。应该注意的是，由相同的正常前缀或异常后缀连接的提示的参数是共享的。在训练过程中，NPs移动到接近正常的视觉特征，而map和lap则远离正常的视觉特征。快速学习的训练损失与CLIP训练损失一致如下：<spanclass="math display">\[\mathcal{L}_{clip}=\mathbb{E}_{\mathbf{z}}\left[-log\frac{\exp(&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;)}{\exp&lt;\mathbf{z},\bar{\mathbf{w}}^n/\tau&gt;+\sum_{\mathbf{w}\in\mathcal{W}}\exp&lt;\mathbf{z},\mathbf{w}/\tau&gt;}\right]\]</span>​  其中，z表示正常的视觉特征，<spanclass="math inline">\(\mathbf{\overline{w}}^n=\frac{\sum_{i=1}^Ng(\mathbf{s}_i^n)}N\)</span>是正常提示功能的原型，$={g()|}$是一个包含所有异常提示特征的集合。由于更多的负样本可以产生更好的对比学习效应[18]，因此将每个异常提示特征与视觉特征进行比较。</p><p>​  备注。在单类异常检测中，传统的提示学习只能设计出可学习的正常提示，这不利于对比损失的影响。所提出的语义连接可以将正常提示的语义转换为具有共享参数的异常语义，从而使正常样本与语义转换（异常提示）形成对比。</p><h1 id="显式异常边缘">显式异常边缘</h1><p>​  由于训练中缺乏异常视觉样本，MAPs和LAPs只能将正常视觉特征作为负样本进行对比，并且在正常和异常提示之间缺乏明确的边缘。因此，我们提出了用于ADprompt学习的显式异常边缘（EAM），它可以控制正常提示特征与异常提示特征之间的边缘。EAM实际上是一种通过边际超参数实现的正则化损失，其定义为：<spanclass="math display">\[\mathcal{L}_{ema}=\mathbb{E}_{\mathbf{z}}\left[\max\left(0,d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^n}{\|\mathbf{\bar{w}}^n\|_2})-d(\frac{\mathbf{z}}{\|\mathbf{z}\|_2},\frac{\mathbf{\bar{w}}^a}{\|\mathbf{\bar{w}}^a\|_2})\right)\right]\]</span>​  式中，d（·，·）为欧氏距离，<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>为所有异常提示特征的原型：<spanclass="math display">\[\bar{\mathbf{w}}^a=\frac{\sum_{i=1}^{N\timesM}g(\mathbf{s}_i^m)+\sum_{i=1}^{N\times L}g(\mathbf{s}_i^l)}{N\timesM+N\times L}\]</span>​  在CLIP中，最终的特征都被投影到单位超球体上，因此<spanclass="math inline">\(\mathcal{L}_{ema}\)</span>中的特征也被归一化，边缘固定为零。与对比损失（<spanclass="math inline">\(\mathcal{L}_{clip}\)</span>）相比，EMA损失保证了正常样本与异常原型之间的距离比正常样本与正常原型之间的距离更大，从而导致了正常样本与异常原型之间的明确区分。</p><p>​  此外，由于map包含足够的异常信息，而lap在没有任何语义指导的情况下被初始化，因此对齐它们有助于lap模拟map的分布。具体来说，我们用平方l2范数来对齐这两个分布的平均值：<spanclass="math display">\[\mathcal{L}_{align}=\lambda\cdot\left\|\frac{\bar{\mathbf{w}}^m}{\|\bar{\mathbf{w}}^m\|_2}-\frac{\bar{\mathbf{w}}^l}{\|\bar{\mathbf{w}}^l\|_2}\right\|_2^2\]</span>​  其中，<span class="math inline">\(\mathbf{\bar{w}}^m\)</span>和<spanclass="math inline">\(\mathbf{\bar{w}}^l\)</span>分别为map和lap的特征均值，λ为控制map和lap对齐程度的超参数。</p><h2 id="异常检测">异常检测</h2><p>​  在测试阶段，使用<spanclass="math inline">\(\mathbf{\bar{w}}^n\)</span>作为正常原型，使用<spanclass="math inline">\(\mathbf{\bar{w}}^a\)</span>作为异常原型，完成快速引导的异常检测。图像级评分<spanclass="math inline">\(\mathbf{S}_t \in[0,1]\)</span>和像素级评分地图<span class="math inline">\(\mathbf{M}_t\in [0,1]^{h\times w}\)</span>通过以下公式得到： <spanclass="math display">\[score=\frac{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;}{\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^n/\tau&gt;+\exp&lt;\mathbf{z}_t,\mathbf{\bar{w}}^a/\tau&gt;}\]</span>​  其中，zt是用于图像高度/像素级异常检测的全局/局部图像特征。</p><p>​  最后，将视觉引导的Mv和提示引导的Mt融合得到像素级异常评分图，融合Mv和St的最大值得到图像海拔异常评分：<spanclass="math display">\[\mathbf{M}_{pix}=1.0/(1.0/\mathbf{M}_v+1.0/\mathbf{M}_t),\\\mathbf{S}_{img}=1.0/(1.0/\max_{ij}\mathbf{M}_v+1.0/\mathbf{S}_t),\]</span>​  其中，我们使用的融合方法是调和平均值，它对较小的值[21]更敏感。</p><h1 id="实验">实验</h1><p>​  我们在1、2和4-shot设置下完成了PromptAD和最新方法之间的比较实验，其中包括图像级和像素级的结果。此外，我们还比较了many-shot和full-shot的方法，以显示PromptAD强大的少镜头性能。最后，我们进行了消融实验，以验证了所提出的SC和EAM对即时学习的改进，并展示了不同的CLIP转换方法[31,57]和超参数的影响。</p><h2 id="数据集">数据集</h2><p>​  在本文中，我们使用的基准测试是MVTec [4]和VisA[61]。这两个基准测试都包含多个子集，每个子集只有一个对象。MVTec包含15个对象，每张图像有700−900像素，而VisA包含12个对象，每张图像约为1.5K×1K像素。异常检测是一类任务，因此训练集只包含正常样本，而测试集包含正常样本和具有图像级和像素级注释的异常样本。此外，还对每个对象中出现的异常类别进行了注释。</p><h2 id="评估指标">评估指标</h2><p>​  我们遵循文献[4]，报告了用于图像级和像素级异常检测的接收机操作特征下面积（AUROC）。</p><h2 id="实施细节">实施细节</h2><p>​  除了超参数τ外，我们还使用了CLIP的OpenCLIP[20]实现及其预训练参数，以及超参数τ的默认值。参考WinCLIP[21]，我们使用了基于LAION-400M [43]的CLIP和ViT-B/16+。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A_Survey_on_Deep_Clustering:_From_the_Prior_Perspective</title>
      <link href="/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/"/>
      <url>/A-Survey-on-Deep-Clustering-From-the-Prior-Perspective/</url>
      
        <content type="html"><![CDATA[<p>一个关于深度聚类的总结：从先验的角度来看</p><p>四川大学计算机科学学院，成都，中国四川</p><h1 id="摘要">摘要</h1><p>​  由于神经网络具有强大的特征提取能力，深度聚类在分析高维和复杂的真实世界数据方面取得了巨大的成功。深度聚类方法的性能受到网络结构和学习目标等各种因素的影响。然而，正如本调查中所指出的，深度聚类的本质是对先验知识的整合和利用，这在很大程度上被现有的工作忽略了。从开创性基于数据结构假设的深度聚类方法到最近基于数据增强不变性的对比聚类方法，深度聚类的发展本质上对应于先验知识的演化。在本调查中，我们通过将深度聚类方法分为六种先验知识类型，提供了一个全面的回顾。我们发现，总的来说，先前的创新遵循两个趋势，即，i)从采矿到建设，以及ii)从内部到外部。此外，我们在五个广泛使用的数据集上提供了一个基准，并分析了具有不同先验的方法的性能。通过提供一个新的先验知识视角，我们希望这次调查能够提供一些新的见解，并启发未来在深度聚类社区的研究。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DeepClustering</title>
      <link href="/DeepClustering/"/>
      <url>/DeepClustering/</url>
      
        <content type="html"><![CDATA[<details open><br/><summary>深度聚类</summary><ul class="task-list"><li><label><input type="checkbox" /><a href="">A Survey on DeepClustering: From the Prior Perspective</a> <ahref="https://arxiv.org/abs/2406.19602"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" /><a href="">Image Clustering withExternal Guidance</a> <a href="https://arxiv.org/abs/2310.11989"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" /><a href="">Scaling Up DeepClustering Methods Beyond ImageNet-1K</a> <ahref="https://arxiv.org/abs/2406.01203"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li><li><label><input type="checkbox" /><a href="">Unsupervised Learning ofVisual Features by Contrasting Cluster Assignments</a> <ahref="https://arxiv.org/abs/2006.09882"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></label></li></ul></details>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>20240627讲座</title>
      <link href="/20240627%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240627%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>https://www.bilibili.com/video/BV1F4421D7DL</p><p>深度伪造反制技术需求迫切</p><figure><img src="./../postimages/0627/image-20240627201621844.png"alt="image-20240627201621844" /><figcaption aria-hidden="true">image-20240627201621844</figcaption></figure><figure><img src="./../postimages/0627/image-20240627201913441.png"alt="image-20240627201913441" /><figcaption aria-hidden="true">image-20240627201913441</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202139680.png"alt="image-20240627202139680" /><figcaption aria-hidden="true">image-20240627202139680</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202304130.png"alt="image-20240627202304130" /><figcaption aria-hidden="true">image-20240627202304130</figcaption></figure><figure><img src="./../postimages/0627/image-20240627202334148.png"alt="image-20240627202334148" /><figcaption aria-hidden="true">image-20240627202334148</figcaption></figure><p>总结<br/>遮挡人脸识别<br/>√渐进式学习-----兼顾非口罩人脸识别性能<br/>√无标签样本助力-----适应真实口罩遮挡<br/>√遮挡预测与身份特征耦合学习-----应对多样性遮挡</p><p>伪造人脸检测及溯源<br/>√隐身份驱动-----解释伪造人脸检测<br/>√身份解耦溯源-----追溯目标人脸</p>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepClusteringSurvey</title>
      <link href="/DeepClusteringSurvey/"/>
      <url>/DeepClusteringSurvey/</url>
      
        <content type="html"><![CDATA[<p>Deep Clustering: A Comprehensive Survey<ahref="https://arxiv.org/abs/2210.04142"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></p><p>Yazhou Ren, <em>Member, IEEE</em>, Jingyu Pu, Zhimeng Yang, Jie Xu,Guofeng Li, Xiaorong Pu, Philip S. Yu, <em>Fellow, IEEE</em>, Lifang He,<em>Member, IEEE</em></p><h1 id="摘要">摘要</h1><p>​  聚类分析在机器学习和数据挖掘中起着不可或缺的作用。学习一个好的数据表示对聚类算法至关重要。近年来，深度聚类可以利用深度神经网络学习聚类友好表示，已广泛应用于聚类任务。现有的深度聚类调查主要集中在单视图领域和网络架构上，忽略了聚类的复杂应用场景。为了解决这个问题，在本文中，我们提供了一个全面的调查，以深度聚类的视图的数据源。在不同的数据源和初始条件下，我们从方法学、先验知识和体系结构等方面系统地区分了聚类方法。具体地说，根据传统的单视角深度聚类、半监督深度聚类、深度多视图聚类和深度转移聚类四种深度聚类方法。最后，我们讨论了深度聚类在不同领域中所面临的开放挑战和潜在的未来机遇。</p><h1 id="引言">引言</h1><h2 id="深度单视图聚类">深度单视图聚类</h2><p>​  通过深度神经网络（DNNs）提取这些数据的表示形式是深度聚类的一个重要特征。然而，更值得注意的是不同的应用深度学习技术，它们与DNNs的结构高度相关。为了比较特定dnn的技术路线，我们将这些算法分为五类：基于深度自动编码器（DAE,deep autoencoder）的深度聚类，基于深度神经网络（DNN, deep neuralnetwork）的深度聚类、基于变分自编码器（VAE, variationalautoencoder）的深度聚类、基于生成对抗网络（GAN, generativeadversarial</p><p>network）的深度聚类和基于图神经网络（GNN, graph nerualnetwork）的深度聚类。</p><h2 id="基于半监督学习的深度聚类">基于半监督学习的深度聚类</h2><p>​  当待处理的数据包含一小部分先验约束时，传统的聚类方法不能有效地利用这些先验信息，而半监督聚类是解决这一问题的有效方法。目前，深度半监督聚类的研究还没有得到很好的探索。然而，半监督聚类是不可避免的，因为通过在模型中添加附加信息作为约束损失，使聚类方法成为半监督聚类方法是可行的</p><h2 id="基于多视图学习的深度聚类">基于多视图学习的深度聚类</h2><p>​  在现实世界中，数据通常来自不同的特征收集器或具有不同的结构。我们称这些数据为“多视图数据”或“多模态数据”，其中每个样本都有多个表示。基于多视图学习的深度聚类的目的是利用多视图数据中包含的一致和互补的信息来提高聚类性能。此外，多视图学习的思想可能对深度单视图聚类具有指导意义。在本调查中，我们将深度多视图聚类分为三类：基于深度嵌入式聚类、基于子空间聚类和基于图神经网络。</p><h2 id="基于迁移学习的深度聚类">基于迁移学习的深度聚类</h2><p>​  对于实例数量有限和高维度的任务，有时我们可以找到一个助手来提供额外的信息。例如，如果任务A类似于另一个任务B和B有更多的信息集群（B标记或B更容易集群），这是有用的转移信息从B转移学习无监督域适应（UDA）近年来提高，其中包含两个域：源域标签和未标记的目标域。迁移学习的目标是将从源任务中学习到的知识或模式应用到一个不同但相关的目标任务中。基于迁移学习的深度聚类方法旨在利用相关任务的信息来提高当前聚类任务的性能。</p><p>在研究相应的聚类方法之前，有必要注意聚类数据的不同特征和条件。在本综述中，对现有的数据源和初始条件下的深度聚类方法系统地进行了分类。分析了不同聚类方法的优缺点和适用条件。最后，我们提出了在深度聚类领域的一些有趣的研究方向。</p><h1 id="定义和初步">定义和初步</h1><p>​  我们将在本节中介绍这些符号。在本文中，我们用大写字母表示矩阵，用小写字母表示向量。除非另有说明，本文中使用的符号总结在表1中。本调查将介绍四种基于不同背景条件的深度聚类问题。在这里，我们正式地定义了这些问题。给定一组数据样本X，我们的目标是找到一个可以将X映射到k簇的映射函数F。映射结果用Yˆ表示。所以我们要处理的任务是：</p><p>​  Deep single-view clustering: <spanclass="math display">\[F\left(X\right)\to\hat{Y}\]</span>​  Semi-supervised deep clustering: <spanclass="math display">\[F\left(X,A\right)\to\hat{Y}\]</span>​  其中A是一个约束矩阵。</p><p>​   Deep multi-view clustering: <spanclass="math display">\[F\begin{pmatrix}X^1,...,X^n\end{pmatrix}\to\hat{Y}\]</span>​  其中，Xi是X的第i个视图。</p><p>​  Deep clustering with domain adaptation: <spanclass="math display">\[F\left(X^s,Y^s,X^t\right)\to\hat{Y}\]</span>​  其中（Xs、Y s）为已标记的源域，Xt为未标记的目标域。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SAA+</title>
      <link href="/SAA/"/>
      <url>/SAA/</url>
      
        <content type="html"><![CDATA[<p>Segment Any Anomaly without Training via Hybrid PromptRegularization</p><p>中国华中科技大学数字制造设备与技术国家重点实验室</p><h1 id="摘要">摘要</h1><p>​  我们提出了一个新的框架，即分段任意异常+（SAA+, Segment Any Anomaly+），用于混合快速正则化的零快速异常分割，以提高现代基础模型的适应性。现有的异常分割模型通常依赖于特定领域的微调，限制了它们在无数异常模式上的泛化。在这项工作中，受到基础模型强大的零镜头泛化能力的启发，我们首先探索它们的组装，以利用不同的多模态先验知识进行异常定位。对于非参数基础模型对异常分割的自适应，我们进一步引入了来自领域专家知识和目标图像上下文的混合提示作为正则化。我们提出的SAA+模型在几个异常分割基准测试上取得了最先进的性能，包括VisA、MVTec-AD、MTD和KSDD2。code:https://github.com/caoyunkang/Segment-Any-Anomaly</p><h1 id="介绍">介绍</h1><p>​  异常分割模型[1,2,3]在工业质量控制[4,5]和医学诊断[6]等各个领域都引起了人们极大的研究兴趣。可靠异常分割的关键是区分异常数据的分布。具体来说，本文考虑了图像上的零样本异常分割（ZSAS,zero-shot anomalysegmentation），这是一种很有前途但未探索的设置，在训练过程中没有为目标类别提供正常和异常图像。</p><p>​  由于缺乏用于训练的异常样本，许多工作都致力于无监督或自监督的异常分割，其目标是在训练过程中学习正常样本的表示。然后，通过计算测试样本与学习到的正态分布之间的差异，可以对异常进行分割。具体来说，这些模型，包括基于自动编码器的重建[7,8,9,10,11,12]、一类分类[13,14,15]和基于记忆的正态分布[3,2,16,17,18]方法，通常需要对特定的有限的类别训练单独的模型。然而，在现实场景中，有数以百万计的工业产品，为单个对象收集大型训练集没有成本效益，这阻碍了在需要高效部署的情况下，例如生产的初始阶段，它们的部署。</p><p>​  最近，基础模型，如SAM [19]和CLIP[20]，通过提示[21,22]检索存储在这些模型中的先验知识，显示出了巨大的零镜头视觉感知能力。在这项工作中，我们想探索如何适应基础模型来实现异常分割下的零样本迁移能力。</p><p><img src="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA）。然而，SAA显示了严重的误警报问题，它错误地检测到所有的“灯芯”，而不是ground-truth异常区域（“过长的灯芯”）。因此，我们在改进的模型（SAA+)中进一步加强了混合提示的正则化，成功地帮助识别了异常区域。</p><p>​  为此，如图1所示，我们首先通过级联提示引导的目标检测[23]和分割基础模型[19]，构造一个普通的基础模型，即分别作为异常区域生成器和异常区域细化器（SAA）。根据解锁基础模型知识[24,25]的实践，使用朴素语言提示，如“缺陷”或“异常”，来分割目标图像所期望的异常。具体来说，语言提示用于提示异常区域生成器为所需的异常区域生成提示条件的框级区域。然后在异常区域细化器中对这些区域进行细化，以产生最终的预测，即用于异常分割的掩模。</p><p>​  然而，如图1所示，普通的基础模型装配（SAA）往往会导致严重的误报，例如，SAA错误地将所有的灯芯称为异常，而只有超长的灯芯是真正的异常，我们将其归因于幼稚的语言提示带来的歧义。首先，当面对基础模型的预训练数据分布与下游数据集之间的域转换时，传统的语言提示可能会变得无效。其次，目标的“异常”程度取决于对象上下文，这对于朴素的粗粒度语言提示，例如“异常区域”，很难准确地表达。</p><p>​  因此，超越幼稚的语言提示，我们将领域专家知识和目标图像纳入我们改进的框架上下文，即分段任何异常+（SAA+）。一方面，专家知识提供了在开放世界场景中与目标相关的异常情况的详细描述。我们利用更具体的描述作为上下文提示，有效地对齐预训练和目标数据集中的图像内容。另一方面，我们利用目标图像上下文来可靠地识别和自适应地校准异常分割预测[26,27]。通过利用目标图像中丰富的上下文信息，我们可以准确地将对象上下文与最终的异常预测联系起来。</p><p>​  从技术上讲，除了单纯的类不可知的提示外，我们还利用领域专家知识来构建面向目标的异常语言提示，即特定于类的语言表达式。此外，由于语言不能准确地检索具有特定对象特征的区域，如数量、大小和位置，精确地[28,29]，我们以阈值过滤器的形式引入对象属性提示。这些提示有助于识别和删除不满足所需属性的候选区域。此外，为了充分利用目标图像的上下文，我们建议利用图像显著性和区域置信度排序作为提示，通过考虑一个区域与图像内其他区域之间的相似性，如欧氏距离，来建模一个区域的异常程度。最后，我们进行了彻底的实验，以确认我们的混合提示在适应基础模型的零镜头异常分割的有效性。具体来说，我们最终的模型（SAA+）在零镜头设置下，在各种异常分割数据集上获得了新的最新性能。总之，我们的主要贡献是：</p><p>​  我们提出了异常分割的SAA框架，允许在不需要训练的情况下协同组装不同的基础模型。</p><p>​  我们引入混合提示作为一种正则化技术，利用领域专家知识和目标图像上下文来适应基础模型进行异常分割。这导致了SAA+的开发，这是我们框架的一个增强版本。</p><p>​  我们的方法在几个基准数据集上实现了最先进的零镜头异常分割，包括VisA、MVTec-AD、KSDD2、MTD的性能。值得注意的是，SAA/SAA+在不需要任何注释的情况下检测与纹理相关的异常方面显示出了显著的能力。</p><h1 id="saa针对zsas的基础模型组装">SAA：针对ZSAS的基础模型组装</h1><h2 id="问题定义zsas">问题定义：ZSAS</h2><p>零样本异常分割（ZSAS， Zero-shot Anomaly Segmentation）</p><p>​  ZSAS的目标是对新对象进行异常分割，而不需要任何相应的对象训练数据。ZSAS试图创建一个基于空训练集∅的异常映射<spanclass="math inline">\(\mathbf{A}\in[0,1]^{h\timesw\times1}\)</span>，以识别包含新对象的图像<spanclass="math inline">\(\mathbf{I}\in\mathbb{R}^{h\timesw\times3}\)</span>中单个像素的异常程度。ZSAS任务有可能显著减少对培训数据的需求，并降低与实际检查部署相关的成本。</p><h2 id="基线模型saa">基线模型：SAA</h2><p>分段任何异常（SAA，Segment Any Anomaly）</p><p>​  对于ZSAS，我们首先构建一个普通的基础模型组件，即分段任何异常（SAA），如图1所示。<br/><imgsrc="./../postimages/SAA/image-20240621152437495.png"alt="image-20240621152437495" />图1：为了在没有训练的情况下分割任何异常，我们首先通过简单的类不可知语言提示（如“异常”），提示进入异常区域生成器（如提示引导目标检测基础模型[23]）和异常区域细化器（如分割基础模型[19]）模块来构建一个相对没有新意的基础模型（SAA)。</p><p>具体来说，给定一个特定的异常分割查询图像，我们首先使用语言作为初始提示，通过一个异常区域生成器粗略地检索粗糙的异常区域建议，即GroundingDINO[23]。然后，使用异常区域细化器将异常区域建议细化为像素级高质量的分割掩模，其中使用提示驱动的分割基础模型，即SAM[19]。</p><h3 id="异常区域发生">异常区域发生</h3><p>​  随着语言视觉模型的蓬勃发展，一些基础模型[24,23,46]逐渐获得了通过语言提示检索图像中对象的能力。给定描述要检测区域的语言提示T，例如“异常”，基础模型可以为查询图像i生成所需区域i。在那里，我们将区域检测器的结构基于文本引导的开集对象检测结构，用于视觉grounding。具体来说，我们采用了一个已经在[41]上预先训练过的大规模语言视觉数据集的GroundingDINO[23]架构。该网络首先通过文本编码器和视觉编码器分别提取语言提示符和查询图像的特征。然后用交叉模态解码器以边界框的形式生成粗糙的对象区域。给定边界盒级区域集RB及其对应的置信度评分集S，异常区域生成器（生成器）的模块可以表示为：<spanclass="math display">\[\mathcal{R}^{B},\mathcal{S}:=\mathrm{Generator}(\mathbf{I},\mathcal{T})\]</span></p><h3 id="异常区域细化">异常区域细化</h3><p>​  为了生成像素级的异常分割结果，我们提出了异常区域细化器，将边界盒级的异常区域候选区域细化为异常分割掩模集。为此，我们使用了一个复杂的基础模型来进行开放世界的视觉分割，即SAM[19]。该模型主要包括一个基于vit的[56]主干和一个提示条件掩码解码器。具体来说，该模型是在一个具有10亿个细粒度掩模的大规模图像分割数据集[19]上进行训练的，这使得在开放集分割下能够具有高质量的掩模生成能力。有提示条件的掩码解码器接受各种类型的提示作为输入。我们将边界框候选RB视为提示，得到像素级分割掩模r。异常区域细化器（Refiner）的模块可以表述如下：<spanclass="math display">\[\mathcal{R}:=\operatorname{Refiner}(\mathbf{I},\mathcal{R}^B)\]</span>​  在此之前，我们以具有相应置信度分数s的高质量分割掩模R的形式获得了区域集。综上所述，我们总结了框架（SAA）如下：<spanclass="math display">\[\mathcal{R},\mathcal{S}:=\text{SAA}(\mathbf{I},\mathcal{T}_n)\]</span>​  其中Tn是一个朴素的类不可知的语言提示，例如”异常“，在SAA中使用。</p><h3 id="基线模型组件的zsas性能分析">基线模型组件的ZSAS性能分析</h3><p>​  我们提出了一些初步的实验来评估基础模型组装对ZSAS的有效性。尽管解决方案的简单和直观，我们观察到一个语言歧义的问题。具体来说，某些语言提示，如“异常”，可能无法检测到所需的异常区域。例如，如图1所示，所有的“灯芯”都被SAA用“异常”提示符错误地识别为异常。</p><p>​  我们将这种语言歧义归因于训练前的语言-视觉数据集和目标ZSAS数据集之间的领域差距，这意味着一些语言提示可能具有不同的含义，并在不同的数据集中与不同的图像内容相关联。此外，在这些大规模的数据集中几乎没有像“异常”这样的形容词表达，这使得这种快速的设计很难理解什么是异常区域。此外，确切的“异常”是特定于对象的，并且会因对象而变化。例如，它表示皮革上的划痕或榛子上的裂缝。语言歧义问题导致ZSAS数据集中严重的误警报。我们建议引入由领域专家知识和目标图像上下文生成的混合提示，以减少语言歧义，从而实现更好的ZSAS性能。</p><h1id="saa通过混合提示正则化的自适应基础模型">SAA+：通过混合提示正则化的自适应基础模型</h1><p>​  为了解决SAA中的语言歧义并提高其在ZSAS上的能力，我们提出了一个名为SAA+的升级版本，它包含了混合提示，如图2所示。除了利用从预先训练过的基础模型中获得的知识外，SAA+还利用领域专家知识和目标图像上下文来生成更准确的异常区域掩模。我们将在下面提供关于这些混合提示的进一步细节。</p><h2 id="从领域专家知识中生成的提示">从领域专家知识中生成的提示</h2><p>​  根据提示学习[48,54]的趋势，我们以语言的形式初始化提示，以解锁基础模型的知识。然而，当只使用朴素的语言提示“异常”时，由领域差距引起的语言歧义问题尤为严重。为了解决这个问题，我们利用了包含关于目标异常区域的有用的先验信息的领域专家知识。具体来说，尽管专家可能没有为新产品提供潜在开放世界异常的全面列表，但他们可以根据他们过去使用类似产品的经验来确定一些候选产品。领域专家知识使我们能够将朴素的“异常”提示细化为更具体的提示，以更详细地描述异常状态。除了语言提示之外，我们还引入了属性提示，以补充现有基础模型[28]中对“count”和“area”[28]等特定属性的认识不足。</p><h3 id="异常的语言表达式作为提示">异常的语言表达式作为提示</h3><p>​  为了描述潜在的开放世界异常情况，我们建议设计更精确的语言提示。这些提示可分为两种类型：类无关的提示和类特定的提示。</p><p>​  <strong>类别不可知论提示（Ta）</strong>是描述非特定于任何特定类别的异常情况的通用提示，例如，“异常”和“缺陷”。尽管预先训练的数据集和目标ZSAS数据集之间存在领域差距，但我们的实证分析（5.3）表明，这些通用提示提供了令人鼓舞的初始性能。</p><p>​  <strong>类别特定提示（Ts）</strong>是基于对类似产品的异常模式的专家知识而设计的，以补充更具体的异常细节。我们使用预先训练的视觉语言数据集中已经使用的提示，例如“黑洞”和“白色气泡”，来查询所需的区域。这种方法重新定义了寻找异常区域的任务，以定位具有特定异常状态表达式的对象，这比利用基础模型在对象上下文中识别“异常”更简单。</p><p>​  通过使用来自领域专家知识的异常语言提示<spanclass="math inline">\(\mathcal{P}^L=\{\mathcal{T}_\mathrm{a},\mathcal{T}_\mathrm{s}\}\)</span>提示SAA，我们生成了更精细的异常区域候选项R和相应的置信分数S。</p><h3 id="异常对象属性作为提示">异常对象属性作为提示</h3><p>​  目前的基础模型[23,57]在查询具有特定属性描述的对象时存在局限性，比如大小或位置，这些对于描述异常很重要，比如“电缆左边的小黑洞”。为了整合这一关键的专家知识，我们建议使用作为规则而不是语言来表述的异常属性提示。具体来说，我们考虑了异常的位置和面积。</p><p>​  <strong>异常定位。</strong>异常的准确定位在区分真实异常和假阳性中起着关键作用。通常，在推理过程中，异常被期望位于感兴趣的对象内。然而，由于背景上下文的影响，异常可能偶尔会出现在被检查的物体之外。为了解决这一挑战，我们利用基础模型的开放世界检测能力来确定被检查对象的位置。随后，我们计算了潜在异常区域和被检查对象之间的并集的交集（IoU）。通过应用expert-derived的IoU阈值，表示为<spanclass="math inline">\(θ_{IoU}\)</span>，我们过滤出了IoU值低于该阈值的异常候选值。此过程确保保留的异常候选项更有可能表示位于被检查对象内的真实异常。</p><p>​  <strong>异常区域。</strong>由其面积所反映的异常现象的大小，也是一种可以提供有用信息的特性。一般来说，异常应小于被检查物体的大小。专家可以为所考虑的特定类型的异常提供一个合适的阈值<spanclass="math inline">\(θ_{area}\)</span>。与<spanclass="math inline">\(θ_{area}\)</span>目标区域不匹配的候选区域可以被过滤掉。</p><p>​  通过结合两个属性提示<spanclass="math inline">\(\mathcal{P}^P=\left\{\theta_{area},\theta_{IoU}\right\}\)</span>，我们可以通过过滤候选区域的过滤函数（Filter）R，得到具有相应置信分数<spanclass="math inline">\(S^P\)</span>的候选<spanclass="math inline">\(R^P\)</span>的子集， <spanclass="math display">\[\mathcal{R}^P,\mathcal{S}^P:=\mathrm{Filter}(\mathcal{R},\mathcal{P}^P)\]</span></p><h2 id="来自目标图像上下文的提示">来自目标图像上下文的提示</h2><p>​  除了结合领域专家知识外，我们还可以利用输入图像本身提供的信息来提高异常区域检测的准确性。在这方面，我们提出了两个由图像上下文引起的提示。</p><h3 id="异常显著为提示">异常显著为提示</h3><p>​  由于预先训练的语言视觉数据集[41]和目标异常分割数据集[4,58]之间的领域差距，由[23]等基础模型生成的预测可能是不可靠的。为了校准个体预测的置信度得分，我们提出了模拟人类直觉的异常显著性提示法。具体来说，人类可以通过与周围区域[40]的差异来识别异常区域，即视觉显著性包含了指示异常程度的有价值的信息。因此，我们通过计算相应的像素特征(f)与其N个最近邻之间的平均距离，来计算输入图像的显著性映射(s)，<span class="math display">\[\mathbf{s}_{ij}:=\frac1N\sum_{\mathbf{f}\inN_p(\mathbf{f}_{ij})}(1-\langle\mathbf{f}_{ij},\mathbf{f}\rangle)\]</span>​  式中，<span class="math inline">\((i,j)\)</span>表示像素位置，<spanclass="math inline">\(N_p(\mathbf{f}_{ij})\)</span>表示对应像素的N个最近邻，<spanclass="math inline">\(\langle\cdot,\cdot\rangle\)</span>表示余弦相似度。我们使用来自大规模图像数据集[59]的预先训练好的cnn来提取图像特征，以确保特征的描述性。显著性地图表示一个区域与其他区域的不同程度。显著性提示PS定义为相应区域掩模内的指数显著性平均值，<spanclass="math display">\[\mathcal{P}^S:=\left\{\exp(\frac{\sum_{ij}\mathbf{r}_{ij}\mathbf{s}_{ij}}{\sum_{ij}\mathbf{r}_{ij}})\quad|\quad\mathbf{r}\in\mathcal{R}^P\right\}\]</span>​  显著性提示提供了异常区域置信度的可靠指示。这些提示是用来重新校准基础模型生成的信心分数，产生新的调整分数<spanclass="math inline">\(S^S\)</span>基于异常显著性提示<spanclass="math inline">\(P^S\)</span>。这些调整分数提供一个综合措施，考虑到信心来自基础模型和地区候选人的显著性。该流程的表述如下：<span class="math display">\[\mathcal{S}^S:=\begin{Bmatrix}p\cdots&amp;|&amp;p\in\mathcal{P}^S,s\in\mathcal{S}^P\end{Bmatrix}\]</span></p><h3 id="异常置信为提示">异常置信为提示</h3><p>​  通常，一个被检查对象中的异常区域的数量是有限的。因此，我们提出异常置信度提示<spanclass="math inline">\(P^C\)</span>根据图像内容识别出置信度得分最高的K个候选对象，并使用它们的平均值进行最终的异常区域检测。这是通过根据其对应的置信度得分选择前K个候选区域来实现的，如下所示，<spanclass="math display">\[\mathcal{R}^C,\mathcal{S}^C:=\mathrm{Top}_K(\mathcal{R}^P,\mathcal{S}^S)\]</span>​  将单个区域及其对应的得分表示为<spanclass="math inline">\(r^C\)</span>和<spanclass="math inline">\(s^C\)</span>，然后我们使用这些K个候选区域来估计最终的异常图，<spanclass="math display">\[\mathbf{A}_{ij}:=\frac{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C\cdots^C}{\sum_{\mathbf{r}^C\in\mathcal{R}^C}\mathbf{r}_{ij}^C}\]</span>​  通过提出的混合提示<spanclass="math inline">\((\mathcal{P}^L,\mathcal{P}^P,\mathcal{P}^S,\text{and}\mathcal{P}^C)\)</span>，SAA在我们最终的框架中进行了正则化，即分段任何异常+（SAA+），从而做出了更可靠的异常预测。</p><h1 id="实验">实验</h1><p>​  在本节中，我们首先评估SAA/SAA+在几个异常分割基准上的性能。然后，我们广泛地研究了个体混合提示的有效性。</p><h2 id="实验设置">实验设置</h2><p>​  <strong>数据集。</strong>我们利用了四个带有像素级注释的数据集。：VisA [58]、MVTec-AD [4]、KSDD2 [60]和MTD[61]。VisA和MVTec-AD由多种对象子集组成，如电路板，而KSDD2和MTD则由纹理异常组成。总之，我们将所有这些数据集的子集分类为通常在单个图像中显示相似模式的纹理（如地毯），以及包括更多样化分布的对象（如蜡烛）。</p><p>​  <strong>评估指标。</strong>ZSAS性能的评估基于两个指标： (I)<strong>max-F1-pixel</strong>（Fp）[25]，它测量在最优阈值下的像素分割的F1分数；（II）<strong>max-F1-region</strong>（Fr），本文提出，以减轻最大f1像素[4]观察到的大缺陷的偏差。具体来说，我们在最优阈值下计算区域分割的f1分数，如果重叠值超过0.6，考虑预测为正。</p><p>​  <strong>实施细节。</strong>我们采用了GroundingDINO和分段任何模型2的官方实现来构建基线（SAA）。关于来自领域专家知识的提示的细节在补充材料中有解释。对于由图像内容诱导的显著性提示，我们使用WideResNet50[62]网络，在ImageNet [59]上进行预训练，并根据之前的研究[40]设置N =400。对于异常置信度提示，我们将超参数K默认设置为5。输入图像的分辨率固定为400×400进行评估。</p><h2 id="主要结果">主要结果</h2><p>​  <strong>比较方法。</strong>我们比较了我们最终的模型，即分段任何异常+（SAA+）与几种并发的最先进的方法，包括WinClip[25]，UTAD [40]，ClipSeg[24]，和我们的香草基线（SAA）。对于WinClip，我们报告其在VisA和MVTec-AD上的官方结果。对于其他三种方法，我们使用官方实现，并使它们适应于ZSAS任务。值得注意的是，由于所有的方法都不需要训练过程，它们的性能是稳定的，方差为±0.00。</p><p>​  <strong>定量结果：</strong>如表1所示，SAA+方法在Fp和Fr方面均显著优于其他方法。虽然WinClip[25]、ClipSeg[24]和SAA也使用基础模型，但SAA+更好地释放了基础模型的能力，并调整它们来解决ZSAS问题。SAA+的显著性能满足了不经训练就能分割任何异常现象的期望。</p><figure><img src="./../postimages/SAA/image-20240621164328885.png"alt="image-20240621164328885" /><figcaption aria-hidden="true">image-20240621164328885</figcaption></figure><p>​  <strong>定性结果：</strong>图3为SAA+与以往竞争方法的定性比较，其中SAA+取得了更好的性能。此外，可视化显示SAA+能够检测纹理异常，如皮革上的小划痕。</p><figure><img src="./../postimages/SAA/image-20240621164520983.png"alt="image-20240621164520983" /><figcaption aria-hidden="true">image-20240621164520983</figcaption></figure><h1 id="消融研究">消融研究</h1><p>​  在表2中，我们执行组件级分析，以消除框架中特定的提示设计。</p><figure><img src="./../postimages/SAA/image-20240621164657482.png"alt="image-20240621164657482" /><figcaption aria-hidden="true">image-20240621164657482</figcaption></figure><p>​  <strong>语言提示符<spanclass="math inline">\((\mathcal{P}^L)\)</span>。</strong>表2验证了来自领域专家知识的语言提示的有效性（Fp中+3.90%，Fr+4.90%）。然后，我们深入研究了Ta和Ts的有效性，这清楚地表明，一般描述和专门设计的异常描述都可以达到合理的性能。此外，它们的组合可以产生协同作用，提高异常分割性能。<spanclass="math inline">\(\mathcal{P}^L\)</span>的改进有助于解锁当前基础模型[23,19]的语言驱动区域检测能力。</p><p>​  <strong>属性提示符<spanclass="math inline">\((\mathcal{P}^P)\)</span>。</strong>除了改善整体性能，属性提示带来显著的改善（从21.83%到53.79%）纹理类别，由于过滤机制过滤掉大量的错误检测异常区域候选人通过高级特征，例如，目标图像的位置和面积。</p><p>​  <strong>显著性提示符<spanclass="math inline">\((\mathcal{P}^S)\)</span>。</strong>表2提供了<spanclass="math inline">\(\mathcal{P}^S\)</span>在异常分割的有效性的明确证据。这是因为区域显著性可以准确地描述一个区域与周围环境的偏离程度。</p><figure><img src="./../postimages/SAA/image-20240621165328601.png"alt="image-20240621165328601" /><figcaption aria-hidden="true">image-20240621165328601</figcaption></figure><p>​  在图4中，我们展示了<spanclass="math inline">\(\mathcal{P}^S\)</span>对异常分割的定性影响，说明了视觉显著性图可以帮助突出异常区域，即与其他区域相比更高的显著性值。通过结合<spanclass="math inline">\(\mathcal{P}^S\)</span>来校准置信度分数，可以获得更精确的分割结果。例如，<spanclass="math inline">\(\mathcal{P}^S\)</span>的使用可以有效地定位榛子的裂缝区域和蜡烛上的过长的灯芯。</p><p>​  <strong>置信度提示符<spanclass="math inline">\((\mathcal{P}^C)\)</span>。</strong>通过加入异常置信度提示，我们限制了异常区域的数量，这有效地减少了假阳性，导致所有类别的Fp平均提高0.72%，如表2所示。</p><figure><img src="./../postimages/SAA/image-20240621165812635.png"alt="image-20240621165812635" /><figcaption aria-hidden="true">image-20240621165812635</figcaption></figure><p>​  超参数K在PC中的影响如图5所示。从图中可以看出，随着K的提高，异常区域检测准确。然而，当K超过一定的阈值（约为K= 5）时，随着更多的区域被错误地识别为异常，性能略有下降。在K =5左右时获得最佳结果，所有类别的平均Fp为34.85%。</p><h1 id="结论">结论</h1><p>​  在这项工作中，我们探索如何在没有任何进一步训练的情况下，通过释放现代基础模型的全部力量来分割任何异常现象。基础模型装配的调整归功于快速设计，这是控制非基础模型功能的关键。因此，我们提出了一个新的框架，即分段任何异常+，利用来自专家知识和目标图像上下文的混合提示来规范无需训练的基础模型。最后，我们成功地采用了多个基础模型来解决零镜头异常分割问题，并在几个基准上获得了新的SoTA结果。我们希望我们的工作能够阐明对异常分割的无标签模型自适应的设计。</p><p>​  <strong>限制。</strong>由于计算的限制，我们目前没有在更大尺度的基础模型上测试我们的方法。我们已经用具有代表性的基础模型完成了对我们的方法的探索，并将在未来探讨这些模型的尺度效应。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning for Image Super-Resolution</title>
      <link href="/Image_Super-Resolution/"/>
      <url>/Image_Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p><h1 id="摘要">摘要：</h1><p>​  图像超分辨率（SR）是在计算机视觉中提高图像和视频分辨率的一类重要的图像处理技术。近年来，利用深度学习技术实现的图像超分辨率取得了显著进展。本文旨在对利用深度学习方法的图像超分辨率的最新进展进行一个全面的调查。一般的来说，我们可以将现有的SR技术的研究大致分为三大类：有监督SR、无监督SR和领域特异性SR。此外，我们还讨论了一些重要的问题，如公开可用的基准数据集和性能评估指标。最后，我们通过强调几个未来的方向和社区应该进一步解决的问题来结束这项调查。</p><h1 id="介绍">介绍</h1><p>​  图像超分辨率（SR）是指从低分辨率（LR）图像中恢复高分辨率（HR）图像的过程，是计算机视觉和图像处理中的一类重要的图像处理技术。它享有广泛的现实世界的应用，如医学成像[1]，[2]，[3]，监视和安全[4]，[5])等。除了提高图像感知质量外，它还有助于改善其他计算机视觉任务[6]、[7]、[8]、[9]。一般来说，这个问题是非常具有挑战性的，并且具有固有的不适定性，因为总是有多个HR图像对应于单个LR图像。在文献中，已经提出了各种经典的SR方法，包括基于预测的方法[10]，[11]，[12]，基于边缘的方法[13]，[14]，统计方法[15]，[16]，基于补丁的方法[13]，[17]，[18]，[19]和稀疏表示方法[20]，[21]，等。</p><p>​  近年来深度学习技术的快速发展，基于深度学习的SR模型已经积极探索，经常实现最先进的性能的各种基准的各种深度学习方法被应用于解决SR任务，从早期基于卷积神经网络（CNN）的方法（例如，SRCNN[22][23]）最近有前途的SR方法使用生成对抗网（GAN）[24]（如SRGAN[25]）。一般来说，使用深度学习技术的SR算法家族在以下主要方面有所不同：不同类型的网络架构[26]、[27]、[28]、不同类型的损失函数[8]、[29]、[30]、不同类型的学习原则和策略[8]、[31]、[32]等。</p><p>​  在本文中，我们全面概述了图像超分辨率的最新进展。虽然有一些现有的SR调查文献，我们的工作不同，我们专注在深度学习SR技术，而大多数早期作品[33]，[34]，[35]，[36]旨在调查传统SR算法或一些研究主要集中在提供定量评估基于全参考指标或人类视觉感知[37]，[38]。与现有的调查不同，本调查采用了一个独特的基于深度学习的视角，以系统和全面的方式回顾了SR技术的最新进展。</p><p>​  这次调查的主要贡献有三方面：<br/>&gt; 1.我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。<br/>&gt;1.我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。<br/>&gt;1.我们讨论挑战和开放的问题，并确定新的趋势和未来的方向，为社区提供深刻的指导。</p><p>​  在下面的章节中，我们将介绍在深度学习中图像超分辨率的最新进展的各个方面。图1显示了本次调查中将以层次结构的方式覆盖的图像SR的分类。第2节给出了问题的定义，并回顾了主流数据集和评估指标。第3节模块化地分析了监督SR的主要成分。第4节简要介绍无监督的SR方法。第5节介绍了一些流行的特定于领域的SR应用程序，第6节还讨论了未来的发展方向和开放的问题。</p><h1 id="问题设置和术语">问题设置和术语</h1><h2 id="问题定义">问题定义</h2><p>​  图像超分辨率的目的是从LR图像中恢复相应的HR图像。通常，LR图像Ix被建模为以下退化的输出：<span class="math display">\[I_x=\mathcal D(I_y;\delta),\]</span>​  式中，D为退化映射函数，Iy为对应的HR图像， $ $为退化过程的参数（如缩放因子或噪声）。</p><p>​  一般来说，退化过程（即D和 $ $）是未知的，只提供LR图像。在这种情况下，也称为盲SR，需要研究人员通过从LR图像Ix中恢复地面真实HR图像^的HR近似，如下：<span class="math display">\[\hat{I}_y=\mathcal{F}(I_x;\theta),\]</span>​  其中，F为超分辨率模型， $ $ 为F的参数。</p><p>​  虽然退化过程是未知的，并且可能受到各种因素的影响（如压缩伪影、各向异性退化、传感器噪声和散斑噪声），但研究人员正试图对退化映射进行建模。大多数工作都直接将退化建模为一个单一的降采样操作，如下所示：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y)\downarrow_s,\{s\}\subset\delta,\]</span>​  其中， $ <em>s $是一个具有缩放因子s的降采样操作。事实上，大多数通用SR的数据集都是基于这种模式构建的，而最常用的降采样操作是带有抗锯齿的双边插值。然而，[39]还有其他一些工作，将退化建模为几种操作的组合：<spanclass="math display">\[\mathcal{D}(I_y;\delta)=(I_y\otimes\kappa)\downarrow_s+n_\varsigma,\{\kappa,s,\varsigma\}\subset\delta,\]</span>​  其中 $ I_y$ 表示模糊核与HR图像Iy之间的卷积， $ n</em>$ 是带有标准差 $$的加性高斯白噪声。与等式的朴素定义相比3、等式的组合降解模式4更接近真实世界的情况，并已被证明对SR[39]更有利。为此目的，SR的目标如下： <spanclass="math display">\[\hat{\theta}=\arg\min_\theta\mathcal{L}(\hat{I}_y,I_y)+\lambda\Phi(\theta),\]</span>​  其中， $ (_y,I_y) $ 表示生成的HR图像 $ _y $与地面真实图像Iy之间的损失函数，$ ()$为正则化项，为权衡参数。虽然SR最流行的损失函数是像素级均方误差（即像素损失），但更强大的模型倾向于使用多个损失函数的组合，这将在第3.4.1节中介绍。</p><h2 id="图像质量评估">图像质量评估</h2><p>​  图像质量是指图像的视觉属性，侧重于对观众的感知评估。一般来说，图像质量评估（IQA）方法包括基于人类感知的主观方法（即图像看起来的真实程度）和客观的计算方法。前者更符合我们的需求，但往往是耗时和昂贵的，因此后者是目前的主流。然而，这些方法之间不一定一致，因为客观方法往往不能非常准确地捕捉人类的视觉感知，这可能导致IQA结果[25]，[58]的很大差异。</p><p>​  此外，客观IQA方法进一步分为三种类型的[58]：使用参考图像进行评估的全参考方法，基于提取特征比较的简化参考方法，以及无任何参考图像的无参考方法（即盲IQA）。接下来，我们将介绍几种最常用的IQA方法，包括主观方法和客观方法。</p><h3 id="峰值信噪比">峰值信噪比</h3><p>​  峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）是有损变换（如图像压缩、图像嵌入绘制）中最常用的重建质量测量方法之一。对于图像的超分辨率，PSNR是通过图像之间的最大像素值（记为L）和均方误差（MSE）来定义的。给定N个像素的groundtruth图像I和重建I，PSNR定义如下：<spanclass="math display">\[\mathrm{PSNR}=10\cdot\log_{10}\left(\frac{L^2}{\frac{1}{N}\sum_{i=1}^N\left(I(i)-\hat{I}(i)\right)^2}\right),\]</span>​  其中，L等于255，在一般情况下使用8位表示。由于PSNR只与像素级MSE相关，只关注对应像素之间的差异而不是视觉感知，这往往导致在真实场景中表示重建质量的表现不佳，而我们通常更关注人类的感知。然而，由于需要与文献作品进行比较，且缺乏完全准确的感知指标，PSNR仍然是目前SR模型中使用最广泛的评价标准。</p><h2 id="操作通道">操作通道</h2><p>​  除了常用的RGB颜色空间外，YCbCr颜色空间也被广泛用于SR。在这个空间中，图像分别用Y、Cb、Cr通道表示，分别表示亮度、蓝差和红差的色度分量。虽然目前还没有公认的最佳实践来执行或评估超分辨率，但早期的模型倾向于在YCbCr空间[26]、[43]、[78]、[79]的Y通道上运行，而最近的模型倾向于在RGB通道[28]、[31]、[57]、[70]上运行。值得注意的是，在不同颜色的空间或通道上进行操作（培训或评估）可以使评估结果差异很大（高达4dB）[23]。</p><h2 id="超分辨率挑战">超分辨率挑战</h2><p>​  在本节中，我们将简要介绍图像SR的两个最流行的挑战，NTIRE [80]和PIRM[47]，[81]。</p><h3 id="ntire的挑战">NTIRE的挑战</h3><p>​  图像恢复和增强（NTIRE, The New Trends in Image RestorationandEnhancement）的新趋势挑战[80]与CVPR相结合，包括多个任务，如SR，去噪和着色。对于图像SR，NTIRE挑战是建立在DIV2K[42]数据集上，由双边降缩放轨迹和具有现实未知退化的盲轨迹组成。这些轨道在降解和比例因子上有所不同，旨在促进在理想条件和现实世界的不利情况下的SR研究。</p><h3 id="pirm挑战">PIRM挑战</h3><p>​  感知图像恢复和操作（PIRM, The Perceptual Image Restoration andManipulation）挑战与ECCV相结合，还包括多个任务。与NTIRE相比，PIRM的一个子挑战[47]侧重于生成准确性和感知质量之间的权衡，而另一个[81]侧重于智能手机上的SR。正如众所周知的[77]一样，针对失真的模型经常产生视觉上不愉快的结果，而针对感知质量的模型在信息上表现较差保真度。具体来说，PIRM根据均方根误差（RMSE）的阈值将感知扭曲平面划分为三个区域。在每个区域，获胜的算法是获得最佳感知质量的[77]，由NIQE[76]和Ma[66]评估。而在另一个子挑战[81]，智能手机上的SR，参与者被要求使用有限的智能手机硬件（包括CPU、GPU、RAM等）执行SR，评价指标包括PSNR、MS-SSIM和MOS测试。通过这种方式，PIRM鼓励对感知-失真的权衡进行高级研究，并在智能手机上驱动轻量级和高效的图像增强。</p><h1 id="监督超分辨率">监督超分辨率</h1><p>​  目前，研究人员已经提出了各种具有深度学习的超分辨率模型。这些模型侧重于有监督的SR，即同时用LR图像和相应的HR图像进行训练。虽然这些模型之间的差异非常大，但它们本质上是一组组件的一些组合，如模型框架、上采样方法、网络设计和学习策略。从这个角度来看，研究人员结合这些组件来建立一个集成的SR模型，以拟合特定的目的。在本节中，我们将集中精力模块化地分析基本组件（如图1所示），而不是孤立地介绍每个模型，并总结它们的优点和局限性。</p><h2 id="超分辨率框架">超分辨率框架</h2><p>​  由于图像超分辨率是一个不适定问题，如何进行上采样（即从LR输入生成HR输出）是关键问题。尽管现有模型的架构差异很大，但基于所采用的上采样操作及其在模型中的位置，它们可以归因于四个模型框架（如图2所示）。</p><h3 id="预上采样超分辨率">预上采样超分辨率</h3><p>​  由于直接学习从低维空间到高维空间的映射的困难，利用传统的上采样算法获得高分辨率的图像，然后利用深度神经网络进行细化是一个简单的解决方案。因此，Dong等人[22]，[23]首先采用预上采样SR框架（如图2a所示），并提出SRCNN来学习从插值的LR图像到HR图像的端到端映射。具体来说，使用传统方法（如双边插值）将LR图像上采样到具有所需大小的粗糙HR图像，然后在这些图像上应用深度cnn来重建高质量的细节。由于最困难的上采样操作已经完成，cnn只需要对粗糙的图像进行细化，这大大降低了学习难度。此外，这些模型可以以任意大小和缩放因子的插值图像作为输入，并给出与单尺度SR模型[26]性能相当的细化结果。因此，它逐渐成为[55]、[56]、[82]、[83]中最流行的框架之一，这些模型之间的主要区别是后验模型设计（第3.3节）和学习策略（第3.4节）。然而，预定义的上采样往往会引入副作用（如噪声放大和模糊），由于大多数操作是在高维空间进行的，时间和空间的成本比其他框架[43]，[84]高得多。</p><h3 id="后上采样超分辨率">后上采样超分辨率</h3><p>​  为了提高计算效率，充分利用深度学习技术自动提高分辨率，研究人员提出在低维空间中用端到端可学习层替换预定义的计算。在该框架的先驱作品[43]，[84]中，即如图2b所示的上采样后SR，LR输入图像在不提高分辨率的情况下输入深度cnn，在网络末端应用端到端可学习的上采样层。</p><p>​  由于计算成本较大的特征提取过程只发生在低维空间中，而分辨率最终只会提高，因此大大降低了计算复杂度和空间复杂度。因此，这种框架也已成为最主流的框架之一，[25]，[31]，[79]，[85]。这些模型的不同主要在于可学习的上采样层（第3.2节）、前CNN结构（第3.3节）和学习策略（第3.4节）等。</p><h3 id="逐步上采样超分辨率">逐步上采样超分辨率</h3><p>​  虽然上采样后的SR框架极大地降低了计算成本，但它仍存在一些缺点。一方面，上采样只进行了一步，这大大增加了对大尺度因子（如4,8）的学习差异。另一方面，每个比例因子都需要训练一个单独的SR模型，这无法应对多尺度SR的需要。为了解决这些缺点，拉普拉斯金字塔SR网络（LapSRN）[27]采用了渐进式上采样框架，如图2c所示。具体来说，该框架下的模型是基于cnn的级联，并逐步重建更高分辨率的图像。在每个阶段，图像被上采样到更高的分辨率，并通过cnn进行细化。</p><p>​  其他的工作，如MS-LapSRN[65]和渐进式SR（ProSR）[32]也采用了这个框架，并实现了相对较高的性能。与LapSRN和MSLapSRN使用中间重建图像作为后续模块的“基础图像”相比，ProSR保留主要信息流，并通过单个头部重建中间分辨率图像。</p><p>​  该框架下的模型将困难任务分解为简单任务，大大降低了学习难度，特别是在因素较大的情况下，并在不引入过多空间和时间成本的情况下应对多尺度SR。此外，一些具体的学习策略，如课程学习（第3.4.3节）和多监督（第3.4.4节），进一步降低学习难度，提高最终成绩。然而，这些模型也遇到了一些问题，如多阶段模型设计复杂和训练稳定性高，需要更多的建模指导和更先进的训练策略。</p><h3 id="迭代上下采样超分辨率">迭代上下采样超分辨率</h3><p>​  为了更好地捕捉LR-HR图像对的相互依赖关系，在SR[44]中加入了一种有效的反投影[12]迭代过程。该SR框架，即迭代上下采样SR（如图2d所示），尝试迭代应用反投影细化，即计算重建误差，然后将其重新融合，调整HR图像强度。具体来说，Haris等人[57]利用迭代上下采样层提出DBPN，将上采样和下采样层交替连接，并使用所有中间重建重建最终的HR结果。类似地，SRFBN[86]采用了一个迭代的上下采样反馈块，具有更密集的跳跃连接，并学习更好的表示。而用于视频超分辨率的RBPN[87]从连续的视频帧中提取上下文，并将这些上下文结合起来，通过一个反向投影模块产生循环输出帧。</p><p>​  该框架下的模型可以更好地挖掘LR-HR图像对之间的深层关系，从而提供更高质量的重建结果。然而，反投影模块的设计标准仍然不清楚。</p><p>​  由于该机制刚刚被引入到基于深度学习的SR中，因此该框架具有巨大的潜力，需要进一步的探索。</p><h2 id="上采样方法">上采样方法</h2><p>​  除了模型中的上采样位置外，如何进行上采样也非常重要。虽然有各种传统的上采样方法[20]、[21]、[88]、[89]，但利用cnn学习端到端上采样已逐渐成为一种趋势。在本节中，我们将介绍一些传统的基于插值的算法和基于深度学习的上采样层。</p><h3 id="基于插值的上采样">基于插值的上采样</h3><p>​  图像插值，a.k.a.图像缩放，是指调整数字图像的大小，并被广泛应用于与图像相关的应用程序中。传统的插值方法包括最近邻插值、双线性和双边插值、Sinc和兰氏重采样等。由于这些方法易于解释和易于实现，其中一些方法仍被广泛应用于基于cnn的SR模型中。</p><h4 id="最近邻插值">最近邻插值</h4><p>最近邻插值是一种简单、直观的算法。它为每个要被插值的位置选择最近的像素的值，而不考虑任何其他像素。因此，这种方法速度非常快，但通常会产生低质量的块状结果。</p><h4 id="双线性插值">双线性插值</h4><p>双线性插值（BLI）首先在图像的一个轴上进行线性插值，然后在另一个轴上进行，如图3所示。由于它导致了一个接受场大小为22的二次插值，因此在保持相对较快的速度的同时，它显示出了比近邻域插值更好的性能。</p><h4 id="二进制插值">二进制插值</h4><p>同样，双边插值（BCI）[10]在两个轴上分别进行三次插值，如图3所示。与BLI相比，BCI考虑了44个像素，并导致更流畅的结果，更少的伪影，但速度更低。事实上，具有抗锯齿的BCI是构建SR数据集的主流方法（即将HR图像降解为LR图像），也广泛应用于预上采样SR框架（第3.1.1节）。</p><p>​  事实上，基于插值的上采样方法仅基于其自身的图像信号来提高图像的分辨率，而没有带来更多的信息。相反，它们经常会引入一些副作用，如计算复杂性、噪声放大、模糊的结果。因此，目前的趋势是用可学习的上采样层取代基于插值的方法。</p><h3 id="基于学习的上采样">基于学习的上采样</h3><p>​  为了克服基于插值的方法的不足，以端到端学习上采样，在SR场中引入了转置卷积层和亚像素层。</p><h4 id="转置卷积层">转置卷积层</h4><p>转置卷积层，a.k.a.反卷积层[90]，[91]，试图执行与正常卷积相反的变换，即，基于类似于卷积输出大小的特征图来预测可能的输入。具体来说，它通过插入零和进行卷积来展开图像来提高图像的分辨率。以33 核的2SR为例（如图4所示），首先将输入的大小扩展原来的两倍，其中添加的像素值设置为0（图4b）。然后对核大小为33、步幅1和填充1进行卷积（图4c）。通过这种方式，输入被上采样了2倍，在这种情况下，接受域最多为22倍。由于转置卷积在保持与普通卷积兼容的连接模式的同时，使图像大小以端到端方式放大，因此在SR模型[57]、[78]、[79]、[85]中被广泛用作上采样层。然而，这一层很容易在每个轴[92]上造成“不均匀的重叠”，并且在两个轴上相乘的结果进一步创建了一个不同大小的棋盘状模式，从而损害了SR性能。</p><h4 id="亚像素层">亚像素层</h4><p>亚像素层[84]是另一个端到可学习的上采样层，通过卷积生成多个信道，然后进行上采样，如图5所示。在这一层中，首先应用卷积来产生具有s2倍通道的输出，其中s是比例因子（图5b）。假设输入大小为hw c，输出大小将为h w s2c。之后，进行整形操作(a.k.a.执行shuffle[84])来产生大小为sh sw c的输出（图5c）。在这种情况下，接受野最高可达33。由于端到端上采样方式，该层也被广泛应用于SR模型[25]、[28]、[39]、[93]。与转置卷积层相比，亚像素层具有更大的接受域，提供了更多的上下文信息，帮助生成更真实的细节。然而，由于感受野的分布是不均匀的，块状区域实际上共享相同的感受野，它可能会在不同块的边界附近产生一些伪影。另一方面，独立预测块状区域中的相邻像素可能会导致输出不平滑输出。因此，Gao等人[94]提出了PixelTCL，它将独立预测替换为相互依赖的序列预测，并产生更平滑、更一致的结果。</p><h4 id="meta上采样模块">Meta上采样模块</h4><p>以往的方法需要对缩放因子进行预细化，即针对不同的因子训练不同的上采样模块，但效率低，不符合实际需求。因此，Hu等人[95]提出了Meta上采样模块（如图6所示），首先基于元学习解决了任意比例因子的SR。具体来说，对于HR图像上的每个目标位置，该模块将其投影到LR特征图上的一个小补丁（即kk cin），根据投影偏移和比例因子预测卷积权值（即k k cincout），并进行卷积。这样，Meta上采样模块就可以通过单一模型的任意因素连续放大。由于大量的训练数据（同时训练多个因素），该模块在固定因素上可以表现出类似甚至更好的性能。虽然该模块在推理过程中需要预测权重，但上采样模块的执行时间只占特征提取[95]时间的1%左右。然而，该方法基于独立于图像内容的几个值来预测每个目标像素的大量卷积权值，因此在面对较大的放大倍数下，预测结果可能不稳定，效率较低。</p><p>​  目前，这些基于学习的层已经成为应用最广泛的上采样方法。特别是在上采样后框架（第3.1.2节）中，这些层通常在最终上采样阶段使用，基于低维空间提取的高级表示重建HR图像，从而在避免高维空间中压倒性操作的同时实现端到端SR。</p><h2 id="网络设计">网络设计</h2><p>​  网络设计是深度学习的重要组成部分之一。在超分辨率领域，研究人员在四种SR框架之上（第3.1节）应用各种网络设计策略来构建最终的网络。在本节中，我们将这些网络分解为网络设计的基本原则或策略，介绍它们，并逐一分析其优点和局限性。</p><h3 id="残差学习">残差学习</h3><p>​  在He等人[96]提出ResNet来学习残差而不是进行彻底的映射之前，残差学习已被SR模型[48]、[88]、[97]广泛使用，如图7a所示。其中，剩余学习策略大致可分为全局残差学习和局部残差学习。</p><h4 id="全局残差余学习">全局残差余学习</h4><p>由于图像SR是一种图像-图像转换任务，输入图像与目标图像高度相关，研究者尝试只学习它们之间的残差，即全局残差学习。在这种情况下，它避免了学习从一个完整图像到另一个完整图像的复杂转换，而是只需要学习一个残差映射来恢复缺失的高频细节。由于大多数区域的残差接近于零，模型的复杂性和学习差异大大降低。因此，它被广泛应用于SR模型[26]、[55]、[56]、[98]。</p><h4 id="局部残差学习">局部残差学习</h4><p>局部残差学习类似于ResNet[96]中的残差学习，用于缓解由于网络深度不断增加而导致的[96]退化问题，降低训练难度，提高学习能力。它也被广泛用于SR[70]、[78]、[85]、[99]。</p><p>​  在实际应用中，上述方法都是通过快捷连接（通常按一个小常量缩放）和元素加法实现的，不同之处在于前者直接连接输入和输出图像，而后者通常在网络内部不同深度的层之间添加多个快捷方式。</p><h3 id="递归学习">递归学习</h3><p>​  为了在不引入压倒性参数的情况下学习更高层次的特征，我们在SR字段中引入了递归学习，即以递归的方式多次应用相同的模块，如图7所示。</p><p>​  其中，16递归DRCN [82]采用单一卷积层作为递归单元，达到4141，远远大于SRCNN [22]的13 13，没有过多参数。DRRN [56]使用一个ResBlock[96]作为25次递归的递归单元，并且获得了比17-ResBlock基线更好的性能。后来Tai等人[55]提出了基于内存块的MemNet，该块由6个递归的重新块组成，每个递归的输出被连接起来，并经过额外的11个卷积进行记忆和遗忘。级联剩余网络（CARN）[28]也采用了类似的递归单元，包括几个重新块。最近，Li等人[86]采用了迭代上下采样SR框架，提出了一种基于递归学习的反馈网络，其中整个网络的权值在所有递归中共享。</p><p>​  此外，研究人员还在不同的部分使用了不同的递归模块。具体来说，Han等人[85]提出了双状态递归网络（DSRN）来在LR和HR状态之间交换信号。在每个时间步长（即递归），每个分支的表示都被更新和交换，以更好地探索LR-HR关系。</p><p>​  类似地，Laiet al.[65]使用嵌入和上采样模块作为递归单元，因此以性能损失很小为代价，大大减少了模型的大小。</p><p>​  一般来说，递归学习确实可以学习更高级的表示，而不引入过多的参数，但仍然不能避免高昂的计算成本。它本质上带来了消失或爆炸的梯度问题，因此一些技术，如残差学习（第3.3.1节）和多监督（第3.4.4节）经常与递归学习集成，以缓解这些问题[55]，[56]，[82]，[85]。</p><h3 id="多路径学习">多路径学习</h3><p>​  多路径学习是指将特征通过多条路径，这些路径执行不同的操作，并将它们融合回来以提供更好的建模能力。具体来说，它可以分为全局、局部和特定规模的多路径学习，如下所述。</p><h4 id="全局多路径学习">全局多路径学习</h4><p>全局多路径学习是指利用多条路径来提取图像的不同方面的特征。这些路径在传播过程中可以相互交叉，从而大大提高了学习能力。具体来说，LapSRN[27]包括一个以粗到细的方式预测子带残差的特征提取路径和另一个基于来自两条路径的信号重建HR图像的路径。同样，DSRN[85]利用两条路径分别在低维和高维空间中提取信息，并不断交换信息以进一步改进学习能力。像素递归超分辨率[64]采用条件反射路径来捕获图像的全局结构，并采用先验路径来捕获生成的像素的串行依赖性。相比之下，Ren等人[100]在模型的末端采用多条具有不平衡结构的路径进行上采样和融合。</p><h4 id="局部多路径学习">局部多路径学习</h4><p>在初始模块[101]的激励下，MSRN[99]采用了一个新的块来进行多尺度特征提取，如图7e所示。在这个块中，采用两个核大小为33 和5 5的卷积层同时提取特征，然后将输出连接并再次进行相同的操作，最后应用额外的11个卷积。快捷方式通过元素添加连接输入和输出。通过这种局部多路径学习，SR模型可以更好地从多个尺度中提取图像特征，进一步提高性能。</p><h4 id="特定尺寸的多路径学习">特定尺寸的多路径学习</h4><p>考虑到不同尺度的SR模型需要进行相似的特征提取，Lim等人[31]提出了特定尺度的多路径学习来应对单一网络的多尺度SR。具体地说，它们共享模型的主成分（即特征提取的中间层），并分别在网络的开始和结束时附加了特定尺度的预处理路径和上采样路径（如图7f所示）。在训练期间，只启用和更新与所选比例对应的路径。通过这种方式，所提出的MDSR[31]通过共享不同尺度的大部分参数，大大减少了模型的大小，并表现出与单尺度模型相当的性能。CARN[28]和ProSR [32]也采用了类似的尺度特异性多路径学习。</p><h3 id="密集连接">密集连接</h3><p>​  由于Huang等人[102]提出了基于密集块的DenseNet，密集连接在视觉任务中越来越流行。对于密集块中的每一层，前面所有层的特征图都被用作输入，其自己的特征图被用作所有后续层的输入，从而导致l层密集块中的（l*(l-1)/2）的连接。密集连接不仅有助于缓解梯度消失，增强信号传播，鼓励特征重用，而且还通过使用小增长率（即密集块中的通道数量）和连接所有输入特征图后压缩通道，大大减少模型大小。</p><p>​  为了融合低层次和高层次的特征，为重构高质量的细节提供更丰富的信息，在SR域中引入了密集的连接，如图7d所示。唐等[79]不仅采用密集块构造一个69层SRDenseNet，还插入密集连接不同密集块，也就是说，对于每一个密集块，所有之前的特征映射块被用作输入，和自己的特性映射被用作输入到所有后续块。MemNet[55]、CARN [28]、RDN [93]和ESRGAN[103]也采用了这些层级和块级的密集连接。DBPN[57]也广泛地采用了密集连接，但它们的密集连接位于所有的上采样单元之间，下采样单元也是如此。</p><h3 id="注意力机制">注意力机制</h3><h4 id="通道注意力">通道注意力</h4><p>考虑到不同通道之间特征表示的相互依赖和相互作用，Hu等人[104]提出了一个“挤压和激励”块，通过明确建模通道相互依赖来提高学习能力，如图7c所示。在这个块中，使用全局平均池化（GAP）将每个输入信道压缩到一个信道描述符（即一个常数）中，然后将这些描述符输入到两个密集的层中，为输入信道生成信道缩放因子。最近，Zhang等人[70]将通道注意机制与SR结合起来，提出了RCAN，显著提高了模型的表示能力和SR性能。为了更好地学习特征相关性，Dai等人的[105]进一步提出了一个二阶信道注意（SOCA）模块。SOCA通过使用二阶特征统计而不是GAP自适应地调整信道特征，并能够提取更多信息性和区别性的表示。</p><h4 id="非本地注意力">非本地注意力</h4><p>大多数现有的SR模型的局部接受域非常有限。然而，一些遥远的对象或纹理可能对局部补丁的生成非常重要。因此，Zhang等人[106]提出了局部和非局部注意块来提取捕获像素之间的长期依赖关系的特征。具体地说，他们提出了一个用于提取特征的主干分支，以及一个（非）局部掩码分支，用于自适应地重新调整主干分支的特征。其中，局部分支采用编码器-解码器结构来学习局部注意，而非局部分支采用嵌入式高斯函数来评估特征图中每两个位置指标之间的成对关系，以预测尺度权值。通过这种机制，该方法很好地捕捉了空间注意力，并进一步提高了表示能力。同样，Dai等人[105]也采用了非局部注意机制来捕获长距离空间背景信息。</p><h3 id="先进的卷积">先进的卷积</h3><p>​  由于卷积操作是深度神经网络的基础，研究人员也试图改进卷积操作，以提高性能或提高效率。</p><h4 id="膨胀卷积">膨胀卷积</h4><p>众所周知，上下文信息有助于生成SR生成现实细节。因此，Zhang等人[107]在SR模型中用扩张卷积来取代常见的卷积，增加了两次以上，获得了更好的性能。</p><h4 id="集团卷积">集团卷积</h4><p>受轻量级CNNs的最新进展的推动，[108]，[109]，Hui等人[98]和Ahn等人[28]分别提出了IDN和CARN-M，用组卷积代替香草卷积。正如之前的一些工作所证明的那样，组卷积大大减少了参数和操作的数量，而牺牲了一点性能损失[28]，[98]。</p><h4 id="深度可分离卷积">深度可分离卷积</h4><p>自从Howard等人[110]提出深度可分离卷积以实现有效的卷积以来，它已经被扩展到各个领域。具体地说，它由一个因子分解的深度卷积和一个点态卷积（即11个卷积）组成，因此只在很小的情况下减少了大量的参数和操作降低精度的[110]。最近，Nie等人的[81]采用了深度可分离卷积，并大大加速了SR体系结构。</p><h3 id="区域递归学习">区域递归学习</h3><h3 id="金字塔池化">金字塔池化</h3><h3 id="小波变换">小波变换</h3><h3 id="desubpixel">Desubpixel</h3><h3 id="xunit">xUnit</h3><h2 id="学习策略">学习策略</h2><h3 id="损失函数">损失函数</h3><h4 id="pixel-loss">Pixel Loss</h4><p>像素损失测量两个图像之间的像素级差异，主要包括L1损失（即平均绝对误差）和L2损失（即均方误差）：<spanclass="math display">\[\mathcal{L}_{\mathrm{pixel}\perp1}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}|\hat{I}_{i,j,k}-I_{i,j,k}|\\\mathcal{L}_{\mathrm{pixel}\perp2}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}(\hat{I}_{i,j,k}-I_{i,j,k})^{2},\]</span></p><h4 id="content-loss">Content Loss</h4><p>为了评价图像的感知质量，将内容损失引入SR[29]，[127]。具体来说，它使用预先训练好的图像分类网络来测量图像之间的语义差异。将该网络表示为f，提取的第1层高级表示表示为fðlÞðIÞ，内容损失表示为两幅图像的高级表示之间的欧氏距离，如下：<spanclass="math display">\[\mathcal{L}_{\mathrm{content}}(\hat{I},I;\phi,l)=\frac{1}{h_lw_lc_l}\sqrt{\sum_{i,j,k}(\phi_{i,j,k}^{(l)}(\hat{I})-\phi_{i,j,k}^{(l)}(I))^2},\]</span></p><h3 id="批量规范化">批量规范化</h3><h3 id="课程学习">课程学习</h3><h3 id="多重监督">多重监督</h3><h3 id="其他改进">其他改进</h3><h4 id="上下文网络融合">上下文网络融合</h4><h4 id="数据增强">数据增强</h4><h4 id="多任务学习">多任务学习</h4><h4 id="网络插值">网络插值</h4><h4 id="自我整合">自我整合</h4><h2 id="最先进的超分辨率模型">最先进的超分辨率模型</h2><p>​  近年来，基于深度学习的图像超分辨率模型受到了越来越多的关注，并取得了最先进的性能。在前面的章节中，我们将SR模型分解为特定的组件，包括模型框架（第3.1节）、上采样方法（第3.2节）、网络设计（第3.3节）和学习策略（第3.4节），对这些组件进行分层分析，并确定它们的优点和局限性。事实上，今天大多数最先进的SR模型基本上都可以归因于我们在上面总结的多种策略的组合。例如，RCAN[70]最大的贡献来自于通道注意机制（第3.3.5节），它还采用了其他策略，如亚像素上采样（第3.3.2.2节）、残差学习（第3.3.1节）、像素L1损失（第3.4.1节）和自集成（第3.5.5节）。以类似的方式，我们总结了一些具有代表性的模型及其关键策略，如表2所示。</p><figure><imgsrc="./../postimages/Image_Super-Resolution/image-20240619184059043.png"alt="image-20240619184059043" /><figcaption aria-hidden="true">image-20240619184059043</figcaption></figure><p>在上面，“Fw", "Rec.", "Res"，"Dense","Att."分别表示SR框架、上采样方法、递归学习、残差学习、密集连接、注意机制</p><p>​  除了SR精度外，效率是另一个非常重要的方面，不同的策略对效率有或多或少的影响。因此，在前面几节中，我们不仅分析了所提出策略的准确性，而且还指出了对效率影响较大的策略的具体影响，如后上采样（3.1.2节）、递归学习（3.3.3.2节）、密集连接（3.3.3.4节）、xUnit（3.3.11节）。我们还对一些具有代表性的SR模型的SR精度（即PSNR）、模型大小（即参数数）和计算成本（即多加数）等方面的SR模型进行了基准测试，如图8所示。精度是通过在4个基准数据集（即Set5[48]，Set14 [49]，B100 [40]和Urban100[50]）上的PSNR的平均值来测量的。模型大小和计算成本用PyTorch-光学传感器[157]计算，其中输出分辨率为720p（即1080720）。所有的统计数据都是根据原始论文或根据官方模型计算得出的，比例因子为2。为了更好地查看和比较，我们还提供了一个交互式的在线版本1。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</title>
      <link href="/UnionFormer/"/>
      <url>/UnionFormer/</url>
      
        <content type="html"><![CDATA[<center>UnionFormer: Unified-Learning Transformer with Multi-View Representationfor Image Manipulation Detection and Localization <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR" /></a></center><center><span class="math inline">\(\text{Shuaibo Li}^{1,2}\quad\text{WeiMa}^{1\dagger}\quad\text{Jianwei Guo}^2\quad\text{ShibiaoXu}^3\quad\text{Benchong Li}^1\quad\text{Xiaopeng Zhang}^2\)</span></center><center>北京理工大学1、MAIS(中国科学院自动化研究所)2、北京邮电大学3</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/UnionFormer/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  我们提出了一个新的框架，通过统一学习集成了三个视图上的篡改线索，用于图像操作检测和定位。特别地，我们构建了一个BSFI-Net，从RGB和噪声视图中提取篡改特征，在调节不同尺度上的空间一致性的同时，增强了对边界伪影的响应性。此外，为了探索对象之间的不一致性作为一种新的线索视角，我们将对象一致性建模与篡改检测和定位结合成一个三任务统一的学习过程，使它们能够相互促进和改进。</p><p>  因此，我们在多尺度监督下获得了一个统一的操作鉴别表示，从三个角度整合信息。这种集成便于高效的并行检测和定位篡改。我们在不同的数据集上进行了大量的实验，结果表明，该方法在篡改检测和定位方面优于最先进的方法。</p><h1 id="引言">引言</h1><p>  数字图像篡改可分为三大类[19]：拼接，即将区域从一幅图像复制到另一幅图像；复制-移动，包括复制或移动同一图像中的元素；移除，删除图像部分和创建视觉一致的内容以掩盖改变的过程。这些操作在被篡改区域和周围环境之间留下痕迹，造成真实区域和伪造区域之间的不一致。与传统的强调高级语义信息的传统检测或分割任务不同，图像篡改检测优先考虑局部语义无关的线索，以区分真实性，而不是语义内容。因此，篡改检测的关键挑战是学习结合不同层次信息并捕获真实和篡改区域之间多尺度不一致的通用特征。以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层[23,27,40,71]的特征，不能充分表示篡改痕迹。受[9,12,67]的启发，我们设计了一个专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork），并将其作为特征编码器集成到我们的框架中。BSFI-Net是一个并行的cnn-Transformer结构，它可以加强边缘响应，同时有效地在局部特征和全局表示之间进行交互，以探索不同尺度上图像内部的一致性。<br/><br/>  另一方面，许多在RGB视图中难以察觉的篡改伪影在噪声视图中变得明显明显。使用固定的[18]或可学习的高通滤波器[6,35,66]将RGB图像转换为噪声图，可以抑制内容，并突出显示低级的伪造线索。因此，开发一种同时建模RGB和噪声维度的多视图策略对于检测细微的篡改痕迹至关重要。我们的框架采用了一个双流架构来独立地构建RGB和噪声视图的表示，随后合并它们以提高鉴别能力和泛化性。此外，我们还结合了对比监督，以改善这两种观点之间的协作。<br/><br/>  此外，为了创建空间相干和语义一致的图像，篡改操作总是改变整个对象来隐藏证据，即执行对象级操作。目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息。相反，我们认为图像操作检测应该不仅仅是识别分布外的像素或补丁，以捕获由操作导致的对象一致性和分布的异常。由于扩散模型[4,5,20,30,44,65,69]生成的超真实的篡改图像，利用对象视图信息变得特别重要。基于扩散的模型[4,30,44]反复更新了整个图像的初始噪声，增强了空间连续性，留下了更少的RGB和噪声痕迹。此外，与真实的图像源不同，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。最近的扩散模型[20,29,55,64]试图通过采用以对象为中心的方法来解决这个问题，强调了使用对象视图线索进行篡改检测的必要性和可行性。然而，创建和集成这样的新视图与其他视图，以篡改伪影表示是一个重大的挑战，需要新的架构和学习策略。<br/><br/>  考虑到上述要点，我们引入了UnionFormer，一个用于图像操作检测和定位的多视图表示的统一学习transformer框架，如图1所示。</p><p><img src="../postimages/UnionFormer/image-20240617100605323.png"alt="image-20240617100605323" /><br/>图1.UnionFormer的组成概述。我们通过整合来自三个视图表示的篡改线索来实现同时的篡改检测和定位，每个视图由不同的颜色背景表示。我们通过BSFI-Net获得了RGB和噪声视图下的表示，并在统一学习中构建了基于两者的对象视图表示。同时，将三个视图的信息交互融合成统一的操作判别表示（UMDR,unified manipulation discriminative representation）进行检测和定位</p><p>  首先，我们使用BSFI-Net作为特征编码器，获得在RGB和噪声视图下的通用化特征，并将其进行组合。然后，我们利用融合的特征进行一个单一化的学习过程，其中包括三个子任务：对象一致性建模、伪造检测和伪造定位。在统一学习中，我们的模型建立了对象视图表示，并将三个视图信息集成到一个统一的操作鉴别表示（UMDR,unified manipulation discriminativerepresentation）中，同时完成伪造检测和定位。综上所述，我们的主要贡献如下：</p><ul><li>我们提出了一种新的图像取证transformer框架，UnionFormer。通过多尺度监督的统一学习，整合三个视角的信息，同时执行图像操作检测和定位。</li><li>我们引入了BSFI-Net，一种用于高级人工表示学习的混合网络结构，它增强了边界响应，同时揭示了不同层次的局部不一致性。</li><li>通过对UMDR的统一学习，我们构建了一种创新的对象视图表示方法，能够从三个视图中捕获对象之间的不一致性和聚合信息，用于伪造检测。</li><li>我们通过各种基准进行了全面的实验，证明了我们的方法在检测和定位任务中都获得了最先进的结果。</li></ul><h1 id="方法">方法</h1><p>  在本节中，我们首先提供对工会成员的概述和对每个组件的详细介绍。我们的目标是充分利用来自三个视图的丰富工件来同时进行篡改检测和定位。我们通过在多尺度监督下的统一学习过程来实现这一目标。<br/><br/>  如图1所示，首先使用受约束的CNN[7]将输入的RGB图像X转换为噪声视图表示N = C(X)，可以显示低级的篡改。<br/><br/>  然后，将X和N分别输入边界敏感特征交互网络（BSFI-Net）进行特征编码。高频边缘特征(H)与X或N一起作为BSFI-Net的输入，以提高边缘响应性。这使得我们能够在RGB和噪声视图下获得可推广的和可鉴别的特征，构造两个特征金字塔$ f_r = _1(X,H), f_n = _2(N,H) $。<br/><br/>  随后，我们使用区域建议网络（RPN）[51]从特征fr中获得一组感兴趣的区域（RoIs），用pi表示。从fr和fn中提取RoI信息，然后扁平得到建议的嵌入表示，记为ri，ni。将每个方案的RGB特征ri和噪声特征ni连接起来，生成融合的方案特征di，并将其输入到I变压器编码器层。<br/><br/>  在统一学习阶段，我们处理了三个子任务：建模对象的一致性、真实性的二进制分类和篡改区域定位。在转换器编码器之后，将伪造-判别查询嵌入DI输入到统一操作判别表示部分，对三个子任务生成三个预测。如图1所示，我们对三个子任务采用了具有统一形式的多尺度监督，包括Lcls、Locm和Lloc。</p><figure><img src="../postimages/UnionFormer/image-20240618124653610.png"alt="image-20240618124653610" /><figcaption aria-hidden="true">image-20240618124653610</figcaption></figure><h2 id="特征交互编码">特征交互编码</h2><h3 id="rgb和噪声视图表示">  RGB和噪声视图表示。</h3><p>  在特征编码阶段，我们利用一个双流结构来利用来自RGB和噪声视图的线索。RGB流被设计为捕获视觉上明显的篡改伪影，而噪声流旨在探索被篡改区域和真实区域之间的分布不一致性。我们利用[7]中提出的可学习约束卷积层将RGB图像转换为噪声视图。如第2节所述，被篡改区域及其周围环境的边缘表现出更明显的篡改线索。因此，我们增强了两个流中的高频边缘信息，将网络的响应集中在被篡改的区域。具体来说，我们利用离散余弦变换（DCT）将图像数据X转换为频域，然后应用高通滤波器得到高频分量。然后，我们将高频分量转换回空间域，以促进特征交互和保持局部一致性。因此，我们得到的边缘增强信息H如下：<spanclass="math display">\[H=\mathcal{T}_d^{-1}\left(\mathcal{F}_h\left(\mathcal{T}_d(X),\beta\right)\right)\]</span>  其中Td表示DCT，Fh表示高通滤波器，β为阈值。我们将X和N分别输入到BSFI-Net中，以及H来进行特征编码，如图2所示。</p><h3 id="边界敏感特征交互网络">  边界敏感特征交互网络。</h3><p>  除了增强边界响应外，集成局部特征和全局表示对图像伪造检测也至关重要。这就要求进行全面分析在不同尺度上的图像内部的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-Transformer并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><br/>图2.BSFI-Net的概述。FCU表示特征耦合单元，BOB表示边界向块。</p><p>  如图2所示，CNN分支作为主分支，以一个RGB或噪声图像作为输入，对局部信息进行编码。变压器分支以输入作为边缘增强信息H，引导CNN分支聚焦于被篡改的区域，并将图像补丁之间的长距离不一致传输给它。我们使用[48]提出的特征耦合单元（FCU）来消除来自CNN分支的特征映射和来自transformer分支的补丁嵌入之间的错位。此外，我们还设计了一个面向边界的块（BOB），以方便将高级补丁一致性和边界信息从变压器分支传输到CNN分支，从而指导CNN分支。<br/>  CNN分支由5个卷积块组成，类似于ResNet构造[24]。与[16,48]一样，transformer分支由5个重复的transformer块组成，由一个多头自注意模块和一个MLP块组成。采用与ViT[16]相同的令牌化操作。在FCU中，在添加补丁嵌入和CNN特征之前，使用1×1的卷积和重新采样来对齐通道和空间维度。在BOB中，CNN分支的特征映射被输入1×1卷积层、批归一化层、s型层，并通过双线性插值上采样到高分辨率。然后，将来自CNN分支的特征与长距离判别权值进行元素级乘法。我们将BSFI-Net作为特征编码器进行预训练，生成RGB和噪声视图表示，特征金字塔网络[38]基于中间特征映射{C2、C3、C4、C5}生成两个特征金字塔fr，fn。培训细节详见第4.1节。</p><h2 id="特征对比性协作">特征对比性协作</h2><p>  在特征协作阶段，受[51,56]的启发，我们首先使用一个基于RGB特征金字塔fr的区域建议网络（RPN）来生成一组感兴趣的区域（RoIs）。然后，我们利用RoIAlign[25]从两个流的特征金字塔fr和fn中提取RoIs的信息。除了特征连接之外，我们还采用对比监督来促进两个视图之间的协作。我们将来自不同流的被篡改的建议视为积极建议，被篡改的建议和真实建议被指定为负对。在InfoNCE损失[47,67]之后，对比度损失被定义为：<span class="math display">\[\begin{aligned}\mathcal{L}_{\mathrm{con}}=-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{1})}-\frac{1}{N}\sum_{i}\log\frac{\exp(s_{0})}{\exp(s_{0})+\sum_{j}\exp(s_{2})}\end{aligned}\]</span>  式中，s0表示正对之间的相似性，s1表示RGB篡改嵌入与噪声真实嵌入之间的相似性，s2表示RGB真实嵌入与噪声篡改嵌入之间的相似性。对比损失Lcon引入统一学习监督，将在第3.3节进行讨论。</p><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><h2 id="具有多尺度监督下的统一学习">具有多尺度监督下的统一学习</h2><p>  <strong>Transformer编码器。</strong>我们的统一学习模块是一个仅限编码器的transformer架构，它处理融合的提议嵌入二，以及它们的特定位置编码作为输入。在转换器编码器的每一层中，自我注意机制通过不同的建议嵌入来聚合信息，并捕获它们的长距离依赖关系，这意味着对象的一致性。详细地说，我们使用了一个变压器解码器，具有六层，宽度为512，和8个注意头。变压器内的前馈网络（FFN）的隐藏大小为2048。在转换器编码器之后，我们生成判别查询嵌入DI，并输入统一操作判别表示（UMDR）部分，以生成三个子任务的预测，即。对象一致性建模、图像操作检测和定位。</p><p>  <strong>统一伪造判别表示</strong>。在转换器编码器之后，DI中的每个篡改判别查询都表示对应建议的三个视图中的篡改线索。图3显示了三个子任务的学习过程。</p><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure><p>图3。多尺度监督下的UMDR学习。图像内部在不同尺度上的不一致性。受[48]的启发，我们提出了一种名为BSFI-Net的cnn-变压器并发网络，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p><p>  UMDR是在真实性分类、对象一致性建模和操作定位分支的监督下学习的。与DETR[9]和SOLQ[12]一样，分类分支是一个完全连接的（FC）层，用来预测真实性可信度Pˆc。目标一致性建模分支是一个多层感知（MLP），隐藏大小为256，用于预测目标空间信息Pˆo。操作定位分支也是一个隐藏大小为1024的多层感知来预测定位掩码向量Pˆm。对前两个分支机构的监管类似于DETR[9]。在第三个分支中，我们利用对地面真实掩码进行编码得到的掩模向量作为监督信息。在推理过程中，将压缩后的编码过程应用于Pˆm来重构定位掩码。在压缩编码中，我们利用主成分分析（PCA）将二维空间二值掩模转换为一维掩模向量。</p><p>  <strong>损失函数。</strong>UnionFormer监督的总体损失职能可表示为：<spanclass="math display">\[\mathcal{L}_{union}=\lambda_{cls}\cdot\mathcal{L}_{cls}+\mathcal{L}_{ocm}+\lambda_{loc}\cdot\mathcal{L}_{loc}+\beta\cdot\mathcal{L}_{con},\]</span>  其中Lcls表示分类的focal损失[39]。Lloc表示定位掩码向量监督的L1损失。Lcon是在第3.2节中引入的对比性学习损失。λcls、λloc、β是相应的调制系数。Locm是对象一致性建模的损失，其定义为：<spanclass="math display">\[\mathcal{L}_{\mathrm{ocm}}=\lambda_{L_1}\cdot\mathcal{L}_{L_1}+\lambda_{gious}\cdot\mathcal{L}_{gious}\]</span>  其中LL1和Lleam为L1损失和广义IoU损失[52]，与DETR相同。λL1和λgious是对应的系数。在[12]之后，Lloc不包括在二部匹配过程中。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>  <strong>训练</strong>。我们使用了一个大规模的训练数据集，包括各种类型的篡改和真实的图像。它分为五个部分：1) CASIA v2 [14]，2)Fantastic Reality[32]，3)Tampered COCO,，来自COCO2017数据集[37]，4)Tampered RAISE，基于RAISE数据集[11]构建，5)从COCO2017和RAISE数据集中选择的原始图像。我们在合成数据中随机添加高斯噪声或应用JPEG压缩来模拟现实场景中的视觉质量和篡改轨迹。在训练过程中，我们依次分三个阶段对BSFI-Net、RPN和UnionFormer进行训练。</p><p>  <strong>测试。</strong>为了全面评估和比较我们的模型与各种最先进的方法，我们使用了6个公开可用的测试数据集和另一个由混合扩散模型[4]创建的超真实篡改图像数据集。具体来说，我们使用了CASIAv1 [14]、Columbia[26]、Coverage[61]、NIST16 [22]、IMD20 [46]和CocoGlide[23]。然后，我们构建了BDNIE，包括512张由先进的混合扩散模型生成的超真实的假图像，用于文本驱动的自然图像编辑。训练和测试数据的细节载于补充资料。</p><p>  <strong>评价指标。</strong>我们评估了该方法在图像篡改检测和定位任务中的性能。对于定位图像操作的任务，我们报告了像素级的曲线下面积（AUC）和F1分数，同时使用最佳的和固定的0.5阈值。对于[23]之后的检测任务，我们采用图像级AUC和平衡精度，同时考虑假报警和遗漏检测，在这种情况下，阈值设置为0.5。为了保证比较的公平性和准确性，我们从文献[23,59]中取出了其他方法的一些结果值。</p><p>  <strong>实施细节。</strong>BSFI-Net采用AdamW优化器[41]进行了100个周期的交叉熵损失训练，批处理大小为512，权重衰减为0.05。初始学习速率被设置为0.001，并在余弦时间表中衰减。</p><p>  在与Lunion一起训练完整的UnionFormer时，受[56,63]的启发，我们采用36周期（3×）计划来训练UnionFormer进行2.7×105次迭代，批大小为16。在这个阶段还使用了一个AdamW优化器。学习速率在开始时被设置为10−4，并在1.8×105和2.4×105迭代时乘以0.1。</p><h2 id="与最先进的技术相比较">与最先进的技术相比较</h2><p>  <strong>Baseline。</strong>为了确保公平和准确的比较，我们只选择了最先进的方法，其中作者提供了预训练的模型，发布的源代码，或在通用标准[27,40,59]下进行评估。为了减少偏差，我们只考虑了在不与测试数据集重叠的数据集上训练的方法或版本。详细地说，我们包括了7种最先进的方法：MantraNet[62]，SPAN[27]，PSCC-Net[40]，MVSS-Net[13]，CAT-Netv2[34]，ObjectFormer[59]，和TruFor[23]。</p><p>  <strong>定位结果。</strong>表2和表1分别显示了基于像素级AUC和F1评分指标的图像篡改定位结果。排名最高的方法用粗体表示，一条水平线表示排名第二的方法，在表4和表3中也采用了相同的注释。</p><figure><img src="../postimages/UnionFormer/image-20240617115640307.png"alt="image-20240617115640307" /><figcaption aria-hidden="true">image-20240617115640307</figcaption></figure><p>  我们的方法在所有数据集上展示了像素级AUC评估的最佳性能。</p><figure><img src="../postimages/UnionFormer/image-20240617115601335.png"alt="image-20240617115601335" /><figcaption aria-hidden="true">image-20240617115601335</figcaption></figure><p>  对于f1评估，我们的方法在所有数据集上排名最好或第二。平均而言，无论是否使用最优或固定的阈值，我们都获得了显著的优势。事实上，在包含基于扩散的局部操作的相对新颖的CocoGlide数据集上，我们在两个阈值上分别比排名第二的TruOfor高出2.2%和1.3%。这是由于联合前体构建的对象视图伪影表达式，它可以揭示由扩散模型生成的区域和真实区域之间的不一致性。这些比较表明，我们的方法具有较强的泛化和捕获篡改伪的能力。</p><p>  <strong>检测结果。</strong>表4为篡改检测的比较结果。</p><figure><img src="../postimages/UnionFormer/image-20240617115836976.png"alt="image-20240617115836976" /><figcaption aria-hidden="true">image-20240617115836976</figcaption></figure><p>  在[23]之后，我们使用定位映射的最大值作为未明确为检测任务设计的方法的检测统计量。UnionFormer在除Columbia外的所有数据集上都取得了最佳的性能，并在平均结果上显示了显著的优势，无论是通过AUC还是平衡精度测量。正如[13,23]中提到的，精度对阈值选择很敏感，如果没有良好校准的数据集，很难确定。然而，我们的方法和次要的TruFor在这个要求很高的场景中取得了值得称赞的结果。我们在平均AUC和精度上分别保持了2.5%和2%的领先优势。这一优势主要归因于我们的框架的统一学习过程。统一学习通常会促进对定位和检测任务的相互增强。通过统一的操作鉴别表示，掌握了两个子任务，进一步提高了模型的性能。</p><p>  <strong>鲁棒性评估。</strong>我们通过对NIST16数据集图像应用图像失真，验证了UnionFormer的鲁棒性。在[40,59]之后，我们包括了四种类型的畸变：1)将图像的大小改变到不同的尺度；2)应用核大小为k的高斯模糊；3)添加以标准偏差σ为特征的高斯噪声；4)对图像进行JPEG压缩，使用质量因子q。我们比较了像素级AUC与其他方法的性能。表3显示，我们的方法对各种失真操作表现出鲁棒性，优于其他方法。</p><figure><img src="../postimages/UnionFormer/image-20240617120221607.png"alt="image-20240617120221607" /><figcaption aria-hidden="true">image-20240617120221607</figcaption></figure><h1 id="可视化结果">可视化结果</h1><h2 id="定性比较">定性比较</h2><p>  <img src="../postimages/UnionFormer/image-20240618103731575.png"alt="image-20240618103731575" /></p><p>  图4显示了跨不同数据集的定位结果。我们的方法可以准确地定位被篡改的区域，预测更详细和清晰的边界。这是由于我们的多视图特征捕获和BSFI-Net，其中频率信息增强了边缘响应，而分支之间的交互作用增强了特征的泛化和识别。由于对对象视图线索的建模和统一的学习框架，我们的方法在具有挑战性的BDNIE数据集上取得了令人满意的结果，而其他方法都失败了。</p><h2 id="不同视图表示法的可视化">不同视图表示法的可视化</h2><p>  在图5中，我们可视化了BSFI-Net中变压器分支的噪声特征和边缘引导特征。</p><figure><img src="../postimages/UnionFormer/image-20240618103938959.png"alt="image-20240618103938959" /><figcaption aria-hidden="true">image-20240618103938959</figcaption></figure><p>如列1到4所示，一些图像在RGB视图中可能看起来很自然，但它们被篡改/真实的部分很容易在频域或噪声视图中被容易区分出来。第5列和第6列显示了由一个CNN分支和BSFI-Net的双分支生成的RGB特性。与只使用CNN分支相比，BSFI-Net更准确地激活了被篡改的区域，这得益于变压器分支提供的边缘引导和长距离线索。</p><p>  此外，我们还定量地分析了对象视图，如图6所示。</p><figure><img src="../postimages/UnionFormer/image-20240618104100130.png"alt="image-20240618104100130" /><figcaption aria-hidden="true">image-20240618104100130</figcaption></figure><p>  在统一学习阶段，我们从transformer编码器中推导出亲和矩阵Ai。基于Ai，我们随机选择提案嵌入的一个子集，计算它们与其他建议的平均亲和力，记为ei。然后将ei归一化到范围[0,1]，并作为一个颜色系数来可视化建议，较浅的颜色表示较低的亲和力。结果表明，使用伪造物体的提案与其他区域的平均亲和力较低，这表明UMDR能够捕捉真实物体和虚假物体之间的不一致性。</p><h1 id="消融研究">消融研究</h1><p>  我们进行了消融研究，以评估我们的方法中关键成分的影响。定量结果见表5。</p><figure><img src="../postimages/UnionFormer/image-20240618104307449.png"alt="image-20240618104307449" /><figcaption aria-hidden="true">image-20240618104307449</figcaption></figure><p>  我们可以观察到，通过在第一个基线模型上添加噪声流，CASIAv1的AUC得分增加8.7%，NIST 16增加8.3%，同时进一步增加对象视图表示，CASIAv1继续增加10.7%，NIST16继续增加7.4%。这证明了噪声和对象视图表示的有效性。此外，当缺乏对比监督，或BSFI-Net被ResNet-50[24]取代时，模型的性能会显著下降。这突出了两个流之间的交互的有效性和BSFI-Net在描述伪造制品方面的特殊能力。</p><p>  BSFI-Net中的BOB和FCU模块改善了其两个分支之间的交互作用，并有效地消除了它们之间的特征失调。当单独去除BOB或FCU时，整体模型在NIST16数据集上的定位AUC得分分别下降了4.8%和6.3%。</p><p>  我们进一步进行了实验，研究了UMDR中几个关键因素的影响。λloc，Locm，掩码向量维度nv，以及压缩编码的类型。</p><figure><img src="../postimages/UnionFormer/image-20240618104459423.png"alt="image-20240618104459423" /><figcaption aria-hidden="true">image-20240618104459423</figcaption></figure><p>  我们比较了三种压缩编码方法：稀疏编码[15]、离散余弦变换（DCT）[2]和主成分分析（PCA）[1]。如表6所示，当设置对比损失时，以PCA为编码类型，并将λloc和Locm分别设置为1和256时，该模型在NIST16数据集上表现最好。</p><h1 id="结论">结论</h1><p>  在本文中，我们介绍了UnionFormer，一个联合学习transformer框架，它利用来自三个不同视图的线索来进行图像操作检测和定位。UnionFormer使用BSFI-Net作为特征编码器，在RGB和噪声视图下提取具有高度区分性的特征。然后，通过三个任务的统一学习过程，UnionFormer建模了对象之间的不连续性，即对象视图表示，并学习统一的判别表示。从三种观点整合信息的统一表示具有较强的通用性和区分性。它可以准确地识别各种图像操作，无论是传统的手动编辑还是基于扩散模型的自然语言驱动的篡改。此外，统一的学习框架使子任务的相互增强，实现了高精度的检测和定位。在不同的数据集上进行的综合实验证明了该方法的有效性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告1</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/</url>
      
        <content type="html"><![CDATA[<h1 id="图像超分辨率的深度学习综述">图像超分辨率的深度学习：综述</h1><p><ahref="%5BDeep%20Learning%20for%20Image%20Super-Resolution:%20A%20Survey%20%7C%20IEEE%20Journals%20&amp;%20Magazine%20%7C%20IEEE%20Xplore%5D(https://ieeexplore.ieee.org/abstract/document/9044873)">TPAMI2020</a></p><p>本综述的主要贡献有三个方面：</p><p>​  1)我们对基于深度学习的图像超分辨率技术进行了全面的回顾，包括问题设置、基准数据集、性能指标、具有深度学习的SR方法家族、特定领域的SR应用等。</p><p>​  2)我们以层次化和结构化的方式系统地概述了基于深度学习的SR技术的最新进展，并总结了每个组件对于一个有效的SR解决方案的优点和局限性。</p><p>​  3)我们讨论了这些挑战和开放的问题，并确定了新的趋势和未来的发展方向，为社区提供了一个深刻的指导。</p><p><strong><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240606164746361.png"alt="image-20240606164746361" /></strong></p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094212179.png"alt="image-20240607094212179" /><figcaption aria-hidden="true">image-20240607094212179</figcaption></figure><h1 id="选取网络">选取网络</h1><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/SR.drawio.png"alt="SR.drawio" /><figcaption aria-hidden="true">SR.drawio</figcaption></figure><p>​  SRCNN</p><p>​  SRResNet</p><p>​  VDSR</p><p>​  CARN</p><p>​  MemNet</p><h1 id="最新图像超分辨率">最新图像超分辨率</h1><p><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28194">AAAI2024</a></p><p>AdaFormer: Efficient Transformer with Adaptive Token Sparsificationfor Image Super-resolution</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A1/image-20240607094111731.png"alt="image-20240607094111731" /><figcaption aria-hidden="true">image-20240607094111731</figcaption></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习报告</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<p>《基于深度学习的图像增强算法》——图像超分辨</p><p>基本要求：</p><ul><li><p>训练集T91-train,测试集Set5-test，不能更改。</p></li><li><p>需要分析网络架构不同所引起的性能变化，并在提交报告中对比分析。（可以是模型不同，也可以只是层数不同）</p></li><li><p>报告内容必须包含数据处理部分、模型部分、训练部分和测试部分。</p></li><li><p>超分辨任务b可以只选择放大倍数为4倍（横纵各4倍，图像大16倍）。</p></li><li><p>老师使用的模型：</p><ul><li><p>SRCNN:</p><ul><li><p>结构：</p><ul><li><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529181952910.png"alt="image-20240529181952910" /><figcaption aria-hidden="true">image-20240529181952910</figcaption></figure></li></ul></li><li><p>损失：nn.MSELoss</p></li><li><p>优化器：optim.Adam</p></li><li><p>训练批次：200轮<br/> <br/> - 评价指标：Peak Signal-to-Noise Ratio(PSNR)<br/> <br/> -<code>&lt;br/&gt;                def calc_psnr(img1, img2):&lt;br/&gt;                    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))&lt;br/&gt;</code></p></li></ul></li></ul></li><li><p>可以比较的模型：（完成意味着代码写完了，可以跑起来，但是效果需要针对训练集进行微调，延续老师代码的损失、优化器和学习率）</p><ul><li><p>初始模型SRCNN<br/> - 输入为16*1*76*76<br/> -输出为16*1*76*76<br/> - 来源于ResNet的模型SRResNet(完成，训练中)<br/> -输入为16*3*19*19<br/> - 输出为16*3*76*76<br/> - best epoch: 166, psnr:24.29<br/> - 来源于DenseNet的模型SRDenseNet <br/> - 基于递归的方法<br/>- DRCN<br/> - DRRN<br/> - CARN<br/> -VDSR(完成，但是在第三个epoch，损失就不在降低了，效果也不如SRCNN，所以需要针对训练集进行微调)</p></li><li><p>基于像素的方法：<br/> - 生成式对抗性网络（GANs）：<br/> -SRGAN<br/> - ESRGAN<br/> - 基于光流的方法</p></li></ul></li><li><p>可以使用的对比评价指标</p><ul><li><p>峰值信噪比（PSNR）：峰值信噪比（PSNR）是评价SISR重建质量最广泛使用的技术之一。它表示SR图像ˆy与实际图像y之间的最大像素值L与均方误差（MSE）之比。</p></li><li><p>结构相似度指数（SSIM）：SSIM和PSNR一样，是一种流行的评价方法，侧重于图像之间结构特征的差异。它通过比较亮度、对比度和结构来独立地捕获结构上的相似性。SSIM估计一个图像y的亮度µy为强度的平均值，而它估计对比度σy为其标准差。</p></li><li><p>平均意见评分（MOS）：MOS是一种主观的测量方法，利用人类的感知质量来评估生成的SR图像。人类观众会看到SR图像，并要求他们进行质量评分，然后映射到数值，然后取平均值。通常，这些范围从1（坏）到5（好），但可能有不同的[15]。虽然这种方法是对人类感知的直接评估，但与客观指标相比，进行它更耗时和麻烦。此外，由于这个度量标准的高度主观性，它很容易受到偏见的影响。</p></li></ul></li></ul><p>后面是学习一些综述</p><h1 id="超分辨率评价指标">超分辨率评价指标：</h1><p>​  图像质量评估（IQA）</p><p>​  许多特性与优秀的图像质量有关，如锐度，对比度，或没有噪声。因此，对SR模型的公平评价具有挑战性。本节展示了属于图像质量评估（IQA）范畴的不同评估方法。广义上说，IQA指的是任何基于对人类观众的感知评估的度量，即应用SR方法后图像的真实程度。IQA可以是主观的（例如，人类评分者）或客观的（例如，正式的指标）。</p><p>​  1)平均意见得分（MOS, Mean OpinionScore）：数字图像最终是为人类观看的。因此，评估图像的最合适的方法是主观评价[12]，[13]。一种常用的主观IQA方法是平均意见评分（MOS）。人类观众给有质量分数的图像打分，通常是1（差）到5（好）。MOS是所有评分的算术平均值。尽管具有可靠性，但调动人力资源是耗时和麻烦的，特别是对于大型数据集。</p><p>​  2)峰值信噪比（PSNR, Peak Signal-to-NoiseRatio）：由于近年来产生的大量图像和主观测量的弱点，客观评估质量具有无可争辩的重要性。一种流行的目标质量测量方法是峰值信噪比（PSNR）。它是可能的最大像素值L（8位表示为255）与参考图像的均方误差（MSE）之间的比率。给定近似值$ y $ 和地面真实值y，PSNR是一个使用分贝尺度[dB]的对数量： <spanclass="math display">\[\mathrm{PSNR}\left(\mathbf{y},\mathbf{\hat{y}}\right)=10\cdot\log_{10}\frac{L^2}{\frac{1}{N_{\mathbf{y}}}\sum_{p\in\Omega_{\mathbf{y}}}\left[\mathbf{y}_p-\mathbf{\hat{y}}_p\right]^2}\]</span>​  虽然它被广泛用作SR模型的评价标准，但在真实场景中往往导致平庸的结果。它关注像素水平的差异，而不是哺乳动物的视觉感知，后者更吸引结构[14]。随后，它与主观感知质量的相关性较差。像素的轻微变化（例如，移动）可能会导致一个显著的PSNR降低，而人类几乎不知道这种差异。因此，新的指标关注于图像中更多的结构性特征。</p><h1 id="srnet">SRnet</h1><h2 id="简单的网络">简单的网络</h2><p>​  简单的网络是一种主要应用卷积链的体系结构。它们很容易理解，并且由于它们的大小，通常只使用最少的计算资源。大多数这些体系结构都可以在基于dl的SR的早期找到，因为它们的性能低于最先进的水平。此外，DL的“越深越好”的范式并不能很好地适用于简单的网络，因为正在消失/爆炸的梯度[98]。</p><p>​  图6显示了不同的简单网络设计。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182208483.png"alt="image-20240529182208483" /><figcaption aria-hidden="true">image-20240529182208483</figcaption></figure><p>​  第一个引入SR数据集的CNN是由Dong等人[39]提出的SRCNN（2014）。它使用双边缘预上采样来匹配地面真实空间大小（见第7.1节）。随后，它由三个卷积层组成，这遵循了图像恢复中流行的策略：补丁提取、非线性映射和重建。SRCNN的作者声称，应用更多的图层会损害性能，这与DL范式“越深越好”的[99]相矛盾。</p><p>​  如下所示，这个观察结果是错误的，需要更高级的构建块才能正确工作，例如，像VDSR[98]中那样的残差连接。</p><p>​  在他们的后续论文中，作者探索了各种加速SRCNN的方法，导致FSRCNN（2016）[33]利用了三个主要技巧：</p><p>​    首先，他们减少了卷积层内核的大小。</p><p>​    其次，他们使用了一个1x1的卷积层来增强和减少在使用3x3卷积的特征处理之前和之后的通道维度。</p><p>​    第三，他们采用了带有换位卷积的后上采样，这是提高速度的主要原因（见第7.1节）。</p><p>​  令人惊讶的是，它们在获得更快的同时优于SRCNN。</p><p>​  一年后，LapSRN（2017）[44]被提出，其关键贡献是一个拉普拉斯金字塔结构[100]，可以实现逐步上采样（见第7.1节）。它以粗分辨率的特征图作为输入，并预测高频残差，逐步细化每个金字塔层的SR重建。为此，在一个前馈通道中预测多尺度图像是可行的，从而促进了资源感知的应用程序。</p><p>​  简单的网络架构设计主要出现在基于dl的SR的早期，因为由于它们的大小，它们学习复杂结构的能力有限。最近，研究人员关注的是更有深度的网络，无论是残差网络，还是基于循环网络的合成深度。下面的部分将介绍这两种可能性。</p><h2 id="残差网络">残差网络</h2><p>​  残差的网络使用跳过连接来跳过图层。增加跳过连接的主要原因有两个：为了避免梯度的消失和降低精度饱和问题[99]。对于SR来说，引入跳过连接开启了深度构建模型的世界。其主要优点是深度架构用大的接受域替代卷积，这对于捕获重要特征至关重要。SRCNN的作者指出，“越深越好”的范式并不支持SR。相比之下，Kim等人用VDSR（2015）[98]驳斥了这一说法，并表明非常深的网络可以显著改善SR，如图7所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182809721.png"alt="image-20240529182809721" /><figcaption aria-hidden="true">image-20240529182809721</figcaption></figure><p>​  他们使用了来自其他DL方法的两种见解：首先，他们应用了一个著名的架构VGG-19[24]作为特征提取块。其次，他们使用了从插值层到最后一层的剩余连接。因此，VGG-19特征提取块在插值中添加了高频细节，导致目标分布呈正态分布，极大地降低了学习难度，如图8所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529182849749.png"alt="image-20240529182849749" /><figcaption aria-hidden="true">image-20240529182849749</figcaption></figure><p>​  此外，由于插值中高频的稀疏表示，它减少了消失/爆炸梯度。这一优点产生了跟踪残余网络的趋势，从而增加了使用的残差的数量。</p><p>​  一个例子是RED-Net（2016）[101]，它将U-Net[102]架构适应SR。它结合了一个下采样编码器和一个上采样解码器网络，它对给定的输出进行下采样以提取特征，然后将特征映射上采样到目标空间大小。在此过程中，RED-Net利用在整个下采样过程中获得的剩余信息扩展了上采样操作，从而减少了消失的梯度。因此，它在几个缩放因子上都优于SRCNN。</p><p>​  另一个例子采用了SRGAN（2016）[13]的出版物中，即由多个残差单元组成的RenseNet[104]，它将残差信息发送给所有后来出现的卷积操作。此外，作者还比较了这些架构应用于像素和对抗性损失（见第3节）。SRResNet由多个堆叠的剩余单元组成，允许高级特征提取通过大量的求和操作访问低级特征信息。因此，它通过提供一个简单的反向传播路径来简化优化。与SRResNet一样，SRDenseNet[40]应用密集的残余块，它利用更多的残余连接来允许直接路径到更早的层。相比之下，SRResNet的性能大大优于SRCNN、DRCN和ESPCN。对SRDenseNet的一个扩展是在2018年提出的剩余密集网络[42]，它在密集块上包含了一个额外的剩余连接。</p><p>​  密集残差拉普拉斯网络（DRLN）[6]是SRDenseNet的扩展，是一种基于后上采样、通道注意的残差网络，并取得了最先进的竞争结果。每个密集块后面都有一个基于拉普拉斯金字塔注意的模块，它学习特征映射之间的层间和层内依赖关系。它在每个DRLM中逐步加权子频带特征，类似于HAN（连接不同深度的各种特征图）。</p><p>​  残差块的另一种变体是信息蒸馏网络（IDN）[43]。它使用剩余连接将特征映射的一部分积累到以后的层。给定六个卷积层，它将特征映射在中间分成两个部分。然后，其中一部分被最后三层进一步处理，并添加到输入部分和另一部分的连接中。简而言之，利用剩余连接的网络是最先进的。它们有效地传播信息的能力有助于对抗消失/爆炸的梯度，从而产生出色的性能。有时，剩余块的使用会与其他体系结构相结合，例如基于循环的网络。</p><h2 id="基于递归的网络">基于递归的网络</h2><p>​  人工深度可以通过重复来完成，其中接受野对于获取重要信息至关重要，通过重复相同的操作而扩大。此外，递归性减少了参数的数量，这有助于对抗小型设备的过拟合和内存消耗。它是通过不引入新的参数而多次应用卷积层来实现的。</p><p>​  Kim等人[105]通过DRCN（2015）引入了第一个基于循环的SR网络。它使用相同的卷积层多达16次，随后的重建层考虑所有递归输出进行最终估计。然而，他们观察到，他们的深度递归网络很难训练，但通过跳过连接和递归监督来缓解它，本质上是辅助训练。图9显示了DRCN。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529190806493.png"alt="image-20240529190806493" /><figcaption aria-hidden="true">image-20240529190806493</figcaption></figure><p>​  结合了DRCN [105]和VDSR[98]的核心思想，DRRN（2017）[65]在一个递归块结构中使用了几个堆叠的残差单元。此外，它使用的参数分别比VDSR和DRCN少6倍和14倍，同时获得了更好的结果。与DRCN相反，DRRN在剩余单元之间共享权值集，而不是在所有递归应用的卷积层中共享一个权值。通过强调多路径，DRNN比DRCN（总共52个）训练得更稳定，递归程度更深。</p><p>​  受DRCN的启发，Tai等人介绍了MemNet（2017）[92]。主要贡献是由递归单元和门单元组成的内存块，以挖掘持久内存。该递归单元被应用了多次，类似于DRCN。输出被连接并发送到一个栅极单元，这是一个简单的1x1卷积层。自适应门单元控制先前信息的量和保留的当前状态。图9显示MemNet。引入门对序列到序列任务（如LSTM[106]）的影响是开创性的，但深入了解其对SR任务的影响标志着一个开放的研究问题。</p><p>​  受DRRN的启发，DSRN（2018）[97]的作者探索了一种具有多路径网络的双态设计。它介绍了两种状态，一种在HR操作，另一种在LR空间，它们共同利用LR和HR信号。通过延迟反馈[107]，信号在lr到hr和hr到lr这两个空间之间反复交换。从lr到hr，它使用了一个转置的卷积层来进行上采样。HR-to-LR是通过分层卷积来执行的。最终的近似值使用了在人力资源空间中所做的所有估计值的平均值。因此，它应用了迭代上下上采样的扩展公式（见第7.1节）。这两种状态使用的参数多于DRRN，但小于DRCN。然而，适当地开发双态设计在未来需要进行更多的探索。</p><p>​  超分辨率反馈网络（SRFBN，2019）[64]也在使用反馈[108]。最基本的贡献是反馈块（FB）作为一个实际的循环细胞。FB使用多个具有密集跳跃连接的迭代上下采样来产生高水平的判别特征。SRFBN为每次迭代生成一个SR图像，并且FB块接收前一次迭代的输出。它尝试在每次迭代中为单个退化任务生成相同的SR图像。对于更复杂的案例，它通过课程学习通过每次迭代返回更好、更好质量的图像（见第6.1节）。与其他框架相比，SRFBN已经显示出了显著的改进，但在未来还需要更多的研究。</p><p>​  Liu等人提出了NLRN（2018）[109]，它提供了一个非局部模块来产生自相似性的特征相关性。图像中的每个位置测量其邻近区域的每个位置的特征相关性。NRLN利用特征相关消息之间的相邻循环阶段。事实上，NLRN的表现也略好于DRCN、DRCN和MemNet。</p><p>​  然而，最近对SR中rnn的主要研究是针对MISR进行的，如视频SR[110]或元学习[111]相关任务。一般来说，基于循环的网络在保存参数方面很有趣，但其主要缺点是通过重复应用相同的操作来实现其计算开销。此外，由于时间依赖性，它们不能并行化。替代方案是轻量级架构，接下来将介绍它们。</p><h2 id="轻量级网络">轻量级网络</h2><p>​  到目前为止，我们已经引入了能够提高SR图像质量的模型，以及一些尝试去做同样的事情，但计算量较少的模型。例如，FSRCNN[33]利用更小的内核大小、后采样和1x1卷积层来增强/减少通道维度，从而比SRCNN[39]更快（见7.2节）。本工作中的另一个例子是基于循环的网络，它减少了第7.4节中所述的冗余参数。这些精益递归网络的缺点是，参数的减少是以增加操作和推理时间为代价的，这是现实世界场景的一个基本方面。例如，移动设备上的SR受到电池容量的限制，这取决于所需的计算功率。因此，轻量级体系结构明确地同时关注执行速度和内存使用情况。补充材料，在线提供，包括参数比较，和执行速度的公平比较是受欢迎的需求。</p><p>​  MDSR（2017）[38]使用多路径方法来学习具有共享参数的多个缩放因子。它有三个不相同的路径作为预处理步骤和三个路径作为上采样。对于给定的比例因子s∈{2,3,4}，MDSR在三条路径之间选择确定性的。大尺度的路径比低尺度因子的路径建立得更深。在预处理和上采样步骤之间是一个由多个剩余块组成的共享模块。该特征提取块经过训练，常用于所有的缩放因子，如图10所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191245073.png"alt="image-20240529191245073" /><figcaption aria-hidden="true">image-20240529191245073</figcaption></figure><p>​  其主要优点是，一个模型就足以在多个尺度上进行训练，从而节省了参数和内存。相比之下，其他SR模型必须在不同的尺度上独立训练，并独立保存用于多尺度应用。然而，添加一个新的缩放因子需要从头开始进行训练。其他轻量级体系结构也采用了这一想法，以实现参数高效的多尺度训练，如CARN/CARNM（2018）[112]。</p><p>​  此外，它还在残差网络[103]上实现了一种级联机制。CARN由多个级联块（见图11）和它们之间的1x1卷积组成。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191409578.png"alt="image-20240529191409578" /><figcaption aria-hidden="true">image-20240529191409578</figcaption></figure><p>​  级联块的输出将被发送到所有后续的1x1卷积中，就像在级联块本身中一样。因此，局部级联几乎与全局级联完全相同。它允许多层次的表示和稳定的训练，如残余网络。最终，它在三条路径中进行选择，通过类似于MDSR的高效亚像素层，将特征映射上采样到2倍、3倍或4倍的缩放因子。受MobileNet[113]的启发，CARN还在每个残差块组件中使用分组卷积。这允许配置模型的效率，因为选择不同的组大小和由此产生的性能是在一种权衡关系中。具有组卷积的残差块根据组卷积的大小，最多可减少计算14倍。他们测试了CARN的一种变体，它设置了组的大小，从而使计算减少最大化，并将其称为CARN-Mobile（CARN-M）。此外，他们通过允许在每个级联块中的残余块的权重共享（与非共享块相比减少了3倍），进一步减少了CARN-M的参数。</p><p>​  受IDN和IMDB[115]的启发，RFDN（2020）[114]通过使用RFDB块重新考虑了IMDB体系结构，如图12所示。RFDB块由特征蒸馏连接组成，它们将1x1的卷积级联到最后一层。此外，它使用浅层残差块（SRBs），其中只有一个3x3的卷积，来进一步处理给定的输入。最后一层是一个1x1的卷积层，它结合了所有的中间结果。最后，它应用了专门为轻量级模型设计的增强型空间注意力[114]。RFDN架构包括后续的RFDB块，并使用具有最终亚像素层的后上采样框架。</p><p>​  XLSR（2021）[116]是一个非常具有硬件感知能力和量化友好性的网络。它应用多路径来减轻卷积操作的负担，并使用1x1卷积来按像素级进行组合。每个卷积层都有一个较小的滤波器尺寸（8、16、27)。在组合之后，它将分割特征贴图，并再次应用多条路径。XLSR的一个核心方面是末端激活层，它利用了量化的好处。量化是有用的，因为它可以通过使用更多的微型位表示[117]来保存参数。不幸的是，许多移动设备都支持8位数据。因此，对在浮32或浮16中表现良好的SR模型应用uint8量化不起作用。裁剪的ReLU（限制为最大值1）作为最后一个激活层而不是典型的ReLU可以消除这个问题。然而，作者建议通过进一步的实验来寻找其他的最大值。</p><p>​  一般来说，有很多想法可以让SR模型轻量级有待发现。它们包括对现有架构的简化、量化和修剪。此外，利用SR的资源有限的设备和应用是一个日益感兴趣的领域。</p><h2 id="小波变换网络">小波变换网络</h2><p>​  不同的图像表示可以带来一些好处，比如提高计算速度。小波理论为表示和存储多分辨率图像提供了稳定的数学基础，描述了上下文和纹理信息[119]。离散小波变换（DWT）将一幅图像分解为一系列小波系数。在SR中最常见的小波是Haar小波，通过二维快速小波变换计算出来。通过对每个输出系数进行迭代重复分解，计算出小波系数。它捕获四个子波段的图像细节：平均（LL）、垂直（HL）、水平（LH）和对角线（HH）信息。DWSR（2017）[120]是第一个使用小波预测的网络之一。它使用了一个简单的网络架构来细化在一个预上采样框架中的LR和HR图像小波分解之间的差异。首先，计算放大（采用双边插值）LR图像的小波系数。然后对小波系数进行卷积层处理。然后，加入初始计算的小波系数，并采用残差连接进行波面处理。因此，卷积层学习了系数的额外细节。最后，采用二维-DWT的反过程得到SR图像，如图13所示。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529191746837.png"alt="image-20240529191746837" /><figcaption aria-hidden="true">image-20240529191746837</figcaption></figure><p>​  利用WIDN（2019）[121]提出了另一种方法，它使用平稳小波变换代替DWT来实现性能更好。与DWSR使用小波-srnet（2017）[122]的同时，我们提出了一个更复杂的模型。为了从LR图像中生成特征映射，它提供了一个由残余块组成的嵌入网络。然后，它多次应用小波变换，并利用多个小波预测网络。最后，它应用反向过程，并使用转置卷积进行上采样。该系数用于小波损失函数，而SR图像用于传统的纹理和MSE损失函数。因此，他们的网络适用于不同放大倍数的不同输入分辨率，并对MS-SR的未知高斯模糊、姿态和遮挡显示出鲁棒性。多级小波CNNS的思想也可以在以后的出版物中找到，即MWCNN（2018）[123]。</p><p>​  下面的工作应用了一种混合的方法，通过混合小波变换与其他著名的SR方法。即Zhang等人提出了一个基于小波的SRGAN（2019）框架[124]，它融合了SRGAN和小波分解的优点。生成器使用嵌入网络将输入处理到特征映射中，类似于小波-srnet。接下来，它使用一个小波预测网络来细化系数，类似于DWSR。2020年，Xue等人[125]将小波与包含通道注意和空间注意模块的残差注意块（混合注意，见下文第5节）相结合，并将其称为网络称为WRAN。在过去的几年里，小波的应用也应用于视频SR[126]。</p><p>​  一般来说，小波变换可以有效地表示图像。因此，使用这种策略的SR模型通常会降低总体模型的大小和计算成本，同时达到与最先进的架构相似的性能。然而，这一研究领域还需要更多的探索。例如，由于高频子带和低高频子带的分布，合适的归一化技术存在显著差异，或者由于高频子带的稀疏表示可能不合适，因此可以替代卷积操作。</p><h1 id="无监督超分辨率">无监督超分辨率</h1><p>​  监督SR的惊人性能归因于它们主要从许多LR-HR图像对学习自然图像的能力，大多是已知的退化映射，这在实践中通常是未知的。因此，经过监督训练的SR模型对于切实可行的用例有时是不可靠的。例如，当训练数据集生成LR图像（保留高频），然后SR模型在该数据集上进行的训练不太适合用于使用抗锯齿生成的真实LR图像（平滑图像）。使用抗混叠方法生成的LR图像（平滑图像）。此外，一些专门的应用领域缺乏LR-HR图像对数据集。因此，人们对无监督SR越来越感兴趣。我们简要地研究了这个领域，为了进一步阅读基于流的方法（退化核的密度估计），我们参考Liu等人[127]的调查。</p><h2 id="弱监督方法">弱监督方法</h2><p>​  弱监督方法使用未配对的LR和HR图像，如WESPE（2018）[128]。WESPE由两个发生器和两个鉴别器组成。第一个生成器获取一个LR图像并对其进行超级解析。第一发生器的输出构成一个SR图像，但也与电视损失[59]正则化。第二个生成器接受对第一个生成器的预测，并执行逆映射。第二个生成器的结果通过内容丢失[12]与原始输入的LR图像进行优化。这两个鉴别器取第一个生成器的SR图像，并被训练来区分预测和原始HR图像。第一鉴别器根据图像颜色将输入分类为SR或HR图像。第二个鉴别器使用图像纹理[61]来进行分类。</p><p>​  一个类似的方法是一个被称为CinCGAN（2018）[52]的周期中循环SR框架，基于CycleGAN[129]。它总共使用了四个发生器和两个鉴别器。第一个生成器取一个有噪声的LR图像，并将其映射到干净的版本。第一个鉴别器被训练来区分来自数据集的干净LR图像和预测的干净图像。第二台发电机训练逆函数。因此，它从预测的干净版本中生成有噪声图像，从而关闭了一个半周期周期的第一个周期。第三个生成器特别有趣，因为它是实际的SR模型，它将LR图像上采样到HR。第二个鉴别器被训练来区分预测的和数据集的HR图像。最后一个生成器将预测的HR图像映射到有噪声的LR图像，从而关闭CycleGAN的第二个周期。除了其有希望的结果和类似的方法[130]外，它还需要进一步的研究来降低学习难度和计算成本。</p><h2 id="零次学习">零次学习</h2><p>​  零次学习或一次学习与对物体的训练和对从未观察到的完全不同的物体的测试有关。理想情况下，如果将“斑马看起来像条纹马”转换为[132]马，那么用马训练的分类器应该识别斑马。关于SR的零次学习的第一个出版物是ZSSR（2017）[87]。我们的目标是只训练手头的一张图像，一张独一无二的图像。ZSSR对LR图像进行下采样，并训练CNN以反转退化映射。训练后的CNN最终直接用于LR图像。令人惊讶的是，该方法的效果优于SRCNN，与VDSR比较接近。</p><p>​  在此基础上，提出了一种基于深度信息[134]的退化仿真网络（DSN，2020）[133]，以避免预定义的退化内核。它使用双循环训练来同时学习未知的退化核和SR图像的重建。MZSR[135]将ZSSR设置与元学习合并，并使用一个外部数据集来学习不同的模糊内核，这被称为元学习领域中的任务分布。然后将SR模型在类似于ZSSR的降采样图像上进行训练，并从元测试阶段返回模糊核。这种方法的好处是，它使SR模型更快地学习特定信息，比纯ZSSR性能更好。SR的零镜头学习标志着进一步研究的一个令人兴奋的领域，因为它非常实用，特别是对于特定于应用程序的数据集很少或不存在的应用程序。</p><h2 id="深度图像先验">深度图像先验</h2><p>​  Ulyanov等人[131]提出了深度图像先验（DIP），这与在大数据集上训练CNN的传统范式相矛盾。它使用一个CNN来预测降采样时的LR图像，给定一些随机的噪声，而不是一个实际的图像。因此，它遵循了ZSSR的策略，只使用LR图像。然而，它将输入固定为随机噪声，并对预测采用固定的降采样方法进行修正。此外，它还优化了降采样预测与LR图像之间的差异。然后，CNN在不使用固定降采样方法的情况下生成SR图像。因此，它利用噪声生成一个SR图像，而不是转换一个原始图像。ZSSR和DIP之间的差异见图14。</p><figure><imgsrc="./../postimages/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A/image-20240529192743577.png"alt="image-20240529192743577" /><figcaption aria-hidden="true">image-20240529192743577</figcaption></figure><p>图14。零镜头超分辨率（ZSSR）[87]和深度图像先验（DIP）[131]。ZSSR使用LR图像进行降采样，SR模型学习反向降采样。对于LR图像的SR图像的最终预测，它直接应用于LR图像。DIP使用固定噪声作为输入，预测SR图像，并进行降采样，以优化降采样图像与给定LR图像之间的差异。最终的预测使用SR模型来预测SR图像，但跳过了退化映射。</p><p>​  令人惊讶的是，研究结果与LapSRN[136]很接近。不幸的是，它是一篇关于图像先验的理论出版物，而且正如作者自己所说的那样，这种方法太慢了，对大多数实际应用都不太有用。然而，它并不排除未来可以提高DIP关于更好的图像重建质量，特别是运行时的实用性的想法。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM1</title>
      <link href="/SAM1/"/>
      <url>/SAM1/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything Model for Medical Images?</p><p>发表于MICCAI 2024</p><p>Testing pipeline of SAM</p><figure><img src="./../postimages/SAM1/image-20240528220811746.png"alt="image-20240528220811746" /><figcaption aria-hidden="true">image-20240528220811746</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/yuhoo0302/Segment-Anything-Model-for-Medical-Images</span><br><span class="line">任务是医学图片的分割</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">优化器和损失函数设计：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"># Set up the optimizer, hyperparameter tuning will improve performance here</span><br><span class="line">optimizer = torch.optim.AdamW(sam_model.mask_decoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line">seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction=&#x27;mean&#x27;)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">为一个训练过程中的代码：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">outputs = []</span><br><span class="line"># do not compute gradients for image encoder and prompt encoder</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    none_grad_features = &#123;&quot;sparse&quot;: &#123;&#125;, &quot;dense&quot;: &#123;&#125;&#125;</span><br><span class="line">    for idx, image_record in enumerate(batched_input):</span><br><span class="line">        sparse_embeddings, dense_embeddings = model.prompt_encoder(</span><br><span class="line">                    points=None,</span><br><span class="line">                    boxes=image_record[&quot;box&quot;].to(device),</span><br><span class="line">                    masks=None,</span><br><span class="line">                )</span><br><span class="line">        none_grad_features[&quot;sparse&quot;][idx] = sparse_embeddings</span><br><span class="line">        none_grad_features[&quot;dense&quot;][idx] = dense_embeddings </span><br><span class="line"></span><br><span class="line">batched_loss = 0</span><br><span class="line">for id, im_record in enumerate(batched_input):</span><br><span class="line">    # low_res_masks.shape == (B, M, 256, 256) M is set to 1</span><br><span class="line">    low_res_masks, iou_predictions = model.mask_decoder(</span><br><span class="line">        image_embeddings=im_record[&quot;img_embed&quot;].unsqueeze(0).to(device), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        image_pe=model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64) !!1 = batch size</span><br><span class="line">        sparse_prompt_embeddings=none_grad_features[&quot;sparse&quot;][id], # (B, 2, 256) !!B = target num instead of batch size</span><br><span class="line">        dense_prompt_embeddings=none_grad_features[&quot;dense&quot;][id], # (B, 256, 64, 64) !!B = target num instead of batch size</span><br><span class="line">        multimask_output=False,</span><br><span class="line">    )</span><br><span class="line">    # upscale + eliminate padding + restore to ori size</span><br><span class="line">    masks = model.postprocess_masks(</span><br><span class="line">        low_res_masks,</span><br><span class="line">        input_size=tuple(im_record[&quot;size_before_pad&quot;]),</span><br><span class="line">        original_size=tuple(im_record[&quot;image_ori_size&quot;]),</span><br><span class="line">    )</span><br><span class="line">    outputs.append(&#123;</span><br><span class="line">        &quot;masks&quot;: masks,</span><br><span class="line">        &quot;iou_predictions&quot;: iou_predictions,</span><br><span class="line">        &quot;low_res_logits&quot;: low_res_masks,</span><br><span class="line">        &quot;gt2D&quot;: im_record[&quot;gt2D&quot;].to(device)</span><br><span class="line">    &#125;)</span><br><span class="line">    # first ele: 1, B, ori_H, ori_W</span><br><span class="line">    # second ele: 1, B, ori_H, ori_W</span><br><span class="line">    # considering the multi-object situation</span><br><span class="line">    batched_loss += criterion(masks.squeeze(1).unsqueeze(0), im_record[&quot;gt2D&quot;].to(device).unsqueeze(0)) </span><br><span class="line">loss = batched_loss / len(batched_input)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">epoch_loss += loss.item()</span><br></pre></td></tr></table></figure><p>Segment Anything in High Quality</p><p>发表于NeurIPS 2023</p><figure><img src="./../postimages/SAM1/image-20240528221000632.png"alt="image-20240528221000632" /><figcaption aria-hidden="true">image-20240528221000632</figcaption></figure><p>图3:HQ-SAM将HQ输出令牌和全局局部特征融合引入SAM，用于高质量掩模预测。为了保持SAM的零样本能力，轻量级HQ-Output-Token重用SAM的掩码解码器，并生成新的MLP层，用于执行具有融合HQ-Features的点向产品。在训练过程中，当我们固定预先训练的SAM的模型参数时，HQ-SAM中只有少数可学习的参数是可训练的。为了清晰起见，此处省略了提示编码器。误差校正简单地用作推理期间SAM的输出令牌和HQ输出令牌的预测logits之间的直接元素和。</p><p>损失函数设置</p><p>We supervise mask prediction of the new HQ-Output token with acombination of both BCE Loss and Dice Loss.</p><p>我们用BCE损失和Dice损失组合的联合损失监督新HQ输出token的掩码预测。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train with box、point、noise_mask</span><br><span class="line">接下来是的代码来自</span><br><span class="line">https://github.com/SysCV/SAM-HQ</span><br><span class="line">任务是SAM的高质量掩模预测问题</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">from utils.loss_mask import loss_masks</span><br><span class="line"></span><br><span class="line">net = MaskDecoderHQ(args.model_type)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">if input_type == &#x27;box&#x27;:</span><br><span class="line">dict_input[&#x27;boxes&#x27;] = labels_box[b_i:b_i+1]</span><br><span class="line">elif input_type == &#x27;point&#x27;:</span><br><span class="line">point_coords = labels_points[b_i:b_i+1]</span><br><span class="line">    dict_input[&#x27;point_coords&#x27;] = point_coords</span><br><span class="line">    dict_input[&#x27;point_labels&#x27;] = torch.ones(point_coords.shape[1], device=point_coords.device)[None,:]</span><br><span class="line">elif input_type == &#x27;noise_mask&#x27;:</span><br><span class="line">    dict_input[&#x27;mask_inputs&#x27;] = labels_noisemask[b_i:b_i+1]</span><br><span class="line">else:</span><br><span class="line">    raise NotImplementedError</span><br><span class="line">dict_input[&#x27;original_size&#x27;] = imgs[b_i].shape[:2]</span><br><span class="line">batched_input.append(dict_input)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">batched_output, interm_embeddings = sam(batched_input, multimask_output=False)</span><br><span class="line"></span><br><span class="line">batch_len = len(batched_output)</span><br><span class="line">encoder_embedding = torch.cat([batched_output[i_l][&#x27;encoder_embedding&#x27;] for i_l in range(batch_len)], dim=0)</span><br><span class="line">image_pe = [batched_output[i_l][&#x27;image_pe&#x27;] for i_l in range(batch_len)]</span><br><span class="line">sparse_embeddings = [batched_output[i_l][&#x27;sparse_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">dense_embeddings = [batched_output[i_l][&#x27;dense_embeddings&#x27;] for i_l in range(batch_len)]</span><br><span class="line">masks_hq = net(</span><br><span class="line">    image_embeddings=encoder_embedding,</span><br><span class="line">    image_pe=image_pe,</span><br><span class="line">    sparse_prompt_embeddings=sparse_embeddings,</span><br><span class="line">    dense_prompt_embeddings=dense_embeddings,</span><br><span class="line">    multimask_output=False,</span><br><span class="line">    hq_token_only=True,</span><br><span class="line">    interm_embeddings=interm_embeddings,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_mask, loss_dice = loss_masks(masks_hq, labels/255.0, len(masks_hq))</span><br><span class="line">loss = loss_mask + loss_dice</span><br><span class="line"></span><br><span class="line">loss_dict = &#123;&quot;loss_mask&quot;: loss_mask, &quot;loss_dice&quot;:loss_dice&#125;</span><br><span class="line"></span><br><span class="line"># reduce losses over all GPUs for logging purposes</span><br><span class="line">loss_dict_reduced = misc.reduce_dict(loss_dict)</span><br><span class="line">losses_reduced_scaled = sum(loss_dict_reduced.values())</span><br><span class="line">loss_value = losses_reduced_scaled.item()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line">metric_logger.update(training_loss=loss_value, **loss_dict_reduced)</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">loss_masks：</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">def loss_masks(src_masks, target_masks, num_masks, oversample_ratio=3.0):</span><br><span class="line">    &quot;&quot;&quot;Compute the losses related to the masks: the focal loss and the dice loss.</span><br><span class="line">    targets dicts must contain the key &quot;masks&quot; containing a tensor of dim [nb_target_boxes, h, w]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # No need to upsample predictions as we are using normalized coordinates :)</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # sample point_coords</span><br><span class="line">        point_coords = get_uncertain_point_coords_with_randomness(</span><br><span class="line">            src_masks,</span><br><span class="line">            lambda logits: calculate_uncertainty(logits),</span><br><span class="line">            112 * 112,</span><br><span class="line">            oversample_ratio,</span><br><span class="line">            0.75,</span><br><span class="line">        )</span><br><span class="line">        # get gt labels</span><br><span class="line">        point_labels = point_sample(</span><br><span class="line">            target_masks,</span><br><span class="line">            point_coords,</span><br><span class="line">            align_corners=False,</span><br><span class="line">        ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    point_logits = point_sample(</span><br><span class="line">        src_masks,</span><br><span class="line">        point_coords,</span><br><span class="line">        align_corners=False,</span><br><span class="line">    ).squeeze(1)</span><br><span class="line"></span><br><span class="line">    loss_mask = sigmoid_ce_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line">    loss_dice = dice_loss_jit(point_logits, point_labels, num_masks)</span><br><span class="line"></span><br><span class="line">    del src_masks</span><br><span class="line">    del target_masks</span><br><span class="line">    return loss_mask, loss_dice</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line">get_uncertain_point_coords_with_randomness：</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">    Sample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The unceratinties</span><br><span class="line">        are calculated for each point using &#x27;uncertainty_func&#x27; function that takes point&#x27;s logit</span><br><span class="line">        prediction as input.</span><br><span class="line">    See PointRend paper for details.</span><br><span class="line">    Args:</span><br><span class="line">        coarse_logits (Tensor): A tensor of shape (N, C, Hmask, Wmask) or (N, 1, Hmask, Wmask) for</span><br><span class="line">            class-specific or class-agnostic prediction.</span><br><span class="line">        uncertainty_func: A function that takes a Tensor of shape (N, C, P) or (N, 1, P) that</span><br><span class="line">            contains logit predictions for P points and returns their uncertainties as a Tensor of</span><br><span class="line">            shape (N, 1, P).</span><br><span class="line">        num_points (int): The number of points P to sample.</span><br><span class="line">        oversample_ratio (int): Oversampling parameter.</span><br><span class="line">        importance_sample_ratio (float): Ratio of points that are sampled via importnace sampling.</span><br><span class="line">    Returns:</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) that contains the coordinates of P</span><br><span class="line">            sampled points.</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">point_sample：</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.</span><br><span class="line">    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside</span><br><span class="line">    [0, 1] x [0, 1] square.</span><br><span class="line">    Args:</span><br><span class="line">        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.</span><br><span class="line">        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains</span><br><span class="line">        [0, 1] x [0, 1] normalized point coordinates.</span><br><span class="line">    Returns:</span><br><span class="line">        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains</span><br><span class="line">            features for points in `point_coords`. The features are obtained via bilinear</span><br><span class="line">            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">sigmoid_ce_loss_jit = torch.jit.script(</span><br><span class="line">    sigmoid_ce_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def sigmoid_ce_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    Returns:</span><br><span class="line">        Loss tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&quot;none&quot;)</span><br><span class="line"></span><br><span class="line">    return loss.mean(1).sum() / num_masks</span><br><span class="line">    </span><br><span class="line">dice_loss_jit = torch.jit.script(</span><br><span class="line">    dice_loss</span><br><span class="line">)  # type: torch.jit.ScriptModule</span><br><span class="line">def dice_loss(</span><br><span class="line">        inputs: torch.Tensor,</span><br><span class="line">        targets: torch.Tensor,</span><br><span class="line">        num_masks: float,</span><br><span class="line">    ):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the DICE loss, similar to generalized IOU for masks</span><br><span class="line">    Args:</span><br><span class="line">        inputs: A float tensor of arbitrary shape.</span><br><span class="line">                The predictions for each example.</span><br><span class="line">        targets: A float tensor with the same shape as inputs. Stores the binary</span><br><span class="line">                 classification label for each element in inputs</span><br><span class="line">                (0 for the negative class and 1 for the positive class).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    inputs = inputs.sigmoid()</span><br><span class="line">    inputs = inputs.flatten(1)</span><br><span class="line">    numerator = 2 * (inputs * targets).sum(-1)</span><br><span class="line">    denominator = inputs.sum(-1) + targets.sum(-1)</span><br><span class="line">    loss = 1 - (numerator + 1) / (denominator + 1)</span><br><span class="line">    return loss.sum() / num_masks</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCIG大会—1</title>
      <link href="/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/"/>
      <url>/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/</url>
      
        <content type="html"><![CDATA[<h1 id="csig-年度学科发展报告论坛">CSIG 年度学科发展报告论坛</h1><h2 id="报告题目分割一切模型综述">报告题目：分割一切模型综述</h2><p><strong>报告嘉宾：</strong>张军平，复旦大学计算机科学技术学院教授、博士生导师，中国自动化学会普及工作委员会主任。主要研究方向包括人工智能、机器学习、图像处理、生物认证、智能交通及气象预测。至今发表论文100 余篇，连续两年（2022、2023）入选全球前2%顶尖科学家榜单终身科学影响力排行榜。著有《人工智能极简史》《爱犯错的智能体》《高质量读研》，主编《人机混合增强智能》，译著《统计学习要素》（第二版）。</p><p><strong>报告摘要：</strong>Meta 公司提出的“分割一切模型”(SegmentAnything Model，简称SAM)于 2023 年在图像分割领域获得了优异的性能。在 SAM开源后不久，科研</p><p>人员提出了一系列改进的方法和应用。为了能全面深入了解分割一切模型的发展脉络，优势与不足，本报告将对SAM 的研究进展进行综述。我将先介绍分割一</p><p>切模型的背景和核心框架。在此基础上，综述相关改进方法，并探讨 SAM在图像处理、视频处理以及其他领域的应用。最后，对 SAM未来的发展方向和潜在</p><p>应用前景进行分析和讨论。</p><h3 id="sam背景">SAM背景</h3><ul><li>分割一切项目<br/> - 任务:提出新的提示分割任务范式。<br/> -模型:图像编码器、提示编码器、轻量级掩码解码器。<br/> -数据:提出新的数据引擎构建了SA-1B数据集。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528163704285.png"alt="image-20240528163704285" /><figcaption aria-hidden="true">image-20240528163704285</figcaption></figure><h3 id="sam的应用">SAM的应用</h3><h4 id="视频超分辨率">视频超分辨率</h4><ul><li>SEEM模块可以利用语义信息增强模型的特征对齐和融合能力。<br/> -具体来说，通过利用注意力机制和特征映射操作实现将SAM的表示与当前输入帧的特征相结合，然后生成语义感知的特征。</li></ul><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164011993.png"alt="image-20240528164011993" /><figcaption aria-hidden="true">image-20240528164011993</figcaption></figure><p>基于滑动窗口的超分辨率方法，引入SEEM改进了三个步骤：即对齐、融合和重建</p><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164206753.png"alt="image-20240528164206753" /><figcaption aria-hidden="true">image-20240528164206753</figcaption></figure><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/image-20240528164239992.png"alt="image-20240528164239992" /><figcaption aria-hidden="true">image-20240528164239992</figcaption></figure><p>在基于循环结构的超分辨率方法，我们将SEEM应用于基于双向递归结构的方法。“F”和“B”是向前和向后传播表示串联操作。</p><h4 id="视频目标追踪">视频目标追踪</h4><ul><li>TAM结合了分割模型SAM和高级视频对象分割模型多重记忆模型，这两个模型以交互的方式集成在一起。</li></ul><h4 id="总结">总结</h4><p>SAM的应用形式主要大致分为四类:</p><ol type="1"><li>在<strong>特定领域</strong>对SAM进行<strong>微调</strong><br/>2.使用<strong>SAM辅助其他领域</strong>原有的模型<br/>3.利用SAM构建其他特定领域的数据集<br/>4.使用生成提示模型自动生成提示来辅助SAM</li></ol><figure><imgsrc="./../postimages/CCIG%E5%A4%A7%E4%BC%9A%E2%80%941/SAM(CCIG).drawio.png"alt="SAM(CCIG).drawio" /><figcaption aria-hidden="true">SAM(CCIG).drawio</figcaption></figure><h3 id="未来研究方法">未来研究方法</h3><ul><li>模块化</li><li>弱监督语义分割</li><li>多模态融合图像分割</li><li>对SAM进行高效率微调</li><li>格式塔心理学的整体认知观加强SAM的对抗鲁棒性</li></ul><p>综述论文</p><p><ahref="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=202311030000002">分割一切模型SAM的潜力与展望：综述-Thepotential and prospects of segement anything model: a survey(cjig.cn)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="第三章-经典参数估计">第三章—-经典参数估计</h1><p>如何评价估计器的好坏？</p><h2 id="两个重要的无偏一致估计">两个重要的无偏一致估计：</h2><h3 id="均值">均值：</h3><p><span class="math display">\[\hat \mu = \frac 1 N \sum _{i=1} ^Nx_i\]</span></p><h3 id="协方差矩阵">协方差矩阵：</h3><p>已知均值 <span class="math display">\[\hat R = \frac 1 N \sum _{i=1}^N (x_i -  \mu _x)(x_i -  \mu _x)^H\]</span> 未知均值 <spanclass="math display">\[\hat R = \frac 1 {N-1} \sum _{i=1} ^N (x_i-  \hat \mu _x)(x_i -  \hat \mu _x)^H\]</span></p><h2 id="均方误差mean-squared-error-mse">均方误差(mean squared error, MSE)</h2><p><span class="math display">\[E[||\hat A -A||^2]=E[e^He]=trE[e^He]=trM\]</span></p><p>$ M=E[ee^H] $ 称为均方误差矩阵，可对其进行进一步分解</p><p>令 $ b = _e $ : <span class="math display">\[R_e = E[(e - \mu _e)(e -\mu _e)^H] = M - bb^H\]</span> 因此 $ M = R_e + bb^H $，可视为协方差和偏差的加权，当估计器为无偏估计时，</p><p>最小方差估计器不一定是无偏的，因此存在<strong>最小均方误差估计</strong>和<strong>最小方差无偏估计</strong>两种不同的准则</p><h2 id="最小方差无偏估计mvdr">最小方差无偏估计MVDR</h2><p>对于$ y=Hx+n $ <em>现有问题</em>：</p><p><spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\\\\\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\\\\\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span></p><p>以下是例题：</p><p>例： $ y=Hx+n $，对于上述线性模型，在满足线性运算的条件下寻找最小方差无偏估计，结果称为minimumvariance distortionless response (MVDR) ，也称作best linear unbiasedestimator (BLUE)：</p><p>解：</p><p>假设线性条件： $ x = W^H y $</p><p>则无偏性条件： <span class="math display">\[E[\hat x] = E[W^H y]=E[W^H(Hx+n)]= E[W^HHx+W^Hn]= W^HHx+W^HE(n)=W^HHx=x\\W^HH=I\]</span>最小化方差： <span class="math display">\[E[||\hat x - x||^2]=trE[(\hatx - x)(\hat x - x)^H]\\=trR_{\hat x \hatx}=tr\{W^HR_{yy}W\}=tr\{W^HR_{nn}W\}\]</span>则原问题变成了MVDR优化<em>现有问题</em>： <spanclass="math display">\[\underset{W}{min} \ tr\{ W^HR_{nn}W \}\\s.t.W^HH=I\]</span> 首先构造拉格朗日函数： <spanclass="math display">\[\mathcal{L}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\sum_{i,j}\lambda_{ij}\left[\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]_{ji}=\mathrm{tr}\left(\mathbf{W}^T\mathbf{R}_{nn}\mathbf{W}\right)-\mathrm{tr}\left[\mathbf{\Lambda}\left(\mathbf{W}^T\mathbf{H}-\mathbf{I}\right)\right]\]</span>对W求偏导 <spanclass="math display">\[\begin{aligned}&amp;\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{R}_{mn}\mathbf{W})}{\partial\mathbf{W}}=2\mathbf{R}_m\mathbf{W}\\&amp;\frac{\partial(\mathbf{\Lambda}\mathbf{W}^T\mathbf{H})}{\partial\mathbf{W}}=\frac{\partial\mathrm{tr}(\mathbf{W}^T\mathbf{H}\mathbf{\Lambda})}{\partial\mathbf{W}}=\mathbf{H}\mathbf{\Lambda}\end{aligned}\quad\Rightarrow\quad\frac{\partial\mathcal{L}}{\partial\mathbf{W}}=2\mathbf{R}_{mn}\mathbf{W}-\mathbf{H}\mathbf{\Lambda}=0\quad\Rightarrow\quad\mathbf{W}=\frac12\mathbf{R}_{mn}^{-1}\mathbf{H}\mathbf{\Lambda}\]</span>由约束条件得： <spanclass="math display">\[\mathbf{W}^{T}\mathbf{H}=\frac{1}{2}\mathbf{\Lambda}^{T}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H}=\mathbf{I}\Rightarrow\mathbf{\Lambda}^{T}=2(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\]</span>解得 <spanclass="math display">\[\mathbf{W}^{T}=(\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{T}\mathbf{R}_{nn}^{-1}\]</span>对于复数情形 <spanclass="math display">\[\mathbf{W}^H=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\]</span>因此，MVDR估计为： <spanclass="math display">\[\hat{\mathbf{x}}=\mathbf{W}^{H}\mathbf{y}=(\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^{H}\mathbf{R}_{nn}^{-1}\mathbf{y}\]</span>MVDR估计的协方差矩阵 <spanclass="math display">\[\begin{aligned}\mathbf{R}_{\mathrm{\hat{x}\hat{x}}}&amp;=\mathbf{W}^H\mathbf{R}_{\mathrm{nn}}\mathbf{W}=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^H\mathbf{R}_{nn}^{-1}\cdot\mathbf{R}_{nn}\cdot\mathbf{R}_{nn}^{-1}\mathbf{H}(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\\&amp;=(\mathbf{H}^H\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\end{aligned}\]</span>例： $ y=sA+n $ ，其中s为已知实信号，A为待估计幅度，n为噪声，其协方差矩阵为 $ R_{nn} $，试设计MVDR估计并优化s,在满足总功率限制的条件下使得的估计误差最小化</p><p>解：首先将信号模型写成矩阵的形式 <spanclass="math display">\[y=sA+n\quad\Rightarrow\quady=Hx+n\\H=s,A=x\]</span>计算权值 <spanclass="math display">\[\mathbf{W}^T=(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{R}_{nn}^{-1}\\=(\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s})^{-1}\mathbf{s}^T\mathbf{R}_{nn}^{-1}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}\]</span>由此可得MVDR估计 <spanclass="math display">\[\hat{\mathbf{A}}=\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\mathbf{s}^T\mathbf{R}_{nn}^{-1}y\]</span>要使得估计误差最小化，即最小化方差，即最小化MSE <spanclass="math display">\[\mathrm{tr}(\mathbf{R}_{\mathrm{xx}})=\mathrm{tr}\left[\left(\mathbf{H}^T\mathbf{R}_{nn}^{-1}\mathbf{H}\right)^{-1}\right]=\frac1{\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}}\]</span>最小化方差等同于最大化分母 ，考虑 $ R_{nn} $ 的EVD分解 <spanclass="math display">\[\mathbf{R}_{\mathrm{nn}}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^T\]</span>同时将s表示为 $ R_{nn} $ 特征向量的加权 <spanclass="math display">\[s=V\alpha\]</span> 所以： <spanclass="math display">\[\mathbf{s}^T\mathbf{R}_{nn}^{-1}\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\Lambda}^{-1}\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\Lambda}^{-1}\mathbf{\alpha}=\sum_{i\operatorname{=}1}^N\frac{|\alpha_i|^2}{\lambda_i}\]</span>考虑功率约束 <spanclass="math display">\[\mathcal{E}=\sum_{k=1}^Ns_k^2=\parallel\mathbf{s}\parallel^2=\mathbf{s}^T\mathbf{s}\]</span>由于 $ s=V$ : <spanclass="math display">\[\mathbf{s}^T\mathbf{s}=\mathbf{\alpha}^T\mathbf{V}^T\mathbf{V}\mathbf{\alpha}=\mathbf{\alpha}^T\mathbf{\alpha}=\parallel\mathbf{\alpha}\parallel^2=\sum_{i=1}^N\alpha_i^2\]</span>为最大化 $ <sup>T<em>{nn}^{-1} = </em>{i1}</sup>N $，应将能量分配到较小的 $ <em>i $ 上，因此最佳方案为<br /><span class="math display">\[\alpha_i^2=\begin{cases} 0,&amp;i=1,...,N-1 ,\\ \mathcal{E},&amp;i=N,\end{cases}\]</span> 此时 $ =</em>{}$，对应于 $ R_{nn} $ 最小特征根对应的特征向量</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计信号处理作业</title>
      <link href="/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/"/>
      <url>/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<h1id="在超密集网络中基于连接的定位crlb理论方差和mle">在超密集网络中基于连接的定位：CRLB，理论方差，和MLE</h1><h1 id="摘要">摘要</h1><p>​  超密集网络（UDNs, ultra-densenetworks）中基于连接的地理定位的性能分析是一项非常重要的任务。虽然已经对无距离定位进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。在本文中，我们首先推导了对无范围定位的性能评估的Cramer-Rao下界（CRLB）。文献中关于无距离定位的所有当前性能分析都用于评估给定算法的实际性能，而所提出的CRLB提供了评估任何无偏无距离定位算法性能的基准，并确定了无偏估计器的方差小于界的物理不可能性。</p><p>​  据我们所知，这是文献中第一次推导出用于无距离定位的CRLB。其次，推导了任意节点分布下基于质心定位的理论方差。与均匀节点分布下CL的现有理论方差相比，所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还给出了所提出的CRLB的特性和理论方差。最后，为了提高定位精度，提出了一种基于最大似然估计器的最优估计器。由于我们的算法有效地利用了空间节点分布的先验信息和连通性，因此所提出的方法比CL方法性能更好，并且可以渐近地获得CRLB。</p><h1 id="引言">引言</h1><p>​  随着网络设备数量的增加，超密集网络（UDN）系统中盲节点（BN）的位置估计近年来引起了[1]的广泛关注。无线位置作为一个重要的公共安全功能，在未来的无线通信系统中创造了许多潜在的应用，如位置敏感的计费、欺诈保护、人员/资产跟踪、车队管理、移动黄页、无线网络设计、无线电资源管理和智能交通系统[2]、[3]。</p><p>​  虽然全球导航卫星系统（GNSSs），如全球定位系统（GPSs），可以提供较高定位精度的定位服务，但其局限性，包括高功率消耗，在室外丰富散射场景和城市峡谷的性能下降，阻止GNSSs应用于复杂的城市和室内环境。</p><p>从广义上讲，无线通信系统中的定位技术可以分为两类：</p><p>​  (1)基于范围的定位方法</p><p>​  (2)无范围的定位方法（也称为基于连接的定位）。</p><p>​  几种基于距离的定位技术，包括到达时间（TOA）、到达时差（TDOA）、到达角度（AOA）、基于接收信号强度（RSS）的方法和混合定位方法，被用于无线定位。基于范围的定位首先利用承载角和绝对或相对距离建立BN和参考节点（RNs）之间的显式几何关系，这些距离是由AOA、TOA、RSS和TDOA测量值估计的。然后，根据几何模型可以得到BN的位置。由于其定位精度高，基于范围的定位方法已经在文献[4]-[25]中得到了广泛的研究。研究了视线（LOS）环境[4]-[9]下的封闭解和迭代算法。对于非视线（NLOS）传播，开发了几何约束条件[10]-[12]和机器学习理论[13]-[16]来减轻NLOS误差。这些研究大多是基于单一的测量路径。为了进一步提高定位精度，在[17]，[18]中提出了基于多天线阵列的多路径传播环境的AOA定位算法。此外，在文献[19]-[25]中，还推导出了许多精度的几何稀释度和Cramer-Rao下界（CRLBs）。</p><p>​  本文从CRLB和理论方差的角度分析了无范围定位的性能。本文还提出了基于最大似然估计量（MLE）的最优估计量。本文的主要贡献如下：</p><p>​  (1)本文推导了一个具有随机分布RNs的UDN中无范围定位的CRLB。虽然已经对无距离定位[26]、[27]、[31]进行了一些性能分析，但确定无距离定位的最佳可实现定位精度仍然是一个有待解决的问题。所有当前的性能分析[26]，[27]，[31]无距离定位用于评估给定算法的实际性能，而提出CRLB提供了一个基准来评估任何无偏的位置算法的性能和确定物理不可能的方差无偏的估计器小于绑定。据我们所知，这是文献中首次推导出无范围定位的CRLB。</p><p>​  (2)推导了具有任意节点分布的CL(centroid-basedlocalization)方法的理论方差。需要注意的是，[26]中CL方法的理论方差是针对均匀节点分布推导出的。所提出的理论方差可用于评估任意节点分布情况下CL的性能。此外，本文还提供了所提出的CRLB的特征和理论方差。</p><p>​  (3)提出了一种基于MLE的最优估计器来提高定位精度。由于该算法有效地利用了空间节点分布的先验信息和连通性，因此该方法优于CL方法，并能渐近得到CRLB。</p><p>​  本文的组织结构如下。第二节给出了信号模型和一些基本的符号。在第三节中，本文首先推导了一个在随机分布的UDN中无距离定位的CRLB。然后，导出了任意节点分布下CL方法的理论方差。在本节的最后给出了所提出的CRLB和理论方差的一些特征。第四节提出了一种基于MLE的基于连通性信息和RN分布的迭代方法。第五节给出了所提出的CRLB的性能评价、理论方差和位置方法。本文的结论见第六节。</p><h1 id="系统模型">系统模型</h1><p>​  无距离定位的目的是利用BN和RNs之间的连接信息来定位BN。通常，CL算法包括两个阶段：监听和定位。在监听阶段，BN尝试监听并与RNs沟通。当接收到的信号功率超过检测阈值时，建立通信链路。在定位阶段，BN的位置近似为在其传输范围内所有RN的位置（RN的质心）位置的平均值。显然，CL的性能受到许多因素的影响，如节点密度和随机性、无线信道环境和位置方案。</p><p>​  无线信道环境对定位系统的性能起着非常重要的作用。这个通道环境决定了可以检测到多少和哪些RNs用于定位。传播模型通常是用来描述无线信道的情况，并预测在距离发射机的给定距离下的平均接收信号强度。虽然有几种传播模型[32]，[33]，本文选择了路径损失法向阴影模型，因为它被广泛应用于通信和定位应用，并已通过现场测量[32]得到证实。</p><p>​  假设（x，y）是待估计的BN的位置，并且N个RNs系统中第i个RN的已知坐标为（xi，yi），如果不失一般性，可以将BN的位置设为（0,0）。第i个RN和BN之间的真实距离可以建模为：<span class="math display">\[r_i = \sqrt{(x_i - x)^2 + (y_i -y)^2}\]</span>​  基于路径损失正态阴影模型，测量的RNi（dBm）的接收功率Pi可以视为对数正态变量[32]。因此，Pi和ri之间的关系变为：<span class="math display">\[P_i=P_0-10 \beta log_{10}(\frac {r_i}{r_0}) + n_i\]</span>​  式中，β为路径损失指数，表示路径损失随距离增加的速率；ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）；P0为参考距离r0处的参考功率，它取决于传输功率。一般来说，r0=1米。为简单起见，本文将r0设为1m。本文利用路径损失法向阴影模型推导出了无距离定位的CRLB和MLE。</p><p>​  节点的随机性是影响该定位方法性能的另一个主要因素。在文献中，基于不同的假设，提出了不同的节点分布。文献中首先提出了均匀分布，建立了一个节点分布模型，假设传感器节点均匀分布在半径为R[26]，[27]，[34]的圆盘中。然而，最近，人们已经认识到，均匀分布节点的假设对于实际部署的无线网络[35]，[36]是相当不可信的。事实上，节点的空间分布依赖于许多因素，如部署方法、节点的周围环境、节点的运动，甚至是通信协议。根据中心极限定理，实际节点位置将遵循高斯分布[35]，[36]。在这个模型中，根据二维高斯空间分布，协方差矩阵σp2i。一个RN位于（xi，yi）的概率可以用概率密度函数（PDF）[36]来描述：<span class="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span>​  需要注意的是，(3)是基于笛卡尔坐标系的。对于极坐标，PDF (3)可以写成：<span class="math display">\[f(r_i) = \frac {1} {\sigma _p ^2}exp(-\frac {r _i ^2} {2 \sigma _p ^2}) r_i\\f(\phi _i)= \frac {1} {2 \pi}\]</span> ​  其中ri是第i个RN和BN之间的范围。 $ _i = acos((x−x_i)/r_i) $是RNi相对于BN的方位角。式(4)表明，RN将在不同方向上以相同的概率出现，而f（xi，yi）只取决于RN与BN之间的距离。这也意味着靠近BN的RN可能比在更大距离的RN有更高的概率。</p><p>​  CL(centroid-based localization,基于质心的定位)是最简单的无范围定位方法，它只需要BN和相邻rN之间的二进制连接信息。CL算法基于以下假设[28]：</p><p>​  (1)有完美的球形无线电传播</p><p>​  (2)所有无线电具有相同的传输范围（功率）</p><p>​  (3)RN对称地分布在一个BN周围。</p><p>​  (4) CL仅基于从相邻RNs收集的连通性信息（单跳假设）。</p><p>​  假设（1-3）保证了CL算法是一个无偏估计量，第四个假设简化了定位过程。由于仿真和实验结果都证明了该模型在整洁环境[28]下非常符合户外无线电传播，因此本文也遵循了这些假设。BN定位于与RNs集合的连通性区域相交重合的区域，该区域由RNs[28]的质心定义： <span class="math display">\[(\hat x , \hat y)=(\frac 1M \sum _{i=1} ^ M x_i , \frac 1 M \sum _{i=1} ^ M y_i )\]</span>​  其中M≤N是实际参与定位过程的RNs的数量。从(5)可以看出，CL算法只对RN的坐标进行平均，用等权值估计BN的位置。为了进一步提高定位精度，文献中提出了几种加权CL（WCL）算法[37]-[48]。[37]中早期的WCL算法使用链路质量指示作为权值，并应用于基于zigbee的传感器网络中<span class="math display">\[(\hat x , \hat y)=(\frac {\sum _{i=1} ^ Mw_i x_i} {\sum _{i=1} ^ M w_i}  , \frac {\sum _{i=1} ^ M w_i y_i} {\sum_{i=1} ^ M w_i} )\]</span> ​  其中，权重wi是RSSPi的函数。由于现有的WCL算法需要RSS测量，这些都是基于范围的定位技术，超出了我们的研究范围。本文主要研究了仅使用单跳连接信息的无范围定位技术。</p><h1id="基于连接性的定位技术的理论分析">基于连接性的定位技术的理论分析</h1><p>​  本节推导出CRLB和理论方差来研究无距离定位技术的性能。CRLB对于参数估计非常重要，因为它提供了一个基准来评估任何无偏估计器的性能，而理论方差被用来评估给定算法的真实性能。</p><h2 id="crlb">CRLB</h2><p>​  对于无距离定位技术，BN的位置使用被检测的位置估计RN的位置。因此，测量向量为s= [x1，y1，···，xM，yM ] T，待估计的参数向量θ为[x，y] T。</p><p>​  假设PDF满足“规律性”条件： <spanclass="math display">\[E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right]=0\text{for all }\boldsymbol{\theta}\]</span>​  然后，将CRLB矩阵定义为Fisher信息矩阵（FIM）Jθ的逆： <spanclass="math display">\[E((\hat{\theta}-\theta)(\hat{\theta}-\theta)^T)\geq\mathbf{J}_\theta^{-1}\]</span>​  Fisher信息矩阵的确定为[49]： <spanclass="math display">\[\mathbf{J}_{\theta}=\begin{bmatrix}J_{xx}&amp;&amp;J_{xy}\\J_{xy}&amp;&amp;J_{yy}\end{bmatrix}=E\left[\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\left(\frac{\partial\lnf\left(\mathbf{s};\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}}\right)^T\right]\]</span>​  其中 <spanclass="math display">\[f\left(\mathbf{s};\theta\right)=\prod_{i=1}^{M}f\left(x_{i},y_{i};\theta\right)\\f\left(x_{i},y_{i};\theta\right)=\frac{\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)}{\gamma}\]</span>​  f（xi，yi）描述了节点的空间分布概率，定义为(3)。 <spanclass="math display">\[f(x_i , y_i) = \frac {1} {2 \pi \sigma _p^2}exp(- \frac {(x_i - x) ^2} {2 \sigma _p ^2} - \frac {(y_i - y) ^2} {2\sigma _p ^2})\]</span> $ (r_i) $ 为检测概率，表示在RNi处接收到的信号功率超过检测阈值Pth的概率。</p><p>γ是一个归一化常数： <spanclass="math display">\[\gamma=\int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}\Phi\left(r_{i}\right)f\left(x_{i},y_{i}\right)dx_{i}dy_{i}=\int\limits_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\]</span></p><p>因为 $ P_i=P_0-10 log_{10}( {r_0}) + n_i $,其中接收功率Pi、β为路径损失指数，表示路径损失随距离增加的速率、ni是一个零均值高斯随机过程，标准方差（std）σ，单位为分贝（dB）、P0为参考距离r0处的参考功率，它取决于传输功率。<spanclass="math display">\[f\left(P_i\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_i-P_0+10\beta\log_{10}\left(r_i\right)\right)^2}{2\sigma^2}\right)\]</span>所以开始， $ (r_i) $ 可计算为： <spanclass="math display">\[\begin{aligned}&amp;\Phi\left(r_{i}\right)\\&amp;=p\left(P_{i}\geqP_{th}\right)=\int_{P_{th}}^{+\infty}f\left(P_{i}\right)dP_{i}\\&amp;=\int_{P_{th}}^{+\infty}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(P_{i}-P_{0}+10\beta\log_{10}\left(r_{i}\right)\right)^{2}}{2\sigma^{2}}\right)dP_{i}\end{aligned}\]</span>使用替代方法，（14）可简化为： <spanclass="math display">\[\Phi\left(r_i\right)=\int_{\varphi\left(r_i\right)}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\nu^2}{2}\right)d\nu\]</span> 其中 <spanclass="math display">\[\varphi\left(r_i\right)=\begin{pmatrix}P_{th}-P_0+10\beta\log_{10}\left(r_i\right)\end{pmatrix}/\sigma\]</span> 和 $ (r_i) $ 可以使用MATLAB函数“erfc”来计算： <spanclass="math display">\[\Phi\left(r_i\right)=0.5erfc\left(\frac{\varphi\left(r_i\right)}{\sqrt{2}}\right)\]</span>方程（17）表明， $ (r_i) $只依赖于一个给定系统的距离ri。将（10）-（17）替换为(9)，如附录所示，基于连接的定位技术的CRLB为：<spanclass="math display">\[CRLB=tr\left\{\mathbf{J}_\theta^{-1}\right\}=\frac{4}{\bar{\psi}^2M}\]</span>其中 $ {}^2 $ ： <spanclass="math display">\[\begin{aligned}\bar{\psi}^{2}&amp; =\frac{1}{\gamma}\int_{0}^{+\infty}\left(\frac{1}{\Phi\left(r\right)}\frac{b}{r}\exp\left(-\frac{\varphi\left(r\right)^{2}}{2}\right)+\frac{r}{\sigma_{p}^{2}}\right)^{2}\times\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\\&amp; b = 10\beta/\left(\ln10\sqrt{2\pi}\sigma\right)\end{aligned}\]</span> 由于M是检测BN的RNs的数量，并由RNsN的总数和信道传输模型(2)决定，因此在每个定位过程中可能会发生变化。为了评估平均性能，将平均CRLB定义为<span class="math display">\[CRLB_{average}=E[CRLB]=E\left[\frac{4}{\bar{\psi}^{2}M}\right]=\frac{4}{\bar{\psi}^{2}}E\left[\frac{1}{M}\right]\]</span>显然，数值计算可以直接用于计算E[1/M]。为了进行进一步的性能分析，本文提出了一种分析方法。当M足够大时，期望均值可以替换为样本均值[34]，平均CRLB近似为：<spanclass="math display">\[CRLB_{average}\approx\frac{4}{\bar{\psi}^{2}}\frac{1}{\bar{M}}\]</span>其中 <span class="math display">\[\begin{aligned}\bar{M}&amp; =E [M]=E[N\Phi (r)]\\&amp;=NE\left[\Phi\left(r\right)\right]=N\int_{0}^{+\infty}\Phi\left(r\right)f\left(r\right)dr\\&amp;=N\int_{0}^{+\infty}\Phi\left(r\right)\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)rdr\end{aligned}\]</span></p><h2id="具有任意节点分布的质心定位的理论方差">具有任意节点分布的质心定位的理论方差</h2><p>​  类似于CRLB，它提供了一个基准来评估任何无偏定位算法的性能，理论方差对于性能分析非常重要，因为它被用来评估给定算法的真实性能。虽然对于均匀节点分布[26]已经提出了CL算法的理论方差，但任意节点分布的情况仍然是一个有待解决的问题。本小节推导了具有任意节点分布的CL算法的理论方差。理论方差的定义为：<spanclass="math display">\[\begin{aligned}\operatorname{cov}(\theta)&amp;=tr\left\{E\left(\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)\left(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}\right)^{T}\right)\right\}\\&amp;=E\left(\left(\widehat{x}-x\right)^{2}+\left(\widehat{y}-y\right)^{2}\right)\end{aligned}\]</span>​  将CL估计值(5)替换入（24），得到： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}+\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)\]</span>​  上式中的第一项可以写成： <spanclass="math display">\[\begin{aligned}&amp;E\left(\left({\frac{1}{M}}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)\\&amp;=E\left(\left(\frac{1}{M}\sum_{i=1}^{M}\left(x_{i}-x\right)\right)^{2}\right)\\&amp;\left.=E\left(\frac{1}{M^{2}}\left(\sum_{i=1}^{M}(x_{i}-x)^{2}+\sum_{i=1}^{M}\sum_{i=1}^{M}(x_{i}-x)\left(x_{j}-x\right)\right)\right)\right)\end{aligned}\]</span>​  注意，在i 不等于j的情况下，xi和xj是独立的，它可以从RN节点分布(3)中得到E（xi−x）=0。因此，（26）可以简化为： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}x_{i}-x\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(x_{i}-x\right)^{2}\right)\]</span>​  同理： <spanclass="math display">\[E\left(\left(\frac{1}{M}\sum_{i=1}^{M}y_{i}-y\right)^{2}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}\left(y_{i}-y\right)^{2}\right)\]</span>​  将（27）和（28）代入（25），CL算法的理论方差可计算为： <spanclass="math display">\[\mathrm{cov}\left(\boldsymbol{\theta}\right)=E\left(\frac{1}{M^{2}}\sum_{i=1}^{M}r_{i}^{2}\right)=E\left(\frac{1}{M}\right)\bar{r}^{2}\approx\frac{\bar{r}^{2}}{\bar{M}}\]</span>​  类似于（22），E [1/M]可以使用数值方法或近似方法（23）来求解。</p><p>​  ¯r2可计算为： <spanclass="math display">\[\bar{r}^{2}=\frac{1}{\gamma}\int_{0}^{+\infty}r^{2}\Phi\left(r\right)f\left(r\right)dr\]</span>​  其中f (r)为rN和BN之间范围的PDF。使用不同的f(r)，可以使用（29）和（30）计算具有任意节点分布的CL的理论方差。</p><p>​  对于高斯节点分布，f (r)可以从(4)得到： <spanclass="math display">\[f\left(r\right)=\frac{1}{\sigma_{p}^{2}}\exp\left(-\frac{r^{2}}{2\sigma_{p}^{2}}\right)r\quad0&lt;r\]</span>​  对于均匀的节点分布，f (r)为： <spanclass="math display">\[f\left(r\right)=\frac{2r}{R^{2}}\quad0&lt;r&lt;R\]</span>​  其中，R为均匀节点分布的分布半径。</p><h2 id="所提出的crlb的特征和理论方差">所提出的CRLB的特征和理论方差</h2><p>所提出的CRLB的特征和理论方差。</p><p>​  命题1：对于高接收功率，所提出的CRLB可以近似为： <spanclass="math display">\[CRLB_H\approx\frac{2\sigma_p^2}N\]</span>​  备注1：命题1表明，在高信噪比（SNR）情况下，无距离方法的性能主要取决于信N的节点分布，而不是信道环境。这种现象的发生是因为所有的rn都可以连接到一个具有良好信道环境的BN。</p><p>​  此外，下面还提出了密度λ对所提出的CRLB的影响</p><p>​  命题2：在高接收功率下，所提出的CRLB可以用密度λ近似为： <spanclass="math display">\[CRLB_H\approx\frac{1}{5.95\pi\lambda}\]</span>​  命题2表明，CRLB与λ呈负相关。这意味着一个更密集的网络将导致更高的定位精度。因此，无范围的方法是UDN的首选解决方案。以下命题提供了CL的实际性能与所提出的CRLB之间的关系。</p><p>​  命题3：在高斯节点分布和高接收功率的情况下，基于质心的定位的理论方差等于CRLB：<spanclass="math display">\[\operatorname{cov}(\theta)_H=CRLB_H=\frac{2\sigma_p^2}{N}\]</span>​  备注2：由于理论方差代表了CL方法的实际性能，因此从命题3中可以确定，在高信噪比的情况下，CL方法可以达到CRLB。这可以解释为这样一个事实，即在高信噪比的情况下，几乎所有的rN都可以与BN通信。因此，对于8(r)→1和γ→1，联合PDF（11）简化为高斯节点分布(3)。对于高斯PDF(3)，质心估计(5)是一个MLE。众所周知，MLE是渐近无偏的，并且可以渐近地获得具有明显的大测量值的CRLB。它是渐近有效的和最优的[49]。因此，CL在高信噪比的情况下具有最佳的性能。命题3也证明了所提出的CRLB的有效性。对于一个更实用的信噪比变化的信道，下一节提出了一种基于MLE的新的定位方法来提高性能。</p><p>基于</p>]]></content>
      
      
      
        <tags>
            
            <tag> 报告 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20240522讲座</title>
      <link href="/20240522%E8%AE%B2%E5%BA%A7/"/>
      <url>/20240522%E8%AE%B2%E5%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>记录5月22日讲座~</p><h1id="题目面对aigc的多功能数字水印与版权保护研究">题目：面对AIGC的多功能数字水印与版权保护研究</h1><p>讲师：张建</p><h2 id="图像重建">图像重建</h2><ol type="1"><li>Zero-Shot Image Restoration Using Denoising Diffusion Null-SpaceModel</li></ol><p>​  大多数现有的图像恢复（IR）模型都是特定于任务的，不能推广到不同的退化算子。在这项工作中，我们提出了去噪扩散零空间模型（DDNM,Denoising Diffusion Null-SpaceModel），这是一种新的零样本框架，用于解决任意线性IR问题，包括但不限于图像超分辨率、着色、修复、压缩感知和去模糊。DDNM只需要一个预先训练的离架扩散模型作为生成先验，而不需要任何额外的训练或网络修改。通过在反向扩散过程中仅细化零空间内容，我们可以产生满足数据一致性和真实性的不同结果。我们进一步提出了一个增强和稳健的版本，称为DDNM+，以支持噪声恢复并提高硬任务的恢复质量。我们在几个红外任务上的实验表明，DDNM优于其他最先进的零样本红外方法。我们还证明了DDNM+可以解决复杂的现实世界应用，例如旧照片恢复。</p><h2 id="图像条件生成">图像条件生成</h2><ol start="2" type="1"><li>FreeDoM: Training-Free Energy-Guided Conditional DiffusionModel</li></ol><p>​  最近，条件扩散模型由于其卓越的生成能力而在许多应用中受到欢迎。然而，许多现有的方法都是需要培训的。他们需要训练一个与时间相关的分类器或与条件相关的分数估计器，这增加了构建条件扩散模型的成本，并且不方便在不同条件下转移。目前的一些工作旨在通过提出无训练的解决方案来克服这一限制，但大多数只能应用于特定类别的任务，而不能应用于更一般的条件。在这项工作中，我们提出了一种用于各种条件的训练自由条件扩散模型（FreeDoM）。具体来说，我们利用现成的预训练网络，如人脸检测模型，来构建与时间无关的能量函数，该函数在不需要训练的情况下指导生成过程。此外，由于能量函数的构造非常灵活，能够适应各种条件，因此我们提出的FreeDoM比现有的无训练方法具有更广泛的应用范围。FreeDoM的优势在于其简单、有效和低成本。实验表明，FreeDoM在各种条件下都是有效的，适用于不同数据域的扩散模型，包括图像域和潜在代码域。</p><h2 id="图像精准控制生成">图像精准控制生成</h2><ol start="3" type="1"><li>T2I-Adapter: Learning Adapters to Dig Out More Controllable Abilityfor Text-to-Image Diffusion Models</li></ol><p>​  大规模文本到图像（T2I,text-to-image）模型令人难以置信的生成能力已经证明了学习复杂结构和有意义语义的强大能力。然而，仅仅依靠文本提示并不能充分利用模型所学到的知识，尤其是在需要灵活准确的控制（如结构和颜色）时。在本文中，我们的目标是“挖掘”T2I模型隐式学习的能力，然后显式地使用它们来更细粒度地控制生成。具体而言，我们建议学习低成本的T2I适配器，以使T2I模型中的内部知识与外部控制信号相一致，同时冻结原始的大型T2I模型。这样，我们可以根据不同的条件训练各种适配器，从而在生成结果的颜色和结构上实现丰富的控制和编辑效果。此外，所提出的T2I适配器具有可组合性和泛化能力等有吸引力的实用价值。大量实验表明，我们的T2I转换器具有良好的生成质量和广泛的应用。我们的代码可在https://github.com/TencentARC/T2I-Adapter.</p><ol start="4" type="1"><li>DragonDiffusion: Enabling Drag-style Manipulation on DiffusionModels</li></ol><p>​  尽管现有的大规模文本到图像（T2I）模型能够从详细的文本描述中生成高质量的图像，但它们往往缺乏精确编辑生成的或真实图像的能力。在本文中，我们提出了一种新的图像编辑方法，DragonDiffusion，可以在Diffusion模型上进行Drag风格的操作。具体来说，我们基于扩散模型中中间特征的强对应性来构建分类器引导。它可以通过特征对应损失将编辑信号转换为梯度，以修改扩散模型的中间表示。基于这种制导策略，我们还构建了一个多尺度制导，同时考虑语义和几何对齐。此外，增加了跨分支的自关注，以保持原始图像和编辑结果之间的一致性。我们的方法通过高效的设计，实现了对生成或真实图像的各种编辑模式，如对象移动、对象大小调整、对象外观替换和内容拖动。值得注意的是，所有编辑和内容保存信号都来自图像本身，并且该模型不需要微调或附加模块。我们的源代码将在这个httpsURL上提供。</p><ol start="5" type="1"><li>DiffEditor: Boosting Accuracy and Flexibility on Diffusion-basedImage Editing</li></ol><p>图像视频隐写</p><ol type="1"><li>Robust Invertible Image Steganography</li></ol><p>​  图像隐写术旨在将秘密图像隐藏到容器图像中，在容器图像中隐藏秘密，并在必要时进行恢复。以前的图像隐写方法在隐藏能力和鲁棒性方面受到限制，通常容易受到容器图像失真的影响，如高斯噪声、泊松噪声和有损压缩。本文提出了一种新的基于流的鲁棒可逆图像隐写框架，称为RIIS。我们引入了条件归一化流，以容器图像为条件对冗余高频分量的分布进行建模。此外，精心设计的容器增强模块（CEM）也有助于稳健的重建。为了调节不同失真水平的网络参数，我们提出了一种基于流的块上的失真引导调制（DGM），使其成为一个一刀切的模型。在干净和失真图像隐写方面，大量实验表明，所提出的RIIS有效地提高了鲁棒性，同时保持了不可见性和容量。据我们所知，我们是文献中第一个增强图像隐写术鲁棒性的基于学习的方案。隐写术鲁棒性的保证大大拓宽了隐写术在现实应用中的应用。</p><ol start="2" type="1"><li>Large-Capacity and Flexible Video Steganography via InvertibleNeural Network</li></ol><p>​  视频隐写术是一种在封面视频中不引人注目地隐藏秘密数据，然后在接收器端通过解码协议恢复秘密数据的技术。尽管已经进行了几次尝试，但大多数都局限于低容量和固定的隐写术。为了弥补这些不足，本文提出了一种大容量、灵活的视频隐写网络（LF-VSN）。对于大容量，我们提出了一种可逆管道，通过单个可逆神经网络（INN）来执行多个视频的隐藏和恢复。我们的方法可以在1个封面视频中隐藏/恢复7个秘密视频，性能良好。为了灵活性，我们提出了一种密钥可控方案，使不同的接收器能够通过特定的密钥从同一封面视频中恢复特定的秘密视频。此外，我们通过提出一种可扩展的多视频隐藏策略，进一步提高了灵活性，该策略可以用单个模型和单个训练会话在封面视频中隐藏可变数量的秘密视频。大量实验表明，随着视频隐写性能的显著提高，我们提出的LF-VSN具有高安全性、大隐藏容量和灵活性。源代码位于https://github.com/MC-E/LF-VSN.</p><h2 id="定制化溯源水印">定制化溯源水印</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 讲座 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM</title>
      <link href="/SAM/"/>
      <url>/SAM/</url>
      
        <content type="html"><![CDATA[<p>Segment Anything <ahref="https://github.com/facebookresearch/segment-anything"><imgsrc="https://img.shields.io/github/stars/facebookresearch/segment-anything?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><div class="row">    <embed src="/postpdfs/SAM/2304.02643v1.pdf" width="100%" height="550" type="application/pdf"></div></details><h1 id="摘要">摘要</h1><p>​  我们介绍了分段任意事物（SA, SegmentAnything）项目：一个新的图像分割任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们建立了迄今为止（迄今为止）最大的分割数据集，在11M许可和尊重隐私的图像上有超过10亿个面具。该模型的设计和训练是及时的，因此它可以转移零镜头到新的图像分布和任务。我们评估了它在许多任务上的能力，发现它的零样本性能令人印象深刻——通常与之前的完全监督结果竞争，甚至更好。</p><h1 id="segment-anything-model">Segment Anything Model</h1><p>​  接下来，我们将描述用于快速分割的分段任何东西模型（SAM, SegmentAnything Model）。</p><figure><img src="./../postimages/SAM/image-20240519201116761.png"alt="image-20240519201116761" /><figcaption aria-hidden="true">image-20240519201116761</figcaption></figure><p><img src="./../postimages/SAM/image-20240519194006526.png"alt="image-20240519194006526" />图4：分段任何东西模型（SAM)概述。重量级图像编码器输出图像嵌入，然后可以通过各种输入提示有效地查询，以平摊的实时速度产生对象掩模。对于对应于多个对象的模糊提示，SAM可以输出多个有效的掩码和相关的置信度分数。</p><p>​  SAM有三个组件，如图4所示：图像编码器、灵活的提示编码器和快速掩码解码器。我们建立在转换视觉模型[14,33,20,62]上，对（摊销）实时性能进行特定的权衡。我们在这里高级描述这些组件，在a中详细说明。</p><h2 id="图像编码器">图像编码器</h2><p>​  一般来说，图像编码器可以是任何输出C×H×W图像嵌入的网络。基于可伸缩性和强大的预训练，我们使用MAE[47]预训练视觉transformer（ViT）[33]，具有最小的适应来处理高分辨率输入，特别是ViT-H/16，有14×14窗口注意和4个等间隔的[62]块。图像编码器的输出是输入图像的16×缩小嵌入。由于我们的运行时目标是实时处理每个提示，因此我们可以提供大量的图像编码器片段，因为它们每幅图像只计算一次，而不是每个提示计算一次。根据标准的实践（例如，[40]），我们使用了1024×1024的输入分辨率，这是通过重新缩放图像和填充较短的边而获得的。因此，图像嵌入值为64×64。为了减少信道维度，在[62]之后，我们使用1×1卷积得到256个通道，然后使用3×3卷积得到256个通道。每个卷积之后都是一个层的归一化[4]。</p><h2 id="提示编码器">提示编码器</h2><p>稀疏提示被映射到256维的向量嵌入如下。</p><p>​  一个点被表示为该点的位置的位置编码[95]和两个学习嵌入之一的总和，这表明该点是在前景中还是在背景中。</p><p>​  盒子由嵌入对表示：(1)其左上角的位置编码与表示“左上角”的学习嵌入求和，(2)相同的结构，但使用学习嵌入表示“右下角”。最后，为了表示自由形式的文本，我们使用CLIP[82]的文本编码器（任何文本编码器都是可能的）。我们将在本部分的其余部分中关注几何提示，并在D.5中深入讨论文本提示。</p><p>​  密集的提示（即掩码）与图像具有空间对应关系。我们以比输入图像低4×的分辨率输入掩模，然后使用两个2×2，步幅-2卷积分别与输出通道4和16缩小额外的4×。最后的1×1卷积将通道维度映射到256。每一层通过GELU激活[50]和层归一化分开。然后，将按元素的方式添加图像嵌入和掩码。如果没有掩码提示，则在每个图像嵌入位置添加一个表示“无掩码”的学习嵌入。</p><h2 id="轻量级掩码器">轻量级掩码器</h2><p>​  该模块有效地将图像嵌入和一组提示嵌入映射到一个输出掩码。为了结合这些输入，我们从transformer分割模型[14,20]中获得灵感，并修改了一个标准的transformer解码器[103]。在应用我们的解码器之前，我们首先在提示嵌入集中嵌入一个学习到的输出tokens嵌入，该嵌入将用于解码器的输出，类似于[33]中的[类]tokens。为简单起见，我们将这些嵌入（不包括图像嵌入）统称为“标记”。</p><p>​  我们的解码器设计如图14所示。</p><figure><img src="./../postimages/SAM/image-20240519195617684.png"alt="image-20240519195617684" /><figcaption aria-hidden="true">image-20240519195617684</figcaption></figure><p>图14：轻量级掩码解码器的细节。一个两层解码器通过交叉注意来更新图像嵌入和提示标记。然后对图像嵌入进行升级，利用更新后的输出标记来动态预测掩模。（为了图形清晰度，没有说明：在每个注意层，位置编码被添加到图像嵌入中，整个原始提示tokens（包括位置编码）被重新添加到tokens查询和键中。）</p><p>​  每个解码器层执行4个步骤：(1)对标记的自我注意，(2)从标记（作为查询）交叉注意到图像嵌入，(3)点级MLP更新每个标记，以及(4)从图像嵌入（作为查询）交叉注意到标记。这最后一步将使用提示信息更新图像嵌入。在交叉注意过程中，将图像嵌入视为一组64225个6维向量。每个自我/交叉注意和MLP都有一个残差连接[49]、层归一化和退出[93]为0.1。下一个解码器层从上一层中获取更新的tokens和更新的图像嵌入。我们使用了一个两层解码器。</p><p>​  为了确保解码器能够访问关键的几何信息，当位置编码参与注意层时，它们将被添加到图像嵌入中。此外，整个原始提示标记（包括它们的位置编码）都会被重新添加到更新后的标记中。这允许强烈地依赖于提示tokens的几何位置和类型。</p><p>​  在运行解码器后，我们用两个转置卷积对更新后的图像嵌入上采样，即4×（现在它相对于输入图像缩小了4×）。然后，tokens再次关注图像嵌入，我们将更新后的输出tokens嵌入传递给一个小的3层MLP，该MLP输出一个与升级图像嵌入的通道维数相匹配的向量。最后，我们预测了一个在升级的图像嵌入和MLP的输出之间具有空间点级乘积的掩模。</p><p>​  该transformer使用的嵌入尺寸为256。TransformerMLP块有一个很大的内部尺寸为2048，但MLP只应用于有相对较少（很少大于20）的提示tokens。在交叉注意层中，我们有一个64×64的图像嵌入，为了提高计算效率，我们将查询、键和值的通道维数降低两倍到128。所有的注意力层都使用8个头。用于升级输出图像嵌入的转置卷积为2×2，步幅为2，输出通道尺寸分别为64和32，并具有GELU激活。它们被层归一化分开。</p><h1 id="sam-adapter">SAM-Adapter</h1><p>​  《SAM Fails to Segment Anything? – SAM-Adapter: Adapting SAM inUnderperformed Scenes: Camouflage, Shadow, Medical ImageSegmentation,and More》<ahref="https://tianrun-chen.github.io/SAM-Adaptor/"><imgsrc="https://img.shields.io/badge/project-blue" alt="project" /></a> <ahref="https://arxiv.org/abs/2304.09148"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/tianrun-chen/SAM-Adapter-PyTorch"><imgsrc="https://img.shields.io/github/stars/tianrun-chen/SAM-Adapter-PyTorch?style=flat"alt="GitHub" /></a></p><p>​  该文指出号称可以“分割一切”的SAM模型虽然在自然图像的通用分割任务中取得了优异的效果，但在许多特殊图像的特定分割任务上表现差强人意，如水下目标分割、阴影分割、伪装对象分割等。作者认为这是由于SAM主要在常见的自然图像中进行训练，其特征提取器不能很好的适应特殊图像。因此作者提出一种轻量化的适配器模块（Adaptor），对SAM的编码器得到的特征图进行适应性调整。编码器的输入为特定任务信息，该文采用了图块嵌入特征和高频成分特征，将两种特征相加后经过两个MLP层得到适配器模块的输出，并将该输出与对应SAM编码器的Transformer层输出相加，并传递至下一层。训练过程中SAM编码器的参数保持不变，解码器部分使用SAM的参数进行初始化，然后利用特定数据集进行微调。</p><figure><img src="./../postimages/SAM/image-20240519202804593.png"alt="image-20240519202804593" /><figcaption aria-hidden="true">image-20240519202804593</figcaption></figure><p>​  如上图所示，该模型使用了SAM的图像编码器和掩模解码器，其中图像编码器冻结了参数，解码器是参与梯度回传的。这样可以有效利用SAM已经预训练好的分割能力，同时解码器更新参数以改装下游任务。另外引入了Adaptor模块，用于引入特殊任务的知识，辅助适配器模型。Adaptor的网络结构由两层MLP层构成，其输入的知识可以是微处理器的，对于文中的任务，其输入可以是纹理信息或者是频率信息等。各种信息用下面的权重来均衡。</p><blockquote><p>该文提出的Adaptor模块包括所使用的两个特定任务信息——图块嵌入特征和高频成分特征，都是来源于另一篇论文《ExplicitVisual Prompting for Low-Level StructureSegmentations》（EVP）。图块嵌入特征就是将图片划分成若干个图块，利用ViT将其映射为一个$ C_{seg} $维的特征；高频成分特征，则是将图片进行快速傅里叶变换，并保留其中的高频成分，再进行反变换得到高频成分对应的时域图，最后经过一个线性映射层得到一个特征向量。</p></blockquote><p>​  实验表明，在多个任务中SAM-Adapter均取得了远超SAM的表现，甚至由于各自领域的其他优秀算法，作为SAM的一种改进思路还是有值得借鉴和学习的地方。然而，整篇论文的思路几乎完全照搬了EVP，只是将模型从SegFormer换成了SAM，其他并没有明显改变。但在实验章节的算法效果对比中却回避了EVP，尤其是有些结果还不如EVP，这就很让人质疑其原创性和先进性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CMX:_Cross-Modal_Fusion_for_RGB-X_Semantic_Segmentation_With_Transformers</title>
      <link href="/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/"/>
      <url>/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/</url>
      
        <content type="html"><![CDATA[<p>CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation WithTransformers(TruFor使用了这个方法)</p><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170446816.png"alt="image-20240516170446816" /><figcaption aria-hidden="true">image-20240516170446816</figcaption></figure><p>1、原任务是分割任务，论文提出了一种将RGB图与其他图特征充分融合的方法RGB-X，可以从RGB图与X图提取特征。</p><p>2、RGB-X主要由两个部分组成：CM-FRM、FFM。</p><p>CM-FRM用于提取图片特征，其可以纠正关于另一个特性的一个特性，反之亦然，将属于同一层次的特征融合成一个单一的特征图。</p><p>FFM参考自注意力机制，设计了一种将特征融合的方法最后通过融合特征，完成分割任务。</p><p>TruFor使用了cmx来提取融合特征</p><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170520387.png"alt="image-20240516170520387" /><figcaption aria-hidden="true">image-20240516170520387</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170527564.png"alt="image-20240516170527564" /><figcaption aria-hidden="true">image-20240516170527564</figcaption></figure><figure><imgsrc="./../postimages/CMX-Cross-Modal-Fusion-for-RGB-X-Semantic-Segmentation-With-Transformers/image-20240516170535748.png"alt="image-20240516170535748" /><figcaption aria-hidden="true">image-20240516170535748</figcaption></figure><p>两阶段的特征融合模块（FFM）来增强信息的交互和组合。</p><p>在信息交换阶段（阶段1），两个分支仍然保持不变，并设计了一种交叉注意机制，在两个分支之间进行全局信息交换。</p><p>在融合阶段（阶段2），通过混合嵌入通道将连接的特征转换为原始大小。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Uncertainty-Uncertainty_Learning_for_Improving_Image_Manipulation_Detection</title>
      <link href="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/"/>
      <url>/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<p>Uncertainty-guided Learning for Improving Image ManipulationDetection</p><h1 id="摘要">摘要</h1><p>​图像操纵检测（IMD）至关重要，因为伪造图像和传播错误信息可能是恶意的，会危害我们的日常生活。IMD是解决这些问题的核心技术，并在两个主要方面提出了挑战：（1）数据不确定性，即被操纵的工件通常很难被人类辨别，并导致噪声标签，这可能会干扰模型训练；（2）模型不确定性，即由于操纵操作，同一对象可能包含不同的类别（篡改或未篡改），这可能会混淆模型训练并导致不可靠的结果。以往的工作主要集中在通过设计细致的特征和网络来解决模型的不确定性问题，但很少考虑数据的不确定性。在本文中，我们通过引入一个不确定性引导的学习框架来解决这两个问题，该框架通过一个新的不确定性估计网络（UEN）来测量数据和模型的不确定性。UEN在动态监督下进行训练，并输出估计的不确定性图来细化操纵检测结果，这显著缓解了学习困难。据我们所知，这是第一项将不确定性建模嵌入IMD的工作。在各种数据集上进行的大量实验证明了最先进的性能，验证了我们方法的有效性和可推广性。</p><p>将不确定性引入图片篡改检测：Model不确定性（同一对象因不同的模型标记不同）与 Data不确定性（误标签与漏标签）</p><p>Model不确定性由U^p测定，p_t是第 t次采样中的估计操纵得分图；U^GT是真实值 y 和 μ̂之间的差异，可以测定Model不确定性和Data不确定性，可以用动态不确定性监督Lu，让U^GT专注于Data不确定性</p><p>其主干网络是HRNetV2 （特征提取网络）</p><p>模型基于数据不确定性可以增强复杂边缘的篡改检测，精细化粗略输出的边缘，得到更好的结果</p><p>该模型是在7张A100上训练，可见训练难度大，不易收敛</p><figure><imgsrc="./../postimages/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/image-20240516170144843.png"alt="image-20240516170144843" /><figcaption aria-hidden="true">image-20240516170144843</figcaption></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Pre-training-free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning</title>
      <link href="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/"/>
      <url>/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>Pre-training-free Image Manipulation Localization throughNon-Mutually Exclusive Contrastive Learning</p><p>四川大学计算机科学学院</p><p>厦门科技大学计算机与信息工程学院</p><h1 id="摘要">摘要</h1><p>深度图像操作定位（IML）模型存在训练数据不足，严重依赖于预训练。我们认为对比学习更适合于解决IML的数据不足问题。形成相互排斥的正性与负性是对比学习的先决条件。然而，当在IML中采用对比学习时，我们遇到了三类图像补丁：篡改、真实和轮廓补丁。篡改和真实的补丁自然是相互排斥的，但是包含篡改和真实像素的轮廓补丁对它们不是相互排斥的。</p><p>简单地取消这些轮廓补丁会导致巨大的性能损失，因为轮廓补丁对学习结果是决定性的。因此，我们提出了非互斥对比学习（NCL）框架来从上述困境中拯救传统的对比学习。在NCL中，为了应对非互斥性，我们首先建立一个具有双分支的枢轴结构，在训练时不断地在正和负之间切换轮廓补丁的作用。然后，我们设计了一个枢轴一致的损失，以避免由角色转换过程造成的空间损坏。</p><p>通过这种方式，NCL既继承了自监督的优点来解决数据不足，又保留了较高的操作定位精度。大量的实验证明，我们的NCL在没有任何预训练的情况下，在所有五个基准测试上都达到了最先进的性能，并且在看不见的真实样本上更健壮。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ANOMALYCLIP</title>
      <link href="/ANOMALYCLIP/"/>
      <url>/ANOMALYCLIP/</url>
      
        <content type="html"><![CDATA[<p>ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING FOR ZERO-SHOT ANOMALYDETECTION <a href="https://arxiv.org/abs/2310.18961"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/zqhang/AnomalyCLIP"><imgsrc="https://img.shields.io/github/stars/zqhang/AnomalyCLIP?style=flat"alt="GitHub" /></a></p><p><strong>Qihang Zhou</strong>1<em>∗</em> <strong>, GuansongPang</strong>2<em>∗</em> <strong>, Yu Tian</strong>3 <strong>, ShiboHe</strong>1<em>†</em> <strong>, Jiming Chen</strong>1<em>†</em></p><p>1浙江大学2新加坡管理大学3哈佛大学</p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/ANOMALYCLIP/2310.18961v8.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  零样本异常检测（ZSAD, Zero-shot anomaly detection）需要使用辅助数据进行训练的检测模型，以便在目标数据集中没有任何训练样本的情况下检测异常。这是一个至关重要的任务时，当训练数据无法访问由于各种问题，例如，数据隐私，但它是具有挑战性的，因为模型需要推广异常在不同领域前景对象的外观，异常区域和背景特性，如缺陷/肿瘤在不同的产品/器官，可以显著不同。最近，大型的预先训练的视觉语言模型（VLMs），如CLIP，在包括异常检测在内的各种视觉任务中显示出了很强的零样本识别能力。然而，它们的ZSAD性能较弱，因为vlm更关注于前景对象的类语义建模，而不是图像中的异常/正常性。</p><p>​  在本文中，我们介绍了一种新的方法，即AnomalyCLIP，使CLIP在不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习与对象无关的文本提示，这些提示捕获图像中的一般正常性和异常性，而不管其前景对象如何。这使得我们的模型能够关注异常的图像区域，而不是对象的语义，从而能够对不同类型的对象概括归纳正常和异常识别。在17个真实异常检测数据集上进行的大规模实验表明，AnomalyCLIP在不同缺陷检测和医学成像领域的高度多样性类语义数据集的异常检测和分割方面具有优越的零镜头性能。</p><h1 id="引用">引用</h1><p>本文的主要贡献如下。</p><p>  1.我们首次揭示了学习对象不可知的文本提示的正常和异常是一种简单而有效的准确的ZSAD方法。与目前主要为对象语义对齐而设计的文本提示方法(Jeong et al., 2023; Zhou et al.,2022b)相比，我们的文本提示嵌入模型语义的一般异常和正常，允许对象无关，广义ZSAD性能。<br/>  1.然后，我们引入了一种新的ZSAD方法，称为AnomalyCLIP，其中我们利用一个对象不可知的提示模板和一个g局部异常损失函数（即全局和局部损失函数的组合）来学习通用异常和正常提示。在此过程中，AnomalyCLIP在很大程度上简化了提示设计，并可以有效地应用于不同的领域，而不需要更改其学习到的两个提示，这与WinCLIP等现有的方法不同，后者的有效性很大程度上依赖于对数百个手动定义提示的广泛工程。<br/>  1.对来自不同工业和医学领域的17个数据集进行的综合实验表明，AnomalyCLIP在检测和分割来自缺陷检查和医学成像领域的高度多样性类语义数据集的异常方面具有优越的ZSAD性能。</p><h1 id="方法">方法</h1><h2 id="与对象无关的提示学习">与对象无关的提示学习</h2><h3 id="方法概述">方法概述</h3><p>​  在本文中，我们提出了一种催化CLIP通过对象不可知的提示学习使CLIP适应ZSAD。</p><figure><img src="./../postimages/anomalyClip/image-20240519164440456.png"alt="image-20240519164440456" /><figcaption aria-hidden="true">image-20240519164440456</figcaption></figure><p>图2：AnomalyCLIP的概述。为了使CLIP适应于ZSAD，AnomalyCLIP引入了与对象无关的文本提示模板来捕获一般的正常性和异常性，而不管对象的语义如何。然后，我们引入了全局上下文优化，将全局和细粒度的异常语义纳入到对象不可知的文本提示学习中。最后，使用文本提示调优和DPAM，在CLIP的文本和局部视觉空间中实现提示学习。</p><p>​  如图2所示，AnomalyCLIP首先引入了对象不可知的文本提示模板，其中我们设计了$ g_n $ 和 $ g_a $的两个通用的对象不可知的文本提示模板，分别学习正态类和异常类的广义嵌入(见Sec.3.2)。为了学习这种通用的文本提示模板，我们引入了全局和局部上下文优化，将全局和细粒度的异常语义合并到与对象无关的文本嵌入学习中。此外，文本提示调优和DPAM还被用于支持在CLIP的文本和局部视觉空间中的学习。最后，我们整合了多个中间层，以提供更多的局部视觉细节。在训练过程中，所有模块通过全局和局部上下文优化进行联合优化。在推理过程中，我们量化了文本和全局/局部视觉嵌入的错位，以分别获得异常得分和异常得分图(见Sec.3.3)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dual_Defense</title>
      <link href="/Dual-Defense/"/>
      <url>/Dual-Defense/</url>
      
        <content type="html"><![CDATA[<p>Dual Defense: Adversarial, Traceable, and Invisible RobustWatermarking Against Face Swapping</p><h1 id="摘要">摘要</h1><p>​深度人脸交换技术的恶意应用构成了虚假信息传播和身份欺诈等安全威胁。一些研究提出了利用鲁棒水印方法来跟踪人脸图像的版权，促进伪造后的身份归属。然而，这些方法并不能从根本上防止或消除换脸的不利影响。为了解决这个问题，我们提出了双重防御，这是一种基于<strong>鲁棒对抗性水印</strong>的创新框架。它通过一次嵌入鲁棒对抗性水印，同时跟踪图像版权并破坏人脸交换模型。</p><p>​ 具体而言，我们提出了一种原域特征冒充攻击（OFEA, Original-domainFeature EmulationAttack）方法，该方法通过专门设计的原始域对抗性损失，使可跟踪水印更具攻击性。此外，我们将小波域图像结构信息补偿损失与通道注意力机制相结合，以联合平衡水印的不可见性、对抗性和可追溯性。此外，我们设计了一种更全面、更合理的评估方法来全面评估对抗性攻击对人脸交换模型的有效性。大量实验表明，双重防御表现出非凡的跨任务通用性和数据集泛化能力。它在原始和稳健的环境中都保持了令人印象深刻的对抗性和可追溯性，超过了目前仅拥有其中一种功能的伪造防御方法。</p><figure><img src="./../postimages/Dual-Defense/image-20240513111747304.png"alt="image-20240513111747304" /><figcaption aria-hidden="true">image-20240513111747304</figcaption></figure><p>图1。深度伪造主动防御场景的说明。基于水印的(a)主动防御。能够追踪伪造图像的来源，但不能防止伪造，并消除其在来源上的不利影响。基于对抗性例子的(b)积极防御。它可以破坏伪造文件的生成，但不支持可追溯性，在攻击失败时不提供可追溯性基础。(c)，我们的双重防御，主动防御。在跟踪面部图像版权的同时，它可以在确保水印完整性的同时破坏FaceSwap模型。此外，它还提供了在攻击失败时的辅助可跟踪性。</p><h1 id="引言">引言</h1><p>​ 我们的贡献可以总结为：</p><ol type="1"><li><p>我们提出了一种新型的可追溯性对抗性水印网络，这是第一种结合了对抗性和可追溯性的针对人脸交换模型的双效应主动防御方法。它具有优异的鲁棒性、跨任务通用性和数据集泛化能力。</p></li><li><p>我们创新性地提出了OFEA方法，通过将可追溯的水印嵌入到载体的鲁棒对抗性特征中，使其具有对抗性。同时，我们通过结合一个专门设计的小波域结构信息补偿损失来解决水印多目标学习中的优化冲突。</p></li><li><p>我们专门设计了一种更合理、更全面的评估方法来充分评估对脸交换的逆性。结合传统的评估指标，我们已经证明了双重防御在三个大数据集上的源跟踪和对抗性攻击中的双重有效性。</p></li></ol><h1 id="网络">网络</h1><p>Dual Defense整体算法流程如图所示：</p><figure><img src="./../postimages/Dual-Defense/image-20240513113840540.png"alt="image-20240513113840540" /><figcaption aria-hidden="true">image-20240513113840540</figcaption></figure><p>图3。双重防御的整个管道。双防御系统通过端到端训练来优化水印模型。该过程首先将目标图像$X_t <span class="math inline">\(和用户定义的水印\)</span> W_{ID}$输入到编码器中，以生成水印图像。随后，水印图像进行FaceSwap进行原始域特征冒充攻击（OFEA），计算原始域对抗损失。受干扰的图像和水印图像都通过水印解码器进行解码器优化。</p><p>​ 。</p><h1 id="实验">实验</h1><h2 id="定量结果">定量结果</h2><p>​本文首先在CASIA-WebFace、VGGFace2和LFW三个大型人脸数据集上，从不可见性、对抗性以及可溯源性三个方面的多个指标全面评估DualDefense的性能。实验表明DualDefense在保证不可见性的同时实现了出色的对抗性以及水印恢复精度。此外，通过跨图像身份及跨数据集测试表明，DualDefense具有显著的身份通用性和数据集泛化性</p><p>Dual Defense在原始设置下的定量结果。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114244119.png"alt="image-20240513114244119" /></p><p>​此外，本文与典型的基于对抗攻击的和基于深度水印的深度伪造主动防御方法分别在原始场景和各种不同的鲁棒场景下进行了对比。实验表明，DualDefense在对抗性及可溯源性方面都几乎保持最优的性能。尤其在对图像进行后处理后，对抗攻击方法的对抗性显著降低，而DualDefense依旧保持显著的对抗性能。</p><p>Dual Defense与其他主动防御方法的比较。N/A表示该方法无对应性能。<br/><imgsrc="./../postimages/Dual-Defense/image-20240513114456325.png"alt="image-20240513114456325" /></p><p>​在真实社交网络传输信道中，图像通常经历各种后处理操作。因此，本文评估了DualDefense在四种常见图像后处理操作下对 FaceSwap的对抗性和可溯源性，实验表明当水印图像经过各种处理操作时，DualDefense始终保持出色的性能，从而验证了本文方法在实际场景中的可行性。</p><figure><img src="./../postimages/Dual-Defense/image-20240513114605619.png"alt="image-20240513114605619" /><figcaption aria-hidden="true">image-20240513114605619</figcaption></figure><p><img src="./../postimages/Dual-Defense/image-20240513114617035.png"alt="image-20240513114617035" /><imgsrc="./../postimages/Dual-Defense/image-20240513114636009.png"alt="image-20240513114636009" /></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning</title>
      <link href="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/"/>
      <url>/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Generic Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning<ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-blue" alt="ICCV" /></a><ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a><ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></p><p>University at Buffalo</p><h1 id="摘要">摘要</h1><p>​随着先进的图像处理技术的出现，检测操作变得越来越重要。尽管最近基于学习的图像操作检测方法取得了成功，但它们通常需要昂贵的像素级注释来进行训练，同时在与训练图像相比被不同操作的图像上进行测试时表现出较差的性能。为了解决这些局限性，我们提出了<strong>弱监督图像操作检测</strong>，使得训练目的只需要二进制图像级别的标签（真实或篡改）。这种弱监督设置可以利用更多的训练图像，并有可能快速适应新的操作技术。为了提高泛化能力，我们提出了弱监督自一致性学习（WSCL）来利用弱注释图像。具体来说，学习了两个一致性属性：多源一致性（MSC,multi-source consistency）和补丁间一致性（IPC, inter-patch consistency）。MSC利用不同的内容无关信息，并通过在线伪标签生成和细化过程实现跨源学习。IPC执行全局成对补丁关系推理，以发现完整的操作区域。大量实验验证了我们的WSCL，即使是弱监督的，在分布内和分布外评估下，与完全监督的WSCL相比，也表现出竞争性能，以及合理的操纵定位能力。</p><p>The single-stream overview:</p><figure><imgsrc="/postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171025018.png"alt="image-20240510171025018" /><figcaption aria-hidden="true">image-20240510171025018</figcaption></figure><p>图2:单流概述。给定输入图像，基线方法（上图）预测操作图。预测图由基于自适应池的分类损失$ L_{A-CLS} $ 和多源一致性学习损失 $ L_{MSC} $监督。同时，补丁间一致性分支（底部）学习测量补丁相似性的一致性体积。一致性卷由补丁间一致性丢失$ L_{IPC} $ 来监督。</p><p>Multi-source overview:</p><figure><imgsrc="/postimages/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/image-20240510171253974.png"alt="image-20240510171253974" /><figcaption aria-hidden="true">image-20240510171253974</figcaption></figure><p>图3。多源一致性学习的例证。分别在RGB图像、SRM噪声图和Bayar噪声图上训练三个并行流。它们的加权平均预测被用作伪地面实况，以监督每个流。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>损失函数合集</title>
      <link href="/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉熵损失crossentropyloss">交叉熵损失CrossEntropyLoss</h1><p>交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight = imbalance_weight)</span><br><span class="line">loss = criterion(pred, tar.long().detach()) </span><br></pre></td></tr></table></figure><h1 id="块对比损失patchcontrastloss">块对比损失PatchContrastLoss</h1><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><p>  我们首先将 $ F∈R^{256×H×W} $ 在空间上划分为k×k个块，从而得到 $f_iR^{256hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个 $ f_i $都变成了 $ R^{256} $的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到$ m_iR^{hw} $ ，其中 $ i{1,2,3…k^2} $ 、 $ h= $ 和 $ w= $ 。为了得到每个$ m_i $的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为$ m_i $ 的值。</p><p>  然后，我们有了像素嵌入 $ f_i $ 和每个嵌入 $ m_i $的相应标签。我们现在得到监督对比损失： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中， $ A_i $表示与 $ f_i $ 具有相同标签的所有其他像素嵌入 $ k^+ $ 的集合。类似地， $k^− $ 是所有与 $ f_i $ 有不同标签的负像素嵌入。损失函数中的所有嵌入都是$ L_2 $归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span></p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/CFL-Net-contrast-loss.drawio.png"alt="CFL-Net-contrast-loss.drawio" /><figcaption aria-hidden="true">CFL-Net-contrast-loss.drawio</figcaption></figure><h1 id="infonce对比损失">InfoNCE对比损失</h1><p>​  FOCAL 的训练过程如图3 (b)所示：</p><figure><img src="/postimages/FOCAL/image-20240518144850862.png"alt="image-20240518144850862" /><figcaption aria-hidden="true">image-20240518144850862</figcaption></figure><p>​  一旦我们从给定的输入X中提取高级特征F，我们就通过像素级对比学习直接监督F。地面真实伪造掩模Y自然为我们提供了正和消极类别的索引，使有效的像素级对比学习。正如很快就会更清晰的那样，焦点的对比学习以逐图像的方式进行监督，这与现有的对整个正向小批执行监督的算法[19,6,15,54,56]有很大的不同。</p><p>​  具体来说，我们采用了一种改进的InfoNCE损失[16,35]来实现焦点中的对比学习。</p><p>​  我们首先通过执行一个扁平化操作来构造一个字典 $ f( \cdot ) : {\mathbb {R}}^{ \hat{H} \times \hat{W} \times \hat{C}} \rightarrow {\mathbb{R}}^{ \hat{H} \hat{W} \times \hat{C}} $ <spanclass="math display">\[f(F) \rightarrow \{ q , k^+_1 , k^+_2 , ...,k^+_J , k^-_1 , k^-_2 , ..., k^-_K \}\]</span> ​  其中， $ { q , k^+_1 ,k^+_2 , ..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K } $被定义为字典，q是一个编码查询。我们让 $ { q , k^+_1 , k^+_2 , ..., k^+_J} $ 表示属于原始区域的特征（以Y中的0为索引），而 $ { k^-_1 , k^-_2 ,..., k^-_K } $ 表示伪造区域的特征（以Y中的1索引）。</p><p>​  在图像伪造检测任务中，伪造或原始区域通常覆盖超过1像素（特征）的区域，这意味着字典中正键J的数量也远远大于1。然后，根据图像伪造任务而定制的改进的信息损失可以计算为<span class="math display">\[{\cal {L}}_{InfoNCE++}=-log \frac { \frac 1J \sum _{ j \in [1,J] } exp(q \cdot k^+_J / \tau ) } { \sum _{ i \in[1,K] } exp(q \cdot k^+_i / \tau ) }\]</span> ​  其中， $ $是一个温度超参数[51]。注意，在原始的InfoNCE loss[16,35]中，字典中只有一个q匹配的正键。在我们改进的InfoNCE损失(2)中，我们通过取q的$ { k^+_j } $的期望，在每个损失计算中涉及所有的正键。这将促进优化过程。</p><p>​  需要强调的是，训练阶段的监督是直接在地面真实伪造掩模Y和提取的特征F之间进行的，而没有生成预测的伪造掩模。</p><p>​  此外，对于前向小批量中的每一幅图像， $ { \cal {L}} _ { InfoNCE++ }$以逐图像的方式（one-by-one）计算，而不是对整个批量进行计算，然后求和计算总体损失。更具体地说，给定一个小批特征$ {F_1、F_2、···、F_B} $ ，总体对比损失 $ { \cal {L}} _ { ct } $ : <spanclass="math display">\[{\cal {L}}_{ct}=\frac {1} {B} \sum _{b=1} ^{B}({\cal {L}}_{InfoNCE++}(F_b))\]</span>​  请注意，在上述（3）式中，没有合并小批特征来计算整体的 $ { \cal{L}}_{InfoNCE++} $，避免了训练数据的交叉图像影响。在伪造/原始像素的相对定义的指导下设计的总损失与[5,15,16,32]中的损失有很大的不同，[5,15,16,32]中的损失计算是在批处理级别进行的。</p><p>​  为了进一步证明(3)的合理性，我们在图4中绘制了传统的基于批处理和我们的逐图像图像的对比损失曲线。<br/><imgsrc="/postimages/FOCAL/image-20240518153103103.png"alt="image-20240518153103103" /></p><p>​  可以清楚地看到，损失函数（橙色线）的逐图像设计不仅使收敛速度更快，而且使优化更加稳定。特别是，在蓝线中检测到的高振幅脉冲表明相关的图像中可能存在严重的冲突，例如，类似于图2(a)和(b)的情况，其中出现冲突的标签。</p><figure><imgsrc="/postimages/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20240519102823673.png"alt="image-20240519102823673" /><figcaption aria-hidden="true">image-20240519102823673</figcaption></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAIG课代表</title>
      <link href="/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/"/>
      <url>/AAIG%E8%AF%BE%E4%BB%A3%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>b站up主 <strong>AAIG课代表</strong></p><table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 10%" /><col style="width: 39%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th style="text-align: left;">关键词</th><th>title</th><th>description</th></tr></thead><tbody><tr class="odd"><td>1</td><td>BV1qr421u7vJ</td><td style="text-align: left;">安全大模型</td><td>揭秘阿里安全审核领域的“专家”｜集内容安全/舆情分析/代码漏洞修复为一身的AI安全大模型长什么样子？</td><td></td></tr><tr class="even"><td>2</td><td>BV1aF4m1w7RB</td><td style="text-align: left;">机器语言大模型ML</td><td>《追AI的人》第38期直播清华大学网络研究院副院长张超教授分享《机器语言大模型MLM：面向软件安全分析》</td><td></td></tr><tr class="odd"><td>3</td><td>BV1dt421j7Fy</td><td style="text-align: left;"></td><td>揭秘阿里AI的全生命期风险治理实践！</td><td></td></tr><tr class="even"><td>4</td><td>BV1DM4m1Q7Kt</td><td style="text-align: left;">内容安全</td><td>防范看不见的直播“陷阱”！解密内容安全背后的四大挑战！电商平台五大要素是？分别有哪些风险？</td><td></td></tr><tr class="odd"><td>5</td><td>BV1KJ4m157Sn</td><td style="text-align: left;">阿里安全大模型</td><td>《追AI的人》第37期阿里巴巴安全部御风大模型算法能力负责人秦鹏达分享《如何用安全大模型锻造企业“盾牌”？浅谈阿里安全大模型的实践应用！》</td><td></td></tr><tr class="even"><td>6</td><td>BV1YH4y1H7zZ</td><td style="text-align: left;">生成式AI</td><td>文字变电影?!揭秘生成式AI的五大构成要素！</td><td></td></tr><tr class="odd"><td>7</td><td>BV11x4y1m7Aa</td><td style="text-align: left;">Sora</td><td>6分钟Get生成式内容检测技术,让你拥有辨别Sora的"火眼金睛"！</td><td></td></tr><tr class="even"><td>8</td><td>BV18i421Z7vH</td><td style="text-align: left;">Sora</td><td>2分钟解密全网爆火的Sora！Sora是什么？它的主要特点是？</td><td></td></tr><tr class="odd"><td>9</td><td>BV1Wz421X7GM</td><td style="text-align: left;">Sora</td><td>《追AI的人》第36期阿里安全刘佳睿分享《解密全网爆火的Sora：如何区分真实与AI生成内容？》</td><td></td></tr><tr class="even"><td>10</td><td>BV1gU421o7e4</td><td style="text-align: left;">生成式AI</td><td>AI生成图与电影画面傻傻分不清？揭秘AI绘画成功背后的技术支持</td><td></td></tr><tr class="odd"><td>11</td><td>BV1i44y1F7o5</td><td style="text-align: left;"></td><td>《追AI的人》第35期中国社会科学院大学互联网法治研究中心执行主任刘晓春分享《人工智能发展的数据流通制度基础》</td><td></td></tr><tr class="even"><td>12</td><td>BV1sC4y1C7wk</td><td style="text-align: left;"></td><td>人工智能"上位"成功？揭秘ChatGPT成功背后的技术突破！</td><td></td></tr><tr class="odd"><td>13</td><td>BV1sw411g7Ui</td><td style="text-align: left;"></td><td>《追AI的人》第34期直播回放复旦大学张谧教授分享《当“巨兽”成为“宠物”：复旦白泽带你领略大模型安全伦理风险与治理》》</td><td></td></tr><tr class="even"><td>14</td><td>BV1Tb4y1j7iU</td><td style="text-align: left;"></td><td>生成式AI能减小人与人之间的差异？生成式AI互动机制如何能够更有效地产生结果？</td><td></td></tr><tr class="odd"><td>15</td><td>BV1Tg4y1f7gf</td><td style="text-align: left;"></td><td>《追AI的人》第33期直播回放中山大学网络空间安全学院院长操晓春教授分享《“病态的”的计算机视觉算法》</td><td></td></tr><tr class="even"><td>16</td><td>BV1yu411F7E5</td><td style="text-align: left;"></td><td>为什么生成式AI更容易取代白领员工,对体力工作者的影响却微乎其微?</td><td></td></tr><tr class="odd"><td>17</td><td>BV1fN41137zv</td><td style="text-align: left;">数字水印</td><td>《追AI的人》第32期直播回放中国科学技术大学张卫明教授分享《人工智能背景下的数字水印》</td><td></td></tr><tr class="even"><td>18</td><td>BV1Wy4y1c7Dm</td><td style="text-align: left;"></td><td>《追AI的人》第31期直播回放清华大学经济管理学院领导力与组织管理系主任李宁教授分享《人机协同、效率与创新：AI时代的组织模式探索》</td><td></td></tr><tr class="odd"><td>19</td><td>BV1Rh4y1i75x</td><td style="text-align: left;"></td><td>《追AI的人》第30期直播上海交通大学张拳石教授分享《较真地追问，神经网络是否可以被严谨地彻底地解释清楚？》</td><td></td></tr><tr class="even"><td>20</td><td>BV1XH4y1o7Nh</td><td style="text-align: left;">多模态信号融合</td><td>从生物心理学角度看多模态大模型发展史！多模态信号如何融合？</td><td></td></tr><tr class="odd"><td>21</td><td>BV1nm4y1N7Q7</td><td style="text-align: left;">安全伦理</td><td>《追AI的人》第29期直播回放复旦大学桂韬老师分享《大模型有何安全伦理风险问题？看MOSS-RLHF如何实现人类与AI的价值观对齐》</td><td></td></tr><tr class="even"><td>22</td><td>BV1M94y1s76K</td><td style="text-align: left;">深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td><td></td></tr><tr class="odd"><td>23</td><td>BV178411v7xy</td><td style="text-align: left;"></td><td>《追AI的人》第13期直播回放IEEE亚太区执委、人道主义科技活动委员会主席董晶分享《AI前沿技术对抗中的”天使”与“恶魔”》</td><td></td></tr><tr class="even"><td>24</td><td>BV1m34y1P7oY</td><td style="text-align: left;"></td><td>《追AI的人》第14期直播回放中国政法大学兼职教授、盈理律师事务所资深顾问赵军分享《《对人工智能产业发展四大要素的保护——数据与知识产权的挑战与实践》》</td><td></td></tr><tr class="odd"><td>25</td><td>BV1QF411k7LA</td><td style="text-align: left;"></td><td>3分钟Get多模态是什么？不许在脑海中想象一头粉红色的大象，你想的是什么？</td><td></td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td style="text-align: left;">图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td><td></td></tr><tr class="odd"><td>27</td><td>BV1dG411o7fn</td><td style="text-align: left;"></td><td>《追AI的人》第27期直播回放AAIG图片视觉大模型与视觉AIGC安全算法负责人洪海文分享《多模态大模型的发展与攻防</td><td></td></tr><tr class="even"><td>28</td><td>BV1DV4y1v7VS</td><td style="text-align: left;"></td><td>以ChatGPT为例3分钟解密生成式语言模型的训练过程！</td><td></td></tr><tr class="odd"><td>29</td><td>BV1nu4y1S7eX</td><td style="text-align: left;"></td><td>教你分清生成式AI、大模型、AIGC！Bert、T5、ChatGPT三者有什么区别？</td><td></td></tr><tr class="even"><td>30</td><td>BV1qP411C7KT</td><td style="text-align: left;"></td><td>“吃“书狂魔是怎么炼成的？解密ChatGPT数据集之谜！</td><td></td></tr><tr class="odd"><td>31</td><td>BV14j411S7ys</td><td style="text-align: left;"></td><td>从感知机到大模型,3分钟读懂AIGC的前世今生！</td><td></td></tr><tr class="even"><td>32</td><td>BV1na4y1A7LK</td><td style="text-align: left;"></td><td>《追AI的人》第26期直播回放阿里巴巴人工智能治理与可持续发展研究中心AAIG人工智能安全实验室主任张荣分享《我们给生成式模型做一个体检》》</td><td></td></tr><tr class="odd"><td>33</td><td>BV1Vo4y1N7Lg</td><td style="text-align: left;"></td><td>《追AI的人》第25期直播回放橙盾科技-塔玑虚拟模特产品算法负责人郎一宁分享《从灵魂画师到光速写手，挖一挖大模型背后的知识结构》</td><td></td></tr><tr class="even"><td>34</td><td>BV1oc411G7qb</td><td style="text-align: left;"></td><td>震惊!白领职业将被替代?如何和AI合作共赢?工作的毁灭和创造的时代来了!</td><td></td></tr><tr class="odd"><td>35</td><td>BV1Da4y1u7bg</td><td style="text-align: left;"></td><td>ChatGPT来抢饭碗了!盘点20个最有可能被取代的职业!</td><td></td></tr><tr class="even"><td>36</td><td>BV1cz4y1a7or</td><td style="text-align: left;"></td><td>《追AI的人》第24期直播回放北京大学国家发展研究院助理教授胡佳胤分享《生成式AI的时代，我们的工作会变成什么样子？》</td><td></td></tr><tr class="odd"><td>37</td><td>BV1bz4y1Y7ss</td><td style="text-align: left;"></td><td>用户、网店、骑手、监管部门如何看待算法?算法是否能完全透明?</td><td></td></tr><tr class="even"><td>38</td><td>BV12c411n7Q1</td><td style="text-align: left;"></td><td>大数据杀熟现象:商业惯例还是不道德行为？如何定义？</td><td></td></tr><tr class="odd"><td>39</td><td>BV15g4y1T7ey</td><td style="text-align: left;"></td><td>什么是三近一反原则?流量为王的时代,教你如何“破圈”创作!</td><td></td></tr><tr class="even"><td>40</td><td>BV1PN411A7FF</td><td style="text-align: left;"></td><td>《追AI的人》第23期直播回放中国广告协会法律与道德工作委员会常务委员杜东为分享《算法治理拉开帷幕，知识需不断碰撞融合》</td><td></td></tr><tr class="odd"><td>41</td><td>BV1sv4y1V7zN</td><td style="text-align: left;"></td><td>《追AI的人》第22期直播回放清华大学副教授眭亚楠老师分享《AI助力瘫痪患者恢复站立和行走》</td><td></td></tr><tr class="even"><td>42</td><td>BV1qg4y1t7xR</td><td style="text-align: left;"></td><td>《追AI的人》第21期直播回放阿里巴巴人工智能治理与可持续发展研究中心算法专家许皓天分享《ChatGPT的前世今生与风险治理》</td><td></td></tr><tr class="odd"><td>43</td><td>BV1yY4y1y77L</td><td style="text-align: left;"></td><td>这份安全收快递指南请收好！这份安全快递指南请收好！如何减少隐私泄漏的风险？如何自我保护？</td><td></td></tr><tr class="even"><td>44</td><td>BV1g54y1c7i3</td><td style="text-align: left;"></td><td>《追AI的人》第20期直播回放南开大学陈兵老师分享《以友好型算法治理为中心，推进可信人工智能健康发展》</td><td></td></tr><tr class="odd"><td>45</td><td>BV1mP4y1k7aM</td><td style="text-align: left;"></td><td>《追AI的人》直播第18期对外经贸许可老师分享《中国民众如何看待算法：经验与规范》</td><td>我们已然生活在“算法社会”中，无处不在的算法无时不刻不在改变着我们的生活和工作。为了更好地了解我国民众对算法应用真实的感受和评价，完善我国算法治理，我们开展了“算法应用用户感知”大规模实证调查，并且由此得出了富含意蕴的启示。</td></tr><tr class="even"><td>46</td><td>BV1p3411d7eC</td><td style="text-align: left;"></td><td>《追AI的人》直播第19期清华梁正老师分享《人工智能治理亟待标准落地》</td><td>过去两年来，世界范围内人工智能治理加速推进，欧盟、美国、英国相继出台了相关法律、政策与战略文件，有专家总结人工智能领域的“治理竞争”已经开始。国内来看，随着数安法、个保法、以及跨境数据流动、数据治理、算法治理、伦理治理等领域重要法规和文件的颁布实施，我国在数字经济乃至人工智能领域的治理架构已基本成型。当前人工智能治理面临的一个突出矛盾是如何将相关合规要求落实为可以具体操作实施的举措，而在这方面，作为治理基本指引和行业最佳实践的标准恰恰可以发挥重要的作用。标准作为“软法”在人工智能治理中如何发挥作用？国</td></tr><tr class="odd"><td>47</td><td>BV1Bd4y1572B</td><td style="text-align: left;"></td><td>大数据是如何精准猜透你的心（下）如何减少推荐用户不喜欢的内容？如何增加推荐系统的多样性？</td><td></td></tr><tr class="even"><td>48</td><td>BV1EG4y1L7eg</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（下）</td><td></td></tr><tr class="odd"><td>49</td><td>BV1qe4y1V7NV</td><td style="text-align: left;"></td><td>信息茧房和马太效应是什么?大数据如何精准猜透你的心?</td><td>在日常生活中，你有没有遇到过这种情况：只要你点赞或收藏了某个短视频，就会持续收到同类型的内容？不仅仅是短视频，还有听音乐的每日推荐，是不是也都是你经常听的类型或者歌手？其实，这些都属于个性化推荐！</td></tr><tr class="even"><td>50</td><td>BV1g44y1R7bK</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享会（中）</td><td></td></tr><tr class="odd"><td>51</td><td>BV1uD4y1V7hF</td><td style="text-align: left;"></td><td>《追AI的人》联合清华特别场NeurIPS 2022 AI安全论文分享(上)</td><td></td></tr><tr class="even"><td>52</td><td>BV16P411u7ea</td><td style="text-align: left;"></td><td>《追AI的人》第16期直播重庆邮电大学校长高新波老师分享《人工智能的未来发展趋势分析》</td><td>近年来，人工智能获得了迅猛的发展，面向特定领域取得了诸多单点突破。但是，从本质上看今天人工智能的成功更多来自“勤能补拙”，距离真正的智慧还有较大的距离。为了使人工智能聪明更可靠。《追AI的人》第16期直播首先介绍了目前人工智能发展所遇到的瓶颈问题，然后提出了未来人工智能发展的六个方向，即绿色低碳更灵巧的人工智能、知识数据双驱动的人工智能、人机物融合的混合人工智能、可信可靠可解释的人工智能、非深度神经网络的人工智能、开放环境自适应的人工智能。最后，一起展望未来人工智能的发展前景。</td></tr><tr class="odd"><td>53</td><td>BV1wW4y1W764</td><td style="text-align: left;"></td><td>什么是大数据杀熟？揭秘“杀熟”误区！为什么老用户看到的价格比新用户更高？</td><td>什么是大数据杀熟？揭秘“杀熟”误区！《如何获取消费者对电商平台价格和用户权益的信任》为什么老用户看到的价格比新用户更高？为什么不少用户会产生“被杀熟”的想法？阿里针对不同用户，可能在获得的优惠结果上不一致的情况下，如何实施遵循原则以保障用户的权益公平性</td></tr><tr class="even"><td>54</td><td>BV1Pv4y1D7Za</td><td style="text-align: left;"></td><td>《追AI的人》直播第15期AAIG自然语言理解实验室EMNLP专场《机器=冰冷？看机器如何捕捉你的小情绪》《文本如药，如何精确提炼“有效成分”？》》</td><td>Part 1: 《机器=冰冷?看机器如何捕捉你的小情绪》人工智能领域日新月异，人们已经不满足于让机器完成指定的任务，从如今大火的各类语音助手和数字人可以看出，“拟人化”是人工智能应用的重大需求。为了让人机对话过程更加丝滑，理解用户的情绪是关键一步。计算机该如何理解我们的情绪呢？构建有人情味的人工智能应用有哪些困难？第一部分将简要对此进行介绍。1、对话中的情感分析问题的定义、应用和当前的困难2、基于有监督原型对比学习的对话情感分析方法3、对话情感分析的未来发展方向 Part 2: 《文本如药,如何</td></tr><tr class="odd"><td>55</td><td>BV1WP411E7Xk</td><td style="text-align: left;"></td><td>如何识别美颜照片的真实性？眼见一定为实吗？虚拟idol也是深度合成吗？什么是深度合成呢？阿里针对深度合成做了哪</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>56</td><td>BV1ae411u7cY</td><td style="text-align: left;"></td><td>算法透明=公开源代码？源代码开放后会带来什么影响和后果呢？为什么算法透明是算法治理的核心要求？</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="odd"><td>57</td><td>BV1Zd4y1d7aa</td><td style="text-align: left;"></td><td>科幻小说中的人类为什么害怕人工智能？如何构建人与人工智能的伦理关系?</td><td>《追AI的人》之AI科普系列短视频，将持续用简单清晰的语言向公众解释对于人工智能的普遍疑问，推动社会就人工智能的发展和治理达成共识。</td></tr><tr class="even"><td>58</td><td>BV1SU4y1v7VL</td><td style="text-align: left;"></td><td>人工智能的生成物能否获得版权的保护？获得著作权保护需要满足哪些条件呢?|《追AI的人》1分钟AI科普短视频</td><td>《追AI的人》系列短视频是一档由阿里巴巴人工智能治理与可持续发展研究中心(AAIG)联合高校和产业界发起的AI治理交互栏目《追AI的人》的衍生短视频。用浅显易懂的词汇把人工智能新技术、AI治理新观点、可持续发展新风向在短短1分钟左右传达给大家，让AI与我们的生活离得更近。</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>JIG图图</title>
      <link href="/JIG%E5%9B%BE%E5%9B%BE/"/>
      <url>/JIG%E5%9B%BE%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<p>b站up主 <strong>JIG图图</strong>的视频一览：</p><table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 15%" /><col style="width: 37%" /><col style="width: 37%" /></colgroup><thead><tr class="header"><th>id</th><th>bv</th><th>关键词</th><th>题目</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>1</td><td>BV1w54y1Q7fj</td><td>机器学习</td><td>【图图Seminar01】汪荣贵：机器学习基本知识体系与入门方法</td><td>《中国图象图形学报》图图Seminar 1期 报告人：合肥工业大学 汪荣贵</td></tr><tr class="even"><td>2</td><td>BV1dz4y1X7a7</td><td>实验数据</td><td>【图图Seminar02】陈强：从Cell封面论文谈AI研究中的实验数据问题</td><td>《中国图象图形学报》图图Seminar 2期 报告人：南京理工大学 陈强</td></tr><tr class="odd"><td>3</td><td>BV1bK411p7En</td><td>低见度图像</td><td>【图图Seminar03】石争浩：从先验到深度：低见度图像智能增强</td><td>《中国图象图形学报》图图Seminar 3期 报告人：西安理工大学 石争浩</td></tr><tr class="even"><td>4</td><td>BV1XK4y1x71s</td><td>画质增强</td><td>【图图Seminar04】任文琦：智能画质增强专题</td><td>《中国图象图形学报》图图Seminar 4期——行知论坛论坛：智能画质增强专题报告人：中国科学院信息工程研究所 任文琦</td></tr><tr class="odd"><td>4</td><td>BV1UD4y1D7s1</td><td>画质增强</td><td>【图图Seminar04】任冬伟：智能画质增强专题</td><td>《中国图象图形学报》图图Seminar 4期——行知论坛：智能画质增强专题报告人：天津大学 任冬伟</td></tr><tr class="even"><td>4</td><td>BV1dp4y1D7Xy</td><td>画质增强</td><td>【图图Seminar04】巩东：智能画质增强专题</td><td>《中国图象图形学报》图图Seminar 4期——行知论坛：智能画质增强专题报告人：阿德莱德大学 巩东</td></tr><tr class="odd"><td>5</td><td>BV1Z5411W78W</td><td>遥感图像</td><td>【图图Seminar05】孙显：遥感图像智能分析：方法与应用</td><td>《中国图象图形学报》图图Seminar 5期报告人：中国科学院空天信息创新研究院 孙显</td></tr><tr class="even"><td>6</td><td>BV1eK411n7aa</td><td>视觉SLAM</td><td>【图图Seminar06】章国锋：视觉SLAM在AR应用上的关键性问题探讨</td><td>《中国图象图形学报》图图Seminar 6期 报告人：浙江大学 章国锋</td></tr><tr class="odd"><td>7</td><td>BV1uZ4y1u7Ri</td><td>机器学习</td><td>【图图Seminar07】林宙辰：机器学习优化算法基础与前沿</td><td>《中国图象图形学报》图图Seminar 7期 报告人：北京大学 林宙辰</td></tr><tr class="even"><td>8</td><td>BV1si4y1V7PM</td><td>医学影像</td><td>【图图Seminar08】《中国图象图形学报》简介及“AI+医学影像”专刊推介</td><td>免费预定2020年10月“AI+医学影像”专刊</td></tr><tr class="odd"><td>8</td><td>BV13k4y1B7BJ</td><td>模糊聚类</td><td>【图图Seminar08】冯朝路：模糊聚类与曲线演化模型——挑战、研究与应用</td><td>《中国图象图形学报》图图Seminar 8期——“医学图像与人工智能”论坛报告人：东北大学 冯朝路</td></tr><tr class="even"><td>8</td><td>BV1Vz4y1D7Lz</td><td>深度学习</td><td>【图图Seminar08】白相志：从简单到复杂问题解决看深度学习</td><td>《中国图象图形学报》图图Seminar 8期——“医学图像与人工智能”论坛报告人：北京航空航天大学 白相志</td></tr><tr class="odd"><td>10</td><td>BV1xa4y1a7xo</td><td>图像感知</td><td>【图图Seminar10】李雷达：以人为中心的图像感知评价：从质量到美学</td><td>《中国图象图形学报》图图Seminar 10期 报告人：西安电子科技大学李雷达</td></tr><tr class="even"><td>11</td><td>BV1GD4y1U7Mq</td><td>深度强化学习</td><td>【图图Seminar11】汪荣贵：深度强化学习第一讲：强化学习与马氏模型</td><td>《中国图象图形学报》图图Seminar 11期——深度强化学习暑期系列课程第一讲报告人：合肥工业大学 汪荣贵</td></tr><tr class="odd"><td>12</td><td>BV1Tt4y1Q7LE</td><td>深度强化学习</td><td>【图图Seminar12】汪荣贵：深度强化学习第二讲：优化计算的基本方法</td><td>《中国图象图形学报》图图Seminar 12期——深度强化学习暑期系列课程第二讲报告人：合肥工业大学 汪荣贵</td></tr><tr class="even"><td>14</td><td>BV17K411T7HZ</td><td>深度强化学习</td><td>【图图Seminar14】汪荣贵：深度强化学习第三讲：面向价值的深度学习</td><td>《中国图象图形学报》图图Seminar 14期——深度强化学习暑期系列课程第三讲报告人：合肥工业大学 汪荣贵</td></tr><tr class="odd"><td>15</td><td>BV1fK4y1f7SH</td><td>深度强化学习</td><td>【图图Seminar15】汪荣贵：深度强化学习第四讲：面向策略的深度学习</td><td>《中国图象图形学报》图图Seminar 15期——深度强化学习暑期系列课程第四讲报告人：合肥工业大学 汪荣贵</td></tr><tr class="even"><td>16</td><td>BV1wV411S7fD</td><td>虚拟服装</td><td>【图图Seminar16】张明敏：虚拟服装中人体拓扑无关的试穿技术研究</td><td>《中国图象图形学报》图图Seminar 16期——“人工智能与虚拟现实”论坛报告人：浙江大学 张明敏</td></tr><tr class="odd"><td>16</td><td>BV1Vk4y127r4</td><td>数字孪生</td><td>【图图Seminar16】郭诗辉：基于织物传感器的人体数字孪生</td><td>《中国图象图形学报》图图Seminar 16期——“人工智能与虚拟现实”论坛报告人：厦门大学 郭诗辉</td></tr><tr class="even"><td>17</td><td>BV1Si4y1M7TX</td><td>遥感图像</td><td>【图图Seminar17】高连如：高光谱遥感图像处理与信息提取</td><td>《中国图象图形学报》图图Seminar 17期报告人：中国科学院空天信息创新研究院 高连如</td></tr><tr class="odd"><td>18</td><td>BV1bh411X7hd</td><td>科研</td><td>【图图Seminar18】杨扬：“出身决定论”？看科研之路如何逆袭！</td><td>《中国图象图形学报》图图Seminar 18期 报告人：云南师范大学 杨扬</td></tr><tr class="even"><td>20</td><td>BV19z4y1Z7T8</td><td>步态识别</td><td>【图图Seminar20】于仕琪：步态识别新动态</td><td>《中国图象图形学报》图图Seminar 20期 报告人：南方科技大学于仕琪</td></tr><tr class="odd"><td>21</td><td>BV1KV411y7Bv</td><td>深度学习</td><td>【图图Seminar21】张拳石：Deep Learning: Interpretability, Capacity,and Evaluation</td><td>《中国图象图形学报》图图Seminar 21期——“深度学习可解释性”论坛报告人：上海交通大学 张拳石</td></tr><tr class="even"><td>25</td><td>BV1BV411t7ta</td><td>人视觉感知</td><td>【图图Seminar25】山世光：从看脸到读心——深刻理解人的视觉感知技术</td><td>《中国图象图形学报》图图Seminar25期——“基于视觉的情感感知技术与应用”论坛报告人：中国科学院计算技术研究所 山世光</td></tr><tr class="odd"><td>25</td><td>BV19t4y1z7wz</td><td>表情识别</td><td>【图图Seminar25】邓伟洪：真实世界人脸与表情识别新问题初探</td><td>《中国图象图形学报》图图Seminar25期——“基于视觉的情感感知技术与应用”论坛 报告人：北京邮电大学邓伟洪</td></tr><tr class="even"><td>26</td><td>BV1AT4y1K7b1</td><td>医学影像</td><td>【图图Seminar26】赵地：深度学习与生物医学影像分析</td><td>《中国图象图形学报》图图Seminar 26期——中关村医学人工智能研讨会报告人：中国科学院计算技术研究所 赵地</td></tr><tr class="odd"><td>26</td><td>BV1UK4y1W7v6</td><td>医学影像</td><td>【图图Seminar26】周少华：医学影像+人工智能的特点、技术与趋势</td><td>《中国图象图形学报》图图Seminar 26期——中关村医学人工智能研讨会报告人：中国科学院计算技术研究所 周少华</td></tr><tr class="even"><td>26</td><td>BV1zy4y1m7YK</td><td>医学影像</td><td>【图图Seminar26】田捷：基于人工智能和医疗大数据的影像组学及其临床应用</td><td>《中国图象图形学报》图图Seminar 26期——中关村医学人工智能研讨会报告人：中国科学院自动化研究所 田捷</td></tr><tr class="odd"><td>27</td><td>BV1BA41157yt</td><td>自动驾驶</td><td>【图图Seminar27】《中国图象图形学报》简介及“自动驾驶技术与应用”专刊推介</td><td>图图Seminar“自动驾驶前沿论坛”——报告人：《中国图象图形学报》陈编辑</td></tr><tr class="even"><td>27</td><td>BV1Hh411D72E</td><td>自动驾驶</td><td>【图图Seminar27】王羽：智能驾驶汽车赛事的科技验证研究</td><td>《中国图象图形学报》图图Seminar 27期——“自动驾驶”论坛报告人：全国汽车行业生产力促进中心主任 王羽</td></tr><tr class="odd"><td>27</td><td>BV16y4y1b7GV</td><td>自动驾驶</td><td>【图图Seminar27】邓伟文：汽车智能驾驶关键技术与挑战</td><td>《中国图象图形学报》图图Seminar 27期——“自动驾驶”论坛报告人：北京航空航天大学 邓伟文</td></tr><tr class="even"><td>27</td><td>BV1LK411w76R</td><td>自动驾驶</td><td>【图图Seminar27】王飞跃：自主无人车如何智能？从平行视觉到平行驾驶</td><td>《中国图象图形学报》图图Seminar 27期——“自动驾驶”论坛报告人：中科院自动化研究所 王飞跃</td></tr><tr class="odd"><td>28</td><td>BV1jU4y1h7z5</td><td>医学影像</td><td>【图图Seminar28】杜宇慧：如何使用脑影像探索脑疾病机制和认识大脑</td><td>《中国图象图形学报》图图Seminar 28期——雁栖湖医学人工智能论坛报告人：山西大学 杜宇慧</td></tr><tr class="even"><td>28</td><td>BV1C54y1b75u</td><td>医学影像</td><td>【图图Seminar28】梁会营：大数据思维范式下的医学影像智能新实践</td><td>《中国图象图形学报》图图Seminar 28期——雁栖湖医学人工智能论坛报告人：广东省人民医院医学大数据研究中心主任 梁会营</td></tr><tr class="odd"><td>28</td><td>BV1Ty4y147ar</td><td>医学影像</td><td>【图图Seminar28】吕乐 :基于多模态医学影像和深度学习的精准癌症肿瘤解决方案：筛查，诊断和预后及病人分层</td><td>《中国图象图形学报》图图Seminar 28期——雁栖湖医学人工智能论坛报告人：平安科技美国华盛顿研究院执行院长 吕乐</td></tr><tr class="even"><td>29</td><td>BV1Wf4y1Y7eD</td><td>迁移学习</td><td>【图图Seminar29】龙明盛：迁移学习理论、算法及开源库</td><td>《中国图象图形学报》图图Seminar 29期——行知论坛 报告人：清华大学龙明盛</td></tr><tr class="odd"><td>30</td><td>BV1xN411Z7Tt</td><td>智能信息伪装</td><td>【图图Seminar30】张卫明：智能信息伪装</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”前沿论坛报告人：中国科学技术大学 张卫明</td></tr><tr class="even"><td>30</td><td>BV1j64y1y7XS</td><td>神经网络水印</td><td>【图图Seminar30】张新鹏：神经网络水印</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”论坛报告人：复旦大学 张新鹏</td></tr><tr class="odd"><td>31</td><td>BV1xK4y1X7fa</td><td>三维重建</td><td>【图图Seminar31】张举勇：三维重建中的高效高精度几何配准</td><td>《中国图象图形学报》图图Seminar 31期——“三维视觉与智能图形”论坛报告人：中国科学技术大学 张举勇</td></tr><tr class="even"><td>31</td><td>BV1nK4y1X7uH</td><td>三维重建</td><td>【图图Seminar31】刘烨斌：三维视觉：过去、现在与未来</td><td>《中国图象图形学报》图图Seminar 31期——“三维视觉与智能图形”前沿论坛报告人：清华大学 刘烨斌</td></tr><tr class="odd"><td>32</td><td>BV1bw411o7jW</td><td></td><td>【图图Seminar32】丛润民，李琦，宋维涛，赵健——IGTA2021</td><td>《中国图象图形学报》图图Seminar32期——第16届图像图形学技术与应用学术会议(IGTA2021) 报告人：北京交通大学丛润民；中科院自动化所 李琦；北京理工大学 宋维涛；中国军事科学院赵健</td></tr><tr class="even"><td>32</td><td>BV1xq4y1s741</td><td>计图</td><td>【图图Seminar32】胡事民：深度几何学习与“计图”框架新进展——IGTA2021</td><td>《中国图象图形学报》图图Seminar32期——第16届图像图形学技术与应用学术会议(IGTA2021) 报告人：清华大学胡事民</td></tr><tr class="odd"><td>32</td><td>BV1rU4y1G7SC</td><td>数学建模竞赛</td><td>【图图Seminar32】魏永生：数学建模竞赛培训方法与获奖技巧探索——IGTA2021</td><td>《中国图象图形学报》图图Seminar32期——第16届图像图形学技术与应用学术会议(IGTA2021) 报告人：江苏师范大学魏永生</td></tr><tr class="even"><td>32</td><td>BV1ug41137iG</td><td>数学建模竞赛</td><td>【图图Seminar32】李雷：大学生数学建模竞赛指导与竞赛——IGTA2021</td><td>《中国图象图形学报》图图Seminar32期——第16届图像图形学技术与应用学术会议(IGTA2021) 报告人：南京邮电大学李雷</td></tr><tr class="odd"><td>32</td><td>BV1t64y1t7cq</td><td>数学建模竞赛</td><td>【图图Seminar32】韩邦合：西电在数学建模竞赛组织方面的一些做法——IGTA2021</td><td>《中国图象图形学报》图图Seminar32期——第16届图像图形学技术与应用学术会议(IGTA2021)报告人：西安电子科技大学 韩邦合</td></tr><tr class="even"><td>33</td><td>BV1jq4y1s78H</td><td>恶劣环境图像智能处理</td><td>【图图Seminar33】山世光，李云松，孟德宇，潘金山，刘家瑛，任文琦——恶劣环境图像智能处理技术高端论坛</td><td>《中国图象图形学报》图图Seminar33期——恶劣环境图像智能处理技术高端论坛 报告人：中国科学院计算技术研究所山世光；西安电子科技大学 李云松；西安交通大学 孟德宇；南京理工大学潘金山；北京大学 刘家瑛；中国科学院信息工程研究所 任文琦</td></tr><tr class="odd"><td>33</td><td>BV1sU4y1G7QU</td><td>恶劣环境图像智能处理</td><td>【图图Seminar33】李波，操晓春，朱策——恶劣环境图像智能处理技术高端论坛</td><td>《中国图象图形学报》图图Seminar33期——恶劣环境图像智能处理技术高端论坛 报告人：北京航空航天大学李波；中国科学院信息工程研究所 操晓春；电子科技大学 朱策</td></tr><tr class="even"><td>34</td><td>BV1QA411w7Qa</td><td>医学影像</td><td>【图图Seminar34】刘天明，徐寿平，刘昊——医学影像及临床应用前沿论坛</td><td>《中国图象图形学报》图图Seminar 34期——医学影像及临床应用前沿论坛报告人：佐治亚大学 刘天明；解放军总医院放疗科 徐寿平；宁夏大学 刘昊</td></tr><tr class="odd"><td>36</td><td>BV1bM4y1T71f</td><td>模式识别</td><td>【图图Seminar36】魏秀参，王旗龙，李莹——南京理工大学模式识别与计算机视觉暑期学校（7/26）</td><td>《中国图象图形学报》图图Seminar36期——南京理工大学模式识别与计算机视觉暑期学校（7/26）报告人：南京理工大学 魏秀参；天津大学 王旗龙；南京师范大学 李莹</td></tr><tr class="even"><td>36</td><td>BV1Eb4y1r7rq</td><td>模式识别</td><td>【图图Seminar36】张长青，连德富，杨杨，尹维冲，张翔宇，邓凯鹏——南京理工大学模式识别与计算机视觉暑期学校（7/25）</td><td>《中国图象图形学报》图图Seminar36期——南京理工大学模式识别与计算机视觉暑期学校（7/25） 报告人：天津大学张长青；中国科学技术大学 连德富；南京理工大学 杨杨；百度 尹维冲；百度张翔宇；百度 邓凯鹏</td></tr><tr class="odd"><td>36</td><td>BV1LL411p7H7</td><td>模式识别</td><td>【图图Seminar36】於东军，陈芝菲——南京理工大学模式识别与计算机视觉暑期学校（7/23）</td><td>《中国图象图形学报》图图Seminar36期——南京理工大学模式识别与计算机视觉暑期学校（7/23）报告人：南京理工大学 於东军；南京理工大学 陈芝菲</td></tr><tr class="even"><td>36</td><td>BV1pM4y1N7rS</td><td>模式识别</td><td>【图图Seminar36】张军，张姗姗——南京理工大学模式识别与计算机视觉暑期学校（7/22）</td><td>《中国图象图形学报》图图Seminar36期——南京理工大学模式识别与计算机视觉暑期学校（7/22）报告人：南京理工大学 张军；南京理工大学 张姗姗</td></tr><tr class="odd"><td>36</td><td>BV1Sf4y157zz</td><td>模式识别</td><td>【图图Seminar36】金忠，黄圣君，王昊奋，谢晋——南京理工大学模式识别与计算机视觉暑期学校（7/21）</td><td>《中国图象图形学报》图图Seminar36期——南京理工大学模式识别与计算机视觉暑期学校（7/21）报告人：南京理工大学 金忠；南京航空航天大学 黄圣君；同济大学王昊奋；南京理工大学 谢晋</td></tr><tr class="even"><td>37</td><td>BV1s341167Ko</td><td>图像图形智能处理</td><td>【图图Seminar37】保文星，周涛，白静，韩强——图像图形智能处理前沿论坛</td><td>《中国图象图形学报》图图Seminar 37期——图像图形智能处理前沿论坛报告人：北方民族大学 保文星、周涛、白静、韩强 主持人：北京航空航天大学白相志</td></tr><tr class="odd"><td>38</td><td>BV1Av411P7CE</td><td>高光谱图像处理</td><td>【图图Seminar38】何明一，李伟，黄鸿，李庆利</td><td>《中国图象图形学报》图图Seminar 38期——高光谱图像处理与应用前沿论坛报告人：西北工业大学 何明一；北京理工大学 李伟；重庆大学黄鸿；华东师范大学 李庆利 主持人：南京理工大学 肖亮</td></tr><tr class="even"><td>39</td><td>BV1BL4y1h7un</td><td>机器学习</td><td>【图图Seminar39】林宙辰：机器学习中优化算法前沿简介</td><td>《中国图象图形学报》图图Seminar 39期 报告人：北京大学 林宙辰</td></tr><tr class="odd"><td>40</td><td>BV113411i7ni</td><td></td><td>【图图Seminar40】吴金文——CSIG文档图像微沙龙21-01期</td><td>《中国图象图形学报》图图Seminar 40期——CSIG文档图像微沙龙21-01期报告人：中国科学院自动化研究所 吴金文</td></tr><tr class="even"><td>42</td><td>BV1ch411J7M7</td><td>图像视频分析与理解</td><td>【图图Seminar42】“图像视频分析与理解”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 42期：首届中国图象图形学报研究生论坛——“图像视频分析与理解”主题论坛</td></tr><tr class="odd"><td>43</td><td>BV1TR4y1H77K</td><td>生物特征识别</td><td>【图图Seminar43】“生物特征识别”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 43期：首届中国图象图形学报研究生论坛——“生物特征识别”主题论坛</td></tr><tr class="even"><td>44</td><td>BV1m34y1U7Hj</td><td>图形技术与3D视觉</td><td>【图图Seminar44】“图形技术与3D视觉”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 44期：首届中国图象图形学报研究生论坛——“图形技术与3D视觉”主题论坛</td></tr><tr class="odd"><td>45</td><td>BV1zL411s7X9</td><td>文档图像分析与识别</td><td>【图图Seminar45】“文档图像分析与识别”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 45期：首届中国图象图形学报研究生论坛——“文档图像分析与识别”主题论坛</td></tr><tr class="even"><td>46</td><td>BV1ZL4y1z7q5</td><td>跨媒体信息处理与内容安全</td><td>【图图Seminar46】“跨媒体信息处理与内容安全”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 46期：首届中国图象图形学报研究生论坛——“跨媒体信息处理与内容安全”主题论坛</td></tr><tr class="odd"><td>47</td><td>BV1nU4y1w7Ma</td><td>医学图像计算</td><td>【图图Seminar47】“医学图像计算”主题论坛——首届中国图象图形学报研究生论坛</td><td>《中国图象图形学报》图图Seminar 47期：首届中国图象图形学报研究生论坛——“医学图像计算”主题论坛</td></tr><tr class="even"><td>48</td><td>BV1UQ4y1B728</td><td></td><td>【图图Seminar48】罗灿杰——CSIG文档图像微沙龙21-02期</td><td>《中国图象图形学报》图图Seminar 48期——CSIG文档图像微沙龙21-02期报告人：华南理工大学 罗灿杰</td></tr><tr class="odd"><td>50</td><td>BV1zS4y1R78H</td><td></td><td>【图图Seminar50】林伟鸿——CSIG文档微沙龙21-03期</td><td>《中国图象图形学报》图图Seminar 50期——CSIG文档图像微沙龙21-03期报告人：微软亚洲研究院 林伟鸿</td></tr><tr class="even"><td>52</td><td>BV13F411z7st</td><td></td><td>【图图Seminar52】章成全——CSIG文档微沙龙21-04期</td><td>《中国图象图形学报》图图Seminar 52期——CSIG文档图像微沙龙21-04期报告人：百度视觉技术部资深研发工程师 章成全</td></tr><tr class="odd"><td>53</td><td>BV1ir4y1v7Rd</td><td></td><td>【图图Seminar53】龙如蛟——CSIG文档微沙龙22-01（总第5）期</td><td>《中国图象图形学报》图图Seminar53期——CSIG文档图像微沙龙22-01（总第5）期 报告人：阿里巴巴达摩院龙如蛟</td></tr><tr class="even"><td>72</td><td>BV18U4y1f7HF</td><td></td><td>【图图Seminar72】方山城——CSIG文档微沙龙22-02期（总第6期）</td><td>《中国图象图形学报》图图Seminar72期——CSIG文档图像微沙龙22-02（总第6期）报告人：中国科学技术大学方山城</td></tr><tr class="odd"><td>73</td><td>BV1QR4y1c7oV</td><td>类脑视觉</td><td>【图图Seminar73】“类脑视觉”主编论坛——图图名师讲堂</td><td>报告专家：北京大学 黄铁军 中国科学院自动化研究所 何晖光论坛主持：北京大学 余肇飞</td></tr><tr class="even"><td>74</td><td>BV1F34y1h7aC</td><td>三维视觉与智能图形</td><td>【图图Seminar74】“三维视觉与智能图形”前沿论坛——图图名师讲堂</td><td>《中国图象图形学报》图图Seminar74期——图图名师讲堂“三维视觉与智能图形”前沿论坛 论坛致辞：北京大学 陈宝权论坛主持：浙江大学 周晓巍 论坛报告：浙江大学 刘勇中国科学院计算技术研究所 高林</td></tr><tr class="odd"><td>75</td><td>BV11F411g7un</td><td></td><td>【图图Seminar75】秦绪功——CSIG文档微沙龙22-03期（总第7期）</td><td>《中国图象图形学报》图图Seminar75期——CSIG文档微沙龙22-03期（总第7期） 报告人：中国科学院信息工程研究所秦绪功</td></tr><tr class="even"><td>76</td><td>BV18Y4y1r7tb</td><td>视频身份识别技术</td><td>【图图Seminar76】“视频身份识别技术”主编论坛——图图名师讲堂</td><td>论坛致辞：中山大学 赖剑煌 报告专家：厦门大学 纪荣嵘 清华大学 王生进南方科技大学 于仕琪 国防科技大学 蓝龙 武汉科技大学 王晓论坛主持：山东科技大学 张鹏</td></tr><tr class="odd"><td>77</td><td>BV1Kr4y147rU</td><td>智能图像安全</td><td>【图图Seminar77】“智能图像安全”主编论坛——图图名师讲堂</td><td>论坛致辞：中国科学技术大学 张卫明 报告专家：华南理工大学 胡永健上海理工大学 秦川 中国科学技术大学 储琪 论坛主持：复旦大学 钱振兴</td></tr><tr class="even"><td>78</td><td>BV1Ut4y1s77v</td><td>医学影像</td><td>【图图Seminar78】“医学影像及临床应用”前沿论坛——图图名师讲堂</td><td>论坛致辞：上海科技大学/联影智能 沈定刚 报告嘉宾：中国科学技术大学周少华 中科院深圳先进技术研究院 梁栋 西北工业大学 夏勇 华中科技大学 邱武上海科技大学 王乾 电子科技大学 蒋希 论坛主持：中山大学 张俭嘉 上海大学施俊</td></tr><tr class="odd"><td>79</td><td>BV1PL4y1F7ba</td><td>复杂场景图像目标智能检测</td><td>【图图Seminar79】“复杂场景图像目标智能检测”主编论坛——图图名师讲堂</td><td>论坛嘉宾：武汉大学 杜博 论坛主持：中山大学 任文琦</td></tr><tr class="even"><td>80</td><td>BV1dU4y1m7zR</td><td>低质图像增强</td><td>【图图Seminar80】“低质图像增强”主编论坛——图图名师讲堂</td><td>论坛致辞：中国科学院计算技术研究所 山世光 报告专家：大连理工大学刘日升 江西财经大学 方玉明 论坛主持：西安理工大学 石争浩</td></tr><tr class="odd"><td>81</td><td>BV1Sa411J7W5</td><td>视觉Transformer</td><td>【图图Seminar81】“视觉Transformer”主编论坛——图图名师讲堂</td><td>论坛致辞：重庆邮电大学 高新波 报告专家：浙江大学 李玺北京航空航天大学 刘偲 大连理工大学 王栋 西安电子大学 王楠楠 之江实验室朱闻韬 论坛主持：厦门大学 王菡子 厦门大学 严严</td></tr><tr class="even"><td>82</td><td>BV1AU4y1m79Q</td><td>OCR学术</td><td>【图图Seminar82】“OCR学术前沿及产业应用”高峰论坛(上午场)——图图名师讲堂</td><td>报告人：中科院自动化所 刘成林 华中科技大学 白翔 微软亚洲研究院 崔磊腾讯 王红法 蚂蚁集团 陈景东 华南理工大学 金连文 中国科学技术大学 谢洪涛北京科技大学 殷绪成</td></tr><tr class="odd"><td>82</td><td>BV1LZ4y1a7zr</td><td>OCR学术</td><td>【图图Seminar82】“OCR学术前沿及产业应用”高峰论坛(下午场)——图图名师讲堂</td><td>报告人：北京科技大学 殷绪成 北京大学 连宙辉 阿里巴巴 杨锐上海合合信息 丁凯 中国科学技术大学 杜俊 中科院信工所 周宇 百度 杜宇宁字节跳动 黄灿 海康威视 程战战 华中科技大学 白翔 华南理工大学 金连文</td></tr><tr class="even"><td>83</td><td>BV1f3411A7DT</td><td>数字图像/视频内容安</td><td>【图图Seminar83】“数字图像/视频内容安全”前沿论坛——图图名师讲堂</td><td>论坛致辞：北京交通大学 赵耀 报告专家：中山大学 谢晓华 湖南大学 欧博深圳大学 陈昌盛 论坛主持：中山大学 卢伟</td></tr><tr class="odd"><td>84</td><td>BV1Y54y1Z7Cc</td><td>图像融合</td><td>【图图Seminar84】“图像融合”主编论坛——图图名师讲堂</td><td>论坛致辞：湖南大学 李树涛 报告专家：武汉大学 马佳义 合肥工业大学刘羽 论坛主持：北京航空航天大学 白相志 江南大学 李辉 论坛总结：江南大学吴小俊</td></tr><tr class="even"><td>86</td><td>BV1jP4y1F7Xw</td><td></td><td>【图图Seminar86】乔梁——CSIG文档微沙龙22-04期（总第8期）</td><td>《中国图象图形学报》图图Seminar86期——CSIG文档微沙龙22-04期（总第8期） 报告人：海康威视研究院 乔梁</td></tr><tr class="odd"><td>87</td><td>BV18Z4y1t7ti</td><td></td><td>【图图Seminar87】李斌——CSIG文档微沙龙22-05期（总第9期）</td><td>《中国图象图形学报》图图Seminar87期——CSIG文档微沙龙22-05期（总第9期） 报告人：复旦大学 李斌</td></tr><tr class="even"><td>88</td><td>BV16Z4y1a7hg</td><td></td><td>【图图Seminar88】王逸之——CSIG文档微沙龙22-06期（总第10期）</td><td>报告题目：面向文字的图形图像生成方法及其应用报告人：北京大学，王逸之博士主持人：北京大学王选计算机研究所，连宙辉副教授</td></tr><tr class="odd"><td>89</td><td>BV1U24y1Z7Lh</td><td></td><td>【图图Seminar89】2022年江苏省视觉计算与可信人工智能暑期学校（7-10下午）</td><td>主讲人：何炳生 教授（南京大学）；魏秀参 教授（南京理工大学）</td></tr><tr class="even"><td>89</td><td>BV1Wg411U71F</td><td></td><td>【图图Seminar89】2022年江苏省视觉计算与可信人工智能暑期学校（7-10上午）</td><td>主讲人：杨健 教授（南京理工大学）；孙权森 教授（南京理工大学）</td></tr><tr class="odd"><td>90</td><td>BV1Ke411g7vt</td><td></td><td>【图图Seminar90】2022年江苏省视觉计算与可信人工智能暑期学校（7-11下午）</td><td>主讲人：肖亮 教授（南京理工大学）；何炳生 教授（南京大学）</td></tr><tr class="even"><td>90</td><td>BV1oV4y1T7ax</td><td></td><td>【图图Seminar90】2022年江苏省视觉计算与可信人工智能暑期学校（7-11上午）</td><td>主讲人：崔振 教授（南京理工大学）</td></tr><tr class="odd"><td>91</td><td>BV1cd4y167y6</td><td></td><td>【图图Seminar91】2022年江苏省视觉计算与可信人工智能暑期学校（7-12下午）</td><td>主讲人：谢国森 教授（南京理工大学）；何炳生 教授（南京大学）</td></tr><tr class="even"><td>91</td><td>BV1pG411377N</td><td></td><td>【图图Seminar91】2022年江苏省视觉计算与可信人工智能暑期学校（7-12上午）</td><td>主讲人：赵才荣 教授（同济大学）</td></tr><tr class="odd"><td>92</td><td>BV1ug411U7K5</td><td></td><td>【图图Seminar92】2022年江苏省视觉计算与可信人工智能暑期学校（7-13下午）</td><td>主讲人：李俊 教授（南京理工大学）；何炳生 教授（南京大学）</td></tr><tr class="even"><td>92</td><td>BV1wP4y1o74t</td><td></td><td>【图图Seminar92】2022年江苏省视觉计算与可信人工智能暑期学校（7-13上午）</td><td>主讲人：潘金山 教授（南京理工大学）</td></tr><tr class="odd"><td>93</td><td>BV1Cd4y1u7KL</td><td></td><td>【图图Seminar93】2022年江苏省视觉计算与可信人工智能暑期学校（7-14下午）</td><td>主讲人：陈强 教授（南京理工大学）；何炳生 教授（南京大学）</td></tr><tr class="even"><td>93</td><td>BV1ra411g73X</td><td></td><td>【图图Seminar93】2022年江苏省视觉计算与可信人工智能暑期学校（7-14上午）</td><td>主讲人：张拳石 副教授（上海交通大学）</td></tr><tr class="odd"><td>94</td><td>BV1Kt4y1j7p4</td><td></td><td>【图图Seminar94】2022年江苏省视觉计算与可信人工智能暑期学校（7-15下午）</td><td>主讲人：练春锋 特聘研究员（西安交通大学）；漆桂林教授（东南大学）；何炳生 教授（南京大学）</td></tr><tr class="even"><td>94</td><td>BV1pa411g7ej</td><td></td><td>【图图Seminar94】2022年江苏省视觉计算与可信人工智能暑期学校（7-15上午）</td><td>主讲人：周涛 教授（南京理工大学）；吴烨 教授（南京理工大学）</td></tr><tr class="odd"><td>95</td><td>BV1Ve4y1y7fr</td><td></td><td>【图图Seminar95】2022年江苏省视觉计算与可信人工智能暑期学校（7-16下午）</td><td>主讲人：肖亮 教授（南京理工大学）</td></tr><tr class="even"><td>95</td><td>BV1RP411n797</td><td></td><td>【图图Seminar95】2022年江苏省视觉计算与可信人工智能暑期学校（7-16上午）</td><td>主讲人：王力哲 教授（中国地质大学）；陆建峰教授（南京理工大学）</td></tr><tr class="odd"><td>96</td><td>BV1ZD4y1q7Ys</td><td></td><td>【图图Seminar96】2022年江苏省视觉计算与可信人工智能暑期学校（7-17上午）</td><td>主讲人：沈红斌 特聘教授（上海交通大学）</td></tr><tr class="even"><td>96</td><td>BV13e411g7jq</td><td></td><td>【图图Seminar96】2022年江苏省视觉计算与可信人工智能暑期学校（7-17下午）</td><td>主讲人：杨健 教授（清华大学）；沈肖波 教授（南京理工大学）</td></tr><tr class="odd"><td>97</td><td>BV1ZY4y1K7Qh</td><td></td><td>【图图Seminar97】2022年江苏省视觉计算与可信人工智能暑期学校（7-18上午）</td><td>主讲人：王昌栋 副教授（中山大学）；于静副研究员（中科院信工所）</td></tr><tr class="even"><td>100</td><td>BV1KP4y1Z7Y1</td><td></td><td>【图图Seminar100】汪嘉鹏——CSIG文档微沙龙22-07期（总第11期）</td><td>报告题目：面向多语言视觉富文档理解的探索与实践报告人：华南理工大学，汪嘉鹏博士 主持人：华中科技大学，刘禹良研究员</td></tr><tr class="odd"><td>102</td><td>BV1oe41157HY</td><td></td><td>【图图Seminar102】刘畅——CSIG文档微沙龙22-08期（总第12期）</td><td>报告题目：OpenCCD:基于上下文解耦的开集文字识别方法报告人：北京科技大学 刘畅博士 主持人：北京科技大学，杨春老师</td></tr><tr class="even"><td>103</td><td>BV1ce4y1m73D</td><td>医学图像</td><td>【图图Seminar103】第二届中国图象图形学报研究生论坛（医学图像处理论坛）</td><td>本场论坛共10位报告人 论坛致辞：沈定刚 教授（上海科技大学）论坛主席：施俊 教授（上海大学）</td></tr><tr class="odd"><td>104</td><td>BV1Ym4y1c7Gv</td><td>遥感图像</td><td>【图图Seminar104】第二届中国图象图形学报研究生论坛（遥感图像处理论坛）</td><td>本场论坛共12位报告人 论坛致辞：张良培 教授（武汉大学）论坛主席：孙显 研究员（中国科学院空天信息创新研究院）</td></tr><tr class="even"><td>105</td><td>BV1gK411S7ss</td><td>图像融合</td><td>【图图Seminar105】第二届中国图象图形学报研究生论坛（图像融合前沿论坛）</td><td>本场论坛共11位报告人 论坛致辞：李树涛 教授（湖南大学）论坛主席：马佳义 教授（武汉大学）</td></tr><tr class="odd"><td>106</td><td>BV1zP411w7Mq</td><td>文档图像处理</td><td>【图图Seminar106】第二届中国图象图形学报研究生论坛（文档图像处理论坛）</td><td>本场论坛共10位报告人 论坛致辞：白翔 教授（华中科技大学）论坛主席：连宙辉 副教授（北京大学王选计算研究所）</td></tr><tr class="even"><td>107</td><td>BV1eP411w7Vi</td><td>AR/VR</td><td>【图图Seminar107】第二届中国图象图形学报研究生论坛（AR/VR前沿论坛）</td><td>本场论坛共6位报告人 论坛致辞：王涌天 教授（北京理工大学）论坛主席：刘越 教授（北京理工大学）</td></tr><tr class="odd"><td>108</td><td>BV13e4y1m7G4</td><td>视觉与学习</td><td>【图图Seminar108】第二届中国图象图形学报研究生论坛（视觉与学习前沿论坛）</td><td>本场论坛共8位报告人 论坛致辞：高新波 教授（重庆邮电大学）论坛主席：严严 教授（厦门大学）</td></tr><tr class="even"><td>109</td><td>BV12G4y1x7U2</td><td></td><td>【图图Seminar109】张镇荣——CSIG文档微沙龙22-09期（总第13期）</td><td>报告题目：基于图注意力机制的多模态文档预训练报告人：中国科学技术大学 张镇荣博士 主持人：科大讯飞AI研究院张建树高级研究员</td></tr><tr class="odd"><td>111</td><td>BV1dW4y1K7iK</td><td></td><td>【图图Seminar111】张明亮——CSIG文档微沙龙22-10期（总第14期）</td><td>报告题目：文档图像中的几何图形识别与理解报告人：中国科学院自动化研究所 张明亮博士 主持人：中国科学院自动化研究所李晓辉助理研究员</td></tr><tr class="even"><td>112</td><td>BV1Q84y1x7Ds</td><td>视觉与智能图像处理</td><td>【图图Seminar112】第二届计算视觉与智能图像处理学术论坛</td><td>报告人：左旺孟 教授（哈尔滨工业大学）；李玺 教授（浙江大学）；孟德宇教授（西安交通大学）；周涛 教授（北方民族大学）；程国建教授（西安培华学院）；王美丽 教授（西北农林科技大学）；祝继华副教授（西安交通大学）；赵世杰 副研究员（西北工业大学）；石程副教授（西安理工大学）</td></tr><tr class="odd"><td>113</td><td>BV1Nv4y1i7R2</td><td>语义理</td><td>【图图Semina113】知识如何辅助语义理解?</td><td>主讲人：秦兵 教授（哈尔滨工业大学）、程明明 教授（南开大学）、魏坤副教授（西安电子科技大学） 主持人：毋立芳 教授（北京工业大学）、胡永利教授（北京工业大学）、简萌 副教授（北京工业大学）</td></tr><tr class="even"><td>114</td><td>BV1cM4y1D7Vg</td><td>ChatGPT</td><td>【图图Seminar114】ChatGPT，前世今生与路在何方</td><td>PanelDiscussion：严骏驰（上海交通大学）、张奇（复旦大学）、张伟（华东师范大学）、林洲汉（上海交通大学）、陈旭（中国人民大学）</td></tr><tr class="odd"><td>115</td><td>BV1MX4y1f7ju</td><td></td><td>【图图Seminar115】杨明锟——CSIG文档微沙龙23-1期（总第15期）</td><td>报告题目：读和写：基于区分式和生成式的文字识别自监督模型报告人：华中科技大学 杨明锟博士 主持人：华中科技大学 余文文博士</td></tr><tr class="even"><td>116</td><td>BV1T84y1M7r2</td><td></td><td>【图图Seminar116】罗楚威、龙如蛟——CSIG文档微沙龙23-2期（总第16期）</td><td>报告题目： 1. 用于视觉信息抽取的几何关系预训练模型 罗楚威阿里巴巴达摩院 2. 自然场景视觉信息抽取 龙如蛟 阿里巴巴达摩院</td></tr><tr class="odd"><td>117</td><td>BV1ho4y1V7Z9</td><td></td><td>【图图Seminar117】百度 李煜林——CSIG文档微沙龙23-3期（总第17期）</td><td>报告题目：StrucTexTv2：“化繁为简”的端到端文档图像理解预训练框架百度视觉技术部OCR团队 李煜林</td></tr><tr class="even"><td>118</td><td>BV1Ch4y1x75q</td><td></td><td>【图图Seminar118】2023CVPR上海论文分享学术报告会</td><td>30篇CVPR 2023录用论文成果报告</td></tr><tr class="odd"><td>119</td><td>BV1jW4y1Q7CV</td><td></td><td>【图图Seminar119】华南理工郑晓怡——CSIG文档微沙龙23-4期（总第18期）</td><td>报告题目：M6Doc：用于现代文档版面分析的大规模多格式多样式数据集华南理工大学研究生 郑晓怡</td></tr><tr class="even"><td>121</td><td>BV1JM4y1x7Ba</td><td>生成式表格结构识别</td><td>【图图Seminar121】华为IIRC黄永帅——基于“视觉-结构”对齐的生成式表格结构识别（CSIG文档微沙龙第19期）</td><td>报告题目：基于“视觉-结构”对齐的生成式表格结构识别华为IT创新研究中心(IIRC) 黄永帅</td></tr><tr class="odd"><td>122</td><td>BV1Sh4y1m7JX</td><td>图像融合</td><td>【图图Seminar122】图像融合前沿论坛——图图专刊优秀成果分享会</td><td>主持人：马佳义 教授（武汉大学） 报告人：唐霖峰博士（武汉大学）；刘磊 博士（安徽大学）；李坤算法研究员（商汤科技）；朱文瑜 算法工程师（本源量子计算科技）</td></tr><tr class="even"><td>123</td><td>BV1sw411i71n</td><td></td><td>【图图Seminar123】联想研究院周忈——大模型时代的文档分析处理技术（CSIG文档微沙龙第20期）</td><td>报告题目：大模型时代的文档分析处理技术 报告人：联想研究院 周忈</td></tr><tr class="odd"><td>124</td><td>BV1nh4y1Y75f</td><td></td><td>【图图Seminar124】图像数据受限前沿论坛——图图专刊优秀成果分享会</td><td>主持人：刘怡光 教授（四川大学） 报告人：傅可人副研究员（四川大学）；吴岸聪 副研究员（中山大学）；胡会扬博士研究生（中国科学院空天信息创新研究院）；高绍兵副研究员（四川大学）</td></tr><tr class="even"><td>125</td><td>BV1ry4y1F7vX</td><td>视频身份识别</td><td>【图图Seminar125】视频身份识别前沿论坛——图图专刊优秀成果分享会</td><td>主持人：于仕琪 副教授（南方科技大学） 报告人：王生进教授（清华大学）；马丙鹏 教授（中国科学院大学）；张权博士生（中山大学）；许文正 硕士生（山东大学）</td></tr><tr class="odd"><td>126</td><td>BV1Wp4y1P7XR</td><td></td><td>【图图Seminar126】多媒体智能前沿论坛——图图专刊优秀成果分享会</td><td>主持人：蒋树强 研究员（中国科学院计算技术研究所） 报告人：杨易教授（浙江大学）；刘偲 教授（北京航空航天大学）；王鑫助理研究员（清华大学）；聂婕 教授（中国海洋大学）</td></tr><tr class="even"><td>127</td><td>BV1kQ4y1W7a7</td><td>手写文字生成</td><td>【图图Seminar127】华南理工大学代港——基于书写者和字符风格解耦的手写文字生成（CSIG文档微沙龙第21期）</td><td>报告题目：基于书写者和字符风格解耦的手写文字生成报告人：华南理工大学 代港博士</td></tr><tr class="odd"><td>128</td><td>BV1wz4y1P7YJ</td><td>风格迁移</td><td>【图图Seminar128】武汉理工大学潘炜——结合全局与可学习部件风格迁移的字体生成（CSIG文档微沙龙第22期）</td><td>报告题目：结合全局与可学习部件风格迁移的字体生成报告人：武汉理工大学 潘炜硕士</td></tr><tr class="even"><td>129</td><td>BV1NN4y1a7Nx</td><td>手写公式识别</td><td>【图图Seminar129】科大讯飞研究院：面向教育场景的手写公式识别（CSIG文档微沙龙第23期）</td><td>报告人：科大讯飞研究院 吴浩 报告人：科大讯飞研究院 陈明军</td></tr><tr class="odd"><td>130</td><td>BV1734y1c7dn</td><td>精准诊断</td><td>【图图Seminar130】精准诊断论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共17位报告人，论坛主席：沈定刚 教授、周涛教授，评审专家：丁忠祥 教授、陈慧灵 教授、李晨 副教授。</td></tr><tr class="even"><td>131</td><td>BV14w41187rS</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar131】数字媒体深度伪造与对抗论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共11位报告人，论坛主席：卢伟 教授、钱振兴 教授，评审专家：李晓龙 教授、张卫明 教授、王员根 教授、秦川 教授。</td></tr><tr class="odd"><td>132</td><td>BV1qw411873s</td><td>图像/视频语义分割</td><td>【图图Seminar132】图像/视频语义分割论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共12位报告人，论坛主席：王井东 首席科学家、桑农教授，评审专家：李玺 教授、高常鑫 教授、余昌黔 算法专家。</td></tr><tr class="even"><td>133</td><td>BV14C4y1y78Y</td><td>智能驾驶</td><td>【图图Seminar133】智能驾驶论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共10位报告人，论坛主席： 鲍泓 教授、 陈振宇教授，评审专家：蔡彦 研究员、 马楠 教授、 任秉韬 讲师、 张煜群副研究员。</td></tr><tr class="odd"><td>134</td><td>BV1Hc411Q7uD</td><td>低质图像处理</td><td>【图图Seminar134】低质图像处理与语义理解论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共16位报告人，论坛主席： 胡清华 教授，评审专家：左旺孟教授、 张长青 副教授、 任冬伟 副教授。</td></tr><tr class="even"><td>135</td><td>BV1o64y177Ny</td><td>数字人</td><td>【图图Seminar135】数字人建模、生成与渲染技术论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共13位报告人，论坛主席：刘利刚 教授，评审专家： 刘烨斌教授、 高林 研究员、 刘利斌 助理教授、王贝贝 副教授。</td></tr><tr class="odd"><td>136</td><td>BV1yc411Q7Pb</td><td>遥感数据融合</td><td>【图图Seminar136】多源遥感数据融合与智能解译论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共19位报告人，论坛主席：肖亮 教授、史振威教授，评审专家：孙显 研究员、邹征夏 教授、付莹 教授。</td></tr><tr class="even"><td>137</td><td>BV1H64y1576Q</td><td></td><td>【图图Seminar137】北京图象图形学学会青年人才成长论坛（第三届中国图象图形学报研究生学术论坛）</td><td>报告人：张三义(中国科学院信息工程研究所)，李佳男(北京理工大学)，郭海云(中国科学院自动化研究所)，彭思达(浙江大学)，田雨(北京理工大学)</td></tr><tr class="odd"><td>138</td><td>BV1iQ4y1g7jF</td><td>多模态机器学习方法</td><td>【图图Seminar138】可信多模态机器学习方法、理论及应用（CSIG图像视频通信专委会青年学者沙龙第一期）</td><td>报告人：天津大学副教授 张长青 主持人：华中科技大学副研究员 周瑜</td></tr><tr class="even"><td>140</td><td>BV1P64y1T7C9</td><td>多媒体信息编码</td><td>【图图Seminar140】数据驱动的多媒体信息编码研究（CSIG图像视频通信专委会青年学者沙龙第二期）</td><td>报告人：福州大学教授 赵铁松 主持人：武汉科技大学副教授 向森</td></tr><tr class="odd"><td>141</td><td>BV1xK411v7ss</td><td>低质量文本图像识别</td><td>【图图Seminar141】低质量文本图像识别（CSIG文档微沙龙第24期）</td><td>报告人：复旦大学 赵珉怿博士 报告人：复旦大学 张路博士</td></tr><tr class="even"><td>142</td><td>BV1kB421z72H</td><td>多模态多任务通用视觉感知</td><td>【图图Seminar142】多模态多任务通用视觉感知（CSIG图像视频通信专委会青年学者沙龙第三期）</td><td>报告人：大连理工大学副教授 王立君 主持人：华中科技大学教授王兴刚</td></tr><tr class="odd"><td>143</td><td>BV13W421A7vp</td><td>文档信息抽取</td><td>【图图Seminar143】基于大模型的文档信息抽取技术（CSIG文档微沙龙第25期）</td><td>报告人：电子科技大学 何家邦 报告人：新加坡管理大学 王磊</td></tr><tr class="even"><td>145</td><td>BV1wx421m7r3</td><td>行人搜索</td><td>【图图Seminar145】视觉行人搜索领域研究进展（CSIG图像视频通信专委会青年学者沙龙第四期）</td><td>报告人：南京理工大学教授 张姗姗 主持人：天津大学研究员 郭晓杰</td></tr><tr class="odd"><td>146</td><td>BV1kA4m1w7Pn</td><td>多模态大模型</td><td>【图图Seminar146】Monkey:分辨率和详细标注对多模态大模型的意义（CSIG文档微沙龙第26期）</td><td>报告人：华中科技大学 杨彪（白翔教授团队） 报告人：华中科技大学谢旭东（白翔教授团队）</td></tr><tr class="even"><td>147</td><td>BV1XH4y1A73R</td><td>文字多模态大模型</td><td>【图图Seminar147】“文字多模态大模型”主编沙龙【2024图图名师讲堂】</td><td>报告一：多模态文档大模型mPLUG-DocOwl 报告人：徐海洋阿里通义实验室高级算法专家报告二：基于概念协同的视觉文本智能技术：思考与未来挑战 报告人：刘皓字节跳动火山引擎算法研究员 报告三：LayoutLLM:基于大语言模型版面指令微调的文档理解 报告人：姚聪阿里巴巴通义实验室自然语言实验室OCR团队负责人</td></tr><tr class="odd"><td>148</td><td>BV1Bt421A7gP</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar148】“数字媒体深度伪造与对抗”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：生成式人工智能与可证明安全隐写 报告人：陈可江中国科学技术大学特任副研究员报告二：联合多重对抗与通道注意力的高安全性图像隐写 报告人：马宾齐鲁工业大学教授 报告三：基于语义解耦的AI生成图像取证技术 报告人：丁峰南昌大学校聘教授</td></tr><tr class="even"><td>149</td><td>BV1Fp421D7CB</td><td>自动驾驶</td><td>【图图Seminar149】“自动驾驶的人工智能技术前沿”主编沙龙【2024图图名师讲堂】</td><td>报告一：开放场景无图自动驾驶 报告人：丁文超 复旦大学 青年研究员报告二：自动驾驶中的点云数据与应用 报告人：浦剑 复旦大学 青年研究员</td></tr><tr class="odd"><td>150</td><td>BV1am421s7fD</td><td>低质图像处理与语义理解</td><td>【图图Seminar150】“低质图像处理与语义理解”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：基于深度学习的低照度图像处理 报告人：石争浩 西安理工大学教授报告二：轻量化 VS 高效性：实时图像语义理解 报告人：周全 南京邮电大学教授报告三：图像无损和近无损压缩方法研究 报告人：柏园超哈尔滨工业大学助理教授</td></tr><tr class="even"><td>151</td><td>BV1k142197or</td><td>复杂场景图像目标智能检测</td><td>【图图Seminar151】“复杂场景图像目标智能检测”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：弱监督学习方法及在目标检测中的应用 报告人：宫辰南京理工大学教授 报告二：智破迷雾：低质量与对抗攻击下的目标检测报告人：任文琦 中山大学教授</td></tr><tr class="odd"><td>152</td><td>BV1Hx4y167T9</td><td>精准诊断</td><td>【图图Seminar152】“精准诊断”专刊优秀成果分享会【2024图图名师讲堂】</td><td>报告一：乳腺癌病理图像数据集制作及有丝分裂细胞核识别方法研究报告人：汪华登 桂林电子科技大学 报告二：多尺度特征融合的肝肿瘤检测报告人：马金林 北方民族大学 报告三：胎儿脑磁共振图像分割研究进展报告人：陈健 福建理工大学报告四：精准牙科影像分析：深度学习在三维牙齿分割中的应用与挑战报告人：李俊诚 上海大学</td></tr><tr class="even"><td>153</td><td>BV18C411H7UL</td><td>文档图像智能处理与识别</td><td>【图图Seminar153】文档图像智能处理与识别”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：文档识别研究前沿之“变”与“不变” 报告人：刘成林中国科学院自动化研究所 研究员报告二：半自回归式大型表格结构识别与填空式提示场景文本问答 报告人：周宇中国科学院信息工程研究所 研究员报告三：数据视角下的文档理解技术的演进与思考 报告人：乔梁 海康威视研究院算法专家</td></tr><tr class="odd"><td>154</td><td>BV18H4y1A757</td><td></td><td>【图图Seminar154】多模态目标检索与语义理解（CSIG图像视频通信专委会青年学者沙龙第五期）</td><td>报告人：叶茫 武汉大学计算机学院 教授 主持人：卢涛 武汉工程大学教授</td></tr><tr class="even"><td>155</td><td>BV1o142167cW</td><td>不确定性分析</td><td>【图图Seminar155】第三届不确定性分析与知识发现学术研讨会（下午）</td><td>报告五：关于教育科学研究底层逻辑和顶层设计的思考 报告人：周傲英华东师范大学 教授 报告六：李群机器学习理论及产业化应用示范报告人：李凡长 苏州大学 教授 报告七：智能化科学设施的思考与实践报告人：杨小康 上海交通大学 教授 报告八：大模型的知识边界探测与知识迁移报告人：黄萱菁 复旦大学 教授 报告九：Shallow-to-Deep Non-IID learning:Beyond Statistical Non-IID Thinking 报告人：Longbing Cao澳大利亚麦考瑞大学 教</td></tr><tr class="odd"><td>155</td><td>BV1yE42157Gv</td><td>不确定性分析</td><td>【图图Seminar155】第三届不确定性分析与知识发现学术研讨会（上午）</td><td>报告一：多粒度认知计算 报告人：王国胤 重庆邮电大学 副校长、教授报告二：低质多模态数据动态可信融合 报告人：胡清华 天津大学 教授报告三：多模态视觉结构学习 报告人：李玺 浙江大学 教授报告四：知识数据融合的脑机接口精准解码 报告人：伍冬睿 华中科技大学教授</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位视频合集</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D%E8%A7%86%E9%A2%91%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 2%" /><col style="width: 7%" /><col style="width: 13%" /><col style="width: 37%" /><col style="width: 37%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>题目</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>30</td><td>BV1xN411Z7Tt</td><td>智能信息伪装</td><td>【图图Seminar30】张卫明：智能信息伪装</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”前沿论坛报告人：中国科学技术大学 张卫明</td></tr><tr class="even"><td>30</td><td>BV1j64y1y7XS</td><td>神经网络水印</td><td>【图图Seminar30】张新鹏：神经网络水印</td><td>《中国图象图形学报》图图Seminar 30期——“图像/视频内容安全”论坛报告人：复旦大学 张新鹏</td></tr><tr class="odd"><td>76</td><td>BV18Y4y1r7tb</td><td>视频身份识别技术</td><td>【图图Seminar76】“视频身份识别技术”主编论坛——图图名师讲堂</td><td>论坛致辞：中山大学 赖剑煌 报告专家：厦门大学 纪荣嵘 清华大学 王生进南方科技大学 于仕琪 国防科技大学 蓝龙 武汉科技大学 王晓论坛主持：山东科技大学 张鹏</td></tr><tr class="even"><td>77</td><td>BV1Kr4y147rU</td><td>智能图像安全</td><td>【图图Seminar77】“智能图像安全”主编论坛——图图名师讲堂</td><td>论坛致辞：中国科学技术大学 张卫明 报告专家：华南理工大学 胡永健上海理工大学 秦川 中国科学技术大学 储琪 论坛主持：复旦大学 钱振兴</td></tr><tr class="odd"><td>125</td><td>BV1ry4y1F7vX</td><td>视频身份识别</td><td>【图图Seminar125】视频身份识别前沿论坛——图图专刊优秀成果分享会</td><td>主持人：于仕琪 副教授（南方科技大学） 报告人：王生进教授（清华大学）；马丙鹏 教授（中国科学院大学）；张权博士生（中山大学）；许文正 硕士生（山东大学）</td></tr><tr class="even"><td>131</td><td>BV14w41187rS</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar131】数字媒体深度伪造与对抗论坛（第三届中国图象图形学报研究生学术论坛）</td><td>本场论坛共11位报告人，论坛主席：卢伟 教授、钱振兴 教授，评审专家：李晓龙 教授、张卫明 教授、王员根 教授、秦川 教授。</td></tr><tr class="odd"><td>148</td><td>BV1Bt421A7gP</td><td>数字媒体深度伪造与对抗</td><td>【图图Seminar148】“数字媒体深度伪造与对抗”专题优秀成果分享会【2024图图名师讲堂】</td><td>报告一：生成式人工智能与可证明安全隐写 报告人：陈可江中国科学技术大学特任副研究员报告二：联合多重对抗与通道注意力的高安全性图像隐写 报告人：马宾齐鲁工业大学教授 报告三：基于语义解耦的AI生成图像取证技术 报告人：丁峰南昌大学校聘教授</td></tr></tbody></table><table><colgroup><col style="width: 4%" /><col style="width: 13%" /><col style="width: 17%" /><col style="width: 65%" /></colgroup><thead><tr class="header"><th>id</th><th>bvid</th><th>关键词</th><th>title</th></tr></thead><tbody><tr class="odd"><td>22</td><td>BV1M94y1s76K</td><td>深度学习对抗攻防</td><td>《追AI的人》第12期直播回放清华大学计算机系副研究员、国家“万人计划”青年拔尖人才苏航老师分享《深度学习对抗攻防:人与算法的无间道》</td></tr><tr class="even"><td>26</td><td>BV1dh4y1U7dr</td><td>图像取证</td><td>《追AI的人》第28期直播回放阿里巴巴媒体安全技术研究团队分享《图像取证探秘：P图假证无处遁形，揭开“美女荷官”骗局》</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards_Effective_Image_Manipulation_Detection_with_Proposal_Contrastive_Learning</title>
      <link href="/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/"/>
      <url>/Towards-Effective-Image-Manipulation-Detection-with-Proposal-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>Towards Effective Image Manipulation Detection with ProposalContrastive Learning</p><h1 id="摘要">摘要：</h1><p>​深度模型在图像操作检测中得到了广泛而成功的应用，旨在对篡改图像进行分类和定位篡改区域。现有的方法大多集中于从被篡改图像中提取全局特征，而忽略了单个被篡改图像中被篡改区域与真实区域之间的局部特征的关系。为了利用这种空间关系，我们提出了建议对比学习（PCL）来进行有效的图像操作检测。我们的PCL由一个双流架构组成，通过分别从RGB和噪声视图中提取两种类型的全局特征。为了进一步提高鉴别能力，我们通过吸引/排斥基于命题的代理命题对比学习任务来利用局部特征的关系。此外，我们还证明了我们的PCL在实践中可以很容易地适应未标记的数据，这可以降低人工标记的成本，并促进更一般化的特性。在几个标准数据集中进行的大量实验表明，我们的PCL可以作为一个通用的模块来获得一致的改进。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CatmullRom Splines-Based Regression for Image Forgery Localization</title>
      <link href="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/"/>
      <url>/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/</url>
      
        <content type="html"><![CDATA[<center>CatmullRom Splines-Based Regression for Image Forgery Localization<ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\textbf{Li Zhang}^{1,2},\textbf{MingliangXu}^{2},\textbf{Dong Li}^{2},\textbf{JianmingDu}^{1,\dagger},\textbf{Rujing Wang}^{1,2\dagger}\)</span></center><center>中国科学院合肥物理科学研究所、中国科技大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/28548.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>  IFL（Image ForgeryLocation）有助于确保数字媒体取证的安全。然而，许多方法存在错误的检测（即FPs）和不准确的边界。在本文中，我们提出了基于CatmullRom样条的回归网络（CSR-Net,CatmullRom Splines-based RegressionNetwork），它从回归的角度重新考虑IFL任务来处理这个问题。</p><p>  具体来说，我们提出了一种自适应的CutmullRom样条变换方案，用于被篡改区域的粗定位。然后，对于假阳性例子，我们开发了一种新的重新评分机制，旨在筛选出不能在分类分支和实例分支上都有响应的样本。随后，为了进一步限制边界，我们设计了一个可学习的纹理提取模块，该模块通过解耦水平和垂直伪造特征来参考和增强轮廓表示，从而提取出更鲁棒的轮廓表示，从而抑制FPs。与基于分割的方法相比，由于不需要后处理，我们的方法简单而有效。大量的实验表明，CSR-Net优于现有的先进方法，不仅在标准的自然图像数据集上，而且在社交媒体数据集上。</p><h1 id="引言">引言</h1><p>  第一个问题是<strong>假阳性（FPs）</strong>。假阳性是指测试结果表明存在一个令人满意的目标区域，而实际上它并不令人信服。传统的基于分割的方法往往会出现这种情况（如图2所示）。</p><p><img src=" ../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503164842397.png" alt="image-20240503164842397 " style="zoom:40%;" /></p><p>  二值化是这些方法中不可或缺的决定性策略，它是一种直接决定前景区域块数量的阈值敏感任务。在传统的分割方法中，一个不合理的阈值往往会导致出现意外的区域（即假阳性的情况）。然而，许多方法在关注潜在的被篡改区域时，通常会忽略误报率。这对数字内容的传播有负面影响，影响了相关新闻来源的可获得性，这限制了分析结果向更有令人信服的方向发展。</p><p>  第二个问题是<strong>不准确的边界</strong>。传统的基于分割的方法存在连续解码器层之间的掩模预测不一致，导致优化目标不一致和特征空间的弱耦合（如图1a所示）。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503165143372.png" alt="image-20240503165143372" style="zoom:67%;" /></p><p>  另一方面，当直接引入一般回归方法来处理任务时，定位效果也不令人满意，因为所使用的边界盒只能以四边形的方式定位目标区域，而且通常目标区域大多出现在不规则曲线中（图1(b)显示了旋转检测的定位效果（Li et al.2022），其中我们使用掩蔽区域的最小边界四边形作为GroundTruth）。越来越复杂的被篡改图像提出了更大的挑战，因为大多数方法不能很好地约束或明确地建模伪造的区域边界，这很容易导致在检测结果中混合其他目标或不兼容的背景。</p><p>  最近，一些基于回归的策略在目标检测领域的假阳性判定方面取得了显著进展(You et al. 2022; Li and Kosecka 2022; Chen et al.2023)。与目标检测任务不同，IFL是一个像素级的任务，这意味着方法迁移的直接性将带来性能下降。为此，需要引入一些定制的方法和处理方法，可以有效地连接这两个任务。</p><p>  首先，对于标记为GT的掩模，我们引入了CatmullRom样条曲线来将其转换为多边形帧，从而使回归策略能够应用于像素级的任务，如IFL。同时，在训练和推理过程中，为了使多边形标记更接近真实标记，提出了自适应参数CatmullRom样条方法，该方法可以最小化预测区域与地面真实值之间的相似性差距和目标区域的曲率。其次，为了进一步、明确地抑制定位结果中的假阳性，我们提出了一种有效的重评分机制：我们通过两个独立的预测分支，直接拒绝在两个分支中都没有接收到响应的假阳性，每个预测分支都有一个区域分类得分和一个实例得分。此外，为了得到更准确的边界，我们通过解耦水平纹理特征和垂直纹理特征，进一步细化预测区域的轮廓，以建模锻造区域边界，减少它们与其他掩模之间的重叠。</p><p>  我们的贡献可以概括为三类：</p><p>  1)我们制作了一个基于CatmullRom样条的回归网络（CSR-Net,<strong>C</strong>atmullRom <strong>S</strong>plines-based<strong>R</strong>egressionNetwork），首次尝试将回归方法引入像素级任务。</p><p>  2)为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><p>  3)在多个公共数据集（包括自然图像数据集和社交媒体数据集）上进行的大量实验表明，我们的方法与IFL中最先进的方法相比具有优越性。</p><h1 id="方法">方法</h1><h2 id="概括">概括</h2><p>  图3是对我们的框架的概述。输入图像表示为 $ X R^{H×W×3} $。首先，我们使用嵌入ResNet-50的FPN作为骨干网络进行Catmull样条检测。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>  更具体的是，我们的方法利用一种基于CatmullRom样条的参数化方法来自适应地识别目标分割区域（橙色部分）。跟随（Chenet al. 2017），我们采用空间空间金字塔池（ASPP, atrous spatial pyramidpooling）和ResNet-50来捕获长期上下文信息和多尺度特征。<br/><br/>  这种无锚定的卷积神经网络大大简化了我们的任务的检测，也允许我们获得一个粗糙的特征图。稍后，我们使用一个重新评分机制（CRA,re-scoringmechanism）来筛选出在粗特征图（蓝色部分）上突出显示的可疑区域的假阳性样本。最后，我们同时从水平和垂直方向的区域进行纹理提取（byVTP），以预期获得更准确的边界（绿色部分）。请注意，保留的每个篡改区域将由VTP独立处理。</p><h2 id="catmullrom样条检测">CatmullRom样条检测</h2><p>  大多数传统方法在IFL中使用基于分割的思想。<br/><br/>  然而，在处理此类面具或基于多边形的数据集时，基于回归的方法往往是一种更有效的方法，例如（Liu等2019；Pranav，正刚等2020；Zhang等2022）。<br/><br/>  一般来说，主流的基于回归的方法需要复杂的处理来适应实例的边界，这导致了实践中的不可靠和不稳定性。近年来，样条曲线被用于计算机图形应用中生成各种形状的曲线。例如，自动驾驶车道线(Ma等2019年；余和陈2017年）、文本检测（刘等2020年；唐等2022年；阮等2021年）、故障检测(Park等2011年；郭和王2005年）等。其中，CatmullRom样条函数是一种经典的插值样条，由于其变换效应和推理代价，适用于被篡改区域的参数化（钱德拉2020；Li2022）。<br/><br/>  具体地说，CatmullRom样条是一组三次插值样条，因此每个点上的切线使用样条上的上一个点和下一个点计算。在给定的控制点下，卡特穆尔罗姆样条函数可以适应任何形状（李和陈2016；李，刘，刘2022）。此外，构造三次小矩阵样条函数只涉及整数系数，与其他样条函数相比，它降低了实现成本。上面提到的所有这些属性都有助于提高更快的推理速度和更低的计算消耗（Flops）。<br/><br/>  数学上，数学样条被定义为等式1: <spanclass="math display">\[c_i(t)=\sum_{j=0}^3b_j(t)\boldsymbol{p}_{i+j},\quadi=0,1,\ldots,n-3\]</span></p><p>  其中，0≤t≤1，<spanclass="math inline">\(p_{i}(i=0,1,\ldots,n-3;n\geq3)\)</span>为控制点，<spanclass="math inline">\(b_{j}(t)\)</span>为基。例如，它可以用等式2来表示当函数<spanclass="math inline">\(b_{j}(t)\)</span>中t的最大幂为3时：</p><p><span class="math display">\[c_i(t)=\frac{1}{2}\cdot[1\quad t\quadt^2\quadt^3]\cdot\begin{bmatrix}0&amp;2&amp;0&amp;0\\-\tau&amp;0&amp;\tau&amp;0\\2\tau&amp;\tau-6&amp;-2(\tau-3)&amp;-\tau\\-\tau&amp;4-\tau&amp;\tau-4&amp;\tau\end{bmatrix}\cdot\begin{bmatrix}p_i\\p_{i+1}\\p_{i+2}\\p_{i+3}\end{bmatrix}\]</span></p><p>  为了协调被篡改区域的任意形状与CatmullRom样条曲线，我们从现有的数据集和真实的图像中深入研究了定向或弯曲的被篡改区域。在卡氏样条曲线中，<spanclass="math inline">\(\tau\)</span>（张力因子）是控制样条线紧性的一个重要参数。张力因子的值越高，曲线在控制点之间弯曲越紧密，从而在移动过程中更接近给定的数据点。相反，较低的张力系数值会使控制点之间的曲线更平滑。直观地看，传统的卡莫样条（参数<spanclass="math inline">\(\tau=1\)</span>）对IFL任务很差，因此我们试图通过调整τ在匹配精度和曲线平滑度之间找到正确的平衡。消融实验（在消融分析部分）表明，当<spanclass="math inline">\(\tau\)</span>设置为16时，CatmullRom样条曲线对该任务是可靠的。它还允许学习到的控制点更接近前景（篡改）区域。</p><h2 id="catmullrom-ground-truth-生成"><strong>CatmullRom Ground Truth生成</strong></h2><p>  在IFL中，许多基准测试使用基于掩码或多边形的数据集作为公共数据集(Dong, Wang, and Tan 2013; Hsu and Chang 2006; Alibaba2021/2022)。给定曲线边界的注释点 $ { p_i } _{i=1} ^n $ ，其中 $ p_i $表示第 $ i $ 个注释点，主要目标是根据等式一获得CatmullRom样条 $ c(t) $的最优参数。为了实现这一点，我们可以简单地应用标准最小二乘法，如等式3所示 :</p><p><spanclass="math display">\[\begin{bmatrix}p_{03t_0}&amp;\cdots&amp;p_{33t_0}\\p_{03t_1}&amp;\cdots&amp;p_{33t_1}\\\vdots&amp;\ddots&amp;\vdots\\p_{03t_m}&amp;\cdots&amp;p_{33t_m}\end{bmatrix}\begin{bmatrix}c_{x_0}&amp;c_{y_0}\\c_{x_1}&amp;c_{y_1}\\c_{x_2}&amp;c_{y_2}\\c_{x_3}&amp;c_{y_3}\end{bmatrix}=\begin{bmatrix}\mathscr{P_{x_0}}&amp;\mathscr{P_{y_0}}\\\mathscr{P_{x_1}}&amp;\mathscr{P_{y_1}}\\\mathscr{P_{x_2}}&amp;\mathscr{P_{y_2}}\\\mathscr{P_{x_3}}&amp;\mathscr{P_{y_3}}\end{bmatrix}\]</span></p><p>  其中，m表示一个曲线边界的标注点的数量。而t是通过使用累积长度与多段线的周长的比值来计算的。$ p_{ij} $ 可以从等式中引用1，我们使用 $ _{i} $表示变换后的新坐标点。根据等式1和等式3，我们将原始的掩码注释转换为一个参数化的CatmullRom样条曲线。</p><p><img src="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240505171016728.png" alt="image-20240505171016728" style="zoom:50%;" /></p><p>图4：立方CatmullRom样条曲线的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。</p><h2 id="综合再评分算法cra">综合再评分算法CRA</h2><p>  MaskR-CNN背后的基本机制是将所得到的边界框的分类一致性视为分数，然后使用一个预先确定的阈值来筛选出背景框。然而，尽管有了一些进展，当边界框包含一个明显不兼容的区域实例时，它伴随着大量的背景信息，并且MaskRCNN经常显示出如此低分数的真阳性，而它保留了一些相对较高可信度的FPs。因此，我们为每个区域实例重新分配分数。具体来说，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。数学上，给定预测的n类分数$~ \mathrm{CLS}=\{s_{ij}^{cls}\mid j\in[0,\cdots,n-1]\} $ 和 $~\mathrm{INS}=\{s_{ij}^{ins}\mid j\in[0,\cdots,n-1]\} $通过等式4计算第i个提出建议的综合得分： <spanclass="math display">\[s_{ij}=\frac{e^{s_{ij}^{cls}+s_{ij}^{ins}}}{\sum_{l=0}^{n-1}e^{s_{il}^{cls}+s_{il}^{ins}}}\]</span>  在我们的工作中，我们采用了n =2，其中这两个类再现了图4：三次组合样条的一个例子。请注意，由于只有两个端点c1和c5，CatmullRom样条线就会退化为一条直线。发送篡改（前景）和真实（背景）区域。因此，我们只需要计算前景类的分数。<br/><br/>  CLS由一个类似于MaskR-CNN的分类分支直接获得，INS是区域实例在全局区域分割图上的激活值。具体来说，它被投影到每个区域实例的篡改区域分割地图上，包含<spanclass="math inline">\(P_{i} = \{p_{i}^{1},p_{i}^{2}\ldots p_{i}^{n}\}\)</span>，并且区域实例区域中Pi的平均值可以表示为： <spanclass="math display">\[s_{i1}^{ins}=\frac{\sum_jp_i^j}N\]</span>  其中，Pi是区域分割地图上第i个区域实例的像素值的集合。将分类得分与实例得分有机结合，得到综合得分，在实践中可以降低FP的可信度。这是因为FPs往往比分割图上的区域有更弱的响应。<br/><br/>  下面的实验结果表明，我们的设计对图片剪切情况更友好，因为剪接情况通常在分割图上享有更强的响应，较高的实例分数将弥补较低的分类分数。</p><h2 id="垂直纹理-交互式感知vtp">垂直纹理-交互式感知VTP</h2><p>  传统的边缘检测操作符（例如，索贝尔、罗伯茨、普雷威特等）有助于提取自然图像处理任务中手工制作的特征，而最大的缺点是它们不能根据任务的特殊性进行动态学习。受（Holla和Lee2022）的启发，我们在一个被称为Sobel层的可学习模块中采用了一个边缘检测算子，见图5。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>图5：Sobel层图，用于VTP，用于增强边缘相关模式和操作边缘检测。来自第i个块首先通过Sobel单元（SU），然后通过一个边缘剩余单元（ERU）。为了进行训练和优化的原因，引入了一种残差学习策略。</p><p>  此外，为了更好地建模被篡改的区域边界，我们在我们的网络中引入了垂直纹理交互式感知（VTP,Vertical Texture interactivePerception）。在VTP中，被篡改的区域用一组等高线点表示，这些包含强纹理特征的点可以精确地定位具有任意形状的被篡改区域。<br/><br/>  看到它们：有两个核心并行分支在VTP分支，在顶部，我们引入一个卷积内核大小1×k滑动功能地图模型局部纹理信息在水平方向，只关注k-range地区的纹理特征。这个巧妙的技巧通过我们的预实验证明是简单且有效的。此外，它几乎是免资源开销的同时保持竞争效率。通过类似的范例，将底部分支通过大小为k×1的卷积核在垂直方向上对纹理特征进行建模。k是控制纹理特征接受场大小的超参数。在实际实验中，我们取k=3。最后，涉及两个独立的s型层，将两个方向上的热图归一化为[0,1]。这样，就可以在两个正交方向上检测到篡改区域，并在两个不同的热图中用等高线点表示，其中任何一个热图都只响应特定方向上的纹理特征。<br/><br/>  由于在两个正交方向上考虑响应值可以有效地抑制假阳性预测，因此通过点重评分算法进一步处理来自VTP的两个热图。具体地说，通过NMS对不同热图中的点进行直接处理，以实现紧密的表示。然后，为了抑制具有强单向或弱正交响应的预测，我们只选择在两个热图中具有不同响应的点作为候选点。最后，篡改区域可以用由这些高质量轮廓点组成的多边形表示。</p><h2 id="最优化">最优化</h2><p>  如上所述，我们的网络包括多任务任务。因此，我们计算了以下组件的损失函数：<span class="math display">\[L=L_{rpn}+\lambda_{1}\cdotL_{cls}+\lambda_{2}\cdot L_{mask}+\lambda_{3}\cdotL_{qts}+\lambda_{4}\cdot L_{CR}\]</span>  其中，Lrpn、Lcls和Lmask是来自MaskR-CNN的标准损失。Lgts用于优化篡改区域检测，定义为： <spanclass="math display">\[L_{gts}=\frac{1}{N}\sum_i-\log\left(\frac{e^{p_i}}{\sum_je^{p_j}}\right)\]</span>  Lgts是Softmax损失，其中p是网络的输出预测。<br/><br/>  LCR用于优化CatmullRom样条检测的ft，定义为：<span class="math display">\[L_{CR}=L_{ctr}+L_{bias}\]</span>  Lctr和Lbias均为FCOS损失（Tian等人，2019年）。前者用于优化从CatmullRom控制点中心的距离损失，而这些控制点到中心的偏移距离受后者的限制。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><h3 id="预训练数据"><strong>预训练数据</strong></h3><p>我们创建了一个规模可观的图像篡改数据集，并使用它来预训练我们的模型。<br/>该数据集包括三类：1）拼接，2）复制移动，和3）删除。</p><h3 id="测试数据集">测试数据集</h3><p>如下（Wang等2022；胡等2020），我们在 CASIA (Dong, Wang, and Tan2013), Columbia (Hsu and Chang 2006), NIST16(Guan et al. 2019), COVER(Wen et al. 2016)上评估我们的模型。</p><h3 id="评价指标">评价指标</h3><p>为了量化定位性能，根据之前的工作（Hu et al.2020），我们在操作掩模上使用像素级的曲线下面积（AUC）和F1分数。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h3 id="实施细节">实施细节</h3><p>输入图像的大小被调整为512×512。在这项工作中，骨干网络是ResNet-50，在ImageNet上进行了预训练。由PyTorch实现，我们的模型使用GeForce GTX3090进行训练，使用Adam作为优化器。</p><h2 id="与sota方法的比较">与SOTA方法的比较</h2><p>遵循经典方法（Hu et al. 2020；Wang et al.2022），我们的模型与其他最先进的篡改定位方法在两种设置下进行了比较：1)训练合成数据集和评估完整的测试数据集，2)调整预训练模型对测试数据集的训练分割和评估它们的测试分割。预先训练的模型将演示每种方法的通用性，fne调整模型将演示一旦域差异显著减少，每种方法在局部的表现如何。</p><h3 id="预训练模型">预训练模型</h3><p>表1显示了不同SOTA方法的预训练模型在像素级AUC下的fve数据集上的定位性能。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823163003537.png"alt="image-20240823163003537" /><figcaption aria-hidden="true">image-20240823163003537</figcaption></figure><p>我们的CSR-Net在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia排名第二。特别是在复制-移动数据集(COVER)上达到了94.4%，其图像伪造区域与背景难以区分。这验证了我们的模型具有抑制FPs和生成更准确的边缘的优越能力。然而，我们未能在Columbia上取得最好的表现，AUC得分比PSCCNet低1.4%。我们推测，原因可能是他们（PSCCNet）合成的训练数据的分布与Columbia数据集非常相似。表2中的结果进一步支持了这一点，这说明CSR-Net在AUC和F1得分上都优于PSCCNet。此外，值得指出的是，我们在较少的预训练数据下取得了不错的结果。</p><h3 id="微调模型">微调模型</h3><p>预训练模型的网络权值用于启动调优模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练部分上进行训练。我们在表2中评估了不同方法的微调模型。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210155557.png"alt="image-20240823210155557" /><figcaption aria-hidden="true">image-20240823210155557</figcaption></figure><p>对于AUC和F1，我们的模型取得了显著的性能提高。这表明CRA模块有效地抑制了假阳性错误实例，提高了VTP预测区域位置和边界的准确性。</p><p>在综合表1和表2中的数据后，我们的方法证明了为像素级任务引入回归方法是有效的，这是在引言中提到的。</p><h3 id="消融分析">消融分析</h3><p>  在本节中，我们将进行实验来证明我们所提出的CSR-Net方法的有效性。与传统的回归方法相比，正式引入了基于CatmullRom样条的回归（CSR）来更好地描述被篡改的区域。综合评分算法（CRA）的目标是选择分类得分高、实例得分高的期望区域，而垂直纹理交互感知（VTP）则采用水平和垂直两种方式对纹理特征进行建模，以参考目标区域。为了进一步评估CSR、CRA和VTP的有效性，我们分别删除它们，并验证它们在CASIA和NIST16数据集上的伪造定位性能。表3显示定量结果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823210814928.png"alt="image-20240823210814928" /><figcaption aria-hidden="true">image-20240823210814928</figcaption></figure><p>  基线(I)表示我们只使用传统的回归方法（Li et al.2022）。在接下来的消融实验中，我们可以推断，当不涉及VTP时，CASIA的F1评分下降了1.9%，NIST16的评分下降了1.7%。而在没有CRA时，AUC评分比（IV）下降更多。然而，当CRA不可用时，在（II）中可以观察到明显的性能下降，即AUC为12.3%，F1为11.2%。<br/><br/>  在图7中，我们展示了CatmullRomGround Truth生成中参数<spanclass="math inline">\(\tau\)</span>的不同值，以验证对自然图像数据集（即CASIA）和社交媒体数据集（即RIFL21）的各自预测效果。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211034561.png"alt="image-20240823211034561" /><figcaption aria-hidden="true">image-20240823211034561</figcaption></figure><p>  直观地看，随着<spanclass="math inline">\(\tau\)</span>的逐渐增加，不在不同数据集中，拟合的CatmullRom控制点与具有掩模水平的地面真值之间的欧几里德距离逐渐减小，显示出更好的拟合。然而，当<spanclass="math inline">\(\tau\)</span>超过16时，欧氏距离反而呈现出扩张的趋势，这意味着拟合效果可能会减少。显然，<spanclass="math inline">\(\tau =16\)</span>是生成基于CatmullRom的最佳GroundTruth的最好选择。</p><h2 id="可视化结果">可视化结果</h2><h3 id="定性结果">定性结果</h3><p>  我们在图6中提供了不同方法的预测伪造掩模。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211647860.png"alt="image-20240823211647860" /><figcaption aria-hidden="true">image-20240823211647860</figcaption></figure><p>图6：通过不同的方法可视化预测的操作掩模。从左到右，我们展示了伪造的图像，对ManTra-Net、SPAN、PSCCNet、TruFor、我们的预测和GT掩模。</p><p>  由于 ObjectFormer（Wang et al.2022）的源代码不可用，因此他们的预测不可用。与最先进的方法相比，我们的CSR-Net在抑制假阳性方面取得了更好的性能，无论是在抑制还是在更准确的篡改区域边界方面。我们有理由相信，改善受益于CRA和VTP。CRA能够更全面地考虑每个可能的区域，确定被篡改区域和真实区域之间的细微差别，而VTP同时通过两种正交方法建模纹理边界，以准确描述目标区域。</p><h3 id="不同的基于样条曲线的回归">不同的基于样条曲线的回归</h3><p>  有许多类型的插值函数，经典的CatmullRom样条和贝塞尔曲线，前者是一个插值样条函数，精确插值一组已知数据点通过使用一系列的节点，而后者是一个近似样条函数，近似一组数据点使用节点。IFL的数据集来自自然图像和社交媒体，被篡改的区域共享不同的形状。通过比较实验，我们发现CutmullRom样条更适合于具有不同曲率的数据集（如IFL），而基于贝塞尔曲线的方法有时容易受到其他目标的干扰。详情请参见图8。</p><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240823211913581.png"alt="image-20240823211913581" /><figcaption aria-hidden="true">image-20240823211913581</figcaption></figure><p>图8：通过不同的回归可视化结果。从左到右，我们展示了伪造的图像，不同的基于样条曲线的回归结果（左边显示了信任分数，而右边是预测的操作掩模），GT掩模。由于空间限制，请放大以更好地可视化。</p><h1 id="结论">结论</h1><p>  在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），以分类评分和实例评分来区分精确的篡改区域。此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（优秀代码鉴赏）(施工中)</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BC%98%E7%A7%80%E4%BB%A3%E7%A0%81%E9%89%B4%E8%B5%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>2024华为软件精英挑战赛决赛代码（初赛粤港澳赛区冠军，复赛粤港澳赛区冠军，全球总决赛第4 名）)<ahref="https://github.com/OrangeQi-CQ/HuaweiCodeCraft2024-Final"><imgsrc="https://img.shields.io/github/stars/OrangeQi-CQ/HuaweiCodeCraft2024-Final?style=flat"alt="GitHub" /></a></p><p>这是 2024 华为软件精英挑战赛<strong>“适可而止矣，涓埃之事，亦央原神”</strong> 队的决赛代码。</p><p>我们属于<strong>粤港澳赛区</strong>，三名队员（<ahref="https://github.com/OrangeQi-CQ">cq</a>、<ahref="https://github.com/yhf4aspe">yhf</a>、<ahref="https://github.com/zzwtx">xsf</a>）都是来自<strong>华南理工大学</strong>的本科生。在2024华为软件精英挑战赛中成绩如下：</p><ul><li><p>初赛：<strong>粤港澳赛区冠军</strong></p></li><li><p>复赛：<strong>粤港澳赛区冠军、全国第 2 名</strong></p></li><li><p>决赛：<strong>全球总决赛第 4 名（季军/三等奖）</strong></p></li></ul><hr /><p>成功之处：</p><ul><li>我们的代码实现能力比较强，能够高效准确地将想法落地并测试效果。有很多想法预期效果很好但实际徒劳无功甚至负作用，而个别想法看似普通却会有很惊喜的效果。我认为将idea 快速落地并测试是在华为软挑取得好成绩的关键。</li><li>有一点点算法基本功（三人都有 icpc/ccpc银），但相比其他一些队伍并不亮眼。</li><li>虽然没有单元测试，但编写了很多集成测试，帮助我们迅速定位没有正常达到目标的模块。</li><li>临时抱佛脚学习了 git（之前只会用 zip压缩+微信传代码）、cmake、clang-format 等工具，并写了一些 python 和shell 脚本。利用工具提升效率。</li></ul><p>不足之处：</p><ul><li>三人都没有大厂实习经验，缺乏项目合作开发的流程。例如没有需求和交付的流程和文档，git分支混乱，git 流程不规范，缺乏设计模式的使用等。</li><li>在决赛中，策略过于保守（意图避免出大错，也不算是一件坏事）。</li></ul><hr /><p>main.cpp</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifdef USE_MFMC</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE USE_MFMC&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef AVOID_SWING</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE AVOID_SWING&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef RAND</span><br><span class="line">    unsigned rand_seed = static_cast&lt;unsigned&gt;(time(nullptr));</span><br><span class="line">    srand(rand_seed);</span><br><span class="line">    std::cerr &lt;&lt; &quot;srand = &quot; &lt;&lt; rand_seed &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef LOCAL</span><br><span class="line">    std::cerr &lt;&lt; &quot;DEFINE LOCAL&quot; &lt;&lt; std::endl;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef DEBUG</span><br><span class="line">    fprintf(stderr, &quot;DEFIND DEBUG\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#if !defined(_WIN32) &amp;&amp; !defined(_WIN64)</span><br><span class="line">    // 将 main 线程绑定到 cpu 0 上</span><br><span class="line">    pthread_t main_thread_id = pthread_self();</span><br><span class="line">    cpu_set_t cpu_set;</span><br><span class="line">    CPU_ZERO(&amp;cpu_set);</span><br><span class="line">    CPU_SET(0, &amp;cpu_set);</span><br><span class="line">    pthread_setaffinity_np(main_thread_id, sizeof(cpu_set), &amp;cpu_set);</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;Set CPU!\n&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    Init();     // 初始化</span><br><span class="line">    Control();  // 控制所有帧</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFL-Net:Image Forgery Localization Using Contrastive Learning2</title>
      <link href="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/"/>
      <url>/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/</url>
      
        <content type="html"><![CDATA[<p>CFL-Net: Image Forgery Localization Using Contrastive Learning<ahref="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><p><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">post1</a><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/">post2</a></p><div class="row">    <embed src="/postpdfs/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/2210.02182v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p>总体框架流程：</p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/image-20240508214833256.png"alt="image-20240508214833256" /><figcaption aria-hidden="true">image-20240508214833256</figcaption></figure><p>ASPP流程：</p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><p>对比学习损失函数的具体计算流程如下：</p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/CFL-Net-contrast-loss.drawio.png"alt="CFL-Net-contrast-loss.drawio" /><figcaption aria-hidden="true">CFL-Net-contrast-loss.drawio</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFL-Net:Image Forgery Localization Using Contrastive Learning</title>
      <link href="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/"/>
      <url>/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<p>CFL-Net: Image Forgery Localization Using Contrastive Learning<ahref="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><p><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">post1</a><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning2/">post2</a></p><div class="row">    <embed src="/postpdfs/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/2210.02182v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h2 id="摘要">摘要</h2><p>  传统的伪造定位方法的缺点是过度拟合和只关注少数特定的伪造痕迹。我们需要一种更通用的图像伪造定位方法，能够很好地适应各种伪造条件。底层伪造区域定位的一个关键假设是，无论伪造类型如何，每个伪造图像样本中未被篡改和被篡改区域的特征分布都存在差异。在本文中，我们的目标是利用这种差异特征分布来帮助图像伪造定位。</p><p>  具体来说，我们使用对比损耗来学习映射到一个特征空间，在该空间中，每个图像的未篡改区域和被篡改区域之间的特征被很好地分离。此外，该方法不需要对伪造类型进行任何先验知识或假设，就可以对伪造区域进行局部定位。我们证明，我们的工作优于几个现有的方法在三个基准的图像处理数据集。</p><h2 id="引言">引言</h2><p>  交叉熵损失鼓励模型对同一类别提取相似特征。这可能有助于对Imagenet或cityscape等数据集进行分类或分割，在这些数据集中，相同类别的对象应该具有类似的特征。然而，在图像伪造定位中，由于不同的操作会在被篡改区域留下不同的伪造足迹，因此对数据集中所有被篡改区域提取相似的特征并不是最优的。因此，在没有附加约束的情况下，一个常见的基于交叉熵损失的框架容易对特定的伪造模式进行过拟合，这不利于泛化。</p><p>  考虑到这些局限性，我们在最近提出的对比损失的基础上，提出了一种新的伪造定位方法，称为对比伪造定位网络CFL-Net。我们的方法依赖于底层伪造区域定位的一般假设，即无论伪造类型如何，未被篡改区域和被篡改区域之间的特征统计量仍存在差异，即颜色、强度、噪声等。在本文中，我们着重于利用特征空间中的这种差异，通过对比损失来帮助图像伪造定位。具体来说，我们的模型学习映射到一个特征空间，在这个空间中，每个图像中未被篡改和被篡改区域之间的特征被很好地分离和分散。因此，我们的方法并不专注于特定的伪造线索。此外，我们还计算了每个样品的对比损失。因此，我们的方法对每个样本的伪造线索进行了不同的处理，这有助于归纳。</p><p>  因此，我们的方法对每个样本的伪造线索处理不同，有助于泛化。我们的主要贡献总结如下：</p><p>  1.提出了一种新的图像伪造定位方法，称为CFL-Net。利用了每个图像样本中未被篡改和被篡改区域之间特征分布的差异，而不关注特定的伪造足迹。因此，我们的方法更适合于检测真实生活中的伪造。</p><p>  2.解决了在无任何约束的情况下使用交叉熵损失进行通用图像伪造定位的问题。我们将对比损失纳入其中，并针对这个问题进行调整。</p><p>  3.我们在基准操作数据集上进行了大量的实验，表明我们的方法优于现有的几种图像伪造定位方法。</p><h2 id="网络框架">网络框架</h2><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/archnew.png"alt="CFL-Net" /><figcaption aria-hidden="true">CFL-Net</figcaption></figure><p>图3：对比学习模块：为了便于可视化，图中的投影头输出了一个形状为256×8×8的特征图F。然后将特征图分成4个×4个补丁。然后，对每个补丁中的4个空间向量进行平均，得到大小为4×4的嵌入（图中表示为‘k×k带标签的嵌入’）。groundtruth掩码也被划分为4×4个补丁，每个补丁中计数出出现的最大像素标签，得到输出的4×4掩码（图中表示为“k×k掩码”）。</p><p>  我们使用了两个流编码器，一个用于RGB输入图像，另一个用于SRM滤波图像。由编码器产生的特性被融合并传递到ASPP模块中。来自ASPP块的输出特征然后通过分割头和Projection头，其中第一个产生最终的预测掩模，后者产生进入对比学习模块的特征。</p><p>  我们使用SRM过滤器，并使用它作为其他流的输入。SRM滤波器是一种高通滤波器，它增强了输入图像的高频信息，从而更突出边缘信息，有利于篡改的定位。我们使用ResNet作为这两个流的Backbone。然后，我们通过将特性通道连接起来，融合两个流中的特性。融合特征图采用ASPP模块，提取多尺度信息。全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过在不同尺度上提取信息来帮助实现这一点，比如全局上下文以及更细粒度的像素级上下文信息变得可用。</p><p>  然后我们使用分割头和Projection头，将ASPP模块提取的上采样多尺度特征作为输入。我们选择一个DeepLab风格的分割头，输出大小为H×W的最终分割图。投影图由Conv-BatchNorm-Conv层构成，该层将特征图投影到<spanclass="math inline">\(F∈R^{256×H×W}\)</span>​​,256为嵌入维数。将嵌入的特征图F传递给对比学习模块。评估时不使用投影头。</p><h3 id="aspp模块">ASPP模块</h3><p>  在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><h3 id="对比学习模块">对比学习模块</h3><figure><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410174105358.png"alt="image-20240410174105358" /><figcaption aria-hidden="true">image-20240410174105358</figcaption></figure><p>  由于我们的嵌入式特征图在空间上的大小是H×W，并且我们有相应的、大小相似的真实掩模M，所以我们知道每个像素嵌入的标签。因此，我们可以使用有监督的对比学习。对于每个查询像素嵌入<spanclass="math inline">\(z_i\)</span>​，该嵌入的对比损失变为： <spanclass="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(z_i\cdot k^+/ \tau)}{exp(z_i\cdot k^+/\tau)+\sum_{k^-}exp(z_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(k^+\)</span>是一个嵌入与查询<spanclass="math inline">\(z_i\)</span>具有相同标签的像素。<spanclass="math inline">\(A_i\)</span>表示投影头输出特征图<spanclass="math inline">\(F\)</span>中所有<spanclass="math inline">\(k_+\)</span>的集合。同样，<spanclass="math inline">\(k_-\)</span>是<spanclass="math inline">\(F\)</span>中与<spanclass="math inline">\(z_i\)</span>不同标签的像素嵌入。</p><p>  然而，用这种方式计算<spanclass="math inline">\(L_i\)</span>有一些主要的局限性。首先，基于单像素嵌入的对比损失没有考虑相邻嵌入的上下文信息。此外，为了计算损失，需要存储大小为HW×HW的点积矩阵，这很消耗内存。因此，为了在上下文和细粒度轨迹之间找到平衡，我们选择将F划分为局部区域。</p><p>  我们首先将<spanclass="math inline">\(F∈R^{256×H×W}\)</span>在空间上划分为k×k个块，从而得到<spanclass="math inline">\(f_i\in R^{256\times h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。然后，我们取每个局部区域中像素嵌入的平均值。从而使每个<spanclass="math inline">\(f_i\)</span>都变成了<spanclass="math inline">\(R^{256}\)</span>的形状。以类似的方式，我们将地面真实掩模M划分为k×k个块。Mask在未被篡改区域的值为0，在伪造区域的值为1。我们得到<spanclass="math inline">\(m_i\in R^{h\times w}\)</span>，其中<spanclass="math inline">\(i\in\{1,2,3…k^2\}\)</span>、<spanclass="math inline">\(h=\frac{H}{k}\)</span>和<spanclass="math inline">\(w=\frac{W}{k}\)</span>。为了得到每个<spanclass="math inline">\(m_i\)</span>的标签值，我们计算了h×w个块中的0和1的数量。然后，我们指定块中的最大值为<spanclass="math inline">\(m_i\)</span>的值。</p><p>  然后，我们有了像素嵌入<spanclass="math inline">\(f_i\)</span>和每个嵌入<spanclass="math inline">\(m_i\)</span>​​​的相应标签。我们现在得到监督对比损失：<span class="math display">\[L_i=\frac{1}{|A_i|}\sum_{k\inA_i}-log(\frac{exp(f_i\cdot k^+/ \tau)}{exp(f_i\cdot k^+/\tau)+\sum_{k^-}exp(f_i\cdot k^-/ \tau)})\]</span>   其中，<spanclass="math inline">\(A_i\)</span>表示与<spanclass="math inline">\(f_i\)</span>具有相同标签的所有其他像素嵌入<spanclass="math inline">\(k^+\)</span>的集合。类似地，<spanclass="math inline">\(k^−\)</span>是所有与<spanclass="math inline">\(f_i\)</span>有不同标签的负像素嵌入。损失函数中的所有嵌入都是<spanclass="math inline">\(L_2\)</span>归一化的。对于单个图像样本，我们通过对所有嵌入的平均得到最终的对比损失：<span class="math display">\[L_{CON}=\frac{1}{k^2}\sum_{i\ink^2}L_i\]</span>   最后要优化的损失是： <spanclass="math display">\[L=L_{CE}+L_{CON}\]</span>   其中<spanclass="math inline">\(L_{CE}\)</span>是交叉熵损失。</p><h2 id="实验">实验</h2><p>  在本节中，我们将描述在三个不同的操作数据集上进行的实验，以探索CFL-Net的有效性。这些数据集是包含几种操作类型的通用操作数据集，并不只特定于一种操作类型。我们使用的评估度量是像素级的曲线下面积（AUC）评分。</p><p>  我们使用ResNet-50作为这两个流的编码器。我们用Adam优化器训练CFL-Net，学习率为1e-4。每过了20个epochs，我们就会将学习率降低20%。我们将输入图像的大小调整为256×256。我们将F划分为总共64个×64个块。温度系数<spanclass="math inline">\(\tau\)</span>​设置为0.1。对交叉熵损失进行加权，使被篡改类的权重增加十倍。我们将批处理大小设置为4，并在NVIDIARTX Titan GPU上训练模型超过100个epochs。</p><h3 id="与各种baseline模型的比较">与各种baseline模型的比较：</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410194714118.png" alt="image-20240410194714118 " style="zoom:50%;" /></p><p>  我们在表1中报告了我们的方法和基线模型的AUC评分（%）。需要注意的是，这里陈述的RGB-N和SPAN的结果是它们在各自的论文中报道的精细结果。JLSTM和Transforensics不进行任何预训练。虽然ManTraNet在合成操作数据集上对它们的模型进行了预训练，但它们并没有对特定的数据集进行微调。从表格中可以看出，CFLNet在基线模型中的所有数据集上都取得了最好的定位性能。特别是，CFLNet在IMD-20数据集上的性能大大优于所有的基线模型，而IMD-20数据集是一个具有各种伪造类型的真实操作数据集。具体来说，CFL-Net在IMD-20数据集上获得了89.9%的AUC分数，比性能第二良好的模型Transforensics提高了5.1%。</p><p>  因此，它验证了我们的主张，即cfll-net非常适合于本地化真实生活中的伪造品。我们的模型在其他数据集上也优于基线模型——Casia和Nist。此外，值得指出的是，CFLNet在没有对合成操作数据进行预训练的情况下就实现了这些结果。</p><h3 id="实验有无对比损失">实验（有无对比损失）</h3><p><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195126470.png"alt="image-20240410195126470" /><br/>表2：最左边的一列显示了所训练的数据集模型。后面的列是评估模型的数据集。“w/o”训练没有对比损失，“w”训练有对比损失。结果以%AUC表示。</p><p>  表2显示了结果。很明显，用对比损失训练的CFL-Net在跨数据集的推广方面表现得非常好。在所有情况下，该模型比没有对比损失的模型表现得更好。当在IMD-20上进行训练并在NIST的测试集上进行评估时，我们提出的模型甚至优于ManTraNet的AUC分数。当在IMD-20数据集上进行训练时，可以看到的性能提高最高。IMD-20是真实的图像操作数据集，因此在这个数据集上进行训练有助于模型学习最一般化的特征。因此，我们提出的在IMD-20上训练并在其他数据集上进行评估的模型，与在没有对比损失的情况下训练的模型相比，产生了最大的性能改进。<br/>  还需要注意的是，在NIST上训练和在其他数据集上评估的两种模型的表现都很差，因为NIST拥有的图像很少，即数据集中有584张图像。因此，很难使用NIST来推广到其他数据集。尽管如此，我们提出的模型还是比训练后的没有对比损失的模型表现得更好。</p><h3 id="定性分析">定性分析</h3><p>  为了证明我们的对比损失通过避免同一类特征的聚类来保持特征的变化，我们通过t-SNE将从图5中分割头获得的类特征可视化。</p><p><imgsrc="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410195906151.png"alt="image-20240410195906151" /><br/>图5：左栏显示了当仅使用交叉熵损失训练CFL-Net时，IMD-20和CASIA测试集上的平均特征的t-SNE图。右列对应于使用交叉熵损失和对比损失训练的CFL-Net。绿色=未篡改特征，红色=篡改特征。</p><p>  左列显示了当仅使用CFL-Net训练交叉熵损失时，IMD-20和CASIA测试集上每个图像样本的平均特征向量。很明显，未被篡改（图中绿色）和被篡改（图中红色）区域对应的特征在这里是堵塞的。与此同时，右栏显示了同时使用交叉熵和对比损失进行CFL-Net训练时的平均特征。这两个区域对应的特征更加分散。<br/>  因此，不同的操作足迹更容易可分离。实验表明，传统的交叉熵损失由于类别内不变性而减少了图像伪造定位的泛化，而我们提出的方法可以通过分散特征分布来提高泛化效果。</p><h3 id="消融实验损失函数变化">消融实验（损失函数变化）</h3><p><img src="/postimages/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/image-20240410200342409.png" alt="image-20240410200342409 " style="zoom:50%;" /></p><p>  在表3中，我们报告了结果。从表中可以清楚地看出，添加对比损失确实有助于定位。这种改进在真实的图像处理数据集IMD-20上更为突出。对比损失有助于提高AUC评分4.7%。需要注意的是，在没有对比损失的情况下，我们的方法已经取得了很好的结果。</p><h2 id="结论">结论</h2><p>  在本文中，我们从一个新的角度来研究通用的图像伪造定位问题。我们发现了现有方法的一个主要缺点，该方法关注特定的伪造足迹，并使用没有任何约束的交叉熵损失来定位伪造。为了解决这一缺点，我们补充了交叉熵损失和对比损失，并提出了一种新的图像伪造定位方法，即对比伪造定位网络CFL-Net。我们在三个基准图像处理数据集上进行了实验，并将实验结果与近年来的主要伪造定位方法进行了比较。CFL-Net在AUC度量方面优于所有方法。此外，在现实生活中的图像处理数据集IMD-2020上的改进更为突出。在未来的工作中，可以考虑一个更复杂的融合机制来融合来自RGB和SRM流的特征映射。例如，注意模块或最近提出的视觉变压器可以被用作一种融合机制。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文合集</title>
      <link href="/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/"/>
      <url>/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image ManipulationDetection</a>(AAAI24)<br/><em>现有问题</em>：</p><ul><li>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。</li><li>基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</li></ul><p><em>解决方案</em>：包含RGB和频率特征的双分支架构，能够检测双压缩伪影的压缩伪影学习模型。</p><details close><br/><summary>具体情况</summary><blockquote><p>RGB和频率特征的双分支架构</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><blockquote><p>双压缩伪影的压缩伪影学习模型</p></blockquote><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /><figcaption aria-hidden="true">image-20240326170313123</figcaption></figure></details><hr /><p><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -所有现有的IMD主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p><em>解决方案</em>：一种基于掩码引导查询的转换器框架（MGQFormer），该框架使用基本事实掩码来引导可学习查询令牌（LQT）识别伪造区域。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png"alt="image-20240401164940206" /><figcaption aria-hidden="true">image-20240401164940206</figcaption></figure><p>  利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征，过空间和通道注意模块（SCAM,spatialand channel attentionmodule）对多模态特征进行融合。其特征提取器如下:</p><p><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822164324909.png"alt="image-20240822164324909" /><br/>  我们设计了两个可学习的查询token来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。其解码器如下:</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><p>  具体来说，我们将噪声的GT掩模输入MGQFrorer，以获得引导查询token（GQT)和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>： -随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。</p><p><em>解决方案</em>：通过关注噪声域内的操纵痕迹来检测和定位图像伪造，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><p>一阶段：</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><br/>为了明确地分离出这两个区域（真实的和伪造的)的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $ 的噪声。</p><figure><imgsrc="./../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822220206200.png"alt="image-20240822220206200" /><figcaption aria-hidden="true">image-20240822220206200</figcaption></figure><p>式中， $ _a $ 、 $ _f $ 为 $ N_a $ 和 $ N_f $ 的标准差， $ _a $ 、 $_f $ 为 $ N_a $ 和 $ N_f $ 的平均值。</p><p><spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>二阶段：</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>利用两个分支来处理RGB和噪声信息，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。</p></details><hr /><p><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a>(AAAI24)</p><p><em>现有问题</em>： 假阳性（FPs）和不准确的边界。</p><p><em>解决方案</em>：基于CatmullRom样条的回归网络（CSR-Net, CatmullRomSplines-based RegressionNetwork），首次尝试将回归方法引入像素级任务。为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,ComprehensiveRe-scoringAlgorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP,Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><details close><br/><summary>具体情况</summary><figure><imgsrc="../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png"alt="image-20240503215025883" /><figcaption aria-hidden="true">image-20240503215025883</figcaption></figure><p>在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。</p><p>详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），我们为每个区域实例重新分配分数，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。</p><p>此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。</p><figure><imgsrc="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240823161357950.png"alt="image-20240823161357950" /><figcaption aria-hidden="true">image-20240823161357950</figcaption></figure><p>因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。</p></details><hr /><p><ahref="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/">Multi-viewFeature Extraction via Tunable Prompts is Enough for Image ManipulationLocalization</a>(ACMMM24)</p><p><em>现有问题</em>：IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。</p><p><em>解决方案</em>：提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。</p><details close><br/><summary>具体情况</summary><blockquote><p>通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png"alt="image-20240824155623293" /><figcaption aria-hidden="true">image-20240824155623293</figcaption></figure><blockquote><p>特征对齐和融合的FAF模块</p></blockquote><figure><imgsrc="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824220942200.png"alt="image-20240824220942200" /><figcaption aria-hidden="true">image-20240824220942200</figcaption></figure></details><hr /><p><a href="/UnionFormer/">UnionFormer: Unified-Learning Transformerwith Multi-View Representation for Image Manipulation Detection andLocalization</a>(CVPR24)</p><p><em>现有问题</em>：以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层的特征，不能充分表示篡改痕迹；目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。</p><p><em>解决方案</em>：设计了专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net,Boundary Sensitive Feature InteractionNetwork）设计了用于图像操作检测和定位的多视图表示的统一学习transformer框架</p><details close><br/><summary>具体情况</summary><blockquote><p>cnn-Transformer并发网络BSFI-Net，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617110632461.png"alt="image-20240617110632461" /><figcaption aria-hidden="true">image-20240617110632461</figcaption></figure><blockquote><p>采用对比监督来促进两个视图之间的协作</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240618124629395.png"alt="image-20240618124629395" /><figcaption aria-hidden="true">image-20240618124629395</figcaption></figure><blockquote><p>统一伪造判别表示，每个篡改判别查询都表示对应建议的三个视图中的篡改线索</p></blockquote><figure><img src="../postimages/UnionFormer/image-20240617214850871.png"alt="image-20240617214850871" /><figcaption aria-hidden="true">image-20240617214850871</figcaption></figure></details><hr /><p><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a>(AAAI24)</p><p><em>现有问题</em>：篡改痕迹主要来源于真实区域和伪造区域的噪声分布则不一致性。</p><p><em>解决方案</em>：使用两阶段训练方法：第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><details close><br/><summary>具体情况</summary><blockquote><p>第一阶段，训练一个特征提取器，使用的是DNE，一个盲去噪网络，使用JS散度来约束Gd，将Gd划分为真实区域Na和伪造区域Nf，将其视为两个高斯分布</p></blockquote><p><span class="math display">\[JSD=\\log\\frac{\\sqrt{\\sigma_{a}^{2}+\\sigma_{f}^{2}}}{2}-\\frac{\\log\\sigma_{a}+\\log\\sigma_{f}}{2}+\\frac{(\\mu_{a}-\\mu_{f})^{2}}{\\sigma_{a}^{2}+\\sigma_{f}^{2}}+\\frac{1}{2}\]</span></p><p><span class="math display">\[\\mathbf{L_{n}}=\\lambda\\left(1-JSD\\right)+\\left(1-\\lambda\\right)\\mathcal{L}\\left(Y,G_{c}\\right)\]</span></p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><blockquote><p>第二阶段，使用一阶段训练好的特征提取器DNE，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。</p></blockquote><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure></details><hr /><p>CVPR '23:</p><p><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a></p><p>ICCV '23:</p><p><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>WACV' 23:</p><p><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> (<em>WACV'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2210.02182"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/niloy193/CFLNet"><strong>Code</strong></a><strong>]</strong></p><p>TPAMI '22:</p><p><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a></p><details close><br/><summary>CLIP</summary><p><br/>原论文：<br/><ahref="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/">LearningTransferable Visual Models From Natural Language Supervision</a></p>Coop：<br/><a href="/Prompt-Learning-for-Vision-Language-Models/">PromptLearning for Vision Language Models</a><br/></details><details close><br/><summary>HRnet</summary><p><br/>原论文：<br/><a href="/HRnet/">Deep High-ResolutionRepresentation Learning for Human Pose Estimation</a></p></details><details close><br/><summary>自监督学习——对比学习</summary><p><a href="">SimCLR</a></p><p>https://proceedings.mlr.press/v119/chen20j.html</p><p>https://github.com/google-research/simclr</p><p>https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning</p><p>本文介绍了SimCLR：一个用于视觉表征对比学习的简单框架。我们简化了最近提出的对比自监督学习算法，而不需要专门的架构或内存库。为了理解是什么使对比预测任务能够学习有用的表征，我们系统地研究了我们框架的主要组成部分。我们表明：（1）数据增强的组成在定义有效的预测任务中起着关键作用；（2）在表征和对比损失之间引入可学习的非线性变换，大大提高了学习表征的质量；（3）与监督学习相比，对比学习受益于更大的批量和更多的训练步骤。通过结合这些发现，我们能够在ImageNet上大大优于以前的自监督和半监督学习方法。在SimCLR学习的自监督表示上训练的线性分类器实现了76.5%的top-1准确率，这比以前的最先进技术提高了7%，与监督ResNet-50的性能相匹配。当只对1%的标签进行微调时，我们实现了85.8%的前五名准确率，比AlexNet少了100倍的标签。</p><p><a href="">Matrix Information Theory for Self-SupervisedLearning</a></p><p>https://paperswithcode.com/paper/kernel-ssl-kernel-kl-divergence-for-self</p></details><details close><summary>监督对比学习</summary><blockquote><p><ahref="https://paperswithcode.com/paper/supervised-contrastive-learning">SupervisedContrastive Learning | Papers With Code</a><br/>&gt; <br/>&gt; <ahref="https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html">SupervisedContrastive Learning NeurIPS 2020</a></p></blockquote></details><details close><summary>arXiv</summary><p><br/><a href="/FOCAL/">Rethinking Image Forgery Detection viaContrastive Learning and Unsupervised Clustering</a></p><p>图像伪造检测旨在检测和定位图像中的伪造区域。大多数现有的伪造检测算法都提出了将像素分类为伪造或原始的分类问题。然而，伪造像素和原始像素的定义仅在单个图像内是相对的，例如，图像a中的伪造区域实际上是其源图像B中的原始区域（拼接伪造）。现有的方法严重忽视了这种相对定义，不必要地将不同图像中的伪造（原始）区域混合到同一类别中。为了解决这一困境，我们提出了模糊控制聚类（FOCAL）方法，这是一种新的、简单但非常有效的基于对比学习和无监督聚类的图像伪造检测方法。</p>具体而言，FOCAL1）利用像素级对比学习，以逐图像的方式监督高级取证特征提取，明确地反映了上述相对定义；2）采用动态无监督聚类算法（而不是经过训练的算法）将学习到的特征聚类为伪造/原始类别，进一步抑制了训练数据对跨图像的影响；以及3）允许在不需要重新训练的情况下通过简单的特征级级联来进一步提高检测性能。<br/></details><details close><summary>论文（c）</summary><p><br/>MMM '24：<br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection andLocalization</a></p><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p></details>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HRnet</title>
      <link href="/HRnet/"/>
      <url>/HRnet/</url>
      
        <content type="html"><![CDATA[<p>Deep High-Resolution Representation Learning for Human PoseEstimation<a href="https://arxiv.org/abs/1902.09212"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><ahref="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"><imgsrc="https://img.shields.io/github/stars/leoxiaobin/deep-high-resolution-net.pytorch?style=flat"alt="GitHub" /></a></p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/hrnet.png"alt="Illustrating the architecture of the proposed HRNet" /><figcaption aria-hidden="true">Illustrating the architecture of theproposed HRNet</figcaption></figure><p>算法流程如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205309290.png"alt="architecture" /><figcaption aria-hidden="true">architecture</figcaption></figure><p>其中Bottleneck的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205400578.png"alt="Bottleneck" /><figcaption aria-hidden="true">Bottleneck</figcaption></figure><p>stage：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205432768.png"alt="stage2" /><figcaption aria-hidden="true">stage2</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205452973.png"alt="stage3" /><figcaption aria-hidden="true">stage3</figcaption></figure><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205529301.png"alt="stage4" /><figcaption aria-hidden="true">stage4</figcaption></figure><p>其中BasicBlock的网络如下：</p><figure><imgsrc="../postimages/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/image-20240408205604131.png"alt="BasicBlock" /><figcaption aria-hidden="true">BasicBlock</figcaption></figure><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> baseline </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hrnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rethinking Image Forgery Detection via Contrastive Learning and Unsupervised Clustering</title>
      <link href="/FOCAL/"/>
      <url>/FOCAL/</url>
      
        <content type="html"><![CDATA[<p>Rethinking Image Forgery Detection via Contrastive Learning andUnsupervised Clustering <ahref="https://github.com/HighwayWu/FOCAL"><imgsrc="https://img.shields.io/github/stars/HighwayWu/FOCAL?style=flat"alt="GitHub" /></a><a href="https://arxiv.org/abs/2308.09307"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg"alt="arXiv" /></a></p><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/FOCAL/2308.09307v1.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  图像伪造检测的目的是检测和定位图像中的伪造区域。大多数现有的伪造检测算法制定了分类问题，以分类像素为伪造或原始。然而，伪造像素和原始像素的定义只在一张图像中是相对的，例如，图像A中的伪造区域实际上是其源图像B中的原始区域（拼接伪造）。这种相对的定义被现有的方法严重忽视了，这些方法不必要地将不同图像中的锻造（原始）区域混合到同一个类别中。为了解决这一难题，我们提出了法医对比聚类（FOCAL）方法，这是一种新的、简单而又非常有效的基于对比学习和无监督聚类的图像伪造检测范式。</p><p>​  具体来说，FOCAL：</p><p>​    1)利用像素级对比学习，利用像素级对比学习以图像-图像的方式监督高级取证特征提取，明确反映上述相对定义；</p><p>​    2)采用动态无监督聚类算法（而不是训练的算法）将学习到的特征聚为伪造/原始类别，进一步抑制训练数据的交叉图像影响；</p><p>​    3)允许通过简单的特征级连接进一步提高检测性能，而不需要再训练。</p><p>​  广泛的实验结果在六个公共测试数据集表明，我们提出的FOCAL明显优于先进的竞争算法：Coverage+24.3%，<em>Columbia</em>+18.6%，FF+++17.5%，MISD+14.2%，CASIA+13.5%，IoU+10.3%。FOCAL的范式可以带来新的见解，并为图像伪造检测任务提供一个新的基准。该代码可以在https://github.com/HighwayWu/FOCAL上找到。</p><h1 id="引言">引言</h1><p>​  一般来说，现有的基于学习的图像伪造检测方法提出了两类分类问题，将像素分类为伪造或原始。需要指出的是，伪造像素和原始像素的定义只是相对于一幅图像的。</p><figure><img src="../postimages/FOCAL/image-20240518122357716.png"alt="image-20240518122357716" /><figcaption aria-hidden="true">image-20240518122357716</figcaption></figure><p>​  例如，图2 (a)中与两个人相关联的像素是原始的，而图2(b).中相同的像素是伪造的。不幸的是，这种相对的定义被现有的基于分类的伪造检测方法严重忽视了，这些方法不必要地将不同图像中的伪造（原始）区域混合到同一类别中。事实上，图2中的α1、α2和α3区域并不一定具有相似的法医特征，尽管它们属于相同的原始类别（与β1和β2相似）。因此，当看到同一组像素被标记为伪造和原始的时，分类器可能会被误导，导致训练不稳定和检测性能较差。</p><p>​  重新思考伪造像素和原始像素的相对定义，促使我们重新制定先前流行的分类问题，形成一个具有对比学习和无监督聚类的新范式。具体地说，我们在这项工作中提出了法医对比聚类（FOCAL,FOrensic ContrAstivecLustering）方法，一种新颖、简单而有效的图像伪造检测范例。FOCAL利用像素级对比学习，以逐幅图像的方式监督高级法医特征提取，明确地利用上述相对定义。ground-truth的伪造掩模自然地提供了积极和消极类别的像素级区分，使我们的像素级对比学习。此外，我们的对比学习的另一个独特特征是逐图像监督，这可以有效地避免一批不同图像间特征的相互影响。</p><p>​  此外，FOCAL采用动态无监督聚类算法将学习到的特征聚类为伪造/原始类别，进一步避免了训练数据的交叉图像干扰。请注意，这里所采用的聚类模块不涉及任何可训练的参数，因此不参与训练过程。研究还表明，通过直接的特征级融合，可以进一步提高性能，而不需要再训练。</p><p>​  广泛的实验结果在六个公共测试数据集表明，我们提出的焦点显著优于先进的竞争算法[8,25,28,50,15]大利润率：Coverage+24.3%，Columbia+18.6%，FF+++17.5%，MISD+14.2%，CASIA+13.5%，+NIST10.3%。FOCAL的范式可以带来新的见解，并为图像伪造检测任务提供一个新的基准。我们的主要贡献可以总结如下：</p><p>  1.我们从伪造/原始像素的相对定义度的角度，重新思考了基于分类的图像伪造检测范式的固有局限性。<br/>  1.我们设计了一种新颖的、简单而有效的基于对比学习和无监督聚类的范式用于图像伪造检测。<br/>  1.在6个（跨域）数据集上，所提出的焦点算法的性能显著优于几种最先进的图像伪造检测方法，IoU的平均增益为19.6%，F1的平均增益为10.4%。</p><h1 id="focal-对比学习"><strong>FOCAL 对比学习</strong></h1><p>​  FOCAL 的训练过程如图3 (b)所示：</p><figure><img src="../postimages/FOCAL/image-20240518144850862.png"alt="image-20240518144850862" /><figcaption aria-hidden="true">image-20240518144850862</figcaption></figure><p>​  一旦我们从给定的输入X中提取高级特征F，我们就通过像素级对比学习直接监督F。地面真实伪造掩模Y自然为我们提供了正和消极类别的索引，使有效的像素级对比学习。正如很快就会更清晰的那样，焦点的对比学习以逐图像的方式进行监督，这与现有的对整个正向小批执行监督的算法[19,6,15,54,56]有很大的不同。</p><p>​  具体来说，我们采用了一种改进的InfoNCE损失[16,35]来实现焦点中的对比学习。</p><p>​  我们首先通过执行一个扁平化操作来构造一个字典 $ f() : {}^{ } ^{ }$<span class="math display">\[f(F) \rightarrow \{ q , k^+_1 , k^+_2 ,..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K \}\]</span> ​  其中， $ { q ,k^+_1 , k^+_2 , ..., k^+_J , k^-_1 , k^-_2 , ..., k^-_K } $被定义为字典，q是一个编码查询。我们让 $ { q , k^+_1 , k^+_2 , ..., k^+_J} $表示属于原始区域的特征（以Y中的0为索引），而 $ { k^-_1 , k^-_2 , ...,k^-_K } $表示伪造区域的特征（以Y中的1索引）。</p><p>​  在图像伪造检测任务中，伪造或原始区域通常覆盖超过1像素（特征）的区域，这意味着字典中正键J的数量也远远大于1。然后，根据图像伪造任务而定制的改进的信息损失可以计算为<span class="math display">\[{\cal {L}}_{InfoNCE++}=-log \frac { \frac 1J \sum _{ j \in [1,J] } exp(q \cdot k^+_J / \tau ) } { \sum _{ i \in[1,K] } exp(q \cdot k^+_i / \tau ) }\]</span> ​  其中， $ $是一个温度超参数[51]。注意，在原始的InfoNCE loss[16,35]中，字典中只有一个q匹配的正键。在我们改进的InfoNCE损失(2)中，通过取q与正类${ k^+_j } $点积的期望，在每个损失计算中涉及所有的正键。这将促进优化过程。</p><p>​  需要强调的是，训练阶段的监督是直接在地面真实伪造掩模Y和提取的特征F之间进行的，而没有生成预测的伪造掩模。</p><p>​  此外，对于前向小批量中的每一幅图像， $ {}<em>{InfoNCE++} $以逐图像的方式（one-by-one）计算，而不是对整个批量进行计算，然后求和计算总体损失。更具体地说，给定一个小批特征$ {F_1、F_2、···、F_B} $ ，总体对比损失 $ {}</em>{ct} $ ： <spanclass="math display">\[{\cal {L}}_{ct}=\frac {1} {B} \sum _{b=1} ^{B}({\cal {L}}_{InfoNCE++}(F_b))\]</span>​  请注意，在上述（3）式中，没有合并小批特征来计算整体的 $ {}_{InfoNCE++}$，避免了训练数据的交叉图像影响。在伪造/原始像素的相对定义的指导下设计的总损失与[5,15,16,32]中的损失有很大的不同，[5,15,16,32]中的损失计算是在批处理级别进行的。</p><p>​  为了进一步证明(3)的合理性，我们在图4中绘制了传统的基于批处理和我们的逐图像图像的对比损失曲线。<br/><imgsrc="../postimages/FOCAL/image-20240518153103103.png"alt="image-20240518153103103" /></p><p>​  可以清楚地看到，损失函数（橙色线）的逐图像设计不仅使收敛速度更快，而且使优化更加稳定。特别是，在蓝线中检测到的高振幅脉冲表明相关的图像中可能存在严重的冲突，例如，类似于图2(a)和(b)的情况，其中出现冲突的标签。</p><p>​  最终，训练有素的提取器将被用于FOCAL测试阶段。正如预期的那样，并将通过实验验证，我们的像素级对比学习与逐幅图像的整体损失设计，显著提高了图像伪造检测性能。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> baseline </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FOCAL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征融合、特征采样方法合集</title>
      <link href="/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/"/>
      <url>/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%81%E7%89%B9%E5%BE%81%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="swim-transformer">Swim Transformer:</h1><p>PatchMerging类 PatchMerging 类是 Swin Transformer架构中用于降低特征图分辨率的层。这个过程通过合并相邻的patch来减少序列长度，同时增加通道数，以保持信息的密度。</p><p>每执行一个stage后，都会执行一个一个下采样操作，也就是PatchMerging类的前向传播。所谓的下采样操作，主要是把x切片成4个，<spanclass="math inline">\(x_0\)</span>、<spanclass="math inline">\(x_1\)</span>、<spanclass="math inline">\(x_2\)</span>、<spanclass="math inline">\(x_3\)</span>这四个是按照长宽间隔去选的：</p><figure><imgsrc="../postimages/特征融合、特征采样方法合集/f34d8f6a311047b18d5e32e49a827f8b.png"alt="Swim Transformer" /><figcaption aria-hidden="true">Swim Transformer</figcaption></figure><p>x原来是8 ∗ 8，取完后变成了4个4 ∗4的，再把4个做一个拼接，拼接完成后再连接一个全连接，使用全连接进行降维。</p><p>构造函数：</p><ul><li>input_resolution ，dim ：输入特征的分辨率和通道数</li><li>reduction，初始化一个线性变换，用于将相邻四个patch的特征合并成一个patch</li></ul><p>前向传播：</p><p>原始输入：</p><ol type="1"><li>torch.Size([4, 3136, 96])<br/>2. H, W =56<em>56，输入特征长宽<br/>3. B, L, C =4</em>3136*96，batch，序列长度，特征维度<br/>4. x： torch.Size([4, 56,56, 96])，将输入特征重塑为四维张量，准备进行patch合并操作<br/>5. x0：torch.Size([4, 28, 28, 96])、x1： torch.Size([4, 28, 28, 96])、x2：torch.Size([4, 28, 28, 96])、x3： torch.Size([4, 28, 28,96])，提取四个相邻patch的特征，每个patch分别来自原始特征图的不同子区域<br/>6.x： torch.Size([4, 28, 28,384])，将四个patch的特征在通道维度上合并<br/>7. x： torch.Size([4, 784,384])，将合并后的特征图重塑，准备进行线性变换<br/>8. x： torch.Size([4,784, 384])，层归一化，维度不变<br/>9. x： torch.Size([4, 784,192])，通过线性变换降低合并后特征的维度，减少通道数</li></ol><details close><br/><summary>Swim Transformer code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_resolution = input_resolution</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        H, W = <span class="variable language_">self</span>.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.reduction(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br/></details><h1 id="hrnet">hrnet：</h1><p>详情<ahref="/Deep-High-Resolution-Representation-Learning-for-Human-Pose-Estimation/">DeepHigh-Resolution Representation Learning for Human PoseEstimation</a></p><p>以下是Hrnet特征间信息交互过程，为x_fuse过程，本身并不改变各个分辨率特征的大小</p><p>fuse_layers是hrnet多个不同分辨率特征信息交互的网络，本身并不改变各个分辨率特征的大小以下为fuse_layers的构造代码：</p><details close><br/><summary>hrnet：code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_branches = <span class="variable language_">self</span>.num_branches</span><br><span class="line">num_inchannels = <span class="variable language_">self</span>.num_inchannels</span><br><span class="line">fuse_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_branches <span class="keyword">if</span> <span class="variable language_">self</span>.multi_scale_output <span class="keyword">else</span> <span class="number">1</span>):</span><br><span class="line">    fuse_layer = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_branches):</span><br><span class="line">        <span class="keyword">if</span> j &gt; i:</span><br><span class="line">            fuse_layer.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    nn.Conv2d(</span><br><span class="line">                        num_inchannels[j],</span><br><span class="line">                        num_inchannels[i],</span><br><span class="line">                        <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span></span><br><span class="line">                    ),</span><br><span class="line">                    nn.BatchNorm2d(num_inchannels[i]),</span><br><span class="line">                    nn.Upsample(scale_factor=<span class="number">2</span>**(j-i), mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> j == i:</span><br><span class="line">            fuse_layer.append(<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv3x3s = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i-j):</span><br><span class="line">                <span class="keyword">if</span> k == i - j - <span class="number">1</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[i]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_outchannels_conv3x3 = num_inchannels[j]</span><br><span class="line">                    conv3x3s.append(</span><br><span class="line">                        nn.Sequential(</span><br><span class="line">                            nn.Conv2d(</span><br><span class="line">                                num_inchannels[j],</span><br><span class="line">                                num_outchannels_conv3x3,</span><br><span class="line">                                <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="literal">False</span></span><br><span class="line">                            ),</span><br><span class="line">                            nn.BatchNorm2d(num_outchannels_conv3x3),</span><br><span class="line">                            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">                        )</span><br><span class="line">                    )</span><br><span class="line">            fuse_layer.append(nn.Sequential(*conv3x3s))</span><br><span class="line">    fuse_layers.append(nn.ModuleList(fuse_layer))</span><br></pre></td></tr></table></figure><p>以下为fuse_layers的前向传播代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.fuse_layers)):</span><br><span class="line">    y = x[<span class="number">0</span>] <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.fuse_layers[i][<span class="number">0</span>](x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="variable language_">self</span>.num_branches):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            y = y + x[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = y + <span class="variable language_">self</span>.fuse_layers[i][j](x[j])</span><br><span class="line">    x_fuse.append(<span class="variable language_">self</span>.relu(y))</span><br></pre></td></tr></table></figure></details><h1 id="aspp">ASPP:</h1><p><a href="https://arxiv.org/abs/1706.05587">Rethinking atrousconvolution for semantic image segmentation</a></p><p>​在融合的特征图上使用了ASPP模块[7]，从而可以提取出多尺度的信息。据[41]报道，全局上下文有助于收集更多的线索，如对比度差异等，用于操作检测。ASPP模块通过提取不同尺度的信息来帮助实现这方面，这样全局上下文以及更细粒度的像素级上下文信息就可用了。</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/image-20240411165011289.png"alt="image-20240411165011289" /><figcaption aria-hidden="true">image-20240411165011289</figcaption></figure><p>其中nn.Conv2d中的dilation参数含义如下：<br/>dilation = 1：</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>dilation=2:</p><figure><imgsrc="/postimages/特征融合、特征采样方法合集/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZCR5LiK55qE6Zi_6bmP,size_11,color_FFFFFF,t_70,g_se,x_16-1712826246932-3.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><details close><br/><summary>ASPP code</summary><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, kernel_size, padding, dilation, BatchNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(_ASPPModule, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,</span><br><span class="line">                                            stride=<span class="number">1</span>, padding=padding, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn = BatchNorm(planes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.atrous_conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_stride == <span class="number">16</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>]</span><br><span class="line">        <span class="keyword">elif</span> output_stride == <span class="number">8</span>:</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">36</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.aspp1 = _ASPPModule(inplanes, outplanes, <span class="number">1</span>, padding=<span class="number">0</span>, dilation=dilations[<span class="number">0</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp2 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">1</span>], dilation=dilations[<span class="number">1</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp3 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">2</span>], dilation=dilations[<span class="number">2</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="variable language_">self</span>.aspp4 = _ASPPModule(inplanes, outplanes, <span class="number">3</span>, padding=dilations[<span class="number">3</span>], dilation=dilations[<span class="number">3</span>], BatchNorm=BatchNorm)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                             nn.Conv2d(inplanes, outplanes, <span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                             BatchNorm(outplanes),</span><br><span class="line">                                             nn.ReLU())</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(outplanes*<span class="number">5</span>, outplanes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = BatchNorm(outplanes)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = <span class="variable language_">self</span>.aspp1(x)</span><br><span class="line">        x2 = <span class="variable language_">self</span>.aspp2(x)</span><br><span class="line">        x3 = <span class="variable language_">self</span>.aspp3(x)</span><br><span class="line">        x4 = <span class="variable language_">self</span>.aspp4(x)</span><br><span class="line">        x5 = <span class="variable language_">self</span>.global_avg_pool(x)</span><br><span class="line">        x5 = F.interpolate(x5, size=x4.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat((x1, x2, x3, x4, x5), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                <span class="comment"># n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels</span></span><br><span class="line">                <span class="comment"># m.weight.data.normal_(0, math.sqrt(2. / n))</span></span><br><span class="line">                torch.nn.init.kaiming_normal_(m.weight)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_aspp</span>(<span class="params">inplanes = <span class="number">2048</span>, outplanes = <span class="number">512</span>, output_stride=<span class="number">16</span>, BatchNorm= nn.BatchNorm2d</span>):</span><br><span class="line">    <span class="keyword">return</span> ASPP(inplanes, outplanes, output_stride, BatchNorm)</span><br></pre></td></tr></table></figure></details><h1 id="srm-滤波器">SRM 滤波器:</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/6197267">RichModels for Steganalysis of Digital Images</a></p><p>SRM滤波器可以捕捉高频篡改痕迹，使用SRM过滤器从RGB图像中提取局部噪声特征</p><p>我们的设置中，噪声是通过像素值与仅通过内插相邻像素的值而产生的该像素值的估计之间的残差来建模的。从30个基本滤波器开始，再加上非线性运算（例如，滤波后附近输出的最大值和最小值），SRM功能将收集基本噪声特征。SRM量化并截断这些滤波器的输出，并提取附近的共现信息作为最终特征。从该过程获得的特征可以被视为局部噪声描述符。我们选择3个内核，其权重如下所示，并将其直接输入经过3通道输入训练的预训练网络中。我们将噪声流中SRM滤波器层的内核大小定义为5×5×3。SRM层的输出通道大小为3。</p><p><span class="math display">\[\frac {1} {4} \begin{bmatrix}0 &amp; 0&amp; 0 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 2&amp; -4 &amp; 2 &amp; 0\\0 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\0 &amp; 0&amp; 0 &amp; 0 &amp; 0\end{bmatrix}\\\frac {1} {12} \begin{bmatrix}-1&amp; 2 &amp; -2 &amp; 2 &amp; -1\\2 &amp; -6 &amp; 8 &amp; -6 &amp;2\\-2 &amp; 8 &amp; -12 &amp; 8 &amp; -2\\2 &amp; -6 &amp; 8 &amp; -6&amp; 2\\-1 &amp; 2 &amp; -2 &amp; 2 &amp; -1\end{bmatrix}\\\frac {1}{2} \begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 0&amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp;0\end{bmatrix}\]</span></p><details close><br/><summary>SRM 滤波器 code 1 from CFL-Net</summary><p>来自于<ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def setup_srm_weights(input_channels: int = 3, output_channel=1) -&gt; torch.Tensor:</span><br><span class="line">    &quot;&quot;&quot;Creates the SRM kernels for noise analysis.</span><br><span class="line">    note: values taken from Zhou et al., &quot;Learning Rich Features for Image Manipulation Detection&quot;, CVPR2018</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional):  Defaults to 3.</span><br><span class="line">        output_channel (int, optional): Defaults to 1.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.Tensor</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    srm_kernel = torch.from_numpy(</span><br><span class="line">        np.array([</span><br><span class="line">            [  # srm 1/2 horiz</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 1., -2., 1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/4</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 2., -4., 2., 0.],</span><br><span class="line">                [0., -1., 2., -1., 0.],</span><br><span class="line">                [0., 0., 0., 0., 0.],</span><br><span class="line">            ],</span><br><span class="line">            [  # srm 1/12</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-2., 8., -12., 8., -2.],</span><br><span class="line">                [2., -6., 8., -6., 2.],</span><br><span class="line">                [-1., 2., -2., 2., -1.],</span><br><span class="line">            ]</span><br><span class="line">        ])).float()</span><br><span class="line">    srm_kernel[0] /= 2</span><br><span class="line">    srm_kernel[1] /= 4</span><br><span class="line">    srm_kernel[2] /= 12</span><br><span class="line">    return srm_kernel.view(3, 1, 5, 5).repeat(output_channel, input_channels, 1, 1)</span><br><span class="line">    </span><br><span class="line">def setup_srm_layer(input_channels: int = 3, output_channel=None) -&gt; torch.nn.Module:</span><br><span class="line">    &quot;&quot;&quot;Creates a SRM convolution layer for noise analysis.</span><br><span class="line">    Args:</span><br><span class="line">        input_channels (int, optional): [description]. Defaults to 3.</span><br><span class="line">        output_channel ([type], optional): [description]. Defaults to None.</span><br><span class="line">    Returns:</span><br><span class="line">        torch.nn.Module: [description]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if output_channel == None:</span><br><span class="line">        weights = setup_srm_weights(input_channels)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)</span><br><span class="line">    else:</span><br><span class="line">        weights = setup_srm_weights(input_channels, output_channel)</span><br><span class="line">        conv = torch.nn.Conv2d(input_channels,</span><br><span class="line">                               out_channels=output_channel,</span><br><span class="line">                               kernel_size=5,</span><br><span class="line">                               stride=1,</span><br><span class="line">                               padding=2,</span><br><span class="line">                               bias=False)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        conv.weight = torch.nn.Parameter(weights, requires_grad=False)</span><br><span class="line">    return conv</span><br><span class="line">    </span><br></pre></td></tr></table></figure></details><details close><br/><summary>SRM 滤波器 code 2 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><p>SRM滤波器[14，66]使用预定义的核来学习中心像素的相邻像素之间不同类型的噪声残差，然后进行线性或非线性的最大/最小运算。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SRMConv2d(nn.Module):</span><br><span class="line">    def __init__(self, stride: int = 1, padding: int = 2, clip: float = 2):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.clip = clip</span><br><span class="line">        self.conv = self._get_srm_filter()</span><br><span class="line"></span><br><span class="line">    def _get_srm_filter(self):</span><br><span class="line">        filter1 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 2, -4, 2, 0],</span><br><span class="line">            [0, -1, 2, -1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        filter2 = [</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-2, 8, -12, 8, -2],</span><br><span class="line">            [2, -6, 8, -6, 2],</span><br><span class="line">            [-1, 2, -2, 2, -1],</span><br><span class="line">        ]</span><br><span class="line">        filter3 = [</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 1, -2, 1, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">            [0, 0, 0, 0, 0],</span><br><span class="line">        ]</span><br><span class="line">        q = [4.0, 12.0, 2.0]</span><br><span class="line">        filter1 = np.asarray(filter1, dtype=float) / q[0]</span><br><span class="line">        filter2 = np.asarray(filter2, dtype=float) / q[1]</span><br><span class="line">        filter3 = np.asarray(filter3, dtype=float) / q[2]</span><br><span class="line">        filters = [</span><br><span class="line">            [filter1, filter1, filter1],</span><br><span class="line">            [filter2, filter2, filter2],</span><br><span class="line">            [filter3, filter3, filter3],</span><br><span class="line">        ]</span><br><span class="line">        filters = torch.tensor(filters).float()</span><br><span class="line">        conv2d = nn.Conv2d(</span><br><span class="line">            3,</span><br><span class="line">            3,</span><br><span class="line">            kernel_size=5,</span><br><span class="line">            stride=self.stride,</span><br><span class="line">            padding=self.padding,</span><br><span class="line">            padding_mode=&quot;zeros&quot;,</span><br><span class="line">        )</span><br><span class="line">        conv2d.weight = nn.Parameter(filters, requires_grad=False)</span><br><span class="line">        conv2d.bias = nn.Parameter(torch.zeros_like(conv2d.bias), requires_grad=False)</span><br><span class="line">        return conv2d</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        if self.clip != 0.0:</span><br><span class="line">            x = x.clamp(-self.clip, self.clip)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    srm = SRMConv2d()</span><br><span class="line">    x = torch.rand((63, 3, 64, 64))</span><br><span class="line">    x = srm(x)</span><br></pre></td></tr></table></figure></details><h1 id="bayarconv">BayarConv:</h1><p><ahref="https://ieeexplore.ieee.org/abstract/document/8335799">BelhassenBayar and Matthew C Stamm. Constrained convolutional neural networks: Anew approach towards general purpose image manipulationdetection</a></p><p>Bayar卷积滤波器[2]通过使用可学习权重来改进SRM滤波器，约束条件是相邻像素的加权和等于中心像素的权重的负。</p><details close><br/><summary>BayarConv code 1 from Towards Generic Image Manipulation Detection withWeakly-Supervised Self-Consistency Learning</summary><p>来自于<ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from einops import rearrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BayarConv2d(nn.Module):</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channles: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        kernel_size: int = 5,</span><br><span class="line">        stride: int = 1,</span><br><span class="line">        padding: int = 0,</span><br><span class="line">        magnitude: float = 1.0,</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line">        assert kernel_size &gt; 1, &quot;Bayar conv kernel size must be greater than 1&quot;</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channles</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.magnitude = magnitude</span><br><span class="line"></span><br><span class="line">        self.center_weight = nn.Parameter(</span><br><span class="line">            torch.ones(self.in_channels, self.out_channels, 1) * -1.0 * magnitude,</span><br><span class="line">            requires_grad=False,</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight = nn.Parameter(</span><br><span class="line">            torch.rand((self.in_channels, self.out_channels, kernel_size**2 - 1)),</span><br><span class="line">            requires_grad=True,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def _constraint_weight(self):</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(2, 0, 1)</span><br><span class="line">        self.kernel_weight.data = torch.div(</span><br><span class="line">            self.kernel_weight.data, self.kernel_weight.data.sum(0)</span><br><span class="line">        )</span><br><span class="line">        self.kernel_weight.data = self.kernel_weight.permute(1, 2, 0) * self.magnitude</span><br><span class="line">        center_idx = self.kernel_size**2 // 2</span><br><span class="line">        full_kernel = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                self.kernel_weight[:, :, :center_idx],</span><br><span class="line">                self.center_weight,</span><br><span class="line">                self.kernel_weight[:, :, center_idx:],</span><br><span class="line">            ],</span><br><span class="line">            dim=2,</span><br><span class="line">        )</span><br><span class="line">        full_kernel = rearrange(</span><br><span class="line">            full_kernel, &quot;ci co (kw kh) -&gt; ci co kw kh&quot;, kw=self.kernel_size</span><br><span class="line">        )</span><br><span class="line">        return full_kernel</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = nn.functional.conv2d(</span><br><span class="line">            x, self._constraint_weight(), stride=self.stride, padding=self.padding</span><br><span class="line">        )</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    device = &quot;cuda&quot;</span><br><span class="line">    bayer_conv2d = BayarConv2d(3, 3, 3, magnitude=1).to(device)</span><br><span class="line">    bayer_conv2d._constraint_weight()</span><br><span class="line">    i = torch.rand(16, 3, 16, 16).to(device)</span><br><span class="line">    o = bayer_conv2d(i)</span><br></pre></td></tr></table></figure></details><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% 读取图像</span><br><span class="line">grayImage = imread(&#x27;image.jpg&#x27;);</span><br><span class="line"></span><br><span class="line">% 检查图像是否为灰度图像，如果不是，转换为灰度图像</span><br><span class="line">if size(grayImage, 3) == 3</span><br><span class="line">    grayImage = rgb2gray(grayImage);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% 显示灰度图像</span><br><span class="line">imshow(grayImage);</span><br><span class="line">title(&#x27;显示灰度图像&#x27;);</span><br><span class="line"></span><br><span class="line">% 保存灰度图像</span><br><span class="line">imwrite(grayImage, &#x27;grayImage.png&#x27;);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TruFor: Leveraging all-round clues for trustworthy image forgery detection and localization</title>
      <link href="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/"/>
      <url>/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/</url>
      
        <content type="html"><![CDATA[<p>TruFor: Leveraging all-round clues for trustworthy image forgerydetection and localization<br/>[<imgsrc="https://img.shields.io/badge/CVPR-2023-orange" alt="CVPR" />] <ahref="https://arxiv.org/abs/2212.10957"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/grip-unina/TruFor"><imgsrc="https://img.shields.io/github/stars/grip-unina/TruFor?style=flat"alt="GitHub" /></a></p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NP++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection</title>
      <link href="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/"/>
      <url>/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<p>MVSS-Net: Multi-View Multi-Scale Supervised Networks for ImageManipulation Detection<ahref="https://ieeexplore.ieee.org/abstract/document/9789576"><imgsrc="https://img.shields.io/badge/TPAMI-2022-orange"alt="TPAMI" /></a><a href="https://arxiv.org/abs/2112.08935"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/dong03/MVSS-Net"><imgsrc="https://img.shields.io/github/stars/dong03/MVSS-Net?style=flat"alt="GitHub" /></a></p><h2 id="摘要">摘要:</h2><p>​由于通过复制移动、拼接和/或绘制来操纵图像可能导致对视觉内容的误解，因此检测这些类型的操作对于媒体取证至关重要。考虑到对内容的各种可能的攻击，设计一种通用的方法是非常重要的。当前基于深度学习的方法在训练数据和测试数据一致时很有前景，但在独立测试时表现不佳。此外，由于缺乏真实的测试图像，其图像级检测特异性值得怀疑。关键问题是如何设计和训练一个深度神经网络，使其能够学习对新数据操作敏感的可泛化特征，同时防止真实数据的误报。我们提出了多视图特征学习来共同利用篡改边界伪影和输入图像的噪声视图。由于这两个线索都是语义不可知论的，因此学习到的特征是可概括的。为了有效地从真实图像中学习，我们使用多尺度(像素/边缘/图像)监督进行训练。我们将新网络命名为MVSS-Net及其增强版本MVSS-Net++。在数据集内和跨数据集场景下进行的实验表明，MVSS-Net++表现最佳，并且对JPEG压缩、高斯模糊和基于截图的图像重捕获具有更好的鲁棒性。</p><h2 id="贡献">贡献:</h2><p>综上所述，我们的主要贡献如下:</p><ol type="1"><li>提出了一种新的图像处理检测网络MVSS-Net。如图2所示，MVSS-Net的技术优势在于它能够以端到端的方式共同利用多视图输入、显式提取的边界构件和整体信息。多视图特征学习的目的是提取语义不可知的特征，从而获得更一般化的特征。<br/>2.通过多尺度监督进行网络训练。这使我们能够有效地从真实的图像中学习，而这些图像被现有技术所忽略。因此，大大提高了操作检测的特异性。<br/>3.在多个基准测试中优于SOTA。在两个训练集和六个测试集上进行的大量实验表明，MVSS-Net优于SOTA。真实测试图像的包含揭示了模型在图像级别的检测特异性。<br/></li></ol><h2 id="mvss模型">MVSS++模型：</h2><p><img src="/postimages/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/image-20240401205128277.png" alt="image-20240401205128277 " style="zoom:80%;" /></p><h2 id="实验">实验：</h2><h3 id="像素级篡改检测">像素级篡改检测：</h3><figure><imgsrc="/postimages/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/image-20240401203711268.png"alt="image-20240401203711268" /><figcaption aria-hidden="true">image-20240401203711268</figcaption></figure><h3 id="图像级篡改检测">图像级篡改检测：</h3><figure><imgsrc="/postimages/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/image-20240401213418833.png"alt="image-20240401213418833" /><figcaption aria-hidden="true">image-20240401213418833</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</title>
      <link href="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/"/>
      <url>/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<center>Learning Discriminative Noise Guidance for Image Forgery Detection andLocalization<ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a></center><center><span class="math inline">\(\text{Jiaying Zhu}^*,\text{DongLi}^*,\text{Xueyang Fu}^\dagger,\text{Gang Yang, Jie Huang, Aiping Liu,Zheng-Jun Zha}\)</span></center><center>中国科学技术大学</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/28608.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><p><br/># 摘要</p><p>​  随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。<br/>​  本研究介绍了一种新的图像伪造检测和定位方法，该方法通过关注<strong>噪声域内的操纵痕迹</strong>来检测和定位图像伪造。我们假设RGB图像中几乎看不见带有篡改痕迹的噪声，有助于识别和定位伪造品。然而，篡改技术的进步使噪声用于伪造检测的直接应用变得复杂，因为伪造区域和真实区域之间的噪声不一致性没有得到充分利用。为了解决这一问题，我们开发了一种两步判别噪声引导的方法，以明确地增强一致性中噪声的表示和使用，从而充分利用噪声信息来提高伪造检测的准确性和鲁棒性。<br/>​  具体而言，我们使用去噪网络和基于统计量的约束，增强了伪造区域与真实区域之间的噪声可识别性。然后，我们将模型驱动的引导学习机制与数据驱动的注意力机制相结合，创建了一个可学习且可微分的噪声引导学习机制。这种复杂的滤波器使我们能够从噪音中学习到的伪造区域的边缘。在多个数据集上的综合实验表明，我们的方法可以可靠地检测和定位伪造品，超过了现有的最先进的方法。</p><h1 id="引言">引言</h1><p>​  真实区域和伪造区域的噪声分布则不一致，导致噪声域中的操纵痕迹。许多研究人员已经利用这种噪音信息来图像伪造信息的检测与定位，实现了显著的结果。这些方法直接构建噪声特征到掩模的端到端映射，并采用融合策略集成RGB和噪声信息，以提高伪造检测精度。然而，随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不再是隐含的关系。<br/>​  鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。因此，我们引入了一种新的两步噪声引导方案。<br/>​  第一阶段，训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异。我们使用去噪网络，然后使用Bayar卷积来构建噪声提取器，并使用基于统计的约束进行优化。使用去噪器的基本原理源于一个合适的去噪网络可以放大两个区域间的噪声分布差异。<br/>​  如图1所示，标准差为25的去噪网络在噪声域内最大化了真实区域和伪造区域之间的差异，而直接提取的噪声不能达到相同的效果。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502202145227.png" alt="image-20240502202145227 " style="zoom:70%;" /></p><p>图1：我们使用不同标准差的去噪网络（CBDNet (Guoet al.2019），用标准差为15、25、50）对伪造图像进行处理，然后分别提取噪声。对于该图像，25个标准差的去噪器可以帮助获得鉴别噪声。<br/>  为了自适应地调整去噪网络，我们基于高斯分布的Jensen-Shannon（JS）散度对处理后的图像噪声施加自定义约束。</p><p>​  第二阶段是将噪声不一致和RGB数据集成，以进行伪造检测和定位。与之前的融合策略不同，我们明确地利用噪声来指导RGB分支，显著提高了有效性。我们将手工制作的引导变换（He，Sun，和Tang2012）和数据驱动的注意力机制（Wang et al.2018）合并，创建了基于交叉注意力的引导滤波器（CAGF，theCross-Attention-Based GuidedFilter）。由于引导变换的局部线性和边缘保存，CAGF不仅充分集成了RGB和噪声域的互补信息，而且确保了结构信息从噪声域到RGB域的传输。<br/>​  本质上，我们的方法明确地学习了第一阶段的噪声不一致，并在第二阶段利用了这种表示。</p><p>​  我们的贡献如下：<br/>​  1.我们提出了一种新的判别噪声引导方案，该方案明确增强了噪声不一致性的表示和利用。<br/>​  2.我们开发了一种方法来突出伪造区域中的噪声不一致性，使用去噪网络来处理图像，并使用基于统计的约束来优化噪声提取。<br/>​  3.我们设计了一种基于交叉注意力的引导滤波器，它结合了模型驱动和数据驱动技术，充分利用伪造信息的噪声表示，显著增强了噪声不一致对RGB分支的引导效果。</p><p>​  在几个有代表性的基准上进行的广泛实验表明，我们的方法优于最先进的方法，特别是在真实数据集IMD20(Novozamsky,Mahdian, and Saic 2020)上。</p><h1 id="方法">方法</h1><p>​  源图像和目标图像之间的噪声特征不太可能匹配（Zhou et al.2018），篡改操作破坏了自然噪声分布（Wang et al.2022a）。此外，使用噪声可以抑制内容信息，这有利于提取IFDL的语义不可知的特征（Chenet al.2021）。然而，随着篡改和后处理技术的发展，噪声的不一致性并不明显，不再是隐含的关系。我们认为，明确地挖掘和利用噪声不一致性可以进一步提高精度和鲁棒性。<br/>​  因此，我们提出了一种两步策略，来充分利用IFDL的噪声不一致性，包括噪声表示学习和噪声引导网络。<br/>​  首先，我们提出了一种使用去噪网络和自定义约束的学习方案。输入图像用$ X R^{H×W×3} $表示，其中H和W表示图像的高度和宽度。将X输入去噪网络得到图像 $ X^{'} $，然后用BayarConv (Wu, AbdAlmageed, and Natarajan 2019)提取噪声 $ G_dR^{H×W×3} $。去噪网络的选择不是本工作的重点，所以我们使用广泛使用的CBDNet（Guo etal. 2019）来权衡性能和计算量。我们对 $ G_d $施加了基于统计的约束，以确保这两个区域的噪声分布被分开。<br/>​  其次，我们训练噪声引导网络（NGNet,noise-guidednetwork）显式地应用噪声不一致性。NGNet是一个双分支网络，它包含多个基于交叉注意力引导的信号，逐步引导具有噪声不一致的RGB信息，以获得更准确的IFDL。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png"alt="image-20240822204529012" /><figcaption aria-hidden="true">image-20240822204529012</figcaption></figure><h2 id="噪声表示学习">噪声表示学习</h2><p>​  为了明确地表示噪声的不一致性，我们设计了一个噪声表示学习网络（NRLNet,noise representation learningnetwork），如图2a所示，它使用一个去噪网络来处理图像和统计损失来优化网络。考虑到伪造图像的噪声分布未知，我们使用盲去噪网络CBDNet（Guoet al. 2019)，并加载盲去噪的权值作为初始化。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png"alt="image-20240502204239503" /><figcaption aria-hidden="true">image-20240502204239503</figcaption></figure><p>​  具体的体系结构和培训策略如下所述。<br/>​  我们使用一个基于CBDNet的网络来处理输入的图像X。具体地说，我们预测了一个噪声水平图$~ \hat{l} \in R^{H×W×3} $ ，它可以看作是与噪声分布相关的权值。然后将 $$ 和输入X一起输入编码解码器结构，得到图像 $~ X^{'} \in R^{H×W×3} $，表示为： <spanclass="math display">\[\hat{l}=\mathrm{NE}\left(X\right)\\X^{&#39;}=\mathrm{D}\left(\mathrm{Concat}\left(X,\hat{l}\right)\right)\]</span></p><p>​  其中，Concat表示连接操作。NE是由5层全卷积网络实现的噪声估计模块，卷积核大小为3×3。D是一种U-Net架构，它可以获得具有鉴别噪声的图像。<br/>​  然后，跟随（Chenet al. 2021），我们采用BayarConv从 $ X^{'} $ 中提取噪声 $~ G_d \inR^{H×W×3} $。此外，为了使学习到的噪声更有利于IFDL，我们将噪声输入Res-CNN，以预测粗定位结果$~ G_c \in R^{H×W×1} $。Res-CNN包含10个res-块，其中一个块由两个3×3卷积和ReLU函数组成。<br/>​  <strong>最优化</strong>。为了明确地分离出这两个区域（真实的和伪造的）的噪声分布，我们引入了JS散度来约束$ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $N_a $ 的噪声和伪造区域 $ N_f $的噪声。图像中的平稳扰动可以建模为高斯分布（Guo et al. 2019）， $ N_a $和 $ N_f $都可以视为噪声的采样值。因此，我们利用连续高斯分布的JS散度来测量两个区域的噪声分布之间的距离：<span class="math display">\[JSD(P_a||P_f)= \frac {1}{2} KL(P_a||M) +\frac {1}{2} KL(P_f||M)\]</span> ​  式中， $ P_a $ 和 $ P_f $ 分别为 $N_a $ 和 $ N_f $ 的分布，M为 $ 2 $ 。两个高斯分布的KL散度计算如下：<span class="math display">\[KL(P_1||P_2)=log \sigma _2 − log \sigma _1+ \frac { \sigma _1 ^2 + {( \mu _1 - \mu _2 )}^2} {2 \sigma _2 ^2} -\frac {1} {2}\]</span> ​  式中， $~ \sigma_1 $ 、 $~ \sigma_2 $ 为 $ P_1$ 和 $ P_2 $ 的标准差， $~ \mu_1 $ 、 $~ \mu_2 $ 为 $ P_1 $ 和 $ P_2 $的平均值。然后，JSD的计算方法如下：</p><p><spanclass="math display">\[JSD=\log\frac{\sqrt{\sigma_{a}^{2}+\sigma_{f}^{2}}}{2}-\frac{\log\sigma_{a}+\log\sigma_{f}}{2}+\frac{(\mu_{a}-\mu_{f})^{2}}{\sigma_{a}^{2}+\sigma_{f}^{2}}+\frac{1}{2}\]</span></p><p>​  式中， $~ \sigma_a $ 、 $~ \sigma_f $ 为 $ N_a $ 和 $ N_f $的标准差， $~ \mu_a $ 、 $~ \mu_f $ 为 $ N_a $ 和 $ N_f $的平均值。此外，如果仅使用JS散度作为损失函数，网络的优化过程就会发生振荡。因此，我们采用伪造损失定位来辅助网络的优化，这也更有利于神经图像的伪造定位。  我们结合了辅助损失和JS散度组成噪声表示学习的损失函数，可以写成：<spanclass="math display">\[\mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right)\]</span>​  其中L表示Dice损失（Chen et al. 2021）， $~ Y \in R^{H×W×1} $为groundtruth， $ $ 为平衡两项的超参数，设置为0.80。</p><h2 id="噪声引导网络">噪声引导网络</h2><p>​  如图2b所示，在NRLNet收敛后，我们将训练好的判别噪声提取器（去噪器和BayarConv）嵌入到噪声引导网络（NGNet）中。与以前的工作不同，我们以显式指导的形式应用噪声。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png"alt="image-20240502211812066" /><figcaption aria-hidden="true">image-20240502211812066</figcaption></figure><p>​  NGNet的网络架构、基于交叉注意的引导滤波（CAGF）和优化具体如下。  <strong>网络架构。</strong>我们利用两个分支来处理RGB和噪声信息。利用训练好的判别噪声提取器获取噪声分支的输入，以更好地提取伪造痕迹。我们使用在ImageNet上预训练的ResNet-50(Deng et al.2009)作为骨干网络。然后，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。最后，我们将提取的具有平卷积层和双线性上采样的特征转换为最后预测掩模$~ G_{out} \in R^{H×W×1} $。  <strong>基于交叉注意力引导的滤波。</strong>现有的IFDL方法直接使用融合策略，不能明确保证噪声域内的篡改伪影得到充分利用。引导滤波器可以保证结构信息在从引导图像到目标图像的传输过程中具有保持边缘的特性(He, Sun, and Tang2012)。受此启发，我们从引导的角度探讨了噪声和RGB信息的融合，从而提出了CAGF，如算法1所示。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502212532274.png"alt="image-20240502212532274" /><figcaption aria-hidden="true">image-20240502212532274</figcaption></figure><p>​  我们在实践中使用了三个CAGF块。传统的引导滤波是由局部线性模型导出的。它通过考虑引导来产生滤波输出结果。  然而，传统的引导滤波是一种不可训练的算法，没有考虑引导与目标之间的相互依赖性，这不适用于IFDL。由于噪声和RGB之间的信息差距很大，简单地将结构信息从噪声传输到RGB就会产生各种伪影。因此，在传统算法的基础上，我们利用注意力机制来计算方差和协方差，并使用卷积层代替平均频率。具体而言，来自噪声流和RGB流的输入特征分别为$~ G_n \in R^{H_s×W_s×C_s} $ 和 $~ G_r \in R^{H_s×W_s×C_s} $ ，我们以 $G_n $ 作为引导图像，以 $ G_r $作为输入图像。首先，我们设计了新的跨模态注意（CMA, cross-modalattention)来获得协方差和方差。以协方差的计算为例，CMA的输入量分别为 $G_n $ 和 $ G_r $ 。CMA利用图3中描述的计算块将它们转换为 $ cov_{nr}R^{H_s×W_s×C_s} $</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171039698.png"alt="image-20240401171039698" /><figcaption aria-hidden="true">image-20240401171039698</figcaption></figure><p>​  具体计算公式如下：</p><p><spanclass="math display">\[\begin{aligned}cov_{nr}&amp;=CMA(G_n,G_r)\\&amp;=Res(C(C(G_n)^T  \otimes C(G_r)) \otimes C(C(G_n) \odotC(G_r)))\end{aligned}\]</span> ​  式中， $ $ 为矩阵乘法， $ $为Hadamard乘积，Res表示包含两个3×3卷积和ReLU函数的Res-块，C表示1×1卷积。我们执行$ G_n $ 和 $ G_r $ 的矩阵乘法获得 $ C R^{N×N} $ （ $ N=H_s×W_s $）而不是传统的引导滤波，并计算两者的Hadamard乘积来代替传统算法的 $mean_n ∗ mean_r $ (He, Sun, and Tang 2012)。在 $ G_n $ 和 $ G_r $的矩阵乘法之前，将两者转换为 $ C_s/r×N $，其中r是一个标量，它降低信道维数以提高计算效率。同样，当CMA的两个输入都是噪声特征$ G_n $ 时，我们可以得到方差 $ var_n R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[var_n = CMA(G_n, G_n)\]</span> ​  res块用于获得系数 $ a R^{H_s×W_s×C_s} $ ： <spanclass="math display">\[a = Res (Concat (cov_{nr}, var_n))\]</span>​  接下来，我们按照算法1中的公式来计算 $ b R^{H_s×W_s×C_s} $ 。受（Wu etal. 2018）的启发，我们使用卷积运算（MFConv）代替平均滤波： <spanclass="math display">\[mean_M = MFConv (M) = \frac {Conv (M)} {Conv (J)}\]</span>​  其中M为MFConv的输入，J为与M大小相同的全一矩阵，Conv为3×3卷积。</p><p>​  最后，我们根据局部线性关系得到CAGF的输出 $ q R^{H_s×W_s×C_s} $ ：<span class="math display">\[q = mean_a \odot G_r + mean_b\]</span>​  其中 $ mean_a $ 和 $ mean_b $ 是a和b经过MFConv后的结果，大小为 $H_s×W_s×C_s $ 。</p><p>​  <strong>检测器。</strong>对于检测器，我们采用了MVSS-Net++ (Dong etal. 2023)提出的ConvGeM，它可以将定位结果 $ G_{out} $ 转换为检测预测 $D_{out} $。ConvGeM通过一个衰减的跳过连接，在检测和定位之间取得了很好的平衡。因此，我们使用ConvGem来获得一个更准确的检测结果：<span class="math display">\[D_{out} = ConvGeM(G_{out})\]</span>​  <strong>最优化。</strong>根据大多数研究 (Chen et al. 2021;Wang et al.2022c)，我们也利用了边缘监督。但是，这并不是这项工作的重点，所以我们使用了一些常见的方  法。跟随（Chenet al. 2021），我们使用Sobel层和残差块，以浅到深的方式获得边缘预测 $ G_eR^{H_e×W_e×1} $ 。对于边缘损失，ground-truth边缘 $ E R^{H×W×1} $被降采样到一个更小的尺寸 $ E^{'} R^{H×W×1} $ 以匹配 $ G_e $。该策略在计算成本和性能方面优于上采样 $ G_e $ 。NGNet的损失可以写成：<span class="math display">\[L_N = \alpha L_1 (Y, G_{out}) + \beta L_2(y, D_{out})+(1 − \alpha − \beta ) L_3 (E^{&#39;},G_e)\]</span> ​  式中 $L_1 $ 和 $ L_3 $ 表示Dice损失（Chen et al. 2021）， $ L_2 $为BCE损失，y表示图像真实性的标签， $ $ ， $ $是平衡损失函数的超参数。实际上， $ $ 设置为0.60， $ $设置为0.2。请注意，真实的图像只用于计算 $ L_2 $ 。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>​  <strong>预训练数据。</strong>我们构建了一个大量的图像篡改数据集，并将其用于预训练我们的模型。该数据集包括三个类别：1）拼接、2）复制移动和3）删除。<br/>​  <strong>测试数据集。</strong>在CASIA (Dong, Wang, and Tan 2013)、Coverage (Wen et al. 2016)、Columbia(Hsu and Chang 2006)、Nist Nimble 2016 (NIST16) (Guan et al.2019)和IMD20(Novozamsky, Mahdian, and Saic2020)上评估我们的模型。我们使用 (Hu et al. 2020; Wang et al.2022b)一样的方法来划分训练，以便进行公平比较。<br/>​  <strong>评估指标。</strong>为了量化定位性能，根据之前的工作（Huet al. 2020；Wang et al.2022b），我们使用像素级曲线下面积（AUC）和F1分数。为了评估检测性能，我们使用了图像级的AUC和F1评分。由于计算f1分数需要二进制掩码，因此我们采用相等错误率（EER）阈值来对它们进行二值化。</p><h2 id="图像伪造定位">图像伪造定位</h2><p>​  在两种情况下，我们的模型与其他最先进的篡改定位方法进行了比较：1)在合成数据集上进行训练和对完整的测试数据集进行评估；2)对测试数据集的训练分割和对它们的测试分割进行评估。<br/>​  <strong>预训练模型。</strong>表1a显示了在像素级AUC下的fve数据集上，不同方法的预训练模型在五个数据集上的定位性能。我们将我们的模型NGNet与MantraNet(Wu, AbdAlmageed, and Natarajan 2019), SPAN (Hu et al. 2020), PSCCNet(Liu et al. 2022),ObjectFormer (Wang et al. 2022b), TANet (Shi, Chen,and Zhang 2023) 和HiFi-Net (Guo et al.2023)进行比较。预训练的NGNet在Coverage、CASIA、NIST16和IMD20上取得了最好的定位性能，在Columbia上排名第二。特别是NGNet在复制-移动数据集Coverage上达到了94.1%，该数据集图像伪造区域与背景难以区分。这验证了我们的模型具有在噪声域中捕获篡改痕迹的优越能力。我们未能在Columbia上取得最好的表现，在AUC下落后TANet0.2%。我们认为，可能的解释是他们合成的训练数据的分布与Columbia数据集非常相似。表1b的结果进一步支持了这一点，结果显示NGNet在AUC和F1分数方面都优于TANet。此外，值得指出的是，NGNet在较少的训练前数据下取得了不错的结果。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171128950.png"alt="image-20240401171128950" /><figcaption aria-hidden="true">image-20240401171128950</figcaption></figure><p>表1：图像伪造检测和定位结果。(a)训练前模型的定位性能。(b)调整模型的(b)定位性能。(c)对CASIA-D数据集的检测性能。（粗体表示最好，下划线表示第二好）。<br/>  <strong>微调模型。</strong>预训练模型的网络权值用于启动调整模型，这些模型将分别在Coverage、CASIA和NIST16数据集的训练分割上进行训练。我们在表1b中评估了不同方法的神经调优模型。对于AUC和F1，我们的模型取得了显著的性能提高。这验证了我们的方法可以通过识别噪声表示学习和基于交叉注意的引导转换来精确捕获细微的篡改痕迹。</p><h2 id="图像伪造检测">图像伪造检测</h2><p>​  为了避免误警报，我们还考虑了伪造检测任务。根据ObjectFormer（Wang etal. 2022b），我们对（Liu et al.2022）引入的CASIA-D数据集进行了实验比较。如表1c所示，我们的方法具有良好的检测性能，即AUC为99.81%，F1为98.72%。我们的方法明确地建模和利用噪声的不一致，从而准确地区分伪造的图像和真实的图像。</p><h2 id="鲁棒性评估">鲁棒性评估</h2><p>​  为了分析我们的定位模型的鲁棒性，我们遵循（Wang et al.2022b）中的失真设置来降解NIST16中的伪造图像。这些失真类型包括将图像调整到不同的尺度，应用核大小k的高斯模糊，添加标准偏差σ的高斯噪声，以及使用质量因子q执行JPEG压缩。我们将我们预训练模型的伪造定位性能（AUC分数）与SPAN和ObjectFormer做比较，并报告的结果在表2中。我们的模型对各种失真技术证明了更好的鲁棒性。值得注意的是，JPEG压缩通常是在向社交媒体上载图片时执行的。我们的模型在压缩图像上表现得明显更好。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171143021.png"alt="image-20240401171143021" /><figcaption aria-hidden="true">image-20240401171143021</figcaption></figure><p>表2：NIST16数据集在各种畸变情况下的性能。报告AUC分数（%），（模糊：高斯模糊：噪声：高斯噪声，压缩：JPEG压缩。）</p><h2 id="消融实验">消融实验</h2><p>​  在本节中，我们进行了实验来证明我们的方法的有效性。噪声表示学习（NRL,noise representationlearning）的设计是为了明确地扩大两个区域之间的噪声分布的差异（真实和伪造）。基于交叉注意力的引导滤波包含跨模态注意力（CMA,cross-modal attention）和引导滤波机制（GF, guided flteringmechanism）。CMA充分集成了RGB和噪声分支中所包含的互补信息，而GF则保证了结构信息从噪声到RGB的传输，并具有保持边缘的特性。为了评估NRL、CMA和GF的有效性，我们将它们从我们的方法中分离出来，并评估它们在CASIA和NIST16上的伪造定位性能。<br/>​  表3显示了定量结果。基线表示我们只使用ResNet-50。可以看出，没有GF，CASIA和NIST16的AUC评分下降了4.5%，而没有CMA，CASIA和NIST16的AUC评分下降了10.9%。此外，当丢弃NRL时，表3的性能严重下降，即AUC为9.5%，F1为20.9%。</p><p><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171152148.png"alt="image-20240401171152148" /><br/>  由于不同的去噪器可能产生不同的性能，我们对去噪器的选择进行了消融研究。我们选择DnCNN(Zhanget al. 2017)、FFDNet (Zhang, Zuo, and Zhang 2018)、RIDNet(Anwar andBarnes 2019) 和DRUNet (Zhang et al. 2021)等盲去噪网络进行比较。如表4所示，CBDNet权衡了性能和计算复杂度。<br/><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240401171207701.png"alt="image-20240401171207701" /></p><h2 id="可视化结果">可视化结果</h2><p>​  <strong>定性的结果。</strong>如图4所示，我们提供了各种方法的预测掩模。结果表明，该方法不仅能准确地定位篡改区域，而且能开发出清晰的边界。它受益于我们的模型能够明确地扩大两个区域之间的噪声差异，并保持边缘。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222847414.png"alt="image-20240502222847414" /><figcaption aria-hidden="true">image-20240502222847414</figcaption></figure><p>图4：通过不同的方法可视化预测的操作掩码。从上到下，我们展示了伪造的图像、GT面具、ManTraNet、SPAN、PSCC-Net、HiFi-Net和我们的预测。<br/>  <strong>噪声表示学习的可视化。</strong>我们在图5中展示了有和没有NRL的特征的变化。可见，NRL有助于对伪造特征的学习，并获得更准确的伪造区域轮廓。这是因为NRL帮助网络捕获噪声域中的篡改痕迹。</p><figure><imgsrc="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502222929983.png"alt="image-20240502222929983" /><figcaption aria-hidden="true">image-20240502222929983</figcaption></figure><p>图5：噪声表示学习和基于交叉注意的引导飞行器的可视化。从左到右，我们显示了没有（w/o）NRL和没有CAGF以及两者都有（NGNet）的特征地图的伪造图像、掩模（Selvaraju等人2017），以及预测。<br/>  <strong>基于交叉注意力的引导滤波器的可视化。</strong>为了验证CAGF的效果，我们在图5中展示了闪烁前后特征的变化。可见，CAGF可以提高伪造定位的精度。没有CAGF的网络将会对类似于伪造文件的物体做出错误的判断。<br/>  <strong>判别噪声表示的可视化。</strong>为了进一步验证我们的方法的动机和有效性，我们分别在图6中展示了提取的没有和有噪声表示学习（NRL）的噪声。可以看出，NRL获得了一个更有鉴别性的噪声表示，这是伪造信息的。</p><p><img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502223135944.png" alt="image-20240502223135944 " style="zoom:67%;" /></p><h1 id="结论">结论</h1><p>​  本文提出了一种包含噪声表示学习和噪声引导网络的两步噪声引导方案。第一步是明确地强调在真实区域和伪造区域之间的噪声分布的可辨别性。在第二步中，设计了一个定制的基于交叉注意力的引导滤波架构，结合了模型驱动和数据分割技术，以增强噪声不一致对RGB分支的引导效果，充分利用伪造噪声表示。我们的工作为解决模糊提取微妙的伪造痕迹的问题提供了一个新的研究策略。在几个基准上的大量实验结果证明了该方案的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BayarConv </tag>
            
            <tag> JS散度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技巧合集</title>
      <link href="/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/"/>
      <url>/%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p><a href="/MarkDown（一）/">MarkDown（一）：基本语法</a></p><p><a href="/MarkDown（二）/">MarkDown（二）：图表流程图</a></p><p><a href="/MarkDown（三）/">MarkDown（三）：公式语法</a></p><p><a href="/Hexo技巧/">Hexo技巧</a></p><p><a href="/typora技巧/">typora技巧</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization</title>
      <link href="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/"/>
      <url>/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/</url>
      
        <content type="html"><![CDATA[<center>MGQFormer: Mask-Guided Query-Based Transformer for Image ManipulationLocalization <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://dml.fudan.edu.cn/d1/65/c35285a643429/page.htm"><imgsrc="https://img.shields.io/badge/News-4096ff.svg" alt="new" /></a><br/></center><center><spanclass="math inline">\(\text{KunlunZeng}^*,\text{RiCheng}^*,\text{WeiminTan}^\dagger,\text{BoYan}^\dagger\)</span></center><center>复旦大学智能信息处理上海关键实验室计算机科学学院</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/MGQFormer.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  基于深度学习的模型在图像篡改定位方面取得了巨大的进展，其目标是区分被篡改和真实区域。然而，这些模型在训练效率上存在问题。这是因为它们主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。</p><p>  为了解决这个问题，我们提出了一个基于<strong>掩码引导查询</strong>的Transformer框架（MGQProtrer），它使用GroundTruth掩码来指导<strong>可学习查询token</strong>（LQT,learnable query token）识别伪造区域。</p><p>  具体地说，提取GroundTruth掩码的特征嵌入作为<strong>指导查询token</strong>（GQT,guiding querytoken），并将GQT和LQT分别输入到MGQFrorter中来估计篡改区域。然后，我们提出了一种掩模引导损失的位置和形状信息，以减少掩模在GQT和LQT之间的特征距离。我们还观察到，这种掩模引导的训练策略对MGQprort训练的收敛速度有显著影响。在多个基准测试上的大量实验表明，我们的方法显著地改进了最先进的方法。</p><h1 id="引言">引言</h1><p>  在训练过程中，我们使用多分支特征提取器从RGB输入图像中提取空间信道感知特征。它使用两个不同的转换器编码器分别从RGB输入图像及其噪声图中提取特征。然后，利用空间注意和通道注意来融合不同分布和域的RGB图像和噪声图特征。最后，将融合的特征输入到我们提出的基于查询的transformer解码器中，以输出伪造区域在图像中的位置。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164911763.png" alt="image-20240401164911763 " style="zoom:80%;" /></p><p>图1：以前的方法和我们的方法之间的区别。我们的方法使用了一个有效的和可解释的基于查询的Transformer。Token相似度是指图像特征与查询Token之间的标量乘积的softmax结果。此外，我们使用GroundTruth掩码来指导可学习的查询token（LQT）来识别真实的和伪造的区域。</p><p>  为了迫使LQT集中于伪造区域，提取GroundTruth掩模特征作为真实的伪造引导查询token（GQT），并将它们输入解码器，以估计伪造区域的位置。由于GQT来自于地面真实掩模，这是预测掩模的目标，因此GQT将包含伪造区域的空间位置和形状细节。因此，提出了一种掩模引导损耗来减小GQT和LQT之间的特征距离。模型经过训练后，LQT也使网络关注锻造区域的位置和形状。因此，我们只在推理过程中使用LQT来定位基于查询的transformer解码器中被篡改的区域。</p><p>  我们介绍了基于掩模引导的基于查询的Transformer，它包含一个基于查询的Transformer解码器，利用可学习的查询token（LQT）来定位被篡改的区域。</p><p>  我们提出了一种掩模引导训练方法，将从GT掩模中提取的引导查询token（GQT）作为参考LQT。此外，我们设计了掩模引导的损失，以迫使GQT引导LQT集中于被篡改区域的空间位置和形状细节。</p><p>  我们在多个基准测试上进行了广泛的实验，并证明了我们的方法在多个数据集上达到了最先进的性能。</p><h1 id="方法">方法</h1><p>  我们的方法旨在使用基于掩码引导的基于查询的Transformer（MGQprorter）来识别可疑图像中被篡改的区域。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png" alt="image-20240401164940206" style="zoom:50%;" /></p><p>图2：提出的框架MGQprorter的概述，由双分支transformer编码器、融合模块和掩模引导transformer解码器组成。在训练过程中，输入是一个可疑图像（H×W×3）和一个地面真实掩模，输出包括一个预测掩模和一个辅助掩模（H×W×1），这两者都涉及损失计算（Lloc和Laux）。请注意，红线部分是由掩模引导的训练，在推理过程中不需要进行训练。</p><p>  我们将输入图像表示为 $~ X \in R^{H×W×3} $，其中H和W分别为图像的高度和宽度。我们利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征。然后，通过空间和通道注意模块（SCAM,spatialand channel attention module）对多模态特征进行融合。</p><p>  我们设计了两个可学习的查询token（LQT）来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。</p><p>  具体来说，我们将噪声的GT掩模输入MGQFormer，以获得引导查询token（GQT）和辅助掩模$ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失$ L_{guide} $ 来减小LQT和GQT之间的距离。</p><h2 id="多分支特征提取器">多分支特征提取器</h2><p>  图像处理定位通常包含复杂的后处理，使得检测微小的差异和伪造痕迹对RGB域具有挑战性。因此，我们采用了一个双分支transformer编码器来完全利用来自两个域的信息。</p><p>  BayarConv 提取噪声特征 $~ X_n \in R^{H×W×3} $。然后将输入的图像和噪声图发送到Transformer编码器。具体地说，我们将X和 $X_n $ 划分为大小为P的补丁，并将补丁重塑为嵌入 $~ X_p \in R^{N×D} $，其中 $ N=HW/P^2 $ 为补丁的数量，D是嵌入的维数。将可学习的位置嵌入 $~pos \in R^{N×D} $ 添加到图像嵌入中，生成序列token $ Z = X_p + pos $，然后通过Transformer层L进行处理。对噪声分支也进行了上述相同的结算。在Transformer编码器之后，两个分支的输出被连接起来，我们得到$~ Z_c \in R^{N×2D} $，用于后续的融合。<br/><br/>  来自不同分支transformer编码器的token具有不同的域和不同的分布。因此，我们使用空间和通道注意模块（SCAM）来完成融合任务。<br/><br/>  我们首先重塑标记$ Z_c $ ，并使用一个卷积层得到 $~ Z_m \in R^{h×w×c} $ ，其中 $ h = H/P,w= W/P,c = D $ 。<br/><br/>  接下来，我们将 $ Z_m $ 和 $ Z_m $的转置分别定义为： <span class="math display">\[V=proj(Z_m) \inR^{hw×c}, K=proj(Z_m) \in R^{hw×c}, Q=transpose(proj(Z_m)) \inR^{c×hw}\]</span>   其中 $ proj $是一个独特的投影层，包括1×1卷积和重塑操作。然后，通道注意模块如下：<span class="math display">\[CAM(Z_m)=proj(V(softmax(QK)))\]</span>  同时，我们继续计算空间注意力，除了转置的Q和K，几乎与通道注意相同。随后，我们可以得到的token如下：<spanclass="math display">\[SAM(Z_m)=proj(softmax(Q^TK^T)V)\\Z_f=CAM(Z_m)+SAM(Z_m)+Z_m\]</span>  然后将图像特征令牌 $~ Z_f \in R^{N×D} $​发送到基于查询的Transformer解码器。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822164324909.png"alt="image-20240822164324909" /><figcaption aria-hidden="true">image-20240822164324909</figcaption></figure><h2 id="掩模transformer解码器">掩模Transformer解码器</h2><p>  我们首先介绍在引用阶段的解码器。</p><p>  对于所提出的基于查询的transformer解码器，我们使用了真实的和伪造的可学习查询令牌$~ LQT \in R^{2×D} $。这些查询被随机初始化，并表示伪造的和真实的特性。具体地说，图像特征token$ Z_f $和LQT由由n个基于Transformer层组成的解码器同时处理。在注意机制过程中，LQT与特征token$ Z_f $ 相互作用，提取丰富的伪造信息。然后，我们得到了图像特征 $ Z_f^∗ $和 $ LQT^∗ $ 。掩码的计算方法如下： <spanclass="math display">\[M^∗=norm(proj(Z_f^∗))∗(norm(proj(LQT^∗))^T\]</span>  其中proj是一个线性层，norm表示 $ L_2 $归一化，我们通过在参考图像特征和可学习查询标记之间执行标量乘积得到 $~M^∗ \in R^{N×2} $ 。<br/><br/>  为了得到最终的掩模，我们将序列重塑为掩模$~ M^{∗∗} \in R^{h×w×2} $ ，并以类的维度应用一个softmax层： <spanclass="math display">\[M=upsample(norm(softmax(M^{∗∗})))\]</span> 其中，$~ M \in R^{H×W} $为预测的掩模，upsample为双线性上采样操作，将掩模的大小调整为与输入图像相同的大小。</p><p>综上所述，我们基于查询的方法利用真实和伪造的LQT来选择与自身高度相似的区域，这使得预测伪造区域的过程更容易解释和有效。</p><figure><imgsrc="./../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240822170134582.png"alt="image-20240822170134582" /><figcaption aria-hidden="true">image-20240822170134582</figcaption></figure><h2 id="掩码引导的训练过程">掩码引导的训练过程</h2><p>  基于查询的模型在相应的任务中取得了很大的成功。然而，这些模型已经被证明存在查询回复的效率较低的问题。先前的方法已经提出了诸如去噪（Li等人2022a）和掩蔽注意（Cheng等人2022a）等方法。<br/><br/>  我们指出，以往的方法缺乏对伪造区域的位置和形状细节的LQT的直接监督，导致训练无效。这些方法主要利用交叉熵损失的地面真实掩模，优先考虑每像素的精度。<br/><br/>  为了解决这个问题，我们提出了一种掩模引导的训练策略，该策略使用引导查询令牌（GQT）来迫使LQT关注伪造区域的位置和形状。GQT通过提取噪声地面真掩模的特征得到，并利用辅助损失使GQT包含伪造区域的空间和形状信息。从而提高了MGQ前体训练的收敛速度。<br/><br/>  特别地，我们首先会向GroundTruth掩模添加噪声。这一步是因为从原始GroundTruth掩码预测辅助掩码对于transformer解码器和延迟训练来说可能太简单了。我们将点噪声应用于掩模中，类似于DN-DETR（Liet al.2022a）用于盒子去噪训练，以获得更鲁棒的模型。我们随机选择掩模内的点，并倒置原始值来表示不同的区域。此外，我们使用一个调优的参数µ来表示面积的噪声百分比，因此噪声点的数量为µ·HW。<br/><br/>  在噪声掩模的情况下，我们通过卷积网络将掩模转换为GQT，以保持掩模中的空间信息，并将GroundTruth$~ G \in R^{H×W} $ 转换为 $~ GQT \in R^{2×N} $ 。然后，GQT连同图像特征 $Z_f $和LQT一起被发送到Transformer解码器。在解码器中，GroundTruth信息GQT作为与其他查询交互的引导，并帮助解码器重构LQT。<br/><br/>  在Transformer解码器之后，我们得到了由GroundTruthtoken GQT引导的图像特征 $ Z_f^∗ $ 和查询令牌 $ LQT^∗ $ 和 $ GQT^∗ $。通过使用与掩模Transformer解码器部分相同的过程，对 $ Z_f^∗ $ 和 $ GQT^∗$ 进行标量乘积，进一步计算了辅助掩模 $~ M_{aux} \in R^{H×W} $。然后我们让 $ M_{aux} $ 参与到损失的计算中来。</p><h2 id="辅助损失">辅助损失</h2><p>  由于我们使用卷积网络将GroundTruth掩码转换为查询token，并且对掩模加噪声以保持鲁棒性，为了使辅助掩码更加精确，需要对卷积网络进行监督。因此，为了使GQT包含锻造区域的空间和形状信息，我们使用像素级交叉熵损失如下：</p><p><span class="math display">\[L_{aux}=−\sum_{i=1}^{HW}G_i\cdotlog(M_{auc},i)\]</span></p><p>  其中 $~ G \in R^{H×W} $​是GroundTruth掩模。注意，为了得到模型预测的精确掩模，我们使用没有噪声的原始GT掩模G来计算辅助损失。</p><h2 id="掩模引导的损失">掩模引导的损失</h2><p>  GQT的目的是引导LQT，并对两者进行相同的处理，生成预测的掩模M和辅助掩模<spanclass="math inline">\(M_{aux}\)</span>​。因此，我们期望LQT与GQT相似，从而使预测更加精确。我们采用余弦相似度损失来减少两个查询的距离，可以表述为：<span class="math display">\[L_{guide}=1-cos(LQT^*,GQT^*)\]</span>  其中cos为余弦相似度。</p><h2 id="损失函数">损失函数</h2><p>  总损失函数L包括三个部分：使<spanclass="math inline">\(M_{aux}\)</span>精确的辅助损失，使$ LQT^∗ $ 和 $GQT^∗ $ 更接近的掩模引导损失，以及预测掩模M的定位损失<spanclass="math inline">\(L_{loc}\)</span>，其中<spanclass="math inline">\(L_{loc}\)</span>​采用了和辅助损失相同的交叉熵损失：<span class="math display">\[L=L_{loc}+L_{aux}+ \lambdaL_{guide}\]</span></p><p>  其中，$ $是一个权重参数，在训练期间设置为0.5。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p>  <strong>测试数据集。</strong>我们首先使用PSCC-Net合成的数据集对我们的模型进行预训练（Liu等人，2022年）。然后，我们在CASIA数据集（Dong、Wang和Tan2013）、Columbia数据集（Hsu和Chang 2006）、NIST16数据集（Guan et al.2019）和IMD20数据集（诺沃扎姆斯基、马赫迪安和Saic2020）上评估我们的模型。特别地，CASIA提供拼接和复制移动图像，这广泛出现在图像伪造场中。Columbia数据集由180张拼接图像组成，它们未压缩，没有经过后处理。NIST16是一个具有挑战性的数据集，它有564张眼睛很难识别的高分辨率图像。IMD20收集了由不同的相机模型捕获的35,000张真实图像，并由不同的内部绘制方法生成的不同类型的操作组成。</p><p>  <strong>评估指标。</strong>为了评估所提出的MGQFrotar的定位性能，在PSCCNet（Liuet al.2022）之后，我们报告了图像级F1评分和曲线下面积（AUC）作为评价度量。我们采用fxed阈值对预测的掩模进行二值化，这是计算f1分数所必需的。</p><p>  <strong>实施细节。</strong>MGQFormer在一个NVIDIA GTX 1080 TiGPU上实现的。所有输入图像的大小都被调整为384×384。我们使用Adam作为优化器，学习率从2.5e-7衰减到1.5e-8，批处理大小为2。特征提取器使用ImageNet预训练的ViT模型权值（Steineret al.2021）初始化，共12层，补丁大小为16，而解码器使用来自6层的截断正态分布的随机权值初始化。</p><h2 id="与最先进的方法的比较">与最先进的方法的比较</h2><p>  我们在两种设置下，将我们的模型与其他最先进的方法进行了比较：</p><p>  1)在合成数据集上进行训练和在完整的测试数据集上进行评估。</p><p>  2)对测试数据集的训练分割和评估其训练分割的预训练模型进行微调。</p><p>  对于预先训练的模型，我们评估的方法有：ManTraNet (Wu, AbdAlmageed,and Natarajan 2019), SPAN (Hu et al. 2020), ObjectFormer (Wang et al.2022), 和ERMPC (Li et al. 2023)，同时进一步比较的方法有： RGB-N (Zhou etal. 2018) 和PSCCNet (Liu et al. 2022)。</p><p>  <strong>预先训练的模型。</strong>表1报告了使用预先训练过的模型获得的最佳定位AUC（%）分数。我们可以观察到，MGQFormer在Columbia、CASIA、IMD20和所有数据集的平均AUC（%）上取得了最高的性能，并在NIST16上获得了具有竞争力的效果。特别是，MGQFormer在现实世界的IMD20数据集上达到88.3%，比ERMPC高2.7%。这验证了我们的方法具有捕获篡改痕迹和泛化到高质量数据集的突出能力。在NIST16数据集上，我们未能达到最佳的性能。我们认为，Transformer网络的性能受到了训练决策的影响。如果测试时的分辨率接近训练，就可以完全实现高性能。然而，NIST16是一个高分辨率的数据集，它大大超过了我们的训练数据集。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185721944.png" alt="image-20240502185721944 " style="zoom:50%;" /></p><p>  <strong>微调模型。</strong>为了补偿合成数据集与标准数据集之间的视觉质量差异，使用预训练模型的网络权值来启动调整模型，该模型将在CASIA数据集的训练分割上进行训练。如表2所示，我们将AUC和F1结果（%）与其他方法进行了比较，我们的模型获得了最好的性能，表明MGQFormer可以通过查询有效地捕获细微的篡改伪。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502185818907.png" alt="image-20240502185818907 " style="zoom:40%;" /></p><h2 id="鲁棒性评估">鲁棒性评估</h2><p>  我们对Columbia数据集的原始图像应用不同的图像失真方法，并评估我们的MGQFormer的鲁棒性。失真类型包括：1)用不同的尺度调整图像的大小，2)用核大小k的高斯模糊，3)用质量因子q的JPEG压缩。我们比较了预训练模型在原始数据集和损坏数据上的操作定位性能（AUC分数），并报告了表3中的结果。与以往的方法相比，MGQFormer对所有失真具有最好的鲁棒性。特别是当面对调整大小和JPEG压缩时，我们的方法的性能略有下降，表示补丁MGQFormer对低质量图像具有鲁棒性。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165650581.png"alt="image-20240401165650581" /><figcaption aria-hidden="true">image-20240401165650581</figcaption></figure><h2 id="消融实验">消融实验</h2><p>  MGQFormer的设计包含多分支特征提取器和掩模引导训练。该多分支特征提取器采用了一个额外的BayarConv分支来利用噪声信息，并利用SCAM融合这两个域。利用掩模引导的训练来添加基础GroundTruth信息，引导LQT专注于目标区域，提高查询回复的效率。</p><p>  <strong>噪声分支的消融研究。</strong>定量结果见表4。基线表示我们只使用单个编码器和基于查询的转换器解码器。为了评估噪声分支的有效性，我们使用了一个RGB分支并去除SCAM。我们可以观察到，如果没有噪声分支，哥伦比亚大学的AUC评分下降了1.1%，CASIA大学的AUC评分下降了2.3%。性能提升验证了多分支特征提取器的使用有效地提高了模型的性能。</p><p>  <strong>掩模引导训练的消融研究。</strong>为了证明掩模引导训练的影响，我们在带有图像特征的transformer解码器中只留下LQT，并在训练过程中取出GroundTruth掩模的输入。如表4所示，在没有面具引导训练的情况下，哥伦比亚大学组的AUC评分下降了2.8%，CASIA组下降了3.6%。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165731530.png"alt="image-20240401165731530" /><figcaption aria-hidden="true">image-20240401165731530</figcaption></figure><p>  除了促进定位外，掩模引导训练进一步提高了收敛速度。为了评估这种效果，我们比较了不同时期训练策略的存在和不存在的结果。如图3所示，我们在训练期间显示了合成数据集的验证分割上的AUC（%）分数。事实证明，MGQFrorer在开始时显著促进了训练，在frst时期比没有面具引导训练的模型多了12.7%，并显著加快了收敛速度。这表明，GQT当然有助于transformer解码器提高重构LQT的效率。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165639502.png"alt="image-20240401165639502" /><figcaption aria-hidden="true">image-20240401165639502</figcaption></figure><p>  <strong>GT引导掩膜对应用噪声与否的消融研究。</strong>在图4中，我们展示了参数µ的不同值，表示噪声点的百分比，以验证其对哥伦比亚和IMD20的影响。随着地面真实掩模的增加，有更多的噪声点，得到更鲁棒和广义的模型；然而，较大的值可能会对空间信息造成损害，误导网络。相比之下，较小的µ值提供了一个更准确的地面真实掩模，但模型可能太容易预测辅助掩模和延迟训练。从比较中可以看出，设置为0.01是最优解。点噪声的使用达到了0.9%/1.2%的AUC增益，如表4所示。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401165703374.png"alt="image-20240401165703374" /><figcaption aria-hidden="true">image-20240401165703374</figcaption></figure><h1 id="可视化结果">可视化结果</h1><p>  <strong>定性的结果。</strong>如图5所示，我们提供了各种方法的预测伪造掩模。可以观察到PSCC-Net和ManTraNet要么输出错误的区域，要么做出不明确的预测。对可视化结果的比较表明，该方法不仅可以更准确地定位篡改区域，而且还可以输出清晰的区域。它受益于多模态信息和基于查询的transformer解码器，它使用全局注意来生成掩码。</p><figure><imgsrc="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191008224.png"alt="image-20240502191008224" /><figcaption aria-hidden="true">image-20240502191008224</figcaption></figure><p>  <strong>掩膜引导训练的可视化。</strong>为了验证掩模引导训练的有效性，我们展示了MGQFormer预测的掩模，未掩模引导训练生成的掩模，以及图6中的辅助掩模。很明显，MGQFrorer利用地面真实掩模关注伪造区域，从预测掩模与辅助掩模之间的相似性可以看出。具体来说，没有掩模引导训练的网络会对相对较小的物体做出错误的判断。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191142614.png" alt="image-20240502191142614 " style="zoom:70%;" /></p><p>  在图7中，我们进一步展示了表示MGQFormertransformer解码器伪造的LQT注意图与未经掩模引导训练的注意图之间的差异。很明显，在掩膜引导训练中，由于GQT的引导，LQT可以准确地聚焦于目标区域。相比之下，没有掩模引导训练的LQT不能很好地检测伪造，甚至被分配到代表真实位置的完全相反的区域。这一比较表明，所提出的包含来自GT掩模的空间和形状信息的GQT可以迫使LQT集中于我们分配给LQT的正确区域类型。</p><p><img src="../postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240502191302621.png" alt="image-20240502191302621 " style="zoom:67%;" /></p><h1 id="结论">结论</h1><p>  在本文中，我们提出了一种新的基于掩模引导的Transformer框架（MGQFormer）。具体来说，第一步，提取RGB和噪声特征，并进一步融合它们。第二步，将噪声GroundTruth掩码转换为引导查询token（GQT），并将GQT和LQT输入MGQFormer分别估计篡改区域。我们进一步提出了辅助损失和掩模引导损失来指导LQT的重建。可视化结果表明，所提出的掩模引导训练策略对MGQ训练的收敛速度和定位性能有显著影响。在几个基准上的大量实验结果证明了我们的算法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BayarConv </tag>
            
            <tag> Mask-Guided </tag>
            
            <tag> Query-Based </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization2</title>
      <link href="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/"/>
      <url>/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/</url>
      
        <content type="html"><![CDATA[<p>Exploring Multi-Modal Fusion for Image Manipulation Detection andLocalization <a href="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></p><details close><br/><summary>论文（arxiv）</summary><br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">post1</a><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/">post2</a><br/><div class="row">    <embed src="/postpdfs/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/2312.01790.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h2 id="数据集">数据集</h2><p>下载 train 数据集:</p><ul><li><ahref="https://github.com/namtpham/casia2groundtruth">Casiav2</a></li><li><ahref="https://github.com/mjkwon2021/CAT-Net#1-downloading-tampcoco--compraise">tampCOCO</a></li><li><a href="http://staff.utia.cas.cz/novozada/db/">IMD2020</a></li><li><ahref="http://zefirus.org/articles/9f78c1e9-8652-4392-9199-df1b6a6c1a3d/">FantasticReality</a></li></ul><p>下载 test 数据:</p><ul><li><ahref="https://github.com/namtpham/casia1groundtruth">Casiav1</a></li><li><ahref="https://www.kaggle.com/datasets/elkamel/corel-images">corel</a></li><li><ahref="https://github.com/grip-unina/TruFor#cocoglide-dataset">CocoGlide</a></li><li><ahref="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/">Columbia</a></li><li><a href="https://github.com/wenbihan/coverage">COVER</a></li><li><ahref="https://recodbr.wordpress.com/code-n-data/#dso1_dsi1">DSO-1</a></li></ul><p>创建Casiav1+数据集需要corel数据集。</p><p>测试集的数据列表分为操纵图像和真实图像，并存在于名为的文件中：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./data/IDT-&lt;DATASET_NAME&gt;-manip.txt</span><br><span class="line">./data/IDT-&lt;DATASET_NAME&gt;-auth.txt</span><br></pre></td></tr></table></figure><p>这是为了在评估本地化时易于使用，因为您只使用操纵的图像（真实图像的本地化F1始终为0！）。</p><p>为了便于使用和再现，训练的数据列表分为训练文件和val文件。我们使用Kwon等人在<ahref="https://github.com/mjkwon2021/CAT-Net">CAT-Net</a>中提出的列车验证划分.列车数据列表位于名为的文件中：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./data/CAT-Net_splits/train/&lt;DATASET_NAME&gt;.txt</span><br><span class="line">./data/CAT-Net_splits/val/&lt;DATASET_NAME&gt;.txt</span><br></pre></td></tr></table></figure><p>对于我们的实验，我们在训练数据集的验证分割上进行了验证，因此测试数据集在评估之前完全不可见。</p><h3 id="数据文件夹结构">数据文件夹结构</h3><p>然后，您应该将数据集放在数据目录中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data/</span><br><span class="line">├── Casiav1</span><br><span class="line">│   ├── Au</span><br><span class="line">│   ├── Gt</span><br><span class="line">│   └── Tp</span><br><span class="line">├── Casiav2</span><br><span class="line">│   ├── Au</span><br><span class="line">│   ├── mask</span><br><span class="line">│   └── tampered</span><br><span class="line">├── CocoGlide</span><br><span class="line">│   ├── fake</span><br><span class="line">│   ├── mask</span><br><span class="line">│   └── real</span><br><span class="line">├── Columbia</span><br><span class="line">│   ├── 4cam_auth</span><br><span class="line">│   └── 4cam_splc</span><br><span class="line">├── compRAISE</span><br><span class="line">│   └── &lt;all images here&gt;</span><br><span class="line">├── corel-1k</span><br><span class="line">│   ├── test_set</span><br><span class="line">│   └── training_set</span><br><span class="line">├── COVER</span><br><span class="line">│   ├── Au</span><br><span class="line">│   ├── mask</span><br><span class="line">│   └── tampered</span><br><span class="line">├── DSO-1</span><br><span class="line">│   ├── images</span><br><span class="line">│   └── masks</span><br><span class="line">├── FantasticReality</span><br><span class="line">│   ├── ColorFakeImages</span><br><span class="line">│   ├── ColorRealImages</span><br><span class="line">│   └── masks</span><br><span class="line">├── IMD2020</span><br><span class="line">│   ├── 1a1ogs</span><br><span class="line">│   ├── 1a3oag</span><br><span class="line">│       .</span><br><span class="line">│       .</span><br><span class="line">│       .</span><br><span class="line">│   └── z41</span><br><span class="line">└── tampCOCO</span><br><span class="line">    └── &lt;all images here&gt;</span><br></pre></td></tr></table></figure><h2 id="训练">训练</h2><h3 id="准备">准备</h3><p>在培训之前，您需要按照说明<ahref="https://github.com/IDT-ITI/MMFusion-IML/blob/main/pretrained/README.md">此处</a>下载经过预培训的网络并将它们放在/pretrained目录中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pretrained/</span><br><span class="line">├── segformer</span><br><span class="line">├── noiseprint</span><br><span class="line">└── modal_extractor</span><br></pre></td></tr></table></figure><h3 id="实验设置">实验设置</h3><p>在训练之前，您需要创建一个实验文件，并将其放在“experiments”文件夹中。此yaml文件包含用于训练的参数。要使用我们的训练设置训练模型，您可以使用提供的ec_example.yaml文件。</p><p>但是，您可以按以下方式更改训练参数：</p><ul><li>如果需要，您可以更改学习参数或进行更多时期的训练：</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">WORKERS: 16</span><br><span class="line">ACCUMULATE_ITERS: 6</span><br><span class="line">BATCH_SIZE: 4</span><br><span class="line">WARMUP_EPOCHS: 2</span><br><span class="line">EPOCHS: 100</span><br></pre></td></tr></table></figure><ul><li>您可以更改优化器或调度程序参数：:</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LEARNING_RATE: 0.005</span><br><span class="line">SGD_MOMENTUM: 0.9</span><br><span class="line">WD: 0.0005</span><br><span class="line">POLY_POWER: 0.9</span><br></pre></td></tr></table></figure><ul><li>您可以更改训练或验证数据集:</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DATASET:</span><br><span class="line">  TRAIN:</span><br><span class="line">    - &#x27;data-list-1&#x27;</span><br><span class="line">    - ...</span><br><span class="line">    - &#x27;data-list-N&#x27;</span><br><span class="line">  VAL:</span><br><span class="line">    - &#x27;val-data-list-1&#x27;</span><br><span class="line">    - ...</span><br><span class="line">    - &#x27;val-data-list-N&#x27;</span><br></pre></td></tr></table></figure><h3 id="localization训练">Localization训练</h3><p>可以通过以下方式运行示例培训：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">example_train.sh</span><br></pre></td></tr></table></figure><p>您可以通过在/experiences目录中创建一个新的实验yaml文件来更改训练参数。检查点另存为:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ckpt/</span><br><span class="line">└── &lt;model_name&gt;</span><br><span class="line">    ├── best_val_loss.pth</span><br><span class="line">    └── final.pth</span><br></pre></td></tr></table></figure><p>model_name参数在实验yaml文件中设置为:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MODEL:</span><br><span class="line">  NAME: &lt;model_name&gt;</span><br></pre></td></tr></table></figure><h3 id="detection训练">Detection训练</h3><p>要运行detection训练（第2阶段），需要在chekpoints文件夹中放置一个本地化检查点（由第1阶段训练生成）。<br/>如果您想使用我们的本地化检查点之一，您可以按照说明<ahref="https://github.com/IDT-ITI/MMFusion-IML/blob/main/ckpt/README.md">此处</a>下载它们.</p><p>第二阶段训练的实验文件应设置为检测：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MODEL:</span><br><span class="line">  TRAIN_PHASE: &#x27;detection&#x27;</span><br></pre></td></tr></table></figure><p>然后，可以在我们的测试数据集上训练检测和评估模型，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source data.sh</span><br><span class="line">exp=&#x27;./experiments/ec_example_phase2.yaml&#x27;</span><br><span class="line">ckpt_loc=&#x27;./ckpt/&lt;path_to_localization_ckpt&gt;&#x27;</span><br><span class="line">ckpt=&#x27;./ckpt/&lt;model_name&gt;/best_val_loss.pth&#x27;</span><br><span class="line">$pint ec_train_phase2.py --ckpt $ckpt_loc --exp $exp</span><br><span class="line"></span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $columbia_manip --auth $columbia_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $cover_manip --auth $cover_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $dso1_manip --auth $dso1_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $cocoglide_manip --auth $cocoglide_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $casiav1_manip --auth $casiav1_auth</span><br></pre></td></tr></table></figure><h2 id="评估">评估</h2><p>您可以按照说明<ahref="https://github.com/IDT-ITI/MMFusion-IML/blob/main/ckpt/README.md">此处</a>下载我们经过预训练的网络并将它们放在/ckpt目录中。然后，您可以使用以下方法评估模型以进行本地化：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">example_test.sh</span><br></pre></td></tr></table></figure><p>以及改变相关参数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source data.sh</span><br><span class="line">exp=&#x27;./experiments/ec_example.yaml&#x27;</span><br><span class="line">ckpt=&#x27;./ckpt/&lt;model_name&gt;/best_val_loss.pth&#x27;</span><br><span class="line"></span><br><span class="line">$pint test_localization.py --exp $exp --ckpt $ckpt --manip $columbia_manip</span><br><span class="line">$pint test_localization.py --exp $exp --ckpt $ckpt --manip $cover_manip</span><br><span class="line">$pint test_localization.py --exp $exp --ckpt $ckpt --manip $dso1_manip</span><br><span class="line">$pint test_localization.py --exp $exp --ckpt $ckpt --manip $cocoglide_manip</span><br><span class="line">$pint test_localization.py --exp $exp --ckpt $ckpt --manip $casiav1_manip</span><br></pre></td></tr></table></figure><p>以同样的方式，可以评估用于检测的模型：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source data.sh</span><br><span class="line">exp=&#x27;./experiments/ec_example_phase2.yaml&#x27;</span><br><span class="line">ckpt=&#x27;./ckpt/&lt;model_name&gt;/best_val_loss.pth&#x27;</span><br><span class="line"></span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $columbia_manip --auth $columbia_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $cover_manip --auth $cover_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $dso1_manip --auth $dso1_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $cocoglide_manip --auth $cocoglide_auth</span><br><span class="line">$pint test_detection.py --exp $exp --ckpt $ckpt --manip $casiav1_manip --auth $casiav1_auth</span><br></pre></td></tr></table></figure><h2 id="鸣谢">鸣谢</h2><p>这项工作得到了欧盟地平线2020研究和创新计划的支持，该计划的赠款协议为H2020-101021866CRiTERIA。</p><p>感谢public repositories：</p><ul><li><a href="https://github.com/jamycheung/DELIVER">DELIVER</a></li><li><a href="https://github.com/grip-unina/TruFor">TruFor</a></li><li><a href="https://github.com/mjkwon2021/CAT-Net">CAT-Net</a></li></ul><h2 id="引用">引用</h2><p>如果您发现我们的方法在您的工作中有用，或者您使用了本回购中提供的一些材料，请引用以下介绍我们的方法和材料的出版物：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;triaridis2024exploring,</span><br><span class="line">    title=&#123;Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization&#125;,</span><br><span class="line">    author=&#123;Triaridis, Konstantinos and Mezaris, Vasileios&#125;,</span><br><span class="line">    year=&#123;2024&#125;,</span><br><span class="line">    month=&#123;Jan.-Feb.&#125;,</span><br><span class="line">    booktitle=&#123;Proc. 30th Int. Conf. on MultiMedia Modeling (MMM 2024)&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NP++ </tag>
            
            <tag> SRM卷积 </tag>
            
            <tag> BayarConv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Transferable Visual Models From Natural Language Supervision2</title>
      <link href="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision2/"/>
      <url>/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision2/</url>
      
        <content type="html"><![CDATA[<p>在 CUDA GPU 上, 运行如下指令:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0</span><br><span class="line">$ pip install ftfy regex tqdm</span><br><span class="line">$ pip install git+https://github.com/openai/CLIP.git</span><br></pre></td></tr></table></figure><p><strong>Zero-Shot Prediction：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import clip</span><br><span class="line">import torch</span><br><span class="line">from torchvision.datasets import CIFAR100</span><br><span class="line"></span><br><span class="line"># Load the model</span><br><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line">model, preprocess = clip.load(&#x27;ViT-B/32&#x27;, device)</span><br><span class="line"></span><br><span class="line"># Download the dataset</span><br><span class="line">cifar100 = CIFAR100(root=os.path.expanduser(&quot;~/.cache&quot;), download=True, train=False)</span><br><span class="line"></span><br><span class="line"># Prepare the inputs</span><br><span class="line">image, class_id = cifar100[3637]</span><br><span class="line">image_input = preprocess(image).unsqueeze(0).to(device)</span><br><span class="line">text_inputs = torch.cat([clip.tokenize(f&quot;a photo of a &#123;c&#125;&quot;) for c in cifar100.classes]).to(device)</span><br><span class="line"></span><br><span class="line"># Calculate features</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    image_features = model.encode_image(image_input)</span><br><span class="line">    text_features = model.encode_text(text_inputs)</span><br><span class="line"></span><br><span class="line"># Pick the top 5 most similar labels for the image</span><br><span class="line">image_features /= image_features.norm(dim=-1, keepdim=True)</span><br><span class="line">text_features /= text_features.norm(dim=-1, keepdim=True)</span><br><span class="line">similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)</span><br><span class="line">values, indices = similarity[0].topk(5)</span><br><span class="line"></span><br><span class="line"># Print the result</span><br><span class="line">print(&quot;\nTop predictions:\n&quot;)</span><br><span class="line">for value, index in zip(values, indices):</span><br><span class="line">    print(f&quot;&#123;cifar100.classes[index]:&gt;16s&#125;: &#123;100 * value.item():.2f&#125;%&quot;)</span><br></pre></td></tr></table></figure><h5 id="linear-probe-evaluation">Linear-probe evaluation：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import clip</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision.datasets import CIFAR100</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line"># Load the model</span><br><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line">model, preprocess = clip.load(&#x27;ViT-B/32&#x27;, device)</span><br><span class="line"></span><br><span class="line"># Load the dataset</span><br><span class="line">root = os.path.expanduser(&quot;~/.cache&quot;)</span><br><span class="line">train = CIFAR100(root, download=True, train=True, transform=preprocess)</span><br><span class="line">test = CIFAR100(root, download=True, train=False, transform=preprocess)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_features(dataset):</span><br><span class="line">    all_features = []</span><br><span class="line">    all_labels = []</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):</span><br><span class="line">            features = model.encode_image(images.to(device))</span><br><span class="line"></span><br><span class="line">            all_features.append(features)</span><br><span class="line">            all_labels.append(labels)</span><br><span class="line"></span><br><span class="line">    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()</span><br><span class="line"></span><br><span class="line"># Calculate the image features</span><br><span class="line">train_features, train_labels = get_features(train)</span><br><span class="line">test_features, test_labels = get_features(test)</span><br><span class="line"></span><br><span class="line"># Perform logistic regression</span><br><span class="line">classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)</span><br><span class="line">classifier.fit(train_features, train_labels)</span><br><span class="line"></span><br><span class="line"># Evaluate using the logistic regression classifier</span><br><span class="line">predictions = classifier.predict(test_features)</span><br><span class="line">accuracy = np.mean((test_labels == predictions).astype(float)) * 100.</span><br><span class="line">print(f&quot;Accuracy = &#123;accuracy:.3f&#125;&quot;)</span><br></pre></td></tr></table></figure><p>其中train 的图片是3<em>224</em>224</p><blockquote><p>train_features是500<em>100</em>512，train_labels是500<em>100<br/>&gt;test_features是100</em>100<em>512，test_labels是100</em>100</p></blockquote><p>LogisticRegression方法：</p><p>使用场景</p><p>  逻辑回归一般使用于分类场景，可以使用参数让普通的二元分类问题变成多分类问题。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> baseline </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Transferable Visual Models From Natural Language Supervision</title>
      <link href="/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/"/>
      <url>/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/</url>
      
        <content type="html"><![CDATA[<p>Learning Transferable Visual Models From Natural LanguageSupervision<a href="https://arxiv.org/abs//2103.00020"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/OpenAI/CLIP"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></p><p><strong>官方解读博客：</strong></p><p><a href="https://openai.com/research/clip">CLIP: Connecting text andimages (openai.com)</a></p><h2 id="clip-论文解读"><strong>1 CLIP 论文解读：</strong></h2><h3 id="背景和动机"><strong>1.1 背景和动机</strong></h3><p><strong>[借助文本的监督方法属于有监督和无监督的一个中间地带]</strong>借助文本的监督方法属于：”借助有限的标注数据进行有监督训练” 和“借助几乎无限量的原始文本进行无监督训练”二者之间的中间地带。相同的是，这两种方式都使用静态的 Softmax分类器来执行预测，缺乏动态输出的机制。这严重限制了它们的灵活性和“Zero-Shot” 能力。</p><p><strong>[CLIP 方法及其结果]</strong>在本文中作者研究了借助大规模自然语言监督训练图像分类器。互联网上存在大量公开可用的无标注文本数据集，作者创建了一个包含4亿对(图像，文本) 的新数据集，并通过对比语言-图像预训练的方式训练了 CLIP模型，是一种从自然语言监督中学习视觉模型的有效新方法。作者发现 CLIP类似于 GPT家族，在预训练期间学习执行一系列任务，包括动作识别，OCR，地理定位，ImageNet-1K图像分类，细粒度图像分类任务等。作者通过在30多个现有数据集上对 CLIP 的“Zero-Shot” 迁移学习性能进行测试，并发现 CLIP可以与有监督训练得到的模型性能相当。比如，CLIP 在 ImageNet-1K上的性能与专门有监督训练的 ResNet-50 相当，但是却没有使用 1.28M 的ImageNet-1K 训练数据集。</p><h3 id="自然语言的监督"><strong>1.2 自然语言的监督</strong></h3><p>本文方法的核心是从自然语言的监督中获得感知能力。只要是你的方法具备这一特点，都可以称之为“接受了自然语言的监督”。那这种方法有哪些优势呢？其一就是可扩展性。因为它不需要经典机器学习方法中大量的有标签数据。</p><h3 id="clip-的数据集"><strong>1.3 CLIP 的数据集</strong></h3><p>本文的一个主要特点是想利用互联网上大量公开可用的数据。由于现有的数据集(MS-COCO 约100,000张，YFCC100M 高质量的仅仅约 15M 张，和 ImageNet-1K大小相似) 不够大，可能会低估这一研究领域的潜力。</p><p>为了解决这个问题，作者构建了一个新的数据集，其中包含4亿对(图像，文本)对，这些数据来自互联网上各种公开可用的资源。而且这个数据清理得非常好，质量是非常高的，这也可能是CLIP 这么强大的主要原因之一。结果数据集的总字数与用于训练 GPT-2 的WebText 数据集相似，因此作者将此数据集称为 WebImageText (WIT)。</p><h3 id="clip-的预训练方法"><strong>1.4 CLIP 的预训练方法</strong></h3><p>本文采取基于对比学习的高效预训练方法。作者的思路是这样的：一开始的方法是联合训练了一个处理图像的CNN 和一个处理文本的 Transformer 模型，来预测图像的caption。这个实验结果如下图1的蓝色曲线所示，可以看到其 Scalability是很差的。橘红色曲线是预测文本的词袋，其效率是蓝色曲线的3倍。这两种方法都有一个关键的相似性，即试图去预测每幅图片对应的文字的确切单词是什么。但我们知道这可不是一件容易的事，因为与同一幅图像对应的描述、注释和相关文本种类繁多。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-25eb6d56491be12552c88613e2a0a3d1_720w.png"alt="img" /></p><p>图1：不同方法的 Zero-Shot ImageNet-1K 精度</p><p>基于最近的图像对比表征学习方面的研究，可以仅预测整个文本与哪个图像配对，而不是该文本的确切单词，实验结果如下图1的绿色曲线所示，其效率是橘红色曲线的4倍。具体的做法是：</p><p><strong>对比学习阶段：</strong><br/>如下图2所示，给定一个 Batch 的N个(图片，文本) 对，图片输入给 Image Encoder 得到表征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，文本输入给 Text Encoder得到表征<span class="math inline">\(I_1\)</span>,<spanclass="math inline">\(I_2\)</span>,…,<spanclass="math inline">\(I_N\)</span>，作者认为<spanclass="math inline">\((I_j,T_j)\)</span>属于是正样本，<spanclass="math inline">\((I_i,T_j)\)</span>属于负样本。最大化N个正样本的Cosine 相似度，最小化<spanclass="math inline">\(N^2-N\)</span>个负样本的 Cosine 相似度。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-65e1dbb935aa7804189dc100783a4940_720w.png"alt="img" />]</p><p>图2：CLIP 的对比学习阶段</p><p>作者从头开始训练 CLIP，不使用 ImageNet-1K 权重初始化 ImageEncoder，也不使用预先训练的权重初始化 Text Encoder。同时使用线性投影(权重为<span class="math inline">\(W_i,W_t\)</span>)将每个编码器的表征映射到多模态的嵌入空间。数据增强只使用随机裁剪，温度系数<spanclass="math inline">\(\tau\)</span>的对数形式随整个模型一起训练。</p><p><strong>Zero-Shot Transfer：</strong>如下图4所示，这个阶段是使用 CLIP的预训练好的 Image Encoder 和 Text Encoder 来做 Zero-ShotTransfer。比如来一张 ImageNet-1K 验证集的图片，我们希望 CLIP预训练好的模型能完成这个分类的任务。但是你想想看，这个 Image Encoder是没有分类头 (最后的 Classifier)的，也就是说它没法直接去做分类任务，所以说呢 CLIP 采用了下面的 PromptTemplate 模式：</p><p>比如来一张 ImageNet-1K 验证集的图片，作者把它喂入 CLIP 预训练好的Image Encoder，得到特征 <spanclass="math inline">\(I_1\)</span>，接下来把所有类别的词汇 “cat”, “dog”等，做成一个 prompt：”A photo of a {object}”，并将这个 prompt 喂入 CLIP预训练好的 Text Encoder，依次得到特征<spanclass="math inline">\(T_1\)</span>,<spanclass="math inline">\(T_2\)</span>,…,<spanclass="math inline">\(T_N\)</span>，最后看<strong>哪个的余弦相似度和<spanclass="math inline">\(I_1\)</span>最高</strong>，就代表<strong>该图片是哪个类别的</strong>。</p><p>那我们就可以注意到貌似这个 prompt 的加入很关键，正好弥补了 ImageEncoder 没有分类头的问题，又正好用上了 CLIP 训练好的 Text Encoder。</p><p>而且重要的是，CLIP 的这种推理的方法摆脱了类别的限制，比如一张“三轮车” 的图片，假设 ImageNet 里面没有 “三轮车” 这个类，那么基于ImageNet 所训练的任何模型都无法正确地讲这个图片分类为 “三轮车” ，但是CLIP 的范式是可以做到的，只需要去做成一个 prompt：”A photo of a{tricycle}”。</p><p>那么我们不禁要问：<strong>其他任务可以像这样使用 prompt吗？或者什么样的 prompt 可以带来 Zero-Shot的性能提升？</strong>作者做了实验发现：</p><ul><li><p>对于细粒度图像分类任务，比如 Oxford-IIIT Pets 数据集，prompt就可以设置为：”A photo of a {label}, a type of pet.”。比如 Food101数据集，prompt 就可以设置为：”A photo of a {label}, a type offood.”。比如 FGVC Aircraft 数据集，prompt 就可以设置为：”A photo of a{label}, a type of aircraft.”</p></li><li><p>对于 OCR 任务，加上一些文本或者数字的引号可以提升性能。</p></li><li><p>对于卫星图像分类数据集，prompt 就可以设置为：”a satellite photoof a {label}.”。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2e0561dbbcf5ae00c5053824d2148c25_720w.png"alt="img" />]</p><p>图4：CLIP 的 Zero-Shot Transfer</p></li></ul><p>作者还开脑洞尝试了通过使用多个上下文的 prompt 来 Ensemble 多个Zero-Shot 分类器，比如一个 prompt 是 ‘A photo of a big {label}”，另一个prompt 是 ‘A photo of a small{label}”。作者观察到这样可以可靠地提高性能。在 ImageNet 上，作者集成了80 个不同的上下文提示，这比上面讨论的单个默认提示提高了 3.5%的性能。当一起考虑时，如下图5所示是 Prompt 工程和 Ensemble策略如何改变一组 CLIP 模型的性能，可以看到 Prompt 工程和 Ensemble 策略将ImageNet 精度提高了近 5%，其中蓝色的线代表直接嵌入类名的结果。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-12b0f33645fcb5ffb3fdcea97a2bc0b6_720w.png"alt="img" />]</p><p>图5：prompt 工程和 Ensemble 对 Zero-Shot 性能的影响</p><h3 id="clip-的模型选择"><strong>1.5 CLIP 的模型选择</strong></h3><p>对于 Image Encoder，作者尝试了改进版的 ResNet-50 和 ViT，对于 TextEncoder，作者使用改进版的 Transformer，作者使用了一个带有8个注意头的 63M参数的12层512宽 Transformer 模型，其输入是一个大小为49152的词汇表的BPE[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_2">2]</a>小写表征。为了计算效率，最大序列长度为76。文本序列用[SOS] 和 [EOS] 令牌括起来，[EOS] 处 Transformer末层的输出被视为文本的特征，然后通过 LN，后接 Linear层投影到多模态空间中。</p><p>至于模型缩放的问题，作者发现对于图像编码器ResNet，同时缩放其深度，宽度，和输入分辨率的效果是最优的。而对于文本编码器Transformer，作者只缩放模型的宽度，使其与 ResNet宽度的计算增量成正比，而无需缩放深度，因为作者发现 CLIP的性能对文本编码器的容量不太敏感。</p><h3 id="零样本迁移-zero-shot-transfer-实验结果"><strong>1.6 零样本迁移(Zero-Shot Transfer) 实验结果</strong></h3><p>本节中的 Zero-Shot是指研究对未见过的数据集的泛化性能，也就是说一个模型训练号以后，在它从未见到过的新数据集上的性能如何。</p><p>作者进一步探索 CLIP 的 Zero-Shot 性能。为了说明这一点，作者比较了CLIP 与基于 ResNet-50完全监督的、正则化的逻辑回归分类器的性能。实验结果如下图7所示，在一共对比的27个数据集中，Zero-ShotCLIP 在16个数据集上面战胜了全监督的 ResNet-50 模型。</p><p>在细粒度分类任务上，可以观察到性能上的广泛差异。在其中两个数据集(Stanford Cars 和 Food101) 上，Zero-Shot CLIP 在 ResNet-50特征上的表现比逻辑回归好 20% 以上，而在另外两个数据集 (Flowers102 和FGVCAircraft) 上，Zero-Shot CLIP 的表现比逻辑回归差 10% 以上。在OxfordPets 和 Birdsnap 上，二者的表现更为接近。</p><p>在 ImageNet、CIFAR10/100、STL10 和 PascalVOC2007 等 “更广义的”分类数据集上，二者的性能相对相似，在所有情况下，Zero-Shot CLIP都有轻微的优势。在 STL10 上，CLIP 在不使用任何训练样本的情况下达到了99.3% 的精度。在 Kinetics70 上，CLIP的表现比ResNet-50高出14.5%，在UCF101 上，Zero-Shot CLIP 的性能也比 ResNet-50 的性能高出7.7%。作者推测这估计是因为与 ImageNet中以名词为中心的对象监督相比，自然语言对涉及动词的视觉概念提供了更广泛的监督。</p><p>也可以看到，Zero-Shot CLIP在一些专业、复杂或抽象的任务上相当弱，如卫星图像分类 (EuroSAT和RESISC45)、淋巴结肿瘤检测 (PatchCamelyon)、合成场景中的物体计数(CLEVRCounts)、与自动驾驶相关的任务，如德国交通标志识别(GTSRB)、识别到最近汽车的距离 (KITTI distance)。这些结果突出了 Zero-ShotCLIP 在更复杂任务中的较差能力。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-9f59b5dfeae10e475d35e3a2b715b0ba_720w.png"alt="img" />]</p><p>图7：Zero-Shot CLIP 与完全监督的基线相比具有竞争力</p><blockquote><p><strong>CLIP 零样本迁移的 Data Efficiency</strong></p></blockquote><p>除此之外，作者还进行了一个有趣的实验，即探究 CLIP的零样本迁移的性能与其他模型的少样本学习性能的比较。这里的其他模型，作者使用的是ImageNet-21K 数据集上面预训练的 BiT-MResNet-152x2。如下图8所示的结果是零样本迁移 (Zero-Shot Transfer) 的 dataefficiency，即少样本学习 (Few-Shot Learning)在样本量为多少时的性能能够跟上 CLIP零样本迁移的性能。可以发现每个数据集的效率差异很大，从有的数据集不到一个标记到有的数据集需要184个标记。比如，Flowers102数据集可以在 1-shot 的情况下就能够跟上 CLIP 零样本迁移的性能，但是像FER2013 数据集在 184-shot 的情况下才能够做得到。平均估计数据效率为每个类20.8 个示例。对于 ImageNet 数据集，CLIP零样本迁移的结果与在相同特征空间上训练的 16-shot线性分类器的结果相当。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-f0b49427382cb9fbf87dbc6ae3eec46d_720w.png"alt="img" />]</p><p>图8：CLIP Zero-Shot Transfer 的 data efficiency</p><h3 id="表征学习-representation-learning-实验结果"><strong>1.7 表征学习(Representation Learning) 实验结果</strong></h3><p>为了更全面地评估 CLIP模型的效果，作者进一步评估了它的表征学习能力。关于表征学习的评估方法，有很多方法来评估某个表征的质量，以及一个“理想”的表征应该具有哪些属性。一种比较常见的方法是冻住模型的骨干部分，只训练最后的分类器，通过在某个数据集上的精度来衡量提取到的特性的好坏。</p><p>如下图9所示是本文关于表征学习研究结果。作者首先研究了[<ahref="https://zhuanlan.zhihu.com/p/625165635?utm_id=0#ref_3">3]</a>论文的12个数据集，虽然像ResNet-50 和 ResNet-101 这样的小型 CLIP 模型比在 ImageNet-1K上训练的其他 ResNet 表现更好，但它们比在 ImageNet-21K (BiT-M) 上训练的ResNet 表现更差。这些小型 CLIP 模型在具有类似计算需求的情况下，也不如EfficientNet系列的模型。作者继续在27个更多的数据集上做了相关研究，在这个更广泛的评估套件上，CLIP的优势更加明显。所有 CLIP模型，无论规模如何，在计算效率方面都优于其他模型。最佳模型的平均分数的提高从2.6% 增加到 5%。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-04eacca100b96d628b95e2f7538bb332_720w.png"alt="img" />]</p><p>图9：CLIP 模型与最先进的计算机视觉模型 Linear Probe 性能的比较</p><p>作者还研究了 CLIP 的特征在各种各样的数据集上与最佳 ImageNet模型的特征的比较。最佳 ImageNet 模型的特征用的是 Noisy StudentEfficientNet-L2 的最佳模型的特征。结果发现在27个数据集上，CLIP取得了21个数据集的优势。CLIP 在需要 OCR (SST2，HatefulMemes)，地理定位和场景识别 (Country211, SUN397) 的任务上改进最多。此外，CLIP在细粒度的汽车和交通标志识别方面也做得更好 (Stanford Cars 和GTSRB)。</p><p>[<imgsrc="../postimages/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision/v2-2faa222d0c791900c6e1551b568a3d6f_720w.webp"alt="img" />]</p><p>图10：CLIP 的特征在各种各样的数据集上优于最佳 ImageNet 模型的特征</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> baseline </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo技巧</title>
      <link href="/Hexo%E6%8A%80%E5%B7%A7/"/>
      <url>/Hexo%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="hexo-butterfly支持mermaid">hexo butterfly支持mermaid</h4><p><a href="https://mermaid.js.org/intro/">mermaid官方文档</a></p><p>如果主题本身自带了mermaid，只需要在config里改mermaid:true即可，以下方法针对没有mermaid的主题。</p><h5 id="安装hexo插件">安装hexo插件</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm i hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure><h5 id="配置">配置</h5><p>在config文件里加入以下代码</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># mermaid chart</span><br><span class="line">mermaid: ## mermaid url https://github.com/knsv/mermaid</span><br><span class="line">  enable: true  # default true</span><br><span class="line">  version: &quot;8.13.8&quot; # default v7.1.2</span><br><span class="line">  options:  # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js</span><br><span class="line">    #startOnload: true  // default true</span><br></pre></td></tr></table></figure><h5 id="主题配置">主题配置</h5><p>找到themes_partials.pug文件，加入这一行代码即可<br/>butterfly的路径为_modules-theme-butterfly.pug</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">script(src=&quot;https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js&quot;)</span><br></pre></td></tr></table></figure><h4 id="hexo-部署-github-错误解决方案">hexo 部署 github错误解决方案</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Failed to connect to github.com port 443 after 21074 ms: Couldn&#x27;t connect to server</span><br></pre></td></tr></table></figure><p>解决方案：</p><p>1.通过git配置文件查看是否使用代理：git config --globalhttp.proxy；</p><p>2.通过git取消代理：</p><ul><li>git config --global --unset http.proxy</li><li>git config --global --unset https.proxy</li></ul>]]></content>
      
      
      <categories>
          
          <category> 方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>typora技巧</title>
      <link href="/typora%E6%8A%80%E5%B7%A7/"/>
      <url>/typora%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h5 id="图片保存路径">图片保存路径：</h5><p>文件</p><ul><li>自动保存</li></ul><p>图像<br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">../../source/postimages/$&#123;filename&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（三）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>转载自https://blog.csdn.net/jzj_c_love/article/details/122279703</p><p>代码都可以在<code>typora</code>中运行，给出的图片链接语法是<code>Ketax</code>，可能有少数的不适用，但基本可以。</p><h5 id="一基本公式">一、基本公式</h5><h6 id="上下标">1. 上下标</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">A_1^2</span><br><span class="line">\\</span><br><span class="line">B_&#123;12&#125;</span><br><span class="line">\\</span><br><span class="line">2^&#123;x^2+y&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A_1^2\\B_{12}\\2^{x^2+y}\]</span></p><h6 id="分数">2. 分数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;x&#125;&#123;1+x^2&#125;</span><br><span class="line">\\</span><br><span class="line">\frac&#123;\frac&#123;1&#125;&#123;2&#125;+x&#125;&#123;y&#125;</span><br><span class="line">\\</span><br><span class="line">\tfrac&#123;a&#125;&#123;b&#125;</span><br><span class="line">\frac&#123;a&#125;&#123;b&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\frac{x}{1+x^2}\\\frac{\frac{1}{2}+x}{y}\\\tfrac{a}{b}\frac{a}{b}\]</span></p><h6 id="开根号">3. 开根号</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sqrt&#123;x&#125;</span><br><span class="line">\sqrt[3]&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\sqrt{x}\sqrt[3]{x}\]</span></p><h6 id="组合数">4. 组合数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\binom&#123;n&#125;&#123;k&#125;</span><br><span class="line">\tbinom&#123;n&#125;&#123;k&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\binom{n}{k}\tbinom{n}{k}\]</span></p><h6 id="导数">5. 导数</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">a&#x27;</span><br><span class="line">a&#x27;&#x27;</span><br><span class="line">a^&#123;\prime&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[a&#39;a&#39;&#39;a^{\prime}\]</span></p><h6 id="取模">6. 取模</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">x \pmod a</span><br><span class="line">\\</span><br><span class="line">2\mod&#123;x&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[x \pmod a\\2\mod{x}\]</span></p><h6 id="积分">7. 积分</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\int_&#123;1&#125;^&#123;2&#125;</span><br><span class="line">\intop_&#123;2&#125;^&#123;1&#125;</span><br><span class="line">\oint</span><br><span class="line">\smallint</span><br><span class="line">\\</span><br><span class="line">\iint</span><br><span class="line">\oiint</span><br><span class="line">\iiint</span><br><span class="line">\oiiint</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\int_{1}^{2}\intop_{2}^{1}\oint\smallint\\\iint\oiint\iiint\oiiint\]</span></p><h6 id="微分">8.微分</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\nabla&emsp;&emsp;</span><br><span class="line">\partial x&emsp;&emsp;</span><br><span class="line">\mathrm&#123;d&#125;x</span><br><span class="line">\dot x&emsp;&emsp;</span><br><span class="line">\ddot y     </span><br><span class="line">\Delta</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\nabla&amp;emsp;&amp;emsp;\partialx&amp;emsp;&amp;emsp;  \mathrm{d}x \dot x&amp;emsp;&amp;emsp;\ddoty     \Delta\]</span></p><h6 id="累积累乘极限">9.累积/累乘/极限</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\sum_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\sum_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\prod_&#123;i=1&#125;^&#123;k&#125;</span><br><span class="line">\displaystyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\textstyle\prod_&#123;i=1&#125;^n</span><br><span class="line">\\</span><br><span class="line">\lim_&#123;k \to \infty&#125;</span><br><span class="line">\lim\limits_&#123;k \to \infty&#125;</span><br><span class="line">\lim\nolimits_&#123;k \to \infty&#125;]</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\sum_{i=1}^{k}\displaystyle\sum_{i=1}^n\textstyle\sum_{i=1}^n\\\prod_{i=1}^{k}\displaystyle\prod_{i=1}^n\textstyle\prod_{i=1}^n\\\lim_{k\to \infty}\lim\limits_{k \to \infty}\lim\nolimits_{k \to\infty}]\]</span></p><h5 id="二修饰符号">二、修饰符号</h5><h6 id="简单的帽子">1. 简单的帽子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\hat&#123;\theta&#125;</span><br><span class="line">\widehat&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;y&#125;</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\tilde&#123;a&#125;</span><br><span class="line">\widetilde&#123;ac&#125;</span><br><span class="line">\\</span><br><span class="line">\bar&#123;a&#125;</span><br><span class="line">\acute&#123;a&#125;</span><br><span class="line">\check&#123;a&#125;</span><br><span class="line">\grave&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\dot&#123;a&#125;</span><br><span class="line">\ddot&#123;a&#125;</span><br><span class="line">\\</span><br><span class="line">\vec&#123;a&#125;</span><br><span class="line">\overline&#123;a&#125;</span><br><span class="line">\underline&#123;a&#125;</span><br><span class="line">\underset&#123;min&#125;&#123;a&#125;</span><br><span class="line">\hat&#123;a&#125;</span><br><span class="line">\tilde&#123;a&#125;</span><br><span class="line">\widehat&#123;a&#125;</span><br><span class="line">\widetilde&#123;a&#125;</span><br><span class="line">\dot&#123;a&#125;</span><br><span class="line">\ddot&#123;a&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\hat{\theta}\widehat{AB}\\\bar{y}\overline{AB}\\\tilde{a}\widetilde{ac}\\\bar{a}\acute{a}\check{a}\grave{a}\\\dot{a}\ddot{a}\\\vec{a}\overline{a}\underline{a}\underset{min}{a}\hat{a}\tilde{a}\widehat{a}\widetilde{a}\dot{a}\ddot{a}\]</span></p><h6 id="帽子和袜子">2. 帽子和袜子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overleftarrow&#123;AB&#125;</span><br><span class="line">\overrightarrow&#123;AB&#125;</span><br><span class="line">\overleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\underleftarrow&#123;AB&#125;</span><br><span class="line">\underrightarrow&#123;AB&#125;</span><br><span class="line">\underleftrightarrow&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overbrace&#123;AB&#125;</span><br><span class="line">\underbrace&#123;AB&#125;</span><br><span class="line">\\</span><br><span class="line">\overline&#123;AB&#125;</span><br><span class="line">\underline&#123;AB&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overleftarrow{AB}\overrightarrow{AB}\overleftrightarrow{AB}\\\underleftarrow{AB}\underrightarrow{AB}\underleftrightarrow{AB}\\\overbrace{AB}\underbrace{AB}\\\overline{AB}\underline{AB}\]</span></p><h6 id="盒子和帽子">3. 盒子和帽子</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\overbrace&#123;a+b+c&#125;^&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\underbrace&#123;a+b+c&#125;_&#123;\text&#123;note&#125;&#125;</span><br><span class="line">\\</span><br><span class="line">\boxed&#123;\pi=3.14&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\overbrace{a+b+c}^{\text{note}}\\\underbrace{a+b+c}_{\text{note}}\\\boxed{\pi=3.14}\]</span></p><h6 id="各种括号">4. 各种括号</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">(</span><br><span class="line">\big(</span><br><span class="line">\Big(</span><br><span class="line">\bigg(</span><br><span class="line">\Bigg(</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[(\big(\Big(\bigg(\Bigg(\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">[]</span><br><span class="line">&lt;&gt;</span><br><span class="line">|-2|</span><br><span class="line">\&#123;\&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[[]&lt;&gt;|-2|\{\}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\lgroup x \rgroup</span><br><span class="line">\lVert a \rVert</span><br><span class="line">\lceil 2.6 \rceil</span><br><span class="line">\lfloor 1.2 \rfloor</span><br><span class="line">\ulcorner</span><br><span class="line">\urcorner</span><br><span class="line">\llcorner</span><br><span class="line">\lrcorner</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p><span class="math display">\[\lgroup x \rgroup\lVert a \rVert\lceil2.6 \rceil\lfloor 1.2\rfloor\ulcorner\urcorner\llcorner\lrcorner\]</span></p><h5 id="三希腊字母">三、希腊字母</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\text&#123;字母&#125;</span><br><span class="line">\bf&#123;字母&#125;</span><br><span class="line">\mathit&#123;字母&#125;</span><br><span class="line">\pmb&#123;字母&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\text{R}\text{A}\text{C}\text{L}\\\bf{R}\bf{A}\bf{C}\bf{L}\\\mathit{R}\mathit{A}\mathit{C}\mathit{L}\\\pmb{R}\pmb{A}\pmb{C}\pmb{L}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\mathbb&#123;字母&#125;</span><br><span class="line">\mathtt&#123;字母&#125;</span><br><span class="line">\mathrm&#123;字母&#125;</span><br><span class="line">\mathsf&#123;字母&#125;</span><br><span class="line">\mathscr&#123;字母&#125;</span><br><span class="line">\mathfrak&#123;字母&#125;</span><br><span class="line">\cal&#123;字母&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\mathbb{R}\mathbb{A}\mathbb{C}\mathbb{L}\\\mathtt{R}\mathtt{A}\mathtt{C}\mathtt{L}\\\mathrm{R}\mathrm{A}\mathrm{C}\mathrm{L}\\\mathsf{R}\mathsf{A}\mathsf{C}\mathsf{L}\\\mathscr{R}\mathscr{A}\mathscr{C}\mathscr{L}\\\mathfrak{R}\mathfrak{A}\mathfrak{C}\mathfrak{L}\\\cal{R}\cal{A}\cal{C}\cal{L}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\tiny ABCabc</span><br><span class="line">\small ABCabc</span><br><span class="line">\normalsize ABCabc</span><br><span class="line">\large ABCabc</span><br><span class="line">\Large ABCabc</span><br><span class="line">\huge ABCabc</span><br><span class="line">\Huge ABCabc</span><br><span class="line">&#123;\tiny ABC&#125; &#123;\large ABC&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\tiny ABCabc\\\small ABCabc\\\normalsizeABCabc\\\large ABCabc\\\Large ABCabc\\\huge ABCabc\\\Huge ABCabc\\{\tinyABC} {\large ABC}\]</span></p><p><span class="math display">\[\alpha   \beta   \gamma  \Gamma\delta  \Delta \epsilon   \\\\ \zeta   \eta  \theta  \Theta\iota   \kappa    \\\\ \lambda  \Lambda \mu   \nu  \xi  \Xi\pi  \Pi  \\\\ \rho   \sigma  \Sigma \tau   \upsilon  \Upsilon\phi  \Phi  \\\\ \chi   \psi  \Psi \omega  \Omega\]</span></p><p><imgsrc="../postimages/MarkDown（三）/e6a627023fa735a129f1725b85da1fa3.png"alt="image" /><br/><imgsrc="../postimages/MarkDown（三）/99c3a73a6e705c382b6b2acc99920392.png"alt="image" /></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\boldsymbol&#123;\alpha&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\boldsymbol{\alpha}\]</span></p><h5 id="四算术运算符号">四、算术运算符号</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\times</span><br><span class="line">\div</span><br><span class="line">\cdot</span><br><span class="line">\%</span><br><span class="line">\circ</span><br><span class="line">\ast</span><br><span class="line">\star</span><br><span class="line">\otimes</span><br><span class="line">\oplus</span><br><span class="line">\odot</span><br><span class="line">\oslash</span><br><span class="line">\pm</span><br><span class="line">\mp</span><br><span class="line">\dotplus</span><br><span class="line">\divideontimes</span><br><span class="line">\textbackslash</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\times\div\cdot\%\circ\ast\star\otimes\oplus\odot\oslash\pm\mp\dotplus\divideontimes\textbackslash\]</span></p><h5 id="五比较运算符">五、比较运算符</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\=</span><br><span class="line">\equiv</span><br><span class="line">\approx</span><br><span class="line">\approxeq</span><br><span class="line">\cong</span><br><span class="line">\sim</span><br><span class="line">\neq</span><br><span class="line">\not=</span><br><span class="line">&lt;</span><br><span class="line">\&gt;</span><br><span class="line">\le</span><br><span class="line">\ge</span><br><span class="line">\gg</span><br><span class="line">\ll</span><br><span class="line">\curlyeqprec</span><br><span class="line">\curlyeqsucc</span><br><span class="line">\prec</span><br><span class="line">\succ</span><br><span class="line">\preceq</span><br><span class="line">\succeq</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\equiv\approx\approxeq\cong\sim\neq\not=&lt;\&gt;\le\ge\gg\ll\curlyeqprec\curlyeqsucc\prec\succ\preceq\succeq\]</span></p><h5 id="六集合运算符">六、集合运算符</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\in</span><br><span class="line">\owns \not</span><br><span class="line">\subset \not</span><br><span class="line">\supset</span><br><span class="line">\subseteq</span><br><span class="line">\supseteq</span><br><span class="line">\\</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\\</span><br><span class="line">\neg</span><br><span class="line">\emptyset</span><br><span class="line">\varnothing</span><br><span class="line">\\</span><br><span class="line">\because</span><br><span class="line">\forall</span><br><span class="line">\exists</span><br><span class="line">\therefore</span><br><span class="line">\cap</span><br><span class="line">\cup</span><br><span class="line">\land</span><br><span class="line">\lor</span><br><span class="line">\sqcup</span><br><span class="line">\sqcap</span><br></pre></td></tr></table></figure><p><span class="math display">\[\in\owns \not\subset\not\supset\subseteq\supseteq\\\cap\cup\land\lor\\\neg\emptyset\varnothing\\\because\forall\exists\therefore\cap\cup\land\lor\sqcup\sqcap\]</span></p><h5 id="七各种箭头">七、各种箭头</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\gets</span><br><span class="line">\leftarrow</span><br><span class="line">\to</span><br><span class="line">\rightarrow</span><br><span class="line">\leftrightarrow</span><br><span class="line">\\</span><br><span class="line">\uparrow</span><br><span class="line">\downarrow</span><br><span class="line">\updownarrow</span><br><span class="line">\Leftarrow</span><br><span class="line">\Rightarrow</span><br><span class="line">\Leftrightarrow</span><br><span class="line">\iff</span><br><span class="line">\\</span><br><span class="line">\Uparrow</span><br><span class="line">\Downarrow</span><br><span class="line">\Updownarrow</span><br><span class="line">\nearrow</span><br><span class="line">\searrow</span><br><span class="line">\swarrow</span><br><span class="line">\nwarrow</span><br><span class="line">\longleftarrow</span><br><span class="line">\longrightarrow</span><br><span class="line">\longleftrightarrow</span><br><span class="line">\Longleftarrow</span><br><span class="line">\Longrightarrow</span><br><span class="line">\Longleftrightarrow</span><br><span class="line">\longmapsto</span><br><span class="line">\xrightarrow&#123;over&#125;</span><br><span class="line">\xrightarrow[over]&#123;&#125;</span><br><span class="line">\xrightarrow[under]&#123;over&#125;</span><br><span class="line">\xleftarrow[]&#123;over&#125;</span><br><span class="line">\xleftarrow[under]&#123;&#125;</span><br><span class="line">\xleftarrow[under]&#123;over&#125;</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\gets\leftarrow\to\rightarrow\leftrightarrow\\\uparrow\downarrow\updownarrow\Leftarrow\Rightarrow\Leftrightarrow\iff\\\Uparrow\Downarrow\Updownarrow\nearrow\searrow\swarrow\nwarrow\longleftarrow\longrightarrow\longleftrightarrow\Longleftarrow\Longrightarrow\Longleftrightarrow\longmapsto\xrightarrow{over}\xrightarrow[over]{}\xrightarrow[under]{over}\xleftarrow[]{over}\xleftarrow[under]{}\xleftarrow[under]{over}\]</span></p><h5 id="七空间间距">七、空间间距</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A\!B</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\\</span><br><span class="line">A\thinspace B</span><br><span class="line">\\</span><br><span class="line">A\:B</span><br><span class="line">\\</span><br><span class="line">A\ B</span><br><span class="line">\\</span><br><span class="line">A \enspace B</span><br><span class="line">\\</span><br><span class="line">A\quad B</span><br><span class="line">\\</span><br><span class="line">A\qquad B</span><br></pre></td></tr></table></figure><p><span class="math display">\[A\!B\\AB\\A\thinspace B\\A\:B\\A\ B\\A\enspace B\\A\quad B\\A\qquad B\]</span></p><h5 id="八矩阵">八、矩阵</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A=</span><br><span class="line">\begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b &amp; \cdots &amp; c  \\</span><br><span class="line">d &amp; e &amp; \cdots &amp; f  \\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\</span><br><span class="line">g &amp; h &amp; \cdots &amp; j</span><br><span class="line">\end&#123;pmatrix&#125;</span><br><span class="line">\tag&#123;5.1&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A=\begin{pmatrix}a &amp; b &amp; \cdots&amp; c  \\\\d &amp; e &amp; \cdots &amp; f  \\\\\vdots &amp; \vdots&amp; \ddots &amp; \vdots  \\\\g &amp; h &amp; \cdots &amp;j\end{pmatrix}\tag{5.1}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A = \begin&#123;matrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[A = \begin{matrix}a &amp; b\\\\c &amp;d\end{matrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">B = \begin&#123;pmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;pmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[B = \begin{pmatrix}a &amp; b\\\\c &amp;d\end{pmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C = \begin&#123;vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[C = \begin{vmatrix}a &amp; b\\\\c &amp;d\end{vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">D = \begin&#123;bmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[D = \begin{bmatrix}a &amp; b\\\\c &amp;d\end{bmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">E = \begin&#123;Vmatrix&#125;</span><br><span class="line">a &amp; b\\</span><br><span class="line">c &amp; d</span><br><span class="line">\end&#123;Vmatrix&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[E = \begin{Vmatrix}a &amp; b\\\\c &amp;d\end{Vmatrix}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">f(x) &amp;= (x+1)^2\\</span><br><span class="line">&amp;= x^2 + 2x + 1</span><br><span class="line">\end&#123;aligned&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{aligned}f(x) &amp;=(x+1)^2\\\\&amp;= x^2 + 2x + 1\end{aligned}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">f(x) = \begin&#123;cases&#125;</span><br><span class="line">a &amp;\text&#123;if b&#125;\\</span><br><span class="line">b &amp;\text&#123;if a&#125;\\</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[f(x) = \begin{cases}a &amp;\text{ifb}\\\\b &amp;\text{if a}\\\\\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\begin&#123;cases&#125;</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x + 2y &amp;= 1\\</span><br><span class="line">3x - y &amp;= 5</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><span class="math display">\[\begin{cases}\begin{aligned}x + 2y&amp;= 1\\\\3x - y &amp;= 5\end{aligned}\end{cases}\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">g(x,y)=\left\&#123;</span><br><span class="line">\begin&#123;array&#125;&#123;rcl&#125;</span><br><span class="line">\frac&#123;M_g - d&#125;&#123;M_f-b&#125;[f(x,y)-b]+d       &amp;      &amp; &#123;b      \leq  f(x,y)  \leq M_f&#125;\\</span><br><span class="line">F^*_L     &amp;      &amp; &#123;S_L \leq 0 &lt; S_M&#125;\\</span><br><span class="line">F^*_R     &amp;      &amp; &#123;S_M \leq 0 &lt; S_R&#125;\\</span><br><span class="line">F_R       &amp;      &amp; &#123;S_R \leq 0&#125;</span><br><span class="line">\end&#123;array&#125; \right.</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[g(x,y)=\begin{cases}\begin{array}{rcl}\frac{M_g -d}{M_f-b}[f(x,y)-b]+d       &amp;      &amp; {b      \leq  f(x,y)  \leqM_f}\\\\F^*_L     &amp;      &amp; {S_L \leq 0 &lt;S_M}\\\\F^*_R     &amp;      &amp; {S_M \leq 0 &lt;S_R}\\\\F_R       &amp;      &amp; {S_R \leq 0}\end{array}\end{cases}\]</span></p><p>九、修改字体大小</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AB</span><br><span class="line">\Huge AB</span><br><span class="line">\huge AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\LARGE AB</span><br><span class="line">\Large AB</span><br><span class="line">\large AB</span><br><span class="line">\\</span><br><span class="line">AB</span><br><span class="line">\small AB</span><br><span class="line">\tiny AB</span><br></pre></td></tr></table></figure><p><span class="math display">\[AB\Huge AB\huge AB\\AB\LARGE AB\LargeAB\large AB\\AB\small AB\tiny AB\]</span></p><h5 id="十划掉">十、划掉</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\cancel&#123;5&#125;</span><br><span class="line">\bcancel&#123;5&#125;</span><br><span class="line">\xcancel&#123;ABC&#125;</span><br><span class="line">\not =</span><br></pre></td></tr></table></figure><p><span class="math display">\[\cancel{5}\bcancel{5}\xcancel{ABC}\not=\]</span></p><p>十一、常见图形</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\Box</span><br><span class="line">\square</span><br><span class="line">\blacksquare</span><br><span class="line">\triangle</span><br><span class="line">\triangledown</span><br><span class="line">\blacktriangle</span><br><span class="line">\diamond</span><br><span class="line">\Diamond</span><br><span class="line">\star</span><br><span class="line">\bigstar</span><br><span class="line">\circ</span><br><span class="line">\bullet</span><br><span class="line">\bigcirc</span><br><span class="line">\bigodot</span><br><span class="line">\diamondsuit</span><br><span class="line">\clubsuit</span><br><span class="line">\heartsuit</span><br><span class="line">\spadesuit</span><br><span class="line">\angle</span><br><span class="line">\measuredangle</span><br><span class="line">\top</span><br><span class="line">\bot</span><br><span class="line">\infty</span><br><span class="line">\checkmark</span><br><span class="line">\dagger</span><br><span class="line">\ddagger</span><br><span class="line">\yen</span><br><span class="line">\$</span><br></pre></td></tr></table></figure><p><spanclass="math display">\[\Box\square\blacksquare\triangle\triangledown\blacktriangle\diamond\Diamond\star\bigstar\circ\bullet\bigcirc\bigodot\diamondsuit\clubsuit\heartsuit\spadesuit\angle\measuredangle\top\bot\infty\checkmark\dagger\ddagger\yen\\]</span>$</p><h5 id="十二声明宏">十二、声明宏</h5><p>对于一些复杂但是只有少许不同的表达式，可以声明一个函数来调用，提高源码的可读性，减少出错</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\macroname#1#2&#123;</span><br><span class="line">your command</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>宏允许带任意数量的参数（也可以不带参），必须是#1,#2,……这样的命名格式，同时注意再定义宏的时候注意让#1与，否则会解析成#。再调用的时候格式为，可以参考一下的例子</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\Normal#1#2#3&#123;</span><br><span class="line">\frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;\ #3&#125;\exp&#123;[-\frac&#123;(#1 - #2)^2&#125;&#123;2\ #3^2&#125;]&#125;</span><br><span class="line">&#125;</span><br><span class="line">f(x)=\Normal&#123;x&#125;&#123;u_1&#125;&#123;\sigma_1&#125;\\</span><br><span class="line">f(y)=\Normal&#123;y&#125;&#123;u_2&#125;&#123;\sigma_2&#125;\\</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\Normal#1#2#3{\frac{1}{\sqrt{2\pi}\#3}\exp{[-\frac{(#1 - #2)^2}{2\ #3^2}]}}f(x)=\Normal{x}{u_1}{\sigma_1}\\f(y)=\Normal{y}{u_2}{\sigma_2}\\\]</span></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\def\EXP&#123;</span><br><span class="line">e^x = 1 + x + \frac&#123;1&#125;&#123;2!&#125;x^2 + \frac&#123;1&#125;&#123;3!&#125;x^3  + \cdots</span><br><span class="line">&#125;</span><br><span class="line">\EXP</span><br></pre></td></tr></table></figure><p><span class="math display">\[\def\EXP{e^x = 1 + x + \frac{1}{2!}x^2 +\frac{1}{3!}x^3  + \cdots}\EXP\]</span></p><hr /><p>当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax对数学公式进行渲染。如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 =  \begin&#123;vmatrix&#125; </span><br><span class="line">\mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\</span><br><span class="line">\end&#123;vmatrix&#125;</span><br><span class="line">$&#123;$tep1&#125;&#123;\style&#123;visibility:hidden&#125;&#123;(x+1)(x+1)&#125;&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><p>显示效果：</p>]]></content>
      
      
      <categories>
          
          <category> 方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（二）</title>
      <link href="/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="图表绘制"><strong>图表绘制</strong></h5><h6 id="横向流程图"><strong>横向流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure><h6 id="竖向流程图">竖向流程图</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><p>显示效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><h6 id="标准流程图"><strong>标准流程图</strong></h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;io-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### 标准流程图（横向）</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始框</span><br><span class="line">op=&gt;operation: 处理框</span><br><span class="line">cond=&gt;condition: 判断框(是或否?)</span><br><span class="line">sub1=&gt;subroutine: 子流程</span><br><span class="line">io=&gt;inputoutput: 输入输出框</span><br><span class="line">e=&gt;end: 结束框</span><br><span class="line">st(right)-&gt;op(right)-&gt;cond</span><br><span class="line">cond(yes)-&gt;io(bottom)-&gt;e</span><br><span class="line">cond(no)-&gt;sub1(right)-&gt;op</span><br></pre></td></tr></table></figure><p><br/>###### UML时序图：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><p><br/><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br></pre></td></tr></table></figure><br/>###### UML时序图源码复杂样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Title: 标题：复杂使用</span><br><span class="line">对象A-&gt;对象B: 对象B你好吗?（请求）</span><br><span class="line">Note right of 对象B: 对象B的描述</span><br><span class="line">Note left of 对象A: 对象A的描述(提示)</span><br><span class="line">对象B--&gt;对象A: 我很好(响应)</span><br><span class="line">对象B-&gt;小三: 你好吗</span><br><span class="line">小三--&gt;&gt;对象A: 对象B找我了</span><br><span class="line">对象A-&gt;对象B: 你真的好吗？</span><br><span class="line">Note over 小三,对象B: 我们是朋友</span><br><span class="line">participant C</span><br><span class="line">Note right of C: 没人陪我玩</span><br></pre></td></tr></table></figure><p><br/>###### UML标准时序图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头</span><br><span class="line">  sequenceDiagram</span><br><span class="line">    participant 张三</span><br><span class="line">    participant 李四</span><br><span class="line">    张三-&gt;王五: 王五你好吗？</span><br><span class="line">    loop 健康检查</span><br><span class="line">        王五-&gt;王五: 与疾病战斗</span><br><span class="line">    end</span><br><span class="line">    Note right of 王五: 合理 食物 &lt;br/&gt;看医生...</span><br><span class="line">    李四--&gt;&gt;张三: 很好!</span><br><span class="line">    王五-&gt;李四: 你怎么样?</span><br><span class="line">    李四--&gt;王五: 很好!</span><br></pre></td></tr></table></figure><p><br/>###### 甘特图样例：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%% 语法示例</span><br><span class="line">        gantt</span><br><span class="line">        dateFormat  YYYY-MM-DD</span><br><span class="line">        title 软件开发甘特图</span><br><span class="line">        section 设计</span><br><span class="line">        需求                      :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">        原型                      :active,  des2, 2014-01-09, 3d</span><br><span class="line">        UI设计                     :         des3, after des2, 5d</span><br><span class="line">    未来任务                     :         des4, after des3, 5d</span><br><span class="line">        section 开发</span><br><span class="line">        学习准备理解需求                      :crit, done, 2014-01-06,24h</span><br><span class="line">        设计框架                             :crit, done, after des2, 2d</span><br><span class="line">        开发                                 :crit, active, 3d</span><br><span class="line">        未来任务                              :crit, 5d</span><br><span class="line">        耍                                   :2d</span><br><span class="line">        section 测试</span><br><span class="line">        功能测试                              :active, a1, after des3, 3d</span><br><span class="line">        压力测试                               :after a1  , 20h</span><br><span class="line">        测试报告                               : 48h</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MarkDown（一）</title>
      <link href="/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/MarkDown%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一编写标题">一、编写标题</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 一级标题(h1)</span><br><span class="line">## 二级标题(h2)</span><br><span class="line">### 三级标题(h3)</span><br><span class="line">#### 四级标题(h4)</span><br><span class="line">##### 五级标题(h5)</span><br><span class="line">###### 六级标题(h6)</span><br></pre></td></tr></table></figure><h5 id="二字体">二、<strong>字体</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*斜体文本*</span><br><span class="line">_斜体文本_</span><br><span class="line">**粗体文本**</span><br><span class="line">__粗体文本__</span><br><span class="line">***粗斜体文本***</span><br><span class="line">___粗斜体文本___</span><br></pre></td></tr></table></figure><p><em>斜体文本</em><br/><em>斜体文本</em><br/><strong>粗体文本</strong><br/><strong>粗体文本</strong><br/><strong><em>粗斜体文本</em></strong><br/><strong><em>粗斜体文本</em></strong></p><h5 id="三分割线">三、<strong>分割线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">***</span><br><span class="line"></span><br><span class="line">* * *</span><br><span class="line"></span><br><span class="line">*****</span><br><span class="line"></span><br><span class="line">- - -</span><br><span class="line"></span><br><span class="line">----------</span><br></pre></td></tr></table></figure><hr /><hr /><hr /><hr /><hr /><h5 id="四删除线">四、<strong>删除线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RUNOOB.COM</span><br><span class="line">GOOGLE.COM</span><br><span class="line">~~BAIDU.COM~~</span><br></pre></td></tr></table></figure><p>RUNOOB.COM<br/>GOOGLE.COM<br/><del>BAIDU.COM</del></p><h5 id="五下划线">五、<strong>下划线</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;u&gt;带下划线文本&lt;/u&gt;</span><br></pre></td></tr></table></figure><p><u>带下划线文本</u></p><h5 id="六脚注">六、<strong>脚注</strong></h5><p>脚注是对文本的补充说明。</p><p>Markdown 脚注的格式如下:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[^要注明的文本]</span><br></pre></td></tr></table></figure><p>以下实例演示了脚注的用法：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建脚注格式类似这样 [^RUNOOB]。</span><br><span class="line"></span><br><span class="line">[^RUNOOB]: 菜鸟教程 -- 学的不仅是技术，更是梦想！！！</span><br></pre></td></tr></table></figure><h5 id="七markdown列表">七、<strong>Markdown列表</strong></h5><p>Markdown 支持有序列表和无序列表。</p><p>无序列表使用星号(*<strong>)、加号(+</strong>)或是减号(-**)作为列表标记：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* 第一项</span><br><span class="line">* 第二项</span><br><span class="line">* 第三项</span><br><span class="line"></span><br><span class="line">+ 第一项</span><br><span class="line">+ 第二项</span><br><span class="line">+ 第三项</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- 第一项</span><br><span class="line">- 第二项</span><br><span class="line">- 第三项</span><br></pre></td></tr></table></figure><ul><li><p>第一项<br/>* 第二项<br/>* 第三项</p></li><li><p>第一项<br/>+ 第二项<br/>+ 第三项</p></li><li><p>第一项</p></li><li><p>第二项</p></li><li><p>第三项</p></li></ul><h5 id="八列表嵌套">八、<strong>列表嵌套</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 第一项：</span><br><span class="line">    - 第一项嵌套的第一个元素</span><br><span class="line">    - 第一项嵌套的第二个元素</span><br><span class="line">2. 第二项：</span><br><span class="line">    - 第二项嵌套的第一个元素</span><br><span class="line">    - 第二项嵌套的第二个元素</span><br></pre></td></tr></table></figure><ol type="1"><li>第一项：<br/> - 第一项嵌套的第一个元素<br/> -第一项嵌套的第二个元素<br/>2. 第二项：<br/> -第二项嵌套的第一个元素<br/> - 第二项嵌套的第二个元素</li></ol><h5 id="九markdown区块">九、<strong>Markdown区块</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 区块引用</span><br><span class="line">&gt; 菜鸟教程</span><br><span class="line">&gt; 学的不仅是技术更是梦想</span><br></pre></td></tr></table></figure><blockquote><p>区块引用<br/>&gt; 菜鸟教程<br/>&gt; 学的不仅是技术更是梦想</p></blockquote><h5 id="十markdown代码">十、<strong>Markdown代码</strong></h5><p>如果是段落上的一个函数或片段的代码可以用反引号把它包起来（<strong>`</strong>），例如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`printf()` 函数</span><br></pre></td></tr></table></figure><p>演示效果如下：</p><p><code>printf()</code> 函数</p><h5 id="十一代码块">十一、<strong>代码块</strong></h5><p>代码区块使用 <strong>```</strong>包裹一段代码，并指定一种语言（也可以不指定）：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```javascript</span><br><span class="line">$(document).ready(function () &#123;</span><br><span class="line">    alert(&#x27;RUNOOB&#x27;);</span><br><span class="line">&#125;);</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">$(document).ready(function () &#123;</span></span><br><span class="line"><span class="string">    alert(&#x27;RUNOOB&#x27;);</span></span><br><span class="line"><span class="string">&#125;);</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br></pre></td></tr></table></figure><h5 id="十二markdown链接"><strong>十二、Markdown链接</strong></h5><p>链接使用方法如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[链接名称](链接地址)</span><br><span class="line">或者</span><br><span class="line">&lt;链接地址&gt;</span><br></pre></td></tr></table></figure><p>例如：</p><p>这是一个链接 <ahref="https://www.cnblogs.com/caoleiCoding/">云中志</a></p><p>https://www.cnblogs.com/caoleiCoding/</p><h5 id="十三高级链接">十三、<strong>高级链接</strong></h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">链接也可以用变量来代替，文档末尾附带变量地址：</span><br><span class="line">这个链接用 1 作为网址变量 [Google][1]</span><br><span class="line">这个链接用 runoob 作为网址变量 [Coding][Coding]</span><br><span class="line">然后在文档的结尾为变量赋值（网址）</span><br><span class="line"></span><br><span class="line">  [1]: http://www.google.com/</span><br><span class="line">  [Coding]: https://www.cnblogs.com/caoleiCoding/</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><p>这个链接用 1 作为网址变量<ahref="http://www.google.com/%3Cbr/%3E%5BCoding%5D:%20https://www.cnblogs.com/caoleiCoding/">Google</a></p><p>这个链接用 runoob 作为网址变量 [Coding][Coding]</p><h5 id="十四markdown图片">十四、<strong>Markdown图片</strong></h5><p>Markdown 图片语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![alt 属性文本](图片地址)</span><br><span class="line"></span><br><span class="line">![alt 属性文本](图片地址 &quot;可选标题&quot;)</span><br></pre></td></tr></table></figure><ul><li>开头一个感叹号 !</li><li>接着一个方括号，里面放上图片的替代文字</li><li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上选择性的‘title’ 属性的文字。</li></ul><p>使用示例：</p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><p><ahref="https://zhaozw-szu.github.io/postimages/MarkDown.assets/adminlogo.gif"><imgsrc="./../postimages/MarkDown（一）/adminlogo.gif"alt="博客园" /></a></p><h5 id="十五markdown表格"><strong>十五、Markdown表格</strong></h5><p>Markdown 制作表格使用 <strong>|</strong> 来分隔不同的单元格，使用<strong>-</strong> 来分隔表头和其他行。</p><p>语法格式如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|  表头   | 表头  |</span><br><span class="line">|  ----  | ----  |</span><br><span class="line">| 单元格  | 单元格 |</span><br><span class="line">| 单元格  | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th>表头</th><th>表头</th></tr></thead><tbody><tr class="odd"><td>单元格</td><td>单元格</td></tr><tr class="even"><td>单元格</td><td>单元格</td></tr></tbody></table><h5 id="十六对齐方式">十六、<strong>对齐方式</strong></h5><p><strong>我们可以设置表格的对齐方式：</strong></p><ul><li><strong>-:</strong> 设置内容和标题栏居右对齐。</li><li><strong>:-</strong> 设置内容和标题栏居左对齐。</li><li><strong>:-:</strong> 设置内容和标题栏居中对齐。</li></ul><p>示例如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :-----| ----: | :----: |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br></pre></td></tr></table></figure><p>显示效果如下：</p><table><thead><tr class="header"><th style="text-align: left;">左对齐</th><th style="text-align: right;">右对齐</th><th style="text-align: center;">居中对齐</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr><tr class="even"><td style="text-align: left;">单元格</td><td style="text-align: right;">单元格</td><td style="text-align: center;">单元格</td></tr></tbody></table><h5 id="十七缩进">十七、<strong>缩进</strong></h5><p>总结一下缩进的方式：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;能缩进一个汉字，可叠加</span><br><span class="line">&amp;ensp;能缩进半个汉字，可叠加</span><br><span class="line">&amp;nbsp;能缩进四分之一，可叠加</span><br></pre></td></tr></table></figure><p>一般需要所缩进两个汉字，以下方式都可以缩进两个汉字</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;&amp;emsp;</span><br><span class="line">&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;</span><br><span class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br></pre></td></tr></table></figure><p> 能缩进一个汉字，可叠加</p><p> 能缩进半个汉字，可叠加</p><p> 能缩进四分之一，可叠加</p><p>  能缩进两个汉字，可叠加</p><h5id="十八markdown高级技巧">十八、<strong>Markdown高级技巧</strong></h5><h6 id="支持的-html-元素"><strong>支持的 HTML 元素</strong></h6><p>不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML撰写。</p><p>目前支持的 HTML 元素有：<code>&lt;kbd&gt;</code><code>&lt;b&gt;</code> <code>&lt;i&gt;</code> <code>&lt;em&gt;</code><code>&lt;sup&gt;</code> <code>&lt;sub&gt;</code><code>&lt;br&gt;</code>等 ，如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑</span><br></pre></td></tr></table></figure><p>使用 <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> 重启电脑</p><h6 id="转义"><strong>转义</strong></h6><p>Markdown使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown使用反斜杠转义特殊字符：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">**文本加粗** </span><br><span class="line">\*\* 正常显示星号 \*\*</span><br></pre></td></tr></table></figure><p>显示效果：</p><p><strong>文本加粗</strong> <br/>** 正常显示星号 **</p>]]></content>
      
      
      <categories>
          
          <category> 方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A New Benchmark and Model for Challenging Image Manipulation Detection</title>
      <link href="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/"/>
      <url>/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/</url>
      
        <content type="html"><![CDATA[<center>A New Benchmark and Model for Challenging Image ManipulationDetection<br/><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a><br/></center><center><br/> <spanclass="math inline">\(ZhenfeiZhang^1,MingyangLi^2,Ming-ChingChang^1\)</span><br/></center><center>美国纽约州立大学奥尔巴尼大学计算机科学系</center><details close><br/><summary>论文（arxiv）</summary><br/><div class="row">    <embed src="/postpdfs/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/2311.14218.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h1 id="摘要">摘要</h1><p>  <strong>所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。此外，基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。</strong><br/><br/>  为了研究在这些具有挑战性的条件下最先进的（SoTA）IMD方法，我们引入了一个新的具有挑战性的图像操作检测（CIMD）基准数据集，它由两个子集组成，分别用于评估基于编辑和基于压缩的IMD方法。数据集的图像是手工拍摄和篡改高质量的注释。<br/><br/>  此外，我们提出了一种新的基于HRNet的双分支网络模型，该模型可以在这些具有挑战性的条件下更好地检测图像编辑和压缩伪影。在CIMD基准上的大量实验表明，我们的模型在CIMD上显著优于SoTAIMD方法。<br/><br/>本文的贡献包括：</p><ul><li>我们提出了一种新的双分支架构，结合了RGB和频率特征，以实现具有挑战性的图像操纵检测。据我们所知，我们的模型是第一个专注于检测小的被篡改区域的方法。</li><li>我们引入了开创性的压缩伪影学习模型，能够检测双压缩伪影，无论量化因子（QFs）是不同的还是相同的。</li><li>我们引入了一个新的高质量的CIMD基准来评估SoTAIMD方法在具有挑战性的操作中的性能。我们将在书面接受后公开CIMD。</li><li>在CIMD上的大量实验表明，该方法在具有挑战性的图像操作检测方面显著优于SoTA。</li></ul><h1 id="数据集">数据集</h1><p>The Challenging Image Manipulation Detection Dataset(CIMD)</p><p>  在这项工作中，我们的目标是建立一个全面的验证数据集（CIMD），专门用于在压缩和未压缩场景下的小区域伪造（平均小于1.5%）。我们的数据集在数据集大小、图像质量、图像多样性和伪造策略方面都具有优势。<br/><br/>  引入了两个独立的子集来分别评估基于图像编辑和基于压缩的方法。收集我们使用佳能RP相机捕获原始图像，包括未压缩的TIFF和压缩的JPG伪造-原始图像对。这些捕捉是在高度多样的多季节拍摄的，特点是复杂和复杂的照明条件。我们的目的是在现实生活中提供一个公正和全面的模型评估。<br/><br/>  两个CIMD数据集。我们提供了两个子集：</p><blockquote><p>CIMD-Raw子集由成对的原始未压缩的TIFF图像组成，用于评估基于图像编辑的方法。<br/>&gt;CIMD-压缩子集包括拼接伪图像及其对应的原始JPEG图像，其统一量化因子（QFs）范围在50到100之间。这个子集评估了基于压缩的模型在相同的QF条件下检测伪造的能力。</p></blockquote><h3 id="cimd-raw子集cimd-r">CIMD-Raw子集(CIMD-R)</h3><p>  CIMD-R旨在提供一个对基于图像编辑的模型在检测未压缩图像上的小篡改复制移动、对象删除和拼接伪造方面的性能的全面评估。未压缩图像的使用消除了伪造区域上不希望的压缩伪影，否则可以被神经网络感知，使对检测的更真实的性能评估。CIMD-R由600张TIFF图像组成，分辨率为2048×1365。还提供了ground-truth标签。此外，CIMD-R采用了一种面向未来的方法，提供16bit的图像对，可以提供多达<spanclass="math inline">\(2^{48}\)</span>种（以万亿）颜色。<br/><br/>  对于复制-移动操作，将图像的一部分复制和粘贴到同一图像中，然后是五种后处理方法，即缩放、旋转、水平/曲线增加、光照变化和颜色再分配。<br/><br/>  对于删除伪造操作，通过ps中的内容感知填充来从图像中删除选定的区域。内容感知填充被广泛应用于多个数据集（Parketal.2018b；Dong，Wang，和Tan2013b），代表了PS根据周围区域绘制物体的最佳猜测。<br/><br/>  对于拼接伪造操作，将一个图像的区域复制粘贴到另一个图片。然后，采用复制-移动操作中相同的后处理方法，使锻造区域与周围环境相协调。</p><h3 id="cimd-compressed子集cimd-c">CIMD-Compressed子集(CIMD-C)</h3><p>  CIMD-C旨在评估基于压缩的模型在主压缩和二次压缩具有相同的QFs时检测双JEPG压缩伪影的能力。该数据集包含200张JPEG图像，分辨率为2048×1365，其中QF均匀分布为50≤QF&lt;100。<br/><br/>  伪造图像的生成类似于CIMD-R的拼接样本，区别在于伪造图像使用JPEG压缩算法保存，使用与原始图像相同的QF。原始图像由RAW文件生成，确保原始图像第一次被压缩，增强了数据集的可信度。在伪造的图像中，背景是双压缩的，而被篡改的区域是单压缩的。此外，该数据集还包括用于压缩的二进制掩码和QF值，从而增强了其对进一步研究不同QFs的影响的效用。</p><h1 id="提出的imd方法">提出的IMD方法</h1><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png"alt="image-20240326151650512" /><figcaption aria-hidden="true">image-20240326151650512</figcaption></figure><p>  我们提出的双分支架构能够检测异常特征和压缩伪影，其灵感来自于（Kwonet al.2022）。此外，我们的模型可以有效地检测小的操作区域和识别双压缩轨迹，应用相同的量化矩阵（Q-矩阵）。为了实现我们的研究目标，我们采用HR-Net（Wanget al.2020）作为我们模型的支柱，基于其提供三倍收益的能力。首先，HR-Net中没有池化层，这确保了这些特性在整个过程中保持高分辨率。其次，该模型在处理不同尺度的特征的同时，也要处理有效的信息交换，这对于获取不同尺度的信息至关重要。最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。<br/><br/>  为了更精确地定位微小的篡改区域，我们应用面积空间金字塔池（ASPP）（Chen等2017；Yang等2021）和注意机制（Vaswani等2017；胡，沈，和孙2018）仔细设计了模型。<br/><br/>  对于RGB流，输入的图像被输入到一个完整的HR-Net，它从视觉内容中学习图像编辑跟踪。<br/><br/>  对于DCT流，我们向主干提供量化的DCT系数、q矩阵和新的多次重压缩残差DCT系数，以检测双压缩伪影。这种设计工作，不考虑QF是否相同。为了提高所提出的双分支模型的性能，我们在最后引入了一种自适应加权热图聚合设计，使用软选择来融合由两个分支生成的热图。<br/><br/>  其次，该模型在处理不同尺度的特征的同时，还能处理有效的信息交换，这对于获取不同尺度的信息至关重要。<br/><br/>  最后，HR-Net的输入大小非常适合于DCT特性。由于经过8个速率的扩展卷积处理后，DCT特征的大小减小到输入大小的1/8，这相当于HR-Net的第二阶段分辨率。我们对RGB流应用完整的HR-Net，而对于频率流，我们使用三分辨率的变体HR-Net，用图5中所示的压缩伪影学习模型代替第一阶段。</p><h2id="压缩伪影学习模型compression-artifacts-learning-model">压缩伪影学习模型CompressionArtifacts Learning Model</h2><p>  当使用相同的QF创建拼接图像时，被操纵的区域被单独压缩，而背景区域被双重压缩。因此，当图像被反复压缩时，不稳定的量化DCT系数逐渐集中在被篡改的区域上，而真实的区域则保持相对稳定。在此基础上，我们引入了一种新的残差DCT图来指导DCT特征，以更好地关注IMD的不稳定区域。</p><p>YCbCr 是一种用于压缩彩色图像的色彩空间，其中：</p><ul><li><strong>Y 通道</strong>表示亮度（Luminance），也即图像的灰度级信息。</li><li><strong>Cb 和 Cr 通道</strong>分别表示蓝色差异和红色差异（Chrominance），即色彩信息。</li></ul><p>  我们的方法只关注于y通道DCT图，因为它对人眼更敏感。给定一个JPEG图像，很容易从JPEG文件报头中得到y通道量化的DCT系数<spanclass="math inline">\(Q_0\)</span>及其相应的<spanclass="math inline">\(Q\)</span>矩阵。<br/>首先重复<spanclass="math inline">\(Q\)</span>矩阵具有与<spanclass="math inline">\(Q_0\)</span>相同的大小，我们将重复的<spanclass="math inline">\(Q\)</span>矩阵设为<spanclass="math inline">\(q\)</span>。</p><p>  然后，我们使用以下方程依次计算(k+1)次再压缩量化JPEG系数<spanclass="math inline">\(Q_{k+1}\)</span>：</p><p><spanclass="math display">\[\begin{cases}\begin{array}{l}D_k=Q_k\odotq\\B_k=IDCT(D_k)\\I_{k+1}=RT(B_k)\\Q_{k+1}=[DCT(I_{k+1})\oslashq]\end{array}\end{cases}\]</span>   其中<spanclass="math inline">\(\oslash\)</span>表示元素级划分，D、B、I、Q分别表示去量化的DCT系数、使用逆DCT进行反变换块的DCT系数、使用图像块进行反变换块的DCT系数和使用量化JPEG系数进行反变换块的DCT系数。上述方程中变量的下标表示重压缩的次数，我们实验设置了<spanclass="math inline">\(k=7\)</span>。<spanclass="math inline">\(RT(.)\)</span>是四舍五入和截断操作。<spanclass="math inline">\([.]\)</span>表示该舍入操作。</p><p>  然后，将k次重压缩后的残余去量化DCT系数R定义为：</p><p><spanclass="math display">\[R=\frac{1}{k}\sum_{i=1}^{k}(Q_i-Q_{i-1})\]</span>  对于原始的通道系数<spanclass="math inline">\(Q_0\)</span>​，在把它们转换成一个二进制卷之后，我们使用一个阈值<spanclass="math inline">\(T\)</span>​来执行一个剪切操作，将此二进制值转换表示为：</p><p><span class="math display">\[f:Q_o^{H\times W}\rightarrow\{0,1\}^{(T+1)\times H\times W}\]</span>   DCT系数<spanclass="math inline">\(Q_0\)</span>​被转换为二进制：</p><p><span class="math display">\[f(Q_0(i,j))=\begin{cases}1, &amp;if|clip(Q_0(i,j))|=t,t\in [0,T]\\\\0, &amp;otherwise\end{cases}\]</span></p><p>  利用<span class="math inline">\(clip(.)\)</span>提取<spanclass="math inline">\([−T,T]\)</span>中的直方图特征，这对GPU内存约束是必不可少的。我们实验中将T设为20。此外，我们应用绝对值运算作为DCT直方图显示的对称性。<br/><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png"alt="image-20240326170313123" /></p><p>  在频率流中，首先将图像输入到图5所示的<strong>压缩伪影学习模型</strong>中，提取各种DCT特征。随后，DCT特性被输入到HR-Net的一个变体中，该变体在三种不同的分辨率（1/8、1/16和1/32）下运行。</p><h2 id="注意力空间金字塔池aspp">注意力空间金字塔池ASPP</h2><p>  为了精确地定位小的篡改区域，我们使用图6(a)所示的注意力空间金字塔池（ASPP）仔细设计了我们的模型。ASPP通过不同的接受域捕获远程距离信息，并处理尺度变化。它由三个具有不同速率的扩张卷积层和一个全局平均池化（GAP）组成。得到的特征被连接并传递到1×1卷积。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095235566.png" alt="image-20240329095235566 " style="zoom:50%;" /></p><h2 id="注意力交互机制">注意力交互机制</h2><p>其中</p><ul><li>左边输入为四个分辨率的分支</li><li>CA：通道注意力ChannelAttention</li><li>UP：双线性上采样BilinearUp-sampling</li><li>SA：空间注意力SpatialAttention</li></ul><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329100333958.png" alt="image-20240329100333958 " style="zoom:45%;" /></p><p>接下来，我们将描述注意力如何在RGB流中交互式地工作，其中的过程实际上与频率流相同，具有不同数量的输出分辨率分支。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329141519802.png" alt="image-20240329141519802 " style="zoom:50%;" /></p><h3 id="通道注意力channelattention">通道注意力ChannelAttention</h3><p>左侧输入为HRnet不同分辨率下的输入：</p><p><span class="math display">\[I\in R^{H \times W \times3}\rightarrow\begin{cases}\begin{aligned}F_1 \in R^{H/4 \times W/4\times C_1} &amp;  &amp; ,C_1=48\\F_2 \in R^{H/8 \times W/8 \times C_2}&amp;  &amp; ,C_2=96\\F_3 \in R^{H/16 \times W/16 \times C_3}&amp;  &amp; ,C_3=192\\F_4 \in R^{H/32 \times W/32 \times C_4}&amp;  &amp; ,C_4=384\\\end{aligned}\end{cases}\]</span>自下而上的通道注意特征的计算使用如下： <span class="math display">\[F_n= C(F_{n+1})\odot F_n, n = 1,2,3\]</span> 其中，<spanclass="math inline">\(C(.)\)</span>表示通道注意块，如下图所示，<spanclass="math inline">\(\odot\)</span>表示元素级乘法。由于<spanclass="math inline">\(F_4\)</span>包含了最高级别的语义信息，因此它在通道级别上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329095324781.png" alt="image-20240329095324781 " style="zoom:50%;" /><span class="math display">\[C(F)=\sigma(E(GAP(Conv_{1\times1}(F))))\]</span></p><p>  其中1×1卷积，以减少信道，GAP（·）为全局平均池，激励过程 <spanclass="math inline">\(E(.)=C^{&#39;}\rightarrow C^{&#39;}/r \rightarrowC^{&#39;}，r=4\)</span>，<span class="math inline">\(\sigma(.)\)</span>为Sigmoid激活函数。</p><h3id="双线性上采样bilinearup-sampling">双线性上采样BilinearUp-sampling</h3><p>在应用自底而上的信道注意后，使用双线性上采样方法对特征图<spanclass="math inline">\(F_2\)</span>、<spanclass="math inline">\(F_3\)</span>和<spanclass="math inline">\(F_4\)</span>进行上采样，以匹配<spanclass="math inline">\(F_1\)</span>的分辨率。</p><h2 id="空间注意力spatialattention">空间注意力SpatialAttention</h2><p>应用自上而下路径的空间注意机制，由： <spanclass="math display">\[F_m=S(F_{m-1})\otimes F_m,m = 2, 3, 4,\]</span>其中，S(.)为空间注意力，如下图所示。由于<spanclass="math inline">\(F_1\)</span>包含了丰富的空间信息，因此在空间层面上保持不变。</p><p><img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240329142515201.png" alt="image-20240329142515201 " style="zoom:50%;" /></p><h2 id="热图聚合heatmapaggregation">热图聚合HeatmapAggregation</h2><p>  每个分支的特征图在经过上采样和交互注意后，具有相同的分辨率。然后将这些特征连接在一起，形成最终特征，用于推理阶段的自适应加权热图聚合。</p><p>  我们的模型生成了两个最终的热图，它们通过软选择进行聚合。具体来说，我们采用双线性特征上采样来升级频率流的热图，以匹配RGB流热图的分辨率。然后，我们将Softmax激活函数应用于热图，然后使用全局最大池化（GMP），记为GMP（·），来选择主热图及其相应的权重。这种选择是基于更高的值，这表明与其他热图相比，它具有更强的定位响应。</p><p>  我们使用<span class="math inline">\(h_m\)</span>和<spanclass="math inline">\(h_s\)</span>​​定义主热图和次热图。因此，加权聚合热图h可以表示为：</p><p><span class="math display">\[h = GMP(h_m)\cdot h_m+(1-GMP(h_m))\cdoth_s\]</span></p><p>  最后，我们在预测的二值掩模上应用一个不可训练的GMP来执行图像级检测，因为图像级检测与像素级预测高度相关。</p><h1 id="实验">实验</h1><h2 id="实验设置">实验设置</h2><p><strong>数据集。</strong>本研究中使用的训练数据集大多采用了（Kwonetal.2022），其中包括CASIAv2、FantasticReality、IMD2020，以及专门为使用不同QFs检测压缩伪影而设计的数据集。测试阶段需要使用CIMD-R和CIMD-C来分别评估基于图像编辑和基于压缩的方法的有效性。补充材料中提供了有关所使用的数据集的进一步细节，以及与选定的公开可访问的数据集进行的评价指标的比较分析。</p><p><strong>实施细节。</strong>我们的模型是使用PyTorch（Paszke等人，2019年）实现的，并在8个RTX2080GPUs，上进行训练，批处理大小为4。我们将初始学习率设置为0.001，并呈指数衰减。为了减轻不平衡数据集对模型训练的影响，我们从每批的每个数据集中随机选择相同数量的样本。训练过程总共250批。该模型被设计为接受各种图像格式，包括JPEG和非JPEG格式。对于非jpeg图像，该模型采用全1的Q矩阵对样本进行压缩，这相当于使用100的QF进行无损压缩。RGB流的主干使用ImageNet进行预训练（(Krizhevsky,Sutskever,andHinton2017），而DCT流使用（Parketal.2018a）引入的双压缩图像进行预训练的方法。为了提高模型检测小篡改区域的灵敏度，设计了训练目标来<strong>最小化像素级的二值交叉熵损失</strong>。</p><p><strong>比较网络的选择。</strong>为了保证公平的比较和评估之前使用新引入的CIMD的模型，我们选择了使用这两个标准的最先进的方法：(1)预训练模型是公开的，(2)我们使用的评估数据集不在它们的训练集中。根据这些标准，我们选择了RRU-Net、MantraNet、CR-CNN、SPAN、PSCC-Net、MVSS-Net、IF-OSN、CAT-Net、DJPEG和对照品。其中，DJPEG和Comprint被设计用于压缩伪影检测，而CATNet可以联合检测异常特征和压缩伪影。上述所有的研究均在相关的工作部分中被适当地引用。我们使用CIMD-R来评估基于图像编辑的方法，用CIMD-C来评估基于压缩的方法。</p><h2 id="在cimd-r上的评估结果">在CIMD-R上的评估结果</h2><p>  使用CIMD-R子集进行评估。表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213659139.png"alt="image-20240327213659139" /><figcaption aria-hidden="true">image-20240327213659139</figcaption></figure><p>表1报告了使用CIMD-R的基于图像编辑的方法的结果，其中所有图像样本都未压缩。像素级f1评分使用每张图像的最佳f1阈值，并使用固定的f1阈值0.5。最佳分数用粗体突出显示。我们的方法在图像级和像素级的检测任务中都取得了最好的性能。值得注意的是，我们的方法在图像级和像素级评估方面都优于现有的SOTA方法，这表明了它在检测小篡改区域方面的优越性。</p><h2id="在cimd-c上评价基于压缩的方法的评估结果">在CIMD-C上评价基于压缩的方法的评估结果</h2><p>  表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327213738541.png"alt="image-20240327213738541" /><figcaption aria-hidden="true">image-20240327213738541</figcaption></figure><p>表2比较了基于压缩的IMD方法的性能，其中所有图像样本都使用相同的QF进行双压缩，评估设置与表1中使用的设置一致。我们的方法在整体性能方面仍然是表现最好的，突出了我们的方法对于具有相同QF的双压缩图像的有效性。</p><h2 id="消融研究">消融研究</h2><p>  我们提供了一个如表3所示的简单的消融研究。请注意，我们的RGB流在压缩的数据和未压缩的数据中都是有效的。</p><figure><imgsrc="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240327214430772.png"alt="image-20240327214430772" /><figcaption aria-hidden="true">image-20240327214430772</figcaption></figure><p>  值得注意的是，由于没有压缩伪影，频率流在CIMD-R中不能产生令人满意的结果。然而，当这两个分支协同工作时，模型的性能在定位和检测评估方面都有所提高。在补充材料中提供了额外的烧蚀研究和实验结果。</p><h1 id="结论">结论</h1><p>  本研究提出了一种新的具有挑战性的图像处理检测（CIMD）数据集，它包括两个子集，分别用于评估基于图像编辑和基于压缩的方法。这些数据集被手动获取和篡改，并提供了高质量的注释。此外，我们提出了一种双分支的方法，在使用CIMD数据集检测图像操作方面优于最先进的模型。我们已经发布了我们的数据集，以促进未来的研究。</p>]]></content>
      
      
      <categories>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NP++ </tag>
            
            <tag> SRM卷积 </tag>
            
            <tag> BayarConv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Multi-Modal Fusion for Image Manipulation Detection and Localization</title>
      <link href="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/"/>
      <url>/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/</url>
      
        <content type="html"><![CDATA[<p>Exploring Multi-Modal Fusion for Image Manipulation Detection andLocalization <a href="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></p><p><ahref="https://paperswithcode.com/sota/image-manipulation-localization-on-casia-v1?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-localization-on-casia-v1"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-detection-on-casia-v1?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-detection-on-casia-v1"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-localization-on-cocoglide?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-localization-on-cocoglide"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-detection-on-cocoglide?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-detection-on-cocoglide"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-localization-on-columbia?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-localization-on-columbia"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-detection-on-columbia?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-detection-on-columbia"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-localization-on-coverage?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-localization-on-coverage"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-detection-on-coverage?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-detection-on-coverage"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-localization-on-dso-1?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-localization-on-dso-1"alt="PWC" /></a><br/><ahref="https://paperswithcode.com/sota/image-manipulation-detection-on-dso-1?p=exploring-multi-modal-fusion-for-image"><imgsrc="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/exploring-multi-modal-fusion-for-image/image-manipulation-detection-on-dso-1"alt="PWC" /></a></p><p>希腊信息技术研究所，研究和技术研究中心，希腊塞萨洛尼基</p><details close><br/><summary>论文（arxiv）</summary><br/><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">post1</a><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization2/">post2</a><br/><div class="row">    <embed src="/postpdfs/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/2312.01790.pdf" width="100%" height="550" type="application/pdf"></div><br/></details><h2 id="摘要">摘要</h2><p>最近的图像操作定位和检测技术通常利用由噪声敏感滤波器产生的法医伪影和痕迹，如SRM和Bayar卷积。</p><p>在本文中，我们展示了在这种方法中常用的不同过滤器擅长于揭示不同类型的操作，并提供互补的法医痕迹。因此，我们探索了合并这些滤波器输出的方法，其目的是利用所产生的伪影的互补性来执行图像操作定位和检测（IMLD）。</p><p>我们提出了两种不同的方法：一种是从每个法医过滤器产生独立的特征，然后将它们融合（称为晚期融合），另一种是执行不同模态输出的早期混合并产生早期组合特征（这称为早期融合）。</p><p>我们证明了这两种方法在图像操作定位和检测方面都取得了具有竞争力的性能，在多个数据集上优于最先进的模型1。</p><h2 id="方法">方法</h2><h3 id="编码器解码器框架">编码器解码器框架</h3><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/architecture.png"alt="Architecture" /><figcaption aria-hidden="true">Architecture</figcaption></figure><p>图片分别经过阶梯分析丰富模型SRM（高通滤波器） , bayar卷积之后和通过NoisePrint++提取的特征送入多尺度编码器进行编码，之后分别通过异常检测解码器和置信度解码器获得预测图和置信图，最后池化后通过篡改解码器，得到篡改可能分数。</p><h3 id="特征融合方法大模型">特征融合方法（大模型）：</h3><p>首先分别从NoisePrint++、SRM和bayar卷积中提取RGB图像x的辅助特征。然后将每个辅助特征与原始RGB一起输入到一个双分支CMX编码器中，生成4尺度的特征图如图所示：</p><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213343139.png"alt="image-20240328213343139" /><figcaption aria-hidden="true">image-20240328213343139</figcaption></figure><p>在每个尺度上，3个编码器的输出被连接起来，以产生编码器的最终输出f。我们使用与TruFor中相同的解码器架构来处理异常和置信解码器。</p><h3id="提出的另外一种特征融合方法小模型">提出的另外一种特征融合方法（小模型）：</h3><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213412073.png"alt="image-20240328213412073" /><figcaption aria-hidden="true">image-20240328213412073</figcaption></figure><p>再次提取了RGB图像x的辅助特征、rbayar。然后每个输入通过卷积块C，生成早期特征fmod。然后将这3组特征映射连接起来，生成完整的早期特征集fef。这些特征然后通过另一个卷积块C，产生混合特征f mf = C（fef）。混合特征fmf和RGB图像x被用作双分支CMX编码器[34]的输入，其方式与TruFor中的相同。</p><p>这是一种特别轻量级的方法来扩展TruFor架构以处理多个辅助模式，因为它不会显著增加参数的数量（与TruFor的68.7M相比，是68.9M参数）。</p><h2 id="实验">实验：</h2><h3 id="与其他方法在f1参数上的比较">与其他方法在F1参数上的比较：</h3><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213436800.png"alt="image-20240328213436800" /><figcaption aria-hidden="true">image-20240328213436800</figcaption></figure><p>其提出的两种特征融合方式除了在DSO-1上不如TruFor，剩下的都好于TruFor<br/>特别是对于只包含复制移动伪造的覆盖数据集，我们的最佳方法比之前的最佳方法TruFor高了6.3%。</p><p>DSO-1数据集主要用于检测包含人的拼接图像。比TruFor落后3%。</p><h3 id="专门与trufor比较结果">专门与TruFor比较结果：</h3><p><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213450185.png"alt="image-20240328213450185" />认为只是误差。</p><h3 id="与其他方法在auc参数上的比较">与其他方法在AUC参数上的比较：</h3><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213506766.png"alt="image-20240328213506766" /><figcaption aria-hidden="true">image-20240328213506766</figcaption></figure><p>可以发现</p><p>小模型显示出了卓越的性能，超过了最先进的平均水平。与之前的领先方法相比，AUC实现了近7%的显著改进，bAcc面实现了9%的显著改进。</p><p>大模型也表现出具有竞争力的AUC性能，但在bAcc方面略落后于TruFor模型。bAcc性能的这种差异可能归因于大模型的大小，这可能容易发生过拟合。进一步的研究和实验需要探索需要额外的正则化技术的可能性，以优化其性能的检测任务。</p><h2 id="消融实验">消融实验：</h2><p>多模态特征输入的消融：</p><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213528368.png"alt="image-20240328213528368" /><figcaption aria-hidden="true">image-20240328213528368</figcaption></figure><p>在本节中，为了对比各种过滤器（SRM，Bayarconv，NoisePrint++），采用了一个双分支CMX架构，其中每个过滤器作为RGB图像的辅助输入。</p><p>结果见表6。在这个训练过程中，bayar卷积层是可训练的，而SRM和NoisePrint则保持冻结。</p><p>我们可以看到，NoisePrint++基于编辑历史的训练有助于实现DSO-1的最佳性能，其中使用后处理操作覆盖操作，而SRM和bayar在编码和覆盖方面表现更好。</p><p>覆盖范围只包含复制移动操作，噪音打印的相机模型识别可能无法提供足够强大的法医痕迹，而编码道的操作是基于扩散的内画，可能导致不同于传统编辑历史的独特文物。因此，噪音打印在有效处理此类案件时遇到了困难。</p><h2 id="鲁棒性分析"><strong>鲁棒性分析：</strong></h2><p>观察在后处理程度逐渐加深下，模型性能的变化</p><p>在本节中，我们包括对不同质量下降的图像进行的实验，以证明我们的方法的鲁棒性。我们使用Casiav1+数据集，使用不同的内核大小进行高斯模糊，使用不同的质量因子进行JPEG压缩，并与TruFor进行比较。</p><figure><imgsrc="/postimages/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/image-20240328213554181.png"alt="image-20240328213554181" /><figcaption aria-hidden="true">image-20240328213554181</figcaption></figure><p>图4中描述的结果表明，我们的两种融合方法在广泛的降解过程中都表现出良好的鲁棒性，在所使用的所有降解水平上保持了比TruFor的一致优势。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
          <category> 篡改检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NP++ </tag>
            
            <tag> SRM卷积 </tag>
            
            <tag> BayarConv </tag>
            
            <tag> 数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像篡改检测定位</title>
      <link href="/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/"/>
      <url>/%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E5%AE%9A%E4%BD%8D/</url>
      
        <content type="html"><![CDATA[<p><a href="/论文总集/">已读论文汇总</a></p><blockquote><p>橘色为A类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/A类-年份-red"alt="A类" /></a><br/>&gt; 黄色为B类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/B类-年份-yellow"alt="B类" /></a><br/>&gt; 绿色为C类会议、期刊:<a href=""><imgsrc="https://img.shields.io/badge/C类-年份-green" alt="C类" /></a></p></blockquote><h3 id="backbone">Backbone</h3><p><strong>骨干网络</strong>，多为图像分类的网络。</p><ul class="task-list"><li><label><input type="checkbox" />[Attention Is All You Need] <ahref="https://proceedings.neurips.cc/paper/7181-attention-is-all"><imgsrc="https://img.shields.io/badge/NeurIPS-2017-red" alt="AAAI" /></a><ahref="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py"><imgsrc="https://img.shields.io/badge/Github-official-blue"alt="GitHub" /></a><ahref="https://github.com/jadore801120/attention-is-all-you-need-pytorch"><imgsrc="https://img.shields.io/badge/Github-community-blue"alt="GitHub" /></a></label></li><li><label><input type="checkbox" />[EfficientNet: Rethinking ModelScaling for Convolutional Neural Networks] (<em>ICML '19</em>)<strong>[</strong><ahref="https://arxiv.org/abs/1905.11946"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale] (<em>ICLR '21</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2010.11929"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/google-research/vision_transformer"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Multi-Dimensional Model Compressionof Vision Transformer] (<em>ICME '22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2201.00043"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Deep Residual Learning for ImageRecognition (<em>CVPR '16</em>) <strong>[</strong><ahref="https://arxiv.org/abs/1512.03385"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Generative Adversarial Networks(<em>NeurIPS '14</em>) <strong>[</strong><ahref="https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Masked Auto-Encoders Meet GenerativeAdversarial Networks and Beyond (<em>CVPR '23</em>) <strong>[</strong><ahref="https://feizc.github.io/resume/ganmae.pdf"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />A Kernel Perspective of SkipConnections in Convolutional Networks (<em>ICLR '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2211.14810"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />EfficientViT: Memory EfficientVision Transformer with Cascaded Group Attention (<em>CVPR '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2305.07027"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/microsoft/Cream/tree/main/EfficientViT"><strong>Code</strong></a><strong>]</strong><strong>[</strong><ahref="https://blog.csdn.net/P_LarT/article/details/130687567"><strong>Note_community</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Vision Transformers Need Registers<em>(ICLR '24)</em> <strong>[<ahref="https://openreview.net/forum?id=2dnO3LLiJ1">Paper</a>]</strong></label></li><li><label><input type="checkbox" /><a href="/SAM/">SegmentAnything</a><a href="https://arxiv.org/abs/2304.02643"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/facebookresearch/segment-anything"><imgsrc="https://img.shields.io/github/stars/facebookresearch/segment-anything?style=flat"alt="GitHub" /></a></label></li></ul><h3 id="image-tampering">Image Tampering</h3><h4 id="image-editing">Image Editing</h4><details open><br/><summary>2024</summary><ul class="task-list"><li><label><input type="checkbox" checked="" /><ahref="/Exploring-Multi-Modal-Fusion-for-Image-Manipulation-Detection-and-Localization/">ExploringMulti-Modal Fusion for Image Manipulation Detection and Localization</a><ahref="https://link.springer.com/chapter/10.1007/978-3-031-53311-2_15"><imgsrc="https://img.shields.io/badge/MMM-2024-green" alt="MMM" /></a> <ahref="https://arxiv.org/abs/2312.01790"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/idt-iti/mmfusion-iml"><imgsrc="https://img.shields.io/github/stars/idt-iti/mmfusion-iml?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">ANew Benchmark and Model for Challenging Image Manipulation Detection</a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><imgsrc="https://img.shields.io/badge/AAAI-2024-red" alt="AAAI" /></a> <ahref="https://arxiv.org/abs/2311.14218"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/ZhenfeiZ/CIMD"><imgsrc="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer:Mask-Guided Query-Based Transformer for Image ManipulationLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">LearningDiscriminative Noise Guidance for Image Forgery Detection andLocalization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRomSplines-Based Regression for Image Forgery Localization</a> <ahref="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><imgsrc="https://img.shields.io/badge/AAAI-2024-red"alt="AAAI" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/UnionFormer/">UnionFormer: Unified-Learning Transformer withMulti-View Representation for Image Manipulation Detection andLocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-red"alt="CVPR" /></a></label></li><li><label><input type="checkbox" /><a href="/DH-GAN/">DH-GAN: Imagemanipulation localization via a dual homology-aware generativeadversarial network</a> <ahref="https://doi.org/10.1016/j.patcog.2024.110658"><imgsrc="https://img.shields.io/badge/PR-2024-yellow"alt="PR" /></a></label></li></ul><details open><br/><summary>2023</summary><ul class="task-list"><li><label><input type="checkbox" /><ahref="/TruFor-Leveraging-all-round-clues-for-trustworthy-image-forgery-detection-and-localization/">TruFor:Leveraging all-round clues for trustworthy image forgery detection andlocalization</a> <ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2023-red" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2212.10957"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/grip-unina/TruFor"><imgsrc="https://img.shields.io/github/stars/grip-unina/TruFor?style=flat"alt="GitHub" /></a> <a href="https://grip-unina.github.io/TruFor/"><imgsrc="https://img.shields.io/badge/project-blue"alt="project" /></a></label></li><li><label><input type="checkbox" checked="" /><ahref="/CFL-Net-Image-Forgery-Localization-Using-Contrastive-Learning/">CFL-Net:Image Forgery Localization Using Contrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/WACV2023/html/Niloy_CFL-Net_Image_Forgery_Localization_Using_Contrastive_Learning_WACV_2023_paper.html"><imgsrc="https://img.shields.io/badge/WACV-2023-yellow" alt="CVPR" /></a> <ahref="https://arxiv.org/abs/2210.02182"><imgsrc="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv" /></a><a href="https://github.com/Kishor-Bhaumik/CFLNet"><imgsrc="https://img.shields.io/github/stars/Kishor-Bhaumik/CFLNet?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Towards-Generic-Image-Manipulation-Detection-with-Weakly-Supervised-Self-Consistency-Learning/">TowardsGeneric Image Manipulation Detection with Weakly-SupervisedSelf-Consistency Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/yhZhai/WSCL"><imgsrc="https://img.shields.io/github/stars/yhZhai/WSCL?style=flat"alt="GitHub" /></a> <ahref="https://www.researchgate.net/publication/373686108_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learningl"><imgsrc="https://img.shields.io/badge/ResearchGate-blue"alt="ResearchGate" /></a></label></li><li><label><input type="checkbox" /><ahref="/Pre-training-free-Image-Manipulation-Localization-through-Non-Mutually-Exclusive-Contrastive-Learning/">Pre-training-freeImage Manipulation Localization through Non-Mutually ExclusiveContrastive Learning</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red" alt="ICCV" /></a> <ahref="https://github.com/Knightzjz/NCL-IML"><imgsrc="https://img.shields.io/github/stars/Knightzjz/NCL-IML?style=flat"alt="GitHub" /></a></label></li><li><label><input type="checkbox" /><ahref="/Uncertainty-Uncertainty-Learning-for-Improving-Image-Manipulation-Detection/">Uncertainty-UncertaintyLearning for Improving Image Manipulation Detection</a> <ahref="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Uncertainty-guided_Learning_for_Improving_Image_Manipulation_Detection_ICCV_2023_paper.html"><imgsrc="https://img.shields.io/badge/ICCV-2023-red"alt="ICCV" /></a></label></li></ul></details></details><details close><br/><summary>2022</summary><ul class="task-list"><li><p><label><input type="checkbox" /><ahref="/PSCC-Net-Progressive-Spatio-Channel-Correlation-Network-for-Image-Manipulation-Detection-and-Localization/">PSCC-Net:Progressive Spatio-Channel Correlation Network for Image ManipulationDetection and Localization</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903/"><imgsrc="https://img.shields.io/badge/TCSVT-2022-yellow" alt="TCSVT" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2103.10596"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/proteus1991/PSCC-Net"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" /><ahref="/MVSS-Net-Multi-View-Multi-Scale-Supervised-Networks-for-Image-Manipulation-Detection/">MVSS-Net:Multi-View Multi-Scale Supervised Networks for Image ManipulationDetection</a> <ahref="https://ieeexplore.ieee.org/abstract/document/9819903"><imgsrc="https://img.shields.io/badge/TPAMI-2022-red" alt="TPAMI" /></a><strong>[</strong><ahref="https://arxiv.org/abs/2112.08935"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/dong03/MVSS-Net"><strong>Code</strong></a><strong>]</strong></label></p><details close><p><br/><br /></p></li><li><p><label><input type="checkbox" />DS-UNet: A dual streams UNet forrefined image forgery localization <em>(InfoS '22)</em> <strong>[<ahref="https://dl.acm.org/doi/abs/10.1016/j.ins.2022.08.005">Paper</a>]</strong></label></p></li><li><p><label><input type="checkbox" />MSMG-Net: Multi-scaleMulti-grained Supervised Metworks for Multi-task Image ManipulationDetection and Localization (<em>ArXiv '22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2211.03140"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />Towards JPEG-Resistant ImageForgery Detection and Localization Via Self-Supervised Domain Adaptation(<em>TPAMI '22</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9904872"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />ESRNet: Efficient Search andRecognition Network for Image Manipulation Detection (<em>TOMCCAP'22</em>) <strong>[</strong><ahref="https://doi.org/10.1145/3506853"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/tampered816/rrr"><strong>Tool</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />Learning to localize imageforgery using end-to-end attention network (<em>Neurocomputing '22</em>)<strong>[</strong><ahref="https://www.sciencedirect.com/science/article/pii/S0925231222011274"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/sadaf-ali/-Learning-to-Localize-Image-Forgery-Using-End-to-End-Attention-Network"><strong>Code</strong></a><strong>]</strong><br/><br /></label></p></li><li><p><label><input type="checkbox" />[Robust Image Forgery DetectionOver Online Social Network Shared Images] (<em>CVPR '22</em>)<strong>[</strong><ahref="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Robust_Image_Forgery_Detection_Over_Online_Social_Network_Shared_Images_CVPR_2022_paper.pdf"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/HighwayWu/ImageForensicsOSN"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[ObjectFormer for ImageManipulation Detection and Localization] (<em>CVPR '22</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2203.14681"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />GCA-Net: Utilizing Gated ContextAttention for Improving Image Forgery Localization and Detection(<em>CVPRW '22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2112.04298"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />Non-Semantic Evaluation of ImageForensics Tools: Methodology and Database (<em>WACV '22</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2105.02700"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/qbammey/trace"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[JPEG Compression-aware ImageForgery Localization] (<em>MM '22</em>) <strong>[</strong><ahref="https://dl.acm.org/doi/abs/10.1145/3503161.3547749"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[Image Manipulation LocalizationUsing Multi-Scale Feature Fusion and Adaptive Edge Supervision] (<em>TMM'22</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9996125/"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[Self-Adversarial Trainingincorporating Forgery Attention for Image Forgery Localization](<em>TIFS '22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2107.02434"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/tansq/SATFL"><strong>Code</strong></a><strong>]</strong>(<em>LocateNet / SATFL</em>)</label></p></li><li><p><label><input type="checkbox" />M2TR: Multi-modal Multi-scaleTransformers for Deepfake Detection (<em>ICMR '22</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2104.09770"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/wangjk666/M2TR-Multi-modal-Multi-scale-Transformers-for-Deepfake-Detection"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />A Principled Design of ImageRepresentation: Towards Forensic Tasks (<em>TPAMI '22</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2203.00913"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/ShurenQi/DIR"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[TBNet: A Two-streamBoundary-aware Network for Generic Image Manipulation Localization](<em>KDE '22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2108.04508"><strong>Paper</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[Learning JPEG CompressionArtifacts for Image Manipulation Detection and Localization] (<em>IJCV'22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2108.12947"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/mjkwon2021/CAT-Net"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />Fighting Malicious Media Data: ASurvey on Tampering Detection and Deepfake Detection (<em>arXiv'22</em>) (<strong>Survey</strong>) <strong>[</strong><ahref="https://arxiv.org/abs/2212.05667"><strong>Paper</strong></a><strong>]</strong><br/></label></p></details><p><br/></p></details></li></ul><details close><br/><summary>2021</summary><ul class="task-list"><li><label><input type="checkbox" />MSTA-Net: Forgery Detection byGenerating Manipulation Trace Based on Multi-Scale Self-TextureAttention (<em>TCSVT '21</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9643421"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Image Manipulation Detection byMulti-View Multi-Scale Supervision (<em>ICCV '21</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2104.06832"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/dong03/MVSS-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[TransForensics: Image ForgeryLocalization with Dense Self-Attention] (<em>ICCV '21</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2108.03871"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Self-supervised Domain Adaptationfor Forgery Localization of JPEG Compressed Images (<em>ICCV '21</em>)<strong>[</strong><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Rao_Self-Supervised_Domain_Adaptation_for_Forgery_Localization_of_JPEG_Compressed_Images_ICCV_2021_paper.html"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Image Tampering Localization Using aDense Fully Convolutional Network (<em>TIFS '21</em>)<strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9393396"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/ZhuangPeiyu/Dense-FCN-for-tampering-localization"><strong>Code</strong></a><strong>]</strong>(<em>DenseFCN</em>)</label></li><li><label><input type="checkbox" />Image Manipulation LocalizationUsing Attentional Cross-Domain CNN Features (<em>TNNLS '21</em>)<strong>[</strong><ahref="https://doi.org/10.1109/TNNLS.2021.3130168"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><details close><br/><summary>2020 and before</summary><ul class="task-list"><li><p><label><input type="checkbox" />ManTra-Net: Manipulation TracingNetwork for Detection and Localization of Image Forgeries With AnomalousFeatures (<em>CVPR '19</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/8953774"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/ISICV/ManTraNet"><strong>Code</strong></a><strong>]</strong><br/><ahref="https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_ManTra-Net_Manipulation_Tracing_Network_for_Detection_and_Localization_of_Image_CVPR_2019_paper.html"><imgsrc="https://img.shields.io/badge/CVPR-2024-red" alt="CVPR" /></a> <ahref="https://github.com/ISICV/ManTraNet"><imgsrc="https://img.shields.io/github/stars/ISICV/ManTraNet?style=flat"alt="GitHub" /></a></label></p><details close></li><li><p><label><input type="checkbox" />A Full-Image Full-ResolutionEnd-to-EndTrainable CNN Framework for Image Forgery Detection (<em>IEEEAccess '20</em>) **[<a href="./">Paper</a>]</label></p></li><li><p><label><input type="checkbox" /><imgsrc="https://img.shields.io/badge/ICME-&#39;20-ffc53d.svg"alt="ConfnPubs" />Constrained R-CNN: A general image manipulationdetection model <strong>[</strong><ahref="https://arxiv.org/abs/1911.08217"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/VedantWani/Constrained-R-CNN"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />A CNNBased Camera ModelFingerprint (<em>TIFS '20</em>) **[<ahref="./">Paper</a>]</label></p></li><li><p><label><input type="checkbox" />An Adaptive Neural Network forUnsupervised Mosaic Consistency Analysis in Image Forensics (<em>CVPR'20</em>) <strong>[</strong><ahref="http://openaccess.thecvf.com/content_CVPR_2020/html/Bammey_An_Adaptive_Neural_Network_for_Unsupervised_Mosaic_Consistency_Analysis_in_CVPR_2020_paper.html"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/qbammey/adaptive_cfa_forensics"><strong>Code</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />A dense u-net with cross-layerintersection for detection and localization of image forgery (<em>ICASSP'20</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9054068"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://blog.csdn.net/weixin_45366180/article/details/128413821"><strong>Note_unofficial</strong></a><strong>]</strong></label></p></li><li><p><label><input type="checkbox" />[Generate, Segment, and Refine:Towards Generic Manipulation Segmentation] (<em>AAAI '20</em>)<strong>[</strong><ahref="https://arxiv.org/abs/1811.09729"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/pengzhou1108/GSRNet"><strong>Code</strong></a><strong>]</strong>(<em>GSRNet</em>)</label></p></li><li><p><label><input type="checkbox" />SPAN: Spatial Pyramid AttentionNetwork for Image Manipulation Localization (<em>ECCV '20</em>)<strong>[</strong><ahref="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660307.pdf"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/tsaishien-chen/SPAN"><strong>Code</strong></a><strong>]</strong><a href="http://media.ee.ntu.edu.tw/research/SPAN/"><imgsrc="https://img.shields.io/badge/Project-Page-159957.svg"alt="GitHub Page" /></a></label></p></li><li><p><label><input type="checkbox" />[Hybrid LSTM and Encoder-DecoderArchitecture for Detection of Image Forgeries] (<em>TIP '19</em>)<strong>[</strong><ahref="https://arxiv.org/abs/1903.02495"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/jawadbappy/forgery_localization_HLED"><strong>Code</strong></a><strong>]</strong><br/><br/></label></p></details><p><br/></p></details></li></ul><h4 id="cnn-synthesized">CNN-synthesized</h4><p><em>上面的一些论文还包含了检测由合成图像的GANs或DM生成的篡改图像的方法</em></p><details close><br/><summary>CNN-synthesized</summary><ul class="task-list"><li><label><input type="checkbox" />Forgery-aware Adaptive Transformerfor Generalizable Synthetic Image Detection <em>(CVPR '24)</em><strong>[<ahref="https://arxiv.org/abs/2312.16649">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Preserving Fairness Generalizationin Deepfake Detection <em>(CVPR '24)</em> <strong>[<ahref="https://arxiv.org/abs/2402.17229">Paper</a>]</strong> <strong>[<ahref="https://github.com/Purdue-M2/Fairness-Generalization">Code</a>]</strong></label></li><li><label><input type="checkbox" />Leveraging Representations fromIntermediate Encoder-blocks for Synthetic Image Detection <em>(arXiv'24)</em> <strong>[<ahref="https://arxiv.org/abs/2402.19091">Paper</a>]</strong> <strong>[<ahref="https://github.com/mever-team/rine">Code</a>]</strong></label></li><li><label><input type="checkbox" />Forgery-aware Adaptive Transformerfor Generalizable Synthetic Image Detection <em>(arXiv '23)</em><strong>[<ahref="https://arxiv.org/abs/2312.16649">Paper</a>]</strong></label></li><li><label><input type="checkbox" />AntifakePrompt: Prompt-TunedVision-Language Models are Fake Image Detectors (<em>arXiv '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2310.17419"><strong>Paper</strong></a><strong>]</strong><strong>[<ahref="https://github.com/nctu-eva-lab/AntifakePrompt">Code</a>]</strong></label></li><li><label><input type="checkbox" />MaLP: Manipulation LocalizationUsing a Proactive Scheme (<em>CVPR '23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2303.16976"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/vishal3477/pro_loc"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Discrepancy-Guided ReconstructionLearning for Image Forgery Detection (<em>IJCAI '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2304.13349"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/znshi/DisGRL"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Generalizable Synthetic ImageDetection via Language-guided Contrastive Learning (<em>arXiv '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2305.13800"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/HighwayWu/LASTED"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Detect Any Deepfakes: SegmentAnything Meets Face Forgery Detection and Localization (<em>arXiv'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2306.17075"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/laiyingxin2/DADF"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Discrepancy-Guided ReconstructionLearning for Image Forgery Detection (<em>arXiv '23</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2304.13349"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Masked Relation Learning forDeepFake Detection (<em>TIFS '23</em>) <strong>[</strong><ahref="https://doi.org/10.1109/TIFS.2023.3249566"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><p><br/>### Image Splicing</p><p><strong>图像的拼接篡改检测定位</strong></p><details close><br/><summary>2024</summary><ul class="task-list"><li><label><input type="checkbox" />Towards Effective Image Forensicsvia A Novel Computationally Efficient Framework and A New Image SpliceDataset <em>(arXiv '24)</em> <strong>[<ahref="https://arxiv.org/abs/2401.06998">Paper</a>]</strong></label></li></ul></details><details close><br/><summary>2023</summary><ul class="task-list"><li><label><input type="checkbox" />GreatSplicing: A Semantically RichSplicing Dataset <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2310.10070">Paper</a>]</strong> <strong>[<ahref="http://www.greatsplicing.net/">Dataset</a>]</strong></label></li><li><label><input type="checkbox" />Multi-scale attention context-awarenetwork for detection and localization of image splicing: Efficient androbust identification network <em>(Appl. Intell. 23')</em> <strong>[<ahref="https://link.springer.com/article/10.1007/s10489-022-04421-3">Paper</a>]</strong></label></li><li><label><input type="checkbox" />A Multi-Stream Fusion Network forImage Splicing Localization <em>(MMM '23)</em> <strong>[</strong><ahref="https://arxiv.org/abs/2212.01128"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Attacking Image Splicing Detectionand Localization Algorithms Using Synthetic Traces <em>(TIFS '23)</em><strong>[</strong><ahref="https://arxiv.org/abs/2211.12314"><strong>Paper</strong></a><strong>]</strong><strong>[<a href="https://doi.org/10.1109/TIFS.2023.3346312">IEEEPaper</a>]</strong></label></li><li><label><input type="checkbox" />Biomedical Image Splicing Detectionusing Uncertainty-Guided Refinement <em>(arXiv '23)</em><strong>[</strong><ahref="https://arxiv.org/abs/2309.16388"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />A New Method to Detect SplicingImage Forgery Using Convolutional Neural Network (<em>Applied Science(IF: 2.8, not included in CCFs), MDPI, '23</em>) <strong>[</strong><ahref="https://www.mdpi.com/2076-3417/13/3/1272"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Multi-scale Target-Aware Frameworkfor Constrained Image Splicing Detection and Localization (<em>MM'23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2308.09357"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><details close><br/><summary>2022</summary><ul class="task-list"><li><label><input type="checkbox" />[Multi-Task SE-Network for ImageSplicing Localization] (<em>TCSVT '22</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9591639"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/YulansZhang/Multi-task-SE-Network-for-Image-Splicing-Localization"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[ET: Edge-Enhanced Transformer forImage Splicing Detection] (<em>SPL '22</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9769936"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Image splicing forgery detection bycombining synthetic adversarial networks and hybrid dense U-net based onmultiple spaces] (<em>IJIS '22</em>) <strong>[</strong><ahref="https://doi.org/10.1002/int.22939"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/yelusaleng/SAN_and_HDU-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[SISL:Self-Supervised ImageSignature Learning for Splicing Detection &amp; Localization] (<em>CVPRW'22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2203.07824"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Deep Metric Color Embeddings forSplicing Localization in Severely Degraded Images (<em>TIFS '22</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2206.10737"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Coarse-to-fine-grained method forimage splicing region detection (<em>PR '22</em>) <strong>[</strong><ahref="https://doi.org/10.1016/j.patcog.2021.108347"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><details><br/><summary>2021</summary><ul class="task-list"><li><label><input type="checkbox" />[CAT-Net: Compression ArtifactTracing Network for Detection and Localization of Image Splicing](<em>WACV '21</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9423390"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/mjkwon2021/CAT-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Image Splicing Detection,Localization and Attribution via JPEG Primary Quantization MatrixEstimation and Clustering (<em>TIFS '21</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/9622213"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/andreacos/CnnJpegPrimaryQuantizationEstimation"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Multiple Image Splicing Dataset(MISD): A Dataset for Multiple Splicing (<em>Data, MDPI '21</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2108.09674"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Reality Transform AdversarialGenerators for Image Splicing Forgery Detection and Localization(<em>ICCV '21</em>) <strong>[</strong><ahref="http://openaccess.thecvf.com/content/ICCV2021/html/Bi_Reality_Transform_Adversarial_Generators_for_Image_Splicing_Forgery_Detection_and_ICCV_2021_paper.html"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Multi-Task Wavelet CorrectedNetwork for Image Splicing Forgery Detection and Localization] (<em>ICME'21</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/abstract/document/9428466/"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Detection and Localization ofMultiple Image Splicing Using MobileNet V1 (<em>IEEE Access '21</em>)<strong>[</strong><ahref="https://arxiv.org/abs/2108.09674"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><details><br/><summary>2020 and before</summary><ul class="task-list"><li><label><input type="checkbox" />D-Unet: A Dual-encoder U-Net forImage Splicing Forgery Detection and Localization <em>(arXiv '20)</em><strong>[</strong><ahref="https://arxiv.org/abs/2012.01821"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Exposing splicing forgery inrealistic scenes using deep fusion network (<em>InfoS '20</em>)<strong>[</strong><ahref="https://www.sciencedirect.com/science/article/pii/S0020025520302796"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Locating splicing forgery byadaptive-SVD noise estimation and vicinity noise descriptor(<em>Neurocomputing '20</em>) <strong>[</strong><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0925231220300278"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Adversarial Learning for ConstrainedImage Splicing Detection and Localization Based on Atrous Convolution(<em>TIFS '19</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/document/8658131"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/yaqiliu-cs/CISDL-DMAC"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[RRU-Net: The Ringed Residual U-Netfor Image Splicing Forgery Detection] (<em>CVPRW' 19</em>)<strong>[</strong><ahref="http://openaccess.thecvf.com/content_CVPRW_2019/html/CV-COPS/Bi_RRU-Net_The_Ringed_Residual_U-Net_for_Image_Splicing_Forgery_Detection_CVPRW_2019_paper.html?ref=https://githubhelp.com"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/yelusaleng/RRU-Net"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Mixed adversarial generators forimage splice detection (<em>NeuIPS '19</em>) <strong>[</strong><ahref="https://papers.nips.cc/paper/8315-the-point-where-reality-meets-fantasy-mixed-adversarial-generators-for-image-splice-detection"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/vlkniaz/MAGritte"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Image Splicing Localization viaSemi-global Network and Fully Connected Conditional Random Fields(<em>ECCV '18</em>)</label></li><li><label><input type="checkbox" />Image splicing localization using amulti-task fully convolutional network (mfcn) (<em>JVCIR '18</em>)<strong>[</strong><ahref="https://arxiv.org/abs/1709.02016"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/namtpham/image_tampering_detection_references.git"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Fighting Fake News: Image SpliceDetection via Learned Self-Consistency] (<em>ECCV '18</em>)<strong>[</strong><ahref="https://openaccess.thecvf.com/content_ECCV_2018/html/Jacob_Huh_Fighting_Fake_News_ECCV_2018_paper.html"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/minyoungg/selfconsistency"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />Deep Fusion Network for SplicingForgery Localization (<em>ECCV '18</em>)</label></li><li><label><input type="checkbox" />Deep matching and validationnetwork: An end-to-end solution to constrained image splicinglocalization and detection (<em>MM '17</em>) <strong>[</strong><ahref="https://arxiv.org/abs/1705.09765"><strong>Paper</strong></a><strong>]</strong></label></li></ul></details><h3 id="copy-move">Copy Move</h3><p><strong>复制移动篡改定位</strong>问题</p><details close><br/><summary>Copy Move</summary><ul class="task-list"><li><label><input type="checkbox" />An effective image copy-move forgerydetection using entropy image <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2312.11793">Paper</a>]</strong></label></li><li><label><input type="checkbox" />CMFDFormer: Transformer-basedCopy-Move Forgery Detection with Continual Learning <em>(arXiv '23)</em><strong>[<ahref="https://arxiv.org/abs/2311.13263">Paper</a>]</strong></label></li><li><label><input type="checkbox" />An approach for copy-move imagemultiple forgery detection based on an optimized pre-trained deeplearning model <em>(KBS '23)</em> <strong>[<ahref="https://www.sciencedirect.com/science/article/pii/S0950705123002587">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Image Copy-Move Forgery Detectionvia Deep Cross-Scale PatchMatch (<em>ICME '23</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2308.04188"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[BusterNet: Detecting Copy-MoveImage Forgery with Source/Target Localization] (<em>ECCV '18</em>)<strong>[</strong><ahref="http://openaccess.thecvf.com/content_ECCV_2018/html/Rex_Yue_Wu_BusterNet_Detecting_Copy-Move_ECCV_2018_paper.html"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/isi-vista/BusterNet"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[A Serial Image Copy-Move ForgeryLocalization Scheme With Source/Target Distinguishment] (<em>TMM'20</em>) <strong>[</strong><ahref="https://ieeexplore.ieee.org/abstract/document/9207851/"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/imagecbj/A-serial-image-copy-move-forgery-localization-scheme-with-source-target-distinguishment"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[DOA-GAN: Dual-Order AttentiveGenerative Adversarial Network for Image Copy-Move Forgery Detection andLocalization] (<em>CVPR '20</em>) <strong>[</strong><ahref="http://openaccess.thecvf.com/content_CVPR_2020/html/Islam_DOA-GAN_Dual-Order_Attentive_Generative_Adversarial_Network_for_Image_Copy-Move_Forgery_CVPR_2020_paper.html"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/asrafulashiq/doagan_clean"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Two-Stage Copy-Move ForgeryDetection with Self Deep Matching and Proposal SuperGlue] (<em>TIP'22</em>) <strong>[</strong><ahref="https://arxiv.org/abs/2012.08697"><strong>Paper</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />QDL-CMFD: A Quality-independent anddeep Learning-based Copy-Move image forgery detection method(<em>Neurocomputing '22</em>) <strong>[</strong><ahref="https://www.sciencedirect.com/science/article/pii/S0925231222011031"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/MehradAria/QDL-CMFD"><strong>Code</strong></a><strong>]</strong></label></li><li><label><input type="checkbox" />[Shrinking the Semantic Gap: SpatialPooling of Local Moment Invariants for Copy-Move Forgery Detection]<em>(TIFS '23)</em> <strong>[</strong><ahref="https://arxiv.org/abs/2207.09135"><strong>Paper</strong></a><strong>]</strong><strong>[</strong><ahref="https://github.com/ChaoWang1016/word2phraseCMFD"><strong>Code</strong></a><strong>]</strong></label></li></ul></details><h3 id="tamper-text-in-detection">Tamper Text in Detection</h3><p>图像中的<strong>文本篡改检测</strong>问题 (parts of)</p><details close><br/><summary>Tamper Text</summary><ul class="task-list"><li><label><input type="checkbox" />A Two-Stage Dual-Path Framework forText Tampering Detection and Recognition <em>(arXiv '24)</em><strong>[<ahref="https://arxiv.org/abs/2402.13545">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Toward Real Text ManipulationDetection: New Dataset and New Solution <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2312.06934">Paper</a>]</strong> <strong>[<ahref="https://github.com/DrLuo/RTM">Code</a>]</strong></label></li><li><label><input type="checkbox" />Towards Robust Tampered TextDetection in Document Image: New dataset and New Solution (<em>CVPR'23</em>) <strong>[</strong><ahref="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf"><strong>Paper</strong></a><strong>]</strong><strong>[<ahref="https://github.com/qcf-568/DocTamper">Code</a>]</strong></label></li><li><label><input type="checkbox" />Progressive Supervision forTampering Localization in Document Images (<em>ICONIP '23</em>)<strong>[<ahref="https://link.springer.com/chapter/10.1007/978-981-99-8184-7_11">Paper</a>]</strong></label></li><li><label><input type="checkbox" />SigScatNet: A Siamese + Scatteringbased Deep Learning Approach for Signature Forgery Detection andSimilarity Assessment <em>(arXiv '23)</em> <strong>[<ahref="https://arxiv.org/abs/2311.05579.pdf">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Image Generation and LearningStrategy for Deep Document Forgery Detection <em>(arXiv '23)</em><strong>[<ahref="https://arxiv.org/abs/2311.03650">Paper</a>]</strong></label></li><li><label><input type="checkbox" />Forgery-free signature verificationwith stroke-aware cycle-consistent generative adversarial network<em>(Neurocomputing '22)</em> <strong>[<ahref="https://doi.org/10.1016/j.neucom.2022.08.017">Paper</a>]</strong><strong>[<ahref="https://github.com/KAKAFEI123/Stroke-cCycleGAN">Code</a>]</strong></label></li><li><label><input type="checkbox" />Document Forgery Detection in theContext of Double JPEG Compression <em>(ICPR '22)</em> <strong>[<ahref="https://link.springer.com/chapter/10.1007/978-3-031-37745-7_5">Paper</a>]</strong></label></li></ul></details><p>datasets 下载:</p><ul><li><p><ahref="https://github.com/namtpham/casia2groundtruth">Casiav2</a></p></li><li><p><ahref="https://github.com/mjkwon2021/CAT-Net#1-downloading-tampcoco--compraise">tampCOCO</a></p></li><li><p><ahref="http://staff.utia.cas.cz/novozada/db/">IMD2020</a></p></li><li><p><ahref="http://zefirus.org/articles/9f78c1e9-8652-4392-9199-df1b6a6c1a3d/">FantasticReality</a></p></li><li><p><ahref="https://github.com/namtpham/casia1groundtruth">Casiav1</a>(创建Casiav1+数据集需要corel数据集)</p></li><li><p><ahref="https://www.kaggle.com/datasets/elkamel/corel-images">corel</a></p></li><li><p><ahref="https://github.com/grip-unina/TruFor#cocoglide-dataset">CocoGlide</a></p></li><li><p><ahref="https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/">Columbia</a></p></li><li><p><a href="https://github.com/wenbihan/coverage">COVER</a></p></li><li><p><ahref="https://recodbr.wordpress.com/code-n-data/#dso1_dsi1">DSO-1</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 合集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（二）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="一stderr-log打印">一、stderr log打印</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sys.stderr = <span class="built_in">open</span>(<span class="string">&quot;errors.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eprint</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(*args, file=sys.stderr, **kwargs)</span><br></pre></td></tr></table></figure><p>这样代码中使用eprint()方法输出的信息会保存在根目录errors.txt文件中</p><h5 id="二机器人状态控制">二、机器人状态控制</h5><p>机器人的状态由变量self.gooding控制，其中参数与其对应的状态如下：</p><table><colgroup><col style="width: 12%" /><col style="width: 60%" /><col style="width: 17%" /><col style="width: 10%" /></colgroup><thead><tr class="header"><th>self.gooding</th><th>状态</th><th>判断条件</th><th>下一个状态</th></tr></thead><tbody><tr class="odd"><td>0</td><td>机器人处于空闲状态，可以通过self.getTarget()方法，得到要去往的goods目标</td><td>无</td><td>1</td></tr><tr class="even"><td>1</td><td>机器人已经找到目标goods，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>1.5</td></tr><tr class="odd"><td>1.5</td><td>机器人已经得到goods，判断是否get成功</td><td>self.goods == 0</td><td>0</td></tr><tr class="even"><td></td><td></td><td>self.goods == 1</td><td>2</td></tr><tr class="odd"><td>2</td><td>机器人处于取得goods状态，可以通过self.getBerthTarget()方法，得到要去往的berth目标</td><td>无</td><td>3</td></tr><tr class="even"><td>3</td><td>机器人已经找到目标berth，此时self.path存储机器人去往目标的路径，而self.getNextAction()可以通过self.path与此时的位置得到下一步的方向</td><td>len(self.path)==0</td><td>0</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024华为精英挑战赛（一）</title>
      <link href="/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024%E5%8D%8E%E4%B8%BA%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h5 id="赛题背景">【赛题背景】</h5><ul><li><p>华为基于自身ICT基础设施能力，通过对港口场景的洞察和理解，以智慧、绿色、高效、安全为目标，数字化、智能化为手段，联合生态伙伴，助力世界一流港口建设。</p></li><li><p>本次赛题抽象自华为云智能港口真实业务难题，选手通过算法完成运输船只智能泊靠、运输机器人智能拣货装货等任务，以最大化提升港口物流效率。</p></li></ul><h5 id="数据集说明"><strong>【数据集说明】</strong></h5><ul><li>初赛练习赛每天使用1张地图进行判题。</li><li>初赛正式赛有3张地图，取3张图的总分作为当次判题成绩。</li><li>初赛练习赛共8张地图，地图数据完全公开给大家下载。练习阶段前8日，每日公开当日判题地图，方便大家本地调试。</li></ul><p><strong>【官方论坛】</strong></p><ul><li><h5id="华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云-huaweicloud.com"><strong><ahref="https://bbs.huaweicloud.com/forum/forum-0168144383617537003-1.html">华为云论坛_云计算论坛_开发者论坛_技术论坛-华为云(huaweicloud.com)</a></strong></h5></li><li><strong><ahref="https://bbs.huaweicloud.com/forum/thread-0239145895671582004-1-1.html">初赛练习阶段赛题相关材料及配套软件（Windows版本）整合版_2024华为软件精英挑战赛_华为软件精英挑战赛_华为云论坛(huaweicloud.com)</a></strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
