<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>zhaozw后院 - 这是我的博客网站</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。">
<meta property="og:type" content="website">
<meta property="og:title" content="zhaozw后院">
<meta property="og:url" content="https://zhaozw-szu.github.io/page/2/index.html">
<meta property="og:site_name" content="zhaozw后院">
<meta property="og:description" content="人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/img/avatar.jpg">
<meta property="article:author" content="Zhaozw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/page/2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'zhaozw后院',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-10-12 21:30:19'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-chart-simple"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-video"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/background.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="zhaozw后院"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">zhaozw后院</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-chart-simple"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-video"></i><span> 数据集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">zhaozw后院</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div id="category-bar"><div class="category-bar-items" id="category-bar-items"><div class="category-bar-item" id="首页"><a href="/">首页</a></div>
      <div class="category-bar-item" id="论文">
      <a href="/categories/论文/">论文</a>
      </div>
      <div class="category-bar-item" id="篡改检测">
      <a href="/categories/篡改检测/">篡改检测</a>
      </div>
      <div class="category-bar-item" id="方法">
      <a href="/categories/方法/">方法</a>
      </div>
      <div class="category-bar-item" id="baseline">
      <a href="/categories/baseline/">baseline</a>
      </div>
      <div class="category-bar-item" id="对比学习">
      <a href="/categories/对比学习/">对比学习</a>
      </div><div class="category-bar-item" id="首页"><a href="/">首页</a></div></div><div class="category-bar-next" id="category-bar-next" onclick="scrollCategoryBarToRight()"><i class="fa-solid"></i></div><a class="category-bar-more" href="/categories/">更多</a></div><div class="wrapper"><div class="button-group"><button onclick="filterPosts('swiper')">轮播</button><button onclick="filterPosts('all')">全部</button><button onclick="filterPosts('A')">CCF-A</button><button onclick="filterPosts('B')">CCF-B</button><button onclick="filterPosts('C')">CCF-C</button></div><div class="main-content"><div class="swiperBar"><div id="ark-swiper-container"><div class="swiper-wrapper ark-swiper-wrapper"><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于TIFS2024，被篡改区域的边界是分离被篡改和未被篡改像素的关键位置，在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于TIFS2023，随着互联网的蓬勃发展，在线社交网络（OSNs， online social networks）已成为图像共享和传输的主导渠道，但OSN传输下所有现有算法的性能都会严重下降，尤其是WeChat、QQ、Telegram和Dingding。为了减轻OSN的负面影响，在本工作中，我们提出了一种新的相机轨迹提取方法，该方法有望对各种OSN平台的传输具有鲁棒性。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于CVPR2024型。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于ACMMM2024,针对IML任务中公共训练数据集的稀缺，通过采用可调提示来利用预训练模型的丰富先验知识，即Prompt-IML框架,即插即用的特征对齐和融合模块。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;UnionFormer/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于CVPR2024,集成三个视图的UnionFormer框架,一个调节不同尺度上空间一致性的篡改特征提取网络BSFI-Net。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/UnionFormer/image-20240618124653610.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">CatmullRom Splines-Based Regression for Image Forgery Localization</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于AAAI2024，提出基于CatmullRom样条的回归网络，为了明确抑制假阳性样本和避免不确定性边界，综合再评分算法（CRA,Comprehensive Re-scoring Algorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP, Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于AAAI2024，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于AAAI2024，为应对交叉熵损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节，设计了基于掩码引导查询的转换器框架（MGQFormer），该框架使用GroundTruth掩码来引导可学习查询令牌（LQT）识别伪造区域。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">A New Benchmark and Model for Challenging Image Manipulation Detection</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于AAAI2024，包含RGB和频率特征的hrnet双分支架构，能够检测双压缩伪影的压缩伪影学习模型。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png" title="" onerror="this.src=/img/loading.gif"></div></div><div class="swiper-slide ark-swiper-item" onclick="pjax.loadUrl(&quot;DH-GAN/&quot;);" title=""><div class="swiper-item-info"><a class="swiper-item-title"><div class="swiper-item-title-link">DH-GAN</div></a><a class="swiper-item-description"><div class="swiper-item-description-text">发表于Pattern Recognition 2024，双同源感知生成对抗网络（DH-GAN）,选择性金字塔（SAP）校准多尺度特征。</div></a></div><div class="swiper-item-cover"><img class="article-cover" src="/postimages/DH-GAN/image-20240824153543322.png" title="" onerror="this.src=/img/loading.gif"></div></div></div><div class="swiper-pagination"></div></div></div><div id="post-list"><div class="post-item" data-rank="[&quot;A类期刊&quot;,&quot;TIFS&quot;]"><a href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/">Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10589438"><img src="https://img.shields.io/badge/TIFS-2024-orange" alt="TIFS"></a><img src="/postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png" alt="Attentive and Contrastive Image Manipulation Localization With Boundary Guidance"><p>发表于TIFS2024，被篡改区域的边界是分离被篡改和未被篡改像素的关键位置，在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。</p><strong>现有问题</strong>：<br/> 被篡改区域的边界是分离被篡改和未被篡改像素的关键位置。然而，如何利用这些边界信息来提高检测被篡改图像区域的性能仍有待探索。<br/>
<strong>解决方案</strong>：<br/> 在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。首先，为了进一步增强操作定位，我们鼓励该模型关注一个被篡改区域周围的边界，其中经常存在非自然的混合。其次，受对比学习的启发，我们寻求学习一个特征空间，即篡改区域内的点远离篡改区域边界附近的非调和区域点，以获得更强大的特性来定位篡改区域。<br/>
<details close> <summary>具体情况</summary>
在注意方面，在我们的框架的解码层中，我们提出了一种新的基于交叉注意的边界感知模块，旨在提取图像中被篡改区域的边界，从而使模型进一步集中于被篡改区域的边界。特别是，边界感知注意模块利用跳连编码特征与前一层解码特征的相关性，提取被篡改区域的边界，进一步用于生成图像篡改定位的掩模。<br/>
<img src="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png" alt="image-20240910222829501" />
在特征学习方面，我们提出的模型是基于一个典型的编解码器架构及其特征学习监督由一个新颖的对比目标函数[16]，[22]，[23]，表示为边界引导篡改对比损失，为了推动分开特征采样的篡改和非篡改区域，从而学习更多的区别特征表示。为此，我们采用边界引导的采样策略来收集负训练对，其中我们在被篡改区域的边界周围采样负样本，而不是整个非被篡改区域。该采样方案不仅鼓励模型关注存在非自然混合的边界区域，而且减轻了未篡改区域内巨大变化引起的干扰（见图1中的可视化特征）。<br/>
<img src="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png" alt="image-20240910221211400" />
</details></div><div class="post-item" data-rank="[&quot;A类期刊&quot;,&quot;TIFS&quot;]"><a href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/">Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10262083"><img src="https://img.shields.io/badge/TIFS-2023-orange" alt="TIFS"></a><img src="/postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png" alt="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning"><p>发表于TIFS2023，随着互联网的蓬勃发展，在线社交网络（OSNs， online social networks）已成为图像共享和传输的主导渠道，但OSN传输下所有现有算法的性能都会严重下降，尤其是WeChat、QQ、Telegram和Dingding。为了减轻OSN的负面影响，在本工作中，我们提出了一种新的相机轨迹提取方法，该方法有望对各种OSN平台的传输具有鲁棒性。</p></div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;CVPR&quot;]"><a href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/">Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Qu_Towards_Modern_Image_Manipulation_Localization_A_Large-Scale_Dataset_and_Novel_CVPR_2024_paper.html"><img src="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR"></a><a target="_blank" rel="noopener" href="https://github.com/qcf-568/MIML"><img src="https://img.shields.io/github/stars/qcf-568/MIML?style=flat" alt="GitHub"></a><img src="/postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png" alt="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods"><p>发表于CVPR2024型。</p></div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;ACMMM&quot;]"><a href="/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/">Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization</a><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=Ci5g2dnrMK"><img src="https://img.shields.io/badge/ACMMM-2024-orange" alt="AAAI"></a><img src="/postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png" alt="Multi-view Feature Extraction via Tunable Prompts is Enough for Image Manipulation Localization"><p>发表于ACMMM2024,针对IML任务中公共训练数据集的稀缺，通过采用可调提示来利用预训练模型的丰富先验知识，即Prompt-IML框架,即插即用的特征对齐和融合模块。</p><strong>现有问题</strong>：<br/> IML任务中公共训练数据集的稀缺直接阻碍了模型的性能。<br/>
<strong>解决方案</strong>：<br/> 提出了一个Prompt-IML框架，该框架通过采用可调提示来利用预训练模型的丰富先验知识。<br/>
<details close> <summary>具体情况</summary>
> 通过集成可调提示，从单个预先训练过的主干中提取和调整多视图特征，从而保持性能和鲁棒性
<img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824155623293.png" alt="image-20240824155623293" />
> 特征对齐和融合的FAF模块
<img src="../postimages/Multi-view-Feature-Extraction-via-Tunable-Prompts-is-Enough-for-Image-Manipulation-Localization/image-20240824220942200.png" alt="image-20240824220942200" />
</details> </div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;CVPR&quot;]"><a href="/UnionFormer/">UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.html"><img src="https://img.shields.io/badge/CVPR-2024-orange" alt="CVPR"></a><img src="/postimages/UnionFormer/image-20240618124653610.png" alt="UnionFormer Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization"><p>发表于CVPR2024,集成三个视图的UnionFormer框架,一个调节不同尺度上空间一致性的篡改特征提取网络BSFI-Net。</p><strong>现有问题</strong>：以往的方法主要利用为高级视觉任务设计的深度卷积神经网络作为特征编码器或直接连接来自不同层的特征，不能充分表示篡改痕迹；目前的高级方法关注于像素或补丁级的一致性，而忽略了对象级的信息，在自然语言提示的引导下，自动生成的伪造部分更有可能表现出对象的不一致。<br/>
<strong>解决方案</strong>：设计了专门用于提取取证工件的边界敏感特征交互网络（BSFI-Net, Boundary Sensitive Feature Interaction Network）设计了用于图像操作检测和定位的多视图表示的统一学习transformer框架
<details close> <summary>具体情况</summary>
> cnn-Transformer并发网络 BSFI-Net，该网络在保持边缘灵敏度的同时，促进了两个分支中不同尺度的特征之间的彻底交互。<br/>
<img src="../postimages/UnionFormer/image-20240617110632461.png" alt="image-20240617110632461" />

> 采用对比监督来促进两个视图之间的协作
<img src="../postimages/UnionFormer/image-20240618124629395.png" alt="image-20240618124629395" />
> 统一伪造判别表示，每个篡改判别查询都表示对应建议的三个视图中的篡改线索
<img src="../postimages/UnionFormer/image-20240617214850871.png" alt="image-20240617214850871" />
</details></div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;AAAI&quot;]"><a href="/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/">CatmullRom Splines-Based Regression for Image Forgery Localization</a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28548"><img src="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI"></a><img src="/postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png" alt="CatmullRom Splines-Based Regression for Image Forgery Localization"><p>发表于AAAI2024，提出基于CatmullRom样条的回归网络，为了明确抑制假阳性样本和避免不确定性边界，综合再评分算法（CRA,Comprehensive Re-scoring Algorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP, Vertical Texture-interactive Perception）控制生成更准确的区域边缘。</p><strong>现有问题</strong>：<br/> 假阳性（FPs）和不准确的边界。<br/>
<strong>解决方案</strong>：<br/> 基于CatmullRom样条的回归网络（CSR-Net, CatmullRom Splines-based Regression Network），首次尝试将回归方法引入像素级任务。为了明确抑制假阳性样本和避免不确定性边界，我们设计两个相互互补和强化的组件，即综合再评分算法（CRA,Comprehensive Re-scoring Algorithm），综合评估每个区域的信任分数作为篡改区域，而垂直纹理交互感知（VTP, Vertical Texture-interactive Perception）控制生成更准确的区域边缘。<br/>
<details close> <summary>具体情况</summary>
<img src=" ../postimages/CatmullRom-Splines-Based-Regression-for-Image-Forgery-Localization/image-20240503215025883.png" alt="image-20240503215025883" />
在本文中，我们精心设计了一个定制的基于CatmullRom样条的回归网络（CSR-Net），并尝试将回归方法引入像素级图像篡改定位（本文中的IFL）。<br/>
详细地说，与传统的边界盒检测方法相比，我们引入了CatmullRom定位技术，该技术对目标区域控制点的轮廓进行了建模，从而实现了更准确和有效的篡改区域定位。然后，为了抑制FPs（假阳性），设计了综合再评分算法（CRA），我们为每个区域实例重新分配分数，区域实例的综合得分由分类得分（CLS）和实例得分（INS）两部分组成。<br/>
此外，我们还提出了一个可学习的区域纹理提取模块垂直纹理交互感知（VTP）来进一步参考边缘。<br/>
<img src="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240823161357950.png" alt="image-20240823161357950" />
因此，CSRNet可以在不接近FPs的情况下感知所有被篡改的区域，并实现准确的定位。大量的实验表明，CSR-Net优于现有的最先进的方法，不仅在自然图像数据集上，而且在社交媒体数据集上。<br/>

</details></div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;AAAI&quot;]"><a href="/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/">Learning Discriminative Noise Guidance for Image Forgery Detection and Localization</a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28608"><img src="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI"></a><img src="/postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240822204529012.png" alt="Learning Discriminative Noise Guidance for Image Forgery Detection and Localization"><p>发表于AAAI2024，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。</p><strong>现有问题</strong>：<br/> -  随着篡改和后处理技术的发展，这两个区域在噪声域之间的差异变得不那么明显，甚至不那么隐藏。鉴于这些缺陷，我们建议明确地学习和利用噪声的不一致性可以进一步提高IFDL的性能。<br/>
<strong>解决方案</strong>：<br/>
通过关注噪声域内的操纵痕迹来检测和定位图像伪造，一种两阶段判别噪声引导的方法，第一阶段训练一个噪声提取器，以明确地扩大真实区域和伪造区域之间的噪声分布差异，第二阶段将噪声不一致和RGB数据集成，以进行伪造检测和定位。<br/>
<details close> <summary>具体情况</summary>
一阶段：<br/>
<img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502204239503.png" alt="image-20240502204239503" /> 为了明确地分离出这两个区域（真实的和伪造的)的噪声分布，我们引入了JS散度来约束 $ G_d $ 。首先，我们利用 groundtruth掩模，将 $ G_d $ 划分为真实区域 $ N_a $ 的噪声和伪造区域 $ N_f $ 的噪声。<br/>
<img src="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822220206200.png" alt="image-20240822220206200" />
式中， $ \sigma_a $ 、 $ \sigma_f $ 为 $ N_a $ 和 $ N_f $ 的标准差， $ \mu_a $ 、 $ \mu_f $ 为 $ N_a $ 和 $ N_f $ 的平均值。<br/>
$$ \mathbf{L_{n}}=\lambda\left(1-JSD\right)+\left(1-\lambda\right)\mathcal{L}\left(Y,G_{c}\right) $$ 二阶段：<br/>
<img src="../postimages/Learning-Discriminative-Noise-Guidance-for-Image-Forgery-Detection-and-Localization/image-20240502211812066.png" alt="image-20240502211812066" />
利用两个分支来处理RGB和噪声信息，为了保证噪声不一致对RGB的引导作用，我们设计了CAGF，并将其与ResNet块交替放置。在噪声的引导下，RGB分支可以提取出与篡改伪影高度相关的特征。<br/>
</details> </div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;AAAI&quot;]"><a href="/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/">MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization</a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28520"><img src="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI"></a><img src="/postimages/MGQFormer-Mask-Guided-Query-Based-Transformer-for-Image-Manipulation-Localization/image-20240401164940206.png" alt="MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization"><p>发表于AAAI2024，为应对交叉熵损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节，设计了基于掩码引导查询的转换器框架（MGQFormer），该框架使用GroundTruth掩码来引导可学习查询令牌（LQT）识别伪造区域。</p><strong>现有问题</strong>：<br/> - 所有现有的IMD主要通过交叉熵损失使用真值掩码，该损失优先考虑逐像素精度，但忽略了篡改区域的空间位置和形状细节。<br/>
<strong>解决方案</strong>：一种基于掩码引导查询的转换器框架（MGQFormer），该框架使用基本事实掩码来引导可学习查询令牌（LQT）识别伪造区域。<br/>
<details close> <summary>具体情况</summary>
&emsp;&emsp;利用BayarConv和Transformer编码器从输入图像中提取RGB和噪声特征，过空间和通道注意模块（SCAM,spatial and channel attention module）对多模态特征进行融合。其特征提取器如下:
<img src="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822164324909.png" alt="image-20240822164324909" /> &emsp;&emsp;我们设计了两个可学习的查询token来表示真实和伪造的特征，它们用于在我们提出的基于查询的Transformer解码器中搜索篡改区域。为了使查询token有效参考和基于查询的解码器快速收敛，我们提出了一种利用GroundTruth掩模的空间位置和形状细节的掩模引导训练策略。其解码器如下:
<img src="../postimages/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/image-20240822170134582.png" alt="image-20240822170134582" />
&emsp;&emsp;具体来说，我们将噪声的GT掩模输入MGQFrorer，以获得引导查询token（GQT)和辅助掩模 $ M_{aux} $ 。然后，利用辅助损失 $ L_{aux} $ ，使GQT包含伪造区域的空间和形状信息。此外，我们提出了一种掩模引导的损失 $ L_{guide} $ 来减小LQT和GQT之间的距离。<br/>
</details></div><div class="post-item" data-rank="[&quot;A类会议&quot;,&quot;AAAI&quot;]"><a href="/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/">A New Benchmark and Model for Challenging Image Manipulation Detection</a><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/28571"><img src="https://img.shields.io/badge/AAAI-2024-orange" alt="AAAI"></a> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.14218"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a><a target="_blank" rel="noopener" href="https://github.com/ZhenfeiZ/CIMD"><img src="https://img.shields.io/github/stars/ZhenfeiZ/CIMD?style=flat" alt="GitHub"></a><img src="/postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png" alt="A New Benchmark and Model for Challenging Image Manipulation Detection"><p>发表于AAAI2024，包含RGB和频率特征的hrnet双分支架构，能够检测双压缩伪影的压缩伪影学习模型。</p><strong>现有问题</strong>：<br/>
- 所有现有的IMD技术在从大图像中检测小的篡改区域时都遇到了挑战。<br/> - 基于压缩的IMD方法在相同质量因子的双重压缩的情况下面临困难。<br/>
<strong>解决方案</strong>：包含RGB和频率特征的双分支架构，能够检测双压缩伪影的压缩伪影学习模型。<br/>
<details close> <summary>具体情况</summary>
> RGB和频率特征的双分支架构
<img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326151650512.png" alt="image-20240326151650512" />
> 双压缩伪影的压缩伪影学习模型
<img src="../postimages/A-New-Benchmark-and-Model-for-Challenging-Image-Manipulation-Detection/image-20240326170313123.png" alt="image-20240326170313123" />
</details></div><div class="post-item" data-rank="[&quot;B类期刊&quot;,&quot;PR&quot;]"><a href="/DH-GAN/">DH-GAN</a><img src="/postimages/DH-GAN/image-20240824153543322.png" alt="DH-GAN"><p>发表于Pattern Recognition 2024，双同源感知生成对抗网络（DH-GAN）,选择性金字塔（SAP）校准多尺度特征。</p></div></div></div><script>filterPosts('swiper')
function filterPosts(rank) {
    var postItems = document.querySelectorAll('.post-item');
    postItems.forEach(function(item) {
        var itemRank = JSON.parse(item.getAttribute('data-rank'));
        //- console.log(itemRank);
        if (rank === 'all' || itemRank.some(r => r.startsWith(rank + '类'))) {
            item.style.display = '';
        } else {
            item.style.display = 'none';
        }
    });
    var swiperBar = document.querySelector('.swiperBar')
    if (rank === 'all' || rank === 'swiper') {
        swiperBar.style.display = '';
    } else {
        swiperBar.style.display = 'none';
    }
}</script></div><div class="recent-post-item"><div class="post_cover right"><a href="/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/" title="经典网络合集"><img class="post-bg" src="/img/coverImage/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="经典网络合集"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/" title="经典网络合集">经典网络合集</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-10-03T11:19:31.000Z" title="发表于 2024-10-03 19:19:31">2024-10-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%90%88%E9%9B%86/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">1. 自编码器（autoencoder）
​  自编码器（autoencoder）内部有一个隐藏层
h，可以产生编码（code）表示输入。​  该网络可以看作由两部分组成：

由函数\(h = f ( x
)\)表示的编码器
生成重构的解码器\(r =
g(h)\)，整体结构如下图所示：



image-20241003192041921

自编码器的一些基本概念

欠完备自编码器：h维度&lt;x维度。


学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。“学习欠完备的表示”意味着编码器被设计成只能生成比输入数据更简单、更压缩的表示。举一个简单的例子：
假设你有一堆猫和狗的图片。一个没有任何限制的自编码器可能会尝试记住每张图片的所有细节（毛色、背景等等）。但是，如果我们限制自编码器只能生成非常简化的表示，它就会被迫关注猫和狗最明显的特征，比如猫的尖耳朵和狗的长尾巴，而忽略背景和毛色等次要特征。


过完备自编码器：h维度&gt;=x维度。


过完备则与欠完备相反。“过完备”意味着编码器的表示空间非常大，能够容纳甚至超过输 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/Deep-Clustering-Survey/" title="Deep_Clustering_Survey"><img class="post-bg" src="/img/coverImage/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deep_Clustering_Survey"></a></div><div class="recent-post-info"><a class="article-title" href="/Deep-Clustering-Survey/" title="Deep_Clustering_Survey">Deep_Clustering_Survey</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-29T02:44:40.000Z" title="发表于 2024-09-29 10:44:40">2024-09-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/Deep-Clustering-Survey/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">
Deep Clustering: A Comprehensive Survey 


Yazhou Ren , Member, IEEE, Jingyu Pu , Zhimeng Yang, Jie Xu , Guofeng
Li, Xiaorong Pu , Philip S. Yu , Fellow, IEEE, and Lifang He , Member,
IEEE

摘要
​  聚类分析在机器学习和数据挖掘中起着不可或缺的作用。学习一个好的数据表示对聚类算法至关重要。近年来，深度聚类（DC，deep
clustering）可以利用深度神经网络（DNNs，deep neural
networks）学习聚类友好表示，已广泛应用于聚类任务。现有的DC总结主要集中在单视图场和网络架构上，忽略了复杂的集群应用场景。为了解决这个问题，在本文中，我们提供了一个对DC的数据源的全面总结。对于不同的数据源，我们从方法学、先验知识和体系结构等方面系统地区分了聚类方法。具体地说，DC方法被分为四类，传统的单视图DC、半监督DC、深度多视图聚类（MVC）和深度传输聚类。最后， ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/" title="聚类方法合集"><img class="post-bg" src="/img/coverImage/cover4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="聚类方法合集"></a></div><div class="recent-post-info"><a class="article-title" href="/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/" title="聚类方法合集">聚类方法合集</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-27T01:06:33.000Z" title="发表于 2024-09-27 09:06:33">2024-09-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">1. ClusterLookup
方法来源：STEGO: Unsupervised Semantic Segmentation by Distilling
Feature Correspondences
class ClusterLookup(nn.Module):    def __init__(self, dim: int, n_classes: int):        super(ClusterLookup, self).__init__()        self.n_classes = n_classes        self.dim = dim        self.clusters = torch.nn.Parameter(torch.randn(n_classes, dim))    def reset_parameters(self):        with torch.no_grad():            self.clusters.copy_(torch.randn(self.n_classes, self.dim))    def forwa ...</div></div></div><div class="recent-post-item1"><div class="recent-post-info first-info"><a class="article-title" href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/" title="Attentive and Contrastive Image Manipulation Localization With Boundary Guidance">Attentive and Contrastive Image Manipulation Localization With Boundary Guidance</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-10T02:46:55.000Z" title="发表于 2024-09-10 10:46:55">2024-09-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div></div><div class="post_cover middle"><a href="/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/" title="Attentive and Contrastive Image Manipulation Localization With Boundary Guidance"><img class="post-bg" src="/postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910110200920.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attentive and Contrastive Image Manipulation Localization With Boundary Guidance"></a></div><div class="recent-post-info"><div class="content">发表于TIFS2024，被篡改区域的边界是分离被篡改和未被篡改像素的关键位置，在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。</div><div class="content"><strong>现有问题</strong>：<br/> 被篡改区域的边界是分离被篡改和未被篡改像素的关键位置。然而，如何利用这些边界信息来提高检测被篡改图像区域的性能仍有待探索。<br/>
<strong>解决方案</strong>：<br/> 在这项工作中，我们提出了一种图像操作检测的边界感知方案，其中我们引入了充分利用篡改区域的边界信息，并从注意和特征学习两个角度实现了我们的方案。首先，为了进一步增强操作定位，我们鼓励该模型关注一个被篡改区域周围的边界，其中经常存在非自然的混合。其次，受对比学习的启发，我们寻求学习一个特征空间，即篡改区域内的点远离篡改区域边界附近的非调和区域点，以获得更强大的特性来定位篡改区域。<br/>
<details close> <summary>具体情况</summary>
在注意方面，在我们的框架的解码层中，我们提出了一种新的基于交叉注意的边界感知模块，旨在提取图像中被篡改区域的边界，从而使模型进一步集中于被篡改区域的边界。特别是，边界感知注意模块利用跳连编码特征与前一层解码特征的相关性，提取被篡改区域的边界，进一步用于生成图像篡改定位的掩模。<br/>
<img src="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910222829501.png" alt="image-20240910222829501" />
在特征学习方面，我们提出的模型是基于一个典型的编解码器架构及其特征学习监督由一个新颖的对比目标函数[16]，[22]，[23]，表示为边界引导篡改对比损失，为了推动分开特征采样的篡改和非篡改区域，从而学习更多的区别特征表示。为此，我们采用边界引导的采样策略来收集负训练对，其中我们在被篡改区域的边界周围采样负样本，而不是整个非被篡改区域。该采样方案不仅鼓励模型关注存在非自然混合的边界区域，而且减轻了未篡改区域内巨大变化引起的干扰（见图1中的可视化特征）。<br/>
<img src="../postimages/Attentive-and-Contrastive-Image-Manipulation-Localization-With-Boundary-Guidance/image-20240910221211400.png" alt="image-20240910221211400" />
</details></div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/KaggleNet/" title="Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning"><img class="post-bg" src="/postimages/KaggleNet/image-20240903142646600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/KaggleNet/" title="Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning">Towards Generalizable Deepfake Detection via Clustered and Adversarial Forgery Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-09T07:02:57.000Z" title="发表于 2024-09-09 15:02:57">2024-09-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/KaggleNet/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">
Towards Generalizable Deepfake Detection via Clustered and Adversarial
Forgery Learning


Yiming Chen, Haiwei Wu, Fengpeng Li, Kemou Li,


Zheng Li, Kahim Wong, Xiangyu Chen, Binbin Song,


Shuning Xu, Jun Liu, $ Jiantao Zhou ^ * $


September 2, 2024

1 团队详细信息
团队名称：JTGroup组长：mortonchen（yimingchen98@gmail.com）团队成员人数（按字母顺序排列）：

chxy95 (chxy95@gmail.com)
fengpengli (fengpeng.li@connect.umac.mo)
highwayw (823215616@qq.com)
kahimwong (jesonwong47@gmail.com)
kemoulee (kemoulee ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/ImageForensicsOSN/" title="ImageForensicsOSN"><img class="post-bg" src="/img/coverImage/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ImageForensicsOSN"></a></div><div class="recent-post-info"><a class="article-title" href="/ImageForensicsOSN/" title="ImageForensicsOSN">ImageForensicsOSN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-04T01:26:12.000Z" title="发表于 2024-09-04 09:26:12">2024-09-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/ImageForensicsOSN/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">
Robust Image Forgery Detection over Online Social Network Shared Images


Haiwei Wu, Jiantao Zhou, Jinyu Tian, and Jun Liu


智慧城市物联网国家重点实验室


澳门大学计算机与信息科学系


{yc07912, jtzhou, yb77405, yc07453}@um.edu.mo

摘要
​  Photoshop和美图等图像编辑软件的滥用，导致数字图像的真实性受到质疑。与此同时，网络社交网络（OSNs）的广泛使用使其成为传输伪造图像、报道假新闻、传播谣言等的主要渠道。不幸的是，osn所采用的各种有损操作，如压缩和调整大小，给实现鲁棒的图像伪造检测带来了巨大的挑战。为了对抗OSN共享的伪造行为，本文提出了一种新的鲁棒训练方案。我们首先对osn引入的噪声进行了彻底的分析，并将其解耦为两部分，即可预测的噪声和看不见的噪声，它们分别建模。前者模拟了所公开的（已知）osn操作所引入的噪声，而后者的设计不仅完成前一个，而且还考虑了探测器本身 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/" title="科研工具"><img class="post-bg" src="/img/coverImage/cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="科研工具"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/" title="科研工具">科研工具</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-03T14:22:26.612Z" title="发表于 2024-09-03 22:22:26">2024-09-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">1.t-SNE(t-DistributedStochasticNeighborEmbedding)
​  t-SNE是一种用于探索高维数据结构的非线性降维技术。它特别适用于高维数据的可视化，因为它能够在低维空间中保留原始高维数据的局部结构。由于这个特性，t-SNE在机器学习和数据分析领域越来越受到重视。
1算法解读：
​  t-SNE的核心思想是在高维空间中为数据点之间定义一种概率分布，表示点与点之间的相似性，然后在低维空间中创建一个相似的概率分布。通过最小化这两个分布之间的差异（使用KL散度），算法将高维数据映射到低维空间，以便我们可以可视化。
2步骤和细节：
Step1.计算高维空间中的相似度
​  我们使用高斯分布（正态分布）来计算点之间的相似性。高斯分布是一种常见的概率分布，其形状呈钟型，由均值和方差（标准差的平方）决定。高斯分布有一个很好的性质：它的形状由均值（中心点）和方差（分布的宽度）决定。当我们围绕一个数据点x画一个高斯分布时，这个分布会给予附近的点较高的概率值，而离得远的点则会有较低的概率值。这与我们直觉上对“相似性”的理解相一致：靠近的点更相似，远离的点 ...</div></div></div><div class="recent-post-item1"><div class="recent-post-info first-info"><a class="article-title" href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/" title="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning">Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-02T12:16:56.000Z" title="发表于 2024-09-02 20:16:56">2024-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div></div><div class="post_cover middle"><a href="/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/" title="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning"><img class="post-bg" src="/postimages/Robust-Camera-Model-Identification-Over-Online-Social-Network-Shared-Images-via-Multi-Scenario-Learning/image-20240902210022345.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust Camera Model Identification Over Online Social Network Shared Images via Multi-Scenario Learning"></a></div><div class="recent-post-info"><div class="content">发表于TIFS2023，随着互联网的蓬勃发展，在线社交网络（OSNs， online social networks）已成为图像共享和传输的主导渠道，但OSN传输下所有现有算法的性能都会严重下降，尤其是WeChat、QQ、Telegram和Dingding。为了减轻OSN的负面影响，在本工作中，我们提出了一种新的相机轨迹提取方法，该方法有望对各种OSN平台的传输具有鲁棒性。</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/experimet/" title="experimet"><img class="post-bg" src="/img/coverImage/cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="experimet"></a></div><div class="recent-post-info"><a class="article-title" href="/experimet/" title="experimet">experimet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-02T01:42:21.000Z" title="发表于 2024-09-02 09:42:21">2024-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/experimet/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div><div class="content">

image-20240902094509834



image-20240902094320599

第一行是FOCAL论文效果第二行是复刻的最好效果第三四行是冻结encoder，训练decoder第五行是一起训练表格如下：























decoderweights
Columbia F1
Columbia IOU
Columbia AUC
COVERAGE F1
COVERAGE IOU
COVERAGE AUC
CASIA F1
CASIA IOU
CASIA AUC
MISD F1
MISD IOU
MISD AUC
NIST F1
NIST IOU
NIST AUC




FOCAL (HRNet)

9620
9290

7690
5240

8640
7060

8570
6390

7100
4030



Log_v09011445/Ep002_0.7782/

9781
9582 ...</div></div></div><div class="recent-post-item1"><div class="recent-post-info first-info"><a class="article-title" href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/" title="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods">Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-26T01:26:05.000Z" title="发表于 2024-08-26 09:26:05">2024-08-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-comments"></i><a class="twikoo-count" href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/#post-comment"><i class="fa-solid fa-spinner fa-spin"></i></a><span class="article-meta-label"> 条评论</span></span></div></div><div class="post_cover middle"><a href="/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/" title="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods"><img class="post-bg" src="/postimages/Towards-Modern-Image-Manipulation-Localization-A-Large-Scale-Dataset-and-Novel-Methods/image-20240826165320985.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods"></a></div><div class="recent-post-info"><div class="content">发表于CVPR2024型。</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/#content-inner">7</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">可以用表情包和匿名评论了</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/" title="公式识别工具"><img src="/img/coverImage/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="公式识别工具"/></a><div class="content"><a class="title" href="/%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E5%85%B7/" title="公式识别工具">公式识别工具</a><time datetime="2024-10-11T08:46:37.000Z" title="发表于 2024-10-11 16:46:37">2024-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Unsupervised-Semantic-Segmentation/" title="Unsupervised_Semantic_Segmentation"><img src="/img/coverImage/cover1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unsupervised_Semantic_Segmentation"/></a><div class="content"><a class="title" href="/Unsupervised-Semantic-Segmentation/" title="Unsupervised_Semantic_Segmentation">Unsupervised_Semantic_Segmentation</a><time datetime="2024-10-10T06:44:09.000Z" title="发表于 2024-10-10 14:44:09">2024-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/EAGLE/" title="EAGLE"><img src="/img/coverImage/cover5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="EAGLE"/></a><div class="content"><a class="title" href="/EAGLE/" title="EAGLE">EAGLE</a><time datetime="2024-10-08T08:01:14.000Z" title="发表于 2024-10-08 16:01:14">2024-10-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/" title="Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering"><img src="/img/coverImage/cover1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering"/></a><div class="content"><a class="title" href="/Unsupervised-Learning-of-Image-Segmentation-Based-on-Differentiable-Feature-Clustering/" title="Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering">Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering</a><time datetime="2024-10-07T02:13:01.000Z" title="发表于 2024-10-07 10:13:01">2024-10-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/End-to-end-Differentiable-Clustering-with-Associative-Memories/" title="End-to-end Differentiable Clustering with Associative Memories"><img src="/img/coverImage/cover5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="End-to-end Differentiable Clustering with Associative Memories"/></a><div class="content"><a class="title" href="/End-to-end-Differentiable-Clustering-with-Associative-Memories/" title="End-to-end Differentiable Clustering with Associative Memories">End-to-end Differentiable Clustering with Associative Memories</a><time datetime="2024-10-06T12:52:27.000Z" title="发表于 2024-10-06 20:52:27">2024-10-06</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%96%B9%E6%B3%95/"><span class="card-category-list-name">方法</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87/"><span class="card-category-list-name">论文</span><span class="card-category-list-count">12</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87/baseline/"><span class="card-category-list-name">baseline</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87/%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B/"><span class="card-category-list-name">篡改检测</span><span class="card-category-list-count">9</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87/%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">对比学习</span><span class="card-category-list-count">2</span></a></li></ul></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/ASPP/" style="font-size: 1.16em; color: #999b9e">ASPP</a> <a href="/tags/%E6%96%B9%E6%B3%95/" style="font-size: 1.44em; color: #99a7ba">方法</a> <a href="/tags/%E6%8A%A5%E5%91%8A/" style="font-size: 1.39em; color: #99a4b4">报告</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 1.1em; color: #999">数据集</a> <a href="/tags/JS%E6%95%A3%E5%BA%A6/" style="font-size: 1.1em; color: #999">JS散度</a> <a href="/tags/TruFor/" style="font-size: 1.27em; color: #99a0a9">TruFor</a> <a href="/tags/%E5%90%88%E9%9B%86/" style="font-size: 1.44em; color: #99a7ba">合集</a> <a href="/tags/SRM%E5%8D%B7%E7%A7%AF/" style="font-size: 1.21em; color: #999ea4">SRM卷积</a> <a href="/tags/FOCAL/" style="font-size: 1.1em; color: #999">FOCAL</a> <a href="/tags/%E8%AE%B2%E5%BA%A7/" style="font-size: 1.16em; color: #999b9e">讲座</a> <a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 1.5em; color: #99a9bf">论文</a> <a href="/tags/Mask-Guided/" style="font-size: 1.1em; color: #999">Mask-Guided</a> <a href="/tags/BayarConv/" style="font-size: 1.33em; color: #99a2af">BayarConv</a> <a href="/tags/%E6%AF%94%E8%B5%9B/" style="font-size: 1.44em; color: #99a7ba">比赛</a> <a href="/tags/hrnet/" style="font-size: 1.1em; color: #999">hrnet</a> <a href="/tags/clip/" style="font-size: 1.1em; color: #999">clip</a> <a href="/tags/Query-Based/" style="font-size: 1.1em; color: #999">Query-Based</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">19</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><span class="card-archive-list-count">10</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><span class="card-archive-list-count">13</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">69</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-10-12T13:30:18.931Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => { 
  const getCommentUrl = () => {
    const eleGroup = document.querySelectorAll('#recent-posts .article-title')
    let urlArray = []
    eleGroup.forEach(i=>{
      urlArray.push(i.getAttribute('href'))
    })
    return urlArray
  }

  const getCount = () => {
    const runTwikoo = () => {
      twikoo.getCommentsCount({
        envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
        region: '',
        urls: getCommentUrl(),
        includeReply: false
      }).then(function (res) {
        document.querySelectorAll('#recent-posts .twikoo-count').forEach((item,index) => {
          item.textContent = res[index].count
        })
      }).catch(function (err) {
        console.log(err)
      })
    }

      if (typeof twikoo === 'object') {
        runTwikoo()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(runTwikoo)
      }
  }

  window.pjax ? getCount() : window.addEventListener('load', getCount)

})()</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>