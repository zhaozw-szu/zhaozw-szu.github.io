<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection | zhaozw后院</title><meta name="author" content="Zhaozw"><meta name="copyright" content="Zhaozw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="发表于ICML2025，将细粒度的伪造特征转化为语言模型的输入，在LLM提示调优后，得到解释性的deepfake检测结果。">
<meta property="og:type" content="article">
<meta property="og:title" content="Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection">
<meta property="og:url" content="https://zhaozw-szu.github.io/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/index.html">
<meta property="og:site_name" content="zhaozw后院">
<meta property="og:description" content="发表于ICML2025，将细粒度的伪造特征转化为语言模型的输入，在LLM提示调优后，得到解释性的deepfake检测结果。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhaozw-szu.github.io/postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png">
<meta property="article:published_time" content="2025-07-07T11:21:37.000Z">
<meta property="article:modified_time" content="2025-07-10T03:04:58.984Z">
<meta property="article:author" content="Zhaozw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhaozw-szu.github.io/postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaozw-szu.github.io/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-10 11:04:58'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="/config/css/heoMainColor.css"><link rel="stylesheet" type="text/css" href="/config/css/categoryBar.css"><link rel="stylesheet" type="text/css" href="/config/css/icat.css"><link rel="stylesheet" type="text/css" href="/config/css/emoticon.css"><link rel="stylesheet" href="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">129</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-database"></i><span> 数据集</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png')"><nav id="nav"><span id="blog-info"><a href="/" title="zhaozw后院"><img class="site-icon" src="/img/favicon.png"/><span class="site-name">zhaozw后院</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-chart-simple"></i><span> 文库</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/rank/"><i class="fa-fw fas fa-line-chart"></i><span> 期刊等级</span></a></li><li><a class="site-page child" href="/dataset/"><i class="fa-fw fas fa-database"></i><span> 数据集</span></a></li><li><a class="site-page child" href="/code/"><i class="fa-fw fas fa-code"></i><span> 代码</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-sun"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li><li><a class="site-page child" href="/essay/"><i class="fa-fw fas fa-music"></i><span> 即刻短文</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 小游戏</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">创建于</span><time class="post-meta-date-created" datetime="2025-07-07T11:21:37.000Z" title="创建于 2025-07-07 19:21:37">2025-07-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-10T03:04:58.984Z" title="更新于 2025-07-10 11:04:58">2025-07-10</time><span class="post-meta-separator">|</span><i class="fas fa-star fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><span class="post-rank">A类会议,ICML,2025</span></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deepfake/">deepfake</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Unlocking the Capabilities of Large Vision-Language Models for
Generalizable and Explainable Deepfake Detection</p>
<p>Peipeng Yu 1 Jianwei Fei 2 Hui Gao 1 Xuan Feng 1 Zhihua Xia 1
Chip-Hong Chang 3</p>
<p>1 济南大学网络安全学院<br/>2 澳门大学<br/>3
南洋理工大学电气与电子工程学院</p>
<h1 id="摘要">摘要</h1>
<p>​  目前的大型视觉语言模型（LVLMs）在理解多模态数据方面表现出显著的能力，但由于其知识和取证模式的错位，它们在深度造假检测方面的潜力仍未得到充分探索。为此，我们提出了一种新框架，旨在挖掘语言模型在深度伪造检测中的潜力。该框架包含知识引导的伪造检测器（KFD,
Knowledge-guided Forgery Detector）、伪造提示学习器（FPL, Forgery Prompt
Learner）和大型语言模型（LLM, Large Language
Model）。KFD用于计算图像特征与原始或深度伪造图像描述嵌入之间的相关性，从而实现伪造的分类和定位。</p>
<h1 id="引言">1.引言</h1>
<p>​  生成式人工智能的快速发展显著加速了深度造假技术的发展，促进了逼真的面部操作和重演。尽管这些技术在娱乐和艺术领域有着显著的应用，例如稳定扩散（Esser等，2024）和DALL·E（Ramesh等，2021)，但它们的滥用对社会构成了严重的安全威胁（Wang等，2024b）。这些工具允许用户通过输入精心设计的提示，合成出逼真但不存在的内容，使得深度伪造生成比以往任何时候都更加容易获取且潜在危险。</p>
<p>​  大型视觉-语言模型（LVLMs）为解决这一问题提供了有希望的解决方案。这些模型在广泛多样的数据集上进行了预训练，能够捕捉大量关于自然物体的知识，从而显著提高识别被篡改内容的泛化能力。通常，LVLM会使用图像编码器提取图像特征，然后将这些特征与文本提示结合，输入到大型语言模型（LLM）中生成响应。例如，输入一张面部图像，并附带这样的提示：“这是一张用于深度伪造检测的面部图像，不应出现局部颜色差异或明显的拼接痕迹。这是一张深度伪造图像吗？”LVLM可以评估图像是否可能被篡改。然而，现有的LVLM主要针对通用图像理解任务进行了优化，可能无法有效捕捉深度伪造检测所需的细节特征。直接进行微调存在挑战，因为LVLM在处理“颜色差异”或“视觉伪影”等专业术语时，可能难以准确地解释这些术语在伪造检测中的含义。因此，设计一个能够更好地理解这些术语的模型至关重要。</p>
<p>​  本文旨在利用大型视觉-语言模型的能力，解决深度伪造检测任务。人们通常通过特定的描述符，如细微的视觉瑕疵、局部光照不一致和过度平滑的纹理，来识别被篡改的内容。然而，这些特征难以仅通过数据模拟或特征增强准确复制，这限制了现有方法对被篡改内容的全面解读(Zhang
et al.,
2024)。为了解决这一局限性，我们提出探索图像与文本描述之间的校正，以辅助深度伪造检测。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707192635895.png"
alt="image-20250707192635895" />
<figcaption aria-hidden="true">image-20250707192635895</figcaption>
</figure>
<p>​  如图1所示，我们提出了一种基于预训练知识的新型LVLM深度伪造检测框架，该框架能够实现通用且可解释的检测(Lester
et al., 2021; Liu et al.,
2022)。我们首先整合外部知识来训练一个伪造检测器，然后将该检测器的特征融入语言模型（LLM）中以生成响应。<br/>​  具体来说，该框架包含两个关键阶段：知识引导的伪造检测器训练和语言模型提示调优。<br/>​  在知识引导的伪造检测器训练阶段，我们的目标是训练出一个高精度的深度伪造检测器。利用预训练的多模态编码器，我们从图像中提取图像特征，并从原始图像和伪造图像的描述中提取文本特征。通过计算这些图像特征与描述文本嵌入之间的相关性，我们生成了一致性图，这些图展示了视觉内容与文本描述之间的对齐情况。这些地图随后由伪造定位器和分类器处理，生成伪造分割图和伪造评分。<br/>​  在语言模型提示调优阶段，我们将深度伪造检测的知识融入语言模型中，生成伪造检测描述。为了确保深度伪造检测的准确性，我们使用专门为本任务定制的模拟伪造图像-文本对来训练语言模型。我们的贡献总结如下：</p>
<ul>
<li>我们提出了一种基于LVLM的深度伪造检测新框架，通过提示调优集成细粒度伪造提示嵌入，显著增强了模型的泛化能力和可解释性。</li>
<li>我们引入了一种知识引导的伪造检测器，生成伪造一致性图，将原始图像和深度伪造图像的描述文本嵌入与图像特征对齐，以增强泛化能力。</li>
<li>在FF++、CDF1、CDF2、DFD、DFDCP、DFDC和DF40等多个基准上的大量实验表明，我们的方案在泛化性能上优于现有方法，并且具有支持多轮对话的能力。</li>
</ul>
<h1 id="相关工作">2.相关工作</h1>
<h2 id="深度造假检测方法">2.1深度造假检测方法</h2>
<p>​  传统的分类架构在早期深度伪造内容的伪造线索检测方面取得了显著成效。近年来，为了提高深度伪造检测模型的泛化能力，研究者们探索了多种策略，包括数据增强(Li
et al., 2020a; Shiohara &amp; Yamasaki, 2022; Nguyen et al.,
2024)、特征一致性分析 (Zhao et al., 2021b; Yan et al.,
2023)以及频域异常检测(Jeong et al., 2022; Liu et al.,2021; Wang et al.,
2023a),。尽管这些方法提高了检测精度，但它们主要依赖于数据增强或增强特征提取（Yan等，2024），并且往往忽略了外部人类知识的整合。然而，许多深度伪造的特征嵌入在人类知识中，仅靠数据或特征增强难以捕捉这些特征。这一局限性显著限制了现有算法的泛化能力。本文提出了一种基于LVLM的深度伪造检测框架，该框架通过将图像特征与真实/伪造描述对齐，增强了模型检测未知深度伪造的能力。</p>
<h2 id="大型视觉语言模型">2.2大型视觉语言模型</h2>
<p>​  近期，大型视觉-语言模型（LVLMs）在多模态任务中的应用潜力得到了显著展示（Gunjal等，2024；Leng等，2024；Gu等，2024）。典型的LVLM架构包括图像编码器、投影器和语言模型（LLM）。图像编码器从输入图像中提取视觉特征，这些特征随后通过投影器转换为视觉提示嵌入。这些视觉嵌入与文本提示嵌入结合，输入到语言模型中生成响应。基于这一架构，诸如BLIP-2（李等，2023)、LLaVA（刘等，2024a)和MiniGPT-4（朱等，2024)等模型，在自然场景的语言教学（苏等，2023；杨等，2024)和视觉推理（陈等，2024)方面取得了显著进展。一些研究还探讨了低维语言模型（LVLMs）在伪造检测中的应用。FakeShield（徐等，2024)构建了一个大规模的图像-文本数据集，并引入了一种专门用于伪造检测的基于LVLM的框架。FKA-Owl（刘等，2024b)提出了一种新的假新闻检测框架，该框架利用特定于伪造的知识来增强LVLMs，使其能够对操纵行为进行推理。同样，FFAA（黄等，2024)提出了一种多模态LVLM方法，用于可解释的、开放世界的面部伪造分析，突显了LVLMs在伪造检测任务中的潜力。</p>
<p>​  尽管取得了这些进展，当前的基于语言模型的深度伪造检测方法（LVLMs）主要集中在通用语言处理和视觉理解上，往往忽略了对深度伪造检测任务至关重要的细节。这一局限性限制了它们在伪造定位和分类上的效果。为了解决这一问题，我们开发了一种新的深度伪造检测框架，该框架基于语言模型，通过构建细粒度的伪造提示嵌入来指导语言模型检测细微的篡改行为。通过在预训练的语言模型中整合丰富的外部知识，我们的方案不仅增强了模型对各种伪造类型的泛化能力，还保留了其原有的对话功能。</p>
<h1 id="拟提出的方法">3.拟提出的方法</h1>
<p>​  我们的目标是使低维线性模型（LVLMs）能够准确地区分真实与伪造的面孔。尽管LVLMs是在大规模数据集上训练的，但它们主要针对通用图像理解任务，往往缺乏对伪造细节的敏感度。为了解决这一问题，我们提出了一种基于LVLM的新深度伪造检测框架，通过构建精细的伪造提示，提高了对深度伪造伪影的敏感度。如图2所示，我们的方案基于传统的语言模型框架，该框架包括图像编码器、投影器和语言模型。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png"
alt="image-20250707193258253" />
<figcaption aria-hidden="true">image-20250707193258253</figcaption>
</figure>
<p>​  图像编码器从输入图像中提取内容特征，这些特征随后由投影器转换为视觉提示嵌入<span
class="math inline">\(E_{visual}\)</span>。此外，用户查询被编码为问题提示嵌入<span
class="math inline">\(E_{question}\)</span>。为了训练用于伪造检测的模型，我们采用了两阶段流程。<br/>​  在第一阶段，我们训练了一个知识引导型伪造检测器（Knowledge-Guided
forgery
Detector，简称KFD），通过计算图像内容特征与原图/深度伪造图像描述之间的相关性来进行伪造检测和定位。这一阶段确保KFD能够通过学习精细的视觉-文本关联，有效分类和定位伪造物。<br/>​  在第二阶段，我们通过LLM提示调优，将KFD的知识整合到LVLM框架中。具体来说，我们设计了一个伪造提示学习器，用于将与伪造相关的特征转换为伪造提示嵌入。这些嵌入，连同问题和视觉提示的嵌入，随后被输入到LLM中，以生成文本检测结果。<br/>​  为了进一步提高模型的可解释性，我们采用了交替训练策略，同时使用深度伪造数据集和通用视觉问答（VQA,
Visual Question
Answering）数据集。这使得模型不仅能够准确检测深度伪造，还能保持多轮对话的能力。</p>
<h2 id="知识引导型伪造检测器">3.1.知识引导型伪造检测器</h2>
<p>​  <strong>伪造的视觉文本对齐</strong><br/>​  为了获取伪造检测的相关知识，我们受到（Jeong等，2023）的启发，将图像内容特征与预定义的文本描述特征进行对齐，以获得细粒度的伪造特征。这一过程如图3所示。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193606348.png"
alt="image-20250707193606348" />
<figcaption aria-hidden="true">image-20250707193606348</figcaption>
</figure>
<p>​  具体来说，该过程涉及一个预训练的图像编码器和一个预训练的文本编码器。图像和文本编码器均来自ImageBind（Girdhar等，2023），这是一个大规模的多模态预训练模型，拥有广泛的跨模态知识。我们首先定义了真实图像描述（Dreal）和虚假图像描述（Dfake），并使用文本编码器提取它们的语义特征。这些特征与一个可学习的嵌入层连接，生成特定任务的文本嵌入<span
class="math inline">\(F_{t e x t}\in\mathbb{R}^{2\times C_{t e x
t}}\)</span>，其中Ctext表示文本嵌入的维度。对于视觉特征，我们从图像编码器中选取l层，获取每层提取的中间特征。这些中间特征通过线性层处理后，生成视觉特征<span
class="math inline">\(F_{v i s}^{i}\in\mathbb{R}^{H_{i}\times
W_{i}\times C_{t e x
t}}\)</span>，其中i表示第i层。计算视觉特征与文本特征之间的相似度图，并将这些相似度图连接起来形成一致性图。计算一致性图的公式如下：
<span class="math display">\[\rho_{t e x t}=\{F_{v i s}^{i}F_{t e x
t}^{\mp}\}.\]</span>
​  此外，为了优化提取的图像特征，我们计算了参考图像（原始图像）<span
class="math inline">\(F_{v r e f}^{i}\)</span>与输入图像特征<span
class="math inline">\(F_{v i
s}^{i}\)</span>之间的余弦相似度。这种相似度优化提高了图像编码器提取特征的鲁棒性。需要注意的是，参考图像仅在训练阶段使用，在推理阶段则不使用。相似度计算如下：
<span class="math display">\[\rho_{r e f}=\{C o s(F_{v r e f}^{i},F_{v i
s}^{i})\}.\]</span>
​  <strong>伪造定位器和分类器</strong><br/>​  为了提高模型对深度伪造内容的敏感度，我们引入了伪造定位器和伪造分类器，用于识别伪造区域并区分原始图像和深度伪造图像。伪造定位器由三个分支组成，每个分支分别对相应的连贯图进行下采样和上采样处理，随后通过插值、连接和卷积层生成分割图。伪造分类器同样包含三个分支。首先，三个特征图通过卷积和池化操作进行处理，然后将这些特征图连接起来，形成统一的特征表示。接下来，我们使用两个全连接层来计算图像的真实或伪造概率。为了提高伪造分割的准确性，我们采用了Dice损失函数。此外，我们还通过优化文本一致性图（<span
class="math inline">\(\rho_{t e x t}\)</span>）与参考一致性图（<span
class="math inline">\(\rho_{r e
f}\)</span>）之间的匹配度，进一步增强了提取的伪造特征的鲁棒性。这两个图都旨在准确地定位伪造区域。定位损失的公式如下：
<span class="math display">\[\mathcal{L}_{l o c}=D i c
e\big(\phi(\rho_{t e x t}),g t\big)+\lambda D i c e\big(\phi(\rho_{r e
f}),g t\big),\]</span> ​  其中，<span
class="math inline">\(\phi\)</span>代表伪造定位器，gt代表真实掩码。Dice损失用于优化预测分割与真实掩码之间的重叠度。λ是用于平衡这两种损失的权重参数。<br/>​  此外，我们采用二元交叉熵损失来优化伪造分类任务的性能，其公式如下：
<span class="math display">\[\mathcal{L}_{c l
s}=-\left(c\log({\hat{c}})+(1-c)\log(1-{\hat{c}})\right),\]</span>
​  其中<span
class="math inline">\(\hat{c}\)</span>是预测的伪造分数，表示该图像是否为假；c是真实标签（0代表真实，1代表伪造）。</p>
<h2 id="伪造提示学习和llm">3.2.伪造提示学习和LLM</h2>
<p>​  <strong>伪造提示学习</strong><br/>​  为了有效将伪造特征转化为语言模型的输入，我们提出了一种伪造提示学习器，该学习器能够将伪造分割图、伪造分数和一致性图转换为伪造提示嵌入。同时，我们还为伪造提示学习器添加了可学习的提示嵌入，以在深度伪造检测任务中融入额外信息。伪造提示学习器由两个卷积神经网络、一个全连接层以及可学习的提示嵌入<span
class="math inline">\(E_{b a s e}\in\mathbb{R}^{n_1\times C_{e m
b}}\)</span>组成，其中<span class="math inline">\(C_{e m
b}\)</span>表示嵌入向量的维度。具体来说，两个卷积网络将伪造分割图和一致性图转换为向量表示，分别为<span
class="math inline">\(E_{l o c}\in\mathbb{R}^{n_2\times C_{e m
b}}\)</span>和<span class="math inline">\(E_{c o n
s}\in\mathbb{R}^{n_3\times C_{e m b}}\)</span>。伪造得分扩展为<span
class="math inline">\(E_{c l s}\in\mathbb{R}^{1\times C_{e m
b}}\)</span>。这些嵌入被连接并输入到卷积层，生成伪造提示嵌入<span
class="math inline">\(E_{forgery}\in\mathbb{R}^{n_f\times C_{e m
b}}\)</span>。最后，伪造提示嵌入、视觉提示嵌入和问题提示嵌入被输入到语言模型中。</p>
<p>​  <strong>LLM</strong><br/>​  LLM通过处理提示嵌入来解读上下文，并准确识别伪造区域。通过结合视觉细节（来自<span
class="math inline">\(E_{forgery}\)</span>和<span
class="math inline">\(E_{visual}\)</span>)与用户查询（来自<span
class="math inline">\(E_{question}\)</span>)，LLM生成的响应不仅提供了伪造检测的判断，还能精确定位被篡改的区域（如眼睛、嘴巴）。在此过程中，我们利用提示调优和LoRA技术，使用专门为深度伪造检测任务定制的模拟图像-文本对来微调LLM。为了确保LLM生成的响应准确无误，我们采用交叉熵损失来衡量预测响应与目标标签之间的差异。公式如下：
<span class="math display">\[\mathcal{L}_{l l
m}=-\sum_{j}y_{j}\log(\hat{y_{j}}),\]</span> ​  其中，<span
class="math inline">\(\hat{y_{j}}\)</span>表示第j个标记的预测概率，而<span
class="math inline">\(y_{j}\)</span>则是对应的真值标签。</p>
<h2 id="llm提示调优的数据">3.3.LLM提示调优的数据</h2>
<p>​  <strong>伪造数据模拟</strong><br/>​  我们计划让LLM能够识别原始图像和深度伪造图像，并且还能定位伪造区域。这需要对专门描绘被篡改区域的图像-文本对进行训练，但目前这类数据尚不可用。为了解决这一问题，我们借鉴了SBI（Shiohara
&amp;
Yamasaki，2022）的技术，利用现有的真实图像构建图像-文本对。首先，我们从真实图像<span
class="math inline">\(I_{real}\)</span>中生成面部特征点，然后随机选择1到n个区域（如鼻子、嘴巴或眼睛）作为目标伪造区域。接着，我们对真实图像应用轻微的仿射变换，生成仿射变换后的图像<span
class="math inline">\(I_{affine}\)</span>。原始的真实图像作为背景（目标面部），而仿射变换后的图像则作为前景（源面部）。根据Nguyen等人（2024）的方法，我们使用泊松混合技术将前景和背景图像结合在一起。混合过程如下：
<span class="math display">\[{\bf I}_{M}={\bf M}\odot{\bf I}_{a f f i n
e}+{\bf(1-M)\odot I_{r e a l}}\]</span>
​  其中M是基于选定伪造区域构建的凸包掩膜，其取值范围为0到1。符号<span
class="math inline">\(\odot\)</span>表示元素级乘法。</p>
<p>​  <strong>问答内容</strong><br/>​  训练LVLM需要大量的视觉问题与答案对。因此，我们为每张图像构建了相应的文本查询。为了确保与深度伪造检测任务的兼容性，我们在每个查询中加入背景描述，例如：“这是一张专为深度伪造检测设计的面部图像，不应出现局部颜色差异或明显的拼接痕迹。”这可以视为一种人类先验知识。此外，我们将知识引导伪造检测器（KFD）的预测结果融入提示中，例如：“根据KFD预测，伪造分数为0.93。”然后，我们会提出一个与图像内容相关的问题，例如：“这是一张深度伪造图像吗？”LVLM的响应会指出图像中是否存在伪造，并指出伪造的具体位置。例如，“是的，这是一张深度伪造的图片，伪造区域位于图像的中心面部。”在这里，伪造区域是根据在伪造数据模拟过程中选定的区域来定义的。通过定义查询和响应，我们可以训练LVLM来区分原始图像和深度伪造图像。输入到LVLM的提示格式如下：</p>
<p>​  <span class="math inline">\(\#\#\#Human:
&lt;Img&gt;E_{visual}&lt;/Img&gt;E_{f o r g e r y}[Task description][KFD
result] Is this a deepfake
image?&lt;br/&gt;\#\#\#Assistant:,\)</span></p>
<p>​  其中，<span
class="math inline">\(E_{visual}\)</span>表示视觉提示嵌入，<span
class="math inline">\(E_{forgery}\)</span>表示伪造提示学习器学习到的伪造提示嵌入，KFD结果表示伪造分类结果，Task
description提供了深度伪造检测任务的文本描述。</p>
<h1 id="实验">4.实验</h1>
<h2 id="实验设置">4.1.实验设置</h2>
<p>​  数据集。FaceForensics++（FF++）数据集（Rossler等人，2019）包含1000段真实视频和5000段伪造视频，涵盖五类深度伪造类别，是深度伪造检测领域应用最广泛的基准数据集之一。DFD（Dufour与Gully，2019）、CDF1、CDF2（Li等人，2020b）、DFDCP（Dolhansky，2019）、DFDC（Dolhansky等人，2020）以及DF40
(Yan等人）等常用数据集，常用于评估深度伪造检测的泛化性能。所有图像均通过Dlib和RetinaFace进行裁剪处理。本研究仅使用FF++数据集的真实数据进行训练。</p>
<p>​  评估指标。根据现有方法（Shiohara &amp;
Yamasaki，2022；Nguyen等，2024），我们采用视频级别的接收者操作特征曲线下的面积（AUC）和平均精度（AP）作为评估指标。此外，我们通过评估语言模型（LLM）文本输出的真实性（是或否）的二分类结果，来评估其性能，从而计算出LLM响应的视频级别AUC。</p>
<p>​  比较方法。我们根据几种最先进的深度伪造检测算法评估我们的方法(Rossler
et al., 2019; Li et al., 2020a; Qian et al., 2020; Zhao et al.,2021a;
Liu et al., 2021; Zhao et al., 2021b; Cao et al., 2022;Shiohara &amp;
Yamasaki, 2022; Wang et al., 2023a;b; Dong et al., 2023; Yan et al.,
2023; Xu et al., 2023; Yan et al.,2024; Nguyen et al., 2024; Cheng et
al., 2024; Lin et al.,2025; Luo et al., 2024; Ba et al., 2024; Fu et
al., 2025)和基于LVLM的方法 (Khan &amp; Dang-Nguyen, 2024; Su et al.,
2023; Liu et al., 2024b; Wang et al., 2024a)</p>
<p>​  实现细节。我们的方法采用了PandaGPT架构，该架构中集成了ImageBindHuge模型作为图像和文本编码器。我们从编码器的第16层、第24层和第32层提取特征，与文本特征计算一致性图，然后将这些图传递给Vicuna-7B模型进行推理。为了实现多轮对话功能，我们在深度伪造数据集和PandaGPT数据集之间交替训练。伪造区域的数量n设定为3。所有图像均裁剪至224×224。训练在两块Nvidia
RTX 4090
GPU上进行，共50个周期，使用Adam优化器，学习率为1e-4，权重衰减为1e-5。损失参数λ设为1。</p>
<h2 id="与sota检测方法的比较">4.2.与SOTA检测方法的比较</h2>
<p>​  我们首先将我们的方法与几种最先进的深度伪造检测方法进行比较。(Li et
al., 2020a; Shiohara &amp;Yamasaki, 2022; Cao et al., 2022; Huang et
al., 2023; Yan et al., 2024; Tan et al., 2024)</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708152829213.png"
alt="image-20250708152829213" />
<figcaption aria-hidden="true">image-20250708152829213</figcaption>
</figure>
<h2 id="与基于lvlm的方法的比较">4.3.与基于LVLM的方法的比较</h2>
<p>​  LVLM检测性能<br/>​  我们对我们的框架进行了基准测试，与最先进的基于LVLM的分类方法（Khan
&amp; Dang-Nguyen，2024；Lin et al.，2025；Fu et
al.，2025）和VQA方法（Su et al.，2023；Wang et al.，2024a；Liu et
al.，2024b）进行了对比。在这些评估中，图像和相应的查询都被作为输入提供给LVLM，模型需要判断图像的真实性（真实或伪造）。对于分类任务，我们的知识引导的伪造检测器（KFD）在检测性能上显著优于现有的基于LVLM的分类方法。对于VQA模型如PandaGPT、Qwen2-VL和FAKOwl，我们利用LLM的输出（“是”或“否”）来判断真实性，并计算AUC值进行评估。如表3所示，我们的方法在FF++、CDF2、DFD、CDF1和DFDCP数据集上始终优于现有的基于LVLM的VQA方法，表现更佳。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153116786.png"
alt="image-20250708153116786" />
<figcaption aria-hidden="true">image-20250708153116786</figcaption>
</figure>
<p>​  对话式可视化。与传统检测方法不同，我们的方案不仅支持深度伪造检测，还能实现多轮对话功能，让用户能更深入地了解图像内容。图5展示了数据集内和跨数据集评估中的部分对话示例。实验结果表明，我们提出的方案能精准识别图像中的伪造区域，而大语言模型则提供了精准且符合上下文的判断。更多多轮对话示例详见补充材料。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153136840.png"
alt="image-20250708153136840" />
<figcaption aria-hidden="true">image-20250708153136840</figcaption>
</figure>
<h2 id="分析">4.4.分析</h2>
<p>​  <strong>训练图像数量。</strong><br/>​  在真实场景中获取大规模人脸图像往往难以实现，因此我们通过调整训练图像数量来评估模型性能。具体而言，我们从FF++数据集中随机抽取50、100、200和500张真实图像，生成对应的假图像-文本对用于训练。训练步骤固定为500次。基于LLM的响应结果计算视频级AUC指标。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153225551.png"
alt="image-20250708153225551" />
<figcaption aria-hidden="true">image-20250708153225551</figcaption>
</figure>
<p>​  如表4所示，仅使用500张训练图像，我们的方法就达到了业界领先水平。尽管减少训练集的规模会导致精度略有下降，但我们的方法即使在仅有100张训练图像的情况下仍能保持竞争力。这突显了我们框架的稳健性，尤其是在数据稀缺的情况下。</p>
<p>​  <strong>即时调优机制的效能验证。</strong><br/>​  该机制通过将伪造检测知识转化为大模型输入，显著提升识别准确率。其核心架构包含伪造提示学习器（FPL）、大模型及LoRA策略。为评估该模块效能，我们在FF++、CDF2和DFDC数据集上开展消融实验。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153312148.png"
alt="image-20250708153312148" />
<figcaption aria-hidden="true">image-20250708153312148</figcaption>
</figure>
<p>​  通过计算大模型在鉴别真伪图像时的输出结果曲线下面积（AUC），如表5所示：配备伪造提示学习器的模型展现出更优的AUC值，表明其在深度伪造检测任务中具有更强的识别能力。值得注意的是，集成LoRA策略后性能进一步提升，在多个数据集上均取得优于未采用该策略的配置结果。</p>
<p>​  <strong>对训练数据集的通用性</strong><br/>​  深度造假检测的通用性与所使用的训练数据的多样性密切相关。为验证该方法在不同数据集上的有效性，我们在多个训练集上训练模型，并在FF++、CDF2、DFDCP和DFDC数据集上进行了跨数据集评估。</p>
<figure>
<img
src="../postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250708153440331.png"
alt="image-20250708153440331" />
<figcaption aria-hidden="true">image-20250708153440331</figcaption>
</figure>
<p>​  如表6所示，我们根据语言模型的响应计算了检测AUC值。结果表明，本方法在多样化数据集上展现出强大的鲁棒性，并能通过不同类型的伪造样本进行泛化训练，有效应对各类伪造类型。</p>
<p>​  <strong>不同LLM架构的影响</strong><br/>​  深度伪造检测性能与所采用的特定大模型架构密切相关，因为不同大模型具有独特的特性。为评估各类大模型架构的检测表现，我们选取了Llama-3.2-
1B、Llama-3.2-3B和Vicuna-7B三个模型进行测试。实验在FF++、CDF1、DFD和DFDC数据集上展开。如表7所示，我们发现大规模架构始终展现出更优的检测效果。这一趋势表明，参数容量更大的模型更能精准捕捉细微的伪造痕迹。</p>
<p>​  <strong>基于参考优化的消融研究</strong><br/>​  为了验证基于参考的优化过程的有效性，我们在两种配置下训练了模型：一种是使用基于参考的优化过程（ROP），另一种是没有使用该过程。随后，我们评估了该方法在不同数据集上的泛化能力。表8总结了在CDF1、CDF2、DFDC和DFDCP数据集上的泛化性能。结果显示，即使不进行相似性优化，所提出的框架也优于现有方法。此外，引入相似性优化过程后，性能进一步提升，这进一步证明了其在增强深度伪造检测模型泛化能力方面的有效性。</p>
<h1 id="结论">5.结论</h1>
<p>​  在这项研究中，我们提出了一种新的深度伪造检测框架，该框架利用语言模型（LLM）来提升泛化能力和解释性。通过整合知识引导的伪造检测器，我们能够有效地将图像特征与原始图像和深度伪造图像的文本描述进行匹配，从而促进伪造分类和定位。此外，我们还引入了一个伪造提示学习器，能够将细粒度的伪造特征转化为语言模型的输入，确保了准确的伪造检测结果。在包括FF++、CDF1、CDF2、DFD、DFDCP和DFDC在内的多个基准测试中，我们的方案在泛化性能上超越了现有方法。值得注意的是，我们的框架还支持多轮对话，提供了交互式且可解释的检测结果。这些发现强调了基于语言模型的方法在提升深度伪造检测技术方面所具有的潜力。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io">Zhaozw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zhaozw-szu.github.io/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/">https://zhaozw-szu.github.io/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaozw-szu.github.io" target="_blank">zhaozw后院</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/postimages/Unlocking-the-Capabilities-of-Large-Vision-Language-Models-for-Generalizable-and-Explainable-Deepfake-Detection/image-20250707193258253.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Knowledge-Distillation/" title="Knowledge_Distillation"><img class="cover" src="/img/coverImage/cover5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Knowledge_Distillation</div></div></a></div><div class="next-post pull-right"><a href="/FOCAL/" title="Rethinking Image Forgery Detection via Soft Contrastive Learning and Unsupervised Clustering"><img class="cover" src="/postimages/FOCAL/image-20250703230604933.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Rethinking Image Forgery Detection via Soft Contrastive Learning and Unsupervised Clustering</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-tools"><div class="comment-randomInfo"><a onclick="addRandomCommentInfo()" href="javascript:void(0)" rel="external nofollow" data-pjax-state="">匿名评论</a></div></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div><script>function addRandomCommentInfo() {
  if (!confirm('开启匿名评论后，任何人将无法回复你的评论（包括博主），是否开启？')) {
    return;
  }
  var inputElements = document.getElementsByClassName('el-input__inner');
  const adjectives = ['幽默的', '豁达的', '温暖的', '优雅的', '活泼的', '迷人的', '甜美的', '聪明的', '坚定的', '善于思考的'];
  const nouns = ['橙子', '茄子', '西瓜', '辣椒', '草莓', '葡萄', '胡萝卜', '柠檬', '苹果', '香蕉'];
  for(var i = 0; i < inputElements.length; i++) {
    var input = inputElements[i];
    var name = input.getAttribute('name');
    const randomAdj = adjectives[Math.floor(Math.random() * adjectives.length)];
    const randomNoun = nouns[Math.floor(Math.random() * nouns.length)];

    switch (name) {
      case 'nick':
        input.value = `${randomAdj}${randomNoun}`;
        break;
      case 'mail':
        input.value = 'zhaozw-szu@users.noreply.github.com';
        break;
      case 'link':
        input.value = 'https://zhaozw-szu.github.io/';
        break;
      default:
        break;
    }
  }  
}</script></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Zhaozw</div><div class="author-info__description">人完成了引以为豪的事,才能够感到荣耀，否则,虚伪的自豪只会腐蚀心灵。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">129</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaozw-szu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaozw-szu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2300432033@email.szu.edu.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">可以用表情包和匿名评论了</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">2.相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E9%80%A0%E5%81%87%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95"><span class="toc-text">2.1深度造假检测方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E5%9E%8B%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.2大型视觉语言模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8B%9F%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">3.拟提出的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%BC%95%E5%AF%BC%E5%9E%8B%E4%BC%AA%E9%80%A0%E6%A3%80%E6%B5%8B%E5%99%A8"><span class="toc-text">3.1.知识引导型伪造检测器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%AA%E9%80%A0%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%E5%92%8Cllm"><span class="toc-text">3.2.伪造提示学习和LLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#llm%E6%8F%90%E7%A4%BA%E8%B0%83%E4%BC%98%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-text">3.3.LLM提示调优的数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">4.实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-text">4.1.实验设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8Esota%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">4.2.与SOTA检测方法的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E5%9F%BA%E4%BA%8Elvlm%E7%9A%84%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">4.3.与基于LVLM的方法的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-text">4.4.分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">5.结论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Zhaozw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/8.13.8/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script defer src="https://npm.elemecdn.com/swiper@8.4.2/swiper-bundle.min.js"></script><script defer data-pjax src="/js/custom/swiper_init.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'all'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
  //- console.log('MathJax loaded')
} else {
  // 重置 TeX 状态并重新渲染
  MathJax.startup.promise.then(() => {
    MathJax.texReset();  // 重置 TeX 编号等状态
    MathJax.typesetPromise();
  });

  //- MathJax.startup.document.state(0)
  //- MathJax.texReset()
  //- MathJax.typesetPromise()
  //- console.log('MathJax reset')
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://zhaozw.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.39/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script async defer src="/config/js/categoryBar.js"></script><script type="text/javascript" src="/config/js/about.js"></script><script async src="/config/js/waterfall.js"></script><script defer src="/config/js/essay.js"></script><script defer src="/config/js/emoticon.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>